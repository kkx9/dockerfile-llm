{
    "What is the difference between the 'COPY' and 'ADD' commands in a Dockerfile?": "You should check the ADD and COPY documentation for a more detailed description of their behaviors, but in a nutshell, the major difference is that ADD can do more than COPY:\nADD allows <src> to be a URL\nReferring to comments below, the ADD documentation states that:\nIf is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory. Resources from remote URLs are not decompressed.\nNote that the Best practices for writing Dockerfiles suggests using COPY where the magic of ADD is not required. Otherwise, you (since you had to look up this answer) are likely to get surprised someday when you mean to copy keep_this_archive_intact.tar.gz into your container, but instead, you spray the contents onto your filesystem.",
    "How do I pass environment variables to Docker containers?": "You can pass environment variables to your containers with the -e (alias --env) flag.\ndocker run -e xx=yy\nAn example from a startup script:\nsudo docker run -d -t -i -e REDIS_NAMESPACE='staging' \\ \n-e POSTGRES_ENV_POSTGRES_PASSWORD='foo' \\\n-e POSTGRES_ENV_POSTGRES_USER='bar' \\\n-e POSTGRES_ENV_DB_NAME='mysite_staging' \\\n-e POSTGRES_PORT_5432_TCP_ADDR='docker-db-1.hidden.us-east-1.rds.amazonaws.com' \\\n-e SITE_URL='staging.mysite.com' \\\n-p 80:80 \\\n--link redis:redis \\  \n--name container_name dockerhub_id/image_name\nOr, if you don't want to have the value on the command-line where it will be displayed by ps, etc., -e can pull in the value from the current environment if you just give it without the =:\nsudo PASSWORD='foo' docker run  [...] -e PASSWORD [...]\nIf you have many environment variables and especially if they're meant to be secret, you can use an env-file:\n$ docker run --env-file ./env.list ubuntu bash\nThe --env-file flag takes a filename as an argument and expects each line to be in the VAR=VAL format, mimicking the argument passed to --env. Comment lines need only be prefixed with #",
    "docker push error: denied: requested access to the resource is denied": "You may need to switch your docker repo to private before docker push.\nThanks to the answer provided by Dean Wu and this comment by ses, before pushing, remember to log out, then log in from the command line to your docker hub account\n# you may need log out first `docker logout` ref. https://stackoverflow.com/a/53835882/248616\ndocker login\nAccording to the docs:\nYou need to include the namespace for Docker Hub to associate it with your account.\nThe namespace is the same as your Docker Hub account name.\nYou need to rename the image to YOUR_DOCKERHUB_NAME/docker-whale.\nSo, this means you have to tag your image before pushing:\ndocker tag firstimage YOUR_DOCKERHUB_NAME/firstimage\nand then you should be able to push it.\ndocker push YOUR_DOCKERHUB_NAME/firstimage",
    "Difference between RUN and CMD in a Dockerfile": "RUN is an image build step, the state of the container after a RUN command will be committed to the container image. A Dockerfile can have many RUN steps that layer on top of one another to build the image.\nCMD is the command the container executes by default when you launch the built image. A Dockerfile will only use the final CMD defined. The CMD can be overridden when starting a container with docker run $image $other_command.\nENTRYPOINT is also closely related to CMD and can modify the way a CMD is interpreted when a container is started from an image.",
    "What's the difference between Docker Compose vs. Dockerfile": "Dockerfile\nA Dockerfile is a simple text file that contains the commands a user could call to assemble an image.\nExample, Dockerfile\nFROM ubuntu:latest\nMAINTAINER john doe \n\nRUN apt-get update\nRUN apt-get install -y python python-pip wget\nRUN pip install Flask\n\nADD hello.py /home/hello.py\n\nWORKDIR /home\nDocker Compose\nDocker Compose\nis a tool for defining and running multi-container Docker applications.\ndefine the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.\nget an app running in one command by just running docker-compose up\nExample, docker-compose.yml\nversion: \"3\"\nservices:\n  web:\n    build: .\n    ports:\n    - '5000:5000'\n    volumes:\n    - .:/code\n    - logvolume01:/var/log\n    links:\n    - redis\n  redis:\n    image: redis\n    volumes:\n      logvolume01: {}",
    "COPY with docker but with exclusion": "Create file .dockerignore in your docker build context directory (so in this case, most likely a directory that is a parent to node_modules) with one line in it:\n**/node_modules\nalthough you probably just want:\nnode_modules\nInfo about dockerignore: https://docs.docker.com/engine/reference/builder/#dockerignore-file",
    "How do I make a comment in a Dockerfile?": "You can use # at the beginning of a line to start a comment (whitespaces before # are allowed):\n# do some stuff\nRUN apt-get update \\\n    # install some packages\n    && apt-get install -y cron\n#'s in the middle of a string are passed to the command itself, e.g.:\nRUN echo 'we are running some # of cool things'",
    "How to set image name in Dockerfile?": "Workaround using docker-compose\nTagging of the image isn't supported inside the Dockerfile. This needs to be done in your build command. As a workaround, you can do the build with a docker-compose.yml that identifies the target image name and then run a docker-compose build. A sample docker-compose.yml would look like\nversion: '2'\n\nservices:\n  man:\n    build: .\n    image: dude/man:v2\nThat said, there's a push against doing the build with compose since that doesn't work with swarm mode deploys. So you're back to running the command as you've given in your question:\ndocker build -t dude/man:v2 .\nPersonally, I tend to build with a small shell script in my folder (build.sh) which passes any args and includes the name of the image there to save typing. And for production, the build is handled by a ci/cd server that has the image name inside the pipeline script.",
    "Dockerfile copy keep subdirectory structure": "Remove star from COPY, with this Dockerfile:\nFROM ubuntu\nCOPY files/ /files/\nRUN ls -la /files/*\nStructure is there:\n$ docker build .\nSending build context to Docker daemon 5.632 kB\nSending build context to Docker daemon \nStep 0 : FROM ubuntu\n ---> d0955f21bf24\nStep 1 : COPY files/ /files/\n ---> 5cc4ae8708a6\nRemoving intermediate container c6f7f7ec8ccf\nStep 2 : RUN ls -la /files/*\n ---> Running in 08ab9a1e042f\n/files/folder1:\ntotal 8\ndrwxr-xr-x 2 root root 4096 May 13 16:04 .\ndrwxr-xr-x 4 root root 4096 May 13 16:05 ..\n-rw-r--r-- 1 root root    0 May 13 16:04 file1\n-rw-r--r-- 1 root root    0 May 13 16:04 file2\n\n/files/folder2:\ntotal 8\ndrwxr-xr-x 2 root root 4096 May 13 16:04 .\ndrwxr-xr-x 4 root root 4096 May 13 16:05 ..\n-rw-r--r-- 1 root root    0 May 13 16:04 file1\n-rw-r--r-- 1 root root    0 May 13 16:04 file2\n ---> 03ff0a5d0e4b\nRemoving intermediate container 08ab9a1e042f\nSuccessfully built 03ff0a5d0e4b",
    "How to add users to Docker container?": "The trick is to use useradd instead of its interactive wrapper adduser. I usually create users with:\nRUN useradd -ms /bin/bash newuser\nwhich creates a home directory for the user and ensures that bash is the default shell.\nYou can then add:\nUSER newuser\nWORKDIR /home/newuser\nto your dockerfile. Every command afterwards as well as interactive sessions will be executed as user newuser:\ndocker run -t -i image\nnewuser@131b7ad86360:~$\nYou might have to give newuser the permissions to execute the programs you intend to run before invoking the user command.\nUsing non-privileged users inside containers is a good idea for security reasons. It also has a few drawbacks. Most importantly, people deriving images from your image will have to switch back to root before they can execute commands with superuser privileges.",
    "How to copy multiple files in one layer using a Dockerfile?": "COPY README.md package.json gulpfile.js __BUILD_NUMBER ./\nor\nCOPY [\"__BUILD_NUMBER\", \"README.md\", \"gulpfile\", \"another_file\", \"./\"]\nYou can also use wildcard characters in the sourcefile specification. See the docs for a little more detail.\nDirectories are special! If you write\nCOPY dir1 dir2 ./\nthat actually works like\nCOPY dir1/* dir2/* ./\nIf you want to copy multiple directories (not their contents) under a destination directory in a single command, you'll need to set up the build context so that your source directories are under a common parent and then COPY that parent.",
    "Docker: How to use bash with an Alpine based docker image?": "Alpine docker image doesn't have bash installed by default. You will need to add the following commands to get bash:\nRUN apk update && apk add bash\nIf you're using Alpine 3.3+ then you can just do:\nRUN apk add --no-cache bash\nTo keep the docker image size small. (Thanks to comment from @sprkysnrky)\nIf you just want to connect to the container and don't need bash, you can use:\ndocker run --rm -i -t alpine /bin/sh --login",
    "Difference between links and depends_on in docker_compose.yml": "The post needs an update after the links option is deprecated.\nBasically, links is no longer needed because its main purpose, making container reachable by another by adding environment variable, is included implicitly with network. When containers are placed in the same network, they are reachable by each other using their container name and other alias as host.\nFor docker run, --link is also deprecated and should be replaced by a custom network.\ndocker network create mynet\ndocker run -d --net mynet --name container1 my_image\ndocker run -it --net mynet --name container1 another_image\ndepends_on expresses start order (and implicitly image pulling order), which was a good side effect of links.",
    "Change directory command in Docker?": "To change into another directory use WORKDIR. All the RUN, CMD and ENTRYPOINT commands after WORKDIR will be executed from that directory.\nRUN git clone XYZ \nWORKDIR \"/XYZ\"\nRUN make",
    "Add a volume to Docker, but exclude a sub-folder": "Using docker-compose I'm able to use node_modules locally, but ignore it in the docker container using the following syntax in the docker-compose.yml\nvolumes:\n   - './angularApp:/opt/app'\n   - /opt/app/node_modules/\nSo everything in ./angularApp is mapped to /opt/app and then I create another mount volume /opt/app/node_modules/ which is now empty directory - even if in my local machine ./angularApp/node_modules is not empty.",
    "Dockerfile if else condition with external arguments": "It might not look that clean but you can have your Dockerfile (conditional) as follow:\nFROM centos:7\nARG arg\nRUN if [[ -z \"$arg\" ]] ; then echo Argument not provided ; else echo Argument is $arg ; fi\nand then build the image as:\ndocker build -t my_docker .  --build-arg arg=45\nor\ndocker build -t my_docker . ",
    "How to get an environment variable value into Dockerfile during \"docker build\"?": "You should use the ARG directive in your Dockerfile which is meant for this purpose.\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.\nSo your Dockerfile will have this line:\nARG request_domain\nor if you'd prefer a default value:\nARG request_domain=127.0.0.1\nNow you can reference this variable inside your Dockerfile:\nENV request_domain=$request_domain\nthen you will build your container like so:\n$ docker build --build-arg request_domain=mydomain Dockerfile\n\nNote 1: Your image will not build if you have referenced an ARG in your Dockerfile but excluded it in --build-arg.\nNote 2: If a user specifies a build argument that was not defined in the Dockerfile, the build outputs a warning:\n[Warning] One or more build-args [foo] were not consumed.",
    "An error, \"failed to solve with frontend dockerfile.v0\"": "I had experienced this issue after upgrading to the latest Docker Desktop version on Mac. Solved with the comment on this issue.\nSolution: Don't use buildkit and it works for me.\nexport DOCKER_BUILDKIT=0\nexport COMPOSE_DOCKER_CLI_BUILD=0",
    "Build and run Dockerfile with one command": "If you want to avoid tagging, docker build -q outputs nothing but the final image hash, which you can use as the argument to docker run:\ndocker run -it $(docker build -q .)\nAnd add --rm to docker run if you want the container removed automatically when it exits.\ndocker run --rm -it $(docker build -q .)",
    "Docker - unable to prepare context: unable to evaluate symlinks in Dockerfile path: GetFileAttributesEx": "While executing the following command,\ndocker build -t docker-whale .\ncheck that Dockerfile is present in your current working directory.",
    "How to define a variable in a Dockerfile?": "You can use ARG - see https://docs.docker.com/engine/reference/builder/#arg\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag. If a user specifies a build argument that was not defined in the Dockerfile, the build outputs an error.\nCan be useful with COPY during build time (e.g. copying tag specific content like specific folders) For example:\nARG MODEL_TO_COPY\nCOPY application ./application\nCOPY $MODEL_TO_COPY ./application/$MODEL_TO_COPY\nWhile building the container:\ndocker build --build-arg MODEL_TO_COPY=model_name -t <container>:<model_name specific tag> .",
    "What is the point of WORKDIR on Dockerfile?": "According to the documentation:\nThe WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile. If the WORKDIR doesn\u2019t exist, it will be created even if it\u2019s not used in any subsequent Dockerfile instruction.\nAlso, in the Docker best practices it recommends you to use it:\n... you should use WORKDIR instead of proliferating instructions like RUN cd \u2026 && do-something, which are hard to read, troubleshoot, and maintain.\nI would suggest to keep it.\nI think you can refactor your Dockerfile to something like:\nFROM node:latest\nWORKDIR /usr/src/app\nCOPY package.json .\nRUN npm install\nCOPY . ./\nEXPOSE 3000\nCMD [ \"npm\", \"start\" ] ",
    "Why doesn't Python app print anything when run in a detached docker container?": "Finally I found a solution to see Python output when running daemonized in Docker, thanks to @ahmetalpbalkan over at GitHub. Answering it here myself for further reference :\nUsing unbuffered output with\nCMD [\"python\",\"-u\",\"main.py\"]\ninstead of\nCMD [\"python\",\"main.py\"]\nsolves the problem; you can see the output now (both, stderr and stdout) via\ndocker logs myapp\nwhy -u ref\n- print is indeed buffered and docker logs will eventually give you that output, just after enough of it will have piled up\n- executing the same script with python -u gives instant output as said above\n- import logging + logging.warning(\"text\") gives the expected result even without -u\nwhat it means by python -u ref. > python --help | grep -- -u\n-u     : force the stdout and stderr streams to be unbuffered;",
    "Understanding \"VOLUME\" instruction in DockerFile": "In short: No, your VOLUME instruction is not correct.\nDockerfile's VOLUME specify one or more volumes given container-side paths. But it does not allow the image author to specify a host path. On the host-side, the volumes are created with a very long ID-like name inside the Docker root. On my machine this is /var/lib/docker/volumes.\nNote: Because the autogenerated name is extremely long and makes no sense from a human's perspective, these volumes are often referred to as \"unnamed\" or \"anonymous\".\nYour example that uses a '.' character will not even run on my machine, no matter if I make the dot the first or second argument. I get this error message:\ndocker: Error response from daemon: oci runtime error: container_linux.go:265: starting container process caused \"process_linux.go:368: container init caused \"open /dev/ptmx: no such file or directory\"\".\nI know that what has been said to this point is probably not very valuable to someone trying to understand VOLUME and -v and it certainly does not provide a solution for what you try to accomplish. So, hopefully, the following examples will shed some more light on these issues.\nMinitutorial: Specifying volumes\nGiven this Dockerfile:\nFROM openjdk:8u131-jdk-alpine\nVOLUME vol1 vol2\n(For the outcome of this minitutorial, it makes no difference if we specify vol1 vol2 or /vol1 /vol2 \u2014 this is because the default working directory within a Dockerfile is /)\nBuild it:\ndocker build -t my-openjdk\nRun:\ndocker run --rm -it my-openjdk\nInside the container, run ls in the command line and you'll notice two directories exist; /vol1 and /vol2.\nRunning the container also creates two directories, or \"volumes\", on the host-side.\nWhile having the container running, execute docker volume ls on the host machine and you'll see something like this (I have replaced the middle part of the name with three dots for brevity):\nDRIVER    VOLUME NAME\nlocal     c984...e4fc\nlocal     f670...49f0\nBack in the container, execute touch /vol1/weird-ass-file (creates a blank file at said location).\nThis file is now available on the host machine, in one of the unnamed volumes lol. It took me two tries because I first tried the first listed volume, but eventually I did find my file in the second listed volume, using this command on the host machine:\nsudo ls /var/lib/docker/volumes/f670...49f0/_data\nSimilarly, you can try to delete this file on the host and it will be deleted in the container as well.\nNote: The _data folder is also referred to as a \"mount point\".\nExit out from the container and list the volumes on the host. They are gone. We used the --rm flag when running the container and this option effectively wipes out not just the container on exit, but also the volumes.\nRun a new container, but specify a volume using -v:\ndocker run --rm -it -v /vol3 my-openjdk\nThis adds a third volume and the whole system ends up having three unnamed volumes. The command would have crashed had we specified only -v vol3. The argument must be an absolute path inside the container. On the host-side, the new third volume is anonymous and resides together with the other two volumes in /var/lib/docker/volumes/.\nIt was stated earlier that the Dockerfile can not map to a host path which sort of pose a problem for us when trying to bring files in from the host to the container during runtime. A different -v syntax solves this problem.\nImagine I have a subfolder in my project directory ./src that I wish to sync to /src inside the container. This command does the trick:\ndocker run -it -v $(pwd)/src:/src my-openjdk\nBoth sides of the : character expects an absolute path. Left side being an absolute path on the host machine, right side being an absolute path inside the container. pwd is a command that \"print current/working directory\". Putting the command in $() takes the command within parenthesis, runs it in a subshell and yields back the absolute path to our project directory.\nPutting it all together, assume we have ./src/Hello.java in our project folder on the host machine with the following contents:\npublic class Hello {\n    public static void main(String... ignored) {\n        System.out.println(\"Hello, World!\");\n    }\n}\nWe build this Dockerfile:\nFROM openjdk:8u131-jdk-alpine\nWORKDIR /src\nENTRYPOINT javac Hello.java && java Hello\nWe run this command:\ndocker run -v $(pwd)/src:/src my-openjdk\nThis prints \"Hello, World!\".\nThe best part is that we're completely free to modify the .java file with a new message for another output on a second run - without having to rebuild the image =)\nFinal remarks\nI am quite new to Docker, and the aforementioned \"tutorial\" reflects information I gathered from a 3-day command line hackathon. I am almost ashamed I haven't been able to provide links to clear English-like documentation backing up my statements, but I honestly think this is due to a lack of documentation and not personal effort. I do know the examples work as advertised using my current setup which is \"Windows 10 -> Vagrant 2.0.0 -> Docker 17.09.0-ce\".\nThe tutorial does not solve the problem \"how do we specify the container's path in the Dockerfile and let the run command only specify the host path\". There might be a way, I just haven't found it.\nFinally, I have a gut feeling that specifying VOLUME in the Dockerfile is not just uncommon, but it's probably a best practice to never use VOLUME. For two reasons. The first reason we have already identified: We can not specify the host path - which is a good thing because Dockerfiles should be very agnostic to the specifics of a host machine. But the second reason is people might forget to use the --rm option when running the container. One might remember to remove the container but forget to remove the volume. Plus, even with the best of human memory, it might be a daunting task to figure out which of all anonymous volumes are safe to remove.",
    "Integrating Python Poetry with Docker": "There are several things to keep in mind when using Poetry together with Docker.\nInstallation\nOfficial way to install Poetry is via:\ncurl -sSL https://install.python-poetry.org | python3 -\nThis way allows Poetry and its dependencies to be isolated from your dependencies.\nYou can also use pip install 'poetry==$POETRY_VERSION'. But, this will install Poetry and its dependencies into your main site-packages/. It might not be ideal.\nAlso, pin this version in your pyproject.toml as well:\n[build-system]\n# Should be the same as `$POETRY_VERSION`:\nrequires = [\"poetry-core>=1.6\"]\nbuild-backend = \"poetry.core.masonry.api\"\nIt will protect you from version mismatch between your local and Docker environments.\nCaching dependencies\nWe want to cache our requirements and only reinstall them when pyproject.toml or poetry.lock files change. Otherwise builds will be slow. To achieve working cache layer we should put:\nCOPY poetry.lock pyproject.toml /code/\nafter Poetry is installed, but before any other files are added.\nVirtualenv\nThe next thing to keep in mind is virtualenv creation. We do not need it in Docker. It is already isolated. So, we use POETRY_VIRTUALENVS_CREATE=false or poetry config virtualenvs.create false setting to turn it off.\nDevelopment vs. Production\nIf you use the same Dockerfile for both development and production as I do, you will need to install different sets of dependencies based on some environment variable:\npoetry install $(test \"$YOUR_ENV\" == production && echo \"--only=main\")\nThis way $YOUR_ENV will control which dependencies set will be installed: all (default) or production only with --only=main flag.\nYou may also want to add some more options for better experience:\n--no-interaction not to ask any interactive questions\n--no-ansi flag to make your output more log friendly\nResult\nYou will end up with something similar to:\nFROM python:3.11.5-slim-bookworm\n\nARG YOUR_ENV\n\nENV YOUR_ENV=${YOUR_ENV} \\\n  PYTHONFAULTHANDLER=1 \\\n  PYTHONUNBUFFERED=1 \\\n  PYTHONHASHSEED=random \\\n  PIP_NO_CACHE_DIR=off \\\n  PIP_DISABLE_PIP_VERSION_CHECK=on \\\n  PIP_DEFAULT_TIMEOUT=100 \\\n  # Poetry's configuration:\n  POETRY_NO_INTERACTION=1 \\\n  POETRY_VIRTUALENVS_CREATE=false \\\n  POETRY_CACHE_DIR='/var/cache/pypoetry' \\\n  POETRY_HOME='/usr/local'\n  POETRY_VERSION=1.7.1\n  # ^^^\n  # Make sure to update it!\n\n# System deps:\nRUN curl -sSL https://install.python-poetry.org | python3 -\n\n# Copy only requirements to cache them in docker layer\nWORKDIR /code\nCOPY poetry.lock pyproject.toml /code/\n\n# Project initialization:\nRUN poetry install $(test \"$YOUR_ENV\" == production && echo \"--only=main\") --no-interaction --no-ansi\n\n# Creating folders, and files for a project:\nCOPY . /code\nYou can find a fully working real-life example here.",
    "How to name Dockerfiles": "[Please read the full answer]Don't change the name of the dockerfile if you want to use the autobuilder at hub.docker.com. Don't use an extension for docker files, leave it null. File name should just be: (no extension at all)\nDockerfile\nHowever, now you can name dockerfiles like,\ntest1.Dockerfile\n$ docker build -f dockerfiles/test1.Dockerfile  -t test1_app .\nor\nDockerfile.test1\n$ docker build -f dockerfiles/Dockerfile.test1  -t test1_app .\nThis will also work.\nIf you handle multiple files that live in the same context, you could use STDIN:\ntest1.Dockerfile\n$ docker build -t test1_app - < test1.Dockerfile",
    "Why is Docker installed but not Docker Compose?": "You also need to install Docker Compose.\nSee the manual. Here are the commands you need to execute:\nsudo curl -L \"https://github.com/docker/compose/releases/download/v2.29.2/docker-compose-$(uname -s)-$(uname -m)\"  -o /usr/local/bin/docker-compose\nsudo mv /usr/local/bin/docker-compose /usr/bin/docker-compose\nsudo chmod +x /usr/bin/docker-compose      \nNote:\nMake sure that the link pointing to the GitHub release is not outdated!. Check out the latest releases on GitHub.",
    "Multiple RUN vs. single chained RUN in Dockerfile, which is better?": "When possible, I always merge together commands that create files with commands that delete those same files into a single RUN line. This is because each RUN line adds a layer to the image, the output is quite literally the filesystem changes that you could view with docker diff on the temporary container it creates. If you delete a file that was created in a different layer, all the union filesystem does is register the filesystem change in a new layer, the file still exists in the previous layer and is shipped over the networked and stored on disk. So if you download source code, extract it, compile it into a binary, and then delete the tgz and source files at the end, you really want this all done in a single layer to reduce image size.\nNext, I personally split up layers based on their potential for reuse in other images and expected caching usage. If I have 4 images, all with the same base image (e.g. debian), I may pull a collection of common utilities to most of those images into the first run command so the other images benefit from caching.\nOrder in the Dockerfile is important when looking at image cache reuse. I look at any components that will update very rarely, possibly only when the base image updates and put those high up in the Dockerfile. Towards the end of the Dockerfile, I include any commands that will run quick and may change frequently, e.g. adding a user with a host specific UID or creating folders and changing permissions. If the container includes interpreted code (e.g. JavaScript) that is being actively developed, that gets added as late as possible so that a rebuild only runs that single change.\nIn each of these groups of changes, I consolidate as best I can to minimize layers. So if there are 4 different source code folders, those get placed inside a single folder so it can be added with a single command. Any package installs from something like apt-get are merged into a single RUN when possible to minimize the amount of package manager overhead (updating and cleaning up).\nUpdate for multi-stage builds:\nI worry much less about reducing image size in the non-final stages of a multi-stage build. When these stages aren't tagged and shipped to other nodes, you can maximize the likelihood of a cache reuse by splitting each command to a separate RUN line.\nHowever, this isn't a perfect solution to squashing layers since all you copy between stages are the files, and not the rest of the image meta-data like environment variable settings, entrypoint, and command. And when you install packages in a linux distribution, the libraries and other dependencies may be scattered throughout the filesystem, making a copy of all the dependencies difficult.\nBecause of this, I use multi-stage builds as a replacement for building binaries on a CI/CD server, so that my CI/CD server only needs to have the tooling to run docker build, and not have a jdk, nodejs, go, and any other compile tools installed.",
    "What is the use of PYTHONUNBUFFERED in docker file?": "Setting PYTHONUNBUFFERED to a non-empty value different from 0 ensures that the python output i.e. the stdout and stderr streams are sent straight to terminal (e.g. your container log) without being first buffered and that you can see the output of your application (e.g. django logs) in real time.\nThis also ensures that no partial output is held in a buffer somewhere and never written in case the python application crashes.\nSince this has been mentioned in several comments and supplementary answers, note that PYTHONUNBUFFERED has absolutely no influence on the input (i.e. the stdin stream).\nIn other words, turning off buffering to stdout/stderr in a docker container is mainly a concern of getting as much information from your running application as fast as possible in the container log and not loosing anything in case of a crash.\nNote that turning buffering off can have an impact on performance depending on your hardware/environment. Meanwhile it should be minor in most situations (unless you have slow disks or are writing a tremendous amount of logs or had the bad idea to configure your docker daemon to write your logs on a slow network drive...). If this is a concern, buffering can be left on and you can flush the buffer directly from your application when needed. See link [4] below on this subject.\nReferences:\n[1] https://docs.python.org/3/using/cmdline.html#envvar-PYTHONUNBUFFERED\n[2] https://alphacoder.xyz/dockerizing-django/\n[3] https://towardsdatascience.com/how-to-contain-your-first-django-in-docker-and-access-it-from-aws-fdb0081bdf1d\n[4] https://github.com/aws/amazon-sagemaker-examples/issues/319",
    "How to pass arguments to a Dockerfile?": "As of Docker 1.9, You are looking for --build-arg and the ARG instruction.\nCheck out this document for reference. This will allow you to add ARG arg to the Dockerfile and then build with\ndocker build --build-arg arg=2.3 .",
    "How do I Docker COPY as non root?": "For versions v17.09.0-ce and newer\nUse the optional flag --chown=<user>:<group> with either the ADD or COPY commands.\nFor example\nCOPY --chown=<user>:<group> <hostPath> <containerPath>\nThe documentation for the --chown flag is now live on the main Dockerfile Reference page.\nIssue 34263 has been merged and is available in release v17.09.0-ce.\nFor versions older than v17.09.0-ce\nDocker doesn't support COPY as a user other than root. You need to chown / chmod the file after the COPY command.\nExample Dockerfile:\nfrom centos:6\nRUN groupadd -r myuser && adduser -r -g myuser myuser\nUSER myuser\n#Install code, configure application, etc...\nUSER root\nCOPY run-my-app.sh /usr/local/bin/run-my-app.sh\nRUN chown myuser:myuser /usr/local/bin/run-my-app.sh && \\\n    chmod 744 /usr/local/bin/run-my-app.sh\nUSER myuser\nENTRYPOINT [\"/usr/local/bin/run-my-app.sh\"]\nPrevious to v17.09.0-ce, the Dockerfile Reference for the COPY command said:\nAll new files and directories are created with a UID and GID of 0.\nHistory This feature has been tracked through multiple GitHub issues: 6119, 9943, 13600, 27303, 28499, Issue 30110.\nIssue 34263 is the issue that implemented the optional flag functionality and Issue 467 updated the documentation.",
    "Connect to mysql in a docker container from the host": "If your Docker MySQL host is running correctly you can connect to it from local machine, but you should specify host, port and protocol like this:\nmysql -h localhost -P 3306 --protocol=tcp -u root\nChange 3306 to port number you have forwarded from Docker container (in your case it will be 12345).\nBecause you are running MySQL inside Docker container, socket is not available and you need to connect through TCP. Setting \"--protocol\" in the mysql command will change that.",
    "standard_init_linux.go:190: exec user process caused \"no such file or directory\" - Docker": "Use notepad++, go to edit -> EOL conversion -> change from CRLF to LF.\nupdate: For VScode users: you can change CRLF to LF by clicking on CRLF present on lower right side in the status bar",
    "/bin/sh: apt-get: not found": "The image you're using is Alpine based, so you can't use apt-get because it's Ubuntu's package manager.\nTo fix this just use:\napk update and apk add",
    "Multiple FROMs - what it means": "As of May 2017, multiple FROMs can be used in a single Dockerfile.\nSee \"Builder pattern vs. Multi-stage builds in Docker\" (by Alex Ellis) and PR 31257 by T\u00f5nis Tiigi.\nThe general syntax involves adding FROM additional times within your Dockerfile - whichever is the last FROM statement is the final base image. To copy artifacts and outputs from intermediate images use COPY --from=<base_image_number>.\nFROM golang:1.7.3 as builder\nWORKDIR /go/src/github.com/alexellis/href-counter/\nRUN go get -d -v golang.org/x/net/html  \nCOPY app.go    .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\nFROM alpine:latest  \nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\nCOPY --from=builder /go/src/github.com/alexellis/href-counter/app    .\nCMD [\"./app\"]  \nThe result would be two images, one for building, one with just the resulting app (much, much smaller)\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n\nmulti               latest              bcbbf69a9b59        6 minutes ago       10.3MB  \ngolang              1.7.3               ef15416724f6        4 months ago        672MB  \nwhat is a base image?\nA set of files, plus EXPOSE'd ports, ENTRYPOINT and CMD.\nYou can add files and build a new image based on that base image, with a new Dockerfile starting with a FROM directive: the image mentioned after FROM is \"the base image\" for your new image.\ndoes it mean that if I declare neo4j/neo4j in a FROM directive, that when my image is run the neo database will automatically run and be available within the container on port 7474?\nOnly if you don't overwrite CMD and ENTRYPOINT.\nBut the image in itself is enough: you would use a FROM neo4j/neo4j if you had to add files related to neo4j for your particular usage of neo4j.\n2018: With the introduction of the --target option in docker build, you gain even more control over multi-stage builds.\nThis feature enables you to select which FROM statement in your Dockerfile you wish to build, allowing for more modular and efficient Docker images. This is especially useful in scenarios where you might want to:\nBuild Only the Dependencies: Create an image that only contains the dependencies of your project. This can be useful for caching purposes or for environments where you only need to run tests or static analysis tools.\nSeparate Build and Runtime Environments: Compile or build your application in a full-featured build environment but create a smaller, more secure image for deployment that only includes the runtime environment and the compiled application.\nCreate Images for Different Environments: Have different stages for development, testing, and production environments, each tailored with the specific tools and configurations needed for those environments.\nExample Using --target\nGiven a Dockerfile with multiple stages named builder, tester, and deployer, you can build up to the tester stage using the --target option like so:\ndocker build --target tester -t myapp-test .\nThis command tells Docker to stop building after the tester stage has been completed, thus creating an image that includes everything from the base image up to the tester stage, but excluding anything from deployer stage and beyond.\nDockerfile Example with --target Usage\n# Builder stage\nFROM golang:1.7.3 as builder\nWORKDIR /go/src/github.com/example/project/\n# Assume app.go exists and has a function\nCOPY app.go .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\n# Tester stage\nFROM builder as tester\nCOPY . .\nRUN go test ./...\n\n# Deployer stage\nFROM alpine:latest as deployer\nCOPY --from=builder /go/src/github.com/example/project/app /app\nCMD [\"/app\"]\nUsing the --target option with this Dockerfile allows for flexibility in building images tailored for specific steps of the development lifecycle.\nAs illustrated in \"Building a multi-stage Dockerfile with --target flag builds all stages instead of just the specified one\", this works well with BuildKit, which is now (2023+) the default builder.\nFrom that page, you have Igor Kulebyakin's answer:\nIf one wants to make sure that the current target stage is force re-built even if it has already been cached without rebuilding the previous dependent stages, once can use the docker build --no-cache-filter flag.\nAn example, given you have a multi-stage Dockerfile with a 'test' stage, would be:\ndocker build --no-cache-filter test --target test --tag your-image-name:version .",
    "How can I use a variable inside a Dockerfile CMD?": "When you use an execution list, as in...\nCMD [\"django-admin\", \"startproject\", \"$PROJECTNAME\"]\n...then Docker will execute the given command directly, without involving a shell. Since there is no shell involved, that means:\nNo variable expansion\nNo wildcard expansion\nNo i/o redirection with >, <, |, etc\nNo multiple commands via command1; command2\nAnd so forth.\nIf you want your CMD to expand variables, you need to arrange for a shell. You can do that like this:\nCMD [\"sh\", \"-c\", \"django-admin startproject $PROJECTNAME\"]\nOr you can use a simple string instead of an execution list, which gets you a result largely identical to the previous example:\nCMD django-admin startproject $PROJECTNAME",
    "How do I set environment variables during the \"docker build\" process?": "ARG is for setting environment variables which are used during the docker build process - they are not present in the final image, which is why you don't see them when you use docker run.\nYou use ARG for settings that are only relevant when the image is being built, and aren't needed by containers which you run from the image. You can use ENV for environment variables to use during the build and in containers.\nWith this Dockerfile:\nFROM ubuntu\nARG BUILD_TIME=abc\nENV RUN_TIME=123\nRUN touch /env.txt\nRUN printenv > /env.txt\nYou can override the build arg as you have done with docker build -t temp --build-arg BUILD_TIME=def .. Then you get what you expect:\n> docker run temp cat /env.txt                                                                                         \nHOSTNAME=b18b9cafe0e0                                                                                                  \nRUN_TIME=123                                                                                                           \nHOME=/root                                                                                                             \nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin                                                      \nBUILD_TIME=def                                                                                                         \nPWD=/ ",
    "what is docker run -it flag?": "-it is short for --interactive + --tty. When you docker run with this command it takes you straight inside the container.\n-d is short for --detach, which means you just run the container and then detach from it. Essentially, you run container in the background.\nEdit: So if you run the Docker container with -itd, it runs both the -it options and detaches you from the container. As a result, your container will still be running in the background even without any default app to run.",
    "How do I use Docker environment variable in ENTRYPOINT array?": "You're using the exec form of ENTRYPOINT. Unlike the shell form, the exec form does not invoke a command shell. This means that normal shell processing does not happen. For example, ENTRYPOINT [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: ENTRYPOINT [ \"sh\", \"-c\", \"echo $HOME\" ].\nWhen using the exec form and executing a shell directly, as in the case for the shell form, it is the shell that is doing the environment variable expansion, not docker.(from Dockerfile reference)\nIn your case, I would use shell form\nENTRYPOINT ./greeting --message \"Hello, $ADDRESSEE\\!\"",
    "Conditional COPY/ADD in Dockerfile?": "Here is a simple workaround:\nCOPY foo file-which-may-exist* /target\nMake sure foo exists, since COPY needs at least one valid source.\nIf file-which-may-exist is present, it will also be copied.\nNOTE: You should take care to ensure that your wildcard doesn't pick up other files which you don't intend to copy. To be more careful, you could use file-which-may-exist? instead (? matches just a single character).\nOr even better, use a character class like this to ensure that only one file can be matched:\nCOPY foo file-which-may-exis[t] /target",
    "apt-get install tzdata noninteractive": "This is the script I used\n(Updated Version with input from @elquimista from the comments)\n#!/bin/bash\n\nln -fs /usr/share/zoneinfo/America/New_York /etc/localtime\nDEBIAN_FRONTEND=noninteractive apt-get install -y tzdata\ndpkg-reconfigure --frontend noninteractive tzdata\nSeems to work fine.\nAs one liner:\nDEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends tzdata",
    "Alpine Dockerfile advantages of --no-cache vs. rm /var/cache/apk/*": "The --no-cache option allows to not cache the index locally, which is useful for keeping containers small.\nLiterally it equals apk update in the beginning and rm -rf /var/cache/apk/* in the end.\nSome example where we use --no-cache option:\n$ docker run -ti alpine:3.7\n/ # apk add nginx\nWARNING: Ignoring APKINDEX.70c88391.tar.gz: No such file or directory\nWARNING: Ignoring APKINDEX.5022a8a2.tar.gz: No such file or directory\nERROR: unsatisfiable constraints:\n  nginx (missing):\n    required by: world[nginx]\n/ # \n/ # apk add --no-cache nginx\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz\n(1/2) Installing pcre (8.41-r1)\n(2/2) Installing nginx (1.12.2-r3)\nExecuting nginx-1.12.2-r3.pre-install\nExecuting busybox-1.27.2-r7.trigger\nOK: 6 MiB in 13 packages\n/ # \n/ # ls -la /var/cache/apk/\ntotal 8\ndrwxr-xr-x    2 root     root          4096 Jan  9 19:37 .\ndrwxr-xr-x    5 root     root          4096 Mar  5 20:29 ..\nAnother example where we don't use --no-cache option:\n$ docker run -ti alpine:3.7\n/ # apk add nginx\nWARNING: Ignoring APKINDEX.70c88391.tar.gz: No such file or directory\nWARNING: Ignoring APKINDEX.5022a8a2.tar.gz: No such file or directory\nERROR: unsatisfiable constraints:\n  nginx (missing):\n    required by: world[nginx]\n/ # \n/ # apk update\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz\nv3.7.0-107-g15dd6b8ab3 [http://dl-cdn.alpinelinux.org/alpine/v3.7/main]\nv3.7.0-105-g4b8b158c40 [http://dl-cdn.alpinelinux.org/alpine/v3.7/community]\nOK: 9048 distinct packages available\n/ # \n/ # apk add nginx\n(1/2) Installing pcre (8.41-r1)\n(2/2) Installing nginx (1.12.2-r3)\nExecuting nginx-1.12.2-r3.pre-install\nExecuting busybox-1.27.2-r7.trigger\nOK: 6 MiB in 13 packages\n/ # \n/ # ls -la /var/cache/apk/\ntotal 1204\ndrwxr-xr-x    2 root     root          4096 Mar  5 20:31 .\ndrwxr-xr-x    6 root     root          4096 Mar  5 20:31 ..\n-rw-r--r--    1 root     root        451508 Mar  3 00:30 APKINDEX.5022a8a2.tar.gz\n-rw-r--r--    1 root     root        768680 Mar  5 09:39 APKINDEX.70c88391.tar.gz\n/ # \n/ # rm -vrf /var/cache/apk/*\nremoved '/var/cache/apk/APKINDEX.5022a8a2.tar.gz'\nremoved '/var/cache/apk/APKINDEX.70c88391.tar.gz'\nAs you can see both cases are valid. As for me, using --no-cache option is more elegant.",
    "What is .build-deps for apk add --virtual command?": "If you see the documentation\n-t, --virtual NAME    Instead of adding all the packages to 'world', create a new \n                      virtual package with the listed dependencies and add that \n                      to 'world'; the actions of the command are easily reverted \n                      by deleting the virtual package\nWhat that means is when you install packages, those packages are not added to global packages. And this change can be easily reverted. So if I need gcc to compile a program, but once the program is compiled I no more need gcc.\nI can install gcc, and other required packages in a virtual package and all of its dependencies and everything can be removed this virtual package name. Below is an example usage\nRUN apk add --virtual mypacks gcc vim \\\n && apk del mypacks\nThe next command will delete all 18 packages installed with the first command.\nIn docker these must be executed as a single RUN command (as shown above), otherwise it will not reduce the image size.",
    "ARG or ENV, which one to use in this case?": "From Dockerfile reference:\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.\nThe ENV instruction sets the environment variable <key> to the value <value>.\nThe environment variables set using ENV will persist when a container is run from the resulting image.\nSo if you need build-time customization, ARG is your best choice.\nIf you need run-time customization (to run the same image with different settings), ENV is well-suited.\nIf I want to add let's say 20 (a random number) of extensions or any other feature that can be enable|disable\nGiven the number of combinations involved, using ENV to set those features at runtime is best here.\nBut you can combine both by:\nbuilding an image with a specific ARG\nusing that ARG as an ENV\nThat is, with a Dockerfile including:\nARG var\nENV var=${var}\nYou can then either build an image with a specific var value at build-time (docker build --build-arg var=xxx), or run a container with a specific runtime value (docker run -e var=yyy)",
    "npm ERR! Tracker \"idealTree\" already exists while creating the Docker image for Node project": "This issue is happening due to changes in NodeJS starting with version 15. When no WORKDIR is specified, npm install is executed in the root directory of the container, which is resulting in this error. Executing the npm install in a project directory of the container specified by WORKDIR resolves the issue.\nUse the following Dockerfile:\n# Specify a base image\nFROM node:alpine\n\n#Install some dependencies\n\nWORKDIR /usr/app\nCOPY ./ /usr/app\nRUN npm install\n\n# Set up a default command\nCMD [ \"npm\",\"start\" ]",
    "Docker-compose check if mysql connection is ready": "version: \"2.1\"\nservices:\n    api:\n        build: .\n        container_name: api\n        ports:\n            - \"8080:8080\"\n        depends_on:\n            db:\n                condition: service_healthy\n    db:\n        container_name: db\n        image: mysql\n        ports:\n            - \"3306\"\n        environment:\n            MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n            MYSQL_USER: \"user\"\n            MYSQL_PASSWORD: \"password\"\n            MYSQL_DATABASE: \"database\"\n        healthcheck:\n            test: [\"CMD\", \"mysqladmin\" ,\"ping\", \"-h\", \"localhost\"]\n            timeout: 20s\n            retries: 10\nThe api container will not start until the db container is healthy (basically until mysqladmin is up and accepting connections.)",
    "Dockerfile - set ENV to result of command": "As an addition to DarkSideF answer.\nYou should be aware that each line/command in Dockerfile is ran in another container.\nYou can do something like this:\nRUN export bleah=$(hostname -f);echo $bleah;\nThis is run in a single container.",
    "How to update /etc/hosts file in Docker image during \"docker build\"": "With a more recent version of docker, this could be done with docker-compose and its extra_hosts directive\nAdd hostname mappings.\nUse the same values as the docker run client --add-host parameter (which should already be available for docker 1.8).\nextra_hosts:\n  - \"somehost:162.242.195.82\"\n  - \"otherhost:50.31.209.229\"\nIn short: modify /etc/hosts of your container when running it, instead of when building it.\nWith Docker 17.x+, you have a docker build --add-host mentioned below, but, as commented in issue 34078 and in this answer:\nThe --add-host feature during build is designed to allow overriding a host during build, but not to persist that configuration in the image.\nThose links point to strategies for dealing with the problem at hand:\nRun an internal DNS; you can set the default DNS server to use in the daemon; that way every container started will automatically use the configured DNS by default\nUse docker compose and provide a docker-compose.yml to your developers.\nThe docker compose file allows you to specify all the options that should be used when starting a container, so developers could just docker compose up to start the container with all the options they need to set.\nThese solutions can take advantage of using of the docker-compose method that was suggested earlier in the answer, with its extra_hosts directive.",
    "Docker images - types. Slim vs slim-stretch vs stretch vs alpine": "Per docker library docs (quote and links below), here's a summary:\nopenjdk:<version>\nThe defacto image. Use it if unsure.\nopenjdk:<version>-buster, openjdk:<version>-stretch and openjdk:<version>-jessie\nbuster, jessie or stretch are the suite code names for releases of Debian and indicate which release the image is based on.\nopenjdk:<version>-alpine\nSimilarly, this image is based on the Alpine Linux, thus being a very small base image. It is recommended if you need an image size is as small as possible. The caveat is that it uses some unusual libs, but shouldn't be a problem for most software. In doubt, check the official docs below.\nopenjdk:<version> (from 12 onwards), openjdk:<version>-oracle and openjdk:<version>-oraclelinux7\nStarting with openjdk:12 the default image as well as the -oracle and -oraclelinux7 variants are based on the official Oracle Linux 7 image. The OpenJDK binaries in the default image as well as the -oracle and -oraclelinux7 variants are built by Oracle and are sourced from the OpenJDK community.\nopenjdk:<version>-slim\nThis image only contains the minimal packages needed to run Java (and is missing many of the UI-related Java libraries, for instance). Unless you are working in an environment where only the openjdk image will be deployed and you have space constraints, the default image is recommended over this one.\nopenjdk:<version>-windowsservercore\nThis image is based on Windows Server Core (microsoft/windowsservercore).\n\nFull docs (version shown below here, latest version here):\nImage Variants\nThe openjdk images come in many flavors, each designed for a specific use case.\nopenjdk:<version>\nThis is the defacto image. If you are unsure about what your needs are, you probably want to use this one. It is designed to be used both as a throw away container (mount your source code and start the container to start your app), as well as the base to build other images off of.\nSome of these tags may have names like jessie or stretch in them. These are the suite code names for releases of Debian and indicate which release the image is based on.\nopenjdk:<version>-alpine\nThis image is based on the popular Alpine Linux project, available in the alpine official image. Alpine Linux is much smaller than most distribution base images (~5MB), and thus leads to much slimmer images in general.\nThis variant is highly recommended when final image size being as small as possible is desired. The main caveat to note is that it does use musl libc instead of glibc and friends, so certain software might run into issues depending on the depth of their libc requirements. However, most software doesn't have an issue with this, so this variant is usually a very safe choice. See this Hacker News comment thread for more discussion of the issues that might arise and some pro/con comparisons of using Alpine-based images.\nTo minimize image size, it's uncommon for additional related tools (such as git or bash) to be included in Alpine-based images. Using this image as a base, add the things you need in your own Dockerfile (see the alpine image description for examples of how to install packages if you are unfamiliar).\nopenjdk:<version>-windowsservercore\nThis image is based on Windows Server Core (microsoft/windowsservercore). As such, it only works in places which that image does, such as Windows 10 Professional/Enterprise (Anniversary Edition) or Windows Server 2016.\nFor information about how to get Docker running on Windows, please see the relevant \"Quick Start\" guide provided by Microsoft:\nWindows Server Quick Start\nWindows 10 Quick Start\nopenjdk:<version>-slim\nThis image installs the -headless package of OpenJDK and so is missing many of the UI-related Java libraries and some common packages contained in the default tag. It only contains the minimal packages needed to run Java. Unless you are working in an environment where only the openjdk image will be deployed and you have space constraints, we highly recommend using the default image of this repository.",
    "Deploying a minimal flask app in docker - server connection issues": "The problem is you are only binding to the localhost interface, you should be binding to 0.0.0.0 if you want the container to be accessible from outside. If you change:\nif __name__ == '__main__':\n    app.run()\nto\nif __name__ == '__main__':\n    app.run(host='0.0.0.0')\nIt should work.\nNote that this will bind to all interfaces on the host, which may in some circumstances be a security risk - see https://stackoverflow.com/a/58138250/4332 for more information on binding to a specific interface.",
    "docker-compose, run a script after container has started?": "This is the way I use for calling a script after a container is started without overriding the entrypoint.\nIn my example, I used it for initializing the replicaset of my local MongoDB\nservices:\n  mongo:\n    image: mongo:4.2.8\n    hostname: mongo\n    container_name: mongodb\n    entrypoint: [\"/usr/bin/mongod\",\"--bind_ip_all\",\"--replSet\",\"rs0\"]\n    ports:\n      - 27017:27017\n  mongosetup:\n    image: mongo:4.2.8\n    depends_on:\n      - mongo\n    restart: \"no\"\n    entrypoint: [ \"bash\", \"-c\", \"sleep 10 && mongo --host mongo:27017 --eval 'rs.initiate()'\"]      \nIn the first part, I simply launch my service (mongo)\nThe second service use a \"bash\" entry point AND a restart: no <= important\nI also use a depends_on between service and setup service for manage the launch order.",
    "Run a script in Dockerfile": "RUN and ENTRYPOINT are two different ways to execute a script.\nRUN means it creates an intermediate container, runs the script and freeze the new state of that container in a new intermediate image. The script won't be run after that: your final image is supposed to reflect the result of that script.\nENTRYPOINT means your image (which has not executed the script yet) will create a container, and runs that script.\nIn both cases, the script needs to be added, and a RUN chmod +x /bootstrap.sh is a good idea.\nIt should also start with a shebang (like #!/bin/sh)\nConsidering your script (bootstrap.sh: a couple of git config --global commands), it would be best to RUN that script once in your Dockerfile, but making sure to use the right user (the global git config file is %HOME%/.gitconfig, which by default is the /root one)\nAdd to your Dockerfile:\nRUN /bootstrap.sh\nThen, when running a container, check the content of /root/.gitconfig to confirm the script was run.",
    "Dockerfile build - possible to ignore error?": "Sure. Docker is just responding to the error codes returned by the RUN shell scripts in the Dockerfile. If your Dockerfile has something like:\nRUN make\nYou could replace that with:\nRUN make; exit 0\nThis will always return a 0 (success) exit code. The disadvantage here is that your image will appear to build successfully even if there are actual errors in the build process.",
    "How to pass environment variable to docker-compose up": "You have two options (option 2. overrides 1.):\nCreate the .env file as already suggested in another answer.\nPrepend KEY=VALUE pair(s) to your docker-compose command, e.g:\nKB_DB_TAG_VERSION=kb-1.3.20-v1.0.0 docker-compose up\nExporting it earlier in a script should also work, e.g.:\nexport KB_DB_TAG_VERSION=kb-1.3.20-v1.0.0\ndocker-compose up\nKeep in mind that these options just pass an environment varible to the docker-compose.yml file, not to a container. For an environment variable to be actually passed to a container you always need something like this in your docker-compose.yml:\n  environment:\n    - KB_DB_TAG_VERSION=$KB_DB_TAG_VERSION",
    "Docker follow symlink outside context": "That is not possible and will not be implemented. Please have a look at the discussion on github issue #1676:\nWe do not allow this because it's not repeatable. A symlink on your machine is the not the same as my machine and the same Dockerfile would produce two different results. Also having symlinks to /etc/paasswd would cause issues because it would link the host files and not your local files.",
    "Failed to solve with frontend Dockerfile": "The name of Docker files doesn't have any extension. It's just Dockerfile with capital D and lowercase f.\nYou can also specify the Dockerfile name, such as docker build . -f Dockerfile.txt if you'd like to name it something else.",
    "What is the purpose of VOLUME in Dockerfile?": "A volume is a persistent data stored in /var/lib/docker/volumes/...\nYou can either declare it in a Dockerfile, which means each time a container is started from the image, the volume is created (empty), even if you don't have any -v option.\nYou can declare it on runtime docker run -v [host-dir:]container-dir.\nCombining the two (VOLUME + docker run -v) means that you can mount the content of a host folder into your volume persisted by the container in /var/lib/docker/volumes/...\ndocker volume create creates a volume without having to define a Dockerfile and build an image and run a container. It is used to quickly allow other containers to mount said volume.\nIf you had persisted some content in a volume, but since then, deleted the container (which by default does not delete its associated volume, unless you are using docker rm -v), you can re-attach said volume to a new container (declaring the same volume).\nSee \"Docker - How to access a volume not attached to a container?\".\nWith docker volume create, this is easy to reattach a named volume to a container.\ndocker volume create --name aname\ndocker run -v aname:/apath --name acontainer\n...\n# modify data in /apath\n...\ndocker rm acontainer\n\n# let's mount aname volume again\ndocker run -v aname:/apath --name acontainer\nls /apath\n# you find your data back!\nWhy volumes were introduced in the first place?\nDocker volumes were introduced primarily to solve the challenge of data persistence and data sharing in containerized environments.\nIn the world of Docker, containers are ephemeral and lightweight, meaning they can be created, started, stopped, and destroyed with ease, and they are designed to be stateless.\nHowever, applications often need to store data permanently, access configuration files, or share data between different containers or between containers and the host system. That is where Docker volumes come into play.\nVolumes provide a mechanism to persist data generated by and used by Docker containers.\nUnlike the container's writable layer, which is tightly coupled to the container's lifecycle and gets destroyed when the container is removed, volumes are managed by Docker and are designed to exist independently of containers.\nThat means data in volumes survives container restarts and can be securely shared among multiple containers. And volumes are platform-independent, which simplifies data migration and backup processes.\nSee \"Docker Engine / Storage / Manage data in Docker\"\nAdditionally, volumes address performance and security concerns. Since they are stored outside the container's filesystem, they offer improved I/O performance, especially important for database storage or heavy read/write operations. They also provide a safer way to handle sensitive data, as volumes can be more securely isolated from the core container filesystem.",
    "Share variable in multi-stage Dockerfile: ARG before FROM not substituted": "ARGs only last for the build phase of a single image. For the multistage, renew the ARG by simply stating:\nARG DARSHAN_VER\nafter your FROM instructions.\ncf. https://docs.docker.com/engine/reference/builder/#arg\nARG DARSHAN_VER=3.1.6\n\nFROM fedora:29 as build\nARG DARSHAN_VER\nRUN dnf install -y \\\n        gcc \\\n        make \\\n        bzip2 bzip2-devel zlib zlib-devel\nRUN curl -O \"ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz\" \\\n    && tar ...\n\n\nFROM fedora:29\nARG DARSHAN_VER\nCOPY --from=build \"/usr/local/darshan-${DARSHAN_VER}\" \"/usr/local/darshan-${DARSHAN_VER}\"\n...\nYou will notice how I declared the initial value at the top of the script, and pull it in on each image.",
    "How to write commands with multiple lines in Dockerfile while preserving the new lines?": "You can use what is called \"ANSI-C quoting\" with $'...'. It was originally a ksh93 feature but it is now available in bash, zsh, mksh, FreeBSD sh and in busybox's ash (but only when it is compiled with ENABLE_ASH_BASH_COMPAT).\nAs RUN uses /bin/sh as shell by default you are required to switch to something like bash first by using the SHELL instruction.\nStart your command with $', end it with ' and use \\n\\ for newlines, like this:\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN echo $'[repo] \\n\\\nname            = YUM Repository \\n\\\nbaseurl         = https://example.com/packages/ \\n\\\nenabled         = 1 \\n\\\ngpgcheck        = 0' > /etc/yum.repos.d/Repo.repoxyz",
    "Docker how to run pip requirements.txt only if there was a change?": "I'm assuming that at some point in your build process, you're copying your entire application into the Docker image with COPY or ADD:\nCOPY . /opt/app\nWORKDIR /opt/app\nRUN pip install -r requirements.txt\nThe problem is that you're invalidating the Docker build cache every time you're copying the entire application into the image. This will also invalidate the cache for all subsequent build steps.\nTo prevent this, I'd suggest copying only the requirements.txt file in a separate build step before adding the entire application into the image:\nCOPY requirements.txt /opt/app/requirements.txt\nWORKDIR /opt/app\nRUN pip install -r requirements.txt\nCOPY . /opt/app\n# continue as before...\nAs the requirements file itself probably changes only rarely, you'll be able to use the cached layers up until the point that you add your application code into the image.",
    "How to copy folders to docker image from Dockerfile?": "Use ADD (docs)\nThe ADD command can accept as a <src> parameter:\nA folder within the build folder (the same folder as your Dockerfile). You would then add a line in your Dockerfile like this:\nADD folder /path/inside/your/container\nor\nA single-file archive anywhere in your host filesystem. To create an archive use the command:\ntar -cvzf newArchive.tar.gz /path/to/your/folder\nYou would then add a line to your Dockerfile like this:\nADD /path/to/archive/newArchive.tar.gz  /path/inside/your/container\nNotes:\nADD will automatically extract your archive.\npresence/absence of trailing slashes is important, see the linked docs",
    "Is there a way to combine Docker images into 1 container?": "You can, with the multi-stage builds feature introduced in Docker Engine 17.05\nTake a look at this:\nFROM golang:1.7.3\nWORKDIR /go/src/github.com/alexellis/href-counter/\nRUN go get -d -v golang.org/x/net/html  \nCOPY app.go .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\nFROM alpine:latest  \nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\nCOPY --from=0 /go/src/github.com/alexellis/href-counter/app .\nCMD [\"./app\"]  \nThen build the image normally:\ndocker build -t alexellis2/href-counter:latest\nFrom : https://docs.docker.com/develop/develop-images/multistage-build/\nThe end result is the same tiny production image as before, with a significant reduction in complexity. You don\u2019t need to create any intermediate images and you don\u2019t need to extract any artifacts to your local system at all.\nHow does it work? The second FROM instruction starts a new build stage with the alpine:latest image as its base. The COPY --from=0 line copies just the built artifact from the previous stage into this new stage. The Go SDK and any intermediate artifacts are left behind, and not saved in the final image.",
    "How to cache the RUN npm install instruction when docker build a Dockerfile": "Ok so I found this great article about efficiency when writing a docker file.\nThis is an example of a bad docker file adding the application code before running the RUN npm install instruction:\nFROM ubuntu\n\nRUN echo \"deb http://archive.ubuntu.com/ubuntu precise main universe\" > /etc/apt/sources.list\nRUN apt-get update\nRUN apt-get -y install python-software-properties git build-essential\nRUN add-apt-repository -y ppa:chris-lea/node.js\nRUN apt-get update\nRUN apt-get -y install nodejs\n\nWORKDIR /opt/app\n\nCOPY . /opt/app\nRUN npm install\nEXPOSE 3001\n\nCMD [\"node\", \"server.js\"]\nBy dividing the copy of the application into 2 COPY instructions (one for the package.json file and the other for the rest of the files) and running the npm install instruction before adding the actual code, any code change wont trigger the RUN npm install instruction, only changes of the package.json will trigger it. Better practice docker file:\nFROM ubuntu\nMAINTAINER David Weinstein <david@bitjudo.com>\n\n# install our dependencies and nodejs\nRUN echo \"deb http://archive.ubuntu.com/ubuntu precise main universe\" > /etc/apt/sources.list\nRUN apt-get update\nRUN apt-get -y install python-software-properties git build-essential\nRUN add-apt-repository -y ppa:chris-lea/node.js\nRUN apt-get update\nRUN apt-get -y install nodejs\n\n# use changes to package.json to force Docker not to use the cache\n# when we change our application's nodejs dependencies:\nCOPY package.json /tmp/package.json\nRUN cd /tmp && npm install\nRUN mkdir -p /opt/app && cp -a /tmp/node_modules /opt/app/\n\n# From here we load our application's code in, therefore the previous docker\n# \"layer\" thats been cached will be used if possible\nWORKDIR /opt/app\nCOPY . /opt/app\n\nEXPOSE 3000\n\nCMD [\"node\", \"server.js\"]\nThis is where the package.json file added, install its dependencies and copy them into the container WORKDIR, where the app lives:\nADD package.json /tmp/package.json\nRUN cd /tmp && npm install\nRUN mkdir -p /opt/app && cp -a /tmp/node_modules /opt/app/\nTo avoid the npm install phase on every docker build just copy those lines and change the ^/opt/app^ to the location your app lives inside the container.",
    "Docker build gives \"unable to prepare context: context must be a directory: /Users/tempUser/git/docker/Dockerfile\"": "You need to point to the directory instead. You must not specify the dockerfile.\ndocker build -t ubuntu-test:latest . does work.\ndocker build -t ubuntu-test:latest ./Dockerfile does not work.",
    "What is the difference between `docker-compose build` and `docker build`?": "docker-compose can be considered a wrapper around the docker CLI (in fact it is another implementation in python as said in the comments) in order to gain time and avoid 500 characters-long lines (and also start multiple containers at the same time). It uses a file called docker-compose.yml in order to retrieve parameters.\nYou can find the reference for the docker-compose file format here.\nSo basically docker-compose build will read your docker-compose.yml, look for all services containing the build: statement and run a docker build for each one.\nEach build can specify a Dockerfile, a context and args to pass to docker.\nTo conclude with an example docker-compose.yml file:\nversion: '3.2'\n\nservices:\n  database:\n    image: mariadb\n    restart: always\n    volumes:\n      - ./.data/sql:/var/lib/mysql\n\n  web:\n    build:\n      dockerfile: Dockerfile-alpine\n      context: ./web\n    ports:\n      - 8099:80\n    depends_on:\n      - database \nWhen calling docker-compose build, only the web target will need an image to be built. The docker build command would look like:\ndocker build -t web_myproject -f Dockerfile-alpine ./web",
    "npm WARN old lockfile The package-lock.json file was created with an old version of npm": "There are several ways to deal with this. (People really seem to like #4.)\nIgnore it. It's just a warning and does not affect the installation of modules.\nRun npm install --package-lock-only (with the newer version of npm) to regenerate a package-lock.json. Commit the updated version of package-lock.json to the repo/Docker image or whatever.\nDowngrade npm to an older version in production. Consider running npm version 6 as that is what ships with the current (as of this writing) Long Term Support (LTS) version of Node.js. In the case being asked about in this question, I imagine you can just leave out the RUN npm -g install npm@7.19.1 from the Dockerfile and instead use the version of npm that is installed with the Docker image (which in this case will almost certainly be npm@6 since that is what ships with Node.js 14.x).\nIf you already have a version of npm installed but want to run one command with an older version of npm but otherwise keep the newer version, you can use npx (which ships with npm) to do that. For example, npx npm@6 ci would run npm ci with npm version 6 even if you have version 7 installed.",
    "Difference between Docker ENTRYPOINT and Kubernetes container spec COMMAND?": "Kubernetes provides us with multiple options on how to use these commands:\nWhen you override the default Entrypoint and Cmd in Kubernetes .yaml file, these rules apply:\nIf you do not supply command or args for a Container, the defaults defined in the Docker image are used.\nIf you supply only args for a Container, the default Entrypoint defined in the Docker image is run with the args that you supplied.\nIf you supply a command for a Container, only the supplied command is used. The default EntryPoint and the default Cmd defined in the Docker image are ignored. Your command is run with the args supplied (or no args if none supplied).\nHere is an example:\nDockerfile:\nFROM alpine:latest\nCOPY \"executable_file\" /\nENTRYPOINT [ \"./executable_file\" ]\nKubernetes yaml file:\n spec:\n    containers:\n      - name: container_name\n        image: image_name\n        args: [\"arg1\", \"arg2\", \"arg3\"]\nhttps://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/",
    "How to view logs for a docker image?": "Had the same problem, I solved it using\ndocker build --no-cache --progress=plain -t my-image .",
    "Docker expose all ports or range of ports from 7000 to 8000": "Since Docker 1.5 you can now expose a range of ports to other linked containers using:\nThe Dockerfile EXPOSE command:\nEXPOSE 7000-8000\nor The Docker run command:\ndocker run --expose=7000-8000\nOr instead you can publish a range of ports to the host machine via Docker run command:\ndocker run -p 7000-8000:7000-8000",
    "E: Package 'mysql-client' has no installation candidate in php-fpm image build using docker compose": "If you still want to use the mysql client, it's called default-mysql-client now.",
    "Can we pass ENV variables through cmd line while building a docker image through dockerfile?": "Containers can be built using build arguments (in Docker 1.9+) which work like environment variables.\nHere is the method:\nFROM php:7.0-fpm\nARG APP_ENV=local\nENV APP_ENV=${APP_ENV}\nRUN cd /usr/local/etc/php && ln -sf php.ini-${APP_ENV} php.ini\nand then build a production container:\ndocker build --build-arg APP_ENV=prod .\nFor your particular problem:\nFROM debian\nENV http_proxy=${http_proxy}\nand then run:\ndocker build --build-arg http_proxy=10.11.24.31 .\nNote that if you build your containers with docker-compose, you can specify these build-args in the docker-compose.yml file, but not on the command-line. However, you can use variable substitution in the docker-compose.yml file, which uses environment variables.",
    "How can I set Bash aliases for docker containers in Dockerfile?": "Basically like you always do, by adding it to the user's .bashrc file:\nFROM foo\nRUN echo 'alias hi=\"echo hello\"' >> ~/.bashrc\nAs usual this will only work for interactive shells:\ndocker build -t test .\ndocker run -it --rm --entrypoint /bin/bash test hi\n/bin/bash: hi: No such file or directory\ndocker run -it --rm test bash\n$ hi\nhello\nFor non-interactive shells you should create a small script and put it in your path, i.e.:\nRUN echo -e '#!/bin/bash\\necho hello' > /usr/bin/hi && \\\n    chmod +x /usr/bin/hi\nIf your alias uses parameters (ie. hi Jim -> hello Jim), just add \"$@\":\nRUN echo -e '#!/bin/bash\\necho hello \"$@\"' > /usr/bin/hi && \\\n    chmod +x /usr/bin/hi",
    "ARG substitution in RUN command not working for Dockerfile": "Another thing to be careful about is that after every FROM statement, all the ARGs get collected and are no longer available. Be careful with multi-stage builds.\nYou can reuse ARG with omitted default value inside FROM to get through this problem:\nARG VERSION=latest\nFROM busybox:$VERSION\nARG VERSION\nRUN echo $VERSION > image_version\nExample taken from docs: https://docs.docker.com/engine/reference/builder/#understand-how-arg-and-from-interact",
    "Docker Copy and change owner": "A --chown flag has finally been added to COPY:\nCOPY --chown=patrick hostPath containerPath\nThis new syntax seems to work on Docker 17.09.\nSee the PR for more information.",
    "Docker - image operating system \"windows\" cannot be used on this platform": "Your Docker host is configured to run Linux containers inside of a VM. To run Windows containers, you need to right click on the Docker icon in the system tray, and select \"Switch to Windows containers\u2026\" in the Docker menu. This option is not available in \"Home\" versions of Windows. Documentation is available here.",
    "How can I use a local file on container?": "Yes, you can do this. What you are describing is a bind mount. See https://docs.docker.com/storage/bind-mounts/ for documentation on the subject.\nFor example, if I want to mount a folder from my home directory into /mnt/mydata in a container, I can do:\ndocker run -v /Users/andy/mydata:/mnt/mydata myimage\nNow, /mnt/mydata inside the container will have access to /Users/andy/mydata on my host.\nKeep in mind, if you are using Docker for Mac or Docker for Windows there are specific directories on the host that are allowed by default:\nIf you are using Docker Machine on Mac or Windows, your Docker Engine daemon has only limited access to your macOS or Windows filesystem. Docker Machine tries to auto-share your /Users (macOS) or C:\\Users (Windows) directory. So, you can mount files or directories on macOS using.\nUpdate July 2019:\nI've updated the documentation link and naming to be correct. These type of mounts are called \"bind mounts\". The snippet about Docker for Mac or Windows no longer appears in the documentation but it should still apply. I'm not sure why they removed it (my Docker for Mac still has an explicit list of allowed mounting paths on the host).",
    "How to remove entrypoint from parent Image in Dockerfile": "Per the discussion here, you should be able to reset the entrypoint with\nENTRYPOINT []",
    "Docker: What is the default WORKDIR in a Dockerfile?": "docker workdir\nsays it is /, so the root directory",
    "Install packages in Alpine docker": "The equivalent of apt or apt-get in Alpine is apk\nA typical Dockerfile will contain, for example:\nRUN apk add --no-cache wget\n--no-cache is the equivalent to: apk add wget && rm -rf /var/cache/apk/*\nor, before the --no-cache option was available:\nRUN apk update && apk add wget\nAlpine rm -rf /var/cache/apk/* has the Debian equivalent rm -rf /var/lib/apt/lists/*.\nSee the Alpine comparison with other distros for more details.",
    "Can a Dockerfile extend another one?": "Using multi-stage build is definitely one part of the answer here.\ndocker-compose v3.4 target being the second and last.\nHere is a example to have 2 containers (1 normal & 1 w/ xdebug installed) living together :\nDockerfile\nFROM php:7-fpm AS php_base \nENV DEBIAN_FRONTEND noninteractive\n\nRUN apt-get update && \\\n    apt-get install -y git libicu-dev libmagickwand-dev libmcrypt-dev libcurl3-dev jpegoptim\nRUN pecl install imagick && \\\n    docker-php-ext-enable imagick\n\nRUN docker-php-ext-install intl\nRUN docker-php-ext-install pdo_mysql\nRUN docker-php-ext-install opcache\nRUN docker-php-ext-install mcrypt\nRUN docker-php-ext-install curl\nRUN docker-php-ext-install zip\n\nFROM php_base AS php_test\n\nRUN pecl install xdebug \nRUN docker-php-ext-enable xdebug\ndocker-compose.yml\nversion: '3.4'\n\nservices:\n  php:\n    build:\n      context: ./\n      target: php_base\n\n  php_test:\n    build:\n      context: ./\n      target: php_test\n  \n# ...",
    "Single file volume mounted as directory in Docker": "Maybe that's clear in the answers above... but it took me some time to figure it out in my case.\nThe underlying reason causing the file being shared with -v to appear as a directory instead of a file is that Docker could not find the file on the host. So Docker creates a new directory in the container with the name being the name of the non existing file on the host as docker thinks that the user just want to share a volume/directory that will be created in the future.\nSo in the problem reported above, if you used a relative directory in the -v command and docker does not understand relative directories, that means that the file was not found on the host and so docker created a directory. And the answer above which suggests to use $(pwd) will be the correct solution when the problem is due to a relative directory.\nBut for those reading this page who are not using a relative directory and are having the same problem... then try to understand why the file is missing on the host.\nIt could just be a stupid typo...\nIt could be that you're running the \"docker run\" command from a client which spawns the docker container on a different host and the file being shared does not exist on that different host. The file being shared with -v must exist on the host where the docker agent will spawn the container... not necessarily on the client where the \"docker run -v ...\" command is executed (although they will be the same in many cases).\nThere are other possible explanations above for Mac and Windows... that could be it too.\nSo the file missing from the host is the problem... troubleshoot the problem in your setup... using $(pwd) could be the solution but not always.",
    "Conditional ENV in Dockerfile": "Yes, it is possible, but you need to use your build argument as flag. You can use parameter expansion feature of shell to check condition. Here is a proof-of-concept Docker file:\nFROM debian:stable\nARG BUILD_DEVELOPMENT\n# if --build-arg BUILD_DEVELOPMENT=1, set NODE_ENV to 'development' or set to null otherwise.\nENV NODE_ENV=${BUILD_DEVELOPMENT:+development}\n# if NODE_ENV is null, set it to 'production' (or leave as is otherwise).\nENV NODE_ENV=${NODE_ENV:-production}\nTesting build:\ndocker build --rm -t env_prod ./\n...\ndocker run -it env_prod bash\nroot@2a2c93f80ad3:/# echo $NODE_ENV \nproduction\nroot@2a2c93f80ad3:/# exit\ndocker build --rm -t env_dev --build-arg BUILD_DEVELOPMENT=1 ./\n...\ndocker run -it env_dev bash\nroot@2db6d7931f34:/# echo $NODE_ENV\ndevelopment",
    "Dockerfile: how to redirect the output of a RUN command to a variable?": "You cannot save a variable for later use in other Dockerfile commands (if that is your intention). This is because each RUN happens in a new shell.\nHowever, if you just want to capture the output of ls you should be able to do it in one RUN compound command. For example:\nRUN file=\"$(ls -1 /tmp/dir)\" && echo $file\nOr just using the subshell inline:\nRUN echo $(ls -1 /tmp/dir)\nIf you have an actual error or problem to solve I could expand on this instead of a hypothetical answer.\nA full example Dockerfile demonstrating this would be:\nFROM alpine:3.7\nRUN mkdir -p /tmp/dir && touch /tmp/dir/file1 /tmp//dir/file2\nRUN file=\"$(ls -1 /tmp/dir)\" && echo $file\nRUN echo $(ls -1 /tmp/dir)\nWhen building you should see steps 3 and 4 output the variable (which contains the list of file1 and file2 creating in step 2). The option for --progress plain forces the output to show the steps in later version of Docker:\n$ docker build --no-cache --progress plain -t test .\nSending build context to Docker daemon  2.048kB\nStep 1/4 : FROM alpine:3.7\n ---> 3fd9065eaf02\nStep 2/4 : RUN mkdir -p /tmp/dir && touch /tmp/dir/file1 /tmp//dir/file2\n ---> Running in abb2fe683e82\nRemoving intermediate container abb2fe683e82\n ---> 2f6dfca9385c\nStep 3/4 : RUN file=\"$(ls -1 /tmp/dir)\" && echo $file\n ---> Running in 060a285e3d8a\nfile1 file2\nRemoving intermediate container 060a285e3d8a\n ---> 2e4cc2873b8c\nStep 4/4 : RUN echo $(ls -1 /tmp/dir)\n ---> Running in 528fc5d6c721\nfile1 file2\nRemoving intermediate container 528fc5d6c721\n ---> 1be7c54e1f29\nSuccessfully built 1be7c54e1f29\nSuccessfully tagged test:latest",
    "Dockerfile: Setting multiple environment variables in single line": "There are two formats for specifying environments. If you need single variable then you below format\nENV X Y\nThis will assign X as Y\nENV X Y Z\nThis will assign X as Y Z\nIf you need to assign multiple environment variables then you use the other format\nENV X=Y Z=A\nThis will assign X as Y and Z as A. So your Dockerfile should be\nFROM alpine:3.6\nENV RUBY_MAJOR=2.4 \\\n    RUBY_VERSION=2.4.1 \\\n    RUBY_DOWNLOAD_SHA256=4fc8a9992de3e90191de369270ea4b6c1b171b7941743614cc50822ddc1fe654 \\\n    RUBYGEMS_VERSION=2.6.12 \\\n    BUNDLER_VERSION=1.15.3\n\nRUN env",
    "How to copy file from host to container using Dockerfile": "Use COPY command like this:\nCOPY foo.txt /data/foo.txt\n# where foo.txt is the relative path on host\n# and /data/foo.txt is the absolute path in the image\nread more details for COPY in the official documentation\nAn alternative would be to use ADD but this is not the best practise if you dont want to use some advanced features of ADD like decompression of tar.gz files.If you still want to use ADD command, do it like this:\nADD abc.txt /data/abc.txt\n# where abc.txt is the relative path on host\n# and /data/abc.txt is the absolute path in the image\nread more details for ADD in the official documentation",
    "Copy multiple directories with one command": "That's the documented behavior of the copy command:\nIf <src> is a directory, the entire contents of the directory are copied, including filesystem metadata.\nNote: The directory itself is not copied, just its contents.\nBest workaround I can suggest is to change your directory layout in your build folder, move the three folders under one parent folder and add the parent.",
    "\"The headers or library files could not be found for jpeg\" installing Pillow on Alpine Linux": "For debian\nsudo apt install libjpeg-dev zlib1g-dev\npip install Pillow",
    "Can't create a docker image for COPY failed: stat /var/lib/docker/tmp/docker-builder error": "You should put those files into the same directory with Dockerfile.",
    "Docker filling up storage on macOS": "WARNING:\nBy default, volumes are not removed to prevent important data from being deleted if there is currently no container using the volume. Use the --volumes flag when running the command to prune volumes as well:\nDocker now has a single command to do that:\ndocker system prune -a --volumes\nSee the Docker system prune docs",
    "How to pass arguments within docker-compose?": "This can now be done as of docker-compose v2+ as part of the build object;\ndocker-compose.yml\nversion: '2'\nservices:\n    my_image_name:\n        build:\n            context: . #current dir as build context\n            args:\n                var1: 1\n                var2: c\nSee the docker compose docs.\nIn the above example \"var1\" and \"var2\" will be sent to the build environment.\nNote: any env variables (specified by using the environment block) which have the same name as args variable(s) will override that variable.",
    "Activate python virtualenv in Dockerfile": "You don't need to use virtualenv inside a Docker Container.\nvirtualenv is used for dependency isolation. You want to prevent any dependencies or packages installed from leaking between applications. Docker achieves the same thing, it isolates your dependencies within your container and prevent leaks between containers and between applications.\nTherefore, there is no point in using virtualenv inside a Docker Container unless you are running multiple apps in the same container, if that's the case I'd say that you're doing something wrong and the solution would be to architect your app in a better way and split them up in multiple containers.\nEDIT 2022: Given this answer get a lot of views, I thought it might make sense to add that now 4 years later, I realized that there actually is valid usages of virtual environments in Docker images, especially when doing multi staged builds:\nFROM python:3.9-slim as compiler\nENV PYTHONUNBUFFERED 1\n\nWORKDIR /app/\n\nRUN python -m venv /opt/venv\n# Enable venv\nENV PATH=\"/opt/venv/bin:$PATH\"\n\nCOPY ./requirements.txt /app/requirements.txt\nRUN pip install -Ur requirements.txt\n\nFROM python:3.9-slim as runner\nWORKDIR /app/\nCOPY --from=compiler /opt/venv /opt/venv\n\n# Enable venv\nENV PATH=\"/opt/venv/bin:$PATH\"\nCOPY . /app/\nCMD [\"python\", \"app.py\", ]\nIn the Dockerfile example above, we are creating a virtualenv at /opt/venv and activating it using an ENV statement, we then install all dependencies into this /opt/venv and can simply copy this folder into our runner stage of our build. This can help with minimizing docker image size.",
    "Editing Files from dockerfile": "I would use the following approach in the Dockerfile\nRUN   echo \"Some line to add to a file\" >> /etc/sysctl.conf\nThat should do the trick. If you wish to replace some characters or similar you can work this out with sed by using e.g. the following:\nRUN   sed -i \"s|some-original-string|the-new-string |g\" /etc/sysctl.conf\nHowever, if your problem lies in simply getting the settings to \"bite\" this question might be of help.",
    "apt-get update' returned a non-zero code: 100": "Because you have an https sources. Install apt-transport-https before executing update.\nFROM ubuntu:14.04.4\nRUN apt-get update && apt-get install -y apt-transport-https\nRUN echo 'deb http://private-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.4.2.0 HDP main' >> /etc/apt/sources.list.d/HDP.list\nRUN echo 'deb http://private-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/ubuntu14 HDP-UTILS main'  >> /etc/apt/sources.list.d/HDP.list\nRUN echo 'deb [arch=amd64] https://apt-mo.trafficmanager.net/repos/azurecore/ trusty main' >> /etc/apt/sources.list.d/azure-public-trusty.list\n\n....\nRest of your Dockerfile.",
    "MYSQL_ROOT_PASSWORD is set but getting \"Access denied for user 'root'@'localhost' (using password: YES)\" in docker container": "The below description is specifically for MySQL but many other official db docker images (postgres, mongodb....) work a similar way. Hence the symptom (i.e. access denied with configured credentials) and workaround (i.e. delete the data volume to start initialization from scratch) are the same.\nTaking for granted you have shown your entire start log, it appears you started your mysql container against a pre-existing db_data volume already containing a mysql database filesystem.\nIn this case, absolutely nothing will be initialized on container start and environment variables are useless. Quoting the official image documentation in the \"Environment Variables\" section:\nDo note that none of the variables below will have any effect if you start the container with a data directory that already contains a database: any pre-existing database will always be left untouched on container startup.\nIf you want your instance to be initialized, you have to start from scratch. It is quite easy to do with docker compose when using a named volume like in your case. Warning: this will permanently delete the contents in your db_data volume, wiping out any previous database you had there. Create a backup first if you need to keep the contents.\ndocker-compose down -v\ndocker-compose up -d\nIf you ever convert to a bind mount, you will have to delete all it's content yourself (i.e. rm -rf /path/to/bind/mount/*)",
    "Why the \"none\" image appears in Docker and how can we avoid it": "Below are some parts from What are Docker <none>:<none> images?\nThe Good <none>:<none>\nThese are intermediate images and can be seen using docker images -a. They don't result into a disk space problem but it is definitely a screen \"real estate\" problem. Since all these <none>:<none> images can be quite confusing as what they signify.\nThe Bad <none>:<none>\nThese images are the dangling ones, which can cause disk space problems. These <none>:<none> images are being listed as part of docker images and need to be pruned.\n(a dangling file system layer in Docker is something that is unused and is not being referenced by any images. Hence we need a mechanism for Docker to clear these dangling images)\nSo,\nif your case has to do with dangling images, it's ok to remove them with:\n docker rmi $(docker images -f \"dangling=true\" -q)\nThere is also the option of docker image prune but the client and daemon API must both be at least v1.25 to use this command.\nif your case has to do with intermediate images, it's ok to keep them, other images are pointing references to them.\nRelated documentation:\ndocker rmi\ndocker image rm\ndocker image prune",
    "Docker COPY from ubuntu absolute path": "The absolute path of your resources refers to an absolute path within the build context, not an absolute path on the host. So all the resources must be copied into the directory where you run the docker build and then provide the path of those resources within your Dockerfiles before building the image. (This refers to the location where you run your Dockerfile)\nThere is a closed issue for this as well.",
    "Why won't my docker-entrypoint.sh execute?": "I was tearing my hair out with an issue very similar to this. In my case /bin/bash DID exist. But actually the problem was Windows line endings.\nIn my case the git repository had an entry point script with Unix line endings (\\n). But when the repository was checked out on a windows machine, git decided to try and be clever and replace the line endings in the files with windows line endings (\\r\\n).\nThis meant that the shebang didn't work because instead of looking for /bin/bash, it was looking for /bin/bash\\r.\nThe solution for me was to disable git's automatic conversion:\ngit config --global core.autocrlf input\nReset the repo using this (don't forget to save your changes):\ngit rm --cached -r .\ngit reset --hard\nAnd then rebuild.\nSome more helpful info here: How to change line-ending settings and here http://willi.am/blog/2016/08/11/docker-for-windows-dealing-with-windows-line-endings/\nFor repo owners and contributors\nIf you own a repo or contribute to it, set mandatory LF line endings for .sh files right in the repo by adding the .gitattributes file with the following line:\n*.sh text eol=lf",
    "How can I see Dockerfile for each docker image?": "As far as I know, no, you can't. Because a Dockerfile is used for building the image, it is not packed with the image itself. That means you should reverse engineer it. You can use docker inspect on an image or container, thus getting some insight and a feel of how it is configured. The layers an image are also visible, since you pull them when you pull a specific image, so that is also no secret.\nHowever, you can usually see the Dockerfile in the repository of the image itself on Dockerhub. I can't say most repositories have Dockerfiles attached, but the most of the repositories I seen do have it.\nDifferent repository maintainers may opt for different ways to document the Dockerfiles. You can see the Dockerfile tab on the repository page if automatic builds are set up. But when multiple parallel versions are available (like for Ubuntu), maintainers usually opt to put links the Dockerfiles for different versions in the description. If you take a look here: https://hub.docker.com/_/ubuntu/, under the \"Supported tags\" (again, for Ubuntu), you can see there are links to multiple Dockerfiles, for each respective Ubuntu version.",
    "Docker multiline CMD or ENTRYPOINT": "It was a typo in the dockerfile. I missed a space between ENTRYPOINT and [. Dockerfile supports multiline ENTRYPOINT and CMD by terminating the line with \\, same as RUN. So, in my case it can be\nENTRYPOINT [ \"/path/myprocess\", \\\n             \"arg1\",            \\\n             \"arg2\"             \\\n]",
    "How to configure different dockerfile for development and production": "As a best practice you should try to aim to use one Dockerfile to avoid unexpected errors between different environments. However, you may have a use case where you cannot do that.\nThe Dockerfile syntax is not rich enough to support such a scenario, however you can use shell scripts to achieve that.\nCreate a shell script, called install.sh that does something like:\nif [ ${ENV} = \"DEV\" ]; then \n    composer install\nelse\n    npm install\nfi\nIn your Dockerfile add this script and then execute it when building\n...\nCOPY install.sh install.sh\nRUN chmod u+x install.sh && ./install.sh\n...\nWhen building pass a build arg to specify the environment, example:\ndocker build --build-arg \"ENV=PROD\" ...",
    "docker run pass arguments to entrypoint": "Use ENTRYPOINT in its exec form\nENTRYPOINT [\"java\", \"-jar\", \"/dir/test-1.0.1.jar\"]\nthen when you run docker run -it testjava $value, $value will be \"appended\" after your entrypoint, just like java -jar /dir/test-1.0.1.jar $value",
    "Docker COPY files using glob pattern?": "There is a solution based on multistage-build feature:\nFROM node:12.18.2-alpine3.11\n\nWORKDIR /app\nCOPY [\"package.json\", \"yarn.lock\", \"./\"]\n# Step 2: Copy whole app\nCOPY packages packages\n\n# Step 3: Find and remove non-package.json files\nRUN find packages \\! -name \"package.json\" -mindepth 2 -maxdepth 2 -print | xargs rm -rf\n\n# Step 4: Define second build stage\nFROM node:12.18.2-alpine3.11\n\nWORKDIR /app\n# Step 5: Copy files from the first build stage.\nCOPY --from=0 /app .\n\nRUN yarn install --frozen-lockfile\n\nCOPY . .\n\n# To restore workspaces symlinks\nRUN yarn install --frozen-lockfile\n\nCMD yarn start\nOn Step 5 the layer cache will be reused even if any file in packages directory has changed.",
    "standard_init_linux.go:211: exec user process caused \"exec format error\"": "This can also happen when your host machine has a different architecture from your guest container image.\nE.g. running an arm container on a host with x86-64 architecture",
    "Docker command/option to display or list the build context": "Answers above are great, but there is a low-tech solution for most cases - ncdu. This utility will show pretty and interactive tree structure with sizes. It has an option that will take patterns from a file and exclude them from scan. So you can just do ncdu -X .dockerignore. You will get something like this:\nThis is pretty close to what you will get in your docker image. One caveat is thou if you add a dot directory (like .yarn) into an image, it will not show in ncdu output.",
    "How to pass ARG value to ENTRYPOINT?": "Like Blake Mitchell sais, you cannot use ARG in ENTRYPOINT. However you can use your ARG as a value for ENV, that way you can use it with ENTRYPOINT:\nDockerfile\nARG my_arg\nENV my_env_var=$my_arg\n\nENTRYPOINT echo $my_env_var\nand run:\ndocker build --build-arg \"my_arg=foo\" ...",
    "What is the difference between Docker Service and Docker Container?": "In short: Docker service is used mostly when you configured the master node with Docker swarm so that docker containers will run in a distributed environment and it can be easily managed.\nDocker run: The docker run command first creates a writeable container layer over the specified image, and then starts it using the specified command.\nThat is, docker run is equivalent to the API /containers/create then /containers/(id)/start\nsource: https://docs.docker.com/engine/reference/commandline/run/#parent-command\nDocker service: Docker service will be the image for a microservice within the context of some larger application. Examples of services might include an HTTP server, a database, or any other type of executable program that you wish to run in a distributed environment.\nWhen you create a service, you specify which container image to use and which commands to execute inside running containers. You also define options for the service including:\nthe port where the swarm will make the service available outside the swarm\nan overlay network for the service to connect to other services in the swarm\nCPU and memory limits and reservations\na rolling update policy\nthe number of replicas of the image to run in the swarm\nsource: https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#services-tasks-and-containers",
    "Docker using gosu vs USER": "Dockerfiles are for creating images. I see gosu as more useful as part of a container initialization when you can no longer change users between run commands in your Dockerfile.\nAfter the image is created, something like gosu allows you to drop root permissions at the end of your entrypoint inside of a container. You may initially need root access to do some initialization steps (fixing uid's, host mounted volume permissions, etc). Then once initialized, you run the final service without root privileges and as pid 1 to handle signals cleanly.\nEdit: Here's a simple example of using gosu in an image for docker and jenkins: https://github.com/bmitch3020/jenkins-docker\nThe entrypoint.sh looks up the gid of the /var/lib/docker.sock file and updates the gid of the docker user inside the container to match. This allows the image to be ported to other docker hosts where the gid on the host may differ. Changing the group requires root access inside the container. Had I used USER jenkins in the dockerfile, I would be stuck with the gid of the docker group as defined in the image which wouldn't work if it doesn't match that of the docker host it's running on. But root access can be dropped when running the app which is where gosu comes in.\nAt the end of the script, the exec call prevents the shell from forking gosu, and instead it replaces pid 1 with that process. Gosu in turn does the same, switching the uid and then exec'ing the jenkins process so that it takes over as pid 1. This allows signals to be handled correctly which would otherwise be ignored by a shell as pid 1.",
    "What does minikube docker-env mean?": "The command minikube docker-env returns a set of Bash environment variable exports to configure your local environment to re-use the Docker daemon inside the Minikube instance.\nPassing this output through eval causes bash to evaluate these exports and put them into effect.\nYou can review the specific commands which will be executed in your shell by omitting the evaluation step and running minikube docker-env directly. However, this will not perform the configuration \u2013 the output needs to be evaluated for that.\nThis is a workflow optimization intended to improve your experience with building and running Docker images which you can run inside the minikube environment. It is not mandatory that you re-use minikube's Docker daemon to use minikube effectively, but doing so will significantly improve the speed of your code-build-test cycle.\nIn a normal workflow, you would have a separate Docker registry on your host machine to that in minikube, which necessitates the following process to build and run a Docker image inside minikube:\nBuild the Docker image on the host machine.\nRe-tag the built image in your local machine's image registry with a remote registry or that of the minikube instance.\nPush the image to the remote registry or minikube.\n(If using a remote registry) Configure minikube with the appropriate permissions to pull images from the registry.\nSet up your deployment in minikube to use the image.\nBy re-using the Docker registry inside Minikube, this becomes:\nBuild the Docker image using Minikube's Docker instance. This pushes the image to Minikube's Docker registry.\nSet up your deployment in minikube to use the image.\nMore details of the purpose can be found in the minikube docs.",
    "Multiple commands on docker ENTRYPOINT": "In case you want to run many commands at entrypoint, the best idea is to create a bash file. For example commands.sh like this\n#!/bin/bash\nmkdir /root/.ssh\necho \"Something\"\ncd tmp\nls\n...\nAnd then, in your DockerFile, set entrypoint to commands.sh file (that execute and run all your commands inside)\nCOPY commands.sh /scripts/commands.sh\nRUN [\"chmod\", \"+x\", \"/scripts/commands.sh\"]\nENTRYPOINT [\"/scripts/commands.sh\"]\nAfter that, each time you start your container, commands.sh will be execute and run all commands that you need. You can take a look here https://github.com/dangminhtruong/drone-chatwork",
    "entrypoint file not found": "I had this problem with Docker for Windows and the solution was changing the entrypoint script file from CRLF -> LF.",
    "Use docker run command to pass arguments to CMD in Dockerfile": "Make sure your Dockerfile declares an environment variable with ENV:\nENV environment default_env_value\nENV cluster default_cluster_value\nThe ENV <key> <value> form can be replaced inline.\nThen you can pass an environment variable with docker run. Note that each variable requires a specific -e flag to run.\ndocker run -p 9000:9000 -e environment=dev -e cluster=0 -d me/app\nOr you can set them through your compose file:\nnode:\n  environment:\n    - environment=dev\n    - cluster=0\nYour Dockerfile CMD can use that environment variable, but, as mentioned in issue 5509, you need to do so in a sh -c form:\nCMD [\"sh\", \"-c\", \"node server.js ${cluster} ${environment}\"]\nThe explanation is that the shell is responsible for expanding environment variables, not Docker. When you use the JSON syntax, you're explicitly requesting that your command bypass the shell and be executed directly.\nSame idea with Builder RUN (applies to CMD as well):\nUnlike the shell form, the exec form does not invoke a command shell.\nThis means that normal shell processing does not happen.\nFor example, RUN [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: RUN [ \"sh\", \"-c\", \"echo $HOME\" ].\nWhen using the exec form and executing a shell directly, as in the case for the shell form, it is the shell that is doing the environment variable expansion, not docker.\nAs noted by Pwnstar in the comments:\nWhen you use a shell command, this process will not be started with id=1.",
    "How does the new Docker --squash work": "If I add a secret file in my first layer, then use the secret file in my second layer, and the finally remove my secret file in the third layer, and then build with the --squash flag.\nWill there be any way now to get the secret file?\nAnswer: Your image won't have the secret file.\nHow --squash works:\nOnce the build is complete, Docker creates a new image loading the diffs from each layer into a single new layer and references all the parent's layers.\nIn other words: when squashing, Docker will take all the filesystem layers produced by a build and collapse them into a single new layer.\nThis can simplify the process of creating minimal container images, but may result in slightly higher overhead when images are moved around (because squashed layers can no longer be shared between images). Docker still caches individual layers to make subsequent builds fast.\nPlease note this feature squashes all the newly built layers into a single layer, it is not squashing to scratch.\nSide notes:\nDocker 1.13 also has support for compressing the build context that is sent from CLI to daemon using the --compress flag. This will speed up builds done on remote daemons by reducing the amount of data sent.\nPlease note as of Docker 1.13 this feature is experimental.\nUpdate 2024: Squash has been moved to buildkit and later on deprecated from buildkit\nWARNING: experimental flag squash is removed with BuildKit. You should squash inside build using a multi-stage Dockerfile for efficiency.\nAs the warning suggests you need to use multi-stage builds instead of squashing layers.\nExample:\n# syntax=docker/dockerfile:1\nFROM golang:1.21\nWORKDIR /src\nCOPY <<EOF ./main.go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n  fmt.Println(\"hello, world\")\n}\nEOF\nRUN go build -o /bin/hello ./main.go\n\nFROM scratch\nCOPY --from=0 /bin/hello /bin/hello\nCMD [\"/bin/hello\"]",
    "Wordpress Docker won't increase upload limit": "it worked for me as follows: i created uploads.ini alongside of docker-compose.yml with following lines. this is exactly how it stated in fist post.\nfile_uploads = On\nmemory_limit = 500M\nupload_max_filesize = 500M\npost_max_size = 500M\nmax_execution_time = 600\nafter this i added\nvolumes: \n   - ./uploads.ini:/usr/local/etc/php/conf.d/uploads.ini\nto my .yml file as it states in first post.\nafter this i had to delete the container/images (basically start from scratch):\ndocker stop [image name]\ndocker rm [image name]\ndocker image rm [image name]\nsome places i end up using ID insted of image name. name or ID basically you have to stop, remove container and image. bottom line is start from scratch with additional line in your .yml file as describe in first post. remember, you'll lose all your wp work. now run\ndocker-compose up -d --build\nupload limit should be increased now. i was able to upload my new bigger theme after this change. no more upload file size error. only question is if you need to increase this upload size limit in the middle of your work then how would you do this?...",
    "rebuild docker image from specific step": "You can rebuild the entire thing without using the cache by doing a\ndocker build --no-cache -t user/image-name\nTo force a rerun starting at a specific line, you can pass an arg that is otherwise unused. Docker passes ARG values as environment variables to your RUN command, so changing an ARG is a change to the command which breaks the cache. It's not even necessary to define it yourself on the RUN line.\nFROM ubuntu:14.04\nMAINTAINER Samuel Alexander <samuel@alexander.com>\n\nRUN apt-get -y install software-properties-common\nRUN apt-get -y update\n\n# Install Java.\nRUN echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | debconf-set-selections\nRUN add-apt-repository -y ppa:webupd8team/java\nRUN apt-get -y update\nRUN apt-get install -y oracle-java8-installer\nRUN rm -rf /var/lib/apt/lists/*\nRUN rm -rf /var/cache/oracle-jdk8-installer\n\n# Define working directory.\nWORKDIR /work\n\n# Define commonly used JAVA_HOME variable\nENV JAVA_HOME /usr/lib/jvm/java-8-oracle\n\n# JAVA PATH\nENV PATH /usr/lib/jvm/java-8-oracle/bin:$PATH\n\n# Install maven\nRUN apt-get -y update\nRUN apt-get -y install maven\n\n# Install Open SSH and git\nRUN apt-get -y install openssh-server\nRUN apt-get -y install git\n\n# clone Spark\nRUN git clone https://github.com/apache/spark.git\nWORKDIR /work/spark\nRUN mvn -DskipTests clean package\n\n# clone and build zeppelin fork, changing INCUBATOR_VER will break the cache here\nARG INCUBATOR_VER=unknown\nRUN git clone https://github.com/apache/incubator-zeppelin.git\nWORKDIR /work/incubator-zeppelin\nRUN mvn clean package -Pspark-1.6 -Phadoop-2.6 -DskipTests\n\n# Install Supervisord\nRUN apt-get -y install supervisor\nRUN mkdir -p var/log/supervisor\n\n# Configure Supervisord\nCOPY conf/supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n\n# bash\nRUN sed -i s#/home/git:/bin/false#/home/git:/bin/bash# /etc/passwd\n\nEXPOSE 8080 8082\nCMD [\"/usr/bin/supervisord\"]\nAnd then just run it with a unique arg:\ndocker build --build-arg INCUBATOR_VER=20160613.2 -t user/image-name .\nTo change the argument with every build, you can pass a timestamp as the arg:\ndocker build --build-arg INCUBATOR_VER=$(date +%Y%m%d-%H%M%S) -t user/image-name .\nor:\ndocker build --build-arg INCUBATOR_VER=$(date +%s) -t user/image-name .\nAs an aside, I'd recommend the following changes to keep your layers smaller, the more you can merge the cleanup and delete steps on a single RUN command after the download and install, the smaller your final image will be. Otherwise your layers will include all the intermediate steps between the download and cleanup:\nFROM ubuntu:14.04\nMAINTAINER Samuel Alexander <samuel@alexander.com>\n\nRUN DEBIAN_FRONTEND=noninteractive \\\n    apt-get -y install software-properties-common && \\\n    apt-get -y update\n\n# Install Java.\nRUN echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | debconf-set-selections && \\\n    add-apt-repository -y ppa:webupd8team/java && \\\n    apt-get -y update && \\\n    DEBIAN_FRONTEND=noninteractive \\\n    apt-get install -y oracle-java8-installer && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/* && \\\n    rm -rf /var/cache/oracle-jdk8-installer && \\\n\n# Define working directory.\nWORKDIR /work\n\n# Define commonly used JAVA_HOME variable\nENV JAVA_HOME /usr/lib/jvm/java-8-oracle\n\n# JAVA PATH\nENV PATH /usr/lib/jvm/java-8-oracle/bin:$PATH\n\n# Install maven\nRUN apt-get -y update && \\\n    DEBIAN_FRONTEND=noninteractive \\\n    apt-get -y install \n      maven \\\n      openssh-server \\\n      git \\\n      supervisor && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# clone Spark\nRUN git clone https://github.com/apache/spark.git\nWORKDIR /work/spark\nRUN mvn -DskipTests clean package\n\n# clone and build zeppelin fork\nARG INCUBATOR_VER=unknown\nRUN git clone https://github.com/apache/incubator-zeppelin.git\nWORKDIR /work/incubator-zeppelin\nRUN mvn clean package -Pspark-1.6 -Phadoop-2.6 -DskipTests\n\n# Configure Supervisord\nRUN mkdir -p var/log/supervisor\nCOPY conf/supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n\n# bash\nRUN sed -i s#/home/git:/bin/false#/home/git:/bin/bash# /etc/passwd\n\nEXPOSE 8080 8082\nCMD [\"/usr/bin/supervisord\"]",
    "Execute command on host during docker build": "(Just a suggestion)\nWe usually have the following structure for building our docker images:\nmy-image/\n\u251c\u2500\u2500 assets\n\u2502   \u251c\u2500\u2500 entrypoint.sh\n\u2502   \u2514\u2500\u2500 install.sh\n\u251c\u2500\u2500 build.sh\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 VERSION\nbuild.sh: This is where you should invoke script_that_creates_magic_file.sh. Other common tasks involve downloading required files or temporarily copying ssh keys from the host. Finally, this script will call docker build .\nDockerfile: As usual, but depending on the number of commands we need to run we might have an install.sh\ninstall.sh: This is copied and run inside the container, installs packages, removes unnecessary files, etc. Without being 100% sure - I think such an approach reduces the number of layers avoiding multiple commands in a single RUN\nentrypoint.sh: Container's entrypoint. Allows us to perform tasks when the container starts (like parse environment variables) and print debugging info\nI find the above structure convenient and self-documented since everyone in the team can build any image (no special instructions/steps). The README is there to explain what the image is doing... but I won't lie to you... it is usually empty... (or has an h1 for the gitlab to display) :)",
    "Docker compose with name other than dockerfile": "In your docker-compose, under the service:\nservices:\n  serviceA:\n    build: \n      context: <folder of your project>\n      dockerfile: <path and name to your Dockerfile>",
    "How to mount local volumes in docker machine": "Docker-machine automounts the users directory... But sometimes that just isn't enough.\nI don't know about docker 1.6, but in 1.8 you CAN add an additional mount to docker-machine\nAdd Virtual Machine Mount Point (part 1)\nCLI: (Only works when machine is stopped)\nVBoxManage sharedfolder add <machine name/id> --name <mount_name> --hostpath <host_dir> --automount\nSo an example in windows would be\n/c/Program\\ Files/Oracle/VirtualBox/VBoxManage.exe sharedfolder add default --name e --hostpath 'e:\\' --automount\nGUI: (does NOT require the machine be stopped)\nStart \"Oracle VM VirtualBox Manager\"\nRight-Click <machine name> (default)\nSettings...\nShared Folders\nThe Folder+ Icon on the Right (Add Share)\nFolder Path: <host dir> (e:)\nFolder Name: <mount name> (e)\nCheck on \"Auto-mount\" and \"Make Permanent\" (Read only if you want...) (The auto-mount is sort of pointless currently...)\nMounting in boot2docker (part 2)\nManually mount in boot2docker:\nThere are various ways to log in, use \"Show\" in \"Oracle VM VirtualBox Manager\", or ssh/putty into docker by IP address docker-machine ip default, etc...\nsudo mkdir -p <local_dir>\nsudo mount -t vboxsf -o defaults,uid=`id -u docker`,gid=`id -g docker` <mount_name> <local_dir>\nBut this is only good until you restart the machine, and then the mount is lost...\nAdding an automount to boot2docker:\nWhile logged into the machine\nEdit/create (as root) /mnt/sda1/var/lib/boot2docker/bootlocal.sh, sda1 may be different for you...\nAdd\nmkdir -p <local_dir>\nmount -t vboxsf -o defaults,uid=`id -u docker`,gid=`id -g docker` <mount_name> <local_dir>\nWith these changes, you should have a new mount point. This is one of the few files I could find that is called on boot and is persistent. Until there is a better solution, this should work.\nOld method: Less recommended, but left as an alternative\nEdit (as root) /mnt/sda1/var/lib/boot2docker/profile, sda1 may be different for you...\nAdd\nadd_mount() {\n  if ! grep -q \"try_mount_share $1 $2\" /etc/rc.d/automount-shares ; then\n    echo \"try_mount_share $1 $2\" >> /etc/rc.d/automount-shares\n  fi\n}\n\nadd_mount <local dir> <mount name>\nAs a last resort, you can take the slightly more tedious alternative, and you can just modify the boot image.\ngit -c core.autocrlf=false clone https://github.com/boot2docker/boot2docker.git\ncd boot2docker\ngit -c core.autocrlf=false checkout v1.8.1 #or your appropriate version\nEdit rootfs/etc/rc.d/automount-shares\nAdd try_mount_share <local_dir> <mount_name> line right before fi at the end. For example\ntry_mount_share /e e\nJust be sure not to set the to anything the os needs, like /bin, etc...\ndocker build -t boot2docker . #This will take about an hour the first time :(\ndocker run --rm boot2docker > boot2docker.iso\nBackup the old boot2docker.iso and copy your new one in its place, in ~/.docker/machine/machines/\nThis does work, it's just long and complicated\ndocker version 1.8.1, docker-machine version 0.4.0",
    "Docker create network should ignore existing network": "Building on @AndyTriggs' answer, a neat (and correct) solution would be:\ndocker network inspect my_local_network >/dev/null 2>&1 || \\\n    docker network create --driver bridge my_local_network",
    "Can we mount sub-directories of a named volume in docker?": "2023: As noted by Michael Bolli in the comments, that feature is now a work-in-progress:\nPR 45687: \"volumes: Implement subpath mount\"\nMake it possible to mount subdirectory of a named volume.\nQ1 2024: this is merged! Commit 31ccdbb.\nPossibly for Moby 26.0.\n[Documentation: docker/docs PR 20577 with example:\nMount a volume subdirectory\nWhen you mount a volume to a container, you can specify a subdirectory of the volume to use, with the volume-subpath parameter for the --mount flag. The subdirectory that you specify must exist in the volume before you attempt to mount it into a container; if it doesn't exist, the mount fails.\nSpecifying volume-subpath is useful if you only want to share a specific portion of a volume with a container. Say for example that you have multiple containers running and you want to store logs from each container in a shared volume. You can create a subdirectory for each container in the shared volume, and mount the subdirectory to the container.\nThe following example creates a logs volume and initiates the subdirectories app1 and app2 in the volume. It then starts two containers and mounts one of the subdirectories of the logs volume to each container. This example assumes that the processes in the containers write their logs to /var/log/app1 and /var/log/app2.\n$ docker volume create logs\n$ docker run --rm \\\n  --mount src=logs,dst=/logs \\\n  alpine mkdir -p /logs/app1 /logs/app2\n$ docker run -d \\\n  --name=app1 \\\n  --mount src=logs,dst=/var/log/app1/,volume-subpath=app1 \\\n  app1:latest\n$ docker run -d \\\n  --name=app2 \\\n  --mount src=logs,dst=/var/log/app2,volume-subpath=app2 \\\n  app2:latest\nWith this setup, the containers write their logs to separate subdirectories of the logs volume. The containers can't access the other container's logs.\nMarch 2024, as mentioned in issue 32582:\nCLI already supports it (starting from v26.0.0-rc1): docker/cli PR #4331\n(and you can already install it from the test channel)\nCompose support is still WIP (it needs to move to v26 first).\nMarch 2024: Pawe\u0142 Gronowski confirms in the same issue:\nmoby v26.0.0 is released, so you can already try out this feature.\n2016: No because compose/config/config.py#load(config_details) check if datavolume/sql_data matches a named volume (in compose/config/validation.py#match_named_volumes()).\ndatavolume would, datavolume/sql_data would not.\nAs memetech points out in the comments, the is an issue tracking this since April 2017:\nmoby/moby issue 32582: \"[feature] Allow mounting sub-directories of named volumes\".\nIn that issue, Joohansson adds (see comment)\nIn the meantime, I use this workaround to mount the whole volume on a separate path and then symlink it to the sub path.\n# In the Dockerfile:\nRUN mkdir -p /data/subdir\nRUN ln -s /data/subdir /var/www/subdir\nThen mount the volume as normal.\nThe /subdir must exist in the volume.\ndocker run -d -v myvol:/data mycontainer\nNow anything read or written by the webserver will be stored in the volume subdir and can't access the other data.",
    "\"docker build\" requires exactly 1 argument(s)": "Parameter -f changes the name of the Dockerfile (when it's different than regular Dockerfile). It is not for passing the full path to docker build. The path goes as the first argument.\nSyntax is:\ndocker build [PARAMS] PATH\nSo in your case, this should work:\ndocker build -f MyDockerfile -t proj:myapp /full/path/to/\nor in case you are in the project directory, you just need to use a dot:\ndocker build -f MyDockerfile -t proj:myapp .",
    "Docker build pull access denied, repository does not exist or may require": "You have stages called base, build, and debug. Then in the final stage you have:\nCOPY --from=publish/app /app .\nWhen docker can't find the stage with that name, publish/app, it tries to find that image, which doesn't exist. I'm guessing you want to copy from the build stage, e.g.\nCOPY --from=build /app .",
    "Is it possible to show the `WORKDIR` when building a docker image?": "There's no builtin way for Docker to print the WORKDIR during a build. You can inspect the final workdir for an image/layer via the .Config.WorkingDir property in the inspect output:\ndocker image inspect -f '{{.Config.WorkingDir}}' {image-name}\nIt's possible to view a Linux containers build steps workdir by printing the shells default working directory:\nRUN pwd\nor the shell often stores the working directory in the PWD environment variable\nRUN echo \"$PWD\"\nIf the RUN step has run before and is cached, add the --no-cache flag. If you are using newer versions of docker with BuildKit, stdout from the build will need to be enabled with --progress=plain\ndocker build --no-cache --progress=plain . ",
    "How to give folder permissions inside a docker container Folder": "I guess you are switching to user \"admin\" which doesn't have the ownership to change permissions on /app directory. Change the ownership using \"root\" user. Below Dockerfile worked for me -\nFROM python:2.7\nRUN pip install Flask==0.11.1 \nRUN useradd -ms /bin/bash admin\nCOPY app /app\nWORKDIR /app\nRUN chown -R admin:admin /app\nRUN chmod 755 /app\nUSER admin\nCMD [\"python\", \"app.py\"] \nPS - Try to get rid of \"777\" permission. I momentarily tried to do it in above Dockerfile.",
    "/bin/sh: 1: apk: not found while creating docker image": "As larsks mentions, apk is for Alpine distributions and you selected FROM ubuntu:trusty which is Debian based with the apt-get command. Change your FROM line to FROM alpine:3.4 to switch to the Alpine based image with apk support.",
    "How do I run a Bash script in an Alpine Docker container?": "Alpine comes with ash as the default shell instead of bash.\nSo you can\nHave a shebang defining /bin/bash as the first line of your sayhello.sh, so your file sayhello.sh will begin with bin/sh\n#!/bin/sh\nInstall Bash in your Alpine image, as you seem to expect Bash is present, with such a line in your Dockerfile:\nRUN apk add --no-cache --upgrade bash",
    "How to add user with dockerfile?": "Use useradd instead of its interactive adduser to add user.\nRUN useradd -ms /bin/bash  vault\nBelow command will not create user .\nUSER vault\nWORKDIR /usr/local/bin/vault\nit will use vault user\nplease Refer Dockerfile User Documentation\nThe USER instruction sets the user name or UID to use when running the image and for any RUN, CMD and ENTRYPOINT instructions that follow it in the Dockerfile.\nNOTE : Ensures that bash is the default shell.\nIf default shell is /bin/sh you can do like:\nRUN ln -sf /bin/bash /bin/sh\nRUN useradd -ms /bin/bash  vault",
    "How to ADD all files/directories except a hidden directory like .git in Dockerfile": "You may exclude unwanted files with the help of the .dockerignore file",
    "How to prevent Dockerfile caching git clone": "Another workaround:\nIf you use GitHub (or gitlab or bitbucket too most likely) you can ADD the GitHub API's representation of your repo to a dummy location.\nADD https://api.github.com/repos/$USER/$REPO/git/refs/heads/$BRANCH version.json\nRUN git clone -b $BRANCH https://github.com/$USER/$REPO.git $GIT_HOME/\nThe API call will return different results when the head changes, invalidating the docker cache.\nIf you're dealing with private repos you can use github's x-oauth-basic authentication scheme with a personal access token like so:\nADD https://$ACCESS_TOKEN:x-oauth-basic@api.github.com/repos/$USER/$REPO/git/refs/heads/$BRANCH version.json\n(thx @captnolimar for a suggested edit to clarify authentication)",
    "Docker-Compose file has yaml.scanner.ScannerError": "Ok, I wasted around 3 hours to debug a similar issue.\nIf you guys ever get the below error\nERROR: yaml.scanner.ScannerError: mapping values are not allowed here\nin \".\\docker-compose.yml\", line 2, column 9\nIt's because a space is needed between\nversion:'3' <-- this is wrong\nversion: '3' <-- this is correct.\nAlso, if you are using eclipse, do yourself a favor and install the YEdit YAML editor plugin",
    "alpine package py-pip missing": "Do update first:\napk add --update py-pip\nOr:\napk update\napk add py-pip",
    "Error response from daemon: Dockerfile parse error Unknown flag: mount": "tl;dr\nDockerfile\n# syntax=docker/dockerfile:experimental\nFROM continuumio/miniconda3\n\nRUN --mount=type=ssh pip install git+ssh://git@github.com/myrepo/myproject.git@develop\nRUN conda install numpy\n...\nNote: the comment on the first line is required voodoo\nThen build your docker image with:\nDOCKER_BUILDKIT=1 docker build --ssh default -t my_image .\nWith this, you will be able to use the --mount option for the RUN directive in your Dockerfile.\nLong answer\nAs found in the documentation here, ssh forwarding when building docker image is enabled only when using the BuildKit backend:\nExternal implementation features\nThis feature is only available when using the BuildKit backend.\nDocker build supports experimental features like cache mounts, build secrets and ssh forwarding that are enabled by using an external implementation of the builder with a syntax directive. To learn about these features, refer to the documentation in BuildKit repository.\nFor this you need Docker 18.09 (or later) and you also need to run the docker build command with the DOCKER_BUILDKIT=1 environment variable and start your Docker file with the following magic comment : # syntax=docker/dockerfile:experimental.\nAlso you can edit /etc/docker/daemon.json and add :\n{\n    \"experimental\" : false,\n    \"debug\" : true,\n    \"features\": {\n        \"buildkit\" : true\n    }\n}",
    "Operation of the mkdir command with dockerfile": "The reason is that you are mounting a volume from your host to /var/www/html. Step by step:\nRUN mkdir -p /var/www/html/foo creates the foo directory inside the filesystem of your container.\ndocker-compose.yml ./code:/var/www/html \"hides\" the content of /var/www/html in the container filesystem behind the contents of ./code on the host filesystem.\nSo actually, when you exec into your container you see the contents of the ./code directory on the host when you look at /var/www/html.\nFix: Either you remove the volume from your docker-compose.yml or you create the foo-directory on the host before starting the container.\nAdditional Remark: In your Dockerfile you declare a volume as VOLUME ./code:/var/www/html. This does not work and you should probably remove it. In a Dockerfile you cannot specify a path on your host.\nQuoting from docker:\nThe host directory is declared at container run-time: The host directory (the mountpoint) is, by its nature, host-dependent. This is to preserve image portability. since a given host directory can\u2019t be guaranteed to be available on all hosts. For this reason, you can\u2019t mount a host directory from within the Dockerfile. The VOLUME instruction does not support specifying a host-dir parameter. You must specify the mountpoint when you create or run the container.",
    "Should I minimize the number of docker layers?": "I work in a small team with a private Docker registry. We won't ever meet the 42 layers restriction and care mostly about performance and development speed.\nIf so, should I minimize the number of docker layers?\nIn your case, no.\nWhat needs to be minimized is the build time, which means:\nmaking sure the most general steps, and the longest are first, that will then cached, allowing you to fiddle with the last lines of your Dockerfile (the most specific commands) while having a quick rebuild time.\nmaking sure the longest RUN command come first and in their own layer (again to be cached), instead of being chained with other RUN commands: if one of those fail, the long command will have to be re-executed. If that long command is isolated in its own (Dockerfile line)/layer, it will be cached.\nThat being said, the documentation you mention comes from docker/docker.github.io, precisely PR 4992 and PR 4854, after a docker build LABEL section.\nSo this section comes after a similar remark about LABEL, and just emphasize the commands creating layers.\nAgain, in your case, that would not be important.",
    "Difference between 'image' and 'build' within docker compose": "image means docker compose will run a container based on that image\nbuild means docker compose will first build an image based on the Dockerfile found in the path associated with build (and then run a container based on that image).\nPR 2458 was eventually merged to allow both (and use image as the image name when building, if it exists).\ntherobyouknow mentions in the comments:\ndockerfile: as a sub-statement beneath build: can be used to specify the filename/path of the Dockerfile.\nversion: '3'\nservices:\n  webapp:\n    build:\n      context: ./dir\n      dockerfile: Dockerfile-alternate\n      args:\n        buildno: 1",
    "How are Packer and Docker different? Which one should I prefer when provisioning images?": "Docker is a system for building, distributing and running OCI images as containers. Containers can be run on Linux and Windows.\nPacker is an automated build system to manage the creation of images for containers and virtual machines. It outputs an image that you can then take and run on the platform you require.\nFor v1.8 this includes - Alicloud ECS, Amazon EC2, Azure, CloudStack, DigitalOcean, Docker, Google Cloud, Hetzner, Hyper-V, Libvirt, LXC, LXD, 1&1, OpenStack, Oracle OCI, Parallels, ProfitBricks, Proxmox, QEMU, Scaleway, Triton, Vagrant, VirtualBox, VMware, Vultr\nDocker's Dockerfile\nDocker uses a Dockerfile to manage builds which has a specific set of instructions and rules about how you build a container.\nImages are built in layers. Each FROM RUN ADD COPY commands modify the layers included in an OCI image. These layers can be cached which helps speed up builds. Each layer can also be addressed individually which helps with disk usage and download usage when multiple images share layers.\nDockerfiles have a bit of a learning curve, It's best to look at some of the official Docker images for practices to follow.\nPacker's Docker builder\nPacker does not require a Dockerfile to build a container image. The docker plugin has a HCL or JSON config file which start the image build from a specified base image (like FROM).\nPacker then allows you to run standard system config tools called \"Provisioners\" on top of that image. Tools like Ansible, Chef, Salt, shell scripts etc. This image will then be exported as a single layer, so you lose the layer caching/addressing benefits compared to a Dockerfile build.\nPacker allows some modifications to the build container environment, like running as --privileged or mounting a volume at build time, that Docker builds will not allow.\nTimes you might want to use Packer are if you want to build images for multiple platforms and use the same setup. It also makes it easy to use existing build scripts if there is a provisioner for it.",
    "Does putting ARG at top of Dockerfile prevent layer re-use?": "In general, it is better to place ARG just before it is used rather than at the top of the file.\nTo be more precise, not all lines are cache invalidated after an ARG declaration. Only those that use ARG values and RUNs. The docker documentation provides details:\nImpact on build caching\nARG variables are not persisted into the built image as ENV variables are. However, ARG variables do impact the build cache in similar ways. If a Dockerfile defines an ARG variable whose value is different from a previous build, then a \u201ccache miss\u201d occurs upon its first usage, not its definition. In particular, all RUN instructions following an ARG instruction use the ARG variable implicitly (as an environment variable), thus can cause a cache miss. All predefined ARG variables are exempt from caching unless there is a matching ARG statement in the Dockerfile.\nhttps://docs.docker.com/engine/reference/builder/#impact-on-build-caching\nYou'll have to move your ARGs under the RUNs that would not need the argument in order to keep layer cache optimized.\nFor more info:\nhttps://github.com/moby/moby/issues/18017\nhttps://github.com/moby/moby/pull/18161\nRUN explanation here: https://github.com/moby/moby/pull/21885",
    "Docker image error: \"/bin/sh: 1: [python,: not found\"": "Use \" instead of ' in CMD. (Documentation)",
    "Maven docker cache dependencies": "You should also consider using mvn dependency:resolve or mvn dependency:go-offline accordingly as other comments & answers suggest.\nUsually, there's no change in pom.xml file but just some other source code changes when you're attempting to start docker image build. In such circumstance you can do this:\nFROM maven:3-jdk-8\n\nENV HOME=/home/usr/app\n\nRUN mkdir -p $HOME\n\nWORKDIR $HOME\n\n# 1. add pom.xml only here\n\nADD pom.xml $HOME\n\n# 2. start downloading dependencies\n\nRUN [\"/usr/local/bin/mvn-entrypoint.sh\", \"mvn\", \"verify\", \"clean\", \"--fail-never\"]\n\n# 3. add all source code and start compiling\n\nADD . $HOME\n\nRUN [\"mvn\", \"package\"]\n\nEXPOSE 8005\n\nCMD [\"java\", \"-jar\", \"./target/dist.jar\"]\nSo the key is:\nadd pom.xml file.\nthen mvn verify --fail-never it, it will download maven dependencies.\nadd all your source file then, and start your compilation(mvn package).\nWhen there are changes in your pom.xml file or you are running this script for the first time, docker will do 1 -> 2 -> 3. When there are no changes in pom.xml file, docker will skip step 1\u30012 and do 3 directly.\nThis simple trick can be used in many other package management circumstances(gradle, yarn, npm, pip).",
    "Hidden file .env not copied using Docker COPY": "If you have .dockerignore file then it might be you added to ignore hidden files like .git, .vagrant etc.\nIf .dockerfile ignoring hidden files then either you can enable to not ignore or change file name.\nFor more info about .dockerignore file",
    "Using docker-compose to set containers timezones": "This is simple solution:\nenvironment:\n  - TZ=America/Denver",
    "How to run kubectl commands inside a container?": "I would use kubernetes api, you just need to install curl, instead of kubectl and the rest is restful.\ncurl http://localhost:8080/api/v1/namespaces/default/pods\nIm running above command on one of my apiservers. Change the localhost to apiserver ip address/dns name.\nDepending on your configuration you may need to use ssl or provide client certificate.\nIn order to find api endpoints, you can use --v=8 with kubectl.\nexample:\nkubectl get pods --v=8\nResources:\nKubernetes API documentation\nUpdate for RBAC:\nI assume you already configured rbac, created a service account for your pod and run using it. This service account should have list permissions on pods in required namespace. In order to do that, you need to create a role and role binding for that service account.\nEvery container in a cluster is populated with a token that can be used for authenticating to the API server. To verify, Inside the container run:\ncat /var/run/secrets/kubernetes.io/serviceaccount/token\nTo make request to apiserver, inside the container run:\ncurl -ik \\\n     -H \"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n     https://kubernetes.default.svc.cluster.local/api/v1/namespaces/default/pods",
    "Docker - What is proper way to rebuild and push updated image to docker cloud?": "I found the problem, thanks to @lorenzvth7!\nI've had two images with same tag (which i was pushing to cloud).\nSolution is:\nInspect your images and find two or more with the same tag:\ndocker images \nDelete them:\ndocker rmi --force 'image id'\nThats it! Follow steps from my question above.",
    "Dockerfile CMD instruction will exit the container just after running it": "A docker container will run as long as the CMD from your Dockerfile takes.\nIn your case your CMD consists of a shell script containing a single echo. So the container will exit after completing the echo.\nYou can override CMD, for example:\nsudo docker run -it --entrypoint=/bin/bash <imagename>\nThis will start an interactive shell in your container instead of executing your CMD. Your container will exit as soon as you exit that shell.\nIf you want your container to remain active, you have to ensure that your CMD keeps running. For instance, by adding the line while true; do sleep 1; done to your shell.sh file, your container will print your hello message and then do nothing any more until you stop it (using docker stop in another terminal).\nYou can open a shell in the running container using docker exec -it <containername> bash. If you then execute command ps ax, it will show you that your shell.sh is still running inside the container.",
    "Reuse inherited image's CMD or ENTRYPOINT": "As mentioned in the comments, there's no built-in solution to this. From the Dockerfile, you can't see the value of the current CMD or ENTRYPOINT. Having a run-parts solution is nice if you control the upstream base image and include this code there, allowing downstream components to make their changes. But docker there's one inherent issue that will cause problems with this, containers should only run a single command that needs to run in the foreground. So if the upstream image kicks off, it would stay running without giving your later steps a chance to run, so you're left with complexities to determine the order to run commands to ensure that a single command does eventually run without exiting.\nMy personal preference is a much simpler and hardcoded option, to add my own command or entrypoint, and make the last step of my command to exec the upstream command. You will still need to manually identify the script name to call from the upstream Dockerfile. But now in your start.sh, you would have:\n#!/bin/sh\n\n# run various pieces of initialization code here\n# ...\n\n# kick off the upstream command:\nexec /upstream-entrypoint.sh \"$@\"\nBy using an exec call, you transfer pid 1 to the upstream entrypoint so that signals get handled correctly. And the trailing \"$@\" passes through any command line arguments. You can use set to adjust the value of $@ if there are some args you want to process and extract in your own start.sh script.",
    "Docker container doesn't expose ports when --net=host is mentioned in the docker run command": "I was confused by this answer. Apparently my docker image should be reachable on port 8080. But it wasn't. Then I read\nhttps://docs.docker.com/network/host/\nTo quote\nThe host networking driver only works on Linux hosts, and is not supported on Docker for Mac, Docker for Windows, or Docker EE for Windows Server.\nThat's rather annoying as I'm on a Mac. The docker command should report an error rather than let me think it was meant to work.\nDiscussion on why it does not report an error\nhttps://github.com/docker/for-mac/issues/2716\nNot sure I'm convinced.\nUpdated 2024: As per comments and other answers there have been changes in this area. See Docker container doesn't expose ports when --net=host is mentioned in the docker run command",
    "Externalising Spring Boot properties when deploying to Docker": "DOCKER IMAGE CONFIGURATION\nIf you look to the way Spring recommends to launch a Spring Boot powered docker container, that's what you find:\nFROM openjdk:8-jdk-alpine\nVOLUME /tmp\nARG JAR_FILE\nCOPY ${JAR_FILE} app.jar\nENTRYPOINT [\"java\",\"-Djava.security.egd=file:/dev/./urandom\",\"-jar\",\"/app.jar\"]\nThat means your image extends openjdk and your container has its own environment. If you're doing like that, it would be enough to declare what you want to override as environment properties and Spring Boot will fetch them, since environment variables take precedence over the yml files.\nEnvironment variables can be passed in your docker command too, to launch the container with your desired configuration. If you want to set some limit for the JVM memory, see the link below.\nDOCKER COMPOSE SAMPLE\nHere you have an example of how I launch a simple app environment with docker compose. As you see, I declare the spring.datasource.url property here as an environment variable, so it overrides whatever you've got in your application.yml file.\nversion: '2'\nservices:\n    myapp:\n        image: mycompany/myapp:1.0.0\n        container_name: myapp\n        depends_on:\n        - mysql\n        environment:\n            - SPRING_DATASOURCE_URL=jdbc:mysql://mysql:3306/myapp?useUnicode=true&characterEncoding=utf8&useSSL=false\n        ports:\n            - 8080:8080\n\n    mysql:\n        image: mysql:5.7.19\n        container_name: mysql\n        volumes:\n            - /home/docker/volumes/myapp/mysql/:/var/lib/mysql/\n        environment:\n            - MYSQL_USER=root\n            - MYSQL_ALLOW_EMPTY_PASSWORD=yes\n            - MYSQL_DATABASE=myapp\n        command: mysqld --lower_case_table_names=1 --skip-ssl --character_set_server=utf8\nSee also:\nHow do I pass environment variables to Docker containers?\nLimit JVM memory consumption in a Docker container",
    "Error in docker: network \"path\" declared as external, but could not be found": "I solved the issue, finally. The issue came from the fact that I had in docker-compose.yml remaxmdcrm_remaxmd-network declared as external. The external network was not created during installation, thus I needed to create a bridging network. I ran the command docker network create \"name_of_network\"\nFor further details, here is the full documentation this",
    "Dockerfile: $HOME is not working with ADD/COPY instructions": "Here's your problem:\nWhen you use the USER directive, it affects the userid used to start new commands inside the container. So, for example, if you do this:\nFROM ubuntu:utopic\nRUN useradd -m aptly\nUSER aptly\nRUN echo $HOME\nYou get this:\nStep 4 : RUN echo $HOME\n ---> Running in a5111bedf057\n/home/aptly\nBecause the RUN commands starts a new shell inside a container, which is modified by the preceding USER directive.\nWhen you use the COPY directive, you are not starting a process inside the container, and Docker has no way of knowing what (if any) environment variables would be exposed by a shell.\nYour best bet is to either set ENV HOME /home/aptly in your Dockerfile, which will work, or stage your files into a temporary location and then:\nRUN cp /skeleton/myfile $HOME/myfile\nAlso, remember that when you COPY files in they will be owned by root; you will need to explicitly chown them to the appropriate user.",
    "Docker CMD exec-form for multiple command execution": "The short answer is, you cannot chain together commands in the exec form.\n&& is a function of the shell, which is used to chain commands together. In fact, when you use this syntax in a Dockerfile, you are actually leveraging the shell functionality.\nIf you want to have multiple commands with the exec form, then you have do use the exec form to invoke the shell as follows...\nCMD [\"sh\",\"-c\",\"mkdir -p ~/my/new/directory/ && cd ~/my/new/directory && touch new.file\"]",
    "Installing Java in Docker image": "I was able to install OpenJDK 8 via the steps below (taken from here). My Dockerfile inherits from phusion/baseimage-docker, which is based on Ubuntu 16.04 LTS.\n# Install OpenJDK-8\nRUN apt-get update && \\\n    apt-get install -y openjdk-8-jdk && \\\n    apt-get install -y ant && \\\n    apt-get clean;\n    \n# Fix certificate issues\nRUN apt-get update && \\\n    apt-get install ca-certificates-java && \\\n    apt-get clean && \\\n    update-ca-certificates -f;\n\n# Setup JAVA_HOME -- useful for docker commandline\nENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/\nRUN export JAVA_HOME\nTo install OpenJDK 7 instead, you may need to prepend\nadd-apt-repository ppa:openjdk-r/ppa\nsuch that the first step becomes\n# Install OpenJDK-7\nRUN add-apt-repository ppa:openjdk-r/ppa && \\\n    apt-get update && \\\n    apt-get install -y openjdk-7-jdk && \\\n    apt-get install -y ant && \\\n    apt-get clean;",
    "Differences Between Dockerfile Instructions in Shell and Exec Form": "There are two differences between the shell form and the exec form. According to the documentation, the exec form is the preferred form. These are the two differences:\nThe exec form is parsed as a JSON array, which means that you must use double-quotes (\u201c) around words not single-quotes (\u2018).\nUnlike the shell form, the exec form does not invoke a command shell. This means that normal shell processing does not happen. For example, CMD [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: CMD [ \"sh\", \"-c\", \"echo $HOME\" ]. When using the exec form and executing a shell directly, as in the case for the shell form, it is the shell that is doing the environment variable expansion, not docker.\nSome additional subtleties here are:\nThe exec form makes it possible to avoid shell string munging, and to RUN commands using a base image that does not contain the specified shell executable.\nIn the shell form you can use a \\ (backslash) to continue a single RUN instruction onto the next line.\nThere is also a third form for CMD:\nCMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT)\nAdditionally, the exec form is required for CMD if you are using it as parameters/arguments to ENTRYPOINT that are intended to be overwritten.",
    "Dockerfile replicate the host user UID and GID to the image": "You can pass it as a build arg. Your Dockerfile can be static:\nFROM ubuntu:xenial-20170214\nARG UNAME=testuser\nARG UID=1000\nARG GID=1000\nRUN groupadd -g $GID -o $UNAME\nRUN useradd -m -u $UID -g $GID -o -s /bin/bash $UNAME\nUSER $UNAME\nCMD /bin/bash\nThen you'd pass the options on your build command:\ndocker build --build-arg UID=$(id -u) --build-arg GID=$(id -g) \\\n  -f bb.dockerfile -t testimg .\nNote that I've solved similar problems to this a different way, by running an entrypoint as root that looks a file/directory permissions of the host volume mount, and adjust the uid/gid of the users inside the container to match the volume uid/gid. After making that change, it drops access from the root user to the modified uid/gid user and runs the original command/entrypoint. The result is the image can be run unchanged on any developer machine. An example of this can be found in my jenkins-docker repo:\nhttps://github.com/sudo-bmitch/jenkins-docker",
    "Is it redundant in a Dockfile to run USER root since you're already root?": "If an image was generated from a source that changed root to a user, you may not have access to all resources inside it. However, if you load the image:\nFROM xxxxxx/xxxxxx:latest\nUSER root\nThat will give you root access to the images resources. I just used that after being refused access to change /etc/apt/sources.list in an existing image that was not mine. It worked fine and let me change the sources.list",
    "Docker compose how to mount path from one to another container?": "What you want to do is use a volume, and then mount that volume into whatever containers you want it to appear in.\nCompletely within Docker\nYou can do this completely inside of Docker.\nHere is an example (stripped-down - your real file would have much more than this in it, of course).\nversion: '3'\nservices:\n  nginx:\n    volumes:\n      - asset-volume:/var/lib/assets\n  asset:\n    volumes:\n      - asset-volume:/var/lib/assets\n\nvolumes:\n  asset-volume:\nAt the bottom is a single volume defined, named \"asset-volume\".\nThen in each of your services, you tell Docker to mount that volume at a certain path. I show example paths inside the container, just adjust these to be whatever path you wish them to be in the container.\nThe volume is an independent entity not owned by any particular container. It is just mounted into each of them, and is shared. If one container modifies the contents, then they all see the changes.\nNote that if you prefer only one can make changes, you can always mount the volume as read-only in some services, by adding :ro to the end of the volume string.\nservices:\n  servicename:\n    volumes:\n      - asset-volume:/var/lib/assets:ro\nUsing a host directory\nAlternately you can use a directory on the host and mount that into the containers. This has the advantage of you being able to work directly on the files using your tools outside of Docker (such as your GUI text editor and other tools).\nIt's the same, except you don't define a volume in Docker, instead mounting the external directory.\nversion: '3'\nservices:\n  nginx:\n    volumes:\n      - ./assets:/var/lib/assets\n  asset:\n    volumes:\n      - ./assets:/var/lib/assets\nIn this example, the local directory \"assets\" is mounted into both containers using the relative path ./assets.\nUsing both depending on environment\nYou can also set it up for a different dev and production environment. Put everything in docker-compose.yml except the volume mounts. Then make two more files.\ndocker-compose.dev.yml\ndocker-compose.prod.yml\nIn these files put only the minimum config to define the volume mount. We'll mix this with the docker-compose.yml to get a final config.\nThen use this. It will use the config from docker-compose.yml, and use anything in the second file as an override or supplemental config.\ndocker-compose -f docker-compose.yml \\\n    -f docker-compose.dev.yml \\\n    up -d\nAnd for production, just use the prod file instead of the dev file.\nThe idea here is to keep most of the config in docker-compose.yml, and only the minimum set of differences in the alternative files.\nExample:\ndocker-compose.prod.yml\nversion: '3'\nservices:\n  nginx:\n    volumes:\n      - asset-volume:/var/lib/assets\ndocker-compose.dev.yml\nversion: '3'\nservices:\n  nginx:\n    volumes:\n      - ./assets:/var/lib/assets",
    "Docker COPY not updating files when rebuilding container": "This is because of cache.\nRun,\ndocker-compose build --no-cache\nThis will rebuild images without using any cache.\nAnd then,\ndocker-compose -f docker-compose-staging.yml up -d",
    "Install python package in docker file": "Recommended base image\nAs suggested in my comment, you could write a Dockerfile that looks like:\nFROM python:3\n\nRUN pip install --no-cache-dir --upgrade pip && \\\n    pip install --no-cache-dir nibabel pydicom matplotlib pillow med2image\n    # Note: we had to merge the two \"pip install\" package lists here, otherwise\n    # the last \"pip install\" command in the OP may break dependency resolution\u2026\n\nCMD [\"cat\", \"/etc/os-release\"]\nAnd the command example above could confirm at runtime (docker build --pull -t test . && docker run --rm -it test) that this image is based on the GNU/Linux distribution \"Debian stable\".\nGeneric Dockerfile template\nFinally to give a comprehensive answer, note that a good practice regarding Python dependencies consists in specifying them in a declarative way in a dedicated text file (in alphabetical order, to ease review and update) so that for your example, you may want to write the following file:\nrequirements.txt\nmatplotlib\nmed2image\nnibabel\npillow\npydicom\nand use the following generic Dockerfile\nFROM python:3\n\nWORKDIR /usr/src/app\n\nCOPY requirements.txt ./\n\nRUN pip install --no-cache-dir --upgrade pip \\\n  && pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"./your-daemon-or-script.py\"]\nTo be more precise, this is the approach suggested in the documentation of the Docker official image python, \u00a7. How to use this image",
    "Docker-compose: deploying service in multiple hosts": "We can do this with docker compose v3 now.\nhttps://docs.docker.com/engine/swarm/#feature-highlights https://docs.docker.com/compose/compose-file/\nYou have to initialize the swarm cluster using command\n$ docker swarm init\nYou can add more nodes as worker or manager -\nhttps://docs.docker.com/engine/swarm/join-nodes/\nOnce you have your both nodes added to the cluster, pass your compose v3 i.e deployment file to create a stack. Compose file should just contain predefined images, you can't give a Dockerfile for deployment in Swarm mode.\n$ docker stack deploy -c dev-compose-deploy.yml --with-registry-auth PL\nView your stack services status -\n$ docker stack services PL\nTry to use Labels & Placement constraints to put services on different nodes.\nExample \"dev-compose-deploy.yml\" file for your reference -\nversion: \"3\"\n\nservices:\n\n  nginx:\n    image: nexus.example.com/pl/nginx-dev:latest\n    extra_hosts:\n      - \"dev-pldocker-01:10.2.0.42\u201d\n      - \"int-pldocker-01:10.2.100.62\u201d\n      - \"prd-plwebassets-01:10.2.0.62\u201d\n    ports:\n      - \"80:8003\"\n      - \"443:443\"\n    volumes:\n      - logs:/app/out/\n    networks:\n      - pl\n    deploy:\n      replicas: 3\n      labels:\n        feature.description: \u201cFrontend\u201d\n      update_config:\n        parallelism: 1\n        delay: 10s\n      restart_policy:\n        condition: any\n      placement:\n        constraints: [node.role == worker]\n    command: \"/usr/sbin/nginx\"\n\n  viz:\n    image: dockersamples/visualizer\n    ports:\n      - \"8085:8080\"\n    networks:\n      - pl\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    deploy:\n      replicas: 1\n      labels:\n        feature.description: \"Visualizer\"\n      restart_policy:\n        condition: any\n      placement:\n        constraints: [node.role == manager]\n\nnetworks:\npl:\n\nvolumes:\nlogs:",
    "How to copy files from dockerfile to host?": "This is now possible since Docker 19.03.0 in July 2019 introduced \"custom build outputs\". See the official docs about custom build outputs.\nTo enable custom build outputs from the build image into the host during the build process, you need to activate the BuildKit which is a newer recommended back-compatible way for the engine to do the build phase. See the official docs for enabling BuildKit.\nThis can be done in 2 ways:\nSet the environment variable DOCKER_BUILDKIT=1, or\nSet it in the docker engine by default by adding \"features\": { \"buildkit\": true } to the root of the config json.\nFrom the official docs about custom build outputs:\ncustom exporters allow you to export the build artifacts as files on the local filesystem instead of a Docker image, which can be useful for generating local binaries, code generation etc.\n...\nThe local exporter writes the resulting build files to a directory on the client side. The tar exporter is similar but writes the files as a single tarball (.tar).\nIf no type is specified, the value defaults to the output directory of the local exporter.\n...\nThe --output option exports all files from the target stage. A common pattern for exporting only specific files is to do multi-stage builds and to copy the desired files to a new scratch stage with COPY --from.\ne.g. an example Dockerfile\nFROM alpine:latest AS stage1\nWORKDIR /app\nRUN echo \"hello world\" > output.txt\n\nFROM scratch AS export-stage\nCOPY --from=stage1 /app/output.txt .\nRunning\nDOCKER_BUILDKIT=1 docker build --file Dockerfile --output out .\nThe tail of the output is:\n => [export-stage 1/1] COPY --from=stage1 /app/output.txt .\n0.0s\n => exporting to client\n0.1s\n => => copying files 45B\n0.1s\nThis produces a local file out/output.txt that was created by the RUN command.\n$ cat out/output.txt\nhello world\nAll files are output from the target stage\nThe --output option will export all files from the target stage. So using a non-scratch stage with COPY --from will cause extraneous files to be copied to the output. The recommendation is to use a scratch stage with COPY --from.\nWindows is not supported for now:\nhttps://docs.docker.com/build/buildkit/",
    "How to show the full (not truncated) \"CREATED BY\" commands in \"docker history\" output?": "Use the docker history --no-trunc option to show the full command.",
    "Is Docker Compose suitable for production?": "Really you need to define \"production\" in your case.\nCompose simply starts and stops multiple containers with a single command. It doesn't add anything to the mix you couldn't do with regular docker commands.\nIf \"production\" is a single docker host, with all instances and relationships defined, then compose can do that.\nBut if instead you want multiple hosts and dynamic scaling across the cluster then you are really looking at swarm or another option.",
    "Docker parallel operations limit": "The options are set in the configuration file (Linux-based OS it is located in the path: /etc/docker/daemon.json and C:\\ProgramData\\docker\\config\\daemon.json on Windows)\nOpen /etc/docker/daemon.json (If doesn't exist, create it)\nAdd the values(for push/pulls) and set parallel operations limit\n{\n    \"max-concurrent-uploads\": 1,\n    \"max-concurrent-downloads\": 1\n}\nRestart daemon: sudo service docker restart",
    "How to push Docker containers managed by Docker-compose to Heroku?": "Just an update on this question since it seems to be getting a lot of traction lately.\nThere is now an officially supported \"Heroku.yml\" solution offered by Heroku. You can now write a .yml file (with a format similar to docker-compose) and Heroku will work out your images. Just follow the link above for details.\nHappy Heroku-ing.",
    "Run jar file in docker image": "There is a difference between images and containers.\nImages will be built ONCE\nYou can start containers from Images\nIn your case:\nChange your image:\nFROM anapsix/alpine-java\nMAINTAINER myNAME \nCOPY testprj-1.0-SNAPSHOT.jar /home/testprj-1.0-SNAPSHOT.jar\nCMD [\"java\",\"-jar\",\"/home/testprj-1.0-SNAPSHOT.jar\"]\nBuild your image:\ndocker build -t imageName .\nNow invoke your program inside a container:\ndocker run --name myProgram imageName\nNow restart your program by restarting the container:\ndocker restart myProgram\nYour program changed? Rebuild the image!:\ndocker rmi imageName\ndocker build -t imageName .",
    "What is --from used in copy command in dockerfile": "This is a multi-stage build. This is used to keep the running docker container small while still be able to build/compile things needing a lot of dependencies.\nFor example a go application could be built by using:\nFROM golang:1.7.3 AS builder\nWORKDIR /go/src/github.com/alexellis/href-counter/\nRUN go get -d -v golang.org/x/net/html  \nCOPY app.go .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\nFROM alpine:latest  \nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\nCOPY --from=builder /go/src/github.com/alexellis/href-counter/app .\nCMD [\"./app\"]  \nSo in the first part we need a complete go environment to compile our software. Notice the name for the first part and the alias builder\nFROM golang:1.7.3 AS builder\nIn the second part beginning from the second FROM we only need the compiled app and no other go dependencies anymore. So we can change the base image to using a much smaller alpine Linux. But the compiled files are still in our builder image and not part of the image we want to start. So we need to copy files from the builder image via\nCOPY --from=builder\nYou can have as many stages as you want. The last one is the one defining the image which will be the template for the docker container.\nYou can read more about it in the official documentation: https://docs.docker.com/develop/develop-images/multistage-build/",
    "Docker RabbitMQ persistency": "TL;DR\nDidn't do too much digging on this, but it appears that the simplest way to do this is to change the hostname as Pedro mentions above.\n\nMORE INFO:\nUsing RABBITMQ_NODENAME\nIf you want to edit the RABBITMQ_NODENAME variable via Docker, it looks like you need to add a hostname as well since the Docker hostnames are generated as random hashes.\nIf you change the RABBITMQ_NODENAME var to something static like my-rabbit, RabbitMQ will throw something like an \"nxdomain not found\" error because it's looking for something like\nmy-rabbit@<docker_hostname_hash>. If you know the Docker hostname and can automate pulling it into your RABBITMQ_NODENAME value like so, my-rabbit@<docker_hostname_hash> I believe it would work.\nUPDATE\nI previously said,\nIf you know the Docker hostname and can automate pulling it into your RABBITMQ_NODENAME value like so, my-rabbit@<docker_hostname_hash> I believe it would work.\nThis would not work as described precisely because the default docker host name is randomly generated at launch, if it is not assigned explicitly. The hurdle would actually be to make sure you use the EXACT SAME <docker_hostname_hash> as your originating run so that the data directory gets picked up correctly. This would be a pain to implement dynamically/robustly. It would be easiest to use an explicit hostname as described below.\nThe alternative would be to set the hostname to a value you choose -- say, app-messaging -- AND ALSO set the RABBITMQ_NODENAME var to something like rabbit@app-messaging. This way you are controlling the full node name that will be used in the data directory.\nUsing Hostname\n(Recommended)\nThat said, unless you have a reason NOT to change the hostname, changing the hostname alone is the simplest way to ensure that your data will be mounted to and from the same point every time.\nI'm using the following Docker Compose file to successfully persist my setup between launches.\nversion: '3'\nservices:\n  rabbitmq:\n    hostname: 'mabbit'\n    image: \"${ARTIFACTORY}/rabbitmq:3-management\"\n    ports:\n      - \"15672:15672\"\n      - \"5672:5672\"\n    volumes:\n      - \"./data:/var/lib/rabbitmq/mnesia/\"\n    networks:\n      - rabbitmq\n\nnetworks:\n  rabbitmq:\n    driver: bridge\nThis creates a data directory next to my compose file and persists the RabbitMQ setup like so:\n./data/\n  rabbit@mabbit/\n  rabbit@mabbit-plugins-expand/\n  rabbit@mabbit.pid\n  rabbit@mabbit-feature_flags",
    "Cache Rust dependencies with Docker build": "Seems like you are not alone in your endeavor to cache rust dependencies via the docker build process. Here is a great article that helps you along the way.\nThe gist of it is you need a dummy.rs and your Cargo.toml first, then build it to cache the dependencies and then copy your application source later in order to not invalidate the cache with every build.\nDockerfile\nFROM rust\nWORKDIR /var/www/app\nCOPY dummy.rs .\nCOPY Cargo.toml .\nRUN sed -i 's#src/main.rs#dummy.rs#' Cargo.toml\nRUN cargo build --release\nRUN sed -i 's#dummy.rs#src/main.rs#' Cargo.toml\nCOPY . .\nRUN cargo build --release\nCMD [\"target/release/app\"]\nCMD application name \"app\" is based on what you have specified in your Cargo.toml for your binary.\ndummy.rs\nfn main() {}\nCargo.toml\n[package]\nname = \"app\"\nversion = \"0.1.0\"\nauthors = [\"...\"]\n[[bin]]\nname = \"app\"\npath = \"src/main.rs\"\n\n[dependencies]\nactix-web = \"1.0.0\"\nsrc/main.rs\nextern crate actix_web;\n\nuse actix_web::{web, App, HttpServer, Responder};\n\nfn index() -> impl Responder {\n    \"Hello world\"\n}\n\nfn main() -> std::io::Result<()> {\n    HttpServer::new(|| App::new().service(web::resource(\"/\").to(index)))\n        .bind(\"0.0.0.0:8080\")?\n        .run()\n}",
    "How do I reduce a python (docker) image size using a multi-stage build?": "ok so my solution is using wheel, it lets us compile on first image, create wheel files for all dependencies and install them in the second image, without installing the compilers\nFROM python:2.7-alpine as base\n\nRUN mkdir /svc\nCOPY . /svc\nWORKDIR /svc\n\nRUN apk add --update \\\n    postgresql-dev \\\n    gcc \\\n    musl-dev \\\n    linux-headers\n\nRUN pip install wheel && pip wheel . --wheel-dir=/svc/wheels\n\nFROM python:2.7-alpine\n\nCOPY --from=base /svc /svc\n\nWORKDIR /svc\n\nRUN pip install --no-index --find-links=/svc/wheels -r requirements.txt\nYou can see my answer regarding this in the following blog post\nhttps://www.blogfoobar.com/post/2018/02/10/python-and-docker-multistage-build",
    "How to install Go in alpine linux": "I just copied it over using multi stage builds, seems to be ok so far\nFROM XXX\n \nCOPY --from=golang:1.13-alpine /usr/local/go/ /usr/local/go/\n \nENV PATH=\"/usr/local/go/bin:${PATH}\"",
    "What is \"/app\" working directory for a Dockerfile?": "There are two important directories when building a docker image:\nthe build context directory.\nthe WORKDIR directory.\nBuild context directory\nIt's the directory on the host machine where docker will get the files to build the image. It is passed to the docker build command as the last argument. (Instead of a PATH on the host machine it can be a URL). Simple example:\ndocker build -t myimage .\nHere the current dir (.) is the build context dir. In this case, docker build will use Dockerfile located in that dir. All files from that dir will be visible to docker build.\nThe build context dir is not necessarily where the Dockerfile is located. Dockerfile location defaults to current dir and is otherwise indicated by the -f otpion. Example:\ndocker build -t myimage -f ./rest-adapter/docker/Dockerfile ./rest-adapter\nHere build context dir is ./rest-adapter, a subdirectory of where you call docker build; the Dokerfile location is indicated by -f.\nWORKDIR\nIt's a directory inside your container image that can be set with the WORKDIR instruction in the Dockerfile. It is optional (default is /, but the parent image might have set it), but setting it is considered a good practice. Subsequent instructions in the Dockerfile, such as RUN, CMD and ENTRYPOINT will operate in this dir. As for COPY and ADD, they use both...\nCOPY and ADD use both dirs\nThese two commands have <src> and <dest>.\n<src> is relative to the build context directory.\n<dest> is relative to the WORKDIR directory.\nFor example, if your Dockerfile contains...\nWORKDIR /myapp\nCOPY . .\nADD data/mydata.csv /usr/share/mydata.csv\nthen...\nthe COPY command will copy the contents of your build context directory to the /myapp dir inside your docker image.\nthe ADD command will copy file data/mydata.csv under your build context dir to the /usr/share dir at the docker image filesystem root level (<dest> in this case starts with /, so it's an absolute path).",
    "How to install docker in docker container?": "I had a similar problem trying to install Docker inside a Bamboo Server image. To solve this:\nfirst remove the line: RUN docker run hello-world from your Dockerfile\nThe simplest way is to just expose the Docker socket, by bind-mounting it with the -v flag or mounting a volume using Docker Compose:\ndocker run -v /var/run/docker.sock:/var/run/docker.sock ...",
    "Build postgres docker container with initial schema": "According to the usage guide for the official PostreSQL Docker image, all you need is:\nDockerfile\nFROM postgres\nENV POSTGRES_DB my_database\nCOPY psql_dump.sql /docker-entrypoint-initdb.d/\nThe POSTGRES_DB environment variable will instruct the container to create a my_database schema on first run.\nAnd any .sql file found in the /docker-entrypoint-initdb.d/ of the container will be executed.\nIf you want to execute .sh scripts, you can also provide them in the /docker-entrypoint-initdb.d/ directory.",
    "How to unset \"ENV\" in dockerfile?": "It depends on what effect you are trying to achieve.\nNote that, as a matter of pragmatics (i.e. how developers actually speak), \"unsetting a variable\" can mean two things: removing it from the environment, or setting the variable to an empty value. Technically, these are two different operations. In practice though I have not run into a case where the software I'm trying to control differentiates between the variable being absent from the environment, and the variable being present in the environment but set to an empty value. I generally can use either method to get the same result.\nIf you don't care whether the variable is in the layers produced by Docker, but leaving it with a non-empty value causes problems in later build steps.\nFor this case, you can use ENV VAR_NAME= at the point in your Dockerfile from which you want to unset the variable. Syntactic note: Docker allows two syntaxes for ENV: this ENV VAR=1 is the same as ENV VAR 1. You can separate the variable name from the value with a space or an equal sign. When you want to \"unset\" a variable by setting it to an empty value you must use the equal sign syntax or you get an error at build time.\nSo for instance, you could do this:\nENV NOT_SENSITIVE some_value\nRUN something\n\nENV NOT_SENSITIVE=\nRUN something_else\nWhen something runs, NOT_SENSITIVE is set to some_value. When something_else runs, NOT_SENSITIVE is set to the empty string.\nIt is important to note that doing unset NOT_SENSITIVE as a shell command will not affect anything else than what executes in this shell. Here's an example:\nENV NOT_SENSITIVE some_value\nRUN unset NOT_SENSITIVE && printenv NOT_SENSITIVE || echo \"does not exist\"\n\nRUN printenv NOT_SENSITIVE\nThe first RUN will print does not exist because NOT_SENSITIVE is unset when printenv executes and because it is unset printenv returns a non-zero exit code which causes the echo to execute. The second RUN is not affected by the unset in the first RUN. It will print some_value to the screen.\nBut what if I need to remove the variable from the environment, not just set it to an empty value?\nIn this case using ENV VAR_NAME= won't work. I don't know of any way to tell Docker \"from this point on, you must remove this variable from the environment, not just set it to an empty value\".\nIf you still want to use ENV to set your variable, then you'll have to start each RUN in which you want the variable to be unset with unset VAR_NAME, which will unset it for that specific RUN only.\nIf you want to prevent the variable from being present in the layers produced by Docker.\nSuppose that variable contains a secret and the layer could fall into the hands of people who should not have the secret. In this case you CANNOT use ENV to set the variable. A variable set with ENV is baked into the layers to which it applies and cannot be removed from those layers. In particular, (assuming the variable is named SENSITIVE) running\nRUN unset SENSITIVE\ndoes not do anything to remove it from the layer. The unset command above only removes SENSITIVE from the shell process that RUN starts. It affects only that shell. It won't affect shells spawned by CMD, ENTRYPOINT, or any command provided through running docker run at the command line.\nIn order to prevent the layers from containing the secret, I would use docker build --secret= and RUN --mount=type=secret.... For instance, assuming that I've stored my secret in a file named sensitive, I could have a RUN like this:\nRUN --mount=type=secret,id=sensitive,target=/root/sensitive \\\n export SENSITIVE=$(cat /root/sensitive) \\\n && [[... do stuff that requires SENSITIVE ]] \\\nNote that the command given to RUN does not need to end with unset SENSITIVE. Due to the way processes and their environments are managed, setting SENSITIVE in the shell spawned by RUN does not have any effect beyond what that shell itself spawns. Environment changes in this shell won't affect future shells nor will it affect what Docker bakes into the layers it creates.\nThen the build can be run with:\n$ DOCKER_BUILDKIT=1 docker build --secret id=secret,src=path/to/sensitive [...]\nThe environment for the docker build command needs DOCKER_BUILDKIT=1 to use BuildKit because this method of passing secrets is only available if Docker uses BuildKit to build the images.",
    "Rails server is still running in a new opened docker container": "You are using an onbuild image, so your working direcotry is mounted in the container image. This is very good for developing, since your app is updated in realtime when you edit your code, and your host system gets updated for example when you run a migration.\nThis also means that your host system tmp directory will be written with the pid file every time a server is running and will remain there if the server is not shut down correctly.\nJust run this command from your host system:\nsudo rm tmp/pids/server.pid \nThis can be a real pain when you are for example using foreman under docker-compose, since just pressing ctrl+c will not remove the pid file.",
    "Multiline comments in Dockerfiles": "As of today, no.\nAccording to Dockerfile reference documentation:\nDocker treats lines that begin with # as a comment, unless the line is a valid parser directive. A # marker anywhere else in a line is treated as an argument.:\nThere is no further details on how to comment lines.\nAs said by some comments already, most IDE will allow you to perform multiline comments easily (such as CTRL + / on IntelliJ)",
    "Dockerfile strategies for Git": "From Ryan Baumann's blog post \u201cGit strategies for Docker\u201d\nThere are different strategies for getting your Git source code into a Docker build. Many of these have different ways of interacting with Docker\u2019s caching mechanisms, and may be more or less appropriately suited to your project and how you intend to use Docker.\nRUN git clone\nIf you\u2019re like me, this is the approach that first springs to mind when you see the commands available to you in a Dockerfile. The trouble with this is that it can interact in several unintuitive ways with Docker\u2019s build caching mechanisms. For example, if you make an update to your git repository, and then re-run the docker build which has a RUN git clone command, you may or may not get the new commit(s) depending on if the preceding Dockerfile commands have invalidated the cache.\nOne way to get around this is to use docker build --no-cache, but then if there are any time-intensive commands preceding the clone they\u2019ll have to run again too.\nAnother issue is that you (or someone you\u2019ve distributed your Dockerfile to) may unexpectedly come back to a broken build later on when the upstream git repository updates.\nA two-birds-one-stone approach to this while still using RUN git clone is to put it on one line1 with a specific revision checkout, e.g.:\nRUN git clone https://github.com/example/example.git && cd example && git checkout 0123abcdef\nThen updating the revision to check out in the Dockerfile will invalidate the cache at that line and cause the clone/checkout to run.\nOne possible drawback to this approach in general is that you have to have git installed in your container.\nRUN curl or ADD a tag/commit tarball URL\nThis avoids having to have git installed in your container environment, and can benefit from being explicit about when the cache will break (i.e. if the tag/revision is part of the URL, that URL change will bust the cache). Note that if you use the Dockerfile ADD command to copy from a remote URL, the file will be downloaded every time you run the build, and the HTTP Last-Modified header will also be used to invalidate the cache.\nYou can see this approach used in the golang Dockerfile.\nGit submodules inside Dockerfile repository\nIf you keep your Dockerfile and Docker build in a separate repository from your source code, or your Docker build requires multiple source repositories, using git submodules (or git subtrees) in this repository may be a valid way to get your source repos into your build context. This avoids some concerns with Docker caching and upstream updating, as you lock the upstream revision in your submodule/subtree specification. Updating them will break your Docker cache as it changes the build context.\nNote that this only gets the files into your Docker build context, you still need to use ADD commands in your Dockerfile to copy those paths to where you expect them in the container.\nYou can see this approach used in the here\nDockerfile inside git repository\nHere, you just have your Dockerfile in the same git repository alongside the code you want to build/test/deploy, so it automatically gets sent as part of the build context, so you can e.g. ADD . /project to copy the context into the container. The advantage to this is that you can test changes without having to potentially commit/push them to get them into a test docker build; the disadvantage is that every time you modify any files in your working directory it will invalidate the cache at the ADD command. Sending the build context for a large source/data directory can also be time-consuming. So if you use this approach, you may also want to make judicious use of the .dockerignore file, including doing things like ignoring everything in your .gitignore and possibly the .git directory itself.\nVolume mapping\nIf you\u2019re using Docker to set up a dev/test environment that you want to share among a wide variety of source repos on your host machine, mounting a host directory as a data volume may be a viable strategy. This gives you the ability to specify which directories you want to include at docker run-time, and avoids concerns about docker build caching, but none of this will be shared among other users of your Dockerfile or container image.",
    "How to start apache2 automatically in a ubuntu docker container?": "The issue is here: CMD service apache2 start When you execute this command process apache2 will be detached from the shell. But Docker works only while main process is alive.\nThe solution is to run Apache in the foreground. Dockerfile must look like this: (only last line changed).\nFROM ubuntu\n\n# File Author / Maintainer\nMAINTAINER rmuktader\n\n# Update the repository sources list\nRUN apt-get update\n\n# Install and run apache\nRUN apt-get install -y apache2 && apt-get clean\n\n#ENTRYPOINT [\"/usr/sbin/apache2\", \"-k\", \"start\"]\n\n\n#ENV APACHE_RUN_USER www-data\n#ENV APACHE_RUN_GROUP www-data\n#ENV APACHE_LOG_DIR /var/log/apache2\n\nEXPOSE 80\nCMD apachectl -D FOREGROUND",
    "docker-compose change name of main container": "When refering to your main container, you are probably refering to the project name, which you could usually set via the -p flag. (See other answers)\nFor docker-compose, you can set the top level variable name to your desired project name.\ndocker-compose.yml file:\nversion: \"3.9\"\nname: my-project-name\nservices:\n  myService:\n    ...\nIf you are using Docker Desktop, make sure Use Docker Compose V2 is enabled there.",
    "Docker-compose volume mount before run": "Erik Dannenberg's is correct, the volume layering means that what I was trying to do makes no sense. (There is another really good explaination on the Docker website if you want to read more). If I want to have Docker do the npm install then I could do it like this:\nFROM node\n\nADD . /usr/src/app\nWORKDIR /usr/src/app\n\nRUN npm install --global gulp-cli \\\n && npm install\n\nCMD [\"gulp\", \"watch\"]\nHowever, this isn't appropriate as a solution for my situation. The goal is to use NPM to install project dependencies, then run gulp to build my project. This means I need read and write access to the project folder and it needs to persist after the container is gone.\nI need to do two things after the volume is mounted, so I came up with the following solution...\ndocker/gulp/Dockerfile:\nFROM node\n\nRUN npm install --global gulp-cli\n\nADD start-gulp.sh .\n\nCMD ./start-gulp.sh\ndocker/gulp/start-gulp.sh:\n#!/usr/bin/env bash\n\nuntil cd /usr/src/app && npm install\ndo\n    echo \"Retrying npm install\"\ndone\ngulp watch\ndocker-compose.yml:\nversion: '2'\n\nservices:\n  build_tools:\n    build: docker/gulp\n    volumes_from:\n      - build_data:rw\n\n  build_data:\n    image: debian:jessie\n    volumes:\n      - .:/usr/src/app\nSo now the container starts a bash script that will continuously loop until it can get into the directory and run npm install. This is still quite brittle, but it works. :)",
    "Benefits of repeated apt cache cleans": "The main reason people do this is to minimise the amount of data stored in that particular docker layer. When pulling a docker image, you have to pull the entire content of the layer.\nFor example, imagine the following two layers in the image:\nRUN apt-get update\nRUN rm -rf /var/lib/apt/lists/*\nThe first RUN command results in a layer containing the lists, which will ALWAYS be pulled by anyone using your image, even though the next command removes those files (so they're not accessible). Ultimately those extra files are just a waste of space and time.\nOn the other hand,\nRUN apt-get update && rm -rf /var/lib/apt/lists/*\nDoing it within a single layer, those lists are deleted before the layer is finished, so they are never pushed or pulled as part of the image.\nSo, why have multiple layers which use apt-get install? This is likely so that people can make better use of layers in other images, as Docker will share layers between images if they're identical in order to save space on the server and speed up builds and pulls.",
    "How to enable/disable buildkit in docker?": "You must adjust the Docker Engine's daemon settings, stored in the daemon.json, and restart the engine. As @Zeitounator suggests, you should be able to temporarily disable the buildkit with DOCKER_BUILDKIT=0 docker build .. Docker CLI will parse that environment variable and should honor it as that checking is done here in the docker/cli source code.\nTo adjust the Docker daemon's buildkit settings, you can follow the instructions below.\nFrom these docs. Partially on the command line, you can do that this way in Powershell:\nOpen the file, on the command line the easiest way to do this is:\nnotepad \"$env:USERPROFILE\\.docker\\daemon.json\"\nChange the value of \"buildkit\" to false so it looks like this:\n{\n  \"registry-mirrors\": [],\n  \"insecure-registries\": [],\n  \"debug\": true,\n  \"experimental\": false,\n  \"features\": {\n    \"buildkit\": false\n  }\n}\nRestart the Docker service:\nRestart-Service *docker*\nAlternatively, on Docker Desktop for Windows app:\nOpen the Dashboard > Settings:\nSelect Docker Engine and edit the json \"features\" field to read false if it's not already:",
    "How to build a docker container for a Java application": "The docker registry hub has a Maven image that can be used to create java containers.\nUsing this approach the build machine does not need to have either Java or Maven pre-installed, Docker controls the entire build process.\nExample\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 pom.xml\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 main\n    \u2502   \u251c\u2500\u2500 java\n    \u2502   \u2502   \u2514\u2500\u2500 org\n    \u2502   \u2502       \u2514\u2500\u2500 demo\n    \u2502   \u2502           \u2514\u2500\u2500 App.java\n    \u2502   \u2514\u2500\u2500 resources\n    \u2502       \u2514\u2500\u2500 log4j.properties\n    \u2514\u2500\u2500 test\n        \u2514\u2500\u2500 java\n            \u2514\u2500\u2500 org\n                \u2514\u2500\u2500 demo\n                    \u2514\u2500\u2500 AppTest.java\nImage is built as follows:\ndocker build -t my-maven .\nAnd run as follows:\n$ docker run -it --rm my-maven\n0    [main] INFO  org.demo.App  - hello world\nDockerfile\nFROM maven:3.3-jdk-8-onbuild\nCMD [\"java\",\"-jar\",\"/usr/src/app/target/demo-1.0-SNAPSHOT-jar-with-dependencies.jar\"]\nUpdate\nIf you wanted to optimize your image to exclude the source you could create a Dockerfile that only includes the built jar:\nFROM java:8\nADD target/demo-1.0-SNAPSHOT-jar-with-dependencies.jar /opt/demo/demo-1.0-SNAPSHOT-jar-with-dependencies.jar\nCMD [\"java\",\"-jar\",\"/opt/demo/demo-1.0-SNAPSHOT-jar-with-dependencies.jar\"]\nAnd build the image in two steps:\ndocker run -it --rm -w /opt/maven \\\n   -v $PWD:/opt/maven \\\n   -v $HOME/.m2:/root/.m2 \\\n   maven:3.3-jdk-8 \\\n   mvn clean install\n\ndocker build -t my-app .\n__\nUpdate (2017-07-27)\nDocker now has a multi-stage build capability. This enables Docker to build an image containing the build tools but only the runtime dependencies.\nThe following example demonstrates this concept, note how the jar is copied from target directory of the first build phase\nFROM maven:3.3-jdk-8-onbuild \n\nFROM java:8\nCOPY --from=0 /usr/src/app/target/demo-1.0-SNAPSHOT.jar /opt/demo.jar\nCMD [\"java\",\"-jar\",\"/opt/demo.jar\"]",
    "`docker pull` returns `denied: access forbidden` from private gitlab registry": "If this is an authenticated registry, then you need to run docker login <registryurl> on the machine where you are building this.\nThis only needs to be done once per host. The command then caches the auth in a file\n$ cat ~/.docker/config.json\n{\n    \"auths\": {\n        \"https://index.docker.io/v1/\": {\n            \"auth\": \"......=\"\n        }\n    }\n}",
    "How to install packages with miniconda in Dockerfile?": "This will work using ARG and ENV:\nFROM ubuntu:18.04\n\nENV PATH=\"/root/miniconda3/bin:${PATH}\"\nARG PATH=\"/root/miniconda3/bin:${PATH}\"\n\n# Install wget to fetch Miniconda\nRUN apt-get update && \\\n    apt-get install -y wget && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Install Miniconda on x86 or ARM platforms\nRUN arch=$(uname -m) && \\\n    if [ \"$arch\" = \"x86_64\" ]; then \\\n    MINICONDA_URL=\"https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\"; \\\n    elif [ \"$arch\" = \"aarch64\" ]; then \\\n    MINICONDA_URL=\"https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh\"; \\\n    else \\\n    echo \"Unsupported architecture: $arch\"; \\\n    exit 1; \\\n    fi && \\\n    wget $MINICONDA_URL -O miniconda.sh && \\\n    mkdir -p /root/.conda && \\\n    bash miniconda.sh -b -p /root/miniconda3 && \\\n    rm -f miniconda.sh\n\nRUN conda --version",
    "Docker build: failed to fetch oauth token for openjdk?": "It looks like you have BuildKit enabled in your docker configuration. BuildKit can cause these type of problems. Please try it again with BuildKit disabled.\nIn Linux, using environment variables:\nexport DOCKER_BUILDKIT=0\nexport COMPOSE_DOCKER_CLI_BUILD=0\nIn Windows and macOS, start the Docker Desktop application, go to Settings, select Docker Engine and look for the existing entry:\n\"buildkit\": true\nChange this entry to disable buildkit:\n\"buildkit\": false\nThen click on Apply & Restart and try it again.",
    "Error: \"error creating aufs mount to\" when building dockerfile": "I had some unresolved errors after removing /var/lib/docker/aufs, which a couple extra steps cleared up.\nTo add to @benwalther answer, since I lack the reputation to comment:\n# Cleaning up through docker avoids these errors\n#   ERROR: Service 'master' failed to build:\n#     open /var/lib/docker/aufs/layers/<container_id>: no such file or directory\n#   ERROR: Service 'master' failed to build: failed to register layer:\n#     open /var/lib/docker/aufs/layers/<container_id>: no such file or directory\ndocker rm -f $(docker ps -a -q)\ndocker rmi -f $(docker images -a -q)\n\n# As per @BenWalther's answer above\nsudo service docker stop\nsudo rm -rf /var/lib/docker/aufs\n\n# Removing the linkgraph.db fixed this error:\n#   Conflict. The name \"/jenkins_data_1\" is already in use by container <container_id>.\n#   You have to remove (or rename) that container to be able to reuse that name.\nsudo rm -f /var/lib/docker/linkgraph.db\n\n\nsudo service docker start",
    "/bin/sh: 1: sudo: not found when running dockerfile": "by default docker container runs as root user\nremove the sudo from Dockerfile and run again.",
    "Syntax highlighting for Dockerfile in Sublime Text?": "Of course you can, by installing this package from Package Control:\nDockerfile Syntax Highlighting, https://packagecontrol.io/packages/Dockerfile%20Syntax%20Highlighting",
    "Can I run an intermediate layer of a Docker image?": "You can run an intermediate image of a Docker layer, which is probably what you want.\nDuring your build you might need to inspect an image at certain point (step) in the build process e.g. in your Docker build output you'll see:\nStep 17/30 : RUN rm -rf /some/directory\n---> Running in 5ab963f2b48d\nWhere 5ab963f2b48d is an image ID, and when you list your images, you'd see that ID in the list e.g.\n$ docker image ls --all\nREPOSITORY    TAG      IMAGE ID       CREATED        SIZE\n<none>        <none>   5ab963f2b48d   7 minutes ago  1.18GB\nTo run that image (with a terminal) for example, you can simply:\ndocker run -i -t 5ab963f2b48d /bin/bash\nAlso see: Run a Docker Image as a Container",
    "Dockerfile and docker-compose not updating with new instructions": "It seems that when using the docker-compose command it saves an intermediate container that it doesnt show you and constantly reruns that never updating it correctly. Sadly the documentation regarding something like this is poor. The way to fix this is to build it first with no cache and then up it like so\ndocker-compose build --no-cache\ndocker-compose up -d",
    "Cannot \"pip install cryptography\" in Docker Alpine Linux 3.3 with OpenSSL 1.0.2g and Python 2.7": "For those who are still experiencing problems installing cryptography==2.1.4 in Alpine 3.7 like this:\nwriting manifest file 'src/cryptography.egg-info/SOURCES.txt'\nrunning build_ext\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_padding.c'\ncreating build/temp.linux-x86_64-2.7\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_constant_time.c'\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_openssl.c'\nbuilding '_openssl' extension\ncreating build/temp.linux-x86_64-2.7/build\ncreating build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7\ngcc -fno-strict-aliasing -Os -fomit-frame-pointer -g -DNDEBUG -Os -fomit-frame-pointer -g -DTHREAD_STACK_SIZE=0x100000 -fPIC -I/usr/include/python2.7 -c build/temp.linux-x86_64-2.7/_openssl.c -o build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7/_openssl.o -Wconversion -Wno-error=sign-conversion\nbuild/temp.linux-x86_64-2.7/_openssl.c:493:30: fatal error: openssl/opensslv.h: No such file or directory\n #include <openssl/opensslv.h>\n                              ^\ncompilation terminated.\nerror: command 'gcc' failed with exit status 1\nSolution\nInstall these dependencies in the Alpine container:\n$ apk add --no-cache libressl-dev musl-dev libffi-dev\nTo install these dependencies using a Dockerfile:\nRUN apk add --no-cache \\\n        libressl-dev \\\n        musl-dev \\\n        libffi-dev && \\\n    pip install --no-cache-dir cryptography==2.1.4 && \\\n    apk del \\\n        libressl-dev \\\n        musl-dev \\\n        libffi-dev\nReference\nInstallation instructions for cryptography on Alpine can be found here:\nhttps://cryptography.io/en/latest/installation/#building-cryptography-on-linux\nA version from the time of writing is available on github\nHere is the relevant portion:\nBuilding cryptography on Linux\n[skipping over the part for non-Alpine Linux] \u2026\n$ pip install cryptography\nIf you are on Alpine or just want to compile it yourself then cryptography requires a compiler, headers for Python (if you're not using pypy), and headers for the OpenSSL and libffi libraries available on your system.\nAlpine\nReplace python3-dev with python-dev if you're using Python 2.\n$ sudo apk add gcc musl-dev python3-dev libffi-dev openssl-dev\nIf you get an error with openssl-dev you may have to use libressl-dev.",
    "How can I install lxml in docker": "I added RUN apk add --update --no-cache g++ gcc libxslt-dev before RUN pip install -r requirements.txt and it worked.",
    "Multiple images, one Dockerfile": "You can use a docker-compose file using the target option:\nversion: '3.4'\nservices:\n  img1:\n    build:\n      context: .\n      target: img1\n  img2:\n    build:\n      context: .\n      target: img2\nusing your Dockerfile with the following content:\nFROM alpine as img1\nCOPY file1.txt .\n\nFROM alpine as img2\nCOPY file2.txt .",
    "SIGTERM not received by java process using 'docker stop' and the official java image": "Assuming you launch a Java service by defining the following in your Dockerfile:\nCMD java -jar ...\nWhen you now enter the container and list the processes e.g. by docker exec -it <containerName> ps AHf (I did not try that with the java but with the ubuntu image) you see that your Java process is not the root process (not the process with PID 1) but a child process of a /bin/sh process:\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 18:27 ?        00:00:00 /bin/sh -c java -jar ...\nroot         8     1  0 18:27 ?        00:00:00   java -jar ...\nSo basically you have a Linux shell that is the main process with PID 1 which has a child process (Java) with PID 8.\nTo get signal handling working properly you should avoid those shell parent process. That can be done by using the builtin shell command exec. That will make the child process taking over the parent process. So at the end the former parent process does not exist any more. And the child process becomes the process with the PID 1. Try the following in your Dockerfile:\nCMD exec java -jar ...\nThe process listing then should show something like:\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 18:30 ?        00:00:00 java -jar ...\nNow you only have that one process with PID 1. Generally a good practice is to have docker containers only contain one process - the one with PID 1 (or if you really need more processes then you should use e.g. supervisord as PID 1 which itself takes care of signal handling for its child processes).\nWith that setup the SIGTERM will be treated directly by the Java process. There is no shell process any more in between which could break signal handling.\nEDIT:\nThe same exec effect could be achieved by using a different CMD syntax that does it implicitly (thanks to Andy for his comment):\nCMD [\"java\", \"-jar\", \"...\"]",
    "Docker: Using COPY when the Dockerfile is located in a subdirectory": "All you need to do here is add context: . and dockerfile in your build section inside your docker-compose.yml file so that your service understands the complete directory structure.\n# docker-compose.yml\nversion: \"3\"\nservices:\n  webserver:\n    build:\n      context: .\n      dockerfile: ./dockerfiles/webserver/Dockerfile\n    image: webserver:php-apache",
    "How can I start spring boot application in docker with profile?": "We have 3 ways:\n1. Passing Spring Profile in a Dockerfile\nFROM openjdk:8-jre-alpine\n...\nENTRYPOINT [\"java\", \"-Djava.security.egd=file:/dev/./urandom\",\"-Dspring.profiles.active=test\",\"-jar\",\"app.jar\"]\n2. Passing Spring Profile in Docker run\ndocker run -d -p 8080:8080 -e \"SPRING_PROFILES_ACTIVE=test\" --name my-app:latest\n3. Passing Spring Profile in DockerCompose\nversion: \"3.5\"\nservices:\n  my-app:\n     image: my-app:latest\n     ports:\n       - \"8080:8080\" \n     environment:\n       - \"SPRING_PROFILES_ACTIVE=test\"",
    "Error in anyjson setup command: use_2to3 is invalid": "Downgrading setuptools worked for me\npip install \"setuptools<58.0.0\"\nAnd then\npip install django-celery",
    "How to specify working directory for ENTRYPOINT in Dockerfile": "WORKDIR /App is a command you can use in your dockerfile to change the working directory.",
    "Why is ARG in a DOCKERFILE not recommended for passing secrets?": "Update August 2018:\nYou now have docker build --secret id=mysecret,src=/secret/file.\nSee \"safe way to use build-time argument in Docker\".\nUpdate January 2017:\nDocker (swarm) 1.13 has docker secret.\nHowever, as commented by Steve Hoffman (bacoboy):\n[...]The secret command only helps swarm users is not a more general solution (like they did with attaching persistent volumes).\nHow you manage your secrets (what they are and who has access to them) is very system dependent and depends on which bits of paid and/or OSS you cobble together to make your \"platform\".\nWith Docker the company moving into providing a platform, I'm not surprised that their first implementation is swarm based just as Hashicorp is integrating Vault into Atlas -- it makes sense.\nReally how the secrets are passed falls outside the space of docker run.\nAWS does this kind of thing with roles and policies to grant/deny permissions plus an SDK.\nChef does it using encrypted databags and crypto \"bootstrapping\" to auth.\nK8S has their own version of what just got released in 1.13.\nI'm sure mesos will add a similar implementation in time.\nThese implementations seem to fall into 2 camps.\npass the secret via volume mount that the \"platform\" provides or (chef/docker secret/k8s\npass credentials to talk to an external service to get things at boot (iam/credstash/etc)\nOriginal answer: Nov. 2015\nThis was introduced in commit 54240f8 (docker 1.9, Nov 2015), from PR 15182,\nThe build environment is prepended to the intermediate continer's command string for aiding cache lookups.\nIt also helps with build traceability. But this also makes the feature less secure from point of view of passing build time secrets.\nissue 13490 reiterates:\nBuild-time environment variables: The build-time environment variables were not designed to handle secrets. By lack of other options, people are planning to use them for this. To prevent giving the impression that they are suitable for secrets, it's been decided to deliberately not encrypt those variables in the process.\nAs mentioned in 9176 comments:\nenv variables are the wrong way to pass secrets around. We shouldn't be trying to reinvent the wheel and provide a crippled security distribution mechanism right out of the box.\nWhen you store your secret keys in the environment, you are prone to accidentally expose them -- exactly what we want to avoid:\nGiven that the environment is implicitly available to the process, it's incredibly hard, if not impossible, to track access and how the contents get exposed\nIt is incredibly common having applications grabbing the whole environment and print it out, since it can be useful for debugging, or even send it as part of an error report. So many secrets get leaked to PagerDuty that they have a well-greased internal process to scrub them from their infrastructure.\nEnvironment variables are passed down to child processes, which allows unintended access and breaks the principle of least privilege. Imagine that as part of your application you call to third-party tool to perform some action, all of a sudden that third-party tool has access to your environment, and god knows what it will do with it.\nIt is very common for applications that crash to store the environment variables in log-files for later debugging. This means secrets in plain-text on disk.\nPutting secrets in env variables quickly turns into tribal knowledge. New engineers don't know they are there, and are not aware they should be careful when handling environment variables (filtering them to sub-processes, etc).\nOverall, secrets in env variables break the principle of least surprise, are a bad practice and will lead to the eventual leak of secrets.",
    "Dockerfile FROM --platform option": "update your docker file, you are missing =\nARG arch\nFROM --platform=linux/${arch} bounz/hgbe.base",
    "Difference between VOLUME declaration in Dockerfile and -v as docker run parameter": "The -v parameter and VOLUME keyword are almost the same. You can use -v to have the same behavior as VOLUME.\ndocker run -v /data\nSame as\nVOLUME /data\nBut also -v have more uses, one of them is where map to the volume:\ndocker run -v data:/data # Named volumes\ndocker run -v /var/data:/data # Host mounted volumes, this is what you refer to -v use, but as you can see there are more uses,\nSo the question is: what is the use of VOLUME in a Dockerfile?\nThe container filesystem is made of layers so writing there, is slower and limited (because the fixed number of layers) than the plain filesystem.\nYou declare VOLUME in your Dockerfile to denote where your container will write application data. For example a database container, its data will go in a volume regardless what you put in your docker run.\nIf you create a docker container for JBoss and you want to use fast filesystem access with libaio yo need to declare the data directory as a VOLUME or JBoss will crash on startup.\nIn summary VOLUME declares a volume regardless what you do in docker run. In fact in docker run you cannot undo a VOLUME declaration made in Dockerfile.\nRegards",
    "Docker image layer: What does `ADD file:<some_hash> in /` mean?": "That Docker Hub history view doesn't show the actual Dockerfile; instead, it shows content essentially extracted from the docker history of the image. That doesn't preserve the specific details you're looking for: it doesn't remember the names of base images, or the build-context file names of things that get ADDed or COPYed in.\nChasing through GitHub and Docker Hub links, the golang:*-buster Dockerfile is built FROM buildpack-deps:...-scm; buildpack-deps:buster-scm is FROM buildpack-deps:buster-curl; that is FROM debian:buster; and that has a very simple Dockerfile (quoted here in its entirety):\nFROM scratch\nADD rootfs.tar.xz /\nCMD [\"bash\"]\nFROM scratch starts from a completely totally empty image; that is the base of the Docker image tree (and what tells docker history and similar tools to stop). The ADD line unpacks a tar file of a Debian system image.\nIf you look at docker history or the Docker Hub history view you cite, you should be able to see these same steps happening. The ADD file:4b0... in / corresponds to the ADD rootfs.tar.gz /, and the second line is the CMD [\"bash\"]. It is not split up by Dockerfile or image, and the original filenames from ADD aren't saved. (You couldn't reproduce the image anyways without the contents of the rootfs.tar.gz, so it's merely slightly helpful to know its filename but not essential.)\nThe ADD file:hash in /path syntax is not standard Dockerfile syntax (the word in in particular is not part of it). I'm not sure there's a reliable way to translate from the host file or URL to the hash, but building the image and looking at its docker history would tell you (assuming you've got a perfect match for the file metadata). There's no way to get back to the original filename or syntax, and definitely no way to get back to the file contents.",
    "Container command '/start.sh' not found or does not exist, entrypoint to container is shell script": "On windows, while building the docker image, i also used to get the same error after building the image, that shell script is missing.. the path and the shebang was also correct.\nLater on, i read some where that it may be due to the encoding issue. I just opened the file in sublime editor and then..VIEW->Line Endings-> UNIX and just save the file, and rebuilded the image. Everything worked fine.\nI was getting this error, when i was building a image from windows.\nOther Option:\nSometime, we forgot to manually change the line format. So,what we can do is add this Run statement before the EntryPoint in dockerfile. It will encode the file in LF format.\n RUN sed -i 's/\\r$//' $app/filename.sh  && \\  \n        chmod +x $app/filename.sh\n\nENTRYPOINT $app/filename.sh",
    "Redis Docker connection refused": "You need to provide more information about your environment (OS, Docker installation, etc), but basically, if you start your Redis container like this:\ndocker run --name=redis-devel --publish=6379:6379 --hostname=redis --restart=on-failure --detach redis:latest\nIt should expose the port no matter what. The only reason you might not be able to connect to it, is if you've messed up your bridge interface, if you're on Linux, or you're using a docker machine with its own network interface and IP address and you're not connecting to that IP address. If you're using Docker for Mac, then that only supports routing to the localhost address, since bridging on Mac hosts doesn't work yet.\nAnyway, on MacOS with Docker for Mac (not the old Docker Toolbox), the following should be enough to get your started:\n\u279c  ~ docker run --name=redis-devel --publish=6379:6379 --hostname=redis --restart=on-failure --detach redis:latest\n6bfc6250cc505f82b56a405c44791f193ec5b53469f1625b289ef8a5d7d3b61e\n\u279c  ~ docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES\n6bfc6250cc50        redis:latest        \"docker-entrypoint.s\u2026\"   10 minutes ago      Up 10 minutes       0.0.0.0:6379->6379/tcp   redis-devel\n\u279c  ~ redis-cli ping\nPONG\n\u279c  ~ ",
    "How to fill user input for interactive command for \"RUN\" command?": "Did you try to disable it?\nFROM ubuntu:19.04\n\nENV DEBIAN_FRONTEND noninteractive\n\nRUN apt update && apt install -y tcl",
    "What is the purpose of the Docker build context?": "TL;DR: \"because the client and daemon may not even run on the same machine\"\nThe docker command is the docker client of the dockerd that is the service that can run directly in your PC (linux) or under a Linux VM under OSX or Windows.\nQ: What is the purpose of the Docker build context?\nFrom here:\nWould probably be good to also mention that this has to happen this way because the client and daemon may not even run on the same machine, so without this \"context\" the daemon machine wouldn't have any other way to get files for ADD or otherwise\nQ: If the build process compresses the current directory contents and sends it to the daemon, where does it go?\nThe docker daemon receives the compressed directory and process it on the fly; does not matter where it is stored in that moment.\nQ: Why doesn't it make that content available for use in the image?\nThink this: How can docker know where do you want to put each file/directory in the target image? With COPY/ADD directives you can control where put each one. The case that you've mentioned is only a trivial example where you have a single directory and a single target.",
    "What does working_dir tag mean in a docker-compose yml file": "working_dir sets the working directory of the container that is created. It is the same as the --workdir flag to docker run.",
    "Load Postgres dump after docker-compose up": "Reading https://hub.docker.com/_/postgres/, the section 'Extend this image' explains that any .sql in /docker-entrypoint-initdb.d will be executed after build.\nI just needed to change my Dockerfile.db to:\nFROM postgres\n\nADD ./devops/db/dummy_dump.sql /docker-entrypoint-initdb.d\nAnd it works!",
    "How to build Docker Images with Dockerfile behind HTTP_PROXY by Jenkins?": "Note: Docker 1.9 might help solve this:\n\"Issue 14634\": Builder - Build-time argument passing (e.g., HTTP_PROXY)\n\"PR 15182\": Support for passing build-time variables in build context\nUsage (proposed):\ndocker build --build-arg http_proxy=http://my.proxy.url  --build-arg foo=bar <<MARK\nFROM busybox\nRUN <command that need http_proxy>\nARG --description=\"foo's description\" foo\nUSER $foo\nMARK",
    "How to run my python script on docker?": "Going by question title, and if one doesn't want to create docker image but just want to run a script using standard python docker images, it can run using below command\ndocker run -it --rm --name my-running-script -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp python:3.7-alpine python script_to_run.py",
    "Dockerfile define multiple ARG arguments in a single line": "YES, After release 1.2.0\nPreviously it was not possible and it threw the following error\nARG requires exactly one argument definition\nBut after release 1.2.0 it is now possible to define ARG like following\nARG CDN_ENDPOINT \\\nAWS_S3_BUCKET",
    "What is the most light weight base image I can use to build a Dockerfile?": "This really depends on your requirements:\nFROM scratch: if you are able to statically compile your application and don't need any other binaries (libraries, shells, or any other command period), then you can use the completely empty \"scratch\". You'll see this used as the starting point for the other base images, and it's also found in a lot of pre-compiled Go commands.\nDistroless: these images are built for a handful of use cases, and ship without a package manager or even shell (excluding their developer images). If you fit in their specific use case, these can be very small, but like with scratch images, difficult to debug.\nBusybox: I consider this less of a base image and more of a convenient utility container. You get a lot of common commands in a very small size. Busybox is a single binary with various commands linked to it, and that binary implements each of the commands depending on the CLI. What you don't get is the general package manager to easily install other components.\nAlpine: This is a minimal distribution, based on busybox, but with the apk package manager. The small size comes at a cost, things like glibc are not included, preferring the musl libc implementation instead. You will find that many of the official images are based on Alpine, so inside of the container ecosystem, this is a very popular option.\nDebian, Ubuntu, and CentOS: These are less of the lightweight base images. But what they lose with size they gain with a large collection of packages you can pull from and lots of people that are testing, fixing bugs, and contributing to things upstream. They also come with a collection of libraries that some applications may expect to be preinstalled.\nWhile that last option is a bit larger, keep in mind that base images should only be pushed over the wire and stored on disk once. After that, unless you change them, any images built on top of them only need to send the manifest that references layers in that base image and the docker engine will see that it already has those layers downloaded. And with the union fs, those layers never need to be copied even if you run 100 containers all pointing back to that image, they each use the same read-only layer on disk for all the image layers and write their changes to the their container specific RW layer.\nIf you find yourself installing a set of common tools on many of your images, the better option is to build your own base image, extending an upstream base image with your common tools. That way those tools only get packaged into a layer once and reused by multiple images.",
    "SSH agent forwarding during docker build": "For Docker 18.09 and newer\nYou can use new features of Docker to forward your existing SSH agent connection or a key to the builder. This enables for example to clone your private repositories during build.\nSteps:\nFirst set environment variable to use new BuildKit\nexport DOCKER_BUILDKIT=1\nThen create Dockerfile with new (experimental) syntax:\n# syntax=docker/dockerfile:experimental\n\nFROM alpine\n\n# install ssh client and git\nRUN apk add --no-cache openssh-client git\n\n# download public key for github.com\nRUN mkdir -p -m 0600 ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts\n\n# clone our private repository\nRUN --mount=type=ssh git clone git@github.com:myorg/myproject.git myproject\nAnd build image with\ndocker build --ssh default .\nRead more about it here: https://medium.com/@tonistiigi/build-secrets-and-ssh-forwarding-in-docker-18-09-ae8161d066",
    "How do I declare multiple maintainers in my Dockerfile?": "You can only specify one MAINTAINER instruction in a Dockerfile.\nFurthermore, MAINTAINER will be deprecated in the upcoming 1.13.0 release, see deprecations and this pull request.\nThe recommended solution is to use LABEL instead, e.g.\nLABEL authors=\"first author,second author\"\nLabels have a key=value syntax. This means you cannot assign the same label more than once and you cannot assign multiple values to a given label. But you can combine multiple values into one with a syntax of your choice as illustrated in the example..",
    "where should I put docker-compose.yml": "I'm asking myself the same question: Where to put my docker-compose.yml file?\nI've decided to do it the following way:\nOne Repo for the docker-compose.yml along with deployment scripts (Jenkins pipeline in my case).\nOne Repo per micro service along with a Dockerfile and its build logic (built image is pushed to private docker registry).\nWhy?\nThe docker-compose.yml describes a system (the composition of micro services) and how it is deployed.\nA micro services should be independent from the system infrastructure.\nThere could be more than one system (different composition of the micro services) with a second docker-compose.yml (and Repo).\nHowever, of course there are reasonable exceptions. Assume you're deploying a customized Database along with a management tool (e.g. customized mariadb and adminer), then most things may live in one repository.",
    "installing `lightdm` in Dockerfile raises interactive keyboard layout menu": "This should work:\nDEBIAN_FRONTEND=noninteractive apt-get install lightdm -y\nI had the same problem installing CUDA and this fixed it.",
    "Docker and .bash_history": "It is the example from the documentation about volume: Mount a host file as a data volume:\ndocker run --rm -it -v ~/.bash_history:/root/.bash_history ubuntu /bin/bash\nThis will drop you into a bash shell in a new container, you will have your bash history from the host and when you exit the container, the host will have the history of the commands typed while in the container.",
    "\"Empty continuation lines will become errors\"\u2026 how should I comment my Dockerfile now?": "On top of what others have said above (the error might be related to comments inside continuation blocks and/or windows cr/lf characters = use dos2unix), this message can also show up when your last command ends with a backslash \\ character. For example, if you have this:\nRUN apt-get update \\\n    && apt-get upgrade \\\n    && apt-get -y install build-essential curl gnupg libfontconfig ca-certificates bzip2 \\\n    && curl -sL https://deb.nodesource.com/setup_16.x  | bash - \\\n    && apt-get -y install nodejs \\\n    && apt-get clean \\\n    && rm -rf /tmp/* /var/lib/apt/lists/* \\\nNotice the last \\ at the end. This will get you the same error:\ndocker [WARNING]: Empty continuation line found in:\nSo, just remove that last \\ and you're all set.",
    "Command line arguments to Docker CMD": "Use ENTRYPOINT for stuff like this. Any CMD parameters are appended to the given ENTRYPOINT.\nSo, if you update the Dockerfile to:\nFROM ubuntu:15.04\nENTRYPOINT [\"/bin/bash\", \"-c\", \"cat\"]\nThings should work as you wish.\nAlso, as you don't need the $1, you should be able to change it to:\nFROM ubuntu:15.04\nENTRYPOINT [\"/bin/cat\"]\nI haven't tested any of this, so let me know if it doesn't work.",
    "How to tag an image in a Dockerfile? [duplicate]": "Unfortunately it is not possible. You can use build.sh script, which contains like this:\n#!/usr/bin/env bash\nif [ $# -eq 0 ]\n  then\n    tag='latest'\n  else\n    tag=$1\nfi\n\ndocker build -t project:$tag .\nRun ./build.sh for creating image project:latest or run ./build.sh your_tag to specify image tag.",
    "How to measure Docker build steps duration?": "BuildKit, which was experimental in 18.06 and generally available in 18.09, has this functionality built in. To configure the dockerd daemon with experimental mode, you can setup the daemon.json:\n$ cat /etc/docker/daemon.json\n{\n  \"experimental\": true\n}\nThen you can enable BuildKit from the client side with an environment variable:\n$ export DOCKER_BUILDKIT=1\n$ docker build -t java-test:latest .\n[+] Building 421.6s (13/13) FINISHED\n => local://context (.dockerignore)                                                                           1.6s\n => => transferring context: 56B                                                                              0.3s\n => local://dockerfile (Dockerfile)                                                                           2.0s\n => => transferring dockerfile: 895B                                                                          0.4s\n => CACHED docker-image://docker.io/tonistiigi/copy:v0.1.3@sha256:e57a3b4d6240f55bac26b655d2cfb751f8b9412d6f  0.1s\n => docker-image://docker.io/library/openjdk:8-jdk-alpine                                                     1.0s\n => => resolve docker.io/library/openjdk:8-jdk-alpine                                                         0.0s\n => local://context                                                                                           1.7s\n => => transferring context: 6.20kB                                                                           0.4s\n => docker-image://docker.io/library/openjdk:8-jre-alpine                                                     1.3s\n => => resolve docker.io/library/openjdk:8-jre-alpine                                                         0.0s\n => /bin/sh -c apk add --no-cache maven                                                                      61.0s\n => copy /src-0/pom.xml java/pom.xml                                                                          1.3s\n => /bin/sh -c mvn dependency:go-offline                                                                    339.4s\n => copy /src-0 java                                                                                          0.9s\n => /bin/sh -c mvn package -Dmaven.test.skip=true                                                            10.2s\n => copy /src-0/gs-spring-boot-docker-0.1.0.jar java/app.jar                                                  0.8s\n => exporting to image                                                                                        1.2s\n => => exporting layers                                                                                       1.0s\n => => writing image sha256:d57028743ca10bb4d0527a294d5c83dd941aeb1033d4fe08949a135677846179                  0.1s\n => => naming to docker.io/library/java-test:latest                                                           0.1s\nThere's also an option to disable the tty console output which generates output more suitable for scripting with each section having a start, stop, and duration:\n$ docker build -t java-test:latest --progress plain .                                                                                                                         \n\n#1 local://dockerfile (Dockerfile)                                                      \n#1       digest: sha256:da721b637ea85add6e26070a48520675cefc2bed947c626f392be9890236d11b\n#1         name: \"local://dockerfile (Dockerfile)\"      \n#1      started: 2018-09-05 19:30:53.899809093 +0000 UTC\n#1    completed: 2018-09-05 19:30:53.899903348 +0000 UTC\n#1     duration: 94.255\u00b5s\n#1      started: 2018-09-05 19:30:53.900069076 +0000 UTC\n#1 transferring dockerfile: 38B done\n#2 ...              \n\n#2 local://context (.dockerignore)  \n#2       digest: sha256:cbf55954659905f4d7bd2fc3e5e52d566055eecd94fd7503565315022d834c21\n#2         name: \"local://context (.dockerignore)\"       \n#2      started: 2018-09-05 19:30:53.899624016 +0000 UTC\n#2    completed: 2018-09-05 19:30:53.899695455 +0000 UTC\n#2     duration: 71.439\u00b5s\n#2      started: 2018-09-05 19:30:53.899839335 +0000 UTC\n#2    completed: 2018-09-05 19:30:54.359527504 +0000 UTC\n#2     duration: 459.688169ms                                                            \n#2 transferring context: 34B done                                \n\n\n#1 local://dockerfile (Dockerfile)\n#1    completed: 2018-09-05 19:30:54.592304408 +0000 UTC\n#1     duration: 692.235332ms\n\n\n#3 docker-image://docker.io/tonistiigi/copy:v0.1.3@sha256:e57a3b4d6240f55ba...           \n#3       digest: sha256:39386c91e9f27ee70b2eefdee12fc8a029bf5edac621b91eb5f3e6001d41dd4f\n#3         name: \"docker-image://docker.io/tonistiigi/copy:v0.1.3@sha256:e57a3b4d6240f55bac26b655d2cfb751f8b9412d6f7bb1f787e946391fb4b21b\"\n#3      started: 2018-09-05 19:30:54.731749377 +0000 UTC \n#3    completed: 2018-09-05 19:30:54.732013326 +0000 UTC\n#3     duration: 263.949\u00b5s\n\n\n#5 docker-image://docker.io/library/openjdk:8-jdk-alpine\n#5       digest: sha256:d680c6a82813d080081fbc3c024d21ddfa7ff995981cc7b4bfafe55edf80a319\n#5         name: \"docker-image://docker.io/library/openjdk:8-jdk-alpine\"\n#5      started: 2018-09-05 19:30:54.731483638 +0000 UTC\n#5    completed: 2018-09-05 19:30:54.732480345 +0000 UTC\n#5     duration: 996.707\u00b5s\n\n\n#4 docker-image://docker.io/library/openjdk:8-jre-alpine\n#4       digest: sha256:9ed31df4e6731a1718ea93bfa77354ad1ea2d1625c1cb16e2087d16d0b84bd00\n#4         name: \"docker-image://docker.io/library/openjdk:8-jre-alpine\"                \n#4      started: 2018-09-05 19:30:54.73176516 +0000 UTC\n#4    completed: 2018-09-05 19:30:54.732603067 +0000 UTC\n#4     duration: 837.907\u00b5s                              \n\n\n#7 local://context\n#7       digest: sha256:efe765161a29e2bf7a41439cd2e6656fcf6fa6bc97da825ac9b5a0d8adecf1ac\n#7         name: \"local://context\"\n#7      started: 2018-09-05 19:30:54.73178732 +0000 UTC\n#7    completed: 2018-09-05 19:30:54.731880943 +0000 UTC\n#7     duration: 93.623\u00b5s\n#7      started: 2018-09-05 19:30:54.792740019 +0000 UTC\n#7 transferring context: 473B done\n#7    completed: 2018-09-05 19:30:55.059008345 +0000 UTC\n#7     duration: 266.268326ms\n\n\n#9 /bin/sh -c mvn dependency:go-offline\n#9       digest: sha256:2197672cd7a44d93e0dba40aa00d7ef41f8680226d91f469d1c925646bdc8d6d\n#9         name: \"/bin/sh -c mvn dependency:go-offline\"\n#9      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#9    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#9     duration: 0s\n#9       cached: true\n\n\n#10 copy /src-0 java\n#10       digest: sha256:36cf252c34be098731bd8c5fb3f273f9c1437a5f74a65a3555d71150c2092fa7\n#10         name: \"copy /src-0 java\"\n#10      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#10    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#10     duration: 0s\n#10       cached: true\n\n#11 /bin/sh -c mvn package -Dmaven.test.skip=true\n#11       digest: sha256:390464b1fdc7a4c833b3476033d95b7714e22bcbfd018469e97b04781cb41532\n#11         name: \"/bin/sh -c mvn package -Dmaven.test.skip=true\"\n#11      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#11    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#11     duration: 0s\n#11       cached: true\n\n\n#12 copy /src-0/gs-spring-boot-docker-0.1.0.jar java/app.jar\n#12       digest: sha256:a7d60191a720f80de72a77ebe0d4bd1b0fd55d44e623661e80916b7fd1952076\n#12         name: \"copy /src-0/gs-spring-boot-docker-0.1.0.jar java/app.jar\"\n#12      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#12    completed: 2018-09-05 19:30:55.203555216 +0000 UTC\n#12     duration: 106.069\u00b5s\n#12       cached: true\n\n\n#6 /bin/sh -c apk add --no-cache maven\n#6       digest: sha256:db505db5e418f195c7bad3a710ad40bec3d91d47ff11a6f464b3ae37af744e7d\n#6         name: \"/bin/sh -c apk add --no-cache maven\"\n#6      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#6    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#6     duration: 0s\n#6       cached: true\n\n\n#8 copy /src-0/pom.xml java/pom.xml\n#8       digest: sha256:f032d4ff111c6ab0efef1a4e37d2467fffe43f48a529b8d56291ec81f96296ab\n#8         name: \"copy /src-0/pom.xml java/pom.xml\"\n#8      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#8    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#8     duration: 0s\n#8       cached: true\n\n\n#13 exporting to image\n#13       digest: sha256:d536dc2895c30fbde898bb4635581350a87c21f3695913ba21850a73d31422d9\n#13         name: \"exporting to image\"\n#13      started: 2018-09-05 19:30:55.203674127 +0000 UTC\n#13 exporting layers done\n#13 writing image sha256:d57028743ca10bb4d0527a294d5c83dd941aeb1033d4fe08949a135677846179 0.1s done\n#13 naming to docker.io/library/java-test:latest\n#13    completed: 2018-09-05 19:30:55.341300051 +0000 UTC\n#13     duration: 137.625924ms\n#13 naming to docker.io/library/java-test:latest 0.0s done",
    "Connect docker python to SQL server with pyodbc": "Need to Run:\nsudo apt-get install gcc\nneed to add a odbcinst.ini file containing:\n[FreeTDS]Description=FreeTDS Driver Driver=/usr/lib/x86_64-linux-gnu/odbc/libtdsodbc.so Setup=/usr/lib/x86_64-linux-gnu/odbc/libtdsS.so\nneed to add folowing to docker file\nADD odbcinst.ini /etc/odbcinst.ini\nRUN apt-get update\nRUN apt-get install -y tdsodbc unixodbc-dev\nRUN apt install unixodbc-bin -y\nRUN apt-get clean -y\nneed to change connection in .py to\nconnection = pyodbc.connect('Driver={FreeTDS};'\n                            'Server=xxxxx;'\n                            'Database=DCMM;'\n                            'UID=xxxxx;'\n                            'PWD=xxxxx')\nNow the container compiles, and gets data from SQL server",
    "connecting to local mongodb from docker container": "On Docker for Mac, you can use host.docker.internal if your mongo is running on your localhost. You could have your code read in an env variable for the mongo host and set it in the Dockerfile like so:\nENV MONGO_HOST \"host.docker.internal\"\nSee here for more details on https://docs.docker.com/docker-for-mac/networking/#use-cases-and-workarounds",
    "Can I mount same volume to multiple docker containers": "Yes you can add same location as a volume to many docker containers.\nAdditionally you can use --volumes-from to mount your log directory in one container not actually running any application and then use the volumes from this container in your other containers without having to repeat the paths everywhere.\nWorth a read Docker volumes",
    "Building common dependencies with docker-compose": "It's possible. There's a kind of workaround. You're close, but you were missing explicit image tags (so you had little ability on child images to declare which image you inherited from).\nversion: '3.2'\nservices:\n  base:\n    image: mybaseimage\n    build: ./path-to-base-dockerfile\n  child1:\n    build: ./path-to-child1-dockerfile\n    depends_on:\n      - base\n  child2:\n    build: ./path-to-child2-dockerfile\n    depends_on:\n      - base\nLet's say you have no images built. You run docker-compose up. The following things will happen:\ndocker-compose sees that child1 and child2 services depend on base. So it will deploy base first.\ndocker-compose sees that you have not yet tagged any image as mybaseimage. It knows how to build mybaseimage (you gave it a build path), so it will build it now, and tag it as mybaseimage.\ndocker-compose deploys the base service.\nideally you should design base so that it quits immediately, or has no entrypoint. since we don't actually want it to run this service.\ndocker-compose considers deploying child1 and child2\ndocker-compose sees that you have not yet tagged any image as child1. It knows how to build child1 (you gave it a build path), so it will build it now, and tag it as child1.\ndocker-compose deploys the child1 service\nsame sequence of steps for child2\nThe next docker-compose up will be simpler (we have tagged images available, so we skip all build steps).\nIf you already have tagged images, and want to rebuild: use docker-compose build to tell it to build all images (yes, base and children will both be rebuilt).",
    "How to use Docker's COPY/ADD instructions to copy a single file to an image": "As stated in the Dockerfile documentation:\nIf <src> is any other kind of file [besides a local tar archive], it is copied individually along with its metadata. In this case, if <dest> ends with a trailing slash /, it will be considered a directory and the contents of <src> will be written at <dest>/base(<src>).\nIf <dest> does not end with a trailing slash, it will be considered a regular file and the contents of <src> will be written at <dest>.\nThus, you have to write COPY test.txt /usr/src/app/ with a trailing /.",
    "Commands to execute background process in Docker CMD": "Besides the comments on your question that already pointed out a few things about Docker best practices you could anyway start a background process from within your start.sh script and keep that start.sh script itself in foreground using the nohup command and the ampersand (&). I did not try it with mongod but something like the following in your start.sh script could work:\n#!/bin/sh\n...\nnohup sh -c mongod --dbpath /test &\n...",
    "How to Specify $docker build --network=\"host\" mode in docker-compose at the time of build": "@dkanejs is right, and here is how you use it (the version number is important):\nversion: '3.4'\nservices:\n  my_image:\n    build:\n      context: .\n      network: host",
    ".dockerignore mentioned files are not ignored": "The .dockerignore rules follow the filepath/#Match.\nTry (for testing) Gemfile.lock instead of /Gemfile.lock.\nAnd check that the eol (end of line) characters are unix-style, not Windows style in your .dockerignore file.\nApparently, (docker 1.10, March 2016) using rule starting with / like /xxx ( or /.*) is not well supported.",
    "How to set timezone inside alpine base docker image?": "You need to install the tzdata package and then set the enviroment variable TZ to a timezone. (List with all the timezones)\nFROM alpine:latest\nRUN apk add --no-cache tzdata\nENV TZ=Europe/Copenhagen\nOutput\n$ docker run --rm alpine date\nTue Aug 31 09:52:08 UTC 2021\n\n$ docker run --rm myimage date\nTue Aug 31 11:52:13 CEST 2021",
    "Failed to create endpoint on network nat: hnsCall failed in Win32: The process cannot access the file": "Not sure how wise this is, but I checked the port wasn't in use with another app and still got the error.\nThis has fixed the issue a couple of times for me. In an Administrative PowerShell console, run the following:\nStop-Service docker\nStop-service hns\nStart-service hns\nStart-Service docker\ndocker network prune\nPartially sourced from this post.",
    "Docker: failed to export image: failed to create image: failed to get layer": "This problem occurs with a specific sequence of COPY commands in a multistage build.\nMore precisely, the bug triggers when there is a COPY instruction producing null effect (for example if the content copied is already present in the destination, with 0 diff), followed immediately by another COPY instruction.\nA workaround could be to add RUN true between COPY statements:\nCOPY ./lib/ /usr/src/app/BOOT-INF/lib/\nRUN true\nCOPY ./lib/entities-1.0-SNAPSHOT.jar /usr/src/app/BOOT-INF/lib/entities-1.0-SNAPSHOT.jar\nRUN true\nCOPY ./app/ /usr/src/app/\nAnother way that seems to work is to launch the build using BUILDKIT, like that:\nDOCKER_BUILDKIT=1 docker build --tag app:test .\nSee: https://github.com/moby/moby/issues/37965",
    "npm ERR! code ENOTEMPTY while npm install": "I think the following command might be more appropriate:\nrm -r node_modules\nThis will remove the node_modules folder in your repository. The command npm install should work now.\nIf you are using Webpack, you can also remove the dist folder using rm -r dist and re-build your repository.",
    "dockerfile: how use CMD or ENTRYPOINT from base image": "If you left it blank in your new Dockerfile, it will inherit the one from the base image.\nFor example:\nbase\nFROM ubuntu\nCMD [\"echo\", \"AAA\"]\nlayer1\nFROM base\nIf you build above images and run layer1 you will get the following:\n$ sudo docker run -it layer1\nAAA",
    "Speed up NPM install in Docker container": "This method works like magic:\nhttps://blog.playmoweb.com/speed-up-your-builds-with-docker-cache-bfed14c051bf\nDocker has a special way of caching things for you, and apparently it's best to use the inborn caching ability.\nCannot say I completely understand how it works, but it does work.\nIf you follow this pattern, it will work for you:\nFROM mhart/alpine-node:5.6.0\nWORKDIR /src\n\n# Expose the port 3000\nEXPOSE 3000\n\n# Set the default command to run when a container starts\nCMD [\"node\", \"server.js\"]\n\n# Install app dependencies\nCOPY package.json /src\nRUN npm install\n\n# Copy your code in the docker image\nCOPY . /src",
    "strconv.Atoi: parsing \"\": invalid syntax": "I also faced the same issue. There was no error except for:\nstrconv.Atoi: parsing \"\": invalid syntax\nScenario:\na go project\ndockerfile\ndocker-compose.yaml\nWindows 10 with docker-desktop\nThe following command will stop and delete all the docker containers that are not defined in docker compose file. There might be many useful containers running in your system that were not created by the docker-compose file or were created by someone else. Use this command with caution!\nSolution:\ndocker-compose down --remove-orphans\nExplaination: For some reason, the docker-compose create some unknown containers along with service. This command remove those unknown problematic containers.",
    "How to avoid question during the Docker build?": "I had the same issue in Dockerfile, then I used ARG DEBIAN_FRONTEND=noninteractive after base image and it works for me:\nExample Dockerfile:\nFROM ubuntu\nARG DEBIAN_FRONTEND=noninteractive",
    "How can I prevent a Dockerfile instruction from being cached?": "A build-time argument can be specified to forcibly break the cache from that step onwards. For example, in your Dockerfile, put\nARG CACHE_DATE=not_a_date\nand then give this argument a fresh value on every new build. The best, of course, is the timestamp.\ndocker build --build-arg CACHE_DATE=$(date +%Y-%m-%d:%H:%M:%S) ...\nMake sure the value is a string without any spaces, otherwise docker client will falsely take it as multiple arguments.\nSee a detailed discussion on Issue 22832.",
    "Multiple images inside one container": "Keep images light. Run one service per container. Use the official images on docker hub for mongodb, nodejs, rabbitmq, nginx etc. Extend them if needed. If you want to run everything in a fat container you might as well just use a VM.\nYou can of course do crazy stuff in a dev setup, but why spend time setting up something that has zero value in a production environment? What if you need to scale up one of the services? How do set memory and cpu constraints on each service? .. and the list goes on.\nDon't make monolithic containers.\nA good start is to use docker-compose to configure a set of services that can talk to each other. You can make a prod and dev version of your docker-compose.yml file.\nGetting into the right frame of mind\nIn a perfect world you would run your containers in clustered environment in production to be able to scale your system and have concurrency, but that might be overkill depending on what you are running. It's at least good to have this in the back of your head because it can help you to make the right decisions.\nSome points to think about if you want to be a purist :\nHow do you have persistent volume storage across multiple hosts?\nReverse proxy / load balancer should probably be the entry point into the system that talks to the containers using the internal network.\nIs my service even able run in a clustered environment (multiple instances of the container)\nYou can of course do dirty things in dev such as mapping in host volumes for persistent storage (and many people who use docker standalone in prod do that as well).\nIdeally we should separate docker in dev and docker i prod. Docker is a fantastic tool during development as you can have redis, memcached, postgres, mongodb, rabbitmq, node or whatnot up and running in minutes sharing that compose setup with the rest of the team. Docker in prod can be a completely different beast.\nI would also like to add that I'm generally against the fanaticism that \"everything should be running in docker\" in prod. Run services in docker when it makes sense. It's also not uncommon for larger companies to make their own base images. This can be a lot of work and will require maintenance to keep up with security fixes etc. It's not necessarily the first thing you jump on when starting with docker.",
    "Dockerfile: COPY / ADD with space character in path (Windows)": "Maybe you can use ARG to help you, like this:\nDockerfile:\nFROM jfloff/alpine-python:2.7\nARG src=\"Folder 1/File.txt\"\nARG target=\"Dir 1/\"\nCOPY ${src} ${target}\nBTW, a / has to be add at the end of Dir 1 if you treat really want to treat it as a folder.\nAnd, JSON format is also ok, just you miss ,, it should be:\nFROM jfloff/alpine-python:2.7\nCOPY [\"Folder 1/File.txt\", \"Dir 1/\"]\nUpdate for your comments:\nIn official guide, it said:\nWhen copying files or directories that contain special characters (such as [ and ]), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern.\nSo, for your case, it should be:\nFROM jfloff/alpine-python:2.7\nARG src=\"[[]Folder 1]/__SLIM_TEMPLATE.mm\"\nARG target=\"[Folder 1]/\"\nCOPY ${src} ${target}\nOr:\nFROM jfloff/alpine-python:2.7\nCOPY [\"[[]Folder 1]/__SLIM_TEMPLATE.mm\", \"[Folder 1]/\"]",
    "How are intermediate containers formed?": "Yes, Docker images are layered. When you build a new image, Docker does this for each instruction (RUN, COPY etc.) in your Dockerfile:\ncreate a temporary container from the previous image layer (or the base FROM image for the first command;\nrun the Dockerfile instruction in the temporary \"intermediate\" container;\nsave the temporary container as a new image layer.\nThe final image layer is tagged with whatever you name the image - this will be clear if you run docker history raghavendar/hands-on:2.0, you'll see each layer and an abbreviation of the instruction that created it.\nYour specific queries:\n1) 532 is a temporary container created from image ID b17, which is your FROM image, ubuntu:14.04.\n2) ea6 is the image layer created as the output of the instruction, i.e. from saving intermediate container 532.\n3) yes. Docker calls this the Union File System and it's the main reason why images are so efficient.",
    "Docker: Mount directory from one container to another": "Rene's answer works, but you could share data without using the host's directory (container1 ==> container2):\ndocker run -v /data/myfolder --name container1 image-name-1\ndocker run --volumes-from container1 image-name-2",
    "How to test the container or image after docker build?": "You have to give a command your container will have to process.\nExample : sh\nyou could try :\ndocker run -ti yourimage sh\n(-ti is used to keep a terminal open)\nIf you want to launch a daemon (like a server), you will have to enter something like :\ndocker run -d yourimage daemontolaunch\nUse docker help run for more options.\nYou also can set a default behaviour with CMD instruction in your Dockerfile so you won't have to give this command to your container each time you want to run it.\nEDIT - about container removing :\nContainers and images are different. A container is an instance of an image. You can run several containers from the same image.\nThe container automatically stops when the process it runs terminates. But the container isn't deleted (just stopped, so you can restart it). But if you want to remove it (removing a container doesn't remove the image) you have two ways to do :\nautomatically removing it at the end of the process by adding --rm option to docker run.\nManually removing it by using the docker rm command and giving it the container ID or its name (a container has to be stopped before being removed, use docker stop for this).\nA usefull command :\nUse docker ps to list containers. -q to display only the container IDs, -a to display even stopped containers.\nMore here.\nEDIT 2:\nThis could also help you to discover docker if you didn't try it.",
    "FIle could not be opened in append mode: failed to open stream: Permission denied laradock": "This worked for me:\nchown -R www-data:www-data \"project foldername\"",
    "How to access GIT repo with my private key from Dockerfile": "The error message Host key verification failed. is not complaining about your private key, but rather the host key for github.com. You can do this to add the github hostkey:\nssh-keyscan -t rsa github.com > ~/.ssh/known_hosts\nPerhaps you have your reasons, but in general cloning the git repo in to the image is not the preferred way to run your code in a container. Instead, put a Dockerfile at the root of your repo, and within the Dockerfile use the ADD command to include your source code in the container.\nAs you have it written now, your private key is part of the Docker image. Anyone you share the image with will also have your private key.",
    "How to know if a docker container is running in privileged mode": "From the docker host\nUse the docker inspect command:\ndocker inspect --format='{{.HostConfig.Privileged}}' <container id>\nAnd within a bash script you could have a test:\nif [[ $(docker inspect --format='{{.HostConfig.Privileged}}' <container id>) == \"false\" ]]; then\n    echo not privileged\nelse\n    echo privileged\nfi\nFrom inside the container itself\nYou have to try to run a command that requires the --privileged flag and see if it fails\nFor instance ip link add dummy0 type dummy is a command which requires the --privileged flag to be successful:\n$ docker run --rm -it ubuntu ip link add dummy0 type dummy\nRTNETLINK answers: Operation not permitted\nwhile\n$ docker run --rm -it --privileged ubuntu ip link add dummy0 type dummy\nruns fine.\nIn a bash script you could do something similar to this:\nip link add dummy0 type dummy >/dev/null\nif [[ $? -eq 0 ]]; then\n    PRIVILEGED=true\n    # clean the dummy0 link\n    ip link delete dummy0 >/dev/null\nelse\n    PRIVILEGED=false\nfi",
    "Pull docker images from a private repository during docker build?": "I was facing the same issue in 2019. I solved this using arguments (ARG).\nhttps://docs.docker.com/engine/reference/builder/#understand-how-arg-and-from-interact\nArguments allow you to set optional parameters (with defaults) that can be used in your FROM line.\nDockerfile-project-dev\nARG REPO_LOCATION=privaterepo.company.net/\nARG BASE_VERSION=latest\nFROM ${REPO_LOCATION}project/base:${BASE_VERSION}\n...\nFor my use-case I normally want to pull from the private repo, but if I'm working on the Dockerfiles I may want to be able to build from an image on my own machine, without having to modify the FROM line in my Dockerfile. To tell Docker to search my local machine for the image at build time I would do this:\ndocker build -t project/dev:latest -f ./Dockerfile-project-dev --build-arg REPO_LOCATION='' .",
    "How to install git on a docker ubuntu image?": "This Dockerfile works for me\nFROM ubuntu:18.04\nRUN apt update\nRUN apt install -y git\nInside the container\n$ which git\n/usr/bin/git",
    "Can dockerfile be put in .dockerignore?": "Yes, you can; you can even throw the .dockerignore itself in there!\nYou're likely doing something else incorrect - possibly in the wrong directory?\nDirectory listing:\n\u279c  ls -la\ntotal 16\ndrwxr-xr-x  4 tj    wheel  128 Nov 30 13:42 .\ndrwxrwxrwt  7 root  wheel  224 Nov 30 13:42 ..\n-rw-r--r--  1 tj    wheel   26 Nov 30 13:41 .dockerignore\n-rw-r--r--  1 tj    wheel   28 Nov 30 13:42 Dockerfile\nContent of files:\n\u279c  cat .dockerignore\n.dockerignore\nDockerfile\n\n\u279c  test_docker_ignore cat Dockerfile\nFROM ubuntu:16.04\nADD . .\nBuild it once; specifying --no-cache to be verbose:\n\u279c  docker build -t test --no-cache .\nSending build context to Docker daemon  3.072kB\nStep 1/2 : FROM ubuntu:16.04\n ---> 20c44cd7596f\nStep 2/2 : ADD . .\n ---> 4d8ded297954\nSuccessfully built 4d8ded297954\nSuccessfully tagged test:latest\nAdd something to the Dockerfile and rebuild: The build will use the cache as it ignores the changes made to the Dockerfile\n\u279c  echo \"# A Test Comment\" >> Dockerfile\n\u279c  docker build -t test .\nSending build context to Docker daemon  3.072kB\nStep 1/2 : FROM ubuntu:16.04\n ---> 20c44cd7596f\nStep 2/2 : ADD . .\n ---> Using cache\n ---> 4d8ded297954\nSuccessfully built 4d8ded297954\nSuccessfully tagged test:latest",
    "The command '/bin/sh -c returned a non-zero code: 127": "Solution to the image with error is to add before the wget CMD\nRUN yum -y install wget\nIf you write it like this, it is the same result, just different execution:\n RUN wget http://www.us.apache.org/dist/tomcat/tomcat-6/v6.0.44/bin/apache-tomcat-6.0.44.tar.gz\nDon't use the quotes and comma in RUN command.",
    "HEALTHCHECK: Dockerfile vs docker-compose.yml": "Adding health check to the Dockerfile, will make the health-check part of the image, so that anyone pulling the image from the registry will get the health check by default.\nCompose files are usually less shared than the actual docker images they run. The dockercompose health-check allows adding/overrriding healthchecks for images for someone who is not creating the image but rather is pulling it from a remote registry. It is more suitable in situations where the pulled image doesn't have a health-check by default.\nIn your case, since you are creating the image, adding the health-check to the dockerfile makes more sense.",
    "How to declare a Named Volume in a Dockerfile?": "It's not possible. I think the docs are worded maybe misleadingly.\n\u201cThe specified name\u201d refers to the path / directory name at which the volume will be created.",
    "Pass args to the Dockerfile from docker-compose": "the key word ARG has a different scope before and after the FROM instruction\nTry using ARG twice in your Dockerfile, and/or you can try the ENV variables\nARG LANDING_PAGE_DOMAIN\nFROM nginx:alpine\n\nCOPY nginx.conf /etc/nginx/nginx.conf\nCOPY production/* /etc/nginx/conf.d/\n\nARG LANDING_PAGE_DOMAIN\nENV LANDING_PAGE_DOMAIN=${LANDING_PAGE_DOMAIN}\n\nRUN sed -i s/{LANDING_PAGE_DOMAIN}/${LANDING_PAGE_DOMAIN}/g /etc/nginx/conf.d/landing.conf\n\nEXPOSE 80 443",
    "Dockerfile COPY instruction failing?": "At the time I originally wrote this, Docker didn\u2019t expand ~ or $HOME. Now it does some expansions inside the build context, but even so they are probably not what you want\u2014they aren\u2019t your home directory outside the context. You need to reference the file explicitly, or package it relative to the Dockerfile itself.",
    "What's the difference between pm2 and pm2-runtime?": "The main difference between pm2 and pm2-runtime is\npm2-runtime designed for Docker container which keeps an application in the foreground which keep the container running,\npm2 is designed for normal usage where you send or run the application in the background.\nIn simple words, the life of the container is the life of CMD or entrypoint.\nFor example\nDockerfile\nFROM node:alpine\nRUN npm install pm2 -g\nCOPY . /app\nWORKDIR /app\nCMD [ \"pm2\", \"start\",\"/app/server.js\"]\nIn this case, the container will die as soon as it run the process.\nTo deal with this, you have pm2-runtime\nFROM node:alpine\nRUN npm install pm2 -g\nCOPY . /app\nWORKDIR /app\nENV NODE_ENV=development\nCMD [ \"pm2-runtime\", \"start\",\"/app/bin/www\"]\nAs the container keeps running and it allocates tty session.\nFrom the documentation\nThe goal of pm2-runtime is to wrap your applications into a proper Node.js production environment. It solves major issues when running Node.js applications inside a container like:\nSecond Process Fallback for High Application Reliability\nProcess Flow Control\nAutomatic Application Monitoring to keep it always sane and high performing\nAutomatic Source Map Discovery and Resolving Support\nFurther than that, using PM2 as a layer between the container and the application brings PM2 powerful features like application declaration file, customizable log system and other great features to manage your Node.js application in production environment.\ndocker-pm2-nodejs",
    "Docker-compose --force-recreate specific service": "If your service depends on (or links to) other services, you can try:\ndocker-compose up --force-recreate --no-deps service-name\nThis will only recreate the specified service, linked or depended services will keep untouched.",
    "Starting container process caused \"exec: \\\"/bin/sh\\\": stat /bin/sh: no such file or directory\": unknown": "There are two things happening here.\nA Dockerfile that starts FROM scratch starts from a base image that has absolutely nothing at all in it. It is totally empty. There is not a set of base tools or libraries or anything else, beyond a couple of device files Docker pushes in for you.\nThe ENTRYPOINT echo ... command gets rewritten by Docker into ENTRYPOINT [\"/bin/sh\", \"-c\", \"echo ...\"], and causes the CMD to be totally ignored. Unless overridden with docker run --entrypoint, this becomes the main process the container runs.\nSince it is a FROM scratch image and contains absolutely nothing at all, it doesn't contain a shell, hence the \"/bin/sh: no such file or directory\" error.",
    "does docker always need an operating system as base image": "The containers on a host share the (host's) kernel but each container must provide (the subset of) the OS that it needs.\nIn Windows, there's a 1:1 mapping of kernel:OS but, with Linux, the kernel is bundled into various OSs: Debian, Ubuntu, Alpine, SuSE, CoreOS etc.\nThe FROM statement often references an operating system but it need not and it is often not necessary (nor a good idea) to bundle an operating system in a container. The container should only include what it needs.\nThe NGINX image uses Debian (Dockerfile).\nIn some cases, the container process has no dependencies beyond the kernel. In these cases, a special FROM: scratch may be used that adds nothing else. It's an empty image (link).",
    "Use sudo inside Dockerfile (Alpine)": "The su-exec can be used in alpine. Do add it the package, if not already available, add the following to your Dockerfile\nRUN apk add --no-cache su-exec\nInside your scripts you'd run inside docker you can use the following to become another user:\nexec su-exec <my-user> <my command>\nAlternatively, you could add the more familiair sudo package while building your docker-file Add the following to your Dockerfile that's FROM alpine\nRUN set -ex && apk --no-cache add sudo\nAfter that you can use sudo\nsudo -u <my-user> <my command>",
    "Copy current directory in to docker image": "Just add / at the end of src in ADD statement.\nADD ./* $HOME/src/",
    "How to create a Mongo Docker Image with default collections and data?": "The problem was that information could not be saved on /db/data, so I've created a solution creating my own data directory.\n# Parent Dockerfile https://github.com/docker-library/mongo/blob/982328582c74dd2f0a9c8c77b84006f291f974c3/3.0/Dockerfile\nFROM mongo:latest\n\n# Modify child mongo to use /data/db2 as dbpath (because /data/db wont persist the build)\nRUN mkdir -p /data/db2 \\\n    && echo \"dbpath = /data/db2\" > /etc/mongodb.conf \\\n    && chown -R mongodb:mongodb /data/db2\n\nCOPY . /data/db2\n\nRUN mongod --fork --logpath /var/log/mongodb.log --dbpath /data/db2 --smallfiles \\\n    && CREATE_FILES=/data/db2/scripts/*-create.js \\\n    && for f in $CREATE_FILES; do mongo 127.0.0.1:27017 $f; done \\\n    && INSERT_FILES=/data/db2/scripts/*-insert.js \\\n    && for f in $INSERT_FILES; do mongo 127.0.0.1:27017 $f; done \\\n    && mongod --dbpath /data/db2 --shutdown \\\n    && chown -R mongodb /data/db2\n\n# Make the new dir a VOLUME to persists it \nVOLUME /data/db2\n\nCMD [\"mongod\", \"--config\", \"/etc/mongodb.conf\", \"--smallfiles\"]\nThanks to @yosifkit from the docker-library/mongo Github project for pointing that the volume would store the data in the resulting image. I missed that on the documentation.",
    "Mount a volume in docker-compose. How is it done?": "Your config is probably not working because your version of docker-compose does not execute shell expansions while creating your container. That means that docker compose is trying to find a literal path $PWD/logstash instead of expanding $PWD to your present directory. Later versions of docker compose do allow for environment variable expansion.\nDocker-compose does allow relative paths though, through the use of ./, which references the folder the compose file is in, not necessarily your pwd, so you just need to change your compose file to be:\nvolumes:\n    - ./logstash:/config_dir",
    "Where does the output for \"docker -build\" go?": "Use:\ndocker images\nto see it\nExample:\nREPOSITORY     TAG                 IMAGE ID            CREATED             SIZE\nsample         latest              c660b762fcd1        5 days ago          1.46GB",
    "How to use multiple base images to build a docker image": "The latest version of docker has the concept of multi stage builds. Refer: (https://docs.docker.com/engine/userguide/eng-image/multistage-build/)\nWith multi-stage builds, you use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each of them begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don\u2019t want in the final image.",
    "Docker Compose: How to specify path to docker file while building as if in different directory?": "This can be done like this, add build in docker-compose.yml and run the build from ABOVE directory:\nversion: '3.8'\n\nservices:\n  your_service:\n    build: # \"context\" and \"dockerfile\" fields have to be under \"build\"\n      context: .\n      dockerfile: <Below_Directory>/Dockerfile",
    "copy a file with different name in destination directory using COPY in Dockerfile": "You can simply do it like COPY test.txt /dest/test_renamed.txt without a trailing /.\nIf you put a trailing / like COPY test.txt /dest/test_2/, it will copy the test.txt file (with the same name, of course) into the directory at /dest/test_2/.",
    "Copying files with execute permissions in Docker Image": "The default file permission is whatever the file permission is in your build context from where you copy the file. If you control the source, then it's best to fix the permissions there to avoid a copy-on-write operation. Otherwise, if you cannot guarantee the system building the image will have the execute bit set on the files, a chmod after the copy operation will fix the permission. E.g.\nCOPY entrypoint.sh .\nRUN chmod +x entrypoint.sh\nA better option with newer versions of docker (and which didn't exist when this answer was first posted) is to use the --chmod flag (the permissions must be specified in octal at last check):\nCOPY --chmod=0755 entrypoint.sh .\nYou do not need to know who will run the container. The user inside the container is typically configured by the image creator (using USER) and doesn't depend on the user running the container from the docker host. When the user runs the container, they send a request to the docker API which does not track the calling user id.\nThe only time I've seen the host user matter is if you have a host volume and want to avoid permission issues. If that's your scenario, I often start the entrypoint as root, run a script called fix-perms to align the container uid with the host volume uid, and then run gosu to switch from root back to the container user.",
    "Using Vault with docker-compose file": "This is my current docker-compose config for using Vault in dev, but I use dedicated servers (not Docker) in production.\n# docker_compose.yml\nversion: '2'\nservices:\n    myvault:\n        image: vault\n        container_name: myvault\n        ports:\n          - \"127.0.0.1:8200:8200\"\n        volumes:\n          - ./file:/vault/file:rw\n          - ./config:/vault/config:rw\n        cap_add:\n          - IPC_LOCK\n        entrypoint: vault server -config=/vault/config/vault.json\nThe volume mounts ensure the vault config is saved if you have to rebuild the container.\nTo use the 'file' backend, to make this setup portable for Docker/Git, you will also need to create a directory called config and put this file into it, named vault.json:\n# config/vault.json\n{\n  \"backend\": {\"file\": {\"path\": \"/vault/file\"}},\n  \"listener\": {\"tcp\": {\"address\": \"0.0.0.0:8200\", \"tls_disable\": 1}},\n  \"default_lease_ttl\": \"168h\",\n  \"max_lease_ttl\": \"0h\"\n}\nNotes:\nAlthough the ROOT_TOKEN is static in this configuration (will not change between container builds), any generated VAULT_TOKEN issued for an app_role will be invalidated every time the vault has to be unsealed.\nI have found the Vault sometimes becomes sealed when the container is restarted.",
    "Docker build taking too long when installing grpcio via pip": "I had the same issue and it was solved by upgrading pip:\n$ pip3 install --upgrade pip\nHere's a word from one of the maintainers of grpc project:\npip grpcio install is (still) very slow #22815",
    "Docker multiple environments": "You could take some clues from \"Using Compose in production\"\nYou\u2019ll almost certainly want to make changes to your app configuration that are more appropriate to a live environment. These changes may include:\nRemoving any volume bindings for application code, so that code stays inside the container and can\u2019t be changed from outside\nBinding to different ports on the host\nSetting environment variables differently (e.g., to decrease the verbosity of logging, or to enable email sending)\nSpecifying a restart policy (e.g., restart: always) to avoid downtime\nAdding extra services (e.g., a log aggregator)\nThe advice is then not quite similar to the example you mention:\nFor this reason, you\u2019ll probably want to define an additional Compose file, say production.yml, which specifies production-appropriate configuration. This configuration file only needs to include the changes you\u2019d like to make from the original Compose file.\ndocker-compose -f docker-compose.yml -f production.yml up -d\nThis overriding mechanism is better than trying to mix dev and prod logic in one compose file, with environment variable to try and select one.\nNote: If you name your second dockerfile docker-compose.override.yml, a simple docker-compose up would read the overrides automatically.\nBut in your case, a name based on the environment is clearer.",
    "unable to add certificates to alpine linux container": "I think below worked for me (I was adding a root certificate on blackfire/blackfire image which extends from alpine):\nRUN apk update && apk add ca-certificates && rm -rf /var/cache/apk/* \\\n  mkdir /usr/local/share/ca-certificates/extra\nCOPY .docker/other/cert_Intertrials-CA.crt /usr/local/share/ca-certificates/extra\nRUN update-ca-certificates\nI then logged into that VM and see it has added it to the merged cert file, /etc/ssl/certs/ca-certificates.crt (I believe i heard it takes each cert file from inside /usr/local/share/ca-certificates and merges into the /etc/ssl/certs/ca-certificates.crt file).\nNow you will get that 'does not contain exactly one certificate or CRL: skipping' error probably, but i heard that is fine.\nhttps://github.com/gliderlabs/docker-alpine/issues/30 mentions: \"that this is just a warning and shouldn't affect anything.\"\nhttps://github.com/gliderlabs/docker-alpine/issues/52 mentions: \"The WARNING: ca-certificates.crt does not contain exactly one certificate or CRL: skipping is just what it says it is, a warning. It is saying that ca-certificates.crt doesn't contain only one certificate (because it is the concatenation of all the certificates), therefore it is skipped and not included in ca-certificates.crt (since it cannot include itself).\"\n\"The warning shown is normal.\"",
    "Docker-compose.yml file that builds a base image, then children based on it?": "Doing a bit more research based on @amiasato 's anser, it looks as if there is a replicated key, which you can set to 0 like so:\nversion: \"3\"\nservices:\n  base-image:\n    build:\n      context: .\n      dockerfile: Dockerfile-base\n    deploy:\n      mode: replicated\n      replicas: 0\nSee https://docs.docker.com/compose/compose-file/compose-file-v3/#replicas",
    "Mysql docker container keeps restarting": "Simply remove the MYSQL_USER and it will work fine because the root user gets created automatically.\nPS. This seems to be a problem with a newer docker version because this used to work before and not throw an error.",
    "Deprecation warning when installing nodejs on docker container using nodesource install script": "The notice from the script is\n  This script, located at https://deb.nodesource.com/setup_X, used to\n  install Node.js is deprecated now and will eventually be made inactive.\n\n  Please visit the NodeSource distributions Github and follow the\n  instructions to migrate your repo.\n  https://github.com/nodesource/distributions \n\n  The NodeSource Node.js Linux distributions GitHub repository contains\n  information about which versions of Node.js and which Linux distributions\n  are supported and how to install it.\n  https://github.com/nodesource/distributions\nThe instructions on github amount to a Dockerfile RUN\nFROM docker.io/debian:12-slim\nRUN set -uex; \\\n    apt-get update; \\\n    apt-get install -y ca-certificates curl gnupg; \\\n    mkdir -p /etc/apt/keyrings; \\\n    curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key \\\n     | gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg; \\\n    NODE_MAJOR=18; \\\n    echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" \\\n     > /etc/apt/sources.list.d/nodesource.list; \\\n    apt-get -qy update; \\\n    apt-get -qy install nodejs;\nThe docker.io/node:18 image maintained by the Node.js project is Debian based if you want to save some time.\nFROM docker.io/node:18-bookworm-slim",
    "docker container exits immediately even with Console.ReadLine() in a .NET Core console application": "If you switch your app to target .NET Core 2.0, you can use the Microsoft.Extensions.Hosting package to host a .NET Core console application by using the HostBuilder API to start/stop your application. Its ConsoleLifetime class would process the general application start/stop method.\nIn order to run your app, you should implement your own IHostedService interface or inherit from the BackgroundService class, then add it to host context within ConfigureServices.\nnamespace Microsoft.Extensions.Hosting\n{\n    //\n    // Summary:\n    //     Defines methods for objects that are managed by the host.\n    public interface IHostedService\n    {\n        // Summary:\n        // Triggered when the application host is ready to start the service.\n        Task StartAsync(CancellationToken cancellationToken);\n\n        // Summary:\n        // Triggered when the application host is performing a graceful shutdown.\n        Task StopAsync(CancellationToken cancellationToken);\n    }\n}\nHere's a sample hosted service:\npublic class TimedHostedService : IHostedService, IDisposable\n{\n    private readonly ILogger _logger;\n    private Timer _timer;\n\n    public TimedHostedService(ILogger<TimedHostedService> logger)\n    {\n        _logger = logger;\n    }\n\n    public Task StartAsync(CancellationToken cancellationToken)\n    {\n        _logger.LogInformation(\"Timed Background Service is starting.\");\n\n        _timer = new Timer(DoWork, null, TimeSpan.Zero, \n            TimeSpan.FromSeconds(5));\n\n        return Task.CompletedTask;\n    }\n\n    private void DoWork(object state)\n    {\n        _logger.LogInformation(\"Timed Background Service is working.\");\n    }\n\n    public Task StopAsync(CancellationToken cancellationToken)\n    {\n        _logger.LogInformation(\"Timed Background Service is stopping.\");\n\n        _timer?.Change(Timeout.Infinite, 0);\n\n        return Task.CompletedTask;\n    }\n\n    public void Dispose()\n    {\n        _timer?.Dispose();\n    }\n}\nThen creating the HostBuilder and adding the service and other components (logging, configuration).\npublic class Program\n{\n    public static async Task Main(string[] args)\n    {\n        var hostBuilder = new HostBuilder()\n             // Add configuration, logging, ...\n            .ConfigureServices((hostContext, services) =>\n            {\n                // Add your services with depedency injection.\n            });\n\n        await hostBuilder.RunConsoleAsync();\n    }\n}",
    "Running a Docker file stored locally": "The process to run Dockerfile is:\ndocker build . -t [tag] -f /path/to/Dockerfile\nAnd then:\ndocker run -d tag",
    "Interactive command in Dockerfile": "You can also do it in several steps, begin with a Dockerfile with instructions until before the interactive part. Then\ndocker build -t image1 .\nNow just\ndocker run -it --name image2 image1 /bin/bash\nyou have a shell inside, you can do your interactive commands, then do something like\ndocker commit image2 myuser/myimage:2.1\nThe doc for docker commit\nhttps://docs.docker.com/engine/reference/commandline/commit/\nyou may need to specify a new CMD or ENTRYPOINT, as stated in the doc\nCommit a container with new CMD and EXPOSE instructions\nFor example some docker images using wine do it in several steps, install wine, then launch and configure the software launched in wine, then docker commit",
    "How to make Network_Mode : \"host\" work in docker-compose.yml file": "network_mode: host is used for sharing the same networking space with the Host. For example you can want to access an application that is running on your Linux PC from the container. If you want to link services together, you can use links, or depends_on, and if the services are on different hosts just create an overlay network",
    "Create a docker image/container from EC2 AMI": "Here is how I did it.\nOn source AMI locate root volume snapshot id in the description\n/dev/sda1=snap-eb79b0b1:15:true:gp2\nLaunch instance with public Ubuntu 14.04 AMI\nCreate volume from snapshot snap-eb79b0b1 (in the same region that the instance runs).\nAttach volume to the instance as /dev/sdf\nmount volume to /mnt\nmount /dev/xvdf /mnt\n(or)\nmount /dev/xvdf1 /mnt\ninstall docker\nhttps://docs.docker.com/engine/installation/ubuntulinux/\nimport docker image from mounted root volume\ntar -c -C /mnt/ . | docker import - appcimage-master-1454216413\nrun\ndocker run -t -i 6d6614111fcb03d5ca79541b8a23955202dfda74995d968b5ffb5d45c7e68da9 /bin/bash",
    "Docker buildkit cache location/size and ID": "Yes, it is somewhat vague in docker 20.10.5. Could use a pull request or two to update documentation.\nThe docker driver cache uses the same storage driver as used for image layers. Metadata is stored in databases at /var/lib/docker/buildkit. When docker uses overlay2 storage driver, the layer is in /var/lib/docker/overlay2/<ID>/diff/. For <ID>, see below. /var/lib/docker can vary depending on data-root in your dockerd configuration. Builders using docker-container or kubernetes driver keeps the data on a volume.\ndocker buildx [--builder name] du --verbose lists build cache. You can also inspect the docker driver caches from docker system df -v --format '{{ .BuildCache | json }}'. The cache type exec.cachemount is the RUN --mount type=cache. You can find the layer using the ID, which is not the same as used in --mount id. The mount type is implemented by buildkit, so the docker run --mount does not recognize it. To get rid of it either docker buildx prune or docker build --no-cache.\nThe cache key is the value from id=. id defaults to value of target. You need to specify id when you need different cache at the same target.\nYes. They are the same cache regardless of the target, Dockerfile or platform. Different builders have their own caches.",
    "How to run wp cli in docker-compose.yml": "Well there are a couple of problems. The first one is that those two containers (wordpress and wordpress-cli) don't share a volume. So while wordpress has a wordpress installation ready, the wordpress-cli doesn't.\nSo you can add volumes to both containers, and then wordpress-cli will find the wordpress installation.\nThen there's a second problem: the wordpress:latest and wordpress:cli images both run with the user www-data, but the problem is that the individual www-data users have different user-id's:\n$ docker run --rm wordpress:latest grep www-data /etc/passwd \nwww-data:x:33:33:www-data:/var/www:/usr/sbin/nologin\n$ docker run --rm wordpress:cli grep www-data /etc/passwd   \nwww-data:x:82:82:Linux User,,,:/home/www-data:/bin/false\nIt seems they aren't exactly compatible here. So if you use a shared volume you have to make sure they both use the same user-id. I solved this by having the wordpress:cli run with the user xfs which also has the user id 33.\nThe last problem is that your containers have dependencies on each other. Wordpress needs a running MySQL instance and the wordpress-cli needs also the MySQL and the Wordpress to be ready. To make sure MySQL is ready for the wordpress cli installation you either use something like \"wait-for-it\" or in a simple case you can just wait a couple of seconds and then try it.\nI have tested all those changes and came up with the following docker-compose.yml. I have annotated all the changes I've made with \"vstm\":\nversion: \"3.3\"\nservices:\n  db:\n    image: mysql:5.7\n    volumes:\n      - db_data:/var/lib/mysql\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: somewordpress\n      MYSQL_DATABASE: wordpress\n      MYSQL_USER: wordpress\n      MYSQL_PASSWORD: wordpress\n\n  wordpress:\n    depends_on:\n      - db\n    image: wordpress:latest\n    ports:\n      - 8000:80\n    restart: always\n    environment:\n      WORDPRESS_DB_HOST: db:3306\n      WORDPRESS_DB_NAME: wordpress\n      WORDPRESS_DB_USER: wordpress\n      WORDPRESS_DB_PASSWORD: wordpress\n      WORDPRESS_TABLE_PREFIX: \"wp_\"\n      WORDPRESS_DEBUG: 1\n    # vstm: add shared volume\n    volumes:\n      - wp_data:/var/www/html\n\n  wordpress-cli:\n    depends_on:\n      - db\n      - wordpress\n    image: wordpress:cli\n    # vstm: This is required to run wordpress-cli with the same\n    # user-id as wordpress. This way there are no permission problems\n    # when running the cli\n    user: '33'\n    # vstm: The sleep 10 is required so that the command is run after\n    # mysql is initialized. Depending on your machine this might take\n    # longer or it can go faster.\n    command: >\n      /bin/sh -c '\n      sleep 10;\n      wp core install --path=\"/var/www/html\" --url=\"http://localhost:8000\" --title=\"Local Wordpress By Docker\" --admin_user=admin --admin_password=secret --admin_email=foo@bar.com\n      '\n    # vstm: add shared volume\n    volumes:\n      - wp_data:/var/www/html\n    # WP CLI needs the environment variables used for the Wordpress image\n    environment:\n      WORDPRESS_DB_HOST: db:3306\n      WORDPRESS_DB_NAME: wordpress\n      WORDPRESS_DB_USER: wordpress\n      WORDPRESS_DB_PASSWORD: wordpress\n\nvolumes:\n  db_data:\n  # vstm: add shared volume\n  wp_data:\nIt uses a docker-volume but you can also map it to a filesystem. Depends on how you plan to use your docker-compose.",
    "pg_restore in postgres docker container": "Here is a way to restore from a file located on the host machine:\ndocker exec -i container_name pg_restore -U postgres_user -v -d database_name < /dir_backup_outside_container/file_name.tar",
    "Building docker images from a source directory using different dockerfiles": "Since Docker 1.5, you can use the -f argument to select the Dockerfile to use e.g:\ndocker build -t doronaviugy/myproject -f dockerfiles/first.docker .\nIf you use stdin to build your image ( the - < first.docker syntax), you won't have a build context so you will be unable to use COPY or ADD instructions that refer to local files.\nIf you have to use an older version of Docker, you'll need to use some scripting to copy your specific Dockerfiles to Dockerfile at the root of the build context before calling docker build.",
    "Docker EXPOSE using run-time environment variables": "A little late but you could also use build args and change your code to:\nFROM python:3.6.5-stretch\n\n[ ... ]\n\nARG MY_SERVICE_PORT=8080\nARG MY_SERVICE_PORT_RPC=50051\n# 8080 and 50051 would be the defaults\n\n[ ... ]\n# Still functions like environment variables :)\nEXPOSE ${MY_SERVICE_PORT}\nEXPOSE ${MY_SERVICE_PORT_RPC}\nThen you can build with docker build --build-arg MY_SERVICE_PORT=80 -t image_tag before you run. This way you could have your containerized application and your container running with the same ports without getting too complex.",
    "How can I keep a docker debian container open?": "You need to explicitly run bash:\ndocker run -it debian /bin/bash\nThe -i means \"run interactively\", and -t means \"allocate a pseudo-tty\".\nA good place to read a bit more is the section Running an interactive shell in the Quickstart documentation.",
    "Docker + mssql-server-linux: How to launch .sql file during build (from Dockerfile)": "I ended up using a slightly modified version of VDR's solution which waits for the sqlservr to start by checking the logs instead of sleeping 10 seconds:\nRUN ( /opt/mssql/bin/sqlservr --accept-eula & ) | grep -q \"Service Broker manager has started\" \\\n    && /opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P 'P@ssw0rd' -i /opt/mssql-scripts/000_create_db.sql \\\n    && pkill sqlservr ",
    "How to access local file when building from Dockerfile?": "Why don't you COPY the file inside the container before executing RUN?\nFROM centos:6\nCOPY hello.rpm /tmp/hello.rpm\nRUN rpm -ivh /tmp/hello.rpm\nThis assumes that hello.rpm is next to your Dockerfile when you build it.",
    "Docker Compose + Rails: best practice to migrate?": "From https://docs.docker.com/engine/reference/builder/#cmd:\nIf you would like your container to run the same executable every time, then you should consider using ENTRYPOINT in combination with CMD. See ENTRYPOINT\nhttps://docs.docker.com/engine/reference/builder/#entrypoint\ntl;dr\nYou could define an entrypoint under app and define a bash file there:\napp:\n  entrypoint: [bin/entry]\n  ..\nbin/entry file example:\n#!/bin/bash\nset -e\n\nrake db:create\nrake db:migrate\n\nexec \"$@\"",
    "How to pass environment variables to a frontend web application?": "The way that I resolved this is as follows:\n1.Set the value in the enviroment.prod.ts with a unique and identificable String:\nexport const environment = {\n  production: true,\n  REST_API_URL: 'REST_API_URL_REPLACE',\n};\n2.Create a entryPoint.sh, this entryPoint will be executed every time that you done a docker run of the container.\n#!/bin/bash\nset -xe\n: \"${REST_API_URL_REPLACE?Need an api url}\"\n\nsed -i \"s/REST_API_URL_REPLACE/$REST_API_URL_REPLACE/g\" /usr/share/nginx/html/main*bundle.js\n\nexec \"$@\"\nAs you can see, this entrypoint get the 'REST_API_URL_REPLACE' argument and replace it (in this case) in the main*bundle.js file for the value of the var.\n3.Add the entrypoint.sh in the dockerfile before the CMD (it need execution permissions):\nFROM node:alpine as builder\nCOPY package.json ./\nRUN npm i && mkdir /app && cp -R ./node_modules ./app\nWORKDIR /app\nCOPY . .\nRUN $(npm bin)/ng build --prod\n\nFROM nginx:alpine\nCOPY nginx/default.conf /etc/nginx/conf.d/\nRUN rm -rf /usr/share/nginx/html/*\nCOPY --from=builder /app/dist /usr/share/nginx/html\n\n# Copy the EntryPoint\nCOPY ./entryPoint.sh /\nRUN chmod +x entryPoint.sh\n\nENTRYPOINT [\"/entryPoint.sh\"]\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n4.Lauch the image with the env or use docker-compose (the slash must be escaped):\ndocker run -e REST_API_URL_REPLACE='http:\\/\\/backend:8080\\/api'-p 80:80 image:tag\nProbably exists a better solution that not need to use a regular expresion in the minified file, but this works fine.",
    "How to source a script with environment variables in a docker build process?": "Each Dockerfile RUN step runs a new container and a new shell. If you try to set an environment variable in one shell, it will not be visible later on. For example, you might experiment with this Dockerfile:\nFROM busybox\nENV FOO=foo1\nRUN export FOO=foo2\nRUN export BAR=bar\nCMD echo FOO is $FOO, BAR is $BAR\n# Prints \"FOO is foo1, BAR is \"\nThere are three good solutions to this. In order from easiest/best to hardest/most complex:\nAvoid needing the environment variables at all. Install software into \u201csystem\u201d locations like /usr; it will be isolated inside the Docker image anyways. (Don\u2019t use an additional isolation tool like Python virtual environments, or a version manager like nvm or rvm; just install the specific thing you need.)\nUse ENV. This will work:\nFROM busybox\nENV FOO=foo2\nENV BAR=bar\nCMD echo FOO is $FOO, BAR is $BAR\n# Prints \"FOO is foo2, BAR is bar\"\nUse an entrypoint script. This typically looks like:\n#!/bin/sh\n# Read in the file of environment settings\n. /opt/wherever/env\n# Then run the CMD\nexec \"$@\"\nCOPY this script into your Dockerfile. Make it be the ENTRYPOINT; make the CMD be the thing you\u2019re actually running.\nFROM busybox\nWORKDIR /app\nCOPY entrypoint.sh .\nCOPY more_stuff .\nENTRYPOINT [\"/app/entrypoint.sh\"]\nCMD [\"/app/more_stuff/my_app\"]\nIf you care about such things, environment variables you set via this approach won\u2019t be visible in docker inspect or a docker exec debug shell; but if you docker run -it ... sh they will be visible. This is a useful and important enough pattern that I almost always use CMD in my Dockerfiles unless I\u2019m specifically trying to do first-time setup like this.",
    "How do I copy variables between stages of multi stage Docker build?": "You got 3 options: The \"ARG\" solution, the \"base\"-stage solution, and \"file\" solution.\n\"ARG\" solution\nARG version_default=v1\n\nFROM alpine:latest as base1\nARG version_default\nENV version=$version_default\nRUN echo ${version}\nRUN echo ${version_default}\n\nFROM alpine:latest as base2\nARG version_default\nRUN echo ${version_default}\n\"base\"-stage solution\nanother way is to use base container for multiple stages:\nFROM alpine:latest as base\nARG version_default\nENV version=$version_default\n\nFROM base\nRUN echo ${version}\n\nFROM base\nRUN echo ${version}\nYou can find more details here: https://github.com/moby/moby/issues/37345\n\"file\" solution\nSave the hash into a file in the first stage, copy it in the second stage, and then read it and use it there.",
    "Unable to load shared library \"libgdiplus\" - Docker [ .NET application with Aspose API]": "You're installing libgdiplus in your build container image, but not in your final container image. You need to make sure libgdiplus is installed in your final container image.\nYou can consider amending your Dockerfile like this:\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.0 AS base\nRUN apt-get update && apt-get install -y libgdiplus\n\nWORKDIR /app\nFROM mcr.microsoft.com/dotnet/core/sdk:3.0 AS build\n[...]",
    "When using COPY with more than one source file, the destination must be a directory and end with a /": "In my case adding the (forward) slash at the end (following docker error message) was enough, like this:\nCOPY package*.json . # (fails!)\nCOPY package*.json ./ # (works:)",
    "How to add docker label after image is made": "It's indeed not possible to add a label to an existing image. Strictly speaking, adding a label would change the images checksum, thus its id, thus it's no longer the same image.\nBut you can build an image based on your existing image with a label added, and then tag this image with the name of the previously existing image. Technically it adds a layer on top of your existing image and thus just \"overrides\" previous labels.\nIt's also possible to do this with a single command. Given you want to add a label to the image \"debian:latest\", you build FROM that image and tag the new image at the same time.\necho \"FROM debian:latest\" | docker build --label my_new_label=\"arbitrary value\" -t \"debian:latest\" -\nProof that \"adding\" the label worked:\n$ docker inspect -f \"{{json .Config.Labels }}\" debian:latest\n{\"my_new_label\":\"arbitrary value\"}",
    "What happens when a volume links an existing populated host and container dir": "When you run a container and mount a volume from the host, all you see in the container is what is on the host - the volume mount points at the host directory, so if there was anything in the directory in the image it gets bypassed.\nWith an image from this Dockerfile:\nFROM ubuntu\nWORKDIR /vol\nRUN touch /vol/from-container\nVOLUME /vol\nWhen you run it without a host mount, the image contents get copied into the volume:\n> docker run vol-test ls /vol\nfrom-container \nBut mount the volume from the host and you only see the host's content:\n> ls $(pwd)/host\nfrom-host\n> docker run -v $(pwd)/host:/vol vol-test ls /vol\nfrom-host\nAnd no, you don't need the VOLUME instruction. The behaviour is the same without it.",
    "Is there a way to lint the Dockerfile?": "Try:\nEither the Haskell Dockerfile Linter (\"hadolint\"), also available online. hadolint parses the Dockerfile into an AST and performs checking and validation based on best practice Docker images rules. It also uses Shellcheck to lint the Bash code on RUN commands.\nOr dockerlinter (node.js-based).\nI've performed a simple test against of a simple Docker file with RUN, ADD, ENV and CMD. dockerlinter was smart about grouping the same violation of rules together but it was not able to inspect as thorough as hadolinter possibly due to the lack of Shellcheck to statically analyze the Bash code.\nAlthough dockerlinter falls short in the scope it can lint, it does seem to be much easier to install. npm install -g dockerlinter will do, while compiling hadolinter requires a Haskell compiler and build environment that takes forever to compile.\n$ hadolint ./api/Dockerfile\nL9 SC2046 Quote this to prevent word splitting.\nL11 SC2046 Quote this to prevent word splitting.\nL8 DL3020 Use COPY instead of ADD for files and folders\nL10 DL3020 Use COPY instead of ADD for files and folders\nL13 DL3020 Use COPY instead of ADD for files and folders\nL18 DL3020 Use COPY instead of ADD for files and folders\nL21 DL3020 Use COPY instead of ADD for files and folders\nL6 DL3008 Pin versions in apt get install. Instead of `apt-get install <package>` use `apt-get install <package>=<version>`\nL6 DL3009 Delete the apt-get lists after installing something\nL6 DL3015 Avoid additional packages by specifying `--no-install-recommends`\n\n$ dockerlint ./api/Dockerfile\nWARN:  ADD instruction used instead of COPY on line 8, 10, 13, 18, 21\nERROR: ./api/Dockerfile failed.\nUpdate in 2018. Since hadolint has the official Docker repository now, you can get the executable quickly:\nid=$(docker create hadolint/hadolint:latest)\ndocker cp \"$id\":/bin/hadolint .\ndocker rm \"$id\"\nor you can use this command\ndocker container run --rm -i hadolint/hadolint hadolint - < Dockerfile\nThis is a statically compiled executable (according to ldd hadolint), so it should run regardless of installed libraries. A reference on how the executable is built: https://github.com/hadolint/hadolint/blob/master/docker/Dockerfile.",
    "Logrotate - nginx logs not rotating inside docker container": "As stated on the edit on my question the problem was that CMD from nginx:1.11 was only starting the nginx process. A work around is to place the following command on my Dockerfile\nCMD service cron start && nginx -g 'daemon off;'\nThis will start nginx as nginx:1.11 starts it and well as start the cron service.\nThe Dockerfile would look something like:\nFROM nginx:1.11\n\n# Remove sym links from nginx image\nRUN rm /var/log/nginx/access.log\nRUN rm /var/log/nginx/error.log\n\n# Install logrotate\nRUN apt-get update && apt-get -y install logrotate\n\n# Copy MyApp nginx config\nCOPY config/nginx.conf /etc/nginx/nginx.conf\n\n#Copy logrotate nginx configuration\nCOPY config/logrotate.d/nginx /etc/logrotate.d/\n\n# Start nginx and cron as a service\nCMD service cron start && nginx -g 'daemon off;'",
    "Dockerfile ARG substitution in a string in RUN command": "Docker doesn't expand ARG values in the RUN command. Instead, it injects the ARG as an environment variable. The shell itself expands the variable, and all of the Linux shells I've used behave differently based on the type of quote.\nThe single quotes direct the shell not to expand anything, and you only need to escape the single quotes and escape characters. While the double quotes include variable expansion along with many other escape characters. See the man page on your shell for more details.\nSo the solution as you've already found is:\nRUN echo \"Hello $w\"",
    "Docker: Set container name inside Dockerfile": "The Dockerfile is for creating images not containers.\nYou can now give names to your containers using the new --name flag for docker run.\nIf --name is not provided Docker will automatically generate an alphanumeric string for the container name.",
    "You installed esbuild on another platform than the one you're currently using": "You've copied node_modules from your local environment to the container. Locally you have packages for the darwin-arm64 arch, but inside the container, it is a Linux system that requires packages for linux-arm64.\nTo avoid such errors you should not copy node_modules to the container.\nAll you need is to add node_modules to .dockerignore file",
    "Docker Compose + Spring Boot + Postgres connection": "Each container has its own network interface with its own localhost. So change how Java points to Postgres:\nspring.datasource.url=jdbc:postgresql://localhost:5432/sample\nTo:\nspring.datasource.url=jdbc:postgresql://db:5432/sample\ndb will resolve to the proper Postgres IP.\nBonus. With docker-compose you don't need to build your image by hand. So change:\nweb:\n  image: myuser/manager:latest\nTo:\nweb:\n  build: .",
    "$(uname -a) returning the same in docker host or any docker container": "Docker uses the host operating system kernel, there is no custom or additional kernel inside the container. All containers running on the machine are sharing this \"host\" kernel.\nSee for more information this question on SuperUser.",
    "How to handle specific hostname like -h option in Dockerfile": "This isn't generally possible in a Dockerfile.\nDepending on the software, you might be able to do some kind of work-around. For example, you could try something like\nRUN echo $(grep $(hostname) /etc/hosts | cut -f1) my.host.name >> /etc/hosts && install-software\nBy setting the hostname within the same RUN command as you install the software, it'll happen inside the same layer of the container. Docker will later overwrite the hostname and you'll have to set it anew when running, but your software might be OK with that.\nIf you have to do a lot of this, you might try Packer for building containers. It can build Docker containers, but doesn't use multiple layers. This makes it slower to rebuild, faster to download the built images, and makes it more convenient to do multiple operations on an image before freezing it into a container.",
    "Dockerfile - How to pass an answer to a prompt post apt-get install?": "This answer has an explanation for the difference between \"assume yes\" and a non-interactive mode.\nI also found an example of a Dockerfile that installs jackd2 here, and it's setting DEBIAN_FRONTEND to 'noninteractive' before installing jackd2.",
    "what does VOLUME command do in Dockerfile? [duplicate]": "The VOLUME command will specify a mount point in the container. This mount point will be mapped to a location on the host that is either specified when the container is created or (when not specified) chosen automatically from a directory created in /var/lib/docker/volumes.\nIf the directory chosen as the mount point contains any files then these files will be copied into this volume. The advantage over mkdir is that it will persist the files to a location on the host machine after the container is terminated.\nIt appears some people have questioned why you would use the VOLUME command since it creates an anonymous volume. Anonymous volumes don't have much use any more and are basically an artifact of the early days of Docker before volumes could be named. You would normally specify the volume name when creating the container:\ndocker container run -v my-volume:/data image_tag\nIn this example, /data is the mount point in the container and my-volume is the volume on the local host. If my-volume does not exist when this command is run then it is created on the local host.",
    "How to output a multiline string in Dockerfile with a single command": "There is another question similar to this with a solution: How to write commands with multiple lines in Dockerfile while preserving the new lines?\nThe answer to this question is more particular in how to use multiline strings in bash rather than how to use Docker.\nFollowing this solution you may accomplish what you want to do as shown below:\nRUN echo $' \\n\\\n*****first row ***** \\n\\\n*****second row ***** \\n\\\n*****third row ***** ' >> /home/myfile\nMore info about this leading dollar sign here: How does the leading dollar sign affect single quotes in Bash?\nNote that this syntax relies on the run command using /bin/bash, not /bin/sh.",
    "What does Docker STOPSIGNAL do?": "SIGTERM is the default signal sent to containers to stop them: https://docs.docker.com/engine/reference/commandline/stop/\nSTOPSIGNAL does allow you to override the default signal sent to the container. Leaving it out of the Dockerfile causes no harm - it will remain the default of SIGTERM.\nThis being said, it is unclear why the author has explicitly defined the STOPSIGNAL as SIGTERM.\nLooking at this commit, we can see that the STOPSIGNAL used to be set to SIGQUIT.\nMy guess is that they left it in explicitly for documentation's sake after making the change.\nDiscussion of the change here: https://github.com/nginxinc/docker-nginx/issues/167",
    "determine OS distribution of a docker image": "The Filesystem Hierarchy Standard has a standard definition for /etc/os-release, which should be available on most distributions:\nThe /etc/os-release and /usr/lib/os-release files contain operating system identification data.\nThe basic file format of os-release is a newline-separated list of environment-like shell-compatible variable assignments. It is possible to source the configuration from shell scripts.\nThis means you can just source /etc/os-release and use $NAME or $ID to identify the distribution. As an example, on Fedora it looks like this:\n% source /etc/os-release\n% echo $NAME\nFedora\n% echo $ID\nfedora\nOn Debian:\n% source /etc/os-release\n% echo $NAME\nDebian GNU/Linux\n% echo $ID\ndebian",
    "How can I run script automatically after Docker container startup": "The image itself has an entrypoint ENTRYPOINT [\"/run/entrypoint.sh\"] specified in the Dockerfile. You can replace it by your own script. So for example create a new script, mount it and first call /run/entrypoint.sh and then wait for start of elasticsearch before running your init_sg.sh.",
    "How to override the CMD command in the docker run line": "The right way to do it is deleting cmd [\"...\"]\n docker run --name test test/test-backend /srv/www/bin/gunicorn.sh",
    "Is there a more elegant way to copy specific files using Docker COPY to the working directory?": "2021: with BuildKit, see \".NET package restore in Docker cached separately from build\" from Palec.\n2018: Considering that wildcard are not well-supported by COPY (moby issue 15858), you can:\neither experiment with adding .dockerignore files in the folder you don't want to copy (while excluding folders you do want): it is cumbersome\nor, as shown here, make a tar of all the folders you want\nHere is an example, to be adapted in your case:\nfind .. -name '*.csproj' -o -name 'Finomial.InternalServicesCore.sln' -o -name 'nuget.config' \\\n  | sort | tar cf dotnet-restore.tar -T - 2> /dev/null\nWith a Dockerfile including:\nADD docker/dotnet-restore.tar ./\nThe idea is: the archive gets automatically expanded with ADD.\nThe OP sturmstrike mentions in the comments \"Optimising ASP.NET Core apps in Docker - avoiding manually copying csproj files (Part 2)\" from Andrew Lock \"Sock\"\nThe alternative solution actually uses the wildcard technique I previously dismissed, but with some assumptions about your project structure, a two-stage approach, and a bit of clever bash-work to work around the wildcard limitations.\nWe take the flat list of csproj files, and move them back to their correct location, nested inside sub-folders of src.\n# Copy the main source project files\nCOPY src/*/*.csproj ./  \nRUN for file in $(ls *.csproj); do mkdir -p src/${file%.*}/ && mv $file src/${file%.*}/; done\nL01nl suggests in the comments an alternative approach that doesn't require compression: \"Optimising ASP.NET Core apps in Docker - avoiding manually copying csproj files\", from Andrew Lock \"Sock\".\nFROM microsoft/aspnetcore-build:2.0.6-2.1.101 AS builder\nWORKDIR /sln\n\nCOPY ./*.sln ./NuGet.config  ./\n\n# Copy the main source project files\nCOPY src/*/*.csproj ./\nRUN for file in $(ls *.csproj); do mkdir -p src/${file%.*}/ && mv $file src/${file%.*}/; done\n\n# Copy the test project files\nCOPY test/*/*.csproj ./\nRUN for file in $(ls *.csproj); do mkdir -p test/${file%.*}/ && mv $file test/${file%.*}/; done\n\nRUN dotnet restore\n\n# Remainder of build process\nThis solution is much cleaner than my previous tar-based effort, as it doesn't require any external scripting, just standard docker COPY and RUN commands.\nIt gets around the wildcard issue by copying across csproj files in the src directory first, moving them to their correct location, and then copying across the test project files.",
    "Docker: Error response from daemon: OCI runtime create failed: container_linux.go:296:": "Docker is telling you that the command hit an error. It is trying to run the node image with the command -w. Since -w is not a command, it throws this error.\nThis is because you have written node in a place you probably didn't mean to.\nYour command is being interpreted like this:\ndocker run -p [port_info] -v [volume_info] node [command]\nYou can rewrite your command like so and it should work fine:\ndocker run -p 8085:3000 -v /home/joel/workspace/plural_docker_webdev:/var/www -w \"/var/www\" node npm start",
    "Correct way to deploy WAR files in docker image": "You should actually ALWAYS deploy the exploded .war.\nThere are two elements of speed to think about here:\nHow fast is it to be able to push up your image to a container repository?\nand\nHow quickly can a new instance of my container start serving requests? (important in an elastic-scaling environment)\nThe answer to both is the same: You are better off exploding the .war file when creating your container and NOT copying the .war file to it.\nThis has the following two very positive effects:\nIt makes the differences between container versions much smaller, and so your upload time is less.\nIt means that, when dynamically scaling to meet application demand, your new container instances don't have to unzip your .war file before they can start responding to requests.\nFor those of us burdened by slow-upload connections, it's also a great idea to use a CI server or even a cloud-hosted VM to build and push your docker images to dockerhub or another container registry. That way you can take advantage of gigabit-scale upload speeds.",
    "appSettings.json for .NET Core app in Docker?": "Try replacing this line:\nENV ASPNET_ENV Development\nWith this:\nENV ASPNETCORE_ENVIRONMENT Development\nYour original Environment Variable name was used in older .NET Core, but has been changed. It can be a pain finding tutorials, etc. for .NET Core because of all of the changes that have happened since it first started!\nDon't get me started on project.json files!\nMore info:\nhttps://learn.microsoft.com/en-us/aspnet/core/fundamentals/configuration https://learn.microsoft.com/en-us/aspnet/core/fundamentals/environments",
    "Failed to install gcc on Python-3.7-alpine docker container": "For me installation of these packages within docker container helped:\nRUN apk update && apk add python3-dev \\\n                        gcc \\\n                        libc-dev",
    "module 'numpy' has no attribute 'object' [closed]": "Since version 1.24 of numpy, np.object is deprecated, and needs to be replaced with object (cf. numpy release notes).\nYou either need to update this in your code, or another package you're using needs to be updated (not possible to answer without more information).\nOne (dirty) workaround for now would be to fix your numpy version to the last version still supporting np.object with pip install numpy==1.23.4",
    "Multiple Docker build args from docker-compose .env file": "Facepalm \ud83e\udd26\u200d\u2640\ufe0f - This is working perfectly. I was putting the - DB_PWD=$DB_PWD argument under the wrong service in my docker-compose.yaml file. I will leave this here as a reference on how to use the .env file with docker build arguments -- and as a reminder to my self that I'm an idiot. I'm embarrassed --100 SOF reputation",
    "How to pass environment variables from docker-compose into the NodeJS project?": "Use process.env in node.js code, like this\nprocess.env.BACKEND_SERVER\nMention your variable in docker-compose file.\nversion: \"3\"\nservices:\n  client-side-app:\n    image: my-client-side-docker-image\n    environment:\n      - BACKEND_SERVER=\"here we need to enter backend server\"\n    ports:\n      - \"8080:8080\"\n  server-side-app:\n    image: my-server-side-docker-image\n    ports:\n      - \"3000:3000\"",
    "Dockerfile \"RUN chmod\" not taking effect": "You should set the owner directly when you copy the files:\nFROM joomla:3.9-php7.2-apache\n\nRUN apt-get update \\\n&& apt-get install -y apt-utils vim curl\n\nCOPY --chown=www-data:www-data ./joomla_html /var/www/html\n\nRUN chmod -R 765 /var/www/html/\n\nCOPY ./docker/php.ini /usr/local/etc/php/conf.d/php-extras.ini\n\nEXPOSE 80",
    "Purpose of FROM command - Docker file": "Containers don't run a full OS, they share the kernel of the host OS (typically, the Linux kernel). That's the \"Host Operating System\" box in your right image.\nThey do provide what's called \"user space isolation\" though - roughly speaking, this means that every container manages its own copy of the part of the OS which runs in user mode- typically, that's a Linux distribution such as Ubuntu. In your right image, that would be contained in the \"Bins/Libs\" box.\nYou can leave out the FROM line in your Dockerfile, or use FROM scratch, to create a blank base image, then add all the user mode pieces on top of a blank kernel yourself.",
    "Dockerfile, how install snap, snapd: unrecognized service": "first of all, you don't want to install the \"snap\" package, as it is not related to \"snapd\". Secondly, myself stumbled across this issue of installing snapd within a docker container: TLDR; Running snapd that way is currently not supported.\nBut that question has been asked already at the snapcraft forums. One of snapd's dependencies is systemd and the snapd-service isn't properly initialized without a reboot or relogin. That is the required procedure according to the documentation across all distributions, but obviously isn't an option within docker.\nAt least this open question replicates your question most: unable-to-install-snapcraft-snap-in-docker-image-ubuntu-19-10\nAnd Evan at the snapcraft forum here posted an approach, that I couldn't get to work either.\nThe only approach that might work is similar to running docker inside of docker, i.e.:\ninstall snapd on the docker host\nmount the snapd-socket at runtime into the container that has snapd installed.\nBut same warnings/side-effects apply as they do to running docker-in-docker.",
    "Docker COPY with folder wildcards": "If you use the dotnet command to manage your solution you can use this piece of code:\nCopy the solution and all project files to the WORKDIR\nList projects in the solution with dotnet sln list\nIterate the list of projects and move the respective *proj files into newly created directories.\nCOPY *.sln ./\nCOPY */*/*.*proj ./\nRUN dotnet sln list | \\\n      tail -n +3 | \\\n      xargs -I {} sh -c \\\n        'target=\"{}\"; dir=\"${target%/*}\"; file=\"${target##*/}\"; mkdir -p -- \"$dir\"; mv -- \"$file\" \"$target\"'",
    "launch a CAT command unix into Dockerfile": "Based on this comment to an issue posted on Github, this works:\nRUN echo 'All of your\\n\\\nmultiline that you ever wanted\\n\\\ninto a dockerfile\\n'\\\n>> /etc/example.conf",
    "Create Docker container with both Java and Node.js": "The best way for you is to take java (which is officially deprecated and it suggests you use openjdk image) and install node in it.\nSo, start with\nFROM openjdk:latest\nThis will use the latest openjdk image, which is 8u151 at this time. Then install node and other dependencies you might need:\nRUN apt-get install -y curl \\\n  && curl -sL https://deb.nodesource.com/setup_9.x | bash - \\\n  && apt-get install -y nodejs \\\n  && curl -L https://www.npmjs.com/install.sh | sh\nYou might want to install things like grunt afterwards, so this might come in handy as well.\nRUN npm install -g grunt grunt-cli\nIn total you will get the following Dockerfile:\nFROM openjdk:latest\n\nRUN apt-get install -y curl \\\n  && curl -sL https://deb.nodesource.com/setup_9.x | bash - \\\n  && apt-get install -y nodejs \\\n  && curl -L https://www.npmjs.com/install.sh | sh \\\nRUN npm install -g grunt grunt-cli\nYou may clone the Dockerfile from my gitlab repo here",
    "Docker error with read-only file system unknown": "If I remember correct after clearing space and Restarting Docker from scratch worked for me.",
    "Error running docker container: starting container process caused \"exec: \\\"python\\\": executable file not found in $PATH\": unknown": "There is no /usr/bin/python in a docker image built by the code above. But there is /usr/bin/python3. So you could either use python3 directly as your ENTRYPOINT or create a symlink.",
    "What is the location of redis.conf in official docker image?": "The default image from redis does not have a redis.conf.\nHere is the link for the image on dockerhub. https://hub.docker.com/_/redis/\nYou will have to copy it to image or have it mapped on the host using a volume mapping.",
    "Docker - Execute command after mounting a volume": "I generally agree with Chris's answer for local development. I am going to offer something that combines with a recent Docker feature that may set a path for doing both local development and eventual production deployment with the same image.\nLet's first start with the image that we can build in a manner that can be used for either local development or deployment somewhere that contains the code and dependencies. In the latest Docker version (17.05) there is a new multi-stage build feature that we can take advantage of. In this case we can first install all your Composer dependencies to a folder in the build context and then later copy them to the final image without needing to add Composer to the final image. This might look like:\nFROM composer as composer\nCOPY . /app\nRUN composer install --ignore-platform-reqs --no-scripts\n\nFROM php:fpm\nWORKDIR /var/www/root/\nRUN apt-get update && apt-get install -y \\\n        libfreetype6-dev \\\n        libjpeg62-turbo-dev \\\n        libmcrypt-dev \\\n        libpng12-dev \\\n        zip \\\n        unzip \\\n    && docker-php-ext-install -j$(nproc) iconv mcrypt \\\n    && docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \\\n    && docker-php-ext-install -j$(nproc) gd \\\n    && docker-php-ext-install mysqli \\\n    && docker-php-ext-enable opcache\nCOPY . /var/www/root\nCOPY --from=composer /app/vendor /var/www/root/vendor\nThis removes all of Composer from the application image itself and instead uses the first stage to install the dependencies in another context and copy them over to the final image.\nNow, during development you have some options. Based on your docker-compose.yml command it sounds like you are mounting the application into the container as .:/var/www/root. You could add a composer service to your docker-compose.yml similar to my example at https://gist.github.com/andyshinn/e2c428f2cd234b718239. Here, you just do docker-compose run --rm composer install when you need to update dependencies locally (this keeps the dependencies build inside the container which could matter for native compiled extensions, especially if you are deploying as containers and developing on Windows or Mac).\nThe other option is to just do something similar to what Chris has already suggested, and use the official Composer image to update and manage dependencies when needed. I've done something like this locally before where I had private dependencies on GitHub which required SSH authentication:\ndocker run --rm --interactive --tty \\\n            --volume $PWD:/app:rw,cached \\\n            --volume $SSH_AUTH_SOCK:/ssh-auth.sock \\\n            --env SSH_AUTH_SOCK=/ssh-auth.sock \\\n            --volume $COMPOSER_HOME:/composer \\\n            composer:1.4 install --ignore-platform-reqs --no-scripts\nTo recap, the reasoning for this method of building the image and installing Composer dependencies using an external container / service:\nPlatform specific dependencies will be built correctly for the container (Linux architecture vs Windows or Mac).\nNo Composer or PHP is required on your local computer (it is all contained inside Docker and Docker Compose).\nThe initial image you built is runnable and deployable without needing to mount code into it. In development, you are just overriding the /var/www/root folder with a local volume.",
    "How to bring credentials into a Docker container during build": "A few new Docker features make this more elegant and secure than it was in the past. The new multi-phase builds let us implement the builder pattern with one Dockerfile. This method puts our credentials into a temporary \"builder\" container, and then that container builds a fresh container that doesn't hold any secrets.\nYou have choices for how you get your credentials into your builder container. For example:\nUse an environment variable: ENV creds=user:pass and curl https://$creds@host.com\nUse a build-arg to pass credentials\nCopy an ssh key into the container: COPY key /root/.ssh/id_rsa\nUse your operating system's own secure credentials using Credential Helpers\nMulti-phase Dockerfile with multiple FROMs:\n## Builder\nFROM alpine:latest as builder\n#\n# -- insert credentials here using a method above --\n#\nRUN apk add --no-cache git\nRUN git clone https://github.com/some/website.git /html\n\n## Webserver without credentials\nFROM nginx:stable\nCOPY --from=builder /html /usr/share/nginx/html",
    "Docker multistage: how to COPY built files between stages?": "Well, apparently, I was mislead by the COPY step used in the first stage in the doc example. In my case, this is actually useless, and I can just COPY --from=haskell in my second stage, without any COPY in the first stage.\nThe following Dockerfile builds without issues:\nFROM haskell:8.6.5 as haskell\nRUN git clone https://gitlab+deploy-token-75:sakyTxfe-PxPHDwqsoGm@gitlab.pasteur.fr/bli/bioinfo_utils.git\nWORKDIR bioinfo_utils/remove-duplicates-from-sorted-fastq/Haskell\nRUN stack --resolver ghc-8.6.5 build && \\\n    stack --resolver ghc-8.6.5 install --local-bin-path .\n\nFROM python:3.7-buster\nRUN python3.7 -m pip install snakemake\nRUN mkdir -p /opt/bin\nCOPY --from=haskell /bioinfo_utils/remove-duplicates-from-sorted-fastq/Haskell/remove-duplicates-from-sorted-fastq /opt/bin/remove-duplicates-from-sorted-fastq\nCMD [\"/bin/bash\"]",
    "How to download and unzip in Dockerfile": "Best to use a multistage docker build. You will need the latest version of docker and buildkit enabled. Then do something along these lines\n# syntax=docker/dockerfile:1\nFROM alpine:latest AS unzipper\nRUN apk add unzip wget curl\nRUN mkdir /opt/ ; \\\n  curl <some-url> | tar xvzf - -C /opt\n\nFROM wordpress:fpm\nCOPY  --from=unzipper /opt/ /var/www/html/wp-content/themes/wordpress/    \nEven better is if there is a Docker image built already with the stuff in you want you just need the 'copy --from' line and give it the image name.\nFinally dont worry about any mess in the 1st stage as its discarded when the build completes, so the fact its alpine, and not using no-cache is irrelevant, and none of the installed packages end up in the final image",
    "How Docker calculates the hash of each layer? Is it deterministic?": "Thanks @thaJeztah. Answer is in https://gist.github.com/aaronlehmann/b42a2eaf633fc949f93b#id-definitions-and-calculations\nlayer.DiffID: ID for an individual layer\nCalculation: DiffID = SHA256hex(uncompressed layer tar data)\nlayer.ChainID: ID for a layer and its parents. This ID uniquely identifies a filesystem composed of a set of layers.\nCalculation:\nFor bottom layer: ChainID(layer0) = DiffID(layer0)\nFor other layers: ChainID(layerN) = SHA256hex(ChainID(layerN-1) + \" \" + DiffID(layerN))\nimage.ID: ID for an image. Since the image configuration references the layers the image uses, this ID incorporates the filesystem data and the rest of the image configuration.\nCalculation: SHA256hex(imageConfigJSON)",
    "Docker add warfile to official Tomcat image": "Reading from the documentation of the repo you would do something like that\nFROM tomcat\nMAINTAINER xyz\n\nADD your.war /usr/local/tomcat/webapps/\n\nCMD [\"catalina.sh\", \"run\"]\nThen build your image with docker build -t yourName <path-to-dockerfile>\nAnd run it with:\ndocker run --rm -it -p 8080:8080 yourName\n--rm removes the container as soon as you stop it\n-p forwards the port to your host (or if you use boot2docker to this IP)\n-it allows interactive mode, so you see if something get's deployed",
    "How to dynamically change the docker's base image": "From Docker version 17.05, you can do something like this:\nARG MYAPP_IMAGE=myorg/myapp:latest\nFROM $MYAPP_IMAGE\nYou can provide MYAPP_IMAGE as a command line paramether:\ndocker build -t container_tag --build-arg MYAPP_IMAGE=localimage:latest .\nMore info here: https://www.jeffgeerling.com/blog/2017/use-arg-dockerfile-dynamic-image-specification",
    "Docker dotnet watch run error: Unable to bind to https://localhost:5000 on the IPv6 loopback interface": "Just ran into this problem myself. I don't think dotnet watch run plays nicely with localhost type urls. Try setting your hosting url to https://0.0.0.0:5000 in your container.\nIn the dockerfile with:\nENTRYPOINT [ \"dotnet\", \"watch\", \"run\", \"--no-restore\", \"--urls\", \"https://0.0.0.0:5000\"]\nOr in launchSettings.json like:\n{\n  \"profiles\": {\n    \"[Put your project name here]\": {\n      \"commandName\": \"Project\",\n      \"launchBrowser\": true,\n      \"environmentVariables\": {\n        \"ASPNETCORE_ENVIRONMENT\": \"Development\",\n        \"DOTNET_USE_POLLING_FILE_WATCHER\": \"true\"\n      },\n      \"applicationUrl\": \"https://0.0.0.0:5000/\"\n    }\n  }\n}\nNow to get it to automatically reload from within the container you have to use the polling file watcher. That's what the second environment variable is for. (This is pretty common, you've got to do this with webpack, angular, etc).\nIn your case, you need to change the esportsapp.volume to a directory on your host:\nvolumes:\n  - ./:/app\nThat will map the /app volume in your container to the docker-compose directory. The problem you're facing is that the app is built in a volume on your project's default docker-compose network, so when you change a file in the source directory, it's not actually changing in that volume. With this fix, however, you'll run into the problem of the dotnet restore and dotnet watch inside the container changing your host's files. There's a fix for all of that, if you're interested...\nMy Usual .Net Core App Docker setup\nTo debug, run: docker-compose -f run.yml up --build\nTo build a release: docker-compose -f build.yml up --build\nProject structure\n/                                               # source control root\n/build.yml                                      # docker-compose file for building a release\n/run.yml                                        # docker-compose file for running locally & debugging\n/project                                        # an application\n/project/build.Dockerfile                       # the docker container that will build \"project\" for release\n/project/run.Dockerfile                         # the docker container that will build and run \"project\" locally for debugging\n/project/.dockerignore                          # speeds up container builds by excluding large directories like \"packages\" or \"node_modules\"\n/project/src                                    # where I hide my source codez\n/project/src/Project.sln\n/project/src/Project/Project.csproj\n/project/src/Project/Directory.Build.props      # keeps a docker mapped volume from overwriting .dlls on your host\n/project/src/Project.Data/Project.Data.csproj   # typical .Net project structure\n/web-api                                        # another application...\nDirectory.Build.props (put this in the same folder as your .csproj, keeps your dotnet watch run command from messing with the source directory on your host)\n<Project>\n\n  <PropertyGroup>\n    <DefaultItemExcludes>$(DefaultItemExcludes);$(MSBuildProjectDirectory)/obj/**/*</DefaultItemExcludes>\n    <DefaultItemExcludes>$(DefaultItemExcludes);$(MSBuildProjectDirectory)/bin/**/*</DefaultItemExcludes>\n  </PropertyGroup>\n\n  <PropertyGroup Condition=\"'$(DOTNET_RUNNING_IN_CONTAINER)' == 'true'\">\n    <BaseIntermediateOutputPath>$(MSBuildProjectDirectory)/obj/container/</BaseIntermediateOutputPath>\n    <BaseOutputPath>$(MSBuildProjectDirectory)/bin/container/</BaseOutputPath>\n  </PropertyGroup>\n\n  <PropertyGroup Condition=\"'$(DOTNET_RUNNING_IN_CONTAINER)' != 'true'\">\n    <BaseIntermediateOutputPath>$(MSBuildProjectDirectory)/obj/local/</BaseIntermediateOutputPath>\n    <BaseOutputPath>$(MSBuildProjectDirectory)/bin/local/</BaseOutputPath>\n  </PropertyGroup>\n\n</Project>\nrun.yml (docker-compose.yml for debugging)\nversion: \"3.5\"\nservices:\n  project:\n    build:\n      context: ./project\n      dockerfile: run.Dockerfile\n    ports:\n      - 5000:80\n    volumes:\n      - ./project/src/Project:/app\nrun.Dockerfile (the Dockerfile for debugging)\nFROM microsoft/dotnet:2.1-sdk\n\n# install the .net core debugger\nRUN apt-get update\nRUN apt-get -y --no-install-recommends install unzip\nRUN apt-get -y --no-install-recommends install procps\nRUN rm -rf /var/lib/apt/lists/*\n\nRUN curl -sSL https://aka.ms/getvsdbgsh | bash /dev/stdin -v latest -l /vsdbg\n\nVOLUME /app\nWORKDIR /app\n\nCMD dotnet watch run --urls http://0.0.0.0:80\nbuild.yml (the docker-compose.yml for building release versions)\nversion: \"3.5\"\nservices:\n  project:\n    build:\n      context: ./project\n      dockerfile: build.Dockerfile\n    volumes:\n      - ./project:/app\nbuild.Dockerfile (the Dockerfile for building release versions)\nFROM microsoft/dotnet:2.1-sdk\n\nVOLUME /app\n\n# restore as a separate layer to speed up builds\nWORKDIR /src\nCOPY src/Project/Project.csproj .\nRUN dotnet restore\n\nCOPY src/Project/ .\nCMD dotnet publish -c Release -o /app/out/",
    "Connecting two docker containers [duplicate]": "Using --link was the only way of connecting containers before the advent of docker networks. These provide a \"cleaner\" solution to the problem of inter-container communication and at the same time solves 2 of the major limits of links:\nrestart of linked container breaks the link\nlinks are not supported between containers running on different hosts\nUsing docker network you would use the --net option to start the containers on the specified network:\ndocker network create example\ndocker run -d --net example --name container1 <image>\ndocker run -d --net example --name container2 <image>\nAt this point the 2 container are mutually reachable via the address <container-name>.example: that is container1.example and container2.example.",
    "Docker File: Chmod on Entrypoint Script": "Docker will copy files into the container with the permissions of their source. If you strip the Linux executable bits somewhere in the chain of pushing to your code repo, or on your build host, then you'll need to add those execute permissions back. I've seen this issue most often reported by Windows users, who are downloading code to a filesystem that doesn't support the Linux permission bits. Hopefully we'll get a COPY --chmod solution soon that will eliminate the need for an extra layer.",
    "Should Dockerfile execute \"npm install\" and \"npm run build\" or should it only copy those files over?": "Building inside a container guarantees a predictable and reproducible build artifact. Running npm install on macOS and Linux can produce different node_modules, for example node-gyp.\nPeople often build node_modules with multi-stage build (if the actual container you're trying to build is not a Node.js application). That is to say, your actual nginx application per se does not depend on Node.js, but the node_modules directory and its containing files instead. So we generate node_modules in a node container, and copy it to the new container (nginx).\nThus, everyone building with the multi-stage Dockerfile will produce exact same container. If you copy your local node_modules into a container during build, other coworkers will not be able to predict the content of node_modules.",
    "Vite: Could not resolve entry module (index.html)": "Vite uses an html page as an entry point by default. So you either need to create one or if you don't have an html page, you can use it in \"library mode\".\nhttps://vitejs.dev/guide/build.html#library-mode\nFrom the docs:\n// vite.config.js\nconst path = require('path')\nconst { defineConfig } = require('vite')\n\nmodule.exports = defineConfig({\n  build: {\n    lib: {\n      entry: path.resolve(__dirname, 'lib/main.js'),\n      name: 'MyLib',\n      fileName: (format) => `my-lib.${format}.js`\n    },\n    rollupOptions: {\n      // make sure to externalize deps that shouldn't be bundled\n      // into your library\n      external: ['vue'],\n      output: {\n        // Provide global variables to use in the UMD build\n        // for externalized deps\n        globals: {\n          vue: 'Vue'\n        }\n      }\n    }\n  }\n})",
    "Docker swarm: 'build' configuration in docker compose file ignored during stack deployment": "Short answer is, you can not use the build command with docker stack deploy.\nFrom the docs:\nNote: The docker stack command build option is ignored when deploying a stack in swarm mode with a (version 3) Compose file. The docker stack command accepts only pre-built images.\nAn alternative is to build the docker image before deploying the your swarm cluster.\nUse the docker build command to create the docker image; Push the created image to a (public or private) docker registry; and reference it in your docker compose file.",
    "Using ARG and ENV in Dockerfile": "In a Dockerfile, each FROM line starts a new image, and generally resets the build environment. If your image needs to specify ARGs, they need to come after the FROM line; if it's a multi-stage build, they need to be repeated in each image as required. ARG before the first FROM are only useful to allow variables in the FROM line and to supply defaults, but can't be used otherwise.\nThis is further discussed under Understand how ARG and FROM interact in the Dockerfile documentation.\nFROM centos:7\n\n# _After_ the FROM line\nARG my_arg\nARG other_arg=other_default\n...",
    "Install package in running docker container": "You can use docker commit:\nStart your container sudo docker run IMAGE_NAME\nAccess your container using bash: sudo docker exec -it CONTAINER_ID bash\nInstall whatever you need inside the container\nExit container's bash\nCommit your changes: sudo docker commit CONTAINER_ID NEW_IMAGE_NAME\nIf you run now docker images, you will see NEW_IMAGE_NAME listed under your local images.\nNext time, when starting the docker container, use the new docker image you just created:\nsudo docker run **NEW_IMAGE_NAME** - this one will include your additional installations.\nAnswer based on the following tutorial: How to commit changes to docker image",
    "How to add private nuget source in dotnet dockerfile?": "Answer for year 2023\nWithout security\nThis code for WebApplication3 works just fine. We use BaGet NuGet server to have a proxy between Nuget.org and our build servers for faster loads of common packages we use.\n#See https://aka.ms/containerfastmode to understand how Visual Studio uses this Dockerfile to build your images for faster debugging.\n\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS build\nWORKDIR /src\nCOPY [\"WebApplication3/WebApplication3.csproj\", \"WebApplication3/\"]\n\n# !!!IMPORTANT PART HERE !!!\n\n# Add your NuGet server here\nRUN dotnet nuget add source https://nuget.yourdomain.com/v3/index.json\n# For our purposes, to hide nuget.org behind a NuGet proxy we disable its source, you can skip that\nRUN dotnet nuget disable source \"nuget.org\"\n\n# Just to see if two lines above work\nRUN dotnet nuget list source\n\nRUN dotnet restore \"WebApplication3/WebApplication3.csproj\"\n\nCOPY . .\nWORKDIR \"/src/WebApplication3\"\nRUN dotnet build \"WebApplication3.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"WebApplication3.csproj\" -c Release -o /app/publish /p:UseAppHost=false\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"WebApplication3.dll\"]\nWith basic authentication\n#See https://aka.ms/containerfastmode to understand how Visual Studio uses this Dockerfile to build your images for faster debugging.\n\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS build\n\n# !!!IMPORTANT PART HERE !!!\n\nARG NUGET_USERNAME\nARG NUGET_PASSWORD\n\nENV NUGET_USERNAME=${NUGET_USERNAME}\nENV NUGET_PASSWORD=${NUGET_PASSWORD}\n\n# Adds this source with basic authentication, other authentication types exist but I'm not sure if they are applicable here in Linux based container\nRUN dotnet nuget add source https://nuget.yourdomain.com/v3/index.json --name=\"Your source name\" --username ${NUGET_USERNAME} --valid-authentication-types basic --store-password-in-clear-text --password ${NUGET_PASSWORD}\n\n\nWORKDIR /src\nCOPY [\"WebApplication3/WebApplication3.csproj\", \"WebApplication3/\"]\n\nRUN dotnet nuget disable source \"nuget.org\"\n\n# Just to see if two lines above work\nRUN dotnet nuget list source\n\nRUN dotnet restore \"WebApplication3/WebApplication3.csproj\"\n\nCOPY . .\nWORKDIR \"/src/WebApplication3\"\nRUN dotnet build \"WebApplication3.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"WebApplication3.csproj\" -c Release -o /app/publish /p:UseAppHost=false\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"WebApplication3.dll\"]",
    "Why does chown increase size of docker image?": "Every step in a Dockerfile generates a new intermediate image, or \"layer\", consisting of anything that changed on the filesystem from the previous layer. A docker image consists of a collection of layers that are applied one on top of another to create the final filesystem.\nIf you have:\nRUN adduser example -D -h /example -s /bin/sh\nThen you are probably changing nothing other than a few files in /etc (/etc/passwd, /etc/group, and their shadow equivalents).\nIf you have:\nRUN adduser example -D -h /example -s /bin/sh && \\\n    chown -R example.example /lib\nThen the list of things that have changed includes, recursively, everything in /lib, which is potentially larger. In fact, in my alpine:edge container, it looks like the contents of /lib is 3.4MB:\n/ # du -sh /lib\n3.4M    /lib\nWhich exactly accounts for the change in image size in your example.\nUPDATE\nUsing your actual Dockerfile, with the npm install ... line commented out, I don't see any difference in the final image size whether or not the adduser and chown commands are run. Given:\nRUN echo \"http://nl.alpinelinux.org/alpine/edge/main\" > /etc/apk/repositories && \\\n    echo \"http://nl.alpinelinux.org/alpine/edge/testing\" >> /etc/apk/repositories && \\\n    apk add -U wget iojs && \\\n    apk upgrade && \\\n    wget -q --no-check-certificate https://ghost.org/zip/ghost-0.6.0.zip -O /tmp/ghost.zip && \\\n    unzip -q /tmp/ghost.zip -d /ghost && \\\n    cd /ghost && \\\n#    npm install --production && \\\n    sed 's/127.0.0.1/0.0.0.0/' /ghost/config.example.js > /ghost/config.js && \\\n    sed -i 's/\"iojs\": \"~1.2.0\"/\"iojs\": \"~1.6.4\"/' package.json && \\\n#   adduser ghost -D -h /ghost -s /bin/sh && \\\n#   chown -R ghost.ghost * && \\\n    npm cache clean && \\\n    rm -rf /var/cache/apk/* /tmp/*\nI get:\n$ docker build -t sotest .\n[...]\nSuccessfully built 058d9f41988a\n$ docker inspect -f '{{.VirtualSize}}' 058d9f41988a\n31783340\nWhereas given:\nRUN echo \"http://nl.alpinelinux.org/alpine/edge/main\" > /etc/apk/repositories && \\\n    echo \"http://nl.alpinelinux.org/alpine/edge/testing\" >> /etc/apk/repositories && \\\n    apk add -U wget iojs && \\\n    apk upgrade && \\\n    wget -q --no-check-certificate https://ghost.org/zip/ghost-0.6.0.zip -O /tmp/ghost.zip && \\\n    unzip -q /tmp/ghost.zip -d /ghost && \\\n    cd /ghost && \\\n#    npm install --production && \\\n    sed 's/127.0.0.1/0.0.0.0/' /ghost/config.example.js > /ghost/config.js && \\\n    sed -i 's/\"iojs\": \"~1.2.0\"/\"iojs\": \"~1.6.4\"/' package.json && \\\n    adduser ghost -D -h /ghost -s /bin/sh && \\\n    chown -R ghost.ghost * && \\\n    npm cache clean && \\\n    rm -rf /var/cache/apk/* /tmp/*\nI get:\n$ docker build -t sotest .\n[...]\nSuccessfully built 696b481c5790\n$ docker inspect -f '{{.VirtualSize}}' 696b481c5790\n31789262\nThat is, the two images are approximately the same size (the difference is around 5 Kb).\nI would of course expect the resulting image to be larger if the npm install command could run successfully (because that would install additional files).",
    "Install libssl-dev for Docker": "Some packages are built against libressl in Alpine 3.6. Try replacing line 6 in your Dockerfile with the following\nRUN apk add libressl-dev",
    "Docker CentOS image does not auto start httpd": "You need to run apache (httpd) directly - you should not use init.d script.\nTwo options:\nyou have to run apache in foreground: /usr/sbin/apache2 -DFOREGROUND ... (or /usr/sbin/httpd in CentOS)\nyou have to start all services (including apache configured as auto-run) by executing /sbin/init as entrypoint.",
    "How to build Docker with env file": "If you are using docker-compose (which now comes bundled with Docker), .env is the default filename for the file that contains variables that are made available to the parser for the docker-compose.yml file ONLY, and not to the build process or container environment variables.\nUsing the --env-file command line argument or setting the env_file variable in your docker-compose.yml file will identify the file from which to load variables into the container environment (and not the build process). You can run set from a shell in your running container to view the environment variables that are loaded.\nIt is a common antipattern to set env_file=.env in docker-compose.yml which is a very confusing thing to do as it makes the .env file provide both variables to the docker-compose.yml parser AND the running container environment - it is almost certainly the reason that you are reading this.\nIf you want variables in your .env to be available to the build process, i.e. you want to use them in your Dockerfile, then you will need to explicitly set them in docker-compose.yml AND you will need to load them at the top of your Dockerfile. This example shows the syntax to use in .env, docker-compose.yml, and Dockerfile:\n# This excerpt is from .env\n# ...\n# The quotes here are important if you have any special \n# characters and want them to work on mac, pc and linux:\nARG_1=\"value of first argument\"\nARG_2=\"value of second argument\"\nARG_3=\"value of third argument\"\n# ...\n# This excerpt is from docker-compose.yml\n# ...\n# env_file (.env.staging) is loaded in the container at runtime:\n    env_file:\n      - .env.staging\n# ...\n    build:\n      context: ./\n      dockerfile: docker/app/Dockerfile\n# The following will let you use ${ARG_x} during your build in docker/app/Dockerfile\n      args:\n        ARG_1: ${ARG_1}\n        ARG_2: ${ARG_2}\n        ARG_3: ${ARG_3}\n# ...\n# This excerpt is from /docker/app/Dockerfile\nFROM somerepo/someimage:ver as something\n# Import variables from docker-compose.yml:\nARG ARG_1=$ARG_1\nARG ARG_2=$ARG_2\nARG ARG_3=$ARG_3\n# ...\n# Now you can use ${ARG_1}, ${ARG_2} etc to reference values from your .env file\n# ...",
    "Use buildx build linux/arm64 in docker-compose file": "You can use buildx in docker-compose by setting ENV variable COMPOSE_DOCKER_CLI_BUILD=1, also if buildx is not set as default, you should add DOCKER_BUILDKIT=1:\nCOMPOSE_DOCKER_CLI_BUILD=1 DOCKER_BUILDKIT=1 docker-compose build",
    "localhost not working docker windows 10": "Most likely a different application already runs at port 80. You'll have to forward your web site to a different port, eg:\ndocker run -d -p 5000:80 --name myapp myasp\nAnd point your browser to http://localhost:5000.\nWhen you start a container you specify which inner ports will be exposed as ports on the host through the -p option. -p 80:80 exposes the inner port 80 used by web sites to the host's port 80.\nDocker won't complain though if another application already listens at port 80, like IIS, another web application or any tool with a web interface that runs on 80 by default.\nThe solution is to:\nMake sure nothing else runs on port 80 or\nForward to a different port.\nForwarding to a different port is a lot easier.\nTo ensure that you can connect to a port, use the telnet command, eg :\ntelnet localhost 5000\nIf you get a blank window immediatelly, it means a server is up and running on this port. If you get a message and timeout after a while, it means nobody is running. You anc use this both to check for free ports and ensure you can connect to your container web app.\nPS I run into this just a week ago, as I was trying to set up a SQL Server container for tests. I run 1 default and 2 named instances already, and docker didn't complain at all when I tried to create the container. Took me a while to realize what was wrong.",
    "Dockerfile COPY from a Windows file system to a docker container": "First, change your Dockerfile to:\nFROM php:7.1-apache\nLABEL maintainer=\"rburton@agsource.com\"\nCOPY MyAgsourceAPI /var/www\nThen, to go your code directory: cd Users/rburton/code.\nWithin that directory, run: docker build -t <image_name> .",
    "Docker daemon memory leak due to logs from long running process": "There is still at least one outstanding issue relating to memory leaks with logs: https://github.com/docker/docker/issues/9139",
    "Install fonts in Linux container for ASP.NET Core": "Got it. Revise the start of your Dockerfile as follows:\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base\n\n#Add these two lines\nRUN sed -i'.bak' 's/$/ contrib/' /etc/apt/sources.list\nRUN apt-get update; apt-get install -y ttf-mscorefonts-installer fontconfig\n\nWORKDIR /app\nEXPOSE 80\n[...]\nThe first line updates the default /etc/apt/sources.list file in the Linux OS to include the 'contrib' archive area (which is where ttf-mscorefonts-installer lives). That ensures apt-get can find it and install it as normal in the second line (along with fontconfig, which you'll also need.)\nFor the record, this page suggested using the \"fonts-liberation\" package instead of ttf-mscorefonts-installer, which you can also get working with two different lines at the start of the Dockerfile as follows:\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base\n\n#Add these two lines for fonts-liberation instead\nRUN apt-get update; apt-get install -y fontconfig fonts-liberation\nRUN fc-cache -f -v\n\nWORKDIR /app\nEXPOSE 80\n\n[...]",
    "Iterate in RUN command in Dockerfile": "It looks like you're using backticks. What's in backticks gets executed and the text in the backticks gets replaced by what's returned by the results.\nTry using single quotes or double quotes instead of backticks.\nTry getting rid of the backticks like so:\nRUN for i in x y z; do echo \"$i\"; done",
    "ssh key generation using dockerfile": "The problem is that ssh-keygen is not available in your container yet. This can be easily solved, for example by installing the openssl-client package on a ubuntu base image.\nThe following Dockerfile does precisely that and places a key in the container's root folder\nFROM ubuntu:latest\n\nRUN apt-get -y install openssh-client\nRUN ssh-keygen -q -t rsa -N '' -f /id_rsa\nBUT READ THIS: My strong advice is not to place keys, certificates whatsoever into the container's file system at all! This might lead to strong security risks, as essentially anyone who obtains the container image can authenticate himself at services the key is valid for; it forces you to handle container images with the same care you would treat cryptographic keys and certificates!\nHence, it is advisable to keep the keys outside of the container. This can be easily achieved by using Docker VOLUMES; and you'd simply mount a volume holding keys/containers into the Docker container when launching it.\nCREATING KEYS OUTSIDE THE CONTAINER The following Dockerfile does instead create the key once the container is started, and it may be used to create the key outside the container's file system\nFROM ubuntu:latest\nRUN apt-get -y install openssh-client \nCMD ssh-keygen -q -t rsa -N '' -f /keys/id_rsa\nFirst, build the container with the following command:\ndocker build -t keygen-container .\nStarting the container using\ndocker run -v /tmp/:/keys keygen-container\nwill create a key on the host in /tmp.",
    "How to check whether python package is installed or not in Docker?": "I figured out.\ndocker exec <container ID> pip list",
    "Create table in PostgreSQL docker image": "In the docker-entrypoint.sh script of the official docker image of postgres is written:\npsql+=( --username \"$POSTGRES_USER\" --dbname \"$POSTGRES_DB\" )\n\n        echo\n        for f in /docker-entrypoint-initdb.d/*; do\n            case \"$f\" in\n                *.sh)     echo \"$0: running $f\"; . \"$f\" ;;\n                *.sql)    echo \"$0: running $f\"; \"${psql[@]}\" < \"$f\"; echo ;;\n                *.sql.gz) echo \"$0: running $f\"; gunzip -c \"$f\" | \"${psql[@]}\"; echo ;;\n                *)        echo \"$0: ignoring $f\" ;;\n            esac\n            echo\ndone\nSo every .sql file you want to execute inside your docker image can just be placed inside that folder. So my dockerfile looks like\nFROM postgres:9.3\nENV POSTGRES_USER docker\nENV POSTGRES_PASSWORD docker\nENV POSTGRES_DB docker\nADD CreateDB.sql /docker-entrypoint-initdb.d/\nAnd the content of my CreateDB.sql:\nCREATE TABLE web_origins (\n    client_id character varying(36) NOT NULL,\n    value character varying(255)\n);\nSo I just start my container with:\ndocker run -d my-postgres\nTo check:\ndocker exec -it 6250aee43d12 bash\nroot@6250aee43d12:/# psql -h localhost -p 5432 -U docker -d docker\npsql (9.3.13)\nType \"help\" for help.\n\ndocker=# \\c\nYou are now connected to database \"docker\" as user \"docker\".\ndocker=# \\dt\n           List of relations\n Schema |    Name     | Type  | Owner\n--------+-------------+-------+--------\n public | web_origins | table | docker\n(1 row)\nYou can find the details for mysql here in this blog.",
    "Bash brace expansion not working on Dockerfile RUN command": "You're not using brace expansion, because you're not using Bash. If you look at the documentation for the RUN command:\nRUN (shell form, the command is run in a shell, which by default is /bin/sh -c on Linux or cmd /S /C on Windows)\nAnd also:\nNote: To use a different shell, other than \u2018/bin/sh\u2019, use the exec form passing in the desired shell. For example, RUN [\"/bin/bash\", \"-c\", \"echo hello\"]\nSo, just change the command to use the exec form and explicitly use a Bash shell:\nRUN [ \"/bin/bash\", \"-c\", \"mkdir -p /opt/seagull/{diameter-env,h248-env,http-env,msrp-env,octcap-env,radius-env,sip-env,synchro-env,xcap-env}/logs\" ]",
    "TSC not found in Docker build": "It's probably a NODE_ENV environment variable problem.\nENV NODE_ENV=production\nIf you set this way, the dependencies in devDependencies will not be installed.",
    "How to get proper docker-compose Multiline environment variables formatting?": "In your first example the last element of the first sequence of the document is a plain scalar (i.e. not having single or double quotes) that extends over multiple lines. In a plain scalar newlines are replaced by spaces (and empty lines replaced by a newline).\nSo if you want newlines within that element you should use (only showing relevant part):\n  - WORDPRESS_DB_PASSWORD=xxxxxxxxxxxxxxx\n  - WORDPRESS_DEBUG=1\n  - WORDPRESS_CONFIG_EXTRA=\n\n      define( 'WP_REDIS_CLIENT', 'predis' );\n\n      define( 'WP_REDIS_SCHEME', 'tcp' );\n\n      define( 'WP_REDIS_HOST', 'redis' );\n\n      define( 'WP_REDIS_PORT', '6379' );\n\n      define( 'WP_REDIS_PASSWORD', 'xxxxxxxxxxxxxxx' );\n\n      define( 'WP_REDIS_DATABASE', '0' );\n\n      define( 'WP_REDIS_MAXTTL', '21600' );\n\n      define( 'WP_CACHE_KEY_SALT', 'xx_ ');\n\n      define( 'WP_REDIS_SELECTIVE_FLUSH', 'xx_ ');\n\n      define( 'WP_AUTO_UPDATE_CORE', false );\nvolumes:\n  - ./wordpress:/var/www/html\nor:\n  - WORDPRESS_DB_PASSWORD=xxxxxxxxxxxxxxx\n  - WORDPRESS_DEBUG=1\n  - |\n    WORDPRESS_CONFIG_EXTRA=\n    define( 'WP_REDIS_CLIENT', 'predis' );\n    define( 'WP_REDIS_SCHEME', 'tcp' );\n    define( 'WP_REDIS_HOST', 'redis' );\n    define( 'WP_REDIS_PORT', '6379' );\n    define( 'WP_REDIS_PASSWORD', 'xxxxxxxxxxxxxxx' );\n    define( 'WP_REDIS_DATABASE', '0' );\n    define( 'WP_REDIS_MAXTTL', '21600' );\n    define( 'WP_CACHE_KEY_SALT', 'xx_ ');\n    define( 'WP_REDIS_SELECTIVE_FLUSH', 'xx_ ');\n    define( 'WP_AUTO_UPDATE_CORE', false );\nvolumes:\n  - ./wordpress:/var/www/html\nUsing |- instead of | excludes the final newline from that element. What you tried ( WORDPRESS_CONFIG_EXTRA: | ) is something completely different, as you split the single scalar element into a mapping with a single key-value pair.\nAlthough the above load as string values with embedded newlines, it can still happen that the processing done by docker-compose, in particular passing things to a shell, can change the newlines into spaces.\nI have also used programs where, if you might have to escape the newline for the \"folllowing\" processing by ending each line with a backslash (\\)",
    "How to COPY/ADD file into current WORKDIR in Dockerfile": "It turns out to be very simple. I just need to use dot to copy to current workdir.\nCOPY local.conf .\nStill cannot figure out if this has some gotchas. But it just work as intended.",
    "How to fix: The feature watch recursively is unavailable on the current platform, which is being used to run Node.js": "Node v14 introduced a breaking change to the fs.watch() API, specifically that the recursive option (which has never been supported on Linux) now raises the ERR_FEATURE_UNAVAILABLE_ON_PLATFORM error if used on Linux.\nA bug report and fix have been submitted to filewatcher: https://github.com/fgnass/filewatcher/pull/6\nUntil that fix is merged and a new version released, you'll need to stick to NodeJS < v14, or override the filewatcher package installed locally to include that patch.",
    "Activate and switch Anaconda environment in Dockerfile during build": "You've got way too many RUN commands in your Dockerfile. It's not just that each RUN creates a new layer in the image. It's also that each RUN command starts a new shell, and conda activate applies only to the current shell.\nYou should combine logical groups of actions into a single RUN command. Use && to combine commands, and \\ to break lines for readability:\nRUN conda activate <myenv> \\\n && conda install <whatever> \\\n && ...\nKeep in mind: at the end of that RUN command, the shell will be gone. So if you want to do something else to that conda environment afterwards, you've got to run conda activate again, or else use -n <myenv> to put something into an environment without activating it first.\nWhen you start a container from the image, you will also have to call conda activate inside the container.",
    "How to mount a directory in a Docker container to the host?": "First, a little information about Docker volumes. Volume mounts occur only at container creation time. That means you cannot change volume mounts after you've started the container. Also, volume mounts are one-way only: From the host to the container, and not vice-versa. When you specify a host directory mounted as a volume in your container (for example something like: docker run -d --name=\"foo\" -v \"/path/on/host:/path/on/container\" ubuntu), it is a \"regular ole\" linux mount --bind, which means that the host directory will temporarily \"override\" the container directory. Nothing is actually deleted or overwritten on the destination directory, but because of the nature of containers, that effectively means it will be overridden for the lifetime of the container.\nSo, you're left with two options (maybe three). You could mount a host directory into your container and then copy those files in your startup script (or if you bring cron into your container, you could use a cron to periodically copy those files to that host directory volume mount).\nYou could also use docker cp to move files from your container to your host. Now that is kinda hacky and definitely not something you should use in your infrastructure automation. But it does work very well for that exact purpose. One-off or debugging is a great situation for that.\nYou could also possibly set up a network transfer, but that's pretty involved for what you're doing. However, if you want to do this regularly for your log files (or whatever), you could look into using something like rsyslog to move those files off your container.",
    "Docker: How to update your container when your code changes": "Even though there are multiple good answers to this question, I think they missed the point, as the OP is asking about the local dev environment. The command I usually use in this situation is:\ndocker-compose up -d --build\nIf there aren't any errors in Dockerfile, it should rebuild all the images before bringing up the stack. It could be used in a shell script if needed.\n#!/bin/bash\n\nsudo docker-compose up -d --build\nIf you need to tear down the whole stack, you can have another script:\n#!/bin/bash\n\nsudo docker-compose down -v\nThe -v flag removes all the volumes so you can have a fresh start.\nNOTE: In some cases, sudo might not be needed to run the command.",
    "How to use wait-for-it in docker-compose file?": "Generally you wouldn't put it in your docker-compose.yml file at all. The script needs to be built into the image, and its standard startup sequence needs to know to run it.\nThere's a fairly common pattern of using an entrypoint script to do some initial setup, and then use exec \"$@\" to run the container's command as the main process. This lets you, for example, use the wait-for-it.sh script to wait for the backend to be up, then run whatever the main command happens to be. For example, a docker-entrypoint.sh script might look like:\n#!/bin/sh\n\n# Abort on any error (including if wait-for-it fails).\nset -e\n\n# Wait for the backend to be up, if we know where it is.\nif [ -n \"$CUSTOMERS_HOST\" ]; then\n  /usr/src/app/wait-for-it.sh \"$CUSTOMERS_HOST:${CUSTOMERS_PORT:-6000}\"\nfi\n\n# Run the main container command.\nexec \"$@\"\nYou need to make sure this script is executable, and make it the image's ENTRYPOINT, but otherwise you can leave the Dockerfile pretty much unchanged.\nFROM node:12.14.0\nWORKDIR /usr/src/app\nCOPY package*.json ./\n# Do this _before_ copying the entire application in\n# to avoid repeating it on rebuild\nRUN npm install\n# Includes wait-for-it.sh and docker-entrypoint.sh\nCOPY . ./\nRUN chmod +x ./wait-for-it.sh ./docker-entrypoint.sh\nEXPOSE 4555\n# Add this line\n# It _must_ use the JSON-array syntax\nENTRYPOINT [\"./docker-entrypoint.sh\"]\nCMD [\"node\", \"main.js\"]\nIn your docker-compose.yml you need to add the configuration to say where the backend is, but you do not need to override command:.\nmain:\n  depends_on: \n    - customers\n  build: './main'\n  ports:\n    - \"4555:4555\"\n  environment:\n    - CUSTOMERS_HOST=customers",
    "Installing specific version of NodeJS and NPM on Alpine docker image": "Use official versions\nI don't know why you are using NVM.\nAccording to your words. It doesn't matter how but you have to install a specific version of node on a specific version of alpine!\nJust change NODE_VERSION and ALPINE_VERSION to what you need.\nHere's my way using an alpine image not a node one:\nARG NODE_VERSION=18.16.0\nARG ALPINE_VERSION=3.17.2\n\nFROM node:${NODE_VERSION}-alpine AS node\n\nFROM alpine:${ALPINE_VERSION}\n\nCOPY --from=node /usr/lib /usr/lib\nCOPY --from=node /usr/local/lib /usr/local/lib\nCOPY --from=node /usr/local/include /usr/local/include\nCOPY --from=node /usr/local/bin /usr/local/bin\n\nRUN node -v\n\nRUN npm install -g yarn --force\n\nRUN yarn -v\n\nCMD [\"node\", \"path/to/your/script.js\"]\nI removed your unnecessary package installations but you can add them if you need them!\ne.g.: RUN apk add bash git helm openssh yq github-cli\ne.g.: RUN rc-update add docker boot\nExplanation\nAdd NODE_VERSION as ARG so you can use it in everywhere!\nCopy binary files from official build of node:<version>-alpine to your version of alpine!\nSimply install yarn using --force flag to avoid errors and DONE!!!!\nCMD path/to/your/script.js is enough to run your long running JS. Don't use additional tools!\nCompile the official versions\nYou can use the code provided by node it self from here. Just change the versions in the first two lines!\nFROM alpine:3.17.2\n\nENV NODE_VERSION 18.16.0\n\nRUN addgroup -g 1000 node \\\n    && adduser -u 1000 -G node -s /bin/sh -D node \\\n    && apk add --no-cache \\\n        libstdc++ \\\n    && apk add --no-cache --virtual .build-deps \\\n        curl \\\n    && ARCH= && alpineArch=\"$(apk --print-arch)\" \\\n      && case \"${alpineArch##*-}\" in \\\n        x86_64) \\\n          ARCH='x64' \\\n          CHECKSUM=\"f3ad9443e8d9d53bfc00ec875181e9dc2ccf86205a50fce119e0610cdba8ccf1\" \\\n          ;; \\\n        *) ;; \\\n      esac \\\n  && if [ -n \"${CHECKSUM}\" ]; then \\\n    set -eu; \\\n    curl -fsSLO --compressed \"https://unofficial-builds.nodejs.org/download/release/v$NODE_VERSION/node-v$NODE_VERSION-linux-$ARCH-musl.tar.xz\"; \\\n    echo \"$CHECKSUM  node-v$NODE_VERSION-linux-$ARCH-musl.tar.xz\" | sha256sum -c - \\\n      && tar -xJf \"node-v$NODE_VERSION-linux-$ARCH-musl.tar.xz\" -C /usr/local --strip-components=1 --no-same-owner \\\n      && ln -s /usr/local/bin/node /usr/local/bin/nodejs; \\\n  else \\\n    echo \"Building from source\" \\\n    # backup build\n    && apk add --no-cache --virtual .build-deps-full \\\n        binutils-gold \\\n        g++ \\\n        gcc \\\n        gnupg \\\n        libgcc \\\n        linux-headers \\\n        make \\\n        python3 \\\n    # use pre-existing gpg directory, see https://github.com/nodejs/docker-node/pull/1895#issuecomment-1550389150\n    && export GNUPGHOME=\"$(mktemp -d)\" \\\n    # gpg keys listed at https://github.com/nodejs/node#release-keys\n    && for key in \\\n      4ED778F539E3634C779C87C6D7062848A1AB005C \\\n      141F07595B7B3FFE74309A937405533BE57C7D57 \\\n      74F12602B6F1C4E913FAA37AD3A89613643B6201 \\\n      DD792F5973C6DE52C432CBDAC77ABFA00DDBF2B7 \\\n      61FC681DFB92A079F1685E77973F295594EC4689 \\\n      8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600 \\\n      C4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8 \\\n      890C08DB8579162FEE0DF9DB8BEAB4DFCF555EF4 \\\n      C82FA3AE1CBEDC6BE46B9360C43CEC45C17AB93C \\\n      108F52B48DB57BB0CC439B2997B01419BD92F80A \\\n    ; do \\\n      gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys \"$key\" || \\\n      gpg --batch --keyserver keyserver.ubuntu.com --recv-keys \"$key\" ; \\\n    done \\\n    && curl -fsSLO --compressed \"https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION.tar.xz\" \\\n    && curl -fsSLO --compressed \"https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc\" \\\n    && gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \\\n    && gpgconf --kill all \\\n    && rm -rf \"$GNUPGHOME\" \\\n    && grep \" node-v$NODE_VERSION.tar.xz\\$\" SHASUMS256.txt | sha256sum -c - \\\n    && tar -xf \"node-v$NODE_VERSION.tar.xz\" \\\n    && cd \"node-v$NODE_VERSION\" \\\n    && ./configure \\\n    && make -j$(getconf _NPROCESSORS_ONLN) V= \\\n    && make install \\\n    && apk del .build-deps-full \\\n    && cd .. \\\n    && rm -Rf \"node-v$NODE_VERSION\" \\\n    && rm \"node-v$NODE_VERSION.tar.xz\" SHASUMS256.txt.asc SHASUMS256.txt; \\\n  fi \\\n  && rm -f \"node-v$NODE_VERSION-linux-$ARCH-musl.tar.xz\" \\\n  && apk del .build-deps \\\n  # Run some smoke tests\n  && node --version \\\n  && npm --version\n\nENV YARN_VERSION 1.22.19\n\nRUN apk add --no-cache --virtual .build-deps-yarn curl gnupg tar \\\n  # use pre-existing gpg directory, see https://github.com/nodejs/docker-node/pull/1895#issuecomment-1550389150\n  && export GNUPGHOME=\"$(mktemp -d)\" \\\n  && for key in \\\n    6A010C5166006599AA17F08146C2130DFD2497F5 \\\n  ; do \\\n    gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys \"$key\" || \\\n    gpg --batch --keyserver keyserver.ubuntu.com --recv-keys \"$key\" ; \\\n  done \\\n  && curl -fsSLO --compressed \"https://yarnpkg.com/downloads/$YARN_VERSION/yarn-v$YARN_VERSION.tar.gz\" \\\n  && curl -fsSLO --compressed \"https://yarnpkg.com/downloads/$YARN_VERSION/yarn-v$YARN_VERSION.tar.gz.asc\" \\\n  && gpg --batch --verify yarn-v$YARN_VERSION.tar.gz.asc yarn-v$YARN_VERSION.tar.gz \\\n  && gpgconf --kill all \\\n  && rm -rf \"$GNUPGHOME\" \\\n  && mkdir -p /opt \\\n  && tar -xzf yarn-v$YARN_VERSION.tar.gz -C /opt/ \\\n  && ln -s /opt/yarn-v$YARN_VERSION/bin/yarn /usr/local/bin/yarn \\\n  && ln -s /opt/yarn-v$YARN_VERSION/bin/yarnpkg /usr/local/bin/yarnpkg \\\n  && rm yarn-v$YARN_VERSION.tar.gz.asc yarn-v$YARN_VERSION.tar.gz \\\n  && apk del .build-deps-yarn \\\n  # smoke test\n  && yarn --version\n\nCOPY docker-entrypoint.sh /usr/local/bin/\nENTRYPOINT [\"docker-entrypoint.sh\"]\n\nCMD [ \"node\" ]",
    "Install ODBC driver in Alpine Linux Docker Container": "I was facing the same issue. I solved this issue by adding RUN apk update before RUN apk add commands.(I was using python:3.6-alpine)\nDockerfile\nFROM python:3.6-alpine\nRUN apk update\nRUN apk add gcc libc-dev g++ libffi-dev libxml2 unixodbc-dev mariadb-dev postgresql-dev",
    "Docker .Net 6 error Program does not contain a static 'Main' method suitable for an entry point": "The underlying issue is that it's not finding files where it's been instructed to. See the bit in your Dockerfile where it says\nCOPY . . ?\nMove the\nWORKDIR \"/src/xxxxxx\"\nline from below it to above it.\nThis will copy the restored packages from the preceding line to correct directory before attempting the\nRUN dotnet build \"xxxxx.csproj\" -c Release -o /app/build\nand that should now work",
    "docker python custom module not found": "You set PYTHONPATH to /test_project/utils. When trying resolve the module utils, it is looking for one of:\nfile /test_project/utils/utils.py\ndirectory /test_project/utils/utils/ that contains __init__.py.\nIt looks like you have this?\nutils/math.py\nutils/logger.py\nI wonder if what you really mean to do is\n# different path...\nENV PYTHONPATH /test_project\n\nfrom utils import math\nfrom utils import logger",
    "How to share & WebRTC stream from /dev/videoX device from a Chromium on host and Chromium in a docker container": "If you just want separate Chrome sessions you can simply start it with:\nchromium-browser --user-data-dir=/tmp/chrome1 \nand another instance with\nchromium-browser --user-data-dir=/tmp/chrome2\nDocker is just a way to document and repeat setting up of a very specific environment. The additional layer of security that it adds in minimal, especially with the extra permissions you're passing on and especially in comparison to Chrome's really well tested security.\nIf you need a bit more isolation, you can create separate users:\n # run these lines individualy, just press Enter for everything, don't set passwords for them, they won't be able to log in. \n sudo adduser chrome1\n sudo adduser chrome2 \n\n # if you want to give each access to only one of the cams you can try this\n sudo chown chrome1:chrome1 /dev/video0 \n sudo chown chrome2:chrome2 /dev/video1\n # keeping in mind unplugging and replugging the camera might reset the permissions unless you update the relevant /etc files\n\n # to allow anyone to use your X\n xhost +\n\n # run the two separate browsers\n sudo su - chrome1 -c chromium-browser & \n sudo su - chrome2 -c chromium-browser &",
    "Docker run script in host on docker-compose up": "I just wish to know the best practices and examples of how to run a script on HOST from INSIDE a CONTAINER, so that the deploy can be as easy for the installing operator to just run docker-compose up\nIt seems that there is no best practice that can be applied to your case. A workaround proposed here: How to run shell script on host from docker container? is to use a client/server trick.\nThe host should run a small server (choose a port and specify a request type that you should be waiting for)\nThe container, after it starts, should send this request to that server\nThe host should then run the script / trigger the changes you want\nThis is something that might have serious security issues, so use at your own risk.",
    "Mongorestore in a Dockerfile": "With help from this answer, Marc Young's answer, and the Dockerfile reference I was able to get this working.\nDockerfile\nFROM mongo\n\nCOPY dump /home/dump\nCOPY mongo.sh /home/mongo.sh\nRUN chmod 777 /home/mongo.sh\n\nCMD /home/mongo.sh\nmongo.sh\n#!/bin/bash\n\n# Initialize a mongo data folder and logfile\nmkdir -p /data/db\ntouch /var/log/mongodb.log\nchmod 777 /var/log/mongodb.log\n\n# Start mongodb with logging\n# --logpath    Without this mongod will output all log information to the standard output.\n# --logappend  Ensure mongod appends new entries to the end of the logfile. We create it first so that the below tail always finds something\n/entrypoint.sh mongod --logpath /var/log/mongodb.log --logappend &\n\n# Wait until mongo logs that it's ready (or timeout after 60s)\nCOUNTER=0\ngrep -q 'waiting for connections on port' /var/log/mongodb.log\nwhile [[ $? -ne 0 && $COUNTER -lt 60 ]] ; do\n    sleep 2\n    let COUNTER+=2\n    echo \"Waiting for mongo to initialize... ($COUNTER seconds so far)\"\n    grep -q 'waiting for connections on port' /var/log/mongodb.log\ndone\n\n# Restore from dump\nmongorestore --drop /home/dump\n\n# Keep container running\ntail -f /dev/null",
    "What is a Dockerfile.dev and how is it different from Dockerfile": "It is a common practice to have seperate Dockerfiles for deployments and development systems.\nYou can define a non default dockerfile while building:\ndocker build -f Dockerfile.dev -t devimage .\nOne image could use a compiled version of the source, and a other image could mount the /src folder into the system for live updates.",
    "docker push fails : manifest invalid": "Ran into the same problem. The issue was that repo does not let you over-write images tags. I changed the tag to a new major version.",
    "Huge files in Docker containers": "Is there a better way of referencing such files?\nIf you already have some way to distribute the data I would use a \"bind mount\" to attach a volume to the containers.\ndocker run -v /path/to/data/on/host:/path/to/data/in/container <image> ...\nThat way you can change the image and you won't have to re-download the large data set each time.\nIf you wanted to use the registry to distribute the large data set, but want to manage changes to the data set separately, you could use a data volume container with a Dockerfile like this:\nFROM tianon/true\nCOPY dataset /dataset\nVOLUME /dataset\nFrom your application container you can attach that volume using:\ndocker run -d --name dataset <data volume image name>\ndocker run --volumes-from dataset <image> ...\nEither way, I think https://docs.docker.com/engine/tutorials/dockervolumes/ are what you want.",
    "How to silent install Postgresql in Ubuntu via. Dockerfile?": "add this to your Dockerfile\nARG DEBIAN_FRONTEND=noninteractive\nbefore installing postgresql\nand I think you may want to use apt-get instead of apt to avoid this warning:\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.",
    "Create files / folders on docker-compose build or docker-compose up": "The image does contain those files\nThe Dockerfile contains instructions on how to build an image. The image you built from that Dockerfile does contain index.html and images/.\nBut, you over-rode them in the container\nAt runtime, you created a container from the image you built. In that container, you mounted the external directory ./docroot as /var/www/html.\nA mount will hide whatever was at that path before, so this mount will hide the prior contents of /var/www/html, replacing them with whatever is in ./docroot.\nPutting stuff in your mount\nIn the comments you asked\nis there a possibility then to first mount and then create files or something? Or is that impossible?\nThe way you have done things, you mounted over your original files, so they are no longer accessible once the container is created.\nThere are a couple of ways you can handle this.\nChange their path in the image\nIf you put these files in a different path in your image, then they will not be overwritten by the mount.\nWORKDIR /var/www/alternate-html\n\nRUN touch index.html \\\n    && mkdir images\n\nWORKDIR /var/www/html\nNow, at runtime you will still have this mount at /var/www/html, which will contain the contents from the external directory. Which may or may not be an empty directory. You can tell the container on startup to run a script and copy things there, if that's what you want.\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod 0755 /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\n(This is assuming you do not have a defined entrypoint - if you do, you'll maybe just need to adjust your existing script instead.)\nentrypoint.sh:\n#!/bin/sh\n\ncp -r /var/www/alternate-html/* /var/www/html\nexec \"$@\"\nThis will run the cp command, and then hand control over to whatever the CMD for this image is.\nHandling it externally\nYou also have the option of simply pre-populating the files you want into ./docroot externally. Then they will just be there when the container starts and adds the directory mount.",
    "Docker container not updating on code change": "When you make a change, you need to run docker-compose up --build. That will rebuild your image and restart containers as needed.\nDocker has no facility to detect code changes, and it is not intended as a live-reloading environment. Volumes are not intended to hold code, and there are a couple of problems people run into attempting it (Docker file sync can be slow or inconsistent; putting a node_modules tree into an anonymous volume actively ignores changes to package.json; it ports especially badly to clustered environments like Kubernetes). You can use a host Node pointed at your Docker MongoDB for day-to-day development, and still use this Docker-based setup for deployment.",
    "Docker build command with --tag unable to tag images": "Okay! I found out the reason for issue.\nDOCKER BUILD PROCESS\nWhen we build a docker image, while creating an image, several other intermediate images are generated in the process. We never see them in docker images because with the generation of next intermediate image the earlier image is removed. And in the end we have only one which is the final image.\nThe tag we provide using -t or --tag is for the final build, and obviously no intermediate container is tagged with the same.\nISSUE EXPLANATION\nWhen we try to build a docker image with Dockerfile sometimes the process is not successfully completed with a similar message like Successfully built image with IMAGEID\nSo it is so obvious that the build which has failed will not be listed in docker images\nNow, the image with tag <none> is some other image (intermediate). This creates a confusion that the image exists but without a tag, but the image is actually not what the final build should be, hence not tagged.",
    "How to install Python on nodejs Docker image": "In fact, this is not a docker question, just a debian question. You need always do apt-get update before install package. So, for you scenario, it should be:\nRUN apt-get update || : && apt-get install python -y\nAs per your comments:\nW: Failed to fetch http://deb.debian.org/debian/dists/jessie-updates/InRelease Unable to find expected entry 'main/binary-amd64/Packages' in Release file (Wrong sources.list entry or malformed file) E: Some index files failed to download. They have been ignored, or old ones used instead. The command '/bin/sh -c apt-get update && apt-get install python -y' returned a non-zero code: 100\nSo, you can add || : after apt-get to ignore the error, as at that time python meta data already finished downloading with other previous url hit, so you can bypass the error.\nUpdate:\nA whole workable solution in case you need to compare:\na.py:\nprint(\"success\")\nindex.js:\nconst spawn = require(\"child_process\").spawn;\nconsole.log('PATH:::::');\n\nconsole.log(process.env.PATH);\nconst pythonProcess = spawn('python', ['/app/a.py']);\npythonProcess.stdout.on('data', (data) => {\n    console.log('DATA::::');\n    console.log(data.toString());\n});\n\npythonProcess.stderr.on('data', (data) => {\n    console.log(\"wow\");\n    console.log(data.toString());\n});\nDockerfile:\nFROM node:9-slim\n\nRUN apt-get update || : && apt-get install python -y\n\nWORKDIR /app\nCOPY . /app\nCMD [\"node\", \"index.js\"]\nTry command:\norange@orange:~/gg$ docker build -t abc:1 .\nSending build context to Docker daemon  4.096kB\n...\nSuccessfully built 756b13952760\nSuccessfully tagged abc:1\n\norange@orange:~/gg$ docker run abc:1\nPATH:::::\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nDATA::::\nsuccess",
    "Extract unit test results from multi-stage Docker build (.NET Core 2.0)": "Thanks for your question - I needed to solve the same thing.\nI added a separate container stage based on results of the build. The tests and its output are all handled in there so they never reach the final container. So build-env is used to build and then an intermediate test container is based on that build-env image and final is based on runtime container with results of build-env copied in.\n# ---- Test ----\n# run tests and capture results for later use. This use the results of the build stage\nFROM build AS test\n#Use label so we can later obtain this container from the multi-stage build\nLABEL test=true\nWORKDIR /\n#Store test results in a file that we will later extract \nRUN dotnet test --results-directory ../../TestResults/ --logger \"trx;LogFileName=test_results.xml\" \"./src/ProjectNameTests/ProjectNameTests.csproj\"\nI added a shell script as a next step that then tags the image as project-test.\n#!bin/bash\nid=`docker images --filter \"label=test=true\"  -q`\ndocker tag $id projectname-test:latest\nAfter that, I basically do what you do which is use docker cp and get the file out. The difference is my test results were never in the final image so I don't touch the final image.\nOverall I think the correct way to handle tests is probably create a test image (based on the build image) and run it with a mounted volume for test results and have it run the unit tests when that container starts. Having a proper image/container would also allow you to run integration tests etc. This article is older but details similar https://blogs.infosupport.com/build-deploy-test-aspnetcore-docker-linux-tfs2015/",
    "How does docker image size impact runtime characteristics?": "The size of the image it's only size of the directories. So it will never take more CPU or RAM (if you did not delete something, that will be loaded in RAM at startup from 'A') And a few words about pulling process: the image will be pulled from registry only first time and will be cached locally. Once you pull base image ('A') only differences will be pulled for image 'B'",
    "Docker - ERROR: failed to register layer: symlink": "To solve this issue, you just Stop and Start docker service from terminal.\n# service docker stop\n# service docker start",
    "Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? inside a Dockerfile": "Try this cmd:\nsudo service docker restart",
    "Running a Docker container that accept traffic from the host": "It is saying port 80 is busy ... run this to see who is using port 80\nsudo netstat -tlnp | grep 80 # sudo apt-get install net-tools # to install netstat\n\ntcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1380/nginx -g daemo\ntcp6       0      0 :::80                   :::*                    LISTEN      1380/nginx -g daemo\nscroll to far right to see offending PID of process holding port 80 ... its PID 1380 so lets do a process list to see that pid\nps -eaf | grep 1380\n\nroot      1380     1  0 11:33 ?        00:00:00 nginx: master process /usr/sbin/nginx -g daemon on; master_process on;\nso teardown that offending process to free up the port 80\nsudo kill 1380  # if you know the pid ( 1380 for example )\n__ or __\nsudo fuser -k 80/tcp #  just kill whatever pid is using port 80 tcp\nIf after doing above its still saying busy then probably the process which you killed got auto relaunched in which case you need to kill off its watcher however you can walk up the process tree from netstat output to identify this parent process and kill that too\nHere is how to identify the parent pid of a given process pid\nps -eafww\n\neve         2720    2718  0 07:56 ?        00:00:00 /usr/share/skypeforlinux/skypeforlinux --type=zygote\nin above pid is 2720 and its parent will be the next column to right pid 2718 ... there are commands to show a process tree to visualize these relationships\nps -x --forest  \nor\npstree  -p\nwith sample output of\nsystemd(1)\u2500\u252c\u2500ModemManager(887)\u2500\u252c\u2500{ModemManager}(902)\n           \u2502                   \u2514\u2500{ModemManager}(906)\n           \u251c\u2500NetworkManager(790)\u2500\u252c\u2500{NetworkManager}(872)\n           \u2502                     \u2514\u2500{NetworkManager}(877)\n           \u251c\u2500accounts-daemon(781)\u2500\u252c\u2500{accounts-daemon}(792)\n           \u2502                      \u2514\u2500{accounts-daemon}(878)\n           \u251c\u2500acpid(782)\n           \u251c\u2500avahi-daemon(785)\u2500\u2500\u2500avahi-daemon(841)\n           \u251c\u2500colord(1471)\u2500\u252c\u2500{colord}(1472)\n           \u2502              \u2514\u2500{colord}(1475)\n           \u251c\u2500containerd(891)\u2500\u252c\u2500containerd-shim(1836)\u2500\u252c\u2500registry(1867)\u2500\u252c\u2500{registry}(1968)\n           \u2502                 \u2502                       \u2502                \u251c\u2500{registry}(1969)\n           \u2502                 \u2502                       \u2502                \u251c\u2500{registry}(1970)",
    "OCI runtime exec failed: exec failed: container_linux.go:344: starting container process": "Before reading this answer just let you know, it's my 2nd day of learning docker, It may not be the perfect help for you.\nThis error may also occur when the ping package is not installed in the container, I resolved the problem as follow, bash into the container like this\ndocker container exec -it my_nginx /bin/bash\nthen install ping package\napt-get update\napt-get install inetutils-ping\nThis solved my problem.",
    "Add shell or bash to a docker image (Distroless based on Debian GNU/Linux)": "You can do it by copying the statically compiled shell from official busybox image in a multi-stage build in your Dockerfile. Or just COPY --from it.\nThe static shell doesn't have too many dependencies, so it will work for a range of different base images. It may not work for some advanced cases, but otherwise it gets the job done.\nThe statically compiled shell is tagged with uclibc. Depending on your base image you may have success with other flavours of busybox as well.\nExample:\nFROM busybox:1.35.0-uclibc as busybox\n\nFROM gcr.io/distroless/base-debian11\n\n# Now copy the static shell into base image.\nCOPY --from=busybox /bin/sh /bin/sh\n\n# You may also copy all necessary executables into distroless image.\nCOPY --from=busybox /bin/mkdir /bin/mkdir\nCOPY --from=busybox /bin/cat /bin/cat\n\nENTRYPOINT [\"/bin/sh\", \"/entrypoint.sh\"]\nThe single-line COPY --from directly from image would also work:\nFROM gcr.io/distroless/base-debian11\n\nCOPY --from=busybox:1.35.0-uclibc /bin/sh /bin/sh\n\nENTRYPOINT [\"/bin/sh\", \"/entrypoint.sh\"]",
    "generate a self signed certificate in docker": "What is wrong with simple RUN command? It works for me and the self-signed certificate is created successfully.\nFROM debian:wheezy\n\nRUN apt-get update && \\\n    apt-get install -y openssl && \\\n    openssl genrsa -des3 -passout pass:x -out server.pass.key 2048 && \\\n    openssl rsa -passin pass:x -in server.pass.key -out server.key && \\\n    rm server.pass.key && \\\n    openssl req -new -key server.key -out server.csr \\\n        -subj \"/C=UK/ST=Warwickshire/L=Leamington/O=OrgName/OU=IT Department/CN=example.com\" && \\\n    openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt\nOnce in Dockerfile, the certificate is created only once during the image build; then you have the certificate available in the image.\nIf you need a new self-signed certificate each time a container starts, it's possible with the use of an external shell script. Like so:\n#!/bin/bash\n\nopenssl genrsa -des3 -passout pass:x -out server.pass.key 2048\nopenssl rsa -passin pass:x -in server.pass.key -out server.key\nrm server.pass.key\nopenssl req -new -key server.key -out server.csr \\\n    -subj \"/C=UK/ST=Warwickshire/L=Leamington/O=OrgName/OU=IT Department/CN=example.com\"\nopenssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt\nAnd then put that shell script into your Dockerfile and set up the default execution:\nFROM debian:wheezy\n\nRUN apt-get update && \\\n    apt-get install -y openssl\n\nCOPY generate-certificate.sh /tmp/generate-certificate.sh\n\nCMD [ \"/tmp/generate-certificate.sh\" ]\nIn this case each time you start a container with docker run ...., a new unique certificate is generated.",
    "Using SSH agent with Docker Compose and Dockerfile": "They have added the ssh flag as option to the build key in compose: https://github.com/compose-spec/compose-spec/pull/234\nservices:\n  sample:\n    build:\n      context: .\n      ssh:\n        - default",
    "docker RUN append to /etc/hosts in Dockerfile not working [duplicate]": "Docker will generate /etc/hosts dynamically every time you create a new container. So that it can link others. You can use --add-host option:\ndocker run --add-host www.domain.com:8.8.8.8 ubuntu ping www.domain.com",
    "Dockerfile: understanding VOLUME instruction": "A container's volumes are just directories on the host regardless of what method they are created by. If you don't specify a directory on the host, Docker will create a new directory for the volume, normally under /var/lib/docker/vfs.\nHowever the volume was created, it's easy to find where it is on the host by using the docker inspect command e.g:\n$ ID=$(docker run -d -v /data debian echo \"Data container\")\n$ docker inspect -f {{.Mounts}} $ID\n[{0d7adb21591798357ac1e140735150192903daf3de775105c18149552a26f951 /var/lib/docker/volumes/0d7adb21591798357ac1e140735150192903daf3de775105c18149552a26f951/_data /data local  true }]\n \nWe can see that Docker has created a directory for the volume at /var/lib/docker/volumes/0d7adb21591798357ac1e140735150192903daf3de775105c18149552a26f951/_data.\nYou are free to modify/add/delete files in this directory from the host, but note that you may need to use sudo for permissions.\nDocker will only delete volume directories in two circumstances:\nIf the --rm option is given to docker run, any volumes will be deleted when the container exits\nIf a container is deleted with docker rm -v CONTAINER, any volumes will be removed.\nIn both cases, volumes will only be deleted if no other containers refer to them. Volumes mapped to specific host directories (the -v HOST_DIR:CON_DIR syntax) are never deleted by Docker. However, if you remove the container for a volume, the naming scheme means you will have a hard time figuring out which directory contains the volume.\nSo, specific questions:\nYes and yes, with above caveats.\nEach Docker managed volume gets a new directory on the host\nThe VOLUME instruction is identical to -v without specifying the host dir. When the host dir is specified, Docker does not create any directories for the volume, will not copy in files from the image and will never delete the volume (docker rm -v CONTAINER will not delete volumes mapped to user-specified host directories).\nMore information here:\nhttps://blog.container-solutions.com/understanding-volumes-docker",
    "How to cat a file inside a docker image?": "This seems to work dependably for me as it resolves the entrypoint conflict and assures output to stdout. It also kills the container immediately after gathering the data to keep things clean, almost as good as not running it at all. I hope it'll help others.\ndocker run -it --rm -a stdout --entrypoint cat <image> <filename>\nAlso easy to alias if you do this a lot. Add the first line to your ~/.bash_aliases or ~/.bashrc.\n$ alias dcat='docker run -it --rm -a stdout --entrypoint cat'\n$ dcat <image> <filename>",
    "Docker, how to run .sql file in an image?": "You can load the sql file during the build phase of the image. To do this you create a Dockerfile for the db service that will look something like this:\nFROM mysql:5.6\nCOPY setup.sh /mysql/setup.sh\nCOPY setup.sql /mysql/setup.sql\nRUN /mysql/setup.sh\nwhere setup.sh looks something like this:\n#!/bin/bash\nset -e\nservice mysql start\nmysql < /mysql/setup.sql\nservice mysql stop\nAnd in your docker-compose.yml you'd change image to build: ./db or the path where you put your files.\nNow this works if you have all your sql in a raw .sql file, but this wont be the case if you're using rails or a similar framework where the sql is actually stored in code. This leaves you with two options.\nInstead of using FROM mysql:5.6 you can use FROM your_app_image_that_has_the_code_in_it and apt-get install mysql .... This leaves you with a larger image that contains both mysql and your app, allowing you to run the ruby commands above. You'd replace the mysql < /mysql/setup/sql with the rails-app bundle exec rake db:create lines. You'd also have to provide an app config that hits a database on localhost:3306 instead of db:3306\nMy preferred option is to create a script which exports the sql into a .sql file, which you can then use to build your database container. This is a bit more work, but is a lot nicer. It means that instead of running rails-app bundle exec rake db:create you'd just run the script to load a db.\nSuch a script would look something like this:\n#!/bin/bash\nset -e\ndocker-compose build rails-app\ndocker run -d --name mysql_empty mysql:5.6\ndocker run --link mysql_empty:db -v $PWD:/output project_rails-app export.sh\nwhere export.sh looks something like this:\n#!/bin/bash\nset -e\nRAILS_ENV=development\nrails-app bundle exec rake db:create\nmysqldump > /output/setup.sql\nYou could also replace the docker run script with a second compose file if you wanted to.",
    "What is the practical purpose of VOLUME in Dockerfile?": "Instructions like VOLUME and EXPOSE are a bit anachronistic. Named volumes as we know them today were introduced in Docker 1.9, almost three years ago.\nBefore Docker 1.9, running a container whose image had one or more VOLUME instructions (or using the --volume option) was the only way to create volumes for data sharing or persistence. In fact, it used to be a best practice to create data-only containers whose sole purpose was to hold one or more volumes, and then share those volumes with your application containers using the --volumes-from option. Here's some articles that describe this outdated pattern.\nDocker Data Containers\nWhy Docker Data Containers (Volumes!) are Good\nAlso, check out moby/moby#17798 (Data-only containers obsolete with docker 1.9.0?) where the change from data-only containers to named volumes was discussed.\nToday, I consider the VOLUME instruction as an advanced tool that should only be used for specialized cases, and after careful thought. For example, the official postgres image declares a VOLUME at /var/lib/postgresql/data. This can improve the performance of postgres containers out of the box by keeping the database data out of the layered filesystem. Docker doesn't have to search through all the layers of the container image for file requests at /var/lib/postgresql/data.\nHowever, the VOLUME instruction does come at a cost.\nUsers might not be aware of the unnamed volumes being created, and continuing to take up storage space on their Docker host after containers are removed.\nThere is no way to remove a volume declared in a Dockerfile. Downstream images cannot add data to paths where volumes exist.\nThe latter issue results in problems like these.\nHow to \u201cundeclare\u201d volumes in docker image?\nGitLab on Docker: how to persist user data between deployments?\nFor the GitLab question, someone wants to extend the GitLab image with pre-configured data for testing purposes, but it's impossible to commit that data in a downstream image because of the VOLUME at /var/opt/gitlab in the parent image.\ntl;dr: VOLUME was designed for a world before Docker 1.9. Best to just leave it out.",
    "How to pass parameters to a .NET core project with dockerfile": "You can do this with a combination of ENTRYPOINT to set the command, and CMD to set default options.\nExample, for an ASP.NET Core app:\nENTRYPOINT [\"dotnet\", \"app.dll\"]\nCMD [\"argument\"]\nIf you run the container with no command, it will execute this command when the container starts:\ndotnet app.dll argument\nAnd the args array will have one entry, \"argument\". But you can pass a command o docker run to override the CMD definition:\ndocker run app arg1 arg2",
    "How to create postgres database and run migration when docker-compose up": "entrypoint.sh (in here I get createdb: command not found)\nRunning createdb in the nodejs container will not work because it is postgres specific command and it's not installed by default in the nodejs image.\nIf you specify POSTGRES_DB: pg_development env var on postgres container, the database will be created automatically when the container starts. So no need to run createdb anyway in entrypoint.sh that is mounted in the nodejs container.\nIn order to make sequelize db:migrate work you need to:\nadd sequelize-cli to dependencies in package.json\nrun npm install so it gets installed\nrun npx sequelize db:migrate\nHere is a proposal:\n# docker-compose.yml\n\nversion: '3'\nservices:\n  db:\n    image: \"postgres:11.2\"\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - ./pgData:/var/lib/postgresql/data\n    environment:\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD:\n      POSTGRES_DB: pg_development\n\n  app:\n    working_dir: /restify-pg\n    entrypoint: [\"/bin/bash\", \"./entrypoint.sh\"]\n    image: node:10.12.0\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - .:/restify-pg\n    environment:\n      DB_HOST: db\n# package.json\n\n{\n  ...\n  \"dependencies\": {\n    ...\n    \"pg\": \"^7.9.0\",\n    \"pg-hstore\": \"^2.3.2\",\n    \"sequelize\": \"^5.2.9\",\n    \"sequelize-cli\": \"^5.4.0\"\n  }\n}\n# entrypoint.sh\n\nnpm install\nnpx sequelize db:migrate\nnpm run dev",
    "How to integrate 'npm install' into ASP.NET CORE 2.1 Docker build": "Found the solution:\nFROM microsoft/dotnet:2.1-aspnetcore-runtime AS base\nWORKDIR /app\nEXPOSE 80\n\nFROM microsoft/dotnet:2.1-sdk AS build\nWORKDIR /src\nCOPY --from=frontend . .\nCOPY [\"myProject.WebUi/myProject.WebUi.csproj\", \"myProject.WebUi/\"]\nCOPY [\"myProject.SearchIndex/myProject.SearchIndex.csproj\", \"myProject.SearchIndex/\"]\nCOPY [\"myProject.SearchIndex.Common/myProject.SearchIndex.Common.csproj\", \"myProject.SearchIndex.Common/\"]\n\nRUN dotnet restore \"myProject.WebUi/myProject.WebUi.csproj\"\nCOPY . .\nWORKDIR \"/src/myProject.WebUi\"\nRUN apt-get update -yq && apt-get upgrade -yq && apt-get install -yq curl git nano\nRUN curl -sL https://deb.nodesource.com/setup_8.x | bash - && apt-get install -yq nodejs build-essential\nRUN npm install -g npm\nRUN npm install\nRUN dotnet build \"myProject.WebUi.csproj\" -c Release -o /app\n\nFROM build AS publish\nRUN dotnet publish \"myProject.WebUi.csproj\" -c Release -o /app\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app .\nENTRYPOINT [\"dotnet\", \"myProject.WebUi.dll\"]",
    "Can you make any sense of Dockers error-messages?": "You need to run docker build -f [docker_file_name] . (don't miss the dot at the end).\nIf the name of your file is Dockerfile then you don't need the -f and the filename.",
    "Run `docker-php-ext-install` FROM container other than php": "New solution\nYou need to create new Dockerfile for specific service, in this case php:\nphp/Dockerfile\nFROM php:7.1.1-fpm\nRUN apt -yqq update\nRUN apt -yqq install libxml2-dev\nRUN docker-php-ext-install pdo_mysql\nRUN docker-php-ext-install xml\nAnd then link to it in your docker-compose.yml file, just like this:\nservices:\n  // other services\n  php:\n    build: ./php\n    ports:\n      - \"9000:9000\"\n    volumes:\n      - .:/dogopic\n    links:\n      - mariadb\nPlease look at build parameter - it points to directory in which is that new Dockerfile located.\nOld solution\nI walked around the problem. I've figured out that I can still run this docker-php-ext-install script using following command:\ndocker-compose exec <your-php-container> docker-php-ext-install pdo pdo_mysql mbstring\nAnd because of the convenience I've created this simple Batch file to simplify composing containers just to one command: ./docker.bat\n@ECHO OFF\n\ndocker-compose build\ndocker-compose exec php docker-php-ext-install pdo pdo_mysql mbstring\ndocker-compose up",
    "FROM...AS in Dockerfile not working as I expect": "The FROM...AS is for multi-stage builds:\nWith multi-stage builds, you use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each of them begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don\u2019t want in the final image. To show how this works, let\u2019s adapt the Dockerfile from the previous section to use multi-stage builds.\nYour dockerfile just has one stage, meanless to use it, a valid use case is next:\nFROM golang:1.7.3 AS builder\nWORKDIR /go/src/github.com/alexellis/href-counter/\nRUN go get -d -v golang.org/x/net/html  \nCOPY app.go    .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\nFROM alpine:latest  \nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\nCOPY --from=builder /go/src/github.com/alexellis/href-counter/app .\nCMD [\"./app\"]  \nHere, the built out binary in first stage(builder) could be copied to the second stage which with a new base(FROM alpine:latest). The benefit is: it can reduce the golang tool chain setup in second stage, just use the binary from the first stage.\nUPDATE 20221012 to fit new comments:\nLooks official guide did not afford a sample app.go, next is a sample:\npackage main\nfunc main() {\n}",
    "--mount=type=cache in buildkit": "It's best to think of --mount=type=cache as being like a named volume in docker, managed by BuildKit, and potentially deleted if the BuildKit cache gets full or a prune is requested. The next time you run a build, that same named volume may be available, significiantly reducing the build time spent downloading dependencies. While very useful, that doesn't appear to be what you're looking for here. To use a cache like this, you'd need to include the go-offline as an earlier step in the Dockerfile:\n#syntax=docker/dockerfile:experimental\nFROM maven:3.6.1-jdk-11 AS build\nWORKDIR /\n# copy just the pom.xml for cache efficiency\nCOPY ./pom.xml /\n# go-offline using the pom.xml\nRUN --mount=type=cache,target=/root/.m2 mvn dependency:go-offline\n# now copy the rest of the code and run an offline build\nCOPY . /\nRUN --mount=type=cache,target=/root/.m2 mvn -o install \n\nFROM scratch\nCOPY --from=build /admin/admin-rest/target/admin-rest.war /webapps/ROOT.war\nTo mount a directory into the container from the host, what you appear to be looking for is a bind mount. And with BuildKit's experimental settings, that is available, but only to the build context, not to any arbitrary directory on the build host. For that, you can place your .m2 directory in the build context directory and then use the following line in your Dockerfile:\nRUN --mount=type=bind,source=./.m2,target=/root/.m2,rw mvn -o install\nNote if any of the dependencies change, then Maven may try to connect over the network again.",
    "How to update source code without rebuilding image each time?": "Quickly answer\nIs there a way to avoid rebuilding my Docker image each time I make a change in my source code ?\nIf your app needs a build step, you cannot skip it.\nIn your case, you can install the requirements before the python app, so on each source code modification, you just need to run your python app, not the entire stack: postgress, proxy, etc\nDocker purpose\nThe main docker goal or feature is to enable developers to package applications into containers which are easy to deploy anywhere, simplifying your infrastructure.\nSo, in this sense, docker is not strictly for the developer stage. In the developer stage, the programmer should use an specialized IDE (eclipse, intellij, visual studio, etc) to create and update the source code. Also some languages like java, c# and frameworks like react/ angular needs a build stage.\nThese IDEs has features like hot reload (automatic application updates when source code change), variables & methods auto-completion, etc. These features achieve to reduce the developer time.\nDocker for source code changes by developer\nIs not the main goal but if you don't have an specialized ide or you are in a very limited developer workspace(no admin permission, network restrictions, windows, ports, etc ), docker can rescue you\nIf you are a java developer (for instance), you need to install java on your machine and some IDE like eclipse, configure the maven, etc etc. With docker, you could create an image with all the required techs and the establish a kind of connection between your source code and the docker container. This connection in docker is called Volumes\ndocker run --name my_job -p 9000:8080 \\\n-v /my/python/microservice:/src \\\npython-workspace-all-in-one\nIn the previous example, you could code directly on /my/python/microservice and you only need to enter into my_job and run python /src/main.py. It will work without python or any requirement on your host machine. All will be in python-workspace-all-in-one\nIn case of technologies that need a build process: java & c#, there is a time penalty because, the developer should perform a build on any source code change. This is not required with the usage of specialized ide as I explained.\nI case of technologies who not require build process like: php, just the libraries/dependencies installation, docker will work almost the same as the specialized IDE.\nDocker for local development with hot-reload\nIn your case, your app is based on python. Python don't require a build process. Just the libraries installation, so if you want to develop with python using docker instead the classic way: install python, execute python app.py, etc you should follow these steps:\nDon't copy your source code to the container\nJust pass the requirements.txt to the container\nExecute the pip install inside of container\nRun you app inside of container\nCreate a docker volume : your source code -> internal folder on container\nHere an example of some python framework with hot-reload:\nFROM python:3\nRUN mkdir -p /usr/src/app\nWORKDIR /usr/src/app\nCOPY requirements.txt /usr/src/app\nRUN pip install -r requirements.txt\nCMD [ \"mkdocs\", \"serve\",  \"--dev-addr=0.0.0.0:8000\" ]\nand how build as dev version:\ndocker build -t myapp-dev .\nand how run it with volumes to sync your developer changes with the container:\ndocker run --name myapp-dev -it --rm -p 8000:8000 -v $(pwd):/usr/src/app mydocs-dev\nAs a summary, this would be the flow to run your apps with docker in a developer stage:\nstart the requirements before the app (database, apis, etc)\ncreate an special Dockerfile for development stage\nbuild the docker image for development purposes\nrun the app syncing the source code with container (-v)\ndeveloper modify the source code\nif you can use some kind of hot-reload library on python\nthe app is ready to be opened from a browser\nDocker for local development without hot-reload\nIf you cannot use a hot-reload library, you will need to build and run whenever you want to test your source code modifications. In this case, you should copy the source code to the container instead the synchronization with volumes as the previous approach:\nFROM python:3\nRUN mkdir -p /usr/src/app\nCOPY . /usr/src/app\nWORKDIR /usr/src/app\nRUN pip install -r requirements.txt\nRUN mkdocs build\nWORKDIR /usr/src/app/site\nCMD [\"python\", \"-m\", \"http.server\", \"8000\" ]\nSteps should be:\nstart the requirements before the app (database, apis, etc)\ncreate an special Dockerfile for development stage\ndeveloper modify the source code\nbuild\ndocker build -t myapp-dev.\nrun\ndocker run --name myapp-dev -it --rm -p 8000:8000 mydocs-dev",
    "\"key cannot contain a space\" error while running docker compose": "Ok this is resolved finally! After beating my head around, I was able to finally resolve this issue by doing the following things:\nUnchecked the option to use \"Docker Compose v2\" from my docker desktop settings. Here is the setting in Docker Desktop\nClosed the docker desktop app and restarted it.\nPlease try these steps in case you face the issue. Thanks!",
    "Docker Compose file is invalid, additional properties not allowed": "You are missing a services keyword, your correct .yml is:\nversion: '2'\nservices:\n  config-server:\n    image: ccc/config-server\n    restart: always\n  registration-server:\n    image: ccc/registration-server\n    restart: always\n    ports:\n      - 1111:1111",
    "Sharing volume between Docker containers": "You may find a lot of pointers mentioning data-only containers and --volumes-from. However, since docker 1.9, volumes have become first class citizens, they can have names, and have more flexibility:\nIt's now easy to achieve the behavior you want, here's an example :\nCreate a named data volume with name service-data:\ndocker volume create --name service-data\nYou can then create a container that mounts it in your /public folder by using the -v flag:\ndocker run -t -i -v service-data:/public debian:jessie /bin/bash\nFor testing purpose we create a small text file in our mapped folder:\ncd public\necho 'hello' > 'hello.txt'\nYou may then attach your named volume to a second container, but this time under the data folder:\ndocker run -t -i -v service-data:/data debian:jessie /bin/bash\nls /data       #-->shows \"hello.txt\"\nJust remember, if both containers are using different images, be careful with ownership and permissions!",
    "docker toolbox mount file on windows": "Try to run it with additional / for volume like:\ndocker run -d --name simple2 -v /c/Users/src://usr/share/nginx/html -p 8082:80 ng1\nOr even for host OS, as\ndocker run -d --name simple2 -v //c/Users/src://usr/share/nginx/html -p 8082:80 ng1\nDue to this issue:\nThis is something that the MSYS environment does to map POSIX paths to Windows paths before passing them to executables.",
    "Is it possible to get the architecture of the docker engine in a Dockerfile?": "If you build using BuildKit, there are some predefined ARGs you can use:\nTARGETPLATFORM - platform of the build result. Eg linux/amd64, linux/arm/v7, windows/amd64.\nTARGETOS - OS component of TARGETPLATFORM\nTARGETARCH - architecture component of TARGETPLATFORM\nTARGETVARIANT - variant component of TARGETPLATFORM\nBUILDPLATFORM - platform of the node performing the build.\nBUILDOS - OS component of BUILDPLATFORM\nBUILDARCH - OS component of BUILDPLATFORM\nBUILDVARIANT - OS component of BUILDPLATFORM\nThese are documented in \"Automatic platform ARGs in the global scope\" in the builder documentation.\nTo use BuildKit, you can enable it within your shell with a variable:\nexport DOCKER_BUILDKIT=1\nAnd then build using docker build (support for BuildKit using docker-compose is being worked on, likely in the next release).",
    "Should I copy `package-lock.json` to the container image in Dockerfile?": "You should absolutely copy the package-lock.json file in. It has a slightly different role from the package.json file: package.json can declare \"I'm pretty sure my application works with version 17 of the react package\", where package-lock.json says \"I have built and tested with exactly version 17.0.1 of that package\".\nOnce you have both files, there is a separate npm ci command that's optimized for this case.\nCOPY package.json package-lock.json .\n# Run `npm ci` _before_ copying the application in\nRUN NODE_ENV=production npm ci\n# If any file in `dist` changes, this will stop Docker layer caching\nCOPY ./dist ./dist",
    "How to do a health check of a Spring Boot application running in a Docker Container?": "If you want to use the spring boot actuator/health as a docker healthcheck, you have to add it like this on your docker-compose file:\n    healthcheck:\n      test: \"curl --fail --silent localhost:8081/actuator/health | grep UP || exit 1\"\n      interval: 20s\n      timeout: 5s\n      retries: 5\n      start_period: 40s\nEdit: here the port is the management.server.port. If you don't have specified it, it should be the server.port value (8080 by default)",
    "Why does Docker build take long time in \"Sending context to daemon\" step?": "I solved it, silly mistake.\nSeems like Docker build tars up the current working directory (i.e. the folder containing the dockerfile). And it then uploads it to the Docker Daemon for the build steps. I had accidently put a big test data file (2.9 GB) in the working directory. And that was getting included in the build context. After removing it things are back to normal.",
    "Is CMD or ENTRYPOINT necessary to mention in Dockerfile?": "Specifying a command at the end of the docker run command line supplies (or overrides) CMD; similarly, the docker run --entrypoint option supplies (or overrides) ENTRYPOINT. In your example you gave a command /bin/sh so there's something for the container to do; if you leave it off, you'll get an error.\nAs a matter of style your Dockerfiles should almost always declare a CMD, unless you're extending a base image that's already running the application automatically (nginx, tomcat). That will let you docker run the image and launch the application embedded in it without having to remember a more specific command-line invocation.",
    "$PWD is not set in ENV instruction in a Dockerfile": "PWD is an special variable that is set inside a shell. When docker RUN something it does that with this form sh -c 'something', passing the pre-defined environment variables from ENV instructions, where PWD is not in that list (see it with docker inspect <image-id>).\nENV instructions does not launch a shell. Simply add or update the current list of env vars in the image metadata.\nI would write your Dockerfile as this:\nFROM ubuntu:16.04\nENV APP_PATH=/some/path\nWORKDIR $APP_PATH\nCOPY . .\nENV PYTHONUSERBASE=$APP_PATH/pyenv PATH=$APP_PATH/pyenv/bin:$PATH\nRUN echo \"PWD is: $PWD\"\nRUN echo \"PYENV is: $PYTHONUSERBASE\"\nFurther info in docs:\nThe WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile. If the WORKDIR doesn\u2019t exist, it will be created even if it\u2019s not used in any subsequent Dockerfile instruction.",
    "How to pass local machine's SSH key to docker container?": "This works for me :\nUsing this workaround : https://stackoverflow.com/a/47544999/3957754 to pass files as build args\nDockerfile\nARG SSH_KEY\nENV SSH_KEY=$SSH_KEY\n\n# Make ssh dir\nRUN mkdir /root/.ssh/\n \n# Create id_rsa from string arg, and set permissions\n\nRUN echo \"$SSH_KEY\" > /root/.ssh/id_rsa\nRUN chmod 600 /root/.ssh/id_rsa\n \n# Create known_hosts\nRUN touch /root/.ssh/known_hosts\n\n# Add git providers to known_hosts\nRUN ssh-keyscan bitbucket.org >> /root/.ssh/known_hosts\nRUN ssh-keyscan github.com >> /root/.ssh/known_hosts\nRUN ssh-keyscan gitlab.com >> /root/.ssh/known_hosts\nBuild\ndocker build -t some-app --build-arg SSH_KEY=\"$(cat ~/file/outside/build/context/id_rsa)\" .\nWith this, you can perform git clone git@github.com... (gitlab, or bitbucket) at build stage or at run stage using ENTRYPOINT [\"docker-entrypoint.sh\"].\nThis could works if you need to pass any file as parameter to your container\nSecurity\nAs commenters said, to pass a file to a container at build time is not safe. The workaround and best practice is : clone the project in the c.i (jenkins, bamboo, circleci, etc) and the perform the docker build .... Clone the project inside of docker is usually just for old required libraries, not for the main source code.",
    "How upgrade a docker image without creating new image?": "Those <none>:<none> are actually the old version of your application, that has lost its named pointer, since you moved the nick_app:latest to your new build.\nWhen building your image, I don't think that you can tell it to destroy the old image, it will simply create a new one. But there is a command that helps you to list or remove those dangling images :\ndocker images --filter dangling=true #lists all images that are dangling and has no pointer to it\ndocker rmi `docker images --filter dangling=true -q` #Removes all those images.",
    "How do I attach to intermediate docker container when building with buildkit": "I think it is not possible at the moment see buildkit/issue#1472.\nBut BuildKit still caches all layers so you could use a work around.\nInspecting the image before the failing RUN command, comment out the failing and all subsequent RUN commands. Rerun docker build and then do docker run to inspect the image.\nInspecting the image after the failing RUN command, add || true at the end of your RUN command to force the command to succeed. Rerun docker build and then do docker run to inspect the image.",
    "How to access the metadata of a docker container from a script running inside the container?": "To get the labels (and anything from the remote API), you could pass the socket into the container and use curl >= 7.40 (it's the minimum version that supports --unix-socket flag) from within the container to access the remote API via the socket:\nDockerfile:\nFROM ubuntu:16.04 \nRUN apt-get update \\\n    && apt-get install curl -y\nLABEL abc = abc_value1\nBuild and run\ndocker build -t image1 .\ndocker run -v /var/run/docker.sock:/var/run/docker.sock -it image1 /bin/bash\nFrom inside the container\ncurl --unix-socket /var/run/docker.sock http:/containers/$(hostname)/json\nFrom here you'll have a huge chunk of JSON (similar to docker inspect). You can then use a CLI tool like jq to pluck out the labels.\nSee more information on docker's website: https://docs.docker.com/engine/reference/api/docker_remote_api/#/docker-remote-api\nAll that said-- this isn't very secure, and environment variables are probably a better bet.",
    "Could not auto-determine entry point from rollupOptions": "You are probably using the command yarn dev run instead of yarn run dev to run the dev server",
    "Why is docker build taking so long to run?": "I have no idea why the file size was 213.8. The only directory that is large is node_modules and that contains .dockerignore so it shouldn't be touching that directory.\nThat's not how .dockerignore works. The .dockerignore file should be in the same directory as your Dockerfile and lists patterns to ignore. Create a file in backend called .dockerignore which simply contains the line node_modules.\nSee here for more information: https://docs.docker.com/engine/reference/builder/#dockerignore-file",
    "Docker-compose command doesn't override Dockerfile CMD": "Overriding takes place at runtime, which is when you create a container based on an image and you then start it. That last step you see is during the building phase of the actual image which (correctly) follows the instructions of your Dockerfile.\nSo, if you continue with docker-compose up, the container that will be created and started, will have the \"overridden\" configuration provided in the docker-compose.yaml file.\nFrom Overriding Dockerfile image defaults\nOverriding Dockerfile image defaults\nWhen a developer builds an image from a Dockerfile or when she commits it, the developer can set a number of default parameters that take effect when the image starts up as a container. Four of the Dockerfile commands cannot be overridden at runtime: FROM, MAINTAINER, RUN, and ADD. Everything else has a corresponding override in docker run.",
    "How do you dockerize a WebSocket Server?": "When you specify a hostname or IP address to listen on (in this case localhost which resolves to 127.0.0.1), then your server will only listen on that IP address.\nListening on localhost isn't a problem when you are outside of a Docker container. If your server only listens on 127.0.0.1:8000, then your client can easily connect to it since the connection is also made from 127.0.0.1.\nWhen you run your server inside a Docker container, it'll only listen on 127.0.0.1:8000 as before. The 127.0.0.1 is a local loopback address and it not accessible outside the container.\nWhen you fire up the docker container with -p 8000:8000, it'll forward traffic heading to 127.0.0.1:8000 to the container's IP address, which in my case is 172.17.0.2.\nThe container gets an IP addresses within the docker0 network interface (which you can see with the ip addr ls command)\nSo, when your traffic gets forwarded to the container on 172.17.0.2:8000, there's nothing listening there and the connection attempt fails.\nThe fix:\nThe problem is with the listen address:\nserver := http.Server{Addr: \"localhost:8000\"}\nTo fix your problem, change it to\nserver := http.Server{Addr: \":8000\"}\nThat'll make your server listen on all it container's IP addresses.\nAdditional info:\nWhen you expose ports in a Docker container, Docker will create iptables rules to do the actual forwarding. See this. You can view these rules with:\niptables -n -L \niptables -t nat -n -L",
    "How to pass Java options/variables to Springboot app in docker run command": "Solution 1\nYou can override any property from your configuration by passing it to docker container using -e option. As explained in Externalized configuration the environment variable name should be uppercased and splitted using underscore. So for example to pass spring.profiles.active property you could use SPRING_PROFILES_ACTIVE environment variable during container run :\ndocker run -p 8000:80 -e SPRING_PROFILES_ACTIVE=dockerdev demo-app\nAnd this variable should be picked automatically by Spring from environment.\nSolution 2\nChange Dockerfile to :\nFROM openjdk:8-jdk-alpine\nARG JAR_FILE=target/demo-app-1.0-SNAPSHOT.jar\n\n# environment variable with default value\nENV SPRING_PROFILE=dev\n\nCOPY ${JAR_FILE} /opt/lib/demo-app.jar\n\nEXPOSE 80\n\n#run with environment variable\nENTRYPOINT java -Dspring.profiles.active=$SPRING_PROFILE -jar /opt/lib/demo-app.jar\nand then run the container passing the environment variable :\ndocker run -p 8000:80 --rm -e SPRING_PROFILE=dockerdev demo-app",
    "Docker: $'\\r': command not found on Windows": "I was passing for the same problem and I found this error is caused for caracters that only the windows use. To solution this error, write the file shellscript using nano or other linux file editors.\nIf you use windows 10, you can start a sub system linux to write the shellscript.\nHope this helps",
    "What is the difference between node images for Docker and when to use which?": "All of the standard Docker Hub images have a corresponding Docker Hub page; for Node it's https://hub.docker.com/_/node. There's a section entitled \"Image Variants\" on that page that lists out the major options.\nDebian/Ubuntu/Alpine. Most Docker images are built on top of one of these base Linux distributions. Debian and Ubuntu are very similar and in fact are closely related; if you see something labeled \"bullseye\", \"buster\", or \"stretch\", these are the names of specific Debian major releases. Alpine has a reputation for being very small, but with that tininess comes some potential low-level C library issues. I'd recommend the Debian-based option unless you really need the tens of megabytes of space savings.\n\"Slim\" images. The node image has a \"slim\" variant. The default image mentions that it's built on top of a buildpack-deps image, but that base image is not small (~300 MB). That image includes a full C toolchain and many development headers. Some npm install commands could need this, but you don't want it in your final image. Prefer the \"slim\" option if it's a choice.\nVersion tags. The top of the Docker Hub page has a bewildering list of tags. If you look at these, each line has several names; as of this writing, node:16, node:16.14, node:16.14.0, node:lts, node:16-buster, and several other things are all actually the same image. One thing to note is that only \"current\" versions get any sort of updates at all; if Node 16.14.0 is the current version then no node:16.13 package will ever be rebuilt. I'd suggest picking a specific major version and using a \"long-term support\" version if it's an option, but not specifying a minor or patch version.\nCombining all of those together, my default would be something like\nFROM node:16-slim # Debian-based",
    "How can I upgrade pip inside a venv inside a Dockerfile?": "The single easiest answer to this is to just not bother with a virtual environment in a Docker image. A virtual environment gives you an isolated filesystem space with a private set of Python packages that don't conflict with the system install, but so does a Docker image. You can just use the system pip in a Docker image and it will be fine.\nFROM python:3.7\nRUN pip install --upgrade pip\nWORKDIR /usr/src/app\nCOPY . .\nRUN pip install .\nCMD [\"myscript\"]\nIf you really want a virtual environment, you either need to specifically run the wrapper scripts from the virtual environment's path\nRUN python -m venv venv\nRUN venv/bin/pip install --upgrade pip\nor run the virtual environment \"activate\" script on every RUN command; the environment variables it sets won't carry over from one step to another. (Each RUN command in effect does its own docker run; docker commit sequence under the hood and will launch a new shell in a new container; the Dockerfile reference describes this a little bit.)\nRUN python -m venv venv\nRUN . venv/bin/activate \\\n && pip install --upgrade pip\nCOPY . .\nRUN . venv/bin/activate \\\n && pip install .\nCMD [\"venv/bin/myscript\"]\nTrying to activate the virtual environment in its own RUN instruction does nothing beyond generate a no-op layer.\n# This step does nothing\nRUN . venv/bin/activate\n# And therefore this upgrades the system pip\nRUN pip install --upgrade pip",
    "How to use pipes(ioredirection) in Dockerfile RUN?": "You can try a sh -c command\nRUN sh -c 'git archive master | tar -x -C /path'\nIf not, you can include that command in a script, COPY the script and RUN it.",
    "Docker file FROM node:12.2.0-alpine": "Alpine is the base image which is based on Alpine Linux, a very compact Linux distribution. So, node:12.2.0-alpine is a Alpine Linux image with node 12.2.0 installed.\nFor the latest Alpine based image you can simply do node:alpine. If you want latest but not specifically Alpine you can do node:latest, that image will be based on stretch which is a Debian distribution.\nYou can find a full list of all supported tags here: https://hub.docker.com/_/node/",
    "How to limit memory usage in docker-compose?": "deploy key only works in swarm mode and with docker-compose file version 3 and above.\nIn your case, use docker-compose file version 2 and define resource limits:\nversion: \"2.2\"\n\nservices:\n  app:\n    image: foo\n    cpus: \"0.5\"\n    mem_limit: 23m\nSee official docs here",
    "Can Docker COPY commands be chained": "Since COPY commands cannot be chained, it's typically best to structure your context (directories you are copying from) in a way that is friendly to copy into the image.\nSo instead of:\nCOPY ./folder1A/* /home/user/folder1B/ && \\\n     ./folder2A/* /home/user/folder2B/ && \\\n     ./folder3A/* /home/user/folder3B/ && \\\n     ./folder4A/* /home/user/folder4B/ && \\\nPlace those folders into a common directory and run:\nCOPY user/ /home/user/\nIf you are copying files, you can copy multiple into a single target:\nCOPY file1.zip file2.txt file3.cfg /target/app/\nIf you try to do this with a directory, you'll find that docker flattens it by one level, hence the suggestion to reorganize your directories into a common parent folder.",
    "Error: \"user\" directive makes sense only if the master process runs with super-user privileges": "NGINX has now an official unprivileged Docker image, with more fine-grained changes (below are only \"notable\" ones, there is more of them):\nremoving user directive in /etc/nginx/nginx.conf\nmoving PID from /var/run/nginx.pid to /tmp/nginx.pid\nchanging *_temp_path variables to /tmp/*\nchanging the listening port to a non-root one (80->8080).\nTo see all these changes, please check out the source at nginxinc/docker-nginx-unprivileged or simply pull one of the resulting unprivileged Docker images from the Docker Hub (nginxinc/nginx-unprivileged), and I strongly recommend the one based on Alpine rather than on Debian to avoid frequent vulnerabilities:\ndocker pull nginxinc/nginx-unprivileged:stable-alpine",
    "Xvfb & Docker - cannot open display": "I solved this by writing a startup script which will:\nstart xvfb\nstart firefox\nExecuting the script via CMD allows the proper sequence of commands to run on container startup.\nDockerfile\n...\nENV DISPLAY :99\n\nADD run.sh /run.sh\nRUN chmod a+x /run.sh\n\nCMD /run.sh\nrun.sh\nXvfb :99 -screen 0 640x480x8 -nolisten tcp &\nfirefox",
    "Is it necessary to RUN apk update && apk upgrade in a docker build stage?": "First of all: the command you propose is not efficient because the --no-cache option affects the 2nd apk only.\nDemonstration\n/ # ls -la /var/cache/apk/\ntotal 0\ndrwxr-xr-x    2 root     root             6 Mar 29 14:27 .\ndrwxr-xr-x    4 root     root            29 Mar 29 14:27 ..\n\n/ # apk update && apk upgrade --no-cache\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.16/main/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.16/community/x86_64/APKINDEX.tar.gz\nv3.16.5-131-g8a958b888f7 [https://dl-cdn.alpinelinux.org/alpine/v3.16/main]\nv3.16.5-127-g643d8ee0752 [https://dl-cdn.alpinelinux.org/alpine/v3.16/community]\nOK: 17042 distinct packages available\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.16/main/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.16/community/x86_64/APKINDEX.tar.gz\n(1/5) Upgrading musl (1.2.3-r2 -> 1.2.3-r3)\n(2/5) Upgrading ca-certificates-bundle (20220614-r0 -> 20230506-r0)\n(3/5) Upgrading libcrypto1.1 (1.1.1t-r2 -> 1.1.1u-r1)\n(4/5) Upgrading libssl1.1 (1.1.1t-r2 -> 1.1.1u-r1)\n(5/5) Upgrading musl-utils (1.2.3-r2 -> 1.2.3-r3)\nExecuting busybox-1.35.0-r17.trigger\nOK: 6 MiB in 14 packages\n\n/ # ls -la /var/cache/apk/\ntotal 2416\ndrwxr-xr-x    1 root     root            70 Jun  9 12:51 .\ndrwxr-xr-x    1 root     root            29 Mar 29 14:27 ..\n-rw-r--r--    1 root     root        657130 Jun  9 12:51 APKINDEX.77a9a2bb.tar.gz\n-rw-r--r--    1 root     root       1810672 Jun  9 12:51 APKINDEX.af244049.tar.gz\nAs you can see, the apk cache is not empty after the execution of your command.\nThe correct command is\nRUN apk upgrade --no-cache\nbecause --no-cache\nallows users to install packages with an index that is updated and used on-the-fly and not cached locally\nas explained at https://github.com/gliderlabs/docker-alpine/blob/master/docs/usage.md\nNow, to answer your question, upgrading the image used for the build is not particularly useful. Upgrading the deployment base image makes much more sense, however I upgrade it only if trivy says that it has some CVE and Docker Hub hasn't a newer version of that base image. In this case I create an upgraded base image and reuse it for all my apps (you could also use it for build stages).\nFor example, at today's date, trivy says that the latest builds of Alpine Linux 3.16, 3.17 and 3.18 have some CVE that has been already fixed but the fixes are not part of an official Alpine release yet. So I have just created my own Alpine with this Dockerfile:\nFROM alpine:3.16 as alpine-upgraded\n\nRUN apk upgrade --no-cache\n\n# Main image\nFROM scratch\n\nCOPY --from=alpine-upgraded / /\nCMD [\"/bin/sh\"]\nTrivy says it is vulnerability-free and docker images says that the new image has the same size of the original (not upgraded) one: 5.54 MB.\nSee also the description of this Docker image.",
    "Pass host environment variables to dockerfile": "I was experiencing the same issue. My solution was to provide the variable inside of a docker-compose.yml because yml supports the use of environment variables.\nIn my opinion this is the most efficient way for me because I didn't like typing it over and over again in the command line using something like docker run -e myuser=$USER . . .\nDeclaring ENV myuser=$USER will NOT work, in the container, $myuser will be set to null.\nSo your docker-compose.yml could look something like this:\nversion: '3'\nservices:\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n       - \"myuser=${USER}\"\nand can be run with the short command docker-compose up\nTo check that the variable has been applied, run docker exec -it container-name printenv to list all variables in the container.",
    "How to Dockerfile FROM another Dockerfile?": "If you want to avoid playing around with unnecessary intermediate tags and you use Docker 20.10+, you can also do this:\n# syntax = edrevo/dockerfile-plus\n\nINCLUDE+ Dockerfile.common\n\n# The rest of Dockerfile.foo would go after the INCLUDE+ instruction\n\nRUN echo \"Hello World\"\nThe INCLUDE+ instruction gets imported by the first line in the Dockerfile. You can read more about the dockerfile-plus at https://github.com/edrevo/dockerfile-plus",
    "What does the -j option do for docker-php-ext-install?": "It is the number of Jobs for the make calls contained inside the script docker-php-ext-install (line 53 stores the option in the variable $j and 105-106 call make -j$j).\nThe command nproc is giving directly to the script the number of physical thread available for your system. For example, on my system it will be reduced to:\nmake -j$(nproc) -> make -j8\nthus it runs make with 8 parallel recipes.\nFrom make manual:\n-j [jobs], --jobs[=jobs]: Specifies the number of jobs (commands) to run simultaneously. If there is more than one -j option, the last one is effective. If the -j option is given without an argument, make will not limit the number of jobs that can run simultaneously.\nwith more information in the GNU make documentation about parallel jobs:\nGNU make knows how to execute several recipes at once. Normally, make will execute only one recipe at a time, waiting for it to finish before executing the next. However, the -j or --jobs option tells make to execute many recipes simultaneously. [...] On MS-DOS, the -j option has no effect, since that system doesn\u2019t support multi-processing.\nIf the -j option is followed by an integer, this is the number of recipes to execute at once; this is called the number of job slots. If there is nothing looking like an integer after the -j option, there is no limit on the number of job slots. The default number of job slots is one, which means serial execution (one thing at a time).\nIdeally, if that number is equal to the number of the physical threads available (roughly the number of processors, or as in this case the number returned by nproc), you should get the fastest compilation possible.\nnproc is a linux command - see man nproc too. It means \"number of processing units available\" - your CPU Cores. You can imagine it as \"Number of processors\" [source: @tron5 comment]\nYou must consider the memory available though. For example, if you allocate 8 slots with only 1GB of RAM and the compilation of 3 simultaneous jobs fill the RAM, then when the fourth will start it will exit with an error due to insufficient memory, arresting the whole compilation process.",
    "Run a script when docker is stopped": "docker stop sends a SIGTERM signal to the main process running inside the Docker container (the entry script). So you need a way to catch the signal and then trigger the exit script.\nSee This link for explanation on signal trapping and an example (near the end of the page)",
    "Mount docker host volume but overwrite with container's contents": "The docker 1.10+ way of sharing files would be through a volume, as in docker volume create.\nThat means that you can use a data volume directly (you don't need a container dedicated to a data volume).\nThat way, you can share and mount that volume in a container which will then keep its content in said volume.\nThat is more in line with how a container is working: isolating memory, cpu and filesystem from the host: that is why you cannot \"mount a volume and have the container's files take precedence over the host file\": that would break that container isolation and expose to the host its content.",
    "Is there a way to suppress \"update-alternatives: warning: skip creation\" warnings when building a Docker image?": "If you just want to save disk space and/or network bandwidth, try this:\necho 'path-exclude /usr/share/doc/*' >/etc/dpkg/dpkg.cfg.d/docker-minimal && \\ \necho 'path-exclude /usr/share/man/*' >>/etc/dpkg/dpkg.cfg.d/docker-minimal && \\\necho 'path-exclude /usr/share/groff/*' >>/etc/dpkg/dpkg.cfg.d/docker-minimal && \\\necho 'path-exclude /usr/share/info/*' >>/etc/dpkg/dpkg.cfg.d/docker-minimal && \\\necho 'path-exclude /usr/share/lintian/*' >>/etc/dpkg/dpkg.cfg.d/docker-minimal && \\\necho 'path-exclude /usr/share/linda/*' >>/etc/dpkg/dpkg.cfg.d/docker-minimal && \\\necho 'path-exclude /usr/share/locale/*' >>/etc/dpkg/dpkg.cfg.d/docker-minimal && \\\necho 'path-include /usr/share/locale/en*' >>/etc/dpkg/dpkg.cfg.d/docker-minimal\nAs long as you'll try to install some APT packages, no man pages would be saved to disk. The warning will still be there, but the man pages not.\nThe problem here is that some packages calls the update-alternatives binary from within the postinst section of the .deb packages. See https://manpages.debian.org/buster/dpkg/update-alternatives.1.en.html. Don't know if you can also ask DPKG to not execute the postinst section at all, but it does not sounds good! You may also write a small wrapper to update-alternatives in order to suppress the warnings, but this sound silly.",
    "docker swarm - how to balance already running containers in a swarm cluster?": "Swarm currently (18.03) does not move or replace containers when new nodes are started, if services are in the default \"replicated mode\". This is by design. If I were to add a new node, I don't necessarily want a bunch of other containers stopped, and new ones created on my new node. Swarm only stops containers to \"move\" replicas when it has to (in replicated mode).\ndocker service update --force <servicename> will rebalance a service across all nodes that match its requirements and constraints.\nFurther advice: Like other container orchestrators, you need to give capacity on your nodes in order to handle the workloads of any service replicas that move during outages. You're spare capacity should match the level of redundancy you plan to support. If you want to handle capacity for 2 nodes failing at once, for instance, you'd need a minimum percentage of resources on all nodes for those workloads to shift to other nodes.",
    "How to Serve HTML Files from a Nginx Server Using Docker": "The default directory that static assets are served out of is /usr/share/nginx/html, not /var/www in the Official NGINX Docker image.\nWith that said, you're also mapping your entire root directory and not the /public/ folder where your folder contents live - unless of course you're running this from that directory on a pre-build image.\nYou'll probably want something like:\n\u279c  docker run -p 80:80 -v $(pwd):/usr/share/nginx/html nginx",
    "failed to solve with frontend dockerfile.v0: failed to read dockerfile?": "I encountered a different issue, so sharing as an FYI. On Windows, I created the docker file as DockerFile instead of Dockerfile. The capital F messed things up.",
    "where does the \"Unable to find fallback package folder\" nuget error come from, when building project in a dockerfile?": "Had the same issue. After some Googling I found this nice issue: https://github.com/dotnet/dotnet-docker/issues/2050\nTo recap the answer there: if you have already built the project outside of Docker, then you will have output folders which will get copied into the Docker build environment causing this problem.\nThe solution is then to add a .dockerignore file which prevents this from happening. Something small like this should do the trick:\n# directories\n**/bin/\n**/obj/\n**/out/\n\n# files\nDockerfile*\n**/*.md",
    "Docker compose Invalid volume destination path: '.' mount path must be absolute": "This is just not allowed in the Compose file, since you do not have a template engine there.\nYou will not need to define\nvolumes: \n        - /opt/h2-data\nSince that will be done automatically (anonymous volume). If you want to have a named volume use\nvolumes: \n        - myname:/opt/h2-data\nor a host mount\nvolumes: \n        - /path/on/the/host:/opt/h2-data\nSo ${DATA_DIR} is not expanded in the volumes ( from the ENV ) in a compose file. There are dialects like rancher-compose providing this, but in general that is not possible\nUPDATED: Updated my answer since I somehow mixed the Dockerfile/docker-compose.yml file. It makes sense in the Dockerfile, since it is just used as a variable. Thank you for hinting me on that @Bmitch (once again)",
    "Dockerfile vs docker-compose.yml": "Dockerfile:\nis a recipe for a Docker Image\nonly supports portable options (others options have to be specified at container run time)\ndocker-compose.yaml:\nis a recipe for a group of running services\nsupports overriding portable options that were defined in the Dockerfile\nsupports non-portable options\nsupports creating and configuring networks and volumes\ncan also configure the build of an Image by using build:\nIt is common to use both together.\nA Dockerfile is almost always used to create a custom image. Some images might be used to run services (long running processes), but some images might be used to run short-lived interactive processes (like running unit tests).\ndocker-compose.yaml is useful when you want to run one or more services.",
    "Consume secret inside dockerfile": "The secrets should be used during run time and provided by execution environment.\nAlso everything that is executing during a container build is written down as layers and available later to anyone who is able to get access to an image. That's why it's hard to consume secrets during the build in a secure way.\nIn order to address this, Docker recently introduced a special option --secret. To make it work, you will need the following:\nSet environment variable DOCKER_BUILDKIT=1\nUse the --secret argument to docker build command\nDOCKER_BUILDKIT=1 docker build --secret id=mysecret,src=mysecret.txt\nAdd a syntax comment to the very top of your Docker file\n# syntax = docker/dockerfile:1.0-experimental\nUse the --mount argument to mount the secret for every RUN directive that needs it\nRUN --mount=type=secret,id=mysecret cat /run/secrets/mysecret\nPlease note that this needs Docker version 18.09 or later.",
    "Docker File - Skipping Project. Because it was not found": "Based on your input I propose below folder structure and Dockerfile.\n[Solution] 'BuySellApi' (3 Projects)\n  |\n  +-- Dockerfile\n  | \n  +-- [BuySellApi]\n  |    |\n  |    +--- BuySellApi.csproj\n  |\n  +-- [BuySellApi.Core]\n  |    |\n  |    +--- BuySellApi.Core.csproj\n  |\n  +-- [BuySellApi.Data]\n       |\n       +--- BuySellApi.Data.csproj\nDockerfile\nFROM microsoft/dotnet:2.2-aspnetcore-runtime AS base\nWORKDIR /app\nEXPOSE 5000\nENV ASPNETCORE_URLS=http://+:5000\n    \nFROM microsoft/dotnet:2.2-sdk AS build\nWORKDIR /src\nCOPY . .\nRUN dotnet restore \". BuySellApi/BuySellApi.csproj\"\nWORKDIR \"/src/BuySellApi\"\nRUN dotnet build \"BuySellApi.csproj\" -c Release -o /app\n    \nFROM build AS publish\nWORKDIR \"/src/BuySellApi\"\nRUN dotnet publish \"BuySellApi.csproj\" -c Release -o /app\n    \nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app .\nENTRYPOINT [\"dotnet\", \"BuySellApi.dll\", \"--server.urls\", \"http://0.0.0.0:5000\"]",
    "How can I pass 'yes' response when npm installing on Dockerfile": "I used the following to install Angular without usage statistics sharing.\nRUN echo n | npm install -g --silent @angular/cli\nI think echo y should work for you",
    "What is causing \"Could not find data collector 'XPlat Code Coverage'\" error in Docker image?": "I was getting the same error and the fix for me was to add the nuget reference below.\n<PackageReference Include=\"coverlet.collector\" Version=\"1.0.1\" />\nI was trying to get the code coverage working in my azure devops pipeline and this did the trick for me.\nI was following below tutorial, with the default template api (WeatehrForecast example)\nhttps://learn.microsoft.com/en-us/azure/devops/pipelines/ecosystems/dotnet-core?view=azure-devops",
    "Dockerizing React in production mode: FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory": "The NODE_OPTIONS solution did not work for me, I am using Node v14 and React Scripts v4.\nThis solution from GitHub finally helped - https://github.com/wojtekmaj/react-pdf/issues/496#issuecomment-566200248\nGENERATE_SOURCEMAP=false\nAdded to my Dockerfile right before the build command.",
    "How to make lightweight docker image for python app with pipenv": "The problem comes when you need things like ciso8601, or some libraries, requiring build process. Build tools are not \"incorporated\" into the both slim and alpine variants, for low-size footprint.\nSo to install deps, you'll have to:\nInstall build tools\nDeploy dependencies from Pipfile.lock system-wide\nUninstall build tools and clean caches\nAnd do that 3 actions inside a single RUN layer, like following:\nFROM python:3.7-slim\n\nWORKDIR /app\n\n# both files are explicitly required!\nCOPY Pipfile Pipfile.lock ./\n\nRUN pip install pipenv && \\\n  apt-get update && \\\n  apt-get install -y --no-install-recommends gcc python3-dev libssl-dev && \\\n  pipenv install --deploy --system && \\\n  apt-get remove -y gcc python3-dev libssl-dev && \\\n  apt-get autoremove -y && \\\n  pip uninstall pipenv -y\n\nCOPY app ./\n\nCMD [\"python\", \"app.py\"]\nManipulating build system would cost you around 300MiB and some extra time\nUninstalling pipenv would save you another 20MiB (which is 10% of resulting size).\nSeparating RUN commands would not delete data from layers, and would result in ~500MiB image. That's docker specifics.\nSo that would result in perfectly working ~200MiB sized image, which is\n5 times less than original python:3.7, (that is >1.0GiB)\nHas no alpine incompabilities (these are typically tied to glibc replacement)\nAt the time, we're fine with slim (debian buster) build variants, preferring slim over alpine (for most compatibility). If you're really up to further size optimization, I'd recommend you to take a look at some excellent builds of these guys:\nAlpine Python\n12.7MiB MariaDB",
    "Docker ENTRYPOINT with ENV variable and optional arguments": "It appears it isn't possible to create an ENTRYPOINT that directly supports both variable expansion and additional command line arguments. While the shell form of ENTRYPOINT will expand ENV variables at run time, it does not accept additional (appended) arguments from the docker run command. While the exec form of ENTRYPOINT does support additional command line arguments, it does not create a shell environment by default so ENV variables are not expanded.\nTo get around this, bash can be called explicitly in the exec form to execute a script that then expands ENV variables and passes command line args to the python process. Here is an example Dockerfile that does this:\nFROM ubuntu:16.04\nARG MODULE_NAME=foo\nENV MODULE_NAME=${MODULE_NAME}\n\nRUN apt-get update -y && apt-get install -y python3.5\n\n# Create the module to be run\nRUN echo \"import sys; print('Args are', sys.argv)\" > /foo.py\n\n# Create a script to pass command line args to python\nRUN echo \"/usr/bin/python3.5 -m $MODULE_NAME \\$@\" > /run_module.sh\n\nENTRYPOINT [\"/bin/bash\", \"/run_module.sh\"]\nOutput from the docker image:\n$ docker run my-image\nArgs are ['/foo.py']\n\n$ docker run my-image a b c\nArgs are ['/foo.py', 'a', 'b', 'c']\nNote that variable expansion occurs during the RUN commands (since they are using shell form) so the contents of run_script.py in the image are:\n/usr/bin/python3.5 -m foo $@\nIf the final RUN command is replaced with this:\n    RUN echo \"/usr/bin/python3.5 -m \\$MODULE_NAME \\$@\" > /run_module.sh\nthen the run_script.sh would contain\n/usr/bin/python3.5 -m $MODULE_NAME $@\nBut output from the running container would be the same since variable expansion will occur at run time. A potential benefit of the second version is that one could override the module to be run at run time without replacing the ENTRYPOINT.",
    "Docker non-root User Best Practices for Python Images?": "In general, the easiest safe approach is to do everything in your Dockerfile as the root user until the very end, at which point you can declare an alternate USER that gets used when you run the container.\nFROM ???\n# Debian adduser(8); this does not have a specific known uid\nRUN adduser --system --no-create-home nonroot\n\n# ... do the various install and setup steps as root ...\n\n# Specify metadata for when you run the container\nUSER nonroot\nEXPOSE 12345\nCMD [\"my_application\"]\nFor your more specific questions:\nIs installing packages with apt-get as root ok?\nIt's required; apt-get won't run as non-root. If you have a base image that switches to a non-root user you need to switch back with USER root before you can run apt-get commands.\nBest location to install these packages?\nThe normal system location. If you're using apt-get to install things, it will put them in /usr and that's fine; pip install will want to install things into the system Python site-packages directory; and so on. If you're installing things by hand, /usr/local is a good place for them, particularly since /usr/local/bin is usually in $PATH. The \"user home directory\" isn't a well-defined concept in Docker and I wouldn't try to use it.\nWhen installing python packages with pip as root, I get the following warning...\nYou can in fact ignore it, with the justification you state. There are two common paths to using pip in Docker: the one you show where you pip install things directly into the \"normal\" Python, and a second path using a multi-stage build to create a fully-populated virtual environment that can then be COPYed into a runtime image without build tools. In both cases you'll still probably want to be root.\nAnything else I am missing or should be aware of?\nIn your Dockerfile:\n## get UID/GID of host user for remapping to access bindmounts on host\nARG UID\nARG GID\nThis is not a best practice, since it means you'll have to rebuild the image whenever someone with a different host uid wants to use it. Create the non-root user with an arbitrary uid, independent from any specific host user.\nRUN usermod -aG sudo flaskuser\nIf your \"non-root\" user has unrestricted sudo access, they are effectively root. sudo has some significant issues in Docker and is never necessary, since every path to run a command also has a way to specify the user to run it as.\nRUN chown flaskuser:users /tmp/requirements.txt\nYour code and other source files should have the default root:root ownership. By default they will be world-readable but not writeable, and that's fine. You want to prevent your application from overwriting its own source code, intentionally or otherwise.\nRUN chmod -R  777 /usr/local/lib/python3.11/site-packages/*\nchmod 0777 is never a best practice. It gives a place for unprivileged code to write out their malware payloads and execute them. For a typical Docker setup you don't need chmod at all.\nThe bind mounted workspace is only for development, for a production image I would copy the necessary files/artifacts into the image/container.\nIf you use a bind mount to overwrite all of the application code with content from the host, then you're not actually running the code from the image, and some or all of the Dockerfile's work will just be lost. This means that, when you go to production without the bind mount, you're running an untested setup.\nSince your development environment will almost always be different from your production environment in some way, I'd recommend using a non-Docker Python virtual environment for day-to-day development, have good (pytest) unit tests that can run outside the container, and do integration testing on the built container before deploying.\nPermission issues can also come up if your application is trying to write out files to a host directory. The best approach here is to restructure your application to avoid it, storing the data somewhere else, like a relational database. In this answer I discuss permission setup for a bind-mounted data directory, though that sounds a little different from what you're asking about here.",
    "Docker container with entrypoint variable expansion and CMD parameters": "With /bin/sh -c \"script\" syntax, anything after the -c argument becomes an argument to your script. You can reach them with $0 and $@ as part of your /bin/sh script:\nENTRYPOINT [\"/bin/sh\", \"-c\", \"exec /usr/bin/mycmd --token=$MY_TOKEN $0 $@\"]\nCMD [\"pull\", \"stuff\"]\nNote that you could also change your entrypoint to be a shell script added to your image that runs exec /usr/bin/mycmd --token=$MY_TOKEN \"$@\" and execute that shell script with docker's exec syntax:\nENTRYPOINT [\"/entrypoint.sh\"]",
    "How to use the path with the space in docker file": "Use the JSON form, you have to use double backslashes inside the braces\nFROM microsoft/windowsservercore \nCOPY [\"C:\\\\docker\\\\prerequisites\\\\MicrosoftSDKs\", \"C:\\\\Program Files (x86)\\\\MicrosoftSDKs\"]\nYou can also use slash:\nCOPY [\"C:/Program Files/nodejs\", \"/windows/system32\"]",
    "Add a new entrypoint to a docker image": "I finally ended up calling the original entrypoint bash script in my new entrypoint bash script, before doing other extra configuration steps.",
    "Dockerfile ADD failed : No Source files were specified": "It generally is recommended to use COPY before ADD, because it serves a lesser purpose and is somewhat more lightweight.\nTo copy your whole directory into the image, just add the following line after editing:\n COPY . /path/to/dir/in/image\nSome helpful links to start writing dockerfiles:\nReference\nBest Practices\nPostgresql example",
    "entrypoint: \"entrypoint.sh\" - docker compose": "entrypoint: \"entrypoint.sh\" overrides ENTRYPOINT [\"test.sh\"] from Dockerfile.\nFrom the docs:\nSetting entrypoint both overrides any default entrypoint set on the service\u2019s image with the ENTRYPOINT Dockerfile instruction, and clears out any default command on the image - meaning that if there\u2019s a CMD instruction in the Dockerfile, it is ignored.\nENTRYPOINT [\"test.sh\"] is set in Dockerfile describing docker image\nentrypoint: \"entrypoint.sh\" is set in docker-compose file which describes multicontainer environment while referencing the Dockerfile.\ndocker-compose build builder will build image and set entrypoint to ENTRYPOINT [\"test.sh\"] set in Dockerfile.\ndocker-compose up builder will start container with entrypoint entrypoint.sh pip wheel --no-index '-f /build' . set in docker-compose file",
    "How to run an electron app on docker": "I will try to help you here in this answer - too long for comment.\nI tried your Docker file on my Win10 and with the same problems. But I figured it out by adding required packages and successfully created docker image. Here is Dockerfile\nFROM node:slim\n\nCOPY . /usr/scr/app\n\n#RUN rm bdstart.sh\nRUN apt-get update\n\n# I think you need to install following \nRUN apt-get -y install libgtkextra-dev libgconf2-dev libnss3 libasound2 libxtst-dev libxss1\nRUN npm install --save-dev electron\n\nRUN npm install\n\nCMD [\"/usr/scr/app/start.sh\"]\nand here is your start.sh\n#!/bin/sh\n./node_modules/.bin/electron ./src\nActually I don't have access to your files and so on, but with this DockerFile was able to create docker image without problems. I also went inside docker container and check whether is possible to run electron - worked.\nIf you want to go into container, you just need to build docker image. I have done it by (simplest way) following command (open console where Dockerfile is located and run):\ndocker build -t test-image .\nAfter Successfully build of image you can run container. If any problems I recommend you to run container with bash entrypoint and debug what fails - bash will open in the same console where you type following script)\ndocker run -it test-image bash",
    "Docker - cannot copy to non-directory: /var/lib/docker/overlay2/xw77p2bxfkhhnwqs5umpl7cbi/merged/app/.git": "Just verify the .git is actually a file or folder. Or check for any name conflict between a folder and file.\nSeems like you are trying to copy a folder to a file(non-directory).\nI have faced a similar issue error: failed to solve: cannot replace to directory /var/lib/docker/overlay2/*/*/folder_a with file. Turns out i have a binary file with a name 'folder_a'.\nDeleting the file which matches the folder_name solved the issue for me.",
    "Docker Multi-stage build - Copy failing": "When you build the application it's building in another cointainer/layer. You i'll need to build the application before and copy the build folder to /usr/src/app.\nSo, this is fine:\nFROM node:9.6.1 as builder\n\nWORKDIR /usr/src/app\n\nENV PATH /usr/src/app/node_modules/.bin:$PATH\n\nCOPY package.json .\nCOPY public public\nCOPY src src\n\nRUN npm install --silent\nRUN npm run build\n\nCOPY build .\n\nRUN rm -rf src\nRUN rm -rf build\n\nFROM nginx:1.13.9-alpine\n\nCOPY --from=builder /usr/src/app /usr/share/nginx/html\n\nEXPOSE 80\nI'm removing the src and build folders since that's not necessary and can expose an critical part of your application.\nHence, no doubt about the security of dockerizing an application.",
    "Fixing World-writable MySql error in Docker": "I just encountered this issue and the fix for me is just to set my.cnf file to read-only in Windows.",
    "Import Data on MongoDB using Docker-Compose": "I ended up removing the Dockerfile, adding the commands in a bash script, then calling the script from the docker-compose file. Used a script rather than one command in the docker-compose file because I'm importing several files thus several commands that are not shown in my example. I needed to use mongo:3.2.6 to make this work. There may be other versions but this one works for sure.\ndocker-compose.yml\nversion: '3'\nservices:\n  mongodb:\n    image: mongo:3.2.6\n    ports:\n      - 27017:27017\n\n  mongo_seed:\n    image: mongo:3.2.6\n    links:\n      - mongodb\n    volumes:\n      - ./mongo-seed:/mongo-seed\n    command:\n      /mongo-seed/import.sh\n/mongo-seed/import.sh\n#! /bin/bash\n\nmongoimport --host mongodb --db test --collection census --type json --file /mongo-seed/census.json --jsonArray",
    "Docker \"Failed to solve: Canceled: context canceled\" when loading build context": "Altough downgrading docker worked, the actual problem was that I didn't exclude node_modules in Dockerignore. I had been running the containers fine for quite a long time.\nAfter purging all containers and images and adding the node_modules/ line to my .dockerignore it fixed it.\nI'm guessing the error had to do with the amount of files inside certain directories.",
    "How to dockerize Jupyter lab": "When you start jupyter lab you should define --ip parameter. For example, --ip=0.0.0.0.\nAfter this you will have another error:\n[C 08:14:56.973 LabApp] Running as root is not recommended. Use --allow-root to bypass.\nSo, if you want to proceed you need to add --allow-root as well.\nThe final Dockerfile is:\nFROM python:3.6\n\nWORKDIR /jup\n\nRUN pip install jupyter -U && pip install jupyterlab\n\nEXPOSE 8888\n\nENTRYPOINT [\"jupyter\", \"lab\",\"--ip=0.0.0.0\",\"--allow-root\"]",
    "rpc error: code = Unknown desc = failed to build LLB": "After a whole day of struggle, I fixed this by giving Docker more RAM (from 2Gb to 6Gb) and CPU (from 2 to 3)...",
    "Build docker in ASP.NET Core: \"no such file or directory\" error": "It's happening because you didn't published your solution. The error message is self-explanatory:\nno such file or directory\nBy default, when you add Docker support to you ASP.NET Core Visual Studio 2017 project, it creates a bunch of docker-compose.yml files, one of them is docker-compose.ci.build.yml which handles the build process. Then, when you build the project through Visual Studio, full docker-compose pipeline is executed.\nThe contents of docker-compose.ci.build.yml, are similiar to this (it depends on custom config and project names obviously):\nversion: '2'\n\nservices:\n  ci-build:\n    image: microsoft/aspnetcore-build:1.0-1.1\n    volumes:\n      - .:/src\n    working_dir: /src\n    command: /bin/bash -c \"dotnet restore ./SolutionName.sln && dotnet publish ./SolutionName.sln -c Release -o ./obj/Docker/publish\"\nAs you can see in the last line, there is a dotnet publish command invoked, which actually builds & publishes your project.\nSo the solution for your issue, will be just building the project before calling docker:\ndotnet publish ./SolutionName.sln -c Release -o ./obj/Docker/publish\ndocker build -t my-docker-image-test .",
    "Request against localhost relative url \"Cannot assign requested address\"": "I am using the same ASP.NET Core docker version and it seems work if you do this:\nreplace http://localhost:<port>\nwith\nhttp://host.docker.internal:<port>\nTry again and check if it works for you!",
    "Docker compose global level logging": "You could also configure the Docker default for this, all your container will have the configuration (that you can override per container).\nHere an example of solution with YAML anchor:\nversion: \"2\"\n\nservices:\n\n  proxy:\n    build: proxy\n    image: kinoulink/proxy\n    ports:\n      - 80:80\n      - 443:443\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    container_name: ktv_manager_proxy\n    environment:\n        - HTTP_AUTH_PASSWORD=$KTV_MANAGER_PASSWORD\n    logging: &logging\n      driver: \"awslogs\"\n      options:\n      awslogs-region: eu-west-1\n      awslogs-group: docker\n\n  rancher:\n    image: rancher/server:v1.1.3\n    volumes:\n      - rancher_mysql:/var/lib/mysql\n      - rancher_cattle:/var/lib/cattle\n    labels:\n      ktv.infra.proxy.domain: 'rancher'\n      ktv.infra.proxy.port: '8080'\n    logging:\n      <<: *logging\nFrom v3.4 (as @tekHedd said), you can use \"extension field\" syntax:\nversion: \"3.4\"\n\nx-logging: \n      &default-logging\n      driver: \"awslogs\"\n      options:\n      awslogs-region: eu-west-1\n      awslogs-group: docker\n\nservices:\nproxy:\n    build: proxy\n    image: kinoulink/proxy\n    ports:\n      - 80:80\n      - 443:443\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    container_name: ktv_manager_proxy\n    environment:\n        - HTTP_AUTH_PASSWORD=$KTV_MANAGER_PASSWORD\n    logging: *default-logging\n\n  rancher:\n    image: rancher/server:v1.1.3\n    volumes:\n      - rancher_mysql:/var/lib/mysql\n      - rancher_cattle:/var/lib/cattle\n    labels:\n      ktv.infra.proxy.domain: 'rancher'\n      ktv.infra.proxy.port: '8080'\n    logging: *default-logging",
    "Docker add files to VOLUME": "Whatever you put in your Dockerfile is just evaluated at build time (and not when you are creating a new container).\nIf you want to make file from the host available in your container use a data volume:\ndocker run -v /host_dir:/container_dir ...\nIn case you just want to copy files from the host to a container as a one-off operation you can use:\ndocker cp /host_dir mycontainer:/container_dir",
    "Dockerfile COPY and RUN in one layer": "take a look to multi-stage:\nUse multi-stage builds\nWith multi-stage builds, you use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each of them begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don\u2019t want in the final image. To show how this works, let\u2019s adapt the Dockerfile from the previous section to use multi-stage builds.\nDockerfile:\nFROM golang:1.7.3\nWORKDIR /go/src/github.com/alexellis/href-counter/\nRUN go get -d -v golang.org/x/net/html  \nCOPY app.go .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\nFROM alpine:latest  \nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\nCOPY --from=0 /go/src/github.com/alexellis/href-counter/app .\nCMD [\"./app\"]  ",
    "Passing env variables to DOCKER Spring Boot": "The easiest (and probably the best way) to do it via environment variable in a docker container:\nSPRING_PROFILES_ACTIVE=dev,swagger\nUPDATE:\nIn order to set environment variables to docker, you do not need to modify Dockerfile. Just build your docker image and then run it with the env variables set:\ndocker run your-docker-container -e SPRING_PROFILES_ACTIVE='dev,swagger' -p 8080:8080",
    "run sed in dockerfile to replace text with build arg value": "Replace the single quotes ' around s/localhost/${DOCKER_HOST}/g with double quotes \". Variables will not be interpolated within single quotes.",
    "Running an executable in a dockerfile": "Bear in mind that name.exe have to be in same directory as your dockerfile. From the documentation:\nThe <src> path must be inside the context of the build; you cannot COPY ../something /something, because the first step of a docker build is to send the context directory (and subdirectories) to the docker daemon.\nYour dockerfile could look like this then:\nFROM ubuntu\nMAINTAINER me@gmail.com\nCOPY name.exe /bin/\nCMD [\"/bin/name.exe\", \"input1\", \"output\"]\nYou can build it then like this:\ndocker build --tag=me/my-image .\nAnd when you run it (docker run me/my-image), it will run /bin/name.exe input1 output.",
    "Docker alpine + oracle java: cannot find java": "You cannot achieve what you want\nAlpine Linux uses MUSL as a Standard C library.\nOracle's Java for linux depends on GNU Standard C library (gclib).\nHere is a bit more detailed info and official stance from Oracle on the topic\nthe JDK source code has not yet been ported to Alpine Linux, or more specifically, the musl C library. That is, it turns out that the thing about Alpine Linux that sticks out/is different from a JDK source code perspective is the C library.\nThe solution\nIf you looking for small Java Docker images, use OpenJDK ones.\nopenjdk:11-jre-slim image is only 77MB.\nIf you insist, on your head be it...\nThere is theoretical way, but it is not as trivial as you think.\nYou can find many examples of Alpine images running with OracleJDK like here or see expert's answer to this question as well. They add the missing Standard GNU C library.\nBe warned however...\nAll of these solutions could be in breach of Oracle's license agreement stating that the license is non-transferable, and the distributable is non-modifiable. In the Dockerfiles you will find however:\nCookie: oraclelicense=accept-securebackup-cookie\"\nand many entries similar to\nrm -rf ${JAVA_HOME}/*src.zip\nFor further details about legality of prepackaged Oracle's JRE or JDK Docker images see this article.",
    "Run SQL script after start of SQL Server on docker": "RUN gets used to build the layers in an image. CMD is the command that is run when you launch an instance (a \"container\") of the built image.\nAlso, if your script depends on those environment variables, if it's an older version of Docker, it might fail because those variables are not defined the way you want them defined!\nIn older versions of docker the Dockerfile ENV command uses spaces instead of \"=\"\nYour Dockerfile should probably be:\nFROM microsoft/mssql-server-windows-express\nCOPY ./create-db.sql .\nENV ACCEPT_EULA Y\nENV SA_PASSWORD ##$wo0RD!\nRUN sqlcmd -i create-db.sql \nThis will create an image containing the database with your password inside it.\n(If the SQL file somehow uses the environment variables, this wouldn't make sense as you might as well update the SQL file before you copy it over.) If you want to be able to override the password between the docker build and docker run steps, by using docker run --env sa_password=##$wo0RD! ..., you will need to change the last line to:\nCMD sqlcmd -i create-db.sql && .\\start -sa_password $env:SA_PASSWORD \\\n-ACCEPT_EULA $env:ACCEPT_EULA -attach_dbs \\\"$env:attach_dbs\\\" -Verbose\nWhich is a modified version of the CMD line that is inherited from the upstream image.",
    "Setting up docker nodejs application with local npm dependencies": "Yes, it's possible but a little bit ugly. The problem for you is that Docker is very restrictive when it comes to its build context. I'm not sure how familiar you are already with that concept, so here is the introduction from the documentation:\nThe docker build command builds an image from a Dockerfile and a context.\nFor example, docker build . uses . as its build context, and since it's not specified otherwise, ./Dockerfile as the Dockerfile. Files or paths outside the build context cannot be referenced in the Dockerfile (so no COPY ..).\nThe issue for you is that during a Docker build, the build context cannot be left. If you have multiple applications that you want to build, you would normally add a Dockerfile for each app.\nsrc/\n\u251c\u2500\u2500 apps   \n\u2502   \u251c\u2500\u2500 my_app\n\u2502   \u2502   \u2514\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 my_other_app\n\u2502       \u2514\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 shared\n    \u2514\u2500\u2500 shared_module\nNaturally, you would cd into my_app and use docker build . to build the application's Docker image. The issue with this is that you can't access ../../shared from the build, since it's outside of the context.\nSo you need to make sure both apps and shared is in the build context. One way would be to place all Dockerfile in src like so:\nsrc/\n\u251c\u2500\u2500 Dockerfile.my_app\n\u251c\u2500\u2500 Dockerfile.my_other\n\u251c\u2500\u2500 apps\n\u2502   \u251c\u2500\u2500 my_app\n\u2502   \u2514\u2500\u2500 my_other_app\n\u2514\u2500\u2500 shared\n    \u2514\u2500\u2500 shared_module\nYou can then build the applications by explicitly specifying the context and the Dockerfile:\nsrc$ docker build -f Dockerfile.my_app .\nAlternatively, you can keep the Dockerfiles inside my_app and my_other_app, and point to them:\nsrc$ docker build -f apps/my_app/Dockerfile .\nThat should also work. In both cases, the build is executed from within src, which means you need to pay a little attention to the paths in the Dockerfile. The working directory is still src:\nCOPY ./apps/my_app /src/apps/my_app\nBy mirroring the folder structure you have locally, you should be able to make your dependencies work without any changes:\nRUN mkdir -p /src\nCOPY ./shared /src/shared\nCOPY ./apps/my_app /src/apps/my_app\nRUN cd /src/apps/my_app && npm install\nHope that helps you get started.",
    "Tilde Expansion Doesn't Work in Docker COPY Command": "Tilde expansion for COPY is not supported.\nFrom The COPY docs:\nThe dest is an absolute path, or a path relative to WORKDIR, into which the source will be copied inside the destination container.\nExample:\nCOPY test relativeDir/   # adds \"test\" to `WORKDIR`/relativeDir/\nCOPY test /absoluteDir/  # adds \"test\" to /absoluteDir/",
    "How to build docker with non-root user privileges to setup python application with pipenv?": "There's nothing wrong with installing software \"globally\" in a Docker image (which will generally only do one thing), and to committing to some implementation details like container-internal usernames and paths. It's totally fine to install software as root and switch to a non-root user to actually run the image.\nI might write this Dockerfile like:\nFROM python:3.6\n\n# Globally install pipenv\nRUN pip3 install pipenv\n\n# Set up the app directory (Docker will create it for us)\nWORKDIR /myapp\nCOPY . ./\nRUN pipenv install --system --deploy --ignore-pipfile\n\n# Establish the runtime user (with no password and no sudo)\nRUN useradd -m myapp\nUSER myapp\n\n# Normal image metadata\nEXPOSE 8002\nCMD gunicorn -k tornado server:app -b 0.0.0.0:8002 -w 4 -p server.pid",
    "How can I run ENTRYPOINT as root user?": "Delete the USER jenkins line in your Dockefile.\nChange the user at the end of your entrypoint script (/root/startup.sh).\nby adding: su - jenkins man su\nExample:\nDockerfile\nFROM debian:8\n\nRUN useradd -ms /bin/bash exemple\n\nCOPY entrypoint.sh /root/entrypoint.sh\n\nENTRYPOINT \"/root/entrypoint.sh\"\nentrypoint.sh\n#!/bin/bash\n\necho \"I am root\" && id\n\nsu - exemple\n\n# needed to run parameters CMD\n$@\nNow you can run\n$ docker build -t so-test .\n$ docker run --rm -it so-test bash\nI am root\nuid=0(root) gid=0(root) groups=0(root)\nexemple@37b01e316a95:~$ id\nuid=1000(exemple) gid=1000(exemple) groups=1000(exemple)\nIt's just a simple example, you can also use the su -c option to run command with changing user.",
    "Pass arguments to parent Dockerfile": "Build arguments are not persisted in images, so they will not be available in builds FROM a parent image.\nUnlike an ARG instruction, ENV values are always persisted in the built image.\nARG variables are not persisted into the built image as ENV variables are.\nThe arguments can be persisted by storing them somewhere, the easiest place is in an environment variable.\nARG IMAGE_USER=jenkins\nENV IMAGE_USER=$IMAGE_USER\nAll RUN steps in the child image will then have access to IMAGE_USER in their environment.",
    "Reloading code in a dockerized node.js app with docker-compose": "I have seen the phrase \"live reload\" used to apply to two different types of reloading:\nkilling & restarting the application when the code changes, and\nautomatically reloading HTML & assets on a client browser when the code changes.\nBased on your question, I think you're referring to the first type, so the answer that follows addresses that.\nThe problem here is one of context.\nRemember that the docker container is isolated from your host - specifically, the processes running in the container are distinct from (and generally cannot interact with) processes running on the host. In your case, you have chosen to mount a host directory in the container, but that's just the filesystem, not the processes.\nThink through what your Docker image does when you instantiate a new container: it runs node index.js in the WORKDIR. Where is the code to stop it and restart it when the code changes? Presumably it's running in a process on the host. This means that it cannot touch the node process running in the container (because it's isolated).\nNow, you haven't mentioned what method you're using to handle the live reloading, but that shouldn't make too much of a difference. They all basically work the same way: on a change to the application code, kill the existing process and start a new one.\nTo solve this, you have two options:\nrun the \"live reloading\" code inside the container, or\nrun your development code outside the container\nFor the first, you could follow @MarkS's suggestion and use nodemon. This should be as simple as replacing\nCMD [\"node\", \"index.js\"]\nin your Dockerfile with\nCMD [\"nodemon\", \"index.js\"]\nprovided, of course, that you have nodemon properly installed in the image.\nThe alternative, and this is what I do, is to run the code on the host outside the Docker environment during development, and then package it up in an image at deployment. This solves two problems:\nthe problem you're running into with isolated node processes, and\npermission problems.\nRemember that apps running in Docker are run as root. This means that if your app creates files, they're going to be owned by root. I tried developing in a Docker environment, but got frustrated by problems where, for example, I wanted to delete a file created by the app and had to sudo (sign in as root) just to clean up stuff.",
    "Run Grunt / Gulp inside Docker container or outside?": "I'd like to suggest a third approach that I have done for a static generated site, the separate build image.\nIn this approach, your main Dockerfile (the one in project root) becomes a build and development image, basically doing everything in the second approach. However, you override the CMD at run time, which is to tar up the built dist folder into a dist.tar or similar.\nThen, you have another folder (something like image) that has a Dockerfile. The role of this image is only to serve up the dist.tar contents. So we do a docker cp <container_id_from_tar_run> /dist. Then the Dockerfile just installs our web server and has a ADD dist.tar /var/www.\nThe abstract is something like:\nBuild the builder Docker image (which gets you a working environment without webserver). At thist point, the application is built. We could run the container in development with grunt serve or whatever the command is to start our built in development server.\nInstead of running the server, we override the default command to tar up our dist folder. Something like tar -cf /dist.tar /myapp/dist.\nWe now have a temporary container with a /dist.tar artifact. Copy it to your actual deployment Docker folder we called image using docker cp <container_id_from_tar_run> /dist.tar ./image/.\nNow, we can build the small Docker image without all our development dependencies with docker build ./image.\nI like this approach because it is still all Docker. All the commands in this approach are Docker commands and you can really slim down the actual image you end up deploying.\nIf you want to check out an image with this approach in action, check out https://github.com/gliderlabs/docker-alpine which uses a builder image (in the builder folder) to build tar.gz files that then get copied to their respective Dockerfile folder.",
    "How do I change the save location of docker containers logs?": "Straight- forward answer - NO, you can't\nWhy?\nThe file written by the json-file logging driver are not intended for consumption by external software, and should be regarded the \"internal storage mechanism\" for the JSON logging driver. For that reason, the location is not configurable.\nIf you want to have the logs written to a different location, consider using (e.g.) the syslog driver, journald, or one of the drivers that allow sending the logs to a central log aggregator\nSource: https://github.com/moby/moby/issues/29680",
    "How to install kubectl in kubernetes container through docker image": "put this in your Dockerfile\nRUN curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\nRUN chmod +x ./kubectl\nRUN mv ./kubectl /usr/local/bin",
    "can't create virtualenv on Ubuntu 18.04 with Python 3.8": "As workaround\nsudo ln -s   /usr/lib/python3.8/_sysconfigdata__linux_x86_64-linux-gnu.py  /usr/lib/python3.8/_sysconfigdata__x86_64-linux-gnu.py\nBut i hope it will be fixed soon in Ubuntu.",
    "Install Oracle Instant client into Docker container for Python cx_Oracle": "After many hours trying it, I finally solved it with this Dockerfile\nNote I am using python 3.7, Django 3.0, Oracle Database 12c and Pipenv for package management\nFROM python:3.7.5-slim-buster\n\n# Installing Oracle instant client\nWORKDIR    /opt/oracle\nRUN        apt-get update && apt-get install -y libaio1 wget unzip \\\n            && wget https://download.oracle.com/otn_software/linux/instantclient/instantclient-basiclite-linuxx64.zip \\\n            && unzip instantclient-basiclite-linuxx64.zip \\\n            && rm -f instantclient-basiclite-linuxx64.zip \\\n            && cd /opt/oracle/instantclient* \\\n            && rm -f *jdbc* *occi* *mysql* *README *jar uidrvci genezi adrci \\\n            && echo /opt/oracle/instantclient* > /etc/ld.so.conf.d/oracle-instantclient.conf \\\n            && ldconfig\n\nWORKDIR    /app\nCOPY       . .  # Copy my project folder content into /app container directory\nRUN        pip3 install pipenv\nRUN        pipenv install\nEXPOSE     8000\n# For this statement to work you need to add the next two lines into Pipfilefile\n# [scripts]\n# server = \"python manage.py runserver 0.0.0.0:8000\"\nENTRYPOINT [\"pipenv\", \"run\", \"server\"]",
    "How to install a local rpm file when building docker instance?": "Put this line before your rpm -i command:\nADD /host/abs/path/to/chrpath-0.13-14.el7.x86_64.rpm /chrpath-0.13-14.el7.x86_64.rpm\nThen you'll be able to do\nRUN rpm -i chrpath-0.13-14.el7.x86_64.rpm",
    "How to retrieve file from docker container?": "There are multiple ways to do this.\nUsing docker cp:\ndocker cp <container_hash>:/path/to/zip/file.zip /path/on/host/new_name.zip\nUsing docker volumes:\nAs you were leading to in your question, you can also mount a path from the container to your host. You can either do this by specifying where on the host you want the mount point to be or don't specify where the mount point is and let docker choose. Both these paths require different approaches.\nLet docker choose host mount location\ndocker volume create random_volume_name\ndocker run -d --name ubuntu-libs -v random_volume_name:<path/to/mount/in/container> ubuntu-libs\nThe content will be located on your host, here:\nls -l /var/lib/docker/volumes/random_volume_name/_data/ \nLet me choose host mount location\ndocker run -d --name ubuntu-libs -v <existing/mount/point/on/host>:<path/to/mount/in/container> ubuntu-libs\nThis creates a clean/empty location that is shared as per the locations defined in the command. Now you need to modify your Dockerfile to copy the artifacts to this path, something like:\nFROM ubuntu\n\nRUN apt-get update && apt-get install -y build-essentials gcc\n\nENTRYPOINT [\"zip\",\"-r\",\"-9\"]\nCMD [\"sh\", \"-c\", \"/lib64.zip\", \"/lib64\", \"cp\", \"path/to/zip/file.zip\", \"<path/to/mount/in/container>\"]\nThe content will now be located on your host, here:\nls -l <existing/mount/point/on/host>\nI got to give a shout out to @joaofnfernandes from here, who does a great job explaining.",
    "How to setup Node environment variable in Dockerfile for running node.js application?": "There a two ways, while building the image or when running the container.\nFor builds:\nAdd to your Dockerfile\nENV NODE_ENV=whatEver\nOr use build arguments if you don't want them to stick during runtime Docker build --build-args NODE_ENV whatEver\nWhen running:\nRun your container with \"-e\"\ndocker run -e NODE_ENV=whatever mycontainer",
    "How to check for unuse images for your docker containers?": "Above answers help us find and remove the dangling images,, but not unused.\nSo to fetch all the unused docker images on the machine\nFetch all the images belonging to the running containers(which are not stopped or exited)\nFetch all the images on the machine\nThen filter the images in step 1 from step 2\nBelow is the basic shell script which could help do that\nrunningImages=$(docker ps --format {{.Image}})\ndocker images --format {{.Repository}}:{{.Tag}} | grep -v \"$runningImages\"\nJust be sure before removing unused images(not the dangling ones) just list them and then decide which one to delete manually.",
    "How to install and run wkhtmltopdf Docker image": "Perhaps this solution will help. Wkhtmltopdf will be install to /usr/bin/wkhtmltopdf\nRUN apt-get update \\\n    && apt-get install -y \\\n    ...\n    wkhtmltopdf \\\n    ...",
    "Docker: Error response from daemon: rpc error: code = 2 desc = \"oci runtime error: exec format error\"": "Did you post your complete entrypoint.sh? The kernel tries to recognize the file type by looking at the first bytes of the executable. For scripts you need to add a so-called shebang line. You might need to add a shebang line at the very top of your entrypoint.sh, e.g.:\n#!/bin/sh\n/usr/bin/docker-quickstart\nservice hadoop-hdfs-namenode restart\nhdfs dfs -mkdir -p input\nhdfs dfs -put /twitter.avro /input/twitter.avro\nspark-submit --class com.abhi.HelloWorld --master local[1] SparkIntegrationTestsAssembly.jar /input/twitter.avro /output",
    "Docker Django 404 for web static files, but fine for admin static files": "This was issue with the STATICFILES_DIRS configuration in the settings.py file.\nThis setting defines the additional locations the staticfiles app will traverse if the FileSystemFinder finder is enabled, e.g. if you use the collectstatic or findstatic management command or use the static file serving view.\nFollowing was the configuration in my settings.py:\nSTATIC_URL = '/static/'\nSTATIC_ROOT      =  os.path.join(BASE_DIR, \"static\") \nNow I updated this code to:\nSTATIC_URL = '/static/'\nSTATICFILES_DIRS = [\n    os.path.join(BASE_DIR, \"static\"),\n]\nAnd every files is loading fine.\nReference Link",
    "How to answer install question in dockerfile?": "I tested your Dockerfile and didn't have to answer any question, but if you want to auto answer when you run a command, you can use yes (for \"y\" and \"n\" responses) or echo.\nEx:\nyes | <YOUR COMMAND>\nyes n | <YOUR COMMAND> \"n\" response\necho | <YOUR COMMAND> echo generate a new line (enter)\necho \"some response\" | <YOUR COMMAND> response with \"some response\"",
    "What Docker image size is considered 'too large'?": "In my opinion, ideal size is only ideal for your exact case. For me and my current company, we have no image bigger than 1GB.\nIf you use an image of 10GB size and have no problems (is it even possible?!), then it is ok for your case.\nAs example of a problem case, you could consider a question such as: \"Is it ok that I am waiting 1-2 hours while my image is deploying over internet to the remote server/dev machine?\" In all likelihood, this is not ok. On the another hand, if you are not facing such a problem, then you have no problems at all.\nAnother problem is while small images start up for a couple of seconds, the huge one starts up for minutes. It also can break a \"hot deploy\" scheme if you use it.\nIt also could be appropriate to check why your image is so big. You can read how layers work.\nConsider the following two Dockerfiles:\nFirst:\nRUN download something huge that weighs 5GB\nRUN remove that something huge from above\nSecond:\nRUN download something huge that weighs 5GB &&\\\n    remove that something huge from above\nThe image built from the second Dockerfile weighs 5GB less than that from the first, while they are the same inside.\nAnother trick is to use a small, basic image from the beginning. Just compare these differences:\nIMAGE NAME     SIZE\nbusybox        1 MB\nalpine         3 MB\ndebian         125 MB\nubuntu         188 MB \nWhile debian and ubuntu are almost the same inside, debian will save you 50MB from the start, and will need fewer dependencies in future.",
    "Output of `tail -f` at the end of a docker CMD is not showing": "The docker filesystem uses copy-on-write with it's layered union fs. So when you write to a file that's part of the image, it will first make a copy of that file to the container filesystem which is a layer above all the image layers.\nWhat that means is when you append a line to the /var/log/cron.log, it will get a new inode in the filesystem and the file that the tail command is following at is no longer the one you see when you docker exec into the container. You can solve that with a minor change to append \"nothing\" to the file which also modifies the last update timestamp which forces a copy-on-write:\nCMD echo \"starting\" && echo \"continuing\" && (cron) \\\n && echo \"tailing...\" && : >> /var/log/cron.log && tail -f /var/log/cron.log\nI put together a gist that goes through this issue with a lot more detail over here: https://gist.github.com/sudo-bmitch/f91a943174d6aff5a57904485670a9eb",
    "Cannot assign requested address (localhost:xxxx) - Docker + Linux Containers": "Using host.docker.internal instead of localhost worked for me.\nSource: https://hamy.xyz/labs/docker-dotnet-core-cannot-assign-requested-address",
    "Building Docker image as non root user": "In order to use Docker, you don't need to be a root user, you just need to be inside of the docker user group.\nOn Linux:\nIf there is not already a docker group, you can create one using the command sudo groupadd docker.\nAdd yourself and any other users you would like to be able to access docker to this group using the command sudo usermod -aG docker [username of user].\nRelog, so that Linux can re-evaluate user groups.\nIf you are not trying to run the command as root, but rather want to run the container as non-root, you can use the following DOCKERFILE contents (insert after FROM but before anything else.)\n# Add a new user \"john\" with user id 8877\nRUN useradd -u 8877 john\n# Change to non-root privilege\nUSER john",
    "Which Dockerfile instructions are inherited in deriving images?": "These instruction are inherited from the base image along with system files.\nEXPOSE\nIf the base image mentioned these EXPOSE 8080 9090 ports in Dockerfile, then the extend Dockerfile do not to need to expose these port. But there is a difference between exposing and publish.\nENV\nIf the base image has some ENV like test-a=abc then extended image will have these ENVs.\nWorkingDir\nIf the base image have set \"WorkingDir\": \"/root\", then extended iamge will have working direcotry /root\nMAINTAINER\nMAINTAINER adiii extended image will have the same author if not overide.\nLabels\nThe extended image will have the same label as the base image\nonbuild\nDesigned to run by extended image.\nENTRYPOINT\nSame entrypoint as in the base image, unless you overwrite it.\nCMD\nThe extended image has the same CMD as the base image, as long as you do not overwrite the entrypoint instruction, see below.\nYou can try it.\nDockerfile A\nFROM node:8.16\nMAINTAINER adiii\nLABEL key=test\nEXPOSE 8080 9090\nENV test-a=abc\nWORKDIR /root\nENTRYPOINT /root\nCMD [\"npm\", \"run\", \"start\"]\nNow build docker image B\nDockerfile B\nFROM a\ndocker build -t b . Inspect the image b docker inspect b:latest you will see the above instruction iherited from the base image, because Dockerfile B did not overwrite the entrypoint instruction.\nIf the extended image overwrites the entrypoint, the documentation says CMD will reset to an empty value and must be redefined if wanted.",
    "adding startup script to dockerfile": "You can use the entrypoint to run the startup script. In the entrypoint you can specify your custom script, and then run catlina.sh.\nExample:\nENTRYPOINT \"bin/startup.sh && catalina.sh run\"\nThis will run your startup script and then start your tomcat server, and it won't exit the container.",
    "git commit hash in dockerfile as label": "I found it atlast,\nuse docker build --build-arg vcs-ref=$(git rev-parse --short HEAD)\nwhile building.\nBut have to initialize the variable in vcs-ref in Dockerfile\nARG vcs-ref=0\nENV vcs-ref=$vcs-ref",
    "How to push docker compose to docker hub": "You should add image names to your services, including your docker hub id, e.g.:\nservices:\n  web:\n    build: ./\n    image: docker-hub-id/web:latest\n    ...\nNow, you can just call docker-compose push.\nSee docker-compose push",
    "Unable to run a Docker image with a Rust executable": "As @Oleg Sklyar pointed out, the problem is that the Rust binary is dynamically-linked.\nThis may be a bit confusing because many people who have heard of Rust have also heard that Rust binaries are statically-linked, but this refers to the Rust code in crates: crates are linked statically because they are all known at the moment of compilation. This does not refer to existing C dynamic libraries that the program may link to, such as libc and other must-have libraries. Often times, these libraries can also be built as statically-linked artifacts (see the end of this post). To check whether your program or library is dynamically-linked, you can use ldd utility:\n$ ldd target/release/t\n    linux-vdso.so.1 (0x00007ffe43797000)\n    libdl.so.2 => /usr/lib/libdl.so.2 (0x00007fa78482d000)\n    librt.so.1 => /usr/lib/librt.so.1 (0x00007fa784625000)\n    libpthread.so.0 => /usr/lib/libpthread.so.0 (0x00007fa784407000)\n    libgcc_s.so.1 => /usr/lib/libgcc_s.so.1 (0x00007fa7841f0000)\n    libc.so.6 => /usr/lib/libc.so.6 (0x00007fa783e39000)\n    /lib64/ld-linux-x86-64.so.2 => /usr/lib64/ld-linux-x86-64.so.2 (0x00007fa784ca2000)\nYou'll need these libraries in your Docker image. You will also need the interpreter; to get its path you can use objdump utility:\n$ LANG=en objdump -s -j .interp target/release/t\n\ntarget/release/t:     file format elf64-x86-64\n\nContents of section .interp:\n 0270 2f6c6962 36342f6c 642d6c69 6e75782d  /lib64/ld-linux-\n 0280 7838362d 36342e73 6f2e3200           x86-64.so.2.  \nCopy the files into the expected directories and everything works okay.\nThere is also a second option which is to use the rust-musl-builder docker image. There are some problems with postgresql and diesel but for most of projects it would be good. It works by producing a statically-linked executable which you may just copy and use. This option is much more preferred than using an interpreter and dynamic libraries if you want to provide a docker image with less size and without having all that useless extra data such as interpreter, unused libraries and so on.",
    "unable to edit /etc/resolv.conf in docker container": "This is by design. /etc/resolv.conf is used by docker engine to handle service discovery. Documentation states the following:\nHow can Docker supply each container with a hostname and DNS configuration, without having to build a custom image with the hostname written inside? Its trick is to overlay three crucial /etc files inside the container with virtual files where it can write fresh information \u2026 This arrangement allows Docker to do clever things like keep resolv.conf up to date across all containers when the host machine receives new configuration over DHCP later. The exact details of how Docker maintains these files inside the container can change from one Docker version to the next, so you should leave the files themselves alone and use the following Docker options instead.\nIf you want to override/reconfigure some dns settings, use --dns parameters during container starting. See more details:\nConfigure DNS in Docker",
    "OCI runtime create failed: container_linux.go:296 - no such file or directory": "OCI runtime create failed: container_linux.go:296\nIn my experience this is an error with the docker daemon itself, not the container you are trying to run. Try deleting all containers, restarting the daemon. I think we also had to clean up the docker networks.",
    "How to install VS Code extensions in a Dockerfile?": "If your goal is not to repeat the installation of the VS code extensions, my suggestion is to mount $HOME/.vscode-server/.\nFor example, in a docker-compose.yml\nservices:\n    your_container:\n        ...\n        volumes:\n            - ./volume/vscode-server:$HOME/.vscode-server\nOr in docker run\ndocker run -it -v ./volume/vscode-server:$HOME/.vscode-server your_image bash\nThen, install the required extensions inside the container. The next time you set up the container, there will be no need to reinstall extensions.",
    "How to execute the Entrypoint of a Docker images at each \"exec\" command?": "if your goal is to run the docker exec with a specific user inside of the container, you can use the --user option.\ndocker exec --user myuser container-name [... your command here]\nIf you want to run gosu every time, you can specify that as the command with docker exec\ndocke exec container-name gosu 1000:1000 [your actual command here]\nin my experience, the best way to encapsulate this into something easily re-usable is with a .sh script (or .cmd file in windows).\ndrop this into a file in your local folder... maybe gs for example.\n#! /bin/sh\ndocker exec container-name gosu 1000:1000 \"$@\"\ngive it execute permissions with chmod +x gs and then run it with ./gs from the local folder",
    "How do you install something that needs restart in a Dockerfile?": "This entirely depends on why they require a reboot. For Linux, rebooting a machine would typically indicate a kernel modification, though it's possible it's for something more simple like a change in user permissions (which would be handled by logging out and back in again). If the install is trying to make an OS level change to the kernel, it should fail if done inside of a container. By default, containers isolate and restrict what the application can do to the running host OS which would impact the host or other running containers.\nIf, the reboot is to force the application service to restart, you should realize that this design doesn't map well to a container since each RUN command runs just that command in an isolated environment. And by running only that command, this also indicates that any OS services that would normally be started on OS bootup (cron, sendmail, or your application) will not be started in the container. Therefore, you'll need to find a way to run the installation command in addition to restarting any dependent services.\nThe last scenario I can think of they want different user permissions to take effect to the logged in user. In that case, the next RUN command will run the requested command with any changed access from prior RUN commands. So there's no need to take any specific action of your own to do a reboot, simply perform the install steps as if there's a complete restart between each step.",
    "Couldn't connect to Docker daemon at http+docker://localhost with docker-compose": "Check your privileges, Following command solved my problem :\nsudo chown $USER /var/run/docker.sock\nIt happens when you try to start docker as non super user and it couldn't get access to it own sockets.",
    "Docker run --mount make all files available in a different folder during RUN": "I think you have misunderstood what the RUN --mount=type=bind... syntax is for. From the documentation:\nThis mount type allows binding directories (read-only) in the context or in an image to the build container.\nIn other words, this does not permit you to access arbitrary host directories in the build stage. It is not an analog to the -v command line option on docker run. It only permits you to:\nMount directories from your build context, or\nMount directories from another stage in a multi-stage build\nSo for example I can do this do mount a directory from one build stage into a subsequent build stage:\n# syntax=docker/dockerfile:experimental\n\nFROM centos AS centos\n\nFROM alpine\nRUN --mount=type=bind,from=centos,source=/,target=/centos ls /centos > /root/centos.txt\nOr if I have a directory named example in my build context, I can do this to mount it during the build process:\n# syntax=docker/dockerfile:experimental\n\nFROM centos AS centos\n\nFROM alpine\nRUN --mount=type=bind,source=example,target=/data cp /data/* /root/\nThe syntax you're using (with no from specified)...\nRUN --mount=type=bind,target=/path/on/host\n...simply mounts the root of your build context on /path/on/host inside the container. Remember that target specifies the mountpoint inside the container. E.g., if my build context looks like this:\n.\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 example\n    \u2514\u2500\u2500 README.md\nAnd example/README.md contains:\nThis is a test.\nAnd the Dockerfile contains a RUN option similar to what you're using:\n# syntax=docker/dockerfile:experimental\n\nFROM centos AS centos\n\nFROM alpine\nRUN --mount=type=bind,target=/data cat /data/example/README.md > /root/README.md\nThen when the image is built, /root/README.md has the contents of example/README.md.",
    "Running apt-get update on docker ubuntu image on Mac causes \"File has unexpected size\"": "I had the same issue and the reasons turned out to be that I had content restriction set on my iPhone and these were automatically synced to my Mac.\nAfter switching the restrictions off everything worked as it should.\nGo to System Preferences > Screen Time > Turn Content & Privacy Restrictions off",
    "How to pnpm and Next.js in multi-stage docker file?": "Another solution is installing pnpm using npm. When you install nodejs it comes with npm as the default package manager. So you can install pnpm using npm using the following command npm install -g pnpm\nIn the docker file it will be written as;\nRUN npm install -g pnpm",
    "CMD in dockerfile vs command in docker-compose.yml": "In the common case, you should have a Dockerfile CMD and not a Compose command:.\ncommand: in the Compose file overrides CMD in the Dockerfile. There are some minor syntactic differences (notably, Compose will never automatically insert a sh -c shell wrapper for you) but they control the same thing in the container metadata.\nHowever, remember that there are other ways to run a container besides Compose. docker run won't read your docker-compose.yml file and so won't see that command: line; it's also not read in tools like Kubernetes. If you build the CMD into the image, it will be honored in all of these places.\nThe place where you do need a command: override is if you need to launch a non-default main process for a container.\nImagine you're building a Python application. You might have a main Django application and a Celery worker, but these have basically the same source code. So for this setup you might make the image's CMD launch the Django server, and override command: to run a Celery worker off the same image.\n# Dockerfile\n# ENTRYPOINT is not required\nCMD [\"./manage.py\", \"runserver\", \"0.0.0.0:8080\"]\n# docker-compose.yml\nversion: '3.8'\nservices:\n  web:\n    build: .\n    ports: ['8080:8080']\n    # no command:\n  worker:\n    build: .\n    command: celery worker",
    "How to set breakpoint in Dockerfile itself?": "You can't set a breakpoint per se, but you can get an interactive shell at an arbitrary point in your build sequence (between steps).\nLet's build your image:\nSending build context to Docker daemon  2.048kB\nStep 1/3 : FROM ubuntu:20.04\n ---> 1e4467b07108\nStep 2/3 : RUN echo \"hello\"\n ---> Running in 917b34190e35\nhello\nRemoving intermediate container 917b34190e35\n ---> 12ebbdc1e72d\nStep 3/3 : RUN echo \"bye\"\n ---> Running in c2a4a71ae444\nbye\nRemoving intermediate container c2a4a71ae444\n ---> 3c52993b0185\nSuccessfully built 3c52993b0185\nEach of the lines that says ---> 0123456789ab with a hex ID has a valid image ID. So from here you can\ndocker run --rm -it 12ebbdc1e72d sh\nwhich will give you an interactive shell on the partial image resulting from the first RUN command.\nThere's no requirement that the build as a whole succeed. If a RUN step fails, you can use this technique to get an interactive shell on the image immediately before that step and re-run the command by hand. If you have a very long RUN command, you may need to break it into two to be able to get a debugging shell at a specific point within the command sequence.",
    "How to make docker-compose pull new images?": "I think you're looking for docker-compose pull:\n$ docker-compose help pull\nPulls images for services defined in a Compose file, but does not start the containers.\nSo docker-compose pull && docker-compose up should do what you want, without needing to constantly wipe your cache or hard-code container names outside of your compose file",
    "Dockerfile CMD not running at container start": "This:\ndocker run -d -p expoPort:contPort -t -i -v /$MOUNTED_VOLUME_DIR/$PROJECT:/$MOUNTED_VOLUME_DIR $CONTAINER_ID /bin/bash\nSays 'run /bin/bash' after instantiating the container. E.g. skip CMD.\nTry this:\ndocker run -d -p expoPort:contPort -t -i -v /$MOUNTED_VOLUME_DIR/$PROJECT:/$MOUNTED_VOLUME_DIR $CONTAINER_ID ",
    "docker copy file from one container to another?": "In recent versions of docker, named volumes replace data containers as the easy way to share data between containers.\ndocker volume create --name myshare\ndocker run -v myshare:/shared task1\ndocker run -v myshare:/shared -p 8080:8080 task2\n...\nThose commands will set up one local volume, and the -v myshare:/shared argument will make that share available as the folder /shared inside each of each container.\nTo express that in a compose file:\nversion: '2'\nservices:\n  task1:\n    build: ./task1\n  volumes:\n    - 'myshare:/shared'\n\n  task2:\n    build: ./task2\n  ports:\n    - '8080:8080'\n  volumes:\n    - 'myshare:/shared'\n\nvolumes:\n  myshare:\n    driver: local \nTo test this out, I made a small project:\n- docker-compose.yml (above)\n- task1/Dockerfile\n- task1/app.py\n- task2/Dockerfile\nI used node's http-server as task2/Dockerfile:\nFROM node\nRUN npm install -g http-server\nWORKDIR /shared\nCMD http-server\nand task1/Dockerfile used python:alpine, to show two different stacks writing and reading.\nFROM python:alpine\nWORKDIR /app\nCOPY . .\nCMD python app.py\nhere's task1/app.py\nimport time\n\ncount = 0\nwhile True:\n  fname = '/shared/{}.txt'.format(count)\n  with open(fname, 'w') as f:\n    f.write('content {}'.format(count))\n    count = count + 1\n  time.sleep(10)\nTake those four files, and run them via docker compose up in the directory with docker-compose.yml - then visit $DOCKER_HOST:8080 to see a steadily updated list of files.\nAlso, I'm using docker version 1.12.0 and compose version 1.8.0 but this should work for a few versions back.\nAnd be sure to check out the docker docs for details I've probably missed here:\nhttps://docs.docker.com/engine/tutorials/dockervolumes/",
    "Docker ENTRYPOINT to run after Volume Mount": "I'm not sure if this is the solution you want but I've been using this run command which uses cat command to supply my script.sh to the container:\ndocker run -it --name=some_name --rm \\\n  -v \"host/path:/path/inside/container\" \\\n  image_name \\\n  /bin/bash  -c \"$(cat ./script.sh)\"\nIn this case the script runs after the mount is complete. I am sure of this as I've used the files from the mounted volumes in the script.",
    "COPY failed: stat /var/lib/docker/tmp/docker-xxx : no such file or directory": "When you run\ndocker build . --file backend/Dockerfile ...\nThe path argument . becomes the context directory. (Docker actually sends itself a copy of this directory tree, which is where the /var/lib/docker/tmp/... path comes from.) The source arguments of COPY and ADD instructions are relative to the context directory, not relative to the Dockerfile.\nIf your source tree looks like\n.\n+-- backend\n| \\-- Dockerfile\n\\-- target\n  \\-- demo-0.0.1-SNAPSHOT.jar\nthat matches the Dockerfile you show. But if instead you have\n.\n+-- backend\n  +-- Dockerfile\n  \\-- target\n    \\-- demo-0.0.1-SNAPSHOT.jar\nyou'll get the error you see.\nIf you don't need to refer to anything outside of the context directory, you can just change what directory you're passing to docker build\nCOPY target/demo-0.0.1-SNAPSHOT.jar /opt/demo-0.0.1/lib/demo-0.0.1-SNAPSHOT.jar\ndocker build backend ...\nOr, if you do have other content you need to copy in, you need to change the COPY paths to be relative to the topmost directory.\nCOPY backend/target/demo-0.0.1-SNAPSHOT.jar /opt/demo-0.0.1/lib/demo-0.0.1-SNAPSHOT.jar\nCOPY common/config/demo.yml /opt/demo-0.0.1/etc/demo.yml\ndocker build . -f backend/Dockerfile ...",
    "How do I get python2.7 and 3.7 both installed in an alpine docker image": "Use something like:\nRUN apk add --no-cache python2\nThis will install the latest version of Python 2 as python2 or python2.7. Python 3.7.3 will still be available using python3, or simply python.",
    "Npm install is failing with docker buildx linux/arm64": "A bit late, but for all others that have the same error. Check if you have installed multi platform support for buildx:\ndocker run --privileged --rm tonistiigi/binfmt --install all\nThe full documentation is on the Docker page here",
    "Heroku (Docker) PORT environment variable in nginx": "I got it working for my app by following this example :\nStep 1: listen to $PORT in default.conf.template\nserver {\n  listen $PORT default_server;\n\n  location / {\n    root   /usr/share/nginx/html;\n    index  index.html;\n  }\n}\nStep 2: add this directive to your Dockerfile\nCOPY default.conf.template /etc/nginx/conf.d/default.conf.template\nStep 3: add this at the end of your Dockerfile\nCMD /bin/bash -c \"envsubst '\\$PORT' < /etc/nginx/conf.d/default.conf.template > /etc/nginx/conf.d/default.conf\" && nginx -g 'daemon off;'",
    "Visual Studio 2017 Docker - change the target for multi stage builds": "To use build target configuration from docker file, you need to add a docker-compose.vs.debug.yml file. In that file, just specify build target for the service and that's it.\nExample:\nversion: '3.4'\n\nservices:\n  my.api:\n    build:\n      target: build",
    "Linking django and mysql containers using docker-compose": "In Django settings.py file make sure you have something like:\nDATABASES = {\n'default': {\n    'ENGINE': 'django.db.backends.mysql',\n    'NAME': 'django1',\n    'USER': 'django',\n    'PASSWORD': 'password', \n    'HOST': 'db',\n    'PORT': 3306,\n    }\n}\nthen in your docker-compose.yml file make sure you have something along the lines of:\ndb:\n  image: mysql\n  environment:\n    MYSQL_ROOT_PASSWORD: docker\n    MYSQL_DATABASE: docker\n    MYSQL_USER: docker\n    MYSQL_PASSWORD: docker\nthen as per the docker/django tutorial you are following run the following again to rebuild everything and things should start working\ndocker-compose run web django-admin.py startproject composeexample .\nIn response to a further question, the mysql root password variable is required by docker when creating new databases.\nEDIT: added run to docker-compose above; see edit comment",
    "Docker ENV in CMD": "To use environment variables, you need to use shell.\nhttps://docs.docker.com/engine/reference/builder/#cmd\nNote: Unlike the shell form, the exec form does not invoke a command shell. This means that normal shell processing does not happen. For example, CMD [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: CMD [ \"sh\", \"-c\", \"echo $HOME\" ]. When using the exec form and executing a shell directly, as in the case for the shell form, it is the shell that is doing the environment variable expansion, not docker.\nBased on this, I think you can work fine by the following Dockerfile.\nFROM ubuntu:xenial\n\nARG EXECUTABLE\n\nENV EXECUTABLE ${EXECUTABLE}\n\nCMD [ \"sh\", \"-c\", \"/opt/foo/bin/${EXECUTABLE}\", \"-bar\"]",
    "Microsoft Compiler in Docker": "I assume you can already run Windows containers, ex. docker run -it microsoft/windowsservercore\nHere is my Dockerfile to install Visual C++ Build Tools 2015 in Docker container using Chocolatey:\n# escape=`\n\nFROM microsoft/windowsservercore\n\n# Install chocolatey\nRUN @powershell -NoProfile -ExecutionPolicy unrestricted -Command \"(iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))) >$null 2>&1\"\n\n# Install Visual C++ Build Tools, as per: https://chocolatey.org/packages/visualcpp-build-tools\nRUN choco install visualcpp-build-tools -version 14.0.25420.1 -y\n\n# Add msbuild to PATH\nRUN setx /M PATH \"%PATH%;C:\\Program Files (x86)\\MSBuild\\14.0\\bin\"\n\n# Test msbuild can be accessed without path\nRUN msbuild -version\n\nCMD [ \"cmd.exe\" ]\nUsing Choco is simpler, but you can get the same result with downloading native web installer and running it in quiet mode, for example:\nvisualcppbuildtools_full.exe /Q /L <LogFile> /Full",
    "How to evaluate a dynamic variable in a docker-compose.yml file?": "What you can do is to generate .env just before calling docker-compose:\n#!/usr/bin/env bash\n\ncat << EOF > .env\nNOW=$(date +%s)\nEOF\n\ndocker-compose up",
    "Setting docker env var from build secret": "To set a variable from a secret, you can use the $(cat /filename) syntax in shell. This affects the shell within that single step, so all of your uses of that variable need to be within the same step. You cannot extract a variable from a RUN step into an ENV step. If you need it to persist to other RUN steps, you would need to write the variable to the filesystem and have in included in the image, which is undesirable (instead just mount the secret a second time in the later RUN step).\nHere's a working example, you could also export that secret with export secret_var:\n$ cat df.secret\nFROM busybox\nRUN --mount=type=secret,id=secret \\\n    secret_var=\"$(cat /run/secrets/secret)\" \\\n && echo ${secret_var}\n\n$ cat secret.txt\nmy_secret\n\n$ docker build --progress=plain --secret id=secret,src=$(pwd)/secret.txt -f df.secret .\n#1 [internal] load build definition from df.secret\n#1 sha256:85a18e77d3e60159b744d6ee3d96908a6fed0bd4f6a46d038e2aa0201a1028de\n#1 DONE 0.0s\n\n#1 [internal] load build definition from df.secret\n#1 sha256:85a18e77d3e60159b744d6ee3d96908a6fed0bd4f6a46d038e2aa0201a1028de\n#1 transferring dockerfile: 152B done\n#1 DONE 0.0s\n\n#2 [internal] load .dockerignore\n#2 sha256:a5a676bca3eaa2c757a3ae40d8d5d5e91b980822056c5b3b6c5b3169fc65f0f1\n#2 transferring context: 49B done\n#2 DONE 0.0s\n\n#3 [internal] load metadata for docker.io/library/busybox:latest\n#3 sha256:da853382a7535e068feae4d80bdd0ad2567df3d5cd484fd68f919294d091b053\n#3 DONE 0.0s\n\n#5 [1/2] FROM docker.io/library/busybox\n#5 sha256:08a03f3ffe5fba421a6403c31e153425ced631d108868f30e04985f99d69326e\n#5 DONE 0.0s\n\n#4 [2/2] RUN --mount=type=secret,id=secret     secret=$(cat /run/secrets/secret)  && echo ${secret}\n#4 sha256:6ef91a8a7daf012253f58dba292a0bd86af1d1a33a90838b6a99aba5abd4cfaf\n#4 0.587 my_secret\n#4 DONE 0.7s\n\n#6 exporting to image\n#6 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n#6 exporting layers 0.0s done\n#6 writing image sha256:a52db3458ad88481406cd60627e2ed6f55b6720c1614f65fa8f453247a9aa4de done\n#6 DONE 0.0s\nNote the line #4 0.587 my_secret showing the secret was output.",
    "Read JSON-file into environment variable with Docker Compose": "An easy way to do this is to load the JSON file in to a local env var, then use that in your yaml file.\nIn docker-compose.yml\nenvironment:\n  METEOR_SETTINGS: ${METEOR_SETTINGS}\nLoad the settings file before invoking docker-compose:\n\u276f METEOR_SETTINGS=$(cat settings.json) docker-compose up",
    "dotnet restore fails from Docker container": "The actual error seems to be:\nUnable to load the service index for source https://api.nuget.org/v3/index.json\nWhich means that nuget is failing to access the endpoint when it is trying to download the dependencies.\nThere are a number of solutions to this listed here\nhttps://github.com/NuGet/Home/issues/2880\nand also\nNuget connection attempt failed \"Unable to load the service index for source\"",
    "Difference between using \"expose\" in dockerfile and docker-compose file?": "EXPOSE in Dockerfile is a just a metadata information. Which tells docker when someone uses docker run -P which ports need to be Exposed.\nUsing them in compose or docker run is a dynamic way of specifying these ports. So an image like nginx or apache which is always supposed to run on port 80 inside the container will use EXPOSE in Dockerfile itself.\nWhile an image which has dynamic port which may be controlled using an environment variable will then use expose in docker run or compose file\ndocker run -e UI_PORT=5556 --expose 5556 -P ....",
    "Writing data to file in Dockerfile": "When you RUN chmod 755 script.sh && ./script.sh it actually execute this script inside the docker container (ie: in the docker layer).\nWhen you ADD file.txt . you are trying to add a file from your local filesystem inside the docker container (ie: in a new docker layer).\nYou can't do that because the file.txt doesn't exist on your computer.\nIn fact, you already have this file inside docker, try docker run --rm -ti mydockerimage cat file.txt and you should see it's content displayed",
    "Dockerfile: Is there any way to read variables from .env file": "UPDATED\nAfter discussion in a chat was realised that there's no problem with the nodejs app container, and the issue comes from a wrongly configured nginx proxy.\nProof for the working nodejs app is the next docker-compose file.\nversion: \"3\"\nservices:\n    api:\n        build: .\n    curl:\n        image: curlimages/curl:7.70.0\n        depends_on:\n          - api\n        entrypoint: \"\"\n        command: curl -si --retry 5 --retry-delay 5 --retry-max-time 40 http://api:6000\n        restart: on-failure\nORIGINAL\nIf you want to change the port while a build process (it will be static later when you run a container) then use build-args\ndocker build --build-arg APP_PORT=3000\nFROM node:11-alpine\nRUN mkdir -p /usr/src/app\nWORKDIR /usr/src/app\n\nARG APP_PORT=80\nEXPOSE ${APP_PORT}\n\nCOPY . .\nRUN APP_PORT=${APP_PORT} npm install\n\nCMD APP_PORT=${APP_PORT} npm run start\nif you want to be able to change the port when you're starting a container - then build-args don't fit and you need to stay with env variables. Notice that after build EXPOSE can't be changed.\nAnyway if you have different ports in EXPOSE and your app listens to - it doesn't break anything, the app's port will be available on the port you want, despite it wasn't specified in EXPOSE.\nYou can even skip EXPOSE in your file, because it's rather more a metadata information of your image than an instruction for a system to open the port: https://docs.docker.com/engine/reference/builder/#expose\nRegardless of the EXPOSE settings, you can override them at runtime by using the -p flag.\nif your image is static after the build (you don't plan to change .env) you can do next, then npm install and npm run start has the same env. And you're still allowed to change port later, but it won't affect npm install.\nFROM node:11-alpine\nRUN mkdir -p /usr/src/app\nWORKDIR /usr/src/app\n\nCOPY . .\nRUN export $(cat .env) && npm install \n\nCMD export $(cat .env) && npm run start\nif you have to keep CMD as an array - then we need to create a bootstrap script\nFROM node:11-alpine\nRUN mkdir -p /usr/src/app\nWORKDIR /usr/src/app\n\nCOPY . .\nRUN export $(cat .env) && npm install\n\nRUN echo '#!/usr/bin/env sh' > start.sh && echo 'export $(cat .env) && npm run start ${@}' >> start.sh\nCMD [\"sh\", \"./start.sh\"]",
    "error: command 'gcc' failed with exit status 1 when installing pip packages on alpine docker image": "Missing the header file Python.h , this file is provide by python2-dev ( -dev mean package for doing development with ) .\nWith this https://pkgs.alpinelinux.org/contents you can search all packages that have Python.h\nI was able to run pip install pygpgme by adding these 3 packages :\npython2-dev\ngpgme-dev\nlibc-dev\nAnd the Dockerfile will be :\nFROM alpine:latest\n\nRUN apk update && apk upgrade\nRUN apk add --no-cache bash\\\n                       python \\\n                       pkgconfig \\\n                       git \\\n                       gcc \\\n                       openldap \\\n                       libcurl \\\n                       python2-dev \\\n                       gpgme-dev \\\n                       libc-dev \\\n    && rm -rf /var/cache/apk/*\nRUN wget https://bootstrap.pypa.io/get-pip.py && python get-pip.py\nRUN pip install setuptools==30.1.0",
    "What are the appropriate names for the parts of a docker image's name?": "According to the reference for docker tag:\nAn image name is made up of slash-separated name components, optionally prefixed by a registry hostname.\nThe tag is generally regarded as the part after the :. As such, though ignoring the case of more than 2 slash-separated components:\nmy-registry is the registry\nmy-registry/my-image is the (image) name\n0.1.0 is the tag (name)\nThere don't seem to be names for the units my-registry/my-image:0.1.0, my-image and my-image:0.1.0 (other than my-image being called a \"name component\"). You could conceivably call my-registry/my-image:0.1.0 the tagged name, and if you took the approach of calling my-image the project (which is my personal approach) then you could call my-image:0.1.0 the tagged project.\nNote that, despite the above, docker image refers to the my-registry/my-name unit as a \"repository\". I personally prefer to use the \"name\" terminology for this unit.",
    "How do I get memory usage of processes running in a Docker container?": "If each docker has mounted /proc/ as usual (see proc(5)...) you could use it (e.g. running pmap(1), etc...)",
    "Can I set docker container labels based on output of previous run command during build?": "Seems like it is just not possible. You need to use build arguments before launching the build.",
    "Docker build fails in Travis CI - \"Error checking context: 'syntax error in pattern'\"": "For those curious, this turned out to be the system's way of telling me that it was erroring on trying to do regex matching on the items in my .dockerignore file (i.e. that file had syntax errors in it -- in this case, I had backslashes instead of forward slashes on my file paths). Nice and cryptic; I had to dig through the Docker source code to figure out what was happening.\nHopefully this helps someone else encountering the same issue! :)",
    "GPG invalid signature error while running apt update inside arm32v7/ubuntu:20.04 docker": "I had this issue on Docker Desktop for Mac recently, running apt-get update in an Ubuntu 20.04 x86_64 container. It turned out the VM hosting the Docker images on macOS had run out of disk space. That somehow manifested itself as apt reporting invalid signatures on package index files. Pruning unused images to free space solved the issue for me:\ndocker image prune -a\nAs mentioned in comment by sema, if the issue is caused by insufficient disk space, another workaround is to increase the size of the virtual disk used by the virtual machine that is running docker. In Docker Desktop for Mac this can be done via Preferences > Resources > Disk image size.",
    "How to run Powershell script in DockerFile?": "To run a PS1 script file, you can do something like this:\nSHELL [\"cmd\", \"/S\", \"/C\"]    \nRUN powershell -noexit \"& \"\"C:\\Chocolatey\\lib\\chocolatey.0.10.8\\tools\\chocolateyInstall.ps1\"\"\"\nYou can also do:\nSHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';\"]\nWORKDIR C:\\\nRUN .\\install_pfx.ps1",
    "I keep getting the \"docker build\" requires exactly 1 argument(s) error": "I was facing the same issue. I missed the little period ( . ) at the end of the command. I copy pasted the command from official docker docs. The period ( . ) at the end of the command tells Docker to use the shell\u2019s current working directory as the build context. Be sure include the period (.) at the end of the command, and be sure to run the command from the current working directory that contains the Dockerfile and application code.",
    "GraphQL ERESOLVE unable to resolve dependency tree when building my docker container": "The problem here is certainly with NPM and the packages you are trying to install rather than anything to do with Docker.\nUnfortunately, I am not able to reproduce the exact error that you are facing. That could be because:\nsomething changed in the time between now and whenever this problem occurred;\nthere are some essential details that you are not showing us.\nEither way, there's a general way in which such issues are solved, which should help. But first an explanation.\nDependencies, peer dependencies and conflicts\nNPM's package (dependency) management mechanism allows packages (dependencies) to have:\n(direct) dependencies - installed automatically with the package;\npeer dependencies - have to be manually installed by the consumer of the package.\nHowever, NPM does not allow multiple versions of the same package to coexist.\nAlso, as you may know, packages use standard semantic versioning, which means that a major version change indicates a breaking change.\nDue to these two reasons, clashes occur if one package requires dependency A to be v1, while another wants the same dependency A to be v2.\nNPM v7\nNPM v7 was recently released and this is the version that current (as of November 2020) node:current images use.\nProbably the biggest changes brought about by NPM7 relate to peer dependencies - NPM should now be able to install them automatically, if possible. Read more here.\nAs described in the document, in cases where it's not possible to solve the conflicts, NPM should now throw errors rather than warnings, which is what you are seeing.\nI, on the other hand, only managed to get warnings and no errors using your setup and NPM v7.0.8, and I don't know why. The problems reported were essentially the same, however, so the resolution ought to be very similar.\nHow to solve conflicts\nThe only solution that I'm aware of is manual conflict resolution - the developer needs to adjust their dependencies to play along.\nIn your specific case the problem seems to be with the graphql package. The latest graphql package is v15, which is also a peer dependency of the latest type-graphql package (v1).\nHowever, apollo-server-express has a few dependencies, which apparently only support graphql up to and including v14.\nWhile you wait for apollo-server-express to fully support v15, you may opt for graphql v14 altogether by downgrading the only package that requires v15. So if you change your npm install to this:\nnpm install --save cors apollo-server-express express graphql@14 reflect-metadata type-graphql@0 apollo-datasource-rest soap jsonwebtoken\nit ought to work... Notice that we are explicitly installing graphql@14 and type-graphql@0 (yes, version zero).\nAlternative solution\nGoing to give you some bad advice too. In some cases a missing peer dependency may not be a problem, particularly if you never use the related functionality. In your case, it may be even less of a problem because you do have the dependency, just not the required version. It's entirely possible that a wrong version would do just fine. If you feel lucky (or if you're sure of you're doing) and you really wish to proceed with graphql v15, you could either:\nsuppress any NPM output to silence the errors;\ndowngrade to NPM v6, which works quite differently (although it will still warn you of peer dependency problems).\nProceed with caution!",
    "Running a bash script before startup in an NGINX docker container": "NGINX 1.19 has a folder /docker-entrypoint.d on the root where place startup scripts executed by thedocker-entrypoint.sh script. You can also read the execution on the log.\n/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration\n/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/\n/docker-entrypoint.sh: Launching\n[..........]\n/docker-entrypoint.sh: Configuration complete; ready for start up",
    "qemu: uncaught target signal 11 (Segmentation fault) - core dumped in docker containers": "I had the same issue using M1 chip with MacOS Monterey 12.5.\nAfter upgrading to MacOS Ventura 13.3 and selecting\nUse Rosetta for x86/amd64 emulation on Apple Silicon\nin Docker Desktop -> Settings -> Features in development, the error disappeared and everything worked fine.\nNote that this option is not available on MacOS versions lower than Ventura 13, so upgrade is required. Source: https://github.com/docker/for-mac/issues/6788\nUpdate December 2023\nYou can find the option in the General tab, as it is no longer in development for the latest versions of Docker Desktop.",
    "`docker build` show output from `RUN` [duplicate]": "Is that what are you looking for?\n$ docker build --progress=plain .\nSending build context to Docker daemon  4.096kB\nStep 1/3 : FROM alpine:3.14\n3.14: Pulling from library/alpine\n5843afab3874: Pull complete \nDigest: sha256:234cb88d3020898631af0ccbbcca9a66ae7306ecd30c9720690858c1b007d2a0\nStatus: Downloaded newer image for alpine:3.14\n ---> d4ff818577bc\nStep 2/3 : COPY . .\n ---> 106aa79185ae\nStep 3/3 : RUN echo \"here are some numbers: $(seq 10)\"\n ---> Running in 30a81b6d5035\nhere are some numbers: 1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nRemoving intermediate container 30a81b6d5035\n ---> 3c059c9b6150\nSuccessfully built 3c059c9b6150\n$ docker --version\nDocker version 19.03.8, build afacb8b",
    "docker run [9] System error: exec format error": "Had the same issue, fixed it by adding #!/bin/sh at the top of the file instead of having other comments.",
    "How to use docker container as apache server?": "You will have to use port forwarding to be able to access your docker container from the outside world.\nFrom the Docker docs:\nBy default Docker containers can make connections to the outside world, but the outside world cannot connect to containers.\n\nBut if you want containers to accept incoming connections, you will need to provide special options when invoking docker run.\nSo, what does this mean? You will have to specify a port on your host machine (typically port 80) and forward all connections on that port to the docker container. Since you are running Apache in your docker container you probably want to forward the connection to port 80 on the docker container as well.\nThis is best done via the -p option for the docker run command.\nsudo docker run -p 80:80 -t -i <yourname>/supervisord\nThe part of the command that says -p 80:80 means that you forward port 80 from the host to port 80 on the container.\nWhen this is set up correctly you can use a browser to surf onto http://88.x.x.x and the connection will be forwarded to the container as intended.\nThe Docker docs describes the -p option thoroughly. There are a few ways of specifying the flag:\n# Maps the provided host_port to the container_port but only \n# binds to the specific external interface\n-p IP:host_port:container_port\n\n# Maps the provided host_port to the container_port for all \n# external interfaces (all IP:s)\n-p host_port:container_port\nEdit: When this question was originally posted there was no official docker container for the Apache web server. Now, an existing version exists.\nThe simplest way to get Apache up and running is to use the official Docker container. You can start it by using the following command:\n$ docker run -p 80:80 -dit --name my-app -v \"$PWD\":/usr/local/apache2/htdocs/ httpd:2.4\nThis way you simply mount a folder on your file system so that it is available in the docker container and your host port is forwarded to the container port as described above.",
    "Docker: RUN touch doesn't create file": "You are doing this during your build:\nRUN touch /var/log/node.log && /\n    node --help 2>&1 > /var/log/node.log\nThe file /var/log/node.log is created and fixed immutably into the resulting image.\nThen you run the container with this volume mount:\nvolumes:\n  - ./mongo/log/:/var/log/\nWhatever is in ./mongo/log/ is mounted as /var/log in the container, which hides whatever was there before (from the image). This is the thing that's making it look like your touch didn't work (even though it probably worked fine).\nYou're thinking about this backward - your volume mount doesn't expose the container's version of /var/log externally - it replaces whatever was there.\nNothing you do in Dockerfile (build) will ever show up in an external mount.",
    "docker deploy won't publish port in swarm": "As far as I understood for the moment you just can publish ports updating the service later the creation, like this:\ndocker service update my-service --publish-add 80:80",
    "Docker container save logs on the host directory": "All you need is a docker volume in order to persist the log files. So in the same directory as your docker-compose.yml create a logs directory, then define a volume mount. When defining a mount remember the syntax is <host_machine_directy>:<container_directory>.\nGive the following volume a try and let me know what you get back.\nversion: '3'\nservices:\n  myapp:\n    build: .\n    image: myapp\n    ports:\n      - \"9001:9001\"\n    volumes:\n      - ./logs:/home/logs\nAlso worth noting that persistence goes both ways with this approach. Any changes made to the files from within the container are reflected back onto the host. Any changes from the host, are also reflected inside the container.",
    "What is a clean way to add a user in Docker with sudo priviledges?": "Generally you should think of a Docker container as a wrapper around a single process. If you ask this question about other processes, it doesn't really make sense. (How do I add a user to my PostgreSQL server with sudo privileges? How do I add a user to my Web browser?)\nIn Docker you almost never need sudo, for three reasons: it's trivial to switch users in most contexts; you don't typically get interactive shells in containers (how do I get a directory listing from inside the cron daemon?); and if you can run any docker command at all you can very easily root the whole host. sudo is also hard to script, and it's very hard to usefully maintain a user password in Docker (writing a root-equivalent password in a plain-text file that can be easily retrieved isn't a security best practice).\nIn the context of your question, if you've already switched to some non-root user, and you need to run some administrative command, use USER to switch back to root.\nUSER janedoe\n...\nUSER root\nRUN apt-get update && apt-get install -y some-package\nUSER janedoe\nSince your containers have some isolation from the host system, you don't generally need containers to have the same user names or user IDs as the host system. The exception is when sharing files with the host using bind mounts, but there it's better to specify this detail when you start the container.\nThe typical practice I'm used to works like this:\nIn your Dockerfile, create some non-root user. It can have any name. It does not need a password, login shell, home directory, or any other details. Treating it as a \"system\" user is fine.\n FROM ubuntu:18.04\n RUN adduser --system --group --no-create-home appuser\nStill in your Dockerfile, do almost everything as root. This includes installing your application.\n RUN apt-get update && apt-get install ...\n WORKDIR /app\n COPY requirements.txt .\n RUN pip install -r requirements.txt\n COPY . .\nWhen you describe the default way to run the container, only then switch to the non-root user.\n EXPOSE 8000\n USER appuser\n CMD [\"./main.py\"]\nIdeally that's the end of the story: your code is built into your image and it stores all of its data somewhere external like a database, so it doesn't care about the host user space at all (there by default shouldn't be docker run -v or Docker Compose volumes: options).\nIf file permissions really matter, you can specify the numeric host user ID to use when you launch the container. The user doesn't specifically need to exist in the container's /etc/passwd file.\n docker run \\\n   --name myapp \\\n   -d \\\n   -p 8000:8000 \\\n   -v $PWD:/data \\\n   -u $(id -u) \\\n   myimage",
    "Docker for MAC | Cannot run program \"docker-credential-desktop\"": "If you are trying to use with Spring Boot - fabric8 plugin and on mac machine do the following :\nIn ~/.docker/config.json change credsStore to credStore",
    "Can docker compose build image from different Dockerfiles at the same folder": "The following worked for me\nabc:\n  build:\n    context: .\n    dockerfile: Dockerfile.one\ndef:\n  build:\n    context: .\n    dockerfile: Dockerfile.two\nOf course you can tweak the context as needed if you have different contexts. I also use this to have separate Dockerfile.production versions to set things up differently for production versions of the app.",
    "installing ssh in the docker containers": "Well, as part of the image file you'll simply have to install openssh-server:\nsudo apt-get install openssh-server\nThe problem then is that traditionally, a running docker container will only run a single command. You can get around this problem by using something like supervisord. There's an example in the docker docs: https://docs.docker.com/engine/admin/using_supervisord/\nYour dockerfile might look like this:\nFROM ubuntu:16.04\nMAINTAINER examples@docker.com\n\nRUN apt-get update && apt-get install -y openssh-server apache2 supervisor\nRUN mkdir -p /var/lock/apache2 /var/run/apache2 /var/run/sshd /var/log/supervisor\n\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n\nEXPOSE 22 80\nCMD [\"/usr/bin/supervisord\"]\nYour supervisord.conf might look something like this:\n[supervisord]\nnodaemon=true\n\n[program:sshd]\ncommand=/usr/sbin/sshd -D\n\n[program:apache2]\ncommand=/bin/bash -c \"source /etc/apache2/envvars && exec /usr/sbin/apache2 -DFOREGROUND\"",
    "Update a docker image in registry": "Short: Upgrade to the latest docker version (preferred) or use the -t tag option.\nYour commands are the same as if you would issue the following two commands:\ndocker tag proj1:latest localhost.com:5000/proj/proj1:latest \ndocker push localhost.com:5000/proj/proj1:latest\nOlder versions of Docker are complaining, since you try to overwrite an existing image with existing tag :latest.\nThe quick&dirty solution is to try with\ndocker tag -f proj1 localhost.com:5000/proj/proj1 \ndocker push -f localhost.com:5000/proj/proj1\nThis will allow to overwrite the existing image localhost.com:5000/proj/proj1:latest on older versions of Docker.\nHowever, I recommend to upgrade docker to version >=1.12.0. There, the -t option is not available and not necessary anymore, since the image will always be replaced. This is the reason, why -f option is not described on the official documentation, but it is mentioned on the Docker Deprecated Engine Features page instead.",
    "Docker Alpine: unable to select packages: python (no such package) while building image for ARM": "RUN apk add --no-cache --virtual .gyp python3 make g++",
    "How can I fix the Error of Docker-compose up exited with code 1": "docker may fail due to many things in the build process. To find the solution here is my advice\nType docker ps -la (to list all containers that exited with error code or failed to start\nIn the result, you should look out for the id of the container\nThen check the logs using: docker logs <container_id>",
    "Docker build ARG always empty string": "This is obviously not the problem in your example, but I got this error when declaring an ARG before the FROM. Moving the ARG I needed below FROM resolved the problem.",
    "Define environment variable in Dockerfile or docker-compose?": "See this:\nYou can set environment variables in a service\u2019s containers with the 'environment' key, just like with docker run -e VARIABLE=VALUE ...\nAlso, you can use ENV in dockerfile to define a environment variable.\nThe difference is:\nEnvironment variable define in Dockerfile will not only used in docker build, it will also persist into container. This means if you did not set -e when docker run, it will still have environment variable same as defined in Dockerfile.\nWhile environment variable define in docker-compose.yaml just used for docker run.\nMaybe next example could make you understand more clear:\nDockerfile:\nFROM alpine\nENV http_proxy http://123\ndocker-compose.yaml:\napp:\n  environment:\n    - http_proxy=http://123\nIf you define environment variable in Dockerfile, all containers used this image will also has the http_proxy as http://123. But the real situation maybe when you build the image, you need this proxy. But, the container maybe run by other people maybe not need this proxy or just have another http_proxy, so they had to remove the http_proxy in entrypoint or just change to another value in docker-compose.yaml.\nIf you define environment variable in docker-compose.yaml, then user could just choose his own http_proxy when do docker-compose up, http_proxy will not be set if user did not configure it docker-compose.yaml.",
    "How do I override the entrypoint and pass Bash commands as a string?": "When Docker launches a container, it combines the \"entrypoint\" and \"command\" parts together into a single command. The docker run --entrypoint option only takes a single \"word\" for the entrypoint command.\nSo, say you need to run some command --with an-arg. You need to\nBreak this into words\nPass the first word as docker run --entrypoint, before the image name\nPass the remaining words as the command part, after the image name.\n# some command --with an-arg\ndocker run \\\n  --entrypoint some\n  image-name \\\n  command --with an-arg\n\n# ls -al /\ndocker run --rm \\\n  --entrypoint /bin/ls\n  image-name \\\n  -al /\n\n# bash -c \"echo aaaa\"\ndocker run --rm \\\n  --entrypoint /bin/bash \\\n  image-name \\\n  -c 'echo aaaa'\nThis construct is kind of awkward. If you control the image, I tend to recommend making the image's CMD be a complete command, and either omitting ENTRYPOINT or making it be a wrapper that takes a complete command as arguments (for example, a shell script that ends in exec \"$@\").\n# Hard to replace with an alternate command:\n# ENTRYPOINT python3 ./manage.py runserver\n\n# Better:\nCMD python3 ./manage.py runserver\n# Takes a complete command as arguments\n# (MUST use JSON-array form)\nENTRYPOINT [\"bundle\", \"exec\"]\n\n# Can be overridden at runtime\n# (But whatever you supply will run in a Ruby Bundler context)\nCMD [\"rails\", \"server\", \"-b\", \"0.0.0.0\"]",
    "Does Docker EXPOSE make a new layer?": "Yes, every instruction in a Dockerfile generates a new layer for the resulting image.\nHowever, layers created via EXPOSE are empty layers. That is, their size is 0 bytes.\nWhile they don't impact you storage-wise, they do count for leveraging layer cache while building or pulling/pushing images from a registry.\nA good way to understand an image's layers is to use the docker history command. For instance, given the following Dockerfile:\nFROM scratch\n\nEXPOSE 4000\nEXPOSE 3000\ndo\ndocker build -t test/image .\nIf you then docker history test/image you'll see:\nIMAGE               CREATED             CREATED BY                           SIZE                COMMENT\nab9f435de7bc        4 seconds ago       /bin/sh -c #(nop)  EXPOSE 4000/tcp   0 B                 \n15e09691c313        5 seconds ago       /bin/sh -c #(nop)  EXPOSE 3000/tcp   0 B     \nIf you switch the order of the EXPOSE statements and build again, you'll see the layer cache being ignored.",
    "ldconfig seems no functional under alpine 3.3": "Alpine's version of ldconfig requires you to specify the target folder or library as an argument. Note that alpine has no /etc/ld.so.conf file, nor does it recognize one if you create it.\nExample with no target path:\n$ docker run -ti alpine sh -c \"ldconfig; echo \\$?\"\n1\nExample with target path:\n$ docker run -ti alpine sh -c \"ldconfig /; echo \\$?\"\n0\nHowever, even with that there are frequently linking errors. Others suggest:\nManual symbolic links\nInstalling glibc into your container.",
    "Pass ARG to ENTRYPOINT": "You could combine ARG and ENV in your Dockerfile, as I mention in \"ARG or ENV, which one to use in this case?\"\nARG FOO\nENV FOO=${FOO}\nThat way, you docker.r2g can access the ${FOO} environment variable.\nI guess the argument could also be passed with docker run instead of during the docker build phase?\nThat is also possible, if it makes more sense to give FOO a value at runtime:\ndocker run -e FOO=$(...) ...",
    "Is it possible to set a MAC address for `docker build`?": "Let's consider the below Dockerfile\nFROM alpine\nRUN ifconfig | grep -i hwaddr\nIf you build it using\ndocker build .\nYou get\nSending build context to Docker daemon  2.048kB\nStep 1/2 : FROM alpine\n ---> 7328f6f8b418\nStep 2/2 : RUN ifconfig | grep -i hwaddr\n ---> Running in c092838dbe31\neth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:02\nRemoving intermediate container c092838dbe31\n ---> 7038787f51b8\nNow we can't control Mac address of docker build, but we can control the network of build and we can control mac address of a container. So let us launch a container with our mac address\n$ docker run --name mac1234deb06b61 --mac-address=\"12:34:de:b0:6b:61\" -d alpine tail -f /dev/null\nc3579e4685933b757f51c5f9e36d620dbe3a62abd0e0d6a421b5f1c04045061c\n\n$ docker build --network container:mac1234deb06b61 --no-cache .\nSending build context to Docker daemon  2.048kB\nStep 1/2 : FROM alpine\n ---> 7328f6f8b418\nStep 2/2 : RUN ifconfig | grep -i hwaddr\n ---> Running in 4390f13cbe8f\neth0      Link encap:Ethernet  HWaddr 12:34:DE:B0:6B:61\nRemoving intermediate container 4390f13cbe8f\n ---> b0b5f7321921\nSuccessfully built b0b5f7321921\nAs you can see, now the docker build takes a updated mac address",
    "Docker: How can I have sqlite db changes persist to the db file?": "You are not mounting volumes in a Dockerfile. VOLUME tells docker that content on those directories can be mounted via docker run --volumes-from\nYou're right. Docker doesn't allow relative paths on volumes on command line.\nRun your docker using absolute path:\ndocker run -v /host/db/local-db:/go/src/beginnerapp/local-db\nYour db will be persisted in the host file /host/db/local-db\nIf you want to use relative paths, you can make it work with docker-compose with \"volumes\" tag:\nvolumes:\n  - ./local-db:/go/src/beginnerapp/local-db\nYou can try this configuration:\nPut the Dockerfile in a directory, (e.g. /opt/docker/myproject)\ncreate a docker-compose.yml file in the same path like this:\nversion: \"2.0\"\nservices:\n  myproject:\n    build: .\n    volumes:\n      - \"./local-db:/go/src/beginnerapp/local-db\"\nExecute docker-compose up -d myproject in the same path.\nYour db should be stored in /opt/docker/myproject/local-db\nJust a comment. The content of local-db (if any) will be replaced by the content of ./local-db path (empty). If the container have any information (initialized database) will be a good idea to copy it with docker cp or include any init logic on an entrypoint or command shell script.",
    "Docker compose .env file array variable": "As Confidence mentioned above, write a comma separated string:\nTAGS=12345,67890\nThen in your application (Python for instance):\nos.getenv('TAGS').split(',')",
    "How to translate docker-compose.yml to Dockerfile": "TL;DR\nYou can pass some informations to your Dockefile (the command to run) but that wouldn't be equivalent and you can't do that with all the docker-compose.yml file content.\nYou can replace your docker-compose.yml file with commands lines though (as docker-compose is precisely to replace it).\nIn your case you can add the command to run to your Dockerfile as a default command (which isn't roughly the same as passing it to containers you start at runtime) :\nCMD [\"python\", \"jk/manage.py\", \"runserver\", \"0.0.0.0:8081\"]\nor pass this command directly in command line like the volume and port which should give something like :\ndocker run -d -v .:/code -p 8081:8080 yourimage python jk/manage.py runserver 0.0.0.0:8081\nBUT\nKeep in mind that Dockerfiles and docker-compose serve two whole different purposes.\nDockerfile are meant for image building, to define the steps to build your images.\ndocker-compose is a tool to start and orchestrate containers to build your applications (you can add some informations like the build context path or the name for the images you'd need, but not the Dockerfile content itself).\nSo asking to \"convert a docker-compose.yml file into a Dockerfile\" isn't really relevant.\nThat's more about converting a docker-compose.yml file into one (or several) command line(s) to start containers by hand.\nThe purpose of docker-compose is precisely to get rid of these command lines to make things simpler (it automates it).\nalso :\nFrom the manage.py documentation:\nDO NOT USE THIS SERVER IN A PRODUCTION SETTING. It has not gone through security audits or performance tests. (And that\u2019s how it\u2019s gonna stay.\nDjango's runserver included in the manage.py tool isn't meant for production.\nYou might want to consider using a WSGI server behind a proxy.",
    "Bind container port to host inside Dockerfile": "In dockerfile you can only use expose. The EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. EXPOSE does not make the ports of the container accessible to the host.\nTo allocate Hostport to container you need to do publish (-p). Or the -P flag to publish all of the exposed ports.\nTo automate the process, You can use docker-compose. In docker compose file you can orchestrate multiple docker run commands with different arguments.",
    "docker COPY with file globbing": "For any non-standard build operation, I prefer wrapping the docker build command in a script (named 'build').\nHere I would\ncreate a subfolder tmp (just beside the Dockerfile, in order to keep it in the docker build context)\nmake the shell cp with globing: cp ./src/**/project.json tmp\ncall docker build, with a Dockerfile including COPY tmp/ /app/\ndeleting tmp.\nThat way, I pre-configure what I need from host, before building the image from the host context.",
    "Custom Docker image doesn't inherit CMD": "This behavior is trying to be more intuitive, but I agree that it's a bit confusing. You see the original issue here. That problem was that most people defining the ENTRYPOINT in a child image no longer wanted the CMD from the parent image.\nWith the current behavior, if you define an ENTRYPOINT in a child image, the CMD from the parent image will be null'd out, so you'll need to redefine it if you need to have it set.",
    "An assembly specified in the application dependencies manifest was not found:": "You need to specify -r linux-x64 parameter in dotnet publish command like that:\ndotnet publish -o obj/Docker/publish -c Release -r linux-x64\nThis will make a standalone deployment.",
    "How to: Docker reuse layers with different base images": "Problem with --cache-from:\nThe suggestion to use --cache-from will not work:\n$ cat df.cache-from\nFROM busybox\nARG UNIQUE_ARG=world\nRUN echo Hello ${UNIQUE_ARG}\nCOPY . /files\n\n$ docker build -t test-from-cache:1 -f df.cache-from --build-arg UNIQUE_ARG=docker .\nSending build context to Docker daemon   26.1MB\nStep 1/4 : FROM busybox\n ---> 54511612f1c4\nStep 2/4 : ARG UNIQUE_ARG=world\n ---> Running in f38f6e76bbca\nRemoving intermediate container f38f6e76bbca\n ---> fada1443b67b\nStep 3/4 : RUN echo Hello ${UNIQUE_ARG}\n ---> Running in ee960473d88c\nHello docker\nRemoving intermediate container ee960473d88c\n ---> c29d98e09dd8\nStep 4/4 : COPY . /files\n ---> edfa35e97e86\nSuccessfully built edfa35e97e86\nSuccessfully tagged test-from-cache:1\n\n$ docker build -t test-from-cache:2 -f df.cache-from --build-arg UNIQUE_ARG=world --cache-from test-from-cache:1 .                                                                                \nSending build context to Docker daemon   26.1MB\nStep 1/4 : FROM busybox\n ---> 54511612f1c4\nStep 2/4 : ARG UNIQUE_ARG=world\n ---> Using cache\n ---> fada1443b67b\nStep 3/4 : RUN echo Hello ${UNIQUE_ARG}\n ---> Running in 22698cd872d3\nHello world\nRemoving intermediate container 22698cd872d3\n ---> dc5f801fc272\nStep 4/4 : COPY . /files\n ---> addabd73e43e\nSuccessfully built addabd73e43e\nSuccessfully tagged test-from-cache:2\n\n$ docker inspect test-from-cache:1 -f '{{json .RootFS.Layers}}' | jq .\n[\n  \"sha256:6a749002dd6a65988a6696ca4d0c4cbe87145df74e3bf6feae4025ab28f420f2\",\n  \"sha256:01bf0fcfc3f73c8a3cfbe9b7efd6c2bf8c6d21b6115d4a71344fa497c3808978\"\n]\n\n$ docker inspect test-from-cache:2 -f '{\n{json .RootFS.Layers}}' | jq .                                                                                         \n[\n  \"sha256:6a749002dd6a65988a6696ca4d0c4cbe87145df74e3bf6feae4025ab28f420f2\",\n  \"sha256:c70c7fd4529ed9ee1b4a691897c2a2ae34b192963072d3f403ba632c33cba702\"\n]\nThe build shows exactly where it stops using the cache, when the command changes. And the inspect shows the change of the second layer id even though the same COPY command was run in each. And anytime the preceding layer differs, the cache cannot be used from the other image build.\nThe --cache-from option is there to allow you to trust the build steps from an image pulled from a registry. By default, docker only trusts layers that were locally built. But the same rules apply even when you provide this option.\nOption 1:\nIf you want to reuse the build cache, you must have the preceding layers identical in both images. You could try using a multi-stage build if the base image for each is small enough. However, doing this would lose all of the settings outside of the filesystem (environment variables, entrypoint specification, etc), so you'd need to recreate that as well:\nARG base_image\nFROM ${base_image} as base\n# the above from line makes the base image available for later copying\nFROM scratch\nCOPY large-content /content\nCOPY --from=base / /\n# recreate any environment variables, labels, entrypoint, cmd, or other settings here\nAnd then build that with:\ndocker build --build-arg base_image=base1 -t image1 .\ndocker build --build-arg base_image=base2 -t image2 .\ndocker build --build-arg base_image=base3 -t image3 .\nThis could also be multiple Dockerfiles if you need to change other settings. This will result in the entire contents of each base image being copied, so make sure your base image is significantly smaller to make this worth the effort.\nOption 2:\nReorder your build to keep common components at the top. I understand this won't work for you, but it may help others coming across this question later. It's the preferred and simplest solution that most people use.\nOption 3:\nRemove the large content from your image and add it to your containers externally as a volume. You lose the immutability + copy-on-write features of layers of the docker filesystem. And you'll manually need to ship the volume content to each of your docker hosts (or use a network shared filesystem). I've seen solutions where a \"sync container\" is run on each of the docker hosts which performs a git pull or rsync or any other equivalent command to keep the volume updated. If you can, consider mounting the volume with :ro at the end to make it read only inside the container where you use it to give you immutability.",
    "What is #syntax=docker/dockerfile:experimental?": "It's a way to enable new syntax in Dockerfiles when building with BuildKit. It's mentioned in the documentation:\nOverriding default frontends\nThe new syntax features in Dockerfile are available if you override the default frontend. To override the default frontend, set the first line of the Dockerfile as a comment with a specific frontend image:\n# syntax=<frontend image>, e.g. # syntax=docker/dockerfile:1.2\nThe examples on this page use features that are available in docker/dockerfile version 1.2.0 and up. We recommend using docker/dockerfile:1, which always points to the latest release of the version 1 syntax. BuildKit automatically checks for updates of the syntax before building, making sure you are using the most current version. Learn more about the syntax directive in the Dockerfile reference.\nI have used it to enable SSH Auth Sock forwarding.",
    "Read txt file from resources folder on maven Quarkus project From Docker Container": "You need to make sure that the resource is included in the native image (it isn't by default).\nAdd a src/main/resources/resources-config.json that includes something like:\n{\n  \"resources\": [\n    {\n      \"pattern\": \"151279\\\\.txt$\"\n    }\n  ]\n}\nYou will also need to set the following property:\nquarkus.native.additional-build-args =-H:ResourceConfigurationFiles=resources-config.json\nSee this for more details.",
    "convert Dockerfile to Bash script": "In short - no.\nBy parsing the Dockerfile with a tool such as dockerfile-parse you could run the individual RUN commands, but this would not replicate the Dockerfile's output.\nYou would have to be running the same version of the same OS.\nThe ADD and COPY commands affect the filesystem, which is in its own namespace. Running these outside of the container could potentially break your host system. Your host will also have files in places that the container image would not.\nVOLUME mounts will also affect the filesytem.\nThe FROM image (which may in turn be descended from other images) may have other applications installed.\nWriting Dockerfiles can be a slow process if there is a large installation or download step. To mitigate that, try adding new packages as a new RUN command (to take advantage of the cache) and add features incrementally, only optimising/compressing the layers when the functionality is complete.\nYou may also want to use something like ServerSpec to get a TDD approach to your container images and prevent regressions during development.\nBest practice docs here, gotchas and the original article.",
    "How can I run Selenium tests in a docker container with a visible browser?": "Please consider using Zalenium (https://opensource.zalando.com/zalenium/). The headline of Zalenium is - A flexible and scalable container based Selenium Grid with video recording, live preview, basic auth & dashboard.\nAs mentioned above, you can check the live preview of your test cases running on the browser.\nP.S.:- Zalenium is a wrapper built on top of Selenium Grid",
    "`--chown` option of COPY and ADD doesn't allow variables. There exists a workaround?": "You can create a user before running the --chown;\nmkdir -p test && cd test\nmkdir -p path/to/host/dir/\ntouch path/to/host/dir/myfile\nCreate your Dockerfile:\nFROM busybox\n\nARG USER_ID=1000\nARG GROUP_ID=1000\n\nRUN addgroup -g ${GROUP_ID} mygroup \\\n && adduser -D myuser -u ${USER_ID} -g myuser -G mygroup -s /bin/sh -h /\n\nCOPY --chown=myuser:mygroup /path/to/host/dir/ /path/to/container/dir\nBuild the image\ndocker build -t example .\nOr build it with a custom UID/GID:\ndocker build -t example --build-arg USER_ID=1234 --build-arg GROUP_ID=2345 .\nAnd verify that the file was chown'ed\ndocker run --rm example ls -la /path/to/container/dir\n\ntotal 8\ndrwxr-xr-x    2 myuser   mygroup       4096 Dec 22 16:08 .\ndrwxr-xr-x    3 root     root          4096 Dec 22 16:08 ..\n-rw-r--r--    1 myuser   mygroup          0 Dec 22 15:51 myfile\nVerify that it has the correct uid/gid:\ndocker run --rm example ls -lan /path/to/container/dir\n\ntotal 8\ndrwxr-xr-x    2 1234     2345          4096 Dec 22 16:08 .\ndrwxr-xr-x    3 0        0             4096 Dec 22 16:08 ..\n-rw-r--r--    1 1234     2345             0 Dec 22 15:51 myfile\nNote: there is an open feature-request for adding this functionality: issue #35018 \"Allow COPY command's --chown to be dynamically populated via ENV or ARG\"",
    "Restart terminal & run command using Dockerfile": "Every new RUN command creates a new layer in docker image, so you can treat it as a new terminal spawns for each \"RUN\".\nWhat seems to be the problem: when running a command in this way: piping it through bash curl https://raw.githubusercontent.com/creationix/nvm/v0.30.2/install.sh | bash it won't throw any errors if something fails.\nHad a similar issue a few weeks ago. Does your base image have all the dependencies? Depending of the base image, before running the script from github add a RUN task which will download curl wget ca-certificates. You can lookup how other people install nvm on their images: https://github.com/mikeyfarrow/docker-nvm/blob/master/Dockerfile",
    "Docker Load key \"/root/.ssh/id_rsa\": invalid format": "Another possible gotcha is if you're using a Makefile to run the docker build command. In that case the command in the Makefile would look something like:\ndocker-build:\n    docker build --build-arg SSH_PRIVATE_KEY=\"$(shell cat ~/.ssh/id_rsa)\"\nMake unfortunately replaces newlines with spaces (make shell)\nThis means that the ssh key which is written into the container has a different format, yielding the error above.\nI was unable to find a way to retain the newlines in the Makefile command, so I resorted to a workaround of copying the .ssh directory into the docker build context, copying the files through the Dockerfile, then removing them afterwards.",
    "You must use Bundler 2 or greater with this lockfile. When running docker-compose up locally": "Adding this line before RUN bundle install to the Dockerfile did the trick for me.\nRUN gem install bundler -v 2.0.1\nLeaving this here for future reference!",
    "Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'": "I suspect that you haven't copied over your requirements.txt file to your Docker image.\nTypically you add the following lines to your Dockerfile to copy your requirements.txt file and install it using pip:\nCOPY requirements.txt /tmp/requirements.txt\nRUN python3 -m pip install -r /tmp/requirements.txt\nIf you don't explicitly copy over anything to your Docker image your image has no data save for what is on the base image.",
    "docker-compose up error, Invalid address": "I just ran into this issue and my issue seems to be issues with how docker caches networks and containers. I had to docker network rm the network that was previously created. I also had to docker ps -a and docker rm the previously created containers as they're cached to use the network that you'll be removing. After I removed all those leftover artifacts, it started up correctly.",
    "Docker: How to add backports to sources.list via Dockerfile?": "You can do it by adding below\nRUN printf \"deb http://httpredir.debian.org/debian jessie-backports main non-free\\ndeb-src http://httpredir.debian.org/debian jessie-backports main non-free\" > /etc/apt/sources.list.d/backports.list",
    "What is tianon/true used for in Dockerfile?": "Now I understand that every container needs an image.\nIn this case appdata is a container that only pointing some directories and It will be used in another docker container.\nappdata:\n  image: tianon/true # Here is the image, if we remove it, it won't work.\n  volumes:\n    - /var/www/html\n    - ~/.composer:/var/www/.composer\n    - ./html/app/code:/home/gujarat/php/html/app/code\n    - ./html/app/design:/home/gujarat/php/html/app/design\n    - ./html/app/etc:/var/www/html/app/etc\n    - ./html/downloads:/var/www/html/downloads\nSo in my docker-compose.yml above it needs a docker image which is really small.And that is tianon/true. It would be waste of resource if we choose another large docker image.\nAnd I found in the short description in this link :\n125 bytes total - nothing but \"true\" (perfect for volume-only containers) Yes, those are \"regular bytes\" - static assembly for the win.\nso that's the tianon/true is used for. :D",
    "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)": "$ sudo apt install locales\n$ sudo locale-gen en_US.UTF-8\n$ sudo dpkg-reconfigure locales\nIn the last step you, would see a text based UI, select en_US.UTF-8 by moving using up and down arrow and selecting via spacebar or typing its id, which is 159.",
    "Assigning additional capabilities using a Docker file": "You can do that with docker-compose.\nThis works for version 2 and 3. For example:\nversion: '2'\nservices:\n  myapp:\n    cap_add:\n    - SYS_ADMIN\n    - DAC_READ_SEARCH",
    "How to use the mounted ssh in docker for subsequent commands in Dockerfile": "Just for posterity, there are 3 prerequisites of this working, so make sure that build is using buildx, inside the Dockerfile you use the RUN command with --mount=type=ssh and you are passing --ssh default parameter to the build command:\nexport DOCKER_BUILDKIT=1\nFROM ...\nRUN --mount=type=ssh composer install --no-dev --no-interaction\ndocker build --ssh default .",
    "Create a dockerfile that runs a python http server to display an html file": "Also, how would I build and run this file once it is done?\nYou were close. Several pointers:\nIf you use python3 then you have to either use http.server or install SimpleHTTPServer separately\nIf you use python 2.7 then you can't use 'latest' tag in manner you are using it\nContainer port and your desired target local port are not the same\nHere are Dockerfile variations for python 3:\nFROM python:latest\nCOPY index.html /\nEXPOSE 7000\nCMD python -m http.server 7000\nand python 2.7:\nFROM python:2.7\nCOPY index.html /\nEXPOSE 7000\nCMD python -m SimpleHTTPServer 7000\nalongside with build\ndocker build -t my-docker-image .\nand run commnand:\ndocker run --rm -it --name my-docker-instance -p 80:7000 my-docker-image\nAfter run you can go to http://localhost to get container's port 7000 there, providing your host doen't run something on port 80 (remap if so).\nNotes:\nUsing latest image is ok for development, but problematic in production\nwork dir is set to root, maybe you would like to position files appropriately\nrunning code off simple server is ok for defvelopment\nEdit: I see that b0gusb beat me to it :)",
    "docker-compose volume is empty even from initialize": "The problem is that you're expecting files from the Container to be mounted on your host.\nThis is not the way it works: it's the other way around:\nDocker mounts your host folder in the container folder you specify. If you go inside the container, you will see that where there were supposed to be the init files, there will be nothing (or whatever was in your host folder(s)), and you can write a file in the folder and it will show up on your host.\nYour best bet to get the init files and modify them for your container is to:\nCreate a container without mounting the folders (original container data will be there)\nRun the container (the container will have the files in the right place from the installation of nginx etc...) docker run <image>\nCopy the files out of the container with docker cp <container>:<container_folder>/* <host_folder>\nNow you have the 'original' files from the container init on your host.\nModify the files as needed for the container.\nRun the container mounting your host folders with the new files in them.\nEDIT: another way to export those files is to mount a folder somewhere else in the container (like /tmp), get into the container (docker exec), and move / copy the files from the container to that mounted folder. The files will appear on the host. Then quit the container, edit the files and re-mount the folder at the right mounting point.\nNotes: You might want to go inside the container with shell (docker run -it <image> /bin/sh) and zip up all the folders to make sure you got everything if there are nested folders, then docker cp ... the zip file\nAlso, be careful about filesystem case sensitivity: on linux files are case sensitive. On Mac OS X, they're not. So if you have Init.conf and init.conf in the same folder, they will collide when you copy them to a Mac OS X host.",
    "How to view Docker image layers on Docker Hub?": "Docker Hub is quite limited at the moment and does not offer the feature you asked for.\nWhen an image is configured to build from source at Docker Hub (an Automated Build) you can see what went into it, but when it is uploaded pre-built you have no information.",
    "Install pandas in a Dockerfile": "I realize this question has been answered, but I have recently had a similar issue with numpy and pandas dependancies with a dockerized project. That being said, I hope that this will be of benefit to someone in the future.\nMy solution:\nAs pointed out by Aviv Sela, Alpine does not contain build tools by default and will need to be added though the Dockerfile. Thus see below my Dockerfile with the build packages required for numpy and pandas for be successfully installed on Alpine for the container.\nFROM python:3.6-alpine3.7\n\nRUN apk add --no-cache --update \\\n    python3 python3-dev gcc \\\n    gfortran musl-dev g++ \\\n    libffi-dev openssl-dev \\\n    libxml2 libxml2-dev \\\n    libxslt libxslt-dev \\\n    libjpeg-turbo-dev zlib-dev\n\nRUN pip install --upgrade pip\n\nADD requirements.txt .\nRUN pip install -r requirements.txt\nThe requirements.txt\nnumpy==1.17.1\npandas==0.25.1\nEDIT:\nAdd the following (code snippet below) to the Dockerfile, before the upgrade pip RUN command. It is critical to the successful installation of pandas as pointed out by Bishwas Mishra in a comment.\nRUN pip install --upgrade cython",
    "How to modify a Docker image?": "As an existing docker image cannot be changed, what I did was that I created a dockerfile for a new Docker image based on my original Docker image for its contents, and modified it to include a test folder from local into the new image.\nThis link was helpful:\nBuild your own image - Docker Documentation\nFROM abc/def:latest\nThe above line in the Docker file tells Docker which image your image is based on. So, the contents from parent image are copied to new image.\nFinally, for including the test folder from local drive, I added the following command in my Docker file\nCOPY test /home/humpty-dumpty/test\n...and the test folder was added into that new image.\nHere is the dockerfile used to create the new image from the existing one.\nFROM abc/def:latest\n\n# Extras\nRUN sudo apt-get install -y vim\n\n# copies local folder into the image \nCOPY test /home/humpty-dumpty/test\nUpdate: For editing a file in the running docker image, we can open that file using vim editor installed through the docker file shown above:\nvim <filename>\nNow, the vim commands can be used to edit and save the file.",
    "Can a Helm Install create a container from a dockerfile?": "No. A helm chart is a templated set of kubernetes manifests. There will usually by a manifest for a Pod, Deployment, or Daemonset. Any of those will have a reference to a docker image (either hard coded or a parameter). That image will usually be in a container registry like dockerhub. You'll need to build your image using the docker file, push it to a registry, reference this image in a helm chart, then install or update helm.",
    "Setting context in docker-compose file for a parent folder": "You've set:\n dockerfile: .\nJust try to use a relative path to you Dockerfile from the set context:\ncontext: ../../\ndockerfile: ./folder1/folder2/Dockerfile",
    "Docker How to run /usr/sbin/init and then other scripts on start up [closed]": "Declaring\nENTRYPOINT [\"/usr/sbin/init\"]\nCMD [\"systemctl\"]\nWill result in:\n/usr/sbin/init systemctl\nIn other words, the ENTRYPOINT directive sets the executable which is used to execute the command given from the COMMAND directive.\nThe default ENTRYPOINT is /bin/sh -c so /bin/sh -c /data/docker-entrypoint.sh should work, if /data/docker-entrypoint.sh contains:\n/usr/sbin/init\nsystemctl restart autofs\npython /data/myapp.py\nThat means: You don't have to change the ENTRYPOINT\nIf you change the the ENTRYPOINT to /data/docker-entrypoint.sh than it should contain something like:\n/usr/sbin/init\nsystemctl restart autofs\npython /data/myapp.py\n# run the command given as arguments from CMD\nexec \"$@\"\nreference",
    "how to rsync from a host computer to docker container using docker cp": "The way to use rsync to copy files into a Docker container\nMake sure your Docker container has rsync installed, and define this alias:\nalias drsync=\"rsync -e 'docker exec -i'\"\nNow, you can use rsync with containers as if they are remote machines:\ndrsync -av /source/ container:/destination/",
    "Can't find module error when building docker for NodeJS app": "The problem was that our front-end developer considered that node imports are case insensitive and he was using windows. I tried to run Dockerfile on mac and that's why it couldn't find the modules. Module name was resetPass!",
    "Running a background process in container during one step in docker build": "As halfer states in his comment, this is not good practice.\nHowever for completeness I want to share a solution to the original question nevertheless:\nRUN nohup bash -c \"redis-server &\" && sleep 4 && /opt/gradle/gradle-4.6/bin/gradle build --info\nThis runs redis-server only for this single layer. The sleep 4 is just there to give redis enough time start up.\nSo the Dockerfile then looks as follows:\nFROM ubuntu:16.04\n\n# apt-get install stuff\n# ...\n# install gradle\n# build and install redis\n\nWORKDIR /app\nADD . /app\n\n# run unit tests / integration tests of app\nRUN nohup bash -c \"redis-server &\" && sleep 4 && /opt/gradle/gradle-4.6/bin/gradle build --info\n\n# TODO: uninstall redis\n\n# build app\nRUN ./gradlew assemble\n\n# start app with\n# docker run\nCMD [\"java\", \"-jar\", \"my_app.jar\"]",
    "If I run `docker-compose up` do I need a Dockerfile in the directory as well as a docker-compose.yml file?": "docker-compose lets you choose between the 2 options\nbuild the container from a specified image:\nservices:\n  example:\n    image: your/imagename\nbuild the container out of a Dockerfile:\nservices:\n  example:\n    build: \n      context: path/to/Dockerfile/dir\n      dockerfile: Dockerfile #here you specify the name of your Dockerfile file",
    "List all files in Build Context and/or in WORKDIR when building container image": "RUN. You can run any command you want in a container.\nRUN ls will run ls and print the output of the command.",
    "Create a Linux-based Docker file for .NET Framework project": "Finally, after a week of trying, I was able to get an answer.\nWe have to base the image on Nginx and install the mono on it.\nCreate a folder that contains the following:\nPublish your asp project in the dist folder.\nIn the Nginx folder create a folder with the sites-available name.\nIn the sites-available folder create a file with the default name and the following codes:\n    server {\n             listen   80;\n             access_log   /var/log/nginx/mono-fastcgi.log;\n             root /var/www/;\n             server_tokens off;\n             more_clear_headers Server X-AspNet-Version;\n    \n             location / {\n                     index index.html index.htm default.aspx Default.aspx;\n                     fastcgi_index /;\n                     fastcgi_pass unix:/var/run/mono-fastcgi.sock;\n                     include /etc/nginx/fastcgi_params;\n             }\n     }\nIn the Nginx folder create a file with the fastcgi_params name and the following codes:\nfastcgi_param  QUERY_STRING       $query_string;\nfastcgi_param  REQUEST_METHOD     $request_method;\nfastcgi_param  CONTENT_TYPE       $content_type;\nfastcgi_param  CONTENT_LENGTH     $content_length;\n\nfastcgi_param  SCRIPT_NAME        $fastcgi_script_name;\nfastcgi_param  REQUEST_URI        $request_uri;\nfastcgi_param  DOCUMENT_URI       $document_uri;\nfastcgi_param  DOCUMENT_ROOT      $document_root;\nfastcgi_param  SERVER_PROTOCOL    $server_protocol;\nfastcgi_param  REQUEST_SCHEME     $scheme;\nfastcgi_param  HTTPS              $https if_not_empty;\n\nfastcgi_param  GATEWAY_INTERFACE  CGI/1.1;\nfastcgi_param  SERVER_SOFTWARE    nginx/$nginx_version;\n\nfastcgi_param  REMOTE_ADDR        $remote_addr;\nfastcgi_param  REMOTE_PORT        $remote_port;\nfastcgi_param  SERVER_ADDR        $server_addr;\nfastcgi_param  SERVER_PORT        $server_port;\nfastcgi_param  SERVER_NAME        $server_name;\n\nfastcgi_param  PATH_INFO          \"\";\nfastcgi_param  SCRIPT_FILENAME    $document_root$fastcgi_script_name;\nIn the pools folder create a file with the sample.webapp name and the following codes:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<apps>\n    <web-application>\n        <name>root</name>\n        <vhost>*</vhost>\n        <vport>-1</vport>\n        <vpath>/</vpath>\n        <path>/var/www/sample-app/</path>\n    </web-application>\n</apps>\nsupervisord.conf file:\n[supervisord]\nlogfile=/var/log/supervisor/supervisord.log\nlogfile_maxbytes = 50MB\nnodaemon=true\nuser=root\n\n[program:mono]\ncommand=fastcgi-mono-server4 --appconfigdir=/etc/mono/pools --socket=unix --filename=/var/run/mono-fastcgi.sock --printlog --name=mono\nuser=root\nstdout_logfile=/dev/stdout\nstdout_logfile_maxbytes=0\nstderr_logfile=/dev/stderr\nstderr_logfile_maxbytes=0\n\n[program:nginx]\ncommand=nginx\nuser=root\nstdout_logfile=/dev/stdout\nstdout_logfile_maxbytes=0\nstderr_logfile=/dev/stderr\nstderr_logfile_maxbytes=0\nFinally Dockerfile content is as follows:\nFROM mono:latest\n\nRUN apt-get update \\\n  && apt-get install -y \\\n  iproute2 supervisor ca-certificates-mono fsharp mono-vbnc nuget \\\n  referenceassemblies-pcl mono-fastcgi-server4 nginx nginx-extras \\\n  && rm -rf /var/lib/apt/lists/* /tmp/* \\\n  && echo \"daemon off;\" | cat - /etc/nginx/nginx.conf > temp && mv temp /etc/nginx/nginx.conf \\\n  && sed -i -e 's/www-data/root/g' /etc/nginx/nginx.conf\n\nCOPY nginx/ /etc/nginx/\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\nCOPY pools /etc/mono/pools\nCOPY dist /var/www/sample-app\n\n\nEXPOSE 80\n\nENTRYPOINT [ \"/usr/bin/supervisord\", \"-c\", \"/etc/supervisor/conf.d/supervisord.conf\" ]",
    "How to use dotnet restore properly in Dockerfile": "When Docker builds an image, it maintains a build cache:\nWhen building an image, Docker steps through the instructions in your Dockerfile, executing each in the order specified. As each instruction is examined, Docker looks for an existing image in its cache that it can reuse, rather than creating a new (duplicate) image.\nImportantly, the ADD and COPY instructions get special treatment:\nFor the ADD and COPY instructions, the contents of the file(s) in the image are examined and a checksum is calculated for each file. The last-modified and last-accessed times of the file(s) are not considered in these checksums. During the cache lookup, the checksum is compared against the checksum in the existing images. If anything has changed in the file(s), such as the contents and metadata, then the cache is invalidated.\nWhen building a .NET Core solution, we can be sure that after running dotnet restore, the result of running dotnet restore again will only change if the .csproj file has changed (e.g. a new package is added or a version is changed).\nBy copying the .csproj files into the image separately, we can take advantage of Docker's build cache, which means that so long as the .csproj file hasn't changed, the dotnet restore step will not be re-executed unnecessarily each and every time the image gets rebuilt.",
    "ERRO[0001] error waiting for container: context canceled": "alpine does not include bash by default. If you want to include bash, you should add RUN apk add --no-cache bash in your Dockerfile.",
    "docker not exposing the port with network host": "--net=host option\nThis option bind the virtual NIC of the container to the host physical NIC (by giving full access to local system services such as D-bus).\nWhen this option is used every program that request a network socket will be granted one by the host from the physical NIC. Your service will then be using the 5555 port as expected.\n-p 5555:5555 option\nThis option bind (through iptable-like mechanism) the network socket containter-ip:5555 to the network socket host-ip:5555.\nIn other words\nIt seems, IMHO, a bit illogical to use them both. If the needs is to publish the containerized service to the socket host-ip:5555 then the cleanest way is to only use the -p 5555:5555 option.",
    "Installed gems not found by bundler when BUNDLE_PATH changed with Docker": "I think that there is a lack of GEM_HOME/GEM_PATH in your code.\nGEM_HOME/GEM_PATH will be used by gem install xxx to install gems in a specific folder. BUNDLE_PATH will be used by bundle install to install gems in a specific folder but not by gem install xx\nTo have a working system you should do :\nFROM ruby:1.9.3\n\nRUN apt-get update -qq && apt-get install -y build-essential libpq-dev vim\nENV APP_HOME /next-reg\nRUN mkdir $APP_HOME\nWORKDIR $APP_HOME\n\nENV BUNDLE_PATH /box\nENV GEM_PATH /box\nENV GEM_HOME /box\n\nADD . $APP_HOME\n\nRUN gem install bundler\nRUN gem install tzinfo -v 1.2.2\n\nCOPY Gemfile Gemfile\n\nRUN  bundle install\nWith this Gemfile :\nsource 'https://rubygems.org'\n\ngem 'tzinfo', '1.2.2'\nWich will produce :\nStep 11/13 : RUN gem install tzinfo -v 1.2.2\n ---> Running in 8a87fa54fa19\nSuccessfully installed thread_safe-0.3.6\nSuccessfully installed tzinfo-1.2.2\n2 gems installed\n ---> 3c91d59bde8a\nRemoving intermediate container 8a87fa54fa19\n\nStep 13/13 : RUN bundle install\n ---> Running in 20f1e4ec93b1\nDon't run Bundler as root. Bundler can ask for sudo if it is needed, and\ninstalling your bundle as root will break this application for all non-root\nusers on this machine.\nFetching gem metadata from https://rubygems.org/...\nFetching version metadata from https://rubygems.org/.\nResolving dependencies...\nRubygems 1.8.23.2 is not threadsafe, so your gems will be installed one at a time. Upgrade to Rubygems 2.1.0 or higher to enable parallel gem installation.\nInstalling rake 12.0.0\nUsing thread_safe 0.3.6\nUsing bundler 1.14.6\nUsing tzinfo 1.2.2\nBundle complete! 2 Gemfile dependencies, 4 gems now installed.\nBundled gems are installed into /box.\nAs you can see in the result output, the bundle install re-use the preloaded gems from gem install",
    "Docker, docker-compose, restart: unless-stopped loose logs in console": "Easy Answer, it's neither a docker bug nor a bug on your end =)\nDocker logs are attached to containers respectively, so when you're trying to see the logs (after your new app container has been created) you'll notice that it's empty and there's no history to scroll up for. While in fact the logs you're looking for were attached to your old container that has been removed and replaced by the new app container.\nas a work around, just always mount volume the log files from within your rails app to outside docker, that way you wont lose any data.\nthe logs/data will be persistent even if you stop or remove the container.",
    "Docker run gives \"CreateProcess: failure in a Windows system call: The system cannot find the file specified. (0x2)\"": "In the runner config.toml, my runner was configured to use the shell pwsh.\nI replaced it with powershell and it worked.\nThe error comes from the fact that, for whatever reason, pwsh is not available.",
    "Dockerfile ENV variable substitution into another ENV variable": "Option 1: at container start\nYou can use a wrapper script to create your environment variables with the inheritance that you wish. Here is a simple wrapper script\nwrapper.sh\n#!/bin/bash\n\n# prep your environement variables\nexport command=\"echo Hello $var_env\"\n\n# run your actual command\necho \"Hello $command\"\nYour dockerfile needs to be adapted to use it\nFROM ubuntu\nCOPY ./wrapper.sh .\nENV var_env=Tarun\nENV command=\"echo Hello $var_env\"\nCMD [\"sh\",\"-c\",\"./wrapper.sh\"]\nOption 2: during build\nYou can archive this by rebuilding your image with different build args. Lets keep your dockerfile almost the same:\nFROM ubuntu\nARG var_env=Tarun\nENV command=\"echo Hello $var_env\"\nCMD [\"sh\",\"-c\",\"echo Hello $var_env\"]\nand run\ndocker build -t test .\nthis gives you your default image as defined in your dockerfile, but your var_env is no longer an environment variable.\nnext we run\ndocker build -t test --build-arg var_env=\"New Env Value\" .\nthis will invalidate the docker cache only from the line in which you have defined your build arg. So keep your definition of your ARG close to where it is used in order to maximize the caching functionality of docker build.\nYou can find more about build args here: https://docs.docker.com/engine/reference/commandline/build/",
    "How do I reuse the cache from a `RUN --mount=type=cache` docker build?": "There doesn't seem to be any way to extract this specific cache from the general docker working files.\nHowever, you can of course backup the whole of /var/lib/docker. This doesn't work for CircleCI's remote docker engine, because you don't have sudo access, but does work for GitHub Actions where you do.\nSee here for an example: https://github.com/Mahoney-playground/docker-cache-action",
    "Using docker --squash in docker-compose when building images": "Instead of using --squash, you can use Docker multi-stage builds.\nHere is a simple example for a Python app that uses the Django web framework. We want to separate out the testing dependencies into a different image, so that we do not deploy the testing dependencies to production. Additionally, we want to separate our automated documentation utilities from our test utilities.\nHere is the Dockerfile:\n# the AS keyword lets us name the image\nFROM python:3.6.7 AS base\nWORKDIR /app\nRUN pip install django\n\n# base is the image we have defined above\nFROM base AS testing\nRUN pip install pytest\n\n# same base as above\nFROM base AS documentation\nRUN pip install sphinx\nIn order to use this file to build different images, we need the --target flag for docker build. The argument of --target should name the name of the image after the AS keyword in the Dockerfile.\nBuild the base image:\ndocker build --target base --tag base .\nBuild the testing image:\ndocker build --target testing --tag testing .\nBuild the documentation image:\ndocker build --target documentation --tag documentation .\nThis lets you build images that branch from the same base image, which can significantly reduce build-time for larger images.\nYou can also use multi-stage builds in Docker Compose. As of version 3.4 of docker-compose.yml, you can use the target keyword in your YAML.\nHere is a docker-compose.yml file that references the Dockerfile above:\nversion: '3.4'\n\nservices:\n    testing:\n        build:\n            context: .\n            target: testing\n    documentation:\n        build:\n            context: .\n            target: documentation\nIf you run docker-compose build using this docker-compose.yml, it will build the testing and documentation images in the Dockerfile. As with any other docker-compose.yml, you can also add ports, environment variables, runtime commands, and so on.",
    "How can I see which file(s) caused a Dockerfile `COPY` statement to invalidate the cache?": "I don't think there is a way to see which file invalidated the cache with the current Docker image design.\nLayers and images since v1.10 are 'content addressable'. Their ID's are based on a SHA256 checksum which reflects their content.\nThe caching code just looks up the ID of the image/layer which will only exist in Docker Engine if the contents of the entire layer match (or possibly a collision).\nSo when you run docker build, a new build context is created for each command in the Dockerfile. A checksum is calculated for the entire layer that command would produce. Then docker checks to see if an existing layer is available with that checksum and run config.\nThe only way I can see to get individual file detail back would be to recompute the destination file checksums, which would probably negate most of the caching speed up. If you did want to do this anyway, the other problem is deciding which layer to check that against. You would have to lookup a previous image build tree (maybe by tag?) to find what the contents of the previous comparable layer were.",
    "Bad file descriptor ERROR during apk update in Docker container... Why?": "--no-cache option allows to not cache the index locally. That is helpful in keeping the container small.\nAlso, it is equivalent to apk update at the top and rm -rf /var/cache/apk/ in the end.\nSo you can try to use it this way:\nRUN apk add --update --no-cache bash \\\n    git \\\n    make \\\n    clang \\\n    g++ \\\n    go && \\\n    mkdir -p $REPO && \\\n    mkdir -p $GODIR/src && \\\n    rm -rf /usr/share/man && \\\n    apk del git clang",
    "Docker Ignore is not woking well with docker compose and my file structure": "At build time, the directive COPY . . (inside the Dockerfile) correctly copies all files not listed in .dockerignore in $RAILS_ROOT (inside the image). No problem here (check that by running docker run --rm custom-web:1.0 ls -al).\nBut here you run docker-compose to start the container, and you have defined a volume :\nvolumes:\n    - .:/var/www/${APP_NAME}_web\nThat means that files from the same directory as docker-compose.yml are shared between the host and the container. That's why you find all files (even those listed in .dockerignore) in $RAILS_ROOT (workdir of custom-web:1.0 image) after starting the container via docker-compose.\nIf you really need to share files between your host and the container (via a volume), I'll suggest you to mount the current directory in another location than the one specified in your Dockerfile, like :\nvolumes:\n    - .:/${APP_NAME}_web\nOtherwise, using COPY . . and a volume is redundant here.",
    "Docker build error OCI runtime create failed \"exec: \\\"/bin/bash\\\": stat /bin/bash": "Your container doesn't have bash installed but probably it has sh so run the container with (replace /bin/bash with /bin/sh):\ndocker exec -it username/imagename /bin/sh",
    "OSX Docker Build: How can I see the full build output? (--progress=plain not the solution!)": "Just set export DOCKER_BUILDKIT=0 in your shell....",
    "ERROR with \"Failed to set locale, defaulting to C\" on Centos at the docker environment (yum install)": "This is a good method to handle this issue, please follow the code to install the package \"glibc-langpack-en\" in your environment or put the command line in your dockerfile.\nDockerfile content\nFROM centos\nRUN yum install -y glibc-langpack-en\nCentos shell script\nsudo yum install -y glibc-langpack-en",
    "Install .NET Framework 3.5 on Windows Server Core Docker": "I took the following steps to resolve this issue:\nGot hold of the Windows Server 2016 Core ISO file. Mounted the file on local computer.\nExtracted the {mount}:/sources/sxs folder into a zip file (sxs.zip). Ensure that the .NET Framework 3.5 cab file (microsoft-windows-netfx3-ondemand-package.cab) is present in the sxs folder. In my case, this was the only file present in the sxs folder.\nCopy the sxs.zip file to my container. I copied it using the dockerfile of the image.\nUnzip the file to C:\\sources\\sxs folder in the container.\nUsed the Install-WindowsFeature powershell command to install the feature.\nInstall-WindowsFeature -Name NET-Framework-Features -Source C:\\sources\\sxs -Verbose\nHope this helps. I also found the following blog useful in understanding the on-demand features. https://blogs.technet.microsoft.com/askcore/2012/05/14/windows-8-and-net-framework-3-5/",
    "How to solve \"Can't separate key from value\" in docker": "Check for the version number of docker and if its 3.4+ then the docker compose v2 is enabled by default. To disable it, go to > docker desktop > preferences > experimental features > un-check \"use Docker Compose V2\" option. This is a move by docker hub to incorporate docker-compose as docker compose and may cause problems to your usual workflow. Enjoy :)",
    "Docker - Failed to ping backend": "I am also having a similar issue, and wanted to force quit the app, so I tried cmd + options + esc, but couldn't find the app in the options.\nI finally solved the issue by killing the docker process using Activity Monitor.\nSteps:\nSearch for activity monitor by pressing (cmd + space), start typing activity monitor, then hit enter\nGo to search bar and type docker, you will see the docker process in red with (not responding)\nDouble click on the docker process\nClick Quit and select Force quit.",
    "WORKDIR $HOME in Dockerfile does not seem to work": "Seems like the best way would be to explicitly set your own default value so you can be sure it's consistent, like:\nENV HOME /root\n\nWORKDIR $HOME\n.. do something in /root ..\n\nWORKDIR /tmp\n.. do something else in /tmp ..\n\nWORKDIR $HOME\n.. continue back in /root ..\nNote:\nThe WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile.\nhttps://docs.docker.com/engine/reference/builder/#/workdir",
    "Dockerfile running from Mysql cannot access apt-get": "Apparently since Oracle bought MySQL in 2010, they have been converting everything over to their proprietary OS. In the last few months, they switched the default mysql package to Oracle OS from Debian.\nSee the packages here: https://hub.docker.com/_/mysql\nYou now need to specify the debian package like:\nFROM mysql:5.7-debian\n\nRUN apt-get -y update && apt-get upgrade -y",
    "Why docker-compose down deletes my volume? how to avoid this action done by 'down'. (Postgresql)": "According to the documentation, docker compose down will not delete any volume unless the -v option is used.",
    "Dockerfile and dpkg command": "Not the most elegant but:\n# continue executing even if command fails\nRUN dpkg -i vufind_3.1.1.deb || true",
    "Dynamically set JAVA_HOME of docker container": "Set JAVA_HOME in docker container\nDefault Docker file of the official image is Dockerfile\nIf you still want your own image with Java home set. Add this lines to your Dockerfile\nRUN apt-get update && \\\n    apt-get install -y openjdk-8-jdk && \\\n    apt-get install -y ant && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/ && \\\n    rm -rf /var/cache/oracle-jdk8-installer;\n    \nENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/\nRUN export JAVA_HOME\nCreate the docker-compose.yml, replace the dynamic path with container updating the environment variable in the environment:",
    "Installing and using Gradle in a docker image/container": "I solved the problem using the ENV docker instructions (link to the documentation).\nENV GRADLE_HOME=/app/gradle-2.4\nENV PATH=$PATH:$GRADLE_HOME/bin",
    "Does WORKDIR create a directory?": "The Dockerfile WORKDIR directive\n... sets the working directory.... If the WORKDIR doesn\u2019t exist, it will be created even if it\u2019s not used in any subsequent Dockerfile instruction.\nI occasionally see SO questions that RUN mkdir a directory before switching WORKDIR to it. Since WORKDIR will create the directory, this isn't necessary.\nAll paths in a Dockerfile are always inside the image, except for the source paths for COPY and ADD instructions, which are inside the build context directory on the host. Absolute paths like /code will be directly inside the root directory in the image, following normal Unix conventions.\nYou can run temporary containers off of your image to examine this, even if the Dockerfile isn't complete yet.\nhost$ docker build -t my-image .\nhost$ docker run --rm my-image ls -l /\nhost$ docker run --rm -it my-image /bin/sh\n0123456789ab# ls -l /\n0123456789ab# exit\n(This will always work, assuming the image includes core tools like sh and ls. docker exec requires the container to be running first; while you're refining the Dockerfile this may not be possible yet.)",
    "adding .net core to docker container with Jenkins": "As of this response you can use the following Dockerfile to get .NetCore 2 installed into the Jenkins container. You can obviously take this further and install the needed plugins and additional software as needed. I hope this helps you out!\nFROM jenkins/jenkins:lts\n # Switch to root to install .NET Core SDK\nUSER root\n\n# Just for my sanity... Show me this distro information!\nRUN uname -a && cat /etc/*release\n\n# Based on instructiions at https://learn.microsoft.com/en-us/dotnet/core/linux-prerequisites?tabs=netcore2x\n# Install depency for dotnet core 2.\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n    curl libunwind8 gettext apt-transport-https && \\\n    curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg && \\\n    mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg && \\\n    sh -c 'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-debian-stretch-prod stretch main\" > /etc/apt/sources.list.d/dotnetdev.list' && \\\n    apt-get update\n\n# Install the .Net Core framework, set the path, and show the version of core installed.\nRUN apt-get install -y dotnet-sdk-2.0.0 && \\\n    export PATH=$PATH:$HOME/dotnet && \\\n    dotnet --version\n\n# Good idea to switch back to the jenkins user.\nUSER jenkins",
    "What exactly is the frontend and backend of docker buildkit?": "TLDR; The frontend and backend concept was born with Buildkit and didn't exist in docker before. Frontend is like a compiler that converts a user's file (eg: Dockerfile) to LLB. Backend executes LLB in the most efficent way to build a docker image.\nHistory\nWithout Buildkit, a docker image is built directly using instructions in a Dockerfile. No intermeditate representation of these instructions is created. The instructions are passed to the docker engine (also called the Moby Engine or classic builder) which then builds the image.\nThen it was realised that to improve and optimise the build process further most of the fundamentals of the build operation would have to be redefined. Hence a proposal was made to create a new engine, and Buildkit was born along with frontend and backend separation of the build process.\nOne of the main design goals of buildkit is to separate frontend and backend concerns during a build process. A frontend is something designed for the users to describe their build definition. Backend solves the problem of finding a most efficient way to solve a common low-level description of the build operations, that has been prepared for them by the frontends.\nLLB\nThe separation of frontend and backend is acheived by LLB(low-leve builder).\nEverything about execution and caching of your builds is defined only in LLB.\nFrontend\nFrontends are components that run inside BuildKit and convert any build definition(file written by the user) to LLB. BuildKit supports loading frontends dynamically from container images by specifying: #syntax=.... A famous frontend is the dockerfile frontend because it is used with the docker engine. You can specify this container image with: #syntax=docker/dockerfile:latest.\nThere are plenty of other frontends that can be used, for example the mockerfile frontend with: #syntax=r2d4/mocker. This then allows you to use a slightly different syntax compared to the usual Dockerfile syntax.\nBackend\nThe Buildkit backend solves the LLB generated from any of a variety of frontends. Since the LLB is a dependency graph, it can be processed to: detect and skip executing unused build stages, parallelize building independent build stages etc. This is why Buildkit is able to improve performance, storage management etc. over the older build process. Also, the caching model has been entirely rewritten.\nThe core part of the builder(Buildkit) is a solver that takes a DAG of low-level build(LLB) instructions from the frontend and finds a way to execute them in a most efficient manner while keeping the cache for the next invocations.\nTo use the Buildkit backend specifyDOCKER_BUILDKIT=1.\nStarting with version 18.09, Docker supports a new backend for executing your builds that is provided by the moby/buildkit project.\nThe Moby Engine(the classic builder) can be called the original backend but remember it doesn't use LLB, therefore its build process doesn't have a frontend and backend as such.\nReferences and resouces:\nThe original proposal to create Buildkit by Tonis Tiigi\nTonis's introductory blog about Buildkit with an intro to LLB\nAn article on how docker build works internally without buildkit\nA Docker blog with content on Buildkit, frontends, and LLB\nMockerfile website, which includes code on how to generate LLB\nDockerfile reference",
    "DL4006 warning: Set the SHELL option -o pipefail before RUN with a pipe in it": "Oh, just found the solution in the wiki page at https://github.com/hadolint/hadolint/wiki/DL4006\nHere is my fixed version:\nFROM strimzi/kafka:0.20.1-kafka-2.6.0\n\nUSER root:root\nRUN mkdir -p /opt/kafka/plugins/debezium\n# Download, unpack, and place the debezium-connector-postgres folder into the /opt/kafka/plugins/debezium directory\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"]\nRUN curl -s https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.7.0.Final/debezium-connector-postgres-1.7.0.Final-plugin.tar.gz | tar xvz --transform 's/debezium-connector-postgres/debezium/' --directory /opt/kafka/plugins/\nUSER 1001\nThe reason adding SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"] is at https://github.com/docker/docker.github.io/blob/master/develop/develop-images/dockerfile_best-practices.md#using-pipes\nBelow is a copy:\nSome RUN commands depend on the ability to pipe the output of one command into another, using the pipe character (|), as in the following example:\nRUN wget -O - https://some.site | wc -l > /number\nDocker executes these commands using the /bin/sh -c interpreter, which only evaluates the exit code of the last operation in the pipe to determine success. In the example above this build step succeeds and produces a new image so long as the wc -l command succeeds, even if the wget command fails.\nIf you want the command to fail due to an error at any stage in the pipe, prepend set -o pipefail && to ensure that an unexpected error prevents the build from inadvertently succeeding. For example:\nRUN set -o pipefail && wget -O - https://some.site | wc -l > /number\nNot all shells support the -o pipefail option.\nIn cases such as the dash shell on Debian-based images, consider using the exec form of RUN to explicitly choose a shell that does support the pipefail option. For example:\nRUN [\"/bin/bash\", \"-c\", \"set -o pipefail && wget -O - https://some.site | wc -l > /number\"]",
    "Install oracle client in docker container": "I have figure out some different way to install Oracle instant client in ubuntu Docker, it might help others\nFollow these simple steps:\nDownload oracle instant client (.rpm file) from oracle official download center\nConvert into .deb (you can use apt-get install alien ) and move somewhere in your working directory.\nNow Update your Dockerfile and make build\nRUN apt-get update\nWORKDIR /opt\nADD ./ORACLE-INSTANT-CLIENT.deb  /opt\n#if libaio also required\nRUN apt-get install libaio1 \nRUN dpkg -i oracle-instantclient.deb",
    "Install build-essential in Docker image without having to do `apt-get update`?": "Create a base image which contains:\nFROM python:3.7-slim\n\nRUN apt-get update && apt-get install build-essential -y\nBuild it:\ndocker build -t mybase .\nThen use it for new images:\nFROM mybase",
    "apt-get install in Ubuntu 16.04 docker image: '/etc/resolv.conf': Device or resource busy": "As mentioned in https://github.com/moby/moby/issues/1297 you can add the following line to your Dockerfile:\nRUN echo \"resolvconf resolvconf/linkify-resolvconf boolean false\" | debconf-set-selections\nThis way it is possible to install resolvconf inside a container.",
    "How to append multi-lines to file in a dockerfile? [duplicate]": "That should do the trick:\nRUN echo $'first line \\n\\\nsecond line \\n\\\nthird line' > /etc/nginx/nginx.conf\nBasically it's wrapped in a $'' and uses \\n\\ for new lines.",
    "Nodemon inside docker container": "In you Dockerfile, you are running npm install after copying your package*json files. A node_modules directory gets correctly created in /usr/src/app and you're good to go.\nWhen you mount your local directory on /usr/src/app, though, the contents of that directory inside your container are overriden with your local version of the node project, which apparently is lacking the node_modules directory, causing the error you are experiencing.\nYou need to run npm install on the running container after you mounted your directory. For example you could run something like:\ndocker exec -ti <containername> npm install\nPlease note that you'll have to temporarily change your CMD instruction to something like:\nCMD [\"sleep\", \"3600\"]\nIn order to be able to enter the container.\nThis will cause a node_modules directory to be created in your local directory and your container should run nodemon correctly (after switching back to your current CMD).",
    "Docker Tomcat users configuration not working": "First you need to expose your application in the container, so you can connect to it from dockerhost/network.\ndocker run -d -p 8000:8080 tomcat:8.5.11-jre8\nYou need to change 2 files in order to access the mangaer app from remote host. (Browser on Docker host is considered remote, only packets received on containers loopback are considered local for tomcat)\n/usr/local/tomcat/webapps/manager/META-INF/context.xml Note the commented section.\n<Context antiResourceLocking=\"false\" privileged=\"true\" >\n<!--\n     <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\"\n         allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\" />\n-->\nPlease note the commented section.\n/usr/local/tomcat/conf/tomcat-users.xml as you stated in the question.\n<tomcat-users xmlns=\"http://tomcat.apache.org/xml\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n          xsi:schemaLocation=\"http://tomcat.apache.org/xml tomcat-users.xsd\"\n          version=\"1.0\">\n<role rolename=\"manager-gui\"/>\n<role rolename=\"manager-script\"/>\n<user username=\"admin\" password=\"password\" roles=\"manager-gui,manager-script\" />\nIn order to make changes to files in the container, You can try building your own image, but I suggest using docker volumes or bind mounts.\nAlso make sure you restart the container so the changes take effect.",
    "What is the difference between `AS base` and `AS build` in the Dockerfile?": "There is no functional difference. it's a name for the build stage.\nA stage is a part of the dockerfile that starts at the FROM keyword and ends before the next FROM keyword.\nImage built in a stage of the dockerfile will be later accessible using that stage's name.\nFor example\nFROM image AS foo\n...\n...\n\nFROM foo AS bar\nRUN touch /example\n...\n...\n\nFROM foo\nCOPY --from=bar /example /var/example\nOptionally a name can be given to a new build stage by adding AS name to the FROM instruction. The name can be used in subsequent FROM and COPY --from=<name|index> instructions to refer to the image built in this stage.\nhttps://docs.docker.com/engine/reference/builder/#from\nIt was added in 17.05 for multistage builds, more on that: https://docs.docker.com/develop/develop-images/multistage-build/",
    "Docker build image complain about: fatal: not a git repository (or any of the parent directories): .git": "change the \"RUN cd /cloned-qt/\" command to \"WORKDIR cloned-qt\" and it will work as expected",
    "Docker error: standard_init_linux.go:228: exec user process caused: exec format error": "A \"multiarch\" Python interpreter built on MacOS is intended to target MacOS-on-Intel and MacOS-on-Apple's-arm64.\nThere is absolutely no binary compatibility with Linux-on-Apple's-arm64, or with Linux-on-aarch64. You can't run MacOS executables on Linux, no matter if the architecture matches or not.",
    "Cannot reach docker container - port not bound": "As David Maze pointed out, order does matter for the docker cmd command.\nThe -dp option needs to come before the image name.\nSo using\ndocker container run -dp 5000:5000 -t test_tag \nworks like a charm.",
    "How do I unset a Docker image label?": "Great question. I did some research and, as far as I know, it's not possible with the current Docker/Moby implementation. It's also a problem for other properties as well, as you can see here (the issue is from 2014!):\nhttps://github.com/moby/moby/issues/3465\nI know it's really annoying, but, if you really want to remove that you can try following this:\nhttps://github.com/moby/moby/issues/3465#issuecomment-383416201\nThe person automatized this process with a Python script that seems to let you do what you want:\nhttps://github.com/gdraheim/docker-copyedit\nIt appears to have the Remove Label operation (https://github.com/gdraheim/docker-copyedit/blob/92091ed4d7a91fda2de39eb3ded8dd280fe61a35/docker-copyedit.py#L304), that is what you want.\nI don't know if it works (I haven't had time to test that), but I think it's worth trying.",
    "docker-compose args from shell": "You can use the same syntax with docker-compose build:\ndocker-compose build --build-arg RSA=\"$(cat ~/.ssh/id_rsa)\"\nUnfortunately, you can't use the build-args option with compose up or start... So you will need to build and then run using the --no-build option",
    "Error [ERR_PACKAGE_PATH_NOT_EXPORTED]: Package subpath './public/extractFiles' is not defined by \"exports\" in": "same problem here.\nIn my case this was raised due to old npm package dependencies.\nextract-files Version 7 uses a deprecated node function:\n(node:2520) [DEP0148] DeprecationWarning: Use of deprecated folder mapping \"./public/\" in the \"exports\" field module resolution of the package at ...\\node_modules\\extract-files\\package.json.\nNode 17 raises this error you posted.\nPossible Solutions:\nUpdate extract-files package to latest\nUse Node 16.x\nBonus tip:\nsince you\u00b4re copying package-lock.json you can run\nnpm ci\ninstead of npm install. Its faster and made for CI/CD pipelines",
    "GeoDjango can't find gdal on docker python alpine based image": "I also struggled with this one for a while, the final solution proved quite simple (im using MySql so less dependencies):\nInstall the dependencies normally in the Dockerfile, e.g:\nRUN apk add --no-cache geos gdal \nAnd then setup their respective variables in the Django settings using glob, e.g:\nfrom glob import glob\n\nGDAL_LIBRARY_PATH=glob('/usr/lib/libgdal.so.*')[0]\nGEOS_LIBRARY_PATH=glob('/usr/lib/libgeos_c.so.*')[0]",
    "How to dockerize an ASP.NET Core 2.0 application?": "In case of related projects you have to run dotnet restore and dotnet publish against your solution file instead and to put your docker file at the solution level so you can access all projects from it.\nBasically the only change you need in the docker file it is:\nCOPY *.sln ./\nCOPY ./your_proj1_folder/*.csproj ./your_proj1_folder/\nCOPY ./your_proj2_folder/*.csproj ./your_proj2_folder/\nRUN dotnet restore",
    "How to use docker-compose yml file for production?": "Suggest you to use Multiple Compose files:\nUsing multiple Compose files enables you to customize a Compose application for different environments or different workflows.\nNext is an example:\n(NOTE: next omit some elements of compose file)\ndocker-compose.yml:\nweb:\n  image: example/my_web_app:latest\ndocker-compose.dev.yml:\nweb:\n  ports:\n    - 80:80\nExecute docker-compose -f docker-compose.yml -d will have no ports map.\nExecute docker-compose -f docker-compose.yml -f docker-compose.dev.yml -d will make docker-compose.dev.yml to override some value of docker-compose.yml which make your aims.\nFor detail, refers to docker doc, it is the official suggestion to handle your scenario, FYI.\nUPDATED:\nYou use build: context: ./mariadb, so compose can always find Dockerfile in the folder mariadb to build, no matter in local dev server or prod server.\nJust above will have image build both on dev & prod server, this is one option for you to follow.\nAnother option as you said in comments:\nBut on prod server, I can only pull and run image, and the image would have to be built with the prod yml file beforehand\nSo you may not want to build image again on prod server?\nThen, next is a updated solution, just an example:\ndocker-compose.yml:\ndb:\n  image: your_maridb_image_name:your_maridb_image_version\n  networks:\n    - default\ndocker-compose.dev.yml:\ndb:\n  build:\n    context: ./mariadb\n  ports:\n    - \"xxx:xxx\"\ndocker-compose.prod.yml:\ndb:\n  otheroptions_special_for_prod_just_a_example: xxx\n1) docker-compose -f docker-compose.yml -f docker-compose.dev.yml -d\nThis will combine as next:\ndb:\n  image: your_maridb_image_name:your_maridb_image_version\n  networks:\n    - default\n  build:\n    context: ./mariadb\n  ports:\n    - \"xxx:xxx\"\nPer docker-compose syntax, if build:context was afford, compose will not docker pull image from docker registry, just find the Dockerfile in context, and finally build a image with the name your specified in image, here it's your_maridb_image_name:your_maridb_image_version.\nThen, you need to push it dockerhub, but you do need to stop your local container.\n2) docker-compose -f docker-compose.yml -f docker-compose.prod.yml -d\nThis will combine as next:\ndb:\n  image: your_maridb_image_name:your_maridb_image_version\n  networks:\n    - default\n  otheroptions_special_for_prod_just_a_example: xxx\nPer docker-compose syntax, no build:context was afford, so compose will directly docker pull image from remote registry(docker hub), remember you have pushed the image to dockerhub after you finished the development on local dev server? So no need to build image again.",
    "'Unable to Find User ContainerUser' when building Windows Dockerfile via Actions Runner": "I had the same issue until I removed the nanoserver part of the FROM statement\nTry:\nFROM mcr.microsoft.com/dotnet/sdk:7.0 AS build\nRather than:\nFROM mcr.microsoft.com/dotnet/sdk:7.0-nanoserver-1809 AS build",
    "How can I pass array into a dockerfile and loop through it?": "You can pass a space-separated string to builds then convert string to an array or just loop over the string.\nDockerfile\nFROM alpine\nARG items\nRUN for item in $items; do \\\n    echo \"$item\"; \\\n    done;\npass value during build time\ndocker build --build-arg items=\"item1 item2 item3 item4\" -t my_image .\noutput\nStep 3/3 : RUN for item in $items; do     echo \"$item\";     done;\n ---> Running in bee1fd1dd3c6\nitem1\nitem2\nitem3",
    "Possible to use pushd/popd in Dockerfile?": "It can be done but your image must have bash and all commands must be in the same RUN directive:\nDockerfile\nFROM debian\nRUN mkdir -p /test/dir1/dir2\nRUN bash -xc \"\\\npushd /test/dir1; \\\npwd; \\\npushd dir2; \\\npwd; \\\npopd; \\\npwd; \\\n\"\nRUN pwd\nSending build context to Docker daemon  77.25MB\nStep 1/4 : FROM debian\n ---> 2d337f242f07\nStep 2/4 : RUN mkdir -p /test/dir1/dir2\n ---> Using cache\n ---> d609d5e33b08\nStep 3/4 : RUN bash -xc \"pushd /test/dir1; pwd; pushd dir2; pwd; popd; pwd; \"\n ---> Running in 79aa21ebdd15\n+ pushd /test/dir1\n+ pwd\n+ pushd dir2\n+ pwd\n+ popd\n+ pwd\n/test/dir1 /\n/test/dir1\n/test/dir1/dir2 /test/dir1 /\n/test/dir1/dir2\n/test/dir1 /\n/test/dir1\nRemoving intermediate container 79aa21ebdd15\n ---> fb1a07d6e342\nStep 4/4 : RUN pwd\n ---> Running in 9dcb064b36bb\n/\nRemoving intermediate container 9dcb064b36bb\n ---> eb43f6ed241a\nSuccessfully built eb43f6ed241a\nSuccessfully tagged test:latest",
    "Multiple WordPress sites with one shared DB using Docker": "Yes, you can install multiple WordPress instances into one database. You just need to change the database prefix for each install when installing. Just check your wp-config and change prefix and DBs credentials.\n// ** MySQL settings - You can get this info from your web host ** //\n/** The name of the database for WordPress */\ndefine('DB_NAME', 'database_name_here');\n\n/** MySQL database username */\ndefine('DB_USER', 'username_here');\n\n/** MySQL database password */\ndefine('DB_PASSWORD', 'password_here');\n\n/** MySQL hostname */\ndefine('DB_HOST', 'localhost');\n\n/** Database Charset to use in creating database tables. */\ndefine('DB_CHARSET', 'utf8');\n\n/** The Database Collate type. Don't change this if in doubt. */\ndefine('DB_COLLATE', '');",
    "Error 'import path does not begin with hostname' when building docker with local package": "The application is built inside the docker container and you need to have your dependencies available when building.\ngolang:onbuild gives compact Dockerfiles for simple cases but it will not fetch your dependencies.\nYou can write your own Dockerfile with the steps needed to build your application. Depending on how your project looks you could use something like this:\nFROM golang:1.6\nADD . /go/src/yourapplication\nRUN go get github.com/jadekler/git-go-websiteskeleton\nRUN go install yourapplication\nENTRYPOINT /go/bin/yourapplication\nEXPOSE 8080\nThis adds your source and your dependency into the container, builds your application, starts it, and exposes it under port 8080.",
    "How to copy/add files in user's home directory in host to container's home directory?": "It has now been two years sice the question has been ask, but I want to cite to official documentation here which states the same as what @Sung-Jin Park already found out.\nADD obeys the following rules:\nThe path must be inside the context of the build; you cannot ADD ../something /something, because the first step of a docker build is to send the context directory (and subdirectories) to the docker daemon.\nDockerfile reference ADD",
    "Permission denied to Docker daemon socket at unix:///var/run/docker.sock": "A quick way to avoid that. Add your user to the group.\nsudo gpasswd -a $USER docker\nThen set the proper permissions.\nsudo setfacl -m \"user:$USER:rw\" /var/run/docker.sock\nShould be good from there.",
    "Build Docker Image From Go Code": "The following works for me;\npackage main\n\nimport (\n    \"archive/tar\"\n    \"bytes\"\n    \"context\"\n    \"io\"\n    \"io/ioutil\"\n    \"log\"\n    \"os\"\n\n    \"github.com/docker/docker/api/types\"\n    \"github.com/docker/docker/client\"\n)\n\nfunc main() {\n    ctx := context.Background()\n    cli, err := client.NewEnvClient()\n    if err != nil {\n        log.Fatal(err, \" :unable to init client\")\n    }\n\n    buf := new(bytes.Buffer)\n    tw := tar.NewWriter(buf)\n    defer tw.Close()\n\n    dockerFile := \"myDockerfile\"\n    dockerFileReader, err := os.Open(\"/path/to/dockerfile\")\n    if err != nil {\n        log.Fatal(err, \" :unable to open Dockerfile\")\n    }\n    readDockerFile, err := ioutil.ReadAll(dockerFileReader)\n    if err != nil {\n        log.Fatal(err, \" :unable to read dockerfile\")\n    }\n\n    tarHeader := &tar.Header{\n        Name: dockerFile,\n        Size: int64(len(readDockerFile)),\n    }\n    err = tw.WriteHeader(tarHeader)\n    if err != nil {\n        log.Fatal(err, \" :unable to write tar header\")\n    }\n    _, err = tw.Write(readDockerFile)\n    if err != nil {\n        log.Fatal(err, \" :unable to write tar body\")\n    }\n    dockerFileTarReader := bytes.NewReader(buf.Bytes())\n\n    imageBuildResponse, err := cli.ImageBuild(\n        ctx,\n        dockerFileTarReader,\n        types.ImageBuildOptions{\n            Context:    dockerFileTarReader,\n            Dockerfile: dockerFile,\n            Remove:     true})\n    if err != nil {\n        log.Fatal(err, \" :unable to build docker image\")\n    }\n    defer imageBuildResponse.Body.Close()\n    _, err = io.Copy(os.Stdout, imageBuildResponse.Body)\n    if err != nil {\n        log.Fatal(err, \" :unable to read image build response\")\n    }\n}",
    "Building Docker images on Windows: Entrypoint script \"no such file or directory\"": "So, even though logs say \"no such file or directory,\" the actual problem (at least in my case) was due to the difference in end-of-line (EOL) characters on Windows and Linux. Windows uses CRLF represent the end of a line and Unix/Linux uses LF.\nI hadn't consider this as a potential problem since the files were freshly cloned from Github and were originally created on Linux. What I didn't know is that on Windows Git is set up to automatically convert EOL characters to CRLF.\nMaking Git retain original EOL characters (disabling autocrlf).\nThere are a few ways to go about doing this. autocrlf is the name of the attribute that decides whether git converts line endings. You'd only need to do one of the following options depending on what you need.\nDisable autocrlf for one command\nYou can clone the files with the following to disable autocrlf as just a one time thing.\ngit clone https://github.com/someuser/somerepo --config core.autocrlf=false\nSpecify EOL type in .gitattributes\nIf you have a single repo that you know you want to always have autocrlf disabled, you can specify it in that repo's .gitattributes file. Just add the following line to your .gitattributes file.\n* text eol=lf\nDisable autocrlf in Git's config file\nNavigate to the folder where Git is installed on your machine. For me it was installed at C:\\ProgramData\\Git. Open config in a text editor. Change autocrlf=true to autocrlf=false.\nChanging EOL characters on existing files.\nIf you've got existing entrypoint scripts that you need to convert, or if you're writing your entrypoint scripts in Windows in the first place, you can easily set the EOL type with most popular text editors. I'll outline how to do it in Vim, Notepad++, and Sublime, but it should be easy enough to figure out by searching \"change EOL\" and the name of your text editor of choice.\nUsing Vim\nTo change the line endings to be compatible with Linux, do :set ff=unix. To change them so that they are compatible with Windows, do :set ff=dos.\nUsing Notepad++\nOn the menu bar click on Edit and then go to EOL Conversion and select the desired conversion. You'll want to select Unix (LF) to make it compatible with Linux.\nUsing Sublime\nOn the menu bar click 'View' and go to 'Line Endings' and from there select the desired conversion. You'll want to select Unix to make it compatible with Linux.\nConverting EOL characters from your Dockerfile.\nAlternatively, there's a useful tool called dos2unix that you can install in your image and use to convert your entrypoint script. Assuming an Ubuntu or Debian based image which uses apt-get , you can use it in the following way.\nFROM php:7.2-fpm\n\nRUN apt-get update && \\\n    apt-get install -y dos2unix\n\nCOPY custom-docker-php-entrypoint /usr/local/bin/\n\nRUN dos2unix /usr/local/bin/custom-docker-php-entrypoint\n\nENTRYPOINT [\"custom-docker-php-entrypoint\"]\nIf your Docker image is based on Alpine linux using apk for a package manager, you'll want to do something like this,\nFROM alpine:latest\n\nRUN apk --update add bash && \\\n    apk add dos2unix\n\nCOPY entrypoint.sh /\n\nRUN dos2unix /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\nIf your Docker image is based on Centos using yum as a package manager, you'll want to do something like this,\nFROM centos:latest\n\nRUN yum update -y && \\\n    yum install dos2unix -y\n\nCOPY entrypoint.sh /\n\nRUN dos2unix /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]",
    "Docker: mounting volume and run node apps": "This is not the right way to use the instruction VOLUME in dockerfile. As documentation says \u201cThe VOLUME instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers\u201d, and I don't think is what you want to do.\nYou don\u2019t need to specify the VOLUME instruction and you don\u2019t need to create the src directoy. Modify the Dockerfile as below:\nFROM node:6.0.0-slim\nMAINTAINER pyprism\n\n# Install dependencies\nWORKDIR /src/\n\nRUN npm install\n\n# Expose the app port\nEXPOSE 8000\n\n# Start the app\nCMD npm start\nNow you can navigate to the Dockerfile directory and build the image:\n$docker build -t node .\nThen run the container using the \u2013v option to mount your local directory, as below:\n $docker run -p 8000:8000 -v path/to/code/dir:/src/ <image-name>\nthis will mount your code directory in /src.\nif you want to use docker compose simply specify the volumes in the docker-compose.yml file:\n web:\n  build: .\n  volumes:\n    - path/to/code/dir:/src/ \n  ports:\n   - '8000:8000'",
    "Dockerfile Build Error: The system cannot find the path specified": "To build a docker image:\ncd /path/where/docker_file/lives\ndocker build .\nAbove is same as:\ndocker build -f Dockerfile .\nYou need to specify Dockerfile name only if it is not default:\ncd /path/where/docker_file/lives\ndocker build -f Dockerfile.modified .",
    "run django in docker container": "can't open file './manage.py runserver 0.0.0.0:8000 --settings=mysite.settings.prod'\nThis is telling you that it is treating that entire string as a single filename.\nI assume something like this works:\nCMD [ \"python\", \"./manage.py\", \"runserver\", \"0.0.0.0:8000\", \"--settings=mysite.settings.prod\" ]",
    "Installing GMP extention on PHP 7.4 FPM Aplpine (Docker)": "Like the error says: configure: error: GNU MP Library version 4.2 or greater required.\nYou can install GNU MP (GMP for short) on Alpine Linux by including the following in your Dockerfile:\nRUN apk add gmp-dev\n(or RUN apt-get install gmp-dev for other debian distros)",
    "COPY . . command in Dockerfile for ASP.NET": "The COPY . . copies the entire project, recursively into the container for the build.\nThe reason for the separation of the first 2 COPY commands with dotnet restore and then the complete COPY . . with dotnet build is a Docker caching trick to speed up container image builds. It is done this way so the project dependencies don't need to be reinstalled every time a code change is made.\nDocker images are built in layers. Docker compares the contents and instructions that would make up the each new layer to previous builds. If they match the SHA256 checksum for the existing layer, the build step for that layer can be skipped.\nCode changes a lot more than dependencies, and dependencies are usually fetched from a slow(ish) network now. If you copy the code after the dependency installs are completed then you don't bust the cached dependency layer for every other change.\nThis is a common theme across many languages with a dependency manager. Go, Python, Node.js etc. The Node.js equivalent does the package.json and package-lock.json before the rest of the application contents:\nWORKDIR /app\nCOPY package.json package-lock.json /app/\nRUN npm install\nCOPY . /app/\nCMD [\"node\", \"app/index.js\"]",
    "Dockerfile: how to Download a file using curl and copy into the container": "Not related but an easier way to handle downloads during build time is to use Docker's ADD directive without curl or wget.\nADD https://raw.githubusercontent.com/vishnubob/wait-for-it/master/wait-for-it.sh /tmp\nCOPY /tmp/wait-for-it.sh /app/wait-for-it.sh\nRight now, we recommend using Docker's ADD directive instead of running wget or curl in a RUN directive - Docker is able to handle the https URL when you use ADD, whereas your base image might not be able to use https, or might not even have wget or curl installed at all.\nhttps://github.com/just-containers/s6-overlay#usage",
    "How to create a Docker container of an AngularJS app?": "First of all, follow this best practice guide to build your angular app structure. The index.html should be placed in the root folder. I am not sure if the following steps will work, if it's not there.\nTo use a nginx, you can follow this small tutorial: Dockerized Angular app with nginx\n1.Create a Dockerfile in the root folder of your app (next to your index.html)\nFROM nginx\nCOPY ./ /usr/share/nginx/html\nEXPOSE 80\n2.Run docker build -t my-angular-app . in the folder of your Dockerfile.\n3.docker run -p 80:80 -d my-angular-app and then you can access your app http://localhost",
    "Customize ONBUILD environment in a dockerfile": "I was having the same problem and I managed to fix it adding this to the Dockerfile:\nCOPY pip.conf pip.conf\nENV PIP_CONFIG_FILE pip.conf\nRUN pip install <my_package_name>\nThe pip.conf file has the next structure:\n[global]\ntimeout = 60\nindex-url = https://pypi.org/simple\ntrusted-host = pypi.org\n               <my_server_page>\nextra-index-url = https://xxxx:yyyy@<my_server_page>:<package_location>\nThis is the only way I found for Docker to find the package from the pypi server. I hope this solution is general and helps other people having this problem.",
    "How to run .NET Core 2 application in Docker on Linux as non-root": "In linux, binding to a port less than 1024 requires the user to be superuser. You can just use the default port 5000 and then publish to port 80 on your host (if you don't have any reverse proxy).",
    "How can I prevent Docker from removing intermediate containers when executing RUN command?": "docker build --rm=false\nRemove intermediate containers after a successful build (default true)",
    "Why do I get \"unzip: short read\" when I try to build an image from Dockerfile?": "Somehow, curl on alpine linux distro can't set cookie headers correctly while downloading jce zip file. It seems it downloads a zip file but in fact it is an html error page. If you view the file you can see that it is an html file. I've used wget instead of curl and it successfully downloaded file. Then unzip operation worked as expected.\nFROM openjdk:8-jdk-alpine\nRUN  apk update && apk upgrade && apk add netcat-openbsd\nRUN mkdir -p /usr/local/configserver\nRUN cd /tmp/ && \\\n    wget 'http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip' --header \"Cookie: oraclelicense=accept-securebackup-cookie\" && \\\n    unzip jce_policy-8.zip && \\\n    rm jce_policy-8.zip && \\\n    yes |cp -v /tmp/UnlimitedJCEPolicyJDK8/*.jar /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/\nADD @project.build.finalName@.jar /usr/local/configserver/\nADD run.sh run.sh\nRUN chmod +x run.sh\nCMD ./run.sh",
    "How can I reduce the size of Docker images": "Using the official node alpine image as a base image, as most here suggested, is a simple solution to reduce the overall size of the image, because even the base alpine image is a lot smaller compared to the base ubuntu image.\nA Dockerfile could look like this:\nFROM node:alpine\n\nARG ENVIRONMENT\nARG PORT\n\nRUN mkdir /consumer_portal \\\n    && npm install -g express path\n\nCOPY . /consumer_portal\nWORKDIR /consumer_portal\n\nRUN npm cache clean \\\n    && npm install\n\nEXPOSE $PORT\n\nCMD [ \"node\",  \"server.js\" ]\nIt's nearly the same and should work as expected. Most of the commands from your ubuntu image can be applied the same way in the alpine image.\nWhen I add mock-data to be create a similar project as you might have, results in an ubuntu image with a size of 491 MB and the alpine version is only 62.5 MB big:\nREPOSITORY   TAG       IMAGE ID        CREATED          SIZE\nalpinefoo    latest    8ca6f338475e    5 minutes ago    62.5MB\nubuntufoo    latest    38620a1bd5a6    6 minutes ago    491MB",
    "Docker-Compose: How to depends_on a container on another network? I am getting an error saying container 'undefined' even though networks are linked": "Depends_on only works on services within the same compose file, so to do what you want, you would need to use something like wait-for-it.sh. Take a look here for more information: https://docs.docker.com/compose/startup-order/\nSomething like this may work for you or you can create a custom wait-for-it script as well:\nservices:\n  frontend:\n    container_name: frontend\n    restart: unless-stopped\n    stdin_open: true\n    build:\n      context: ../realm-frontend\n    volumes:\n      - static:/realm-frontend/build\n    command: [\"./wait-for-it.sh\", \"wordpress:80\", \"--\", \"yourfrontendcmd\"]\n    networks:\n      - cms_wpsite",
    "what does VOLUME inside Dockerfile do": "Docker Volumes:\nVolumes decouple the life of the data being stored in them from the life of the container that created them. This makes it so you can docker rm my_container and your data will not be removed.\nA volume can be created in two ways:\nSpecifying VOLUME /some/dir in a Dockerfile\nSpecying it as part of your run command as docker run -v /some/dir\nEither way, these two things do exactly the same thing. It tells Docker to create a directory on the host, within the docker root path (by default /var/lib/docker), and mount it to the path you've specified (/some/dir above). When you remove the container using this volume, the volume itself continues to live on.\nIf the path specified does not exist within the container, a directory will be automatically created.\nYou can tell docker to remove a volume along with the container:\ndocker rm -v my_container\nSometimes you've already got a directory on your host that you want to use in the container, so the CLI has an extra option for specifying this:\ndocker run -v /host/path:/some/path ...\nThis tells docker to use the specified host path specifically, instead of creating one itself within the docker root, and mount that to the specified path within the container (/some/path above).\nNote, that this can also be a file instead of a directory. This is commonly referred to as a bind-mount within docker terminology (though technically speaking, all volumes are bind-mounts in the sense of what is actually happening). If the path on the host does not exist, a directory will be automatically be created at the given path.\nFrom the docker documentation:\nVOLUME [\"/data\"]\nThe VOLUME instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers. The value can be a JSON array, VOLUME [\"/var/log/\"], or a plain string with multiple arguments, such as VOLUME /var/log or VOLUME /var/log /var/db. For more information/examples and mounting instructions via the Docker client, refer to Share Directories via Volumes documentation.\nThe docker run command initializes the newly created volume with any data that exists at the specified location within the base image. For example, consider the following Dockerfile snippet:\nFROM ubuntu\nRUN mkdir /myvol\nRUN echo \"hello world\" > /myvol/greeting\nVOLUME /myvol\nThis Dockerfile results in an image that causes docker run to create a new mount point at /myvol and copy the greeting file into the newly created volume.\nAnswer:\nSo in the above case , the VOLUME [\"/var/lib/bootstrap\"] instruction is persisting the data by creating a volume in /var/lib/docker on the host and mount it on /var/lib/bootstrap in the container.\nNotes about specifying volumes\nKeep the following things in mind about volumes in the Dockerfile.\nVolumes on Windows-based containers: When using Windows-based containers, the destination of a volume inside the container must be one of:\na non-existing or empty directory\na drive other than C:\nChanging the volume from within the Dockerfile: If any build steps change the data within the volume after it has been declared, those changes will be discarded.\nJSON formatting: The list is parsed as a JSON array. You must enclose words with double quotes (\")rather than single quotes (').\nThe host directory is declared at container run-time: The host directory (the mountpoint) is, by its nature, host-dependent. This is to preserve image portability, since a given host directory can\u2019t be guaranteed to be available on all hosts. For this reason, you can\u2019t mount a host directory from within the Dockerfile. The VOLUME instruction does not support specifying a host-dir parameter. You must specify the mountpoint when you create or run the container.",
    "Can't load prometheus.yml config file with docker (prom/prometheus)": "By \u201cthe file already exists\u201d, do you mean that the file is on your host at /prometheus-data/prometheus.yml? If so, then you need to bind mount it into your container for it to be accessible to Prometheus.\nsudo docker run -p 9090:9090 -v /prometheus-data/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus\nIt's covered under Volumes & bind-mount in the documentation.",
    "Remove sensitive information from environment variables in postgres docker container": "use args without values to build the image in your Dockerfile:\nARG PASSWORD \nand build it using\nexport PASSWORD=\"MYPASS\" && docker build ...\nin this way the ARG is not there when running the container\nhere is a complete example:\ndockerfile:\nFROM postgres:10.0-alpine\n\nARG my_user\nARG my_pass\nCompose:\nversion: \"3\"\nservices:\n       db:\n         build:\n           context: .\n           args:\n            - my_user\n            - my_pass       \n         environment:\n           - POSTGRES_USER=${my_user}\n           - POSTGRES_PASSWORD=${my_pass}\n           - POSTGRES_DB=db\nrun it:\nexport my_user=test && export my_pass=test1cd && docker-compose up -d --build\nnow if you login to the container and try echo $my_pass you get an empty string\nresult :\ndocker exec -ti 3b631d907153 bash\n\nbash-4.3# psql -U test db\npsql (10.0)\nType \"help\" for help.\n\ndb=#",
    "Adding large files to docker during build": "These files might change once in a while, and I don't mind to rebuild my container and re-deploy it when it happens.\nThen a source control is not the best fit for such artifact.\nA binary artifact storage service, like Nexus or Artifactory (which both have free editions, and have their own docker image if you need one) is more suited to this task.\nFrom there, your Dockerfile can fetch from Nexus/Artifactory your large file(s).\nSee here for proper caching and cache invalidation.",
    "How do I check if Oracle is up in Docker?": "Using docker-compose.yml and Official Oracle docker images you can use checkDBStatus.sh script as a healthcheck. The script returns non-0 while db is in ORA-01033 state. Below is an example. Notice the combination of db's service healthcheck and tomcat's depends_on with service_healthy condition:\n  tomcat:\n    image: \"tomcat:9.0\"\n    depends_on:\n      oracle-db:\n        condition: service_healthy\n    links:\n      - oracle-db\nservices:\n  oracle-db:\n    build:\n      context: src/main/docker/oracle_db\n      dockerfile: Dockerfile.xe\n    mem_reservation: 2g\n    environment:\n      - ORACLE_PWD=oracle\n    volumes:\n      - oracle-data:/opt/oracle/oradata\n    healthcheck:\n      test: [ \"CMD\", \"/opt/oracle/checkDBStatus.sh\"]\n      interval: 2s\n\nvolumes:\n  oracle-data:",
    "Docker, Copying image, error - ERROR: failed to solve: failed to compute cache key: failed to calculate checksum": "Double check your .dockerignore file, if it exists make sure that the file mentioned in the error message is not present in this file.",
    "How to get back to shell in nodejs:latest docker image?": "In order to overwrite the entry point of the docker image you're using, you will need to use the --entrypoint flag in the run command.\ndocker run -it --entrypoint bash node:latest\nFor better understanding on how to work with already running docker container you can refer to the following question",
    "Docker exec quoting variables": "Ok, I found a way to do it, all you need to do is evaluate command with bash\ndocker exec -it <container id> bash -c 'echo something-${CLI}'\nreturns something-/usr/local/bin/myprogram\nIf the CLI environment variable is not already set in the container, you can also pass it in such as:\ndocker exec -it -e CLI=/usr/local/bin/myprogram <container id> bash -c 'echo something-${CLI}'\nSee the help file:\n docker exec --help\n\n Usage: docker exec [OPTIONS] CONTAINER COMMAND [ARG...]\n\n Run a command in a running container\n\nOptions:\n-d, --detach               Detached mode: run command in the background\n-e, --env list             Set environment variables\n....",
    "Docker build ADD vs RUN curl": "ADD is executed in docker host.\nThe ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the image at the path <dest>.\nRUN is executed inside your container.\nThe RUN instruction will execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the Dockerfile.\nSpecifically the command RUN curl -o file.txt http://X.X.X.X/path/to/file/file.txt executes curl that must have already been installed in the image we are using. If the curl command has not been installed (and is not present in the base image) the entire RUN command fails. Instead the command ADD url can be performed even without having installed curl (or analogues) inside the container just because it is executed by the host (it uses the Go libraries with which it is written docker) during the creation of our image.\nIs http://X.X.X.X/path/to/file/file.txt accessible outside of your docker container?\nEdit: as confirmed by the author of the question:\nMy docker host lives behind a firewall that has a proxy set in the /etc/default/docker file. So while I wanted to grab a file internal to the network I'm on, the proxy caused it to look outside the network.",
    "Docker build error, archive/tar: sockets not supported": "It looks like your Dockerfile is probably in /home/shim/?\nWhen you do docker build ., docker will tar up the contents of the current directory and send it to the docker daemon. It looks like some of the files in /home/shim/.ServiceHub are actually sockets, so this operation fails.\nBest practice is to have the Dockerfile in its own, isolated, directory to avoid stuff like this.\nAlso, I suggest having a read through dockerfile_best-practices, in particular the bit about RUN & apt-get",
    "apt-key command works on shell but fails on Dockerfile": "Solved by adding --no-tty on the apt-key adv command.\nAny idea however why this was happening?",
    "How can I cache Maven dependencies and plugins in a Docker Multi Stage Build Layer?": "I came across the same question. I found out it's due to differences between Maven targets (e.g. dependency:resolve vs dependency:resolve-plugin). Basically, dependency:resolve is for application libraries, dependency:resolve-plugin is for plugin libraries. Hence, libraries are downloaded in both RUN steps.\ndependency:resolve tells Maven to resolve all dependencies and displays the version. JAVA 9 NOTE: will display the module name when running with Java 9.\ndependency:resolve-plugins tells Maven to resolve plugins and their dependencies.\nhttps://maven.apache.org/plugins/maven-dependency-plugin/index.html\nEven with dependency:resolve-plugins, Maven will not download all required libraries as package is a built-in target and requires additional libraries which dependency:resolve-plugin won't know to resolve in the first RUN. I also tried dependency:go-offline without success.\nOne solution is to run your build targets before and after adding your code to the build image. This will pull all the plugin dependencies into the lower layer allowing them to be re-used.\nApplying this solution to your example above is as follows:\nFROM maven:3-jdk-8 as mvnbuild\nRUN mkdir -p /opt/workspace\nWORKDIR /opt/workspace\nCOPY pom.xml .\nRUN mvn -B -s /usr/share/maven/ref/settings-docker.xml dependency:resolve-plugins dependency:resolve clean package\nCOPY . .\nRUN mvn -B -s /usr/share/maven/ref/settings-docker.xml clean package\n\nFROM openjdk:8-jre-alpine",
    "error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory": "Using openssl as a dependency like\nopenssl = { version = \"0.10.59\", features = [\"vendored\"] }\ndid fix it for me",
    "Replacing character in dockerfile variable - error: missing ':' in substitution": "The ${parameter/pattern/string} syntax is actually a Bash feature (cf. shell parameter expansion), not a POSIX feature.\nAccording to the official documentation, the Dockerfile directives only supports:\n$var\n${var}\n${var:-word} \u2192 if var is not set then word is the result;\n${var:+word} \u2192 if var is set then word is the result, otherwise the empty string\nWorkaround 1\nSo the problem does not have a \"direct\" solution, but if the variable you would like to substitute will be used, in the end, in some shell command (in a RUN, ENTRYPOINT or CMD directive), you could just as well keep the initial value as is (with no substitution), then substitute it later on?\nI mean for example, the following Dockerfile:\nFROM debian\n\nARG ABC_VERSION=1.2.3\nENV SOME_OTHER_VARIABLE=/app/${ABC_VERSION}\n\nWORKDIR /app\n\nRUN /bin/bash -c 'touch \"${SOME_OTHER_VARIABLE//./_}\"'\n\n# RUN touch \"${SOME_OTHER_VARIABLE//./_}\"\n# would raise /bin/sh: 1: Bad substitution\n\nCMD [\"/bin/bash\", \"-c\", \"ls -hal \\\"${SOME_OTHER_VARIABLE//./_}\\\"\"]\nAs an aside:\nI replaced ARG SOME_OTHER_VARIABLE with ENV SOME_OTHER_VARIABLE just to be able to use it from CMD.\nIt can be recalled that ENTRYPOINT and CMD directives should rather be written in exec form \u2212 CMD [\"\u2026\", \"\u2026\"] \u2212 rather in shell form (see e.g. that question: CMD doesn't run after ENTRYPOINT in Dockerfile).\nWorkaround 2\nOr as an alternative workaround, you may want to split your version number in major, minor, patch, to write something like this?\nARG MAJOR=1\nARG MINOR=2\nARG PATCH=3\nARG ABC_VERSION=$MAJOR.$MINOR.$PATCH\nARG SOME_OTHER_VARIABLE=/dir_name/abc_${MAJOR}_${MINOR}_${PATCH}\n\u2026\nA more concise syntax for workaround 1\nFollowing the OP's edit, I guess one concern is the relative verbosity of this line that I mentioned in the \"workaround 1\":\n\u2026\nRUN /bin/bash -c 'touch \"${SOME_OTHER_VARIABLE//./_}\"'\nTo alleviate this, Docker allows one to replace the implied shell (by default sh) with Bash, which does support the shell parameter expansion you are interested in. The key point is the following directive that has to be written before the RUN command (and which was precisely part of the Dockerfile the OP mentioned):\nSHELL [\"/bin/bash\", \"-c\"]\nThus, the Dockerfile becomes:\n\u2026    \nARG ABC_VERSION=1.2.3\n\nSHELL [\"/bin/bash\", \"-c\"]\nRUN touch \"/dir_name/abc_${ABC_VERSION//./_}\" \\\n  && ls -hal \"/dir_name/abc_${ABC_VERSION//./_}\"\nor taking advantage of some temporary environment variable:\n\u2026    \nARG ABC_VERSION=1.2.3\n\nSHELL [\"/bin/bash\", \"-c\"]\nRUN export SOME_OTHER_VARIABLE=\"/dir_name/abc_${ABC_VERSION//./_}\" \\\n  && touch \"$SOME_OTHER_VARIABLE\" \\\n  && ls -hal \"$SOME_OTHER_VARIABLE\"",
    "Specify dockerignore from command line": "2018: No, docker build does not offer an alternative to the .dockerignore file.\nThat is why I usually keep a symbolic link .dockerignore pointing to the actual .dockerignore_official, except for certain case, where I switch the symlink to .dockerignore_module.\nThis is a workaround, but that allows me to version the different version of dockerignore I might need, and choose between the two.\nUpdate April 2019: as mentioned by Alba Mendez in the comments, PR 901 should help:\ndockerfile: add dockerignore override support\nFrontend will first check for <path/to/Dockerfile>.dockerignore and, if it is found, it will be used instead of the root .dockerignore.\nSee moby/buildkit commit b9db1d2.\nIt is in Docker v19.03.0 beta1, and Alba has posted an example here:\nYou need to enable Buildkit mode to use:\n$ export DOCKER_BUILDKIT=1\n\n$ echo \"FROM ubuntu \\n COPY . tmp/\" > Dockerfile\n$ cp Dockerfile Dockerfile2\n$ touch foo bar\n$ echo \"foo\" > .dockerignore\n$ echo \"bar\" > Dockerfile2.dockerignore\n\n$ docker build -t container1 -f Dockerfile .\n$ docker build -t container2 -f Dockerfile2 .\n$ docker run container1 ls tmp\nDockerfile\nDockerfile2\nDockerfile2.dockerignore\nbar\n\n$ docker run container2 ls tmp\nDockerfile\nDockerfile2\nDockerfile2.dockerignore\nfoo\nUpdate August 2019: this is now in Docker 19.03, with the following comment from T\u00f5nis Tiigi:\n#12886 (comment) allows setting a dockerignore file per Dockerfile if the repository contains many. (Note that this was the exact description for the initial issue)\nBuildKit automatically ignores files that are not used, automatically removing the problem where different sets of files needed to ignore each other.\nThe cases where same Dockerfile uses different sets of files for different \"modes\" of build (eg. dev vs prod) can be achieved with multi-stage builds and defining the mode changes with build arguments.\nNote: issue 37129 \"add support for multiple (named) build-contexts\" reports in May 2023 it is now supported with:\ndocker build (23+)/docker buildx build\nCompose\ndocker buildx bake\nSee also \"Build docker image using different directory contexts\"",
    "How to install nvm in a Dockerfile?": "I made the following changes to your Dockerfile to make it work:\nFirst, replace...\nRUN sh /root/.nvm/install.sh;\n...with:\nRUN bash /root/.nvm/install.sh;\nWhy? On Redhat-based systems, /bin/sh is a symlink to /bin/bash. But on Ubuntu, /bin/sh is a symlink to /bin/dash. And this is what happens with dash:\nroot@52d54205a137:/# bash -c '[ 1 == 1 ] && echo yes!'\nyes!\nroot@52d54205a137:/# dash -c '[ 1 == 1 ] && echo yes!'\ndash: 1: [: 1: unexpected operator\nSecond, replace...\nRUN nvm ls-remote;\n...with:\nRUN bash -i -c 'nvm ls-remote';\nWhy? Because, the default .bashrc for a user in Ubuntu (almost at the top) contains:\n# If not running interactively, don't do anything\n[ -z \"$PS1\" ] && return\nAnd the source-ing of nvm's scripts takes place at the bottom. So we need to make sure that bash is invoked interactively by passing the argument -i.\nThird, you could skip the following lines in your Dockerfile:\nRUN export NVM_DIR=\"$HOME/.nvm\";\nRUN echo \"[[ -s $HOME/.nvm/nvm.sh ]] && . $HOME/.nvm/nvm.sh\" >> $HOME/.bashrc;\nWhy? Because bash /root/.nvm/install.sh; will automatically do it for you:\n[fedora@myhost ~]$ sudo docker run --rm -it 2a283d6e2173 tail -2 /root/.bashrc\nexport NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"  # This loads nvm",
    "Docker ADD doesn't extract file": "This is documented:\nIf is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory.\nNotice the emphasis on \"local\".\nThis was actually changed and reverted at some point, so it's unlikely this will ever change again.",
    "Managing Dockerfile dynamically for different tenants in CI/CD pipeline implementation": "Quoting from 12 Factor - Config\nAn app\u2019s config is everything that is likely to vary between deploys (staging, production, developer environments, etc). This includes:\nResource handles to the database, Memcached, and other backing services\nCredentials to external services such as Amazon S3 or Twitter\nPer-deploy values such as the canonical hostname for the deploy\nYou should not build separate docker images for each tenant as the binary should be the same and any runtime configurations should be injected through the environment.\nThere are different options to inject runtime configuration\nEnvironment variables\nInstead of hardcoding the profile in the entrypoint add a environment variable\nENTRYPOINT [\"java\", \"-jar\", \"-Dspring.profiles.active=$TENANT_PROFILE\" , \"TestProject.war\"] \nThen inject the environment variable from the kubernetes deployment configuration Refer https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/\nMount the profile configuration as a config and refer it\nYour entrypoint will look like\nENTRYPOINT [\"java\", \"-jar\", --spring.config.location=\"file:/path/to/tenantconfig.yaml\" , \"TestProject.war\"]  Then mount the required config file as a kubernetes config.\nEither way externalize the runtime configuration from the docker image and inject it through the deployment configuration as a environment variable or a config.",
    "How to install ODBC Driver 17 in a docker image based on windows server core?": "So I ran the MSI manually in the container with logging enabled. It turns out that the failure was occurring due to missing VC++ redistributable.\nSo, I updated the Dockerfile by adding a line to copy and install vc_redist.x64.exe which fixed the issue for me.\nSnippet from the Dockerfile that solved the problem for me.\nFROM mcr.microsoft.com/dotnet/framework/aspnet:4.7.2\nCOPY vc_redist.x64.exe c:/ \\\n     msodbcsql_17.3.1.1_x64.msi c:/\nRUN c:\\\\vc_redist.x64.exe /install /passive /norestart \nRUN msiexec.exe /i C:\\\\msodbcsql_17.3.1.1_x64.msi /norestart /qn /quiet /passive IACCEPTMSODBCSQLLICENSETERMS=YES \n\n...\nJust posting this answer here in case someone else stumbles upon the same issue.",
    "docker-compose rebuild after each Gemfile update?": "First, about the workaround with docker exec. It's not a good approach to modify container state. What if you need to run one more instance of app container? There will be no changes made by exec. You'll have to install gems there again, or rebuild image. It's not a rare case when you need to run multiple containers. For example, you use docker-compose up to run dev environment, and docker-compose run --rm web bash in the near terminal to run shell in the second app container and use it to run tests, migrations, generators or use rails console without stopping containers launched by docker-compose up.\nNow about the solution. When you run docker-compose run --rm app bundle install, you create the new container, install new gems into it (this operation updates Gemfile.lock, and you see this changes, because your project dir is mounted to container), and exit. Container gets removed because of --rm flag. Changes made in container don't affect image.\nTo avoid image rebuilding on each gem install, you can add a service to store gems. Here is modified docker-compose.yml, based on the one from docs.\nversion: '3'\nservices:\n  db:\n    image: postgres\n  web:\n    build: .\n    command: bash -c \"bundle install && bundle exec rails s -p 3000 -b 0.0.0.0\"\n    volumes:\n      - .:/myapp\n      - bundle_cache:/bundle_cache\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      - db\n    environment:\n      - BUNDLE_PATH=/bundle_cache\n  bundle_cache:\n    image: busybox\n    volumes:\n      - bundle_cache:/bundle_cache\n\nvolumes:\n  bundle_cache:\nWhen you use container that stores gems for all your app containers, you don't need to rebuild image because of adding new gems at all until you run docker-compose down that deletes all your containers (it's really rarely needed) or until you delete bundle_cache container yourself. And of course you don't need to use bundle exec for each container where you want to install new gems. So it's much easier and time-saving.\nThis, however, requires additional bundle install after initial docker-compose build, because on the creation and first mounting /bundle_cache to the container with application it will be empty. But after that your gems will be stored in the separate container, and this storage will be available for each started application container.",
    "Why does Dockerfile RUN print echo options?": "Try running:\nRUN bash -c 'echo -e ...'\nSource",
    "What's the reason for the 42 layer limit in Docker? [closed]": "I'm starting to suspect that there isn't any such a hard limit.\nCreate the following python script, and name it \"makeDockerfile.py\"\nwith open(\"Dockerfile\", \"w\") as file:\n    file.write(\"from alpine:3.8\\n\")\n    for i in range(0, 201):\n        file.write(\"run echo {i}\\n\".format(i=i))\nthen run python makeDockerfile.py && docker build --tag manylayer . && docker run -it manylayer /bin/sh You'll see that you are running a working container with > 200 layers.\n(note, this was tested with linux containers on linux)\nNote that this doesn't mean that this many layers are necessarily SUPPORTED, just that they are possible in some cases.\nIn fact, I've seen containers fail with far fewer than 42 layers, and removing any arbitrary layer seems to fix it. (see https://github.com/docker/for-win/issues/676#issuecomment-462991673 )\nEDIT:\nthaJeztah, maintainer of Docker, has this to say about it:\nThe \"42 layer limit\" on aufs was on older versions of aufs, but should no longer be the case.\nHowever, the 127 layer limit is still there. This is due to a restriction of Linux not accepting more than X arguments to a syscall that's used.\nAlthough this limit can be raised in modern kernels, it's not the default, so if we'd go beyond that maximum, an Image built on one machine may not be usable on another machine.\n( see https://github.com/docker/docker.github.io/issues/8230 )",
    "Many <none> images created after build a docker image": "Docker image is composed of layers:\ndocker history aario/centos\nEach row you see using command above is a separate layer. Also, with time, a number of \"orphaned\" layers will fill up your disk space. You can safely remove them with:\ndocker image prune",
    "Add private key to ssh-agent in docker file": "I spent several days going through the same issue. ssh-keygen -p ensured the passphrase was empty, but I needed to ssh-agent and ssh-add in my Dockerfile to be able to pull from a private repo. Several of my peers told me they were able to make it work; I would copy what they had and still be asked for a passphrase. Finally I came across this issue. After manually inputting in the rsa key line by line and seeing it succeed, I realized it was because I was building the image and passing in the key via a make target, and the Makefile was processing the newlines as whitespaces. Ultimately it was just a matter of updating how the key was being cat as an argument so that it ran as bash instead to preserve the newlines.\nHere was the build command inside my Makefile:\nmake container:    \n    docker build --rm \\\n    --build-arg ssh_prv_key=\"$$(cat ~/.ssh/id_rsa)\" \\\n    --squash -f Dockerfile -t $(DOCKER_IMAGE) .\nI will also note that I needed to include\necho \"StrictHostKeyChecking no\" >> /etc/ssh/ssh_config\nto one of my Dockerfile RUN commands as well",
    "cannot remove 'folder': Device or resource busy": "Another pretty much simple answer is following:\n1. Close all your terminal windows (bash, shell, etc...)\n2. Start a new terminal\n3. Execute your command again e.g.:\nrm -f -r ./folder\n4. done\nHopefully it helps others!",
    "How to name docker-compose files": "According to the -f documentation:\nIf you don\u2019t provide this flag on the command line, Compose [...] looks for a docker-compose.yml and a docker-compose.override.yml file.\nIf you don't want to use the -f flag you can use the default name docker-compose.yml and override it with docker-compose.override.yml.\nHowever, if you use -f, since you'll provide the filename, you can use whatever you want.\nI think a good way to name them depending on the environment could be docker-compose.{env}.yml and place them at the top level directory:\ndocker-compose.prod.yml\ndocker-compose.dev.yml\ndocker-compose.test.yml\ndocker-compose.staging.yml\nAnd you can use the default docker-compose.yml to define the base configuration that is common to all environments.",
    "Dockerfile ADD tar.gz does not extract on ubuntu VM with Docker": "This is a known issue with 17.06 and patched in 17.06.1. The documented behavior is to download the tgz but not unpack it when pulling from a remote URL. Automatically unpacking the tgz was an unexpected change in behavior in 17.06 that they reverted back to only downloading the tgz in 17.06.1.\nRelease notes for 17.06 (see the note at the top): https://github.com/docker/docker-ce/releases/tag/v17.06.0-ce\nRelease notes for 17.06.01: https://github.com/docker/docker-ce/releases/tag/v17.06.1-ce\nIssue: https://github.com/moby/moby/issues/33849\nPR of Fix: https://github.com/docker/docker-ce/pull/89\nEdit, the minimize the number of layers in your image, I'd recommend doing the download, unpack, and cleanup as a single RUN command in your Dockerfile. E.g. here are two different Dockerfiles:\n$ cat df.tgz-add\nFROM busybox:latest\nENV GO_VERSION 1.8\nWORKDIR /tmp\n\nADD https://storage.googleapis.com/golang/go$GO_VERSION.linux-amd64.tar.gz ./\nRUN tar -xzf go$GO_VERSION.linux-amd64.tar.gz \\\n && rm go$GO_VERSION.linux-amd64.tar.gz\n\nCMD ls -l .\n\n$ cat df.tgz-curl\nFROM busybox:latest\nENV GO_VERSION 1.8\nWORKDIR /tmp\n\nRUN wget -O go$GO_VERSION.linux-amd64.tar.gz https://storage.googleapis.com/golang/go$GO_VERSION.linux-amd64.tar.gz \\\n && tar -xzf go$GO_VERSION.linux-amd64.tar.gz \\\n && rm go$GO_VERSION.linux-amd64.tar.gz\n\nCMD ls -l .\nThe build output is truncated here...\n$ docker build -t test-tgz-add -f df.tgz-add .\n...\n\n$ docker build -t test-tgz-curl -f df.tgz-curl .\n...\nThey run identically:\n$ docker run -it --rm test-tgz-add\ntotal 4\ndrwxr-xr-x   11 root     root          4096 Aug 31 20:27 go\n\n$ docker run -it --rm test-tgz-curl\ntotal 4\ndrwxr-xr-x   11 root     root          4096 Aug 31 20:29 go\nHowever, doing a single RUN to download, build, and cleanup saves you the 80MB of download from your layer history:\n$ docker images | grep test-tgz\ntest-tgz-curl               latest                                     2776133659af        30 seconds ago      269MB\ntest-tgz-add                latest                                     d625455998ff        2 minutes ago       359MB",
    "Docker how to start container with defined nameservers in /etc/resolv.conf": "If you use docker-compose you can simple add your dns-server in docker-compose.yml\n  my-app:\n     build: my-app\n     dns:\n       - 10.20.20.1  # dns server 1\n       - 10.21.21.2  # dns server 2\n     dns_search: ibm-edv.ibmnet.int\nsee https://bitbucket.org/snippets/mountdiablo/9yKxG/docker-compose-reference-yaml-file-with",
    "How to determine the base image of a Docker image?": "For pulled images, I don't think there is a way to find the base image without seeing the actual dockerfile because when you pull an image, image manifest is downloaded only for the leaf layer. So the image id of the non-leaf layers in marked as <missing> in docker history and you wouldn't know the repo tags of those layers.\nIf the image is built on your machine but if you don't have the dockerfile, you can find the base image as follows:\ndocker history prints the image ids of the layers. Then you can get the repo tags using docker inspect. The base image used will usually be the last layer in the output of docker history.\neg:\n$ docker history t:1\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n10b4cce00fb8        3 days ago          /bin/sh -c #(nop)  CMD [\"flask\" \"run\"]          0B\n824987ef6cab        3 days ago          /bin/sh -c #(nop) COPY dir:1973b65388e92428e\u2026   406B\nd4b6f433a5df        3 days ago          /bin/sh -c pip install -r requirements.txt      4.98MB\n8827b3f01d00        3 days ago          /bin/sh -c #(nop) COPY file:98271fcaff00c6ef\u2026   0B\n65b8c98138e6        2 weeks ago         /bin/sh -c apk add --no-cache gcc musl-dev l\u2026   113MB\n01589531f46d        2 weeks ago         /bin/sh -c #(nop)  ENV FLASK_RUN_HOST=0.0.0.0   0B\n6c4640b8027a        2 weeks ago         /bin/sh -c #(nop)  ENV FLASK_APP=app.py         0B\nb4c8fc7f03d6        2 weeks ago         /bin/sh -c #(nop) WORKDIR /code                 0B\n16a54299f91e        2 weeks ago         /bin/sh -c #(nop)  CMD [\"python3\"]              0B\n$ docker inspect 16a54299f91e\n[\n    {\n        \"Id\": \"sha256:16a54299f91ef62cf19d7329645365fff3b7a3bff4dfcd8d62f46d0c9845b9c6\",\n        \"RepoTags\": [\n            \"python:3.7-alpine\"   ---> Base image used in FROM instruction. \nFollowing the output of docker history in reverse order, you can approximately recreate the Dockerfile.\nYou can also use the chenzj/dfimage image which runs a script which does the same to reconstruct the dockerfile.\nalias dfimage=\"docker run -v /var/run/docker.sock:/var/run/docker.sock --rm chenzj/dfimage\"\n\n$ dfimage 10b4cce00fb8\nFROM python:3.7-alpine\nWORKDIR /code\nENV FLASK_APP=app.py\nENV FLASK_RUN_HOST=0.0.0.0\nRUN /bin/sh -c apk add --no-cache gcc musl-dev linux-headers\nCOPY file:98271fcaff00c6efb6e58bd09ca726c29947e0cfe7031a8d98878cc01561fbbf in requirements.txt\nRUN /bin/sh -c pip install -r requirements.txt\nCOPY dir:1973b65388e92428e30f835a67ebc8c7b00ec648fbea0717af6d501af162186b in .\nCMD [\"flask\" \"run\"]\nbash-3.2$",
    "Copy files from Container to host using Dockerfile": "It is probably a bad idea to copy files from the container to the host during build. You should seriously consider your use case.\nHowever, it can be done and I will share with you a procedure because it is an opportunity for me to show off my Docker knowledge - not because I think you should do this. There are other ways to do this. My way is not better or worse - they are all kludges.\nModify your dockerd configuration as explained in https://success.docker.com/article/how-do-i-enable-the-remote-api-for-dockerd. Basically add -H tcp://0.0.0.0:2376. This is a very risky procedure b/c it opens you open to be rooted by anyone on your network. There are ways to mitigate this risk with authentication, but really the best way is to JUST DON'T DO IT.\nModify your Dockerfile:\nAdd a ARG DOCKER_HOST before the RUN blocks.\nIn the run blocks:\nInstall docker.\nAdd `export DOCKER_HOST=${DOCKER_HOST}.\nAdd docker container run --mount type=bind,source=/,destination=/srv/host alpine:3.4 ...\nDetermine the IP address of your host computer. Let us assume it is 10.10.20.100.\nModify your build command by adding --build-arg DOCKER_HOST=10.10.20.100.\nIn step 2.2.3 you have rooted the host computer and you can do whatever you want - including writing to any file.\nThis is a dumb idea, but it shows that since you can run docker from within a build, there really is not anything you can not do from inside a build. If you want to run a gui app from inside a build you can do it.",
    "Passing docker runtime environment variables in docker image": "I want to override the default environment variables being set below from whatever is passed in the docker run command mentioned in the end\nThat means overriding an image file (/usr/local/tomcat/conf/test.properties) when running the image as a container (docker run), not building the image (docker build and its --build-args option and its ARG Dockerfile entry).\nThat means you create locally a script file which:\nmodifies /usr/local/tomcat/conf/test.properties\ncalls catalina.sh run $@ (see also \"Store Bash script arguments $@ in a variable\" from \"Accessing bash command line args $@ vs $*\")\nThat is:\nmyscript.sh\n\n#!/bin/sh\necho dummy_url=$dummy_url >> /usr/local/tomcat/conf/test.properties\necho database=$database >> /usr/local/tomcat/conf/test.properties\nargs=(\"$@\")\ncatalina.sh run \"${args[@]}\"\nYou would modify your Dockerfile to COPY that script and call it:\nCOPY myscript.sh /usr/local/\n...\nENTRYPOINT [\"/usr/local/myscript.sh\"]\nThen, and only then, the -e options of docker run would work.",
    "Development dependencies in Dockerfile or separate Dockerfiles for production and testing": "No, you don't need to have different Dockerfiles and in fact you should avoid that.\nThe goal of docker is to ship your app in an immutable, well tested artifact (docker images) which is identical for production and test and even dev.\nWhy? Because if you build different artifacts for test and production how can you guarantee what you have already tested is working in production too? you can't because they are two different things.\nGiven all that, if by test you mean unit tests, then you can mount your source code inside docker container and run tests without building any docker images. And that's fine. Remember you can build image for tests but that terribly slow and makes development quiet difficult and slow which is not good at all. Then if your test passed you can build you app container safely.\nBut if you mean acceptance test that actually needs to run against your running application then you should create one image for your app (only one) and run tests in another container (mount test source code for example) and run tests against that container. This obviously means what your build for your app is different for npm installs for your tests.\nI hope this gives you some over view.",
    "docker run script which exports env variables": "There's no way to export a variable from a script to a child image. As a general rule, environment variables travel down, never up to a parent.\nENV will persist in the build environment and to child images and containers.\nDockerfile\nFROM busybox\nENV PLATFORM_HOME test\nRUN echo $PLATFORM_HOME\nDockerfile.child\nFROM me/platform\nRUN echo $PLATFORM_HOME\nCMD [\"sh\", \"-c\", \"echo $PLATFORM_HOME\"]\nBuild the parent\ndocker build -t me/platform .\nThen build the child:\n\u2192 docker build -f Dockerfile.child -t me/platform-test  .\nSending build context to Docker daemon  3.072kB\nStep 1/3 : FROM me/platform\n ---> 539b52190af4\nStep 2/3 : RUN echo $PLATFORM_HOME\n ---> Using cache\n ---> 40e0bfa872ed\nStep 3/3 : CMD sh -c echo $PLATFORM_HOME\n ---> Using cache\n ---> 0c0e842f99fd\nSuccessfully built 0c0e842f99fd\nSuccessfully tagged me/platform-test:latest\nThen run\n\u2192 docker run --rm me/platform-test\ntest",
    "getting error /bin/sh: 1: source: not found [duplicate]": "From Docker docs:\nThe default shell for the shell form can be changed using the SHELL command.\nIn the shell form you can use a \\ (backslash) to continue a single RUN instruction onto the next line. For example, consider these two lines: RUN /bin/bash -c 'source $HOME/.bashrc ;\\ echo $HOME' Together they are equivalent to this single line: RUN /bin/bash -c 'source $HOME/.bashrc ; echo $HOME'\nNote: To use a different shell, other than \u2018/bin/sh\u2019, use the exec form passing in the desired shell. For example, RUN [\"/bin/bash\", \"-c\", \"echo hello\"]\nYou could try:\nRUN curl   https://raw.githubusercontent.com/creationix/nvm/v0.25.0/install.sh | bash\n# RUN source ~/.profile\nRUN [\"/bin/bash\", \"-c\", \"source ~/.profile\"]",
    "Why dockerize a service or application when you could install it? [closed]": "PROS:\nQuick local environment set up for your team - if you have all your services containerized. It will be a quick environment set up for your development team.\nHelps Avoid the \"It works on mine, but doesn't work on yours problem\" - a lot of our development issue usually stems from development environment setup. If you have your services containerized, a big chunk of this gets offloaded somewhere else.\nEasier deployments - while we all have different processes for deploying code, it goes to tell that having them containerized makes thing a hell lot easier.\nBetter Version Control - as you already know, can be tagged, which helps in VERSION CONTROL.\nEasier Rollbacks - since you have things version controlled, it goes to say that it is easier to rollback your code. Sometimes, by just simply pointing to your previously working version.\nEasy Multi-environment Setup - as most development teams do, we set up a local, integration, staging and production environment. This is done easier when services are containerized, and, most of the times, with just a switch of ENVIRONMENT VARIABLES.\nCommunity Support - we have a strong community of software engineers who continuously contribute great images that can be reused for developing great software. You can leverage that support. Why re-invent the wheel, right?\nMany more.. but there's a lot of great blogs out there you can read that from. =)\nCONS: I don't really see much cons with it but here's one I can think of.\nLearning Curve - yes, it does have some learning curve. But from what I have seen from my junior engineers, it doesn't take too much time to learn how to set it up. It usually takes you longer when you are figuring out how to containerized.\nSOME CONCERNS:\nData Persistence - some engineers are having concerns with data persistence. You can simply fix this by mounting a volume to your container. If you want to use your own database installation, you can simply switch your HOST, DB_NAME, USERNAME and PASSWORD with the one you have in your localhost:5432 and all should be fine.\nI hope this helps!",
    "Is it possible to to run a target build stage in docker without running all the previous build stages": "Set DOCKER_BUILDKIT=1 environment variable to use buildkit like this:\nDOCKER_BUILDKIT=1 docker build -t build-stage-tag --target build -<<EOF\nFROM alpine as base\nRUN echo \"running BASE commands\"\n\nFROM base AS test\nRUN echo \"running TEST commands\"\n\nFROM base AS build\nRUN echo \"running BUILD commands\"\nEOF\noutput:\n[+] Building 4.4s (7/7) FINISHED\n => [internal] load .dockerignore                                                                                                                               0.5s\n => => transferring context: 2B                                                                                                                                 0.0s\n => [internal] load build definition from Dockerfile                                                                                                            0.3s\n => => transferring dockerfile: 204B                                                                                                                            0.0s\n => [internal] load metadata for docker.io/library/alpine:latest                                                                                                0.0s\n => [base 1/2] FROM docker.io/library/alpine                                                                                                                    0.1s\n => => resolve docker.io/library/alpine:latest                                                                                                                  0.0s\n => [base 2/2] RUN echo \"running BASE commands\"                                                                                                                 1.4s\n => [build 1/1] RUN echo \"running BUILD commands\"                                                                                                               1.5s\n => exporting to image                                                                                                                                          0.7s\n => => exporting layers                                                                                                                                         0.6s\n => => writing image sha256:c6958c8bb64b1c6d5a975d8fa4b68c713ee5b374ba9a9fa00f8a0b9b5b314d5e                                                                    0.0s\n => => naming to docker.io/library/build-stage-tag                                                                                                              0.0s",
    "What does the DOCKER_TLS_VERIFY and DOCKER_CERT_PATH variable do?": "As mentioned in the README:\nBy default, boot2docker runs docker with TLS enabled. It auto-generates certificates and stores them in /home/docker/.docker inside the VM.\nThe boot2docker up command will copy them to ~/.boot2docker/certs on the host machine once the VM has started, and output the correct values for the DOCKER_CERT_PATH and DOCKER_TLS_VERIFY environment variables.\neval \"$(boot2docker shellinit)\" will also set them correctly.\nWe strongly recommend against running Boot2Docker with an unencrypted Docker socket for security reasons, but if you have tools that cannot be easily switched, you can disable it by adding DOCKER_TLS=no to your /var/lib/boot2docker/profile file.\nIn a more dynamic environment, where the boot2docker ip can change, see issue 944.",
    "Docker Container Listening on http://[::]:80": "As they've mentioned it seems your container is running on port 80. So for whatever reason that's the port being exposed. Maybe the EXPOSE $PORT is not returning 8081 as you expect?\nWhen you run the container, unless you specify where to map it, it will only be available at the container's IP at the exposed port (80 in your case). Find out this container Ip easily by running docker inspect <container_id>\nTest your image by doing something like docker run -p 8080:80 yourimage. You'll see that in addition to the port 80 that the image exposes, it is being mapped to your local port 8080 so that http://localhost:8080 should be reachable.\nSee this in case it helps you",
    "Docker build sometimes fail on \"file not found or excluded by .dockerignore\" for a nested ignored file": "Clean your system.\ndocker system prune -a\nNote this will remove also all images not used in a container.",
    "Sending build context to Docker daemon (it doesn't stop)": "mv your Dockerfile to an empty folder, and build it. When build a docker image, docker will \"use\" all files in current folder as its \"context\". You can also create a .dockerignore file to exclude files or directories like .gitignore.",
    "How to use the official docker elasticsearch container?": "I recommend using docker-compose (which makes lot of things much easier) with following configuration.\nConfiguration (for development)\nConfiguration starts 3 services: elastic itself and extra utilities for development like kibana and head plugin (these could be omitted, if you don't need them).\nIn the same directory you will need three files:\ndocker-compose.yml\nelasticsearch.yml\nkibana.yml\nWith following contents:\ndocker-compose.yml\nversion: '2'\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:5.4.0\n    container_name: elasticsearch_540\n    environment:\n      - http.host=0.0.0.0\n      - transport.host=0.0.0.0\n      - \"ES_JAVA_OPTS=-Xms1g -Xmx1g\"\n    volumes:\n      - esdata:/usr/share/elasticsearch/data\n      - ./elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n    ports:\n      - 9200:9200\n      - 9300:9300\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n      nofile:\n        soft: 65536\n        hard: 65536\n    mem_limit: 2g\n    cap_add:\n      - IPC_LOCK\n  kibana:\n    image: docker.elastic.co/kibana/kibana:5.4.0\n    container_name: kibana_540\n    environment:\n      - SERVER_HOST=0.0.0.0\n    volumes:\n      - ./kibana.yml:/usr/share/kibana/config/kibana.yml\n    ports:\n      - 5601:5601\n  headPlugin:\n    image: mobz/elasticsearch-head:5\n    container_name: head_540\n    ports:\n      - 9100:9100\n\nvolumes:\n  esdata:\n    driver: local\nelasticsearch.yml\ncluster.name: \"chimeo-docker-cluster\"\nnode.name: \"chimeo-docker-single-node\"\nnetwork.host: 0.0.0.0\n\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\nhttp.cors.allow-headers: \"Authorization\"\nkibana.yml\nserver.name: kibana\nserver.host: \"0\"\nelasticsearch.url: http://elasticsearch:9200\nelasticsearch.username: elastic\nelasticsearch.password: changeme\nxpack.monitoring.ui.container.elasticsearch.enabled: true\nRunning\nWith above three files in the same directory and that directory set as current working directory you do (could require sudo, depends how you have your docker-compose set up):\ndocker-compose up\nIt will start up and you will see logs from three different services: elasticsearch_540, kibana_540 and head_540.\nAfter initial start up you will have your elastic cluster available for http under 9200 and for tcp under 9300. Validate with following curl if the cluster started up:\ncurl -u elastic:changeme http://localhost:9200/_cat/health \nThen you can view and play with your cluster using either kibana (with credentials elastic / changeme):\nhttp://localhost:5601/\nor head plugin:\nhttp://localhost:9100/?base_uri=http://localhost:9200&auth_user=elastic&auth_password=changeme",
    "docker build error Gem::Ext::BuildError: ERROR: Failed to build gem native extension for mimemagic-0.3.9": "Modify the Dockerfile to install the shared-mime-info package. E.g. on Debian-based images:\nRUN apt-get update && apt-get install -y shared-mime-info\nIf it still won't work, then you may need to update the mimemagic gem. On your host, update mimemagic in the Rails app's Gemfile/Gemfile.lock. You may need to install shared-mime-info first: If the host is macOS, you may need to run brew install shared-mime-info; if the host is Ubuntu, you may need to run apt-get install shared-mime-info. Then run\nbundle update mimemagic\nIf your Dockerfile downloads the Rails app from a repo, push your changes to that repo first. Or, for testing, modify the Dockerfile to copy in the Rails app from the host instead.",
    "Using Docker for windows to volume-mount a windows drive into a Linux container": "If you're just trying to mount a windows path to a Linux based container, here's an example using the basic docker run command, and a Docker Compose example as well:\ndocker run -d --name qbittorrent -v '/mnt/f/Fetched Media/Unsorted:/downloads' -v '/mnt/f/Fetched Media/Blackhole:/blackhole' linuxserver/qbittorrent\nThis example shares the f:\\Fetched Media\\Unsorted and f:\\Fetched Media\\Blackhole folders on the Windows host to the container; and within the Linux container you'd see the files from those Windows folders in their respective Linux paths shown to the right of the colon(s).\ni.e. the f:\\Fetched Media\\Unsorted folder will be in the /downloads folder in the Linux container.\n*First though, make sure you've shared those Windows folders within the Docker Desktop settings area in the GUI.\nUpdate for WSL(2):\nYou don't need to specifically share the Windows folder paths; that's only needed when not using WSL.\nUpdate:\nThis seems to be a popular answer, so I thought I'd also include a Docker Compose version of the above example, for the sake of thoroughness (includes how to set a path as read-write (rw), or read-only (ro)):\nqbittorrent:\n  image: 'linuxserver/qbittorrent:latest'\n  volumes:\n    - '/mnt/f/Fetched Media/Unsorted:/downloads:rw'\n    - '/mnt/f/Fetched Media/Blackhole:/blackhole:rw'\n    - '/mnt/e/Logs/qbittorrent:/config/logs:rw'\n    - '/opt/some-local-folder/you-want/read-only:/some-folder-inside-container:ro'",
    "Cannot connect to MongoDB via node.js in Docker": "Try:\nmongodb.MongoClient.connect('mongodb://mongo:27017', ... );\nChange your docker-compose.yml:\nversion: \"2\"\n\nservices:\n\n  web:\n    build: .\n    volumes:\n      - ./:/app\n    ports:\n      - \"3000:3000\"\n    links:\n      - mongo\n\n  mongo:\n    image: mongo\n    ports:\n      - \"27017:27017\"\nAnd use some docker compose commands:\ndocker-compose down\ndocker-compose build\ndocker-compose up -d mongo\ndocker-compose up web",
    "Docker Conditional build image": "Just to put this in right context, it is now (since May 2017) possible to achieve this with pure docker since 17.05 (https://github.com/moby/moby/pull/31352)\nDockerfile should look like (yes, commands in this order):\nARG APP_VERSION\nARG GIT_VERSION\nFROM app:$APP_VERSION-$GIT_VERSION\nThen build is invoked with\ndocker build --build-arg APP_VERSION=1 --build-arg GIT_VERSION=c351dae2 .\nDocker will try to base the build on image app:1-c351dae2\nHelped me immensely to reduce logic around building images.",
    "Access env file variables in docker-compose file": "To get this right it is important to correctly understand the differences between the environment and env_file properties and the .env file:\nenvironment and env_file let you specify environment variables to be set in the container:\nwith environment you can specify the variables explicitly in the docker-compose.yml\nwith env_file you implicitly import all variables from that file\nmixing both on the same service is bad practice since it will easily lead to unexpected behavior\nthe .env file is loaded by docker-compose into the environment of docker-compose itself where it can be used\nto alter the behavior of docker-compose with respective CLI environment variables\nfor variable substitution in the docker-compose.yml\nSo your current configuration does not do what you probably think it does:\nenv_file: project/myproject/.env\nWill load that .env file into the environment of the container and not of docker-compose. Therefor Database_User won't be set for variable substitution:\nenvironment:\n  - POSTGRES_USER=${Database_User}\nThe solution here: remove env_file from your docker-compose.yml and place .env in the same directory you are starting docker-compose from. Alternatively you can specify an .env file by path with the --env-file flag:\ndocker-compose --env-file project/myproject/.env up",
    "Docker [for mac] file system became read-only which breaks almost all features of docker": "Go to your docker for mac Icon in the top right, click on it and then click Restart. After that Docker works as expected.\nThis seems to be an temporary issue since I cannot reproduce it after restarting docker. My guess is that I had an network communication breakdown while docker tried to download and install the packages in the Dockerfile.",
    "Multiple Dockerfiles failure at building images in docker-compose": "TypeError: You must specify a directory to build in path\nWhen you add build: ./Dockerfile-headless-chrome to your docker-compose.yml, you are setting the context folder, and not the Dockerfile file. The attribute you want is build -> dockerfile:\n  services:\n    headless-chrome:\n      build:\n        context: .\n        dockerfile: Dockerfile-alternate\n    dev-server:\n      build: .\nFor dev-server I am not setting anything because it will use the default (context: . and dockerfile: Dockerfile). Actually, you also don't need the context: . at the headless-chrome, because it is the default.\nDocker Compose reference - build\nEdited - Part 2\nThere is also a problem at the command. For it to be automated, you have to change your /bin/bash.\nCreate a file for all your tests (entrypoint.sh):\nnpm run selenium-docker\nnpm run uat\nAdd this file to your image (ADD) and run it on start the container (CMD).\n# Dockerfile\n\n... your commands\n\nADD entrypoint.sh\nCMD bash entrypoint.sh\nAnd it is ready. Build again docker-compose build (because you edited the Dockerfile) and run docker-compose up",
    "Build docker image fail : Exiting on user command": "Simply add -y to yum.\nExample:\nRUN yum -y install git ...",
    "Docker how to make python 3.8 as default": "Replacing the system python in this way is usually not a good idea (as it can break operating-system-level programs which depend on those executables) -- I go over that a little bit in this video I made \"why not global pip / virtualenv?\"\nA better way is to create a prefix and put that on the PATH earlier (this allows system executables to continue to work, but bare python / python3 / etc. will use your other executable)\nin the case of deadsnakes which it seems like you're using, something like this should work:\nFROM ubuntu:bionic\n\nRUN : \\\n    && apt-get update \\\n    && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\\n        software-properties-common \\\n    && add-apt-repository -y ppa:deadsnakes \\\n    && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\\n        python3.8-venv \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && :\n\nRUN python3.8 -m venv /venv\nENV PATH=/venv/bin:$PATH\nthe ENV line is the key here, that puts the virtualenv on the beginning of the path\n$ docker build -t test . \n...\n$ docker run --rm -ti test bash -c 'which python && python --version && which pip && pip --version'\n/venv/bin/python\nPython 3.8.5\n/venv/bin/pip\npip 20.1.1 from /venv/lib/python3.8/site-packages/pip (python 3.8)\ndisclaimer: I'm the maintainer of deadsnakes",
    "How to list all directories and files inside docker container?": "Simply use the exec command.\nNow because you run a windowsservercore based image use powershell command (and not /bin/bash which you can see on many examples for linux based images and which is not installed by default on a windowsservercore based image) so just do:\ndocker exec -it <container_id> powershell\nNow you should get an iteractive terminal and you can list your files with simply doing ls or dir\nBy the way, i found this question :\nExploring Docker container's file system\nIt contains a tons of answers and suggestions, maybe you could find other good solutions there (there is for example a very friendly CLI tool to exploring containers : https://github.com/wagoodman/dive)",
    "docker repository name component must match": "So this regular expression: [a-z0-9]+(?:[._-][a-z0-9]+)* doesn't include any upper case letters. So you should change your image name to devopsclient",
    "How to reduce my java/gradle docker image size?": "I am really confused about your image size. I have typical Spring Boot applications offering a REST service including an embedded servlet container in less than 200MB! It looks like your project dependencies can and should be optimised.\nDocker Image\nThe openjdk:8 (243MB compressed) can be replaced by one with a reduced Alpine unix image like openjdk:8-jdk-alpine (52MB) as a base image but if you don't need compiler capabilities (e.g. don't use JSPs) you may also go for openjdk:8-jre-alpine (42MB) which includes the runtime only, have a look into Docker Hub. I use that for Spring Boot based REST services working great.\nJava Dependencies\nThe Java dependencies needed for compile and runtime have to be included but you may have unused dependencies included:\ncheck your dependencies, are the current compile/runtime dependencies really used or maybe can be removed or moved to test, see Gradle Java Plugin\nsome dependencies have a lot of transitive dependencies (display using gradle dependencies), check out for unnecessary ones and exclude them if unused, see Gradle Dependency Management. Be sure to do integration tests before applying finally, some transitive dependencies are not well documented but may be essential!",
    "Docker how to ADD a file without committing it to an image?": "According to the documentation, if you pass an archive file from the local filesystem (not a URL) to ADD in the Dockerfile (with a destination path, not a path + filename), it will uncompress the file into the directory given.\nIf <src> is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory. Resources from remote URLs are not decompressed. When a directory is copied or unpacked, it has the same behavior as tar -x: the result is the union of:\n1) Whatever existed at the destination path and 2) The contents of the source tree, with conflicts resolved in favor of \"2.\" on a file-by-file basis.\ntry:\nADD /files/apache-stratos.zip /opt/\nand see if the files are there, without further decompression.",
    "How to show a dockerfile of image docker": "The raw output, as described in \"How to generate a Dockerfile from an image?\", would be:\ndocker history --no-trunc <IMAGE_ID>\nBut a more complete output would be from CenturyLinkLabs/dockerfile-from-image\ndocker run -v /var/run/docker.sock:/var/run/docker.sock \\\n centurylink/dockerfile-from-image <IMAGE_TAG_OR_ID>\nNote there were limitations.\nIn 2020, as illustrated here:\ndocker run -v /var/run/docker.sock:/var/run/docker.sock --rm alpine/dfimage \\\n  -sV=1.36 <IMAGE_TAG_OR_ID>",
    "How to make Postgres (Docker) accessible for any IP remotely?": "You have to create postgresql.conf whith parameters, and set listen_addresses = '*'\nattach when starting your container.\ndocker run -p 5432:5432 -e POSTGRES_PASSWORD=123456789 \\\n -d postgres:9.3.6 \\\n -c config_file=/path/to/postgresql.conf\nnext solution. Create Dockerfile and add follows:\nFROM ubuntu\n\n# Add the PostgreSQL PGP key to verify their Debian packages.\n# It should be the same key as https://www.postgresql.org/media/keys/ACCC4CF8.asc\nRUN apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys B97B0AFCAA1A47F044F244A07FCC7D46ACCC4CF8\n\n# Add PostgreSQL's repository. It contains the most recent stable release\n#     of PostgreSQL, ``9.3``.\nRUN echo \"deb http://apt.postgresql.org/pub/repos/apt/ precise-pgdg main\" > /etc/apt/sources.list.d/pgdg.list\n\n# Install ``python-software-properties``, ``software-properties-common`` and PostgreSQL 9.3\n#  There are some warnings (in red) that show up during the build. You can hide\n#  them by prefixing each apt-get statement with DEBIAN_FRONTEND=noninteractive\nRUN apt-get update && apt-get install -y python-software-properties software-properties-common postgresql-9.3 postgresql-client-9.3 postgresql-contrib-9.3\n\n# Note: The official Debian and Ubuntu images automatically ``apt-get clean``\n# after each ``apt-get``\n\n# Run the rest of the commands as the ``postgres`` user created by the ``postgres-9.3`` package when it was ``apt-get installed``\nUSER postgres\n\n# Create a PostgreSQL role named ``docker`` with ``docker`` as the password and\n# then create a database `docker` owned by the ``docker`` role.\n# Note: here we use ``&&\\`` to run commands one after the other - the ``\\``\n#       allows the RUN command to span multiple lines.\nRUN    /etc/init.d/postgresql start &&\\\n    psql --command \"CREATE USER docker WITH SUPERUSER PASSWORD 'docker';\" &&\\\n    createdb -O docker docker\n\n# Adjust PostgreSQL configuration so that remote connections to the\n# database are possible.\nRUN echo \"host all  all    0.0.0.0/0  md5\" >> /etc/postgresql/9.3/main/pg_hba.conf\n\n# And add ``listen_addresses`` to ``/etc/postgresql/9.3/main/postgresql.conf``\nRUN echo \"listen_addresses='*'\" >> /etc/postgresql/9.3/main/postgresql.conf\n\n# Expose the PostgreSQL port\nEXPOSE 5432\n\n# Add VOLUMEs to allow backup of config, logs and databases\nVOLUME  [\"/etc/postgresql\", \"/var/log/postgresql\", \"/var/lib/postgresql\"]\n\n# Set the default command to run when starting the container\nCMD [\"/usr/lib/postgresql/9.3/bin/postgres\", \"-D\", \"/var/lib/postgresql/9.3/main\", \"-c\", \"config_file=/etc/postgresql/9.3/main/postgresql.conf\"]\nBuild an image from the Dockerfile assign it a name.\n$ docker build -t my_postgresql .\nRun the PostgreSQL server container (in the foreground):\n$ docker run --rm -P --name pg_test my_postgresql\nConnecting from your host system $ docker ps\nCONTAINER ID        IMAGE                  COMMAND                CREATED             STATUS              PORTS                                      NAMES\n5e24362f27f6        my_postgresql:latest   /usr/lib/postgresql/   About an hour ago   Up About an hour    0.0.0.0:49153->5432/tcp                    pg_test\n\n$ psql -h localhost -p 49153 -d docker -U docker --password",
    "Difference between pushing a docker image and installing helm image": "Amount to effort\nTo deploy a service on Kubernetes using docker image you need to manually create various configuration files like deployment.yaml. Such files keep on increasing as you have more and more services added to your environment.\nIn the Helm chart, we can provide a list of all services that we wish to deploy in requirements.yaml file and Helm will ensure that all those services get deployed to the target environment using deployment.yaml, service.yaml & values.yaml files.\nConfigurations to maintain\nAlso adding configuration like routing, config maps, secrets, etc becomes manually and requires configuration over-&-above your service deployment.\nFor example, if you want to add an Nginx proxy to your environment, you need to separately deploy it using the Nginx image and all the proxy configurations for your functional services.\nBut with Helm charts, this can be achieved by configuring just one file within your Helm chart: ingress.yaml\nFlexibility\nUsing docker images, we need to provide configurations for each environment where we want to deploy our services.\nBut using the Helm chart, we can just override the properties of the existing helm chart using the environment-specific values.yaml file. This becomes even easier using tools like ArgoCD.\nCode-Snippet:\nBelow is one example of deployment.yaml file that we need to create if we want to deploy one service using docker-image.\nInline, I have also described how you could alternatively populate a generic deployment.yaml template in Helm repository using different files like requirements.yaml and Values.yaml\ndeployment.yaml for one service\ncrazy-project/charts/accounts/templates/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: accounts\nspec:\n  replicas: 1\n  selector:\n     matchLabels:\n       app.kubernetes.io/name: accounts\n       app.kubernetes.io/instance: crazy-project\n  template:\n     metadata:\n       labels:\n         app.kubernetes.io/name: accounts\n         app.kubernetes.io/instance: crazy-project\n     spec:\n       serviceAccountName: default\n       automountServiceAccountToken: true\n       imagePullSecrets:\n         - name: regcred\n       containers:\n       - image: \"image.registry.host/.../accounts:1.2144.0\"   <-- This version can be fetched from 'requirements.yaml'\n         name: accounts\n         env:      <-- All the environment variables can be fetched from 'Values.yaml'\n         - name: CLUSTERNAME\n           value: \"com.company.cloud\"\n         - name: DB_URI\n           value: \"mongodb://connection-string&replicaSet=rs1\"\n         imagePullPolicy: IfNotPresent\n         volumeMounts:\n         - name: secretfiles\n           mountPath: \"/etc/secretFromfiles\"\n           readOnly: true\n         - name: secret-files\n           mountPath: \"/etc/secretFromfiles\"\n           readOnly: true\n         ports:\n         - name: HTTP\n           containerPort: 9586\n           protocol: TCP\n         resources:\n           requests:\n             memory: 450Mi\n             cpu: 250m\n           limits:\n             memory: 800Mi\n             cpu: 1\n       volumes:\n       - name: secretFromfiles\n         secret:\n           secretName: secret-from-files\n       - name: secretFromValue\n         secret:\n           secretName: secret-data-vault\n           optional: true\n           items:...\nYour deployment.yaml in Helm chart could be a generic template(code-snippet below) where the details are populated using values.yaml file.\nenv:\n{{- range $key, $value := .Values.global.envVariable.common }}\n    - name: {{ $key }}\n      value: {{ $value  | quote }}\n    {{- end }}\nYour Values.yaml would look like this:\naccounts:\n  imagePullSecrets:\n    - name: regcred\n  envVariable:\n    service:\n      vars:\n        spring_data_mongodb_database: accounts_db\n        spring_product_name: crazy-project\n        ...\nYour requirements.yaml would be like below. 'dependencies' are the services that you wish to deploy.\ndependencies:\n  - name: accounts\n    repository: \"<your repo>\"\n    version: \"= 1.2144.0\"\n  - name: rollover\n    repository: \"<your repo>\"\n    version: \"= 1.2140.0\"\nThe following diagram will help you visualize what I have mentioned above:",
    "How to find out which Linux is installed inside docker image?": "A docker image doesn't need an OS. There's a possibility of extending the scratch image which is purposely empty and the container may only contain one binary or some volume.\nHaving an entire OS is possible but also misleading: The host shares its kernel with the container. (This is not a virtual machine.)\nThat means that no matter what \"OS\" you are running, the same kernel in the container is found:\nBoth:\ndocker run --rm -it python:3.6 uname -a\ndocker run --rm -it python:3.6-alpine uname -a\nwill report the same kernel of your host machine.\nSo you have to look into different ways:\ndocker run --rm -it python:3.6 cat /etc/os-release\nor\nlsb_release -sirc\nor for Cent OS:\ncat /etc/issue\nInstead of scratch, a lot of images are also alpine-based, to avoid the size overhead. An Ubuntu base image can easily have 500MB fingerprint whereas alpine uses around 5MB; so I'd rather check for that as well.\nAlso avoid the trap of manually installing everything onto one Ubuntu image inside one big Dockerfile. Docker works best if each service is its own container that you link together. (For that, check out docker-compose.)\nIn the end, you, as a user, shouldn't care about the OS of an image, but rather its size. Only as a developer of the Dockerfile is it relevant to know the OS and that you'll find out either by looking into the Dockerfile the image was built (if it's on docker hub you can read it there).\nYou basically have to look what was used to create your image and use the appropriate tools for the job. (Debian-based images use apt-get, alpine uses apk, and Fedora uses yum.)",
    "Docker Container with Apache Spark in standalone cluster mode": "UPDATED ANSWER (for spark 2.4.0):\nTo start spark master on foreground, just set the ENV variable SPARK_NO_DAEMONIZE=true on your environment before running ./start-master.sh\nand you are good to go.\nfor more info, check $SPARK_HOME/sbin/spark-daemon.sh\n# Runs a Spark command as a daemon.\n#\n# Environment Variables\n#\n#   SPARK_CONF_DIR  Alternate conf dir. Default is ${SPARK_HOME}/conf.\n#   SPARK_LOG_DIR   Where log files are stored. ${SPARK_HOME}/logs by default.\n#   SPARK_MASTER    host:path where spark code should be rsync'd from\n#   SPARK_PID_DIR   The pid files are stored. /tmp by default.\n#   SPARK_IDENT_STRING   A string representing this instance of spark. $USER by default\n#   SPARK_NICENESS The scheduling priority for daemons. Defaults to 0.\n#   SPARK_NO_DAEMONIZE   If set, will run the proposed command in the foreground. It will not output a PID file.\n##",
    "Docker Ubuntu 18.04 unable to install msodbcsql17 SQL Server ODBC Driver 17": "I could get it working. Below is the updated Docker file snippet\nFROM ubuntu:18.04\n\nRUN apt update -y  &&  apt upgrade -y && apt-get update \nRUN apt install -y curl python3.7 git python3-pip openjdk-8-jdk unixodbc-dev\n\n# Add SQL Server ODBC Driver 17 for Ubuntu 18.04\nRUN curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\nRUN curl https://packages.microsoft.com/config/ubuntu/18.04/prod.list > /etc/apt/sources.list.d/mssql-release.list\nRUN apt-get update\nRUN ACCEPT_EULA=Y apt-get install -y --allow-unauthenticated msodbcsql17\nRUN ACCEPT_EULA=Y apt-get install -y --allow-unauthenticated mssql-tools\nRUN echo 'export PATH=\"$PATH:/opt/mssql-tools/bin\"' >> ~/.bash_profile\nRUN echo 'export PATH=\"$PATH:/opt/mssql-tools/bin\"' >> ~/.bashrc\n\nCOPY startup.sh /\nRUN chmod +x /startup.sh\nENTRYPOINT [\"sh\",\"/startup.sh\"]",
    "How to Edit Docker Image?": "There are 4 Steps\nStart the image you want to edit: docker run IMAGE\nModify the running image by shelling into it with docker exec -it <container-id> (you can get the container id with docker ps)\nMake any modifications (install new things, make a directory or file)\nIn a new terminal tab/window run docker commit <container-id> my-new-image (substituting in the container id of the container you want to save)\nAn example\n# Run an existing image\ndocker run -dt existing_image \n\n# See that it's running\ndocker ps\n# CONTAINER ID   IMAGE            COMMAND   CREATED              STATUS              \n# c7e6409a22bf   existing-image   \"R\"       6 minutes ago        Up 6 minutes\n\n# Shell into it\ndocker exec -it c7e6409a22bf bash\n\n# Make a new directory for demonstration purposes\n# (note that this is inside the existing image)\nmkdir NEWDIRECTORY\n\n# Open another terminal tab/window, and save the running container you modified\ndocker commit c7e6409a22bf my-new-image\n\n# Inspect to ensure it saved correctly\ndocker image ls\n# REPOSITORY           TAG       IMAGE ID       CREATED         SIZE\n# existing-image       latest    a7dde5d84fe5   7 minutes ago   888MB\n# my-new-image         latest    d57fd15d5a95   2 minutes ago   888MB",
    "Default value with shell expressions in Dockerfile ARG and ENV": "From the documentation:\nThe ${variable_name} syntax also supports a few of the standard bash modifiers as specified below:\n${variable:-word} indicates that if variable is set then the result will be that value. If variable is not set then word will be the result.\n${variable:+word} indicates that if variable is set then word will be the result, otherwise the result is the empty string.\nENV is special docker build command and doesn't support this. What you are looking for is to run Shell commands in ENV. So this won't work.\nPossible solution is to use a bash script\ncuda_version.sh\n#!/bin/bash\nCUDA_FULL=\"${CUDA_VERSION:-8.0.61_375.26}\"\nCUDA_MAJOR=\"$(echo ${CUDA_VERSION:-8.0.61_375.26} | cut -d. -f1)\"\nCUDA_MINOR=\"$(echo ${CUDA_VERSION:-8.0.61_375.26} | cut -d. -f2)\"\nCUDA_MAJMIN=\"$CUDA_MAJOR.$CUDA_MINOR\" \nCUDNN_FULL=\"${CUDNN_VERSION:-7.0.1}\"\nCUDNN_MAJOR=\"$(echo ${CUDNN_VERSION:-7.0.1} | cut -d. -f1)\"\nCUDNN_MINOR=\"$(echo ${CUDNN_VERSION:-7.0.1} | cut -d. -f2)\"\nCUDNN_MAJMIN=\"$CUDNN_MAJOR.$CUDNN_MINOR\"\nAnd change your dockerfile to\nARG CUDA_VERSION=8.0.61_375.26\nARG CUDNN_VERSION=7.0.1\n\nENV CUDA_VERSION=${CUDA_VERSION} CUDNN_VERSION=${CUDNN_VERSION}\nCOPY cuda_version.sh /cuda_version.sh\nRUN bash -c \"source /cuda_version.sh && curl -LO https://.../${CUDNN_FULL}/.../...${CUDA_MAJMIN}...\"\nYou can remove the default values from your shell file as they will always be there from the Dockerfile arguments/environment",
    "/var/run/docker.sock: permission denied while running docker within Python CGI script": "Permission denied on a default install indicates you are trying to access the socket from a user other than root or that is not in the docker group. You should be able to run:\nsudo usermod -a -G docker $username\non your desired $username to add them to the group. You'll need to logout and back in for this to take effect (use newgrp docker in an existing shell, or restart the daemon if this is an external service accessing docker like your cgi scripts).\nNote that doing this effectively gives that user full root access on your host, so do this with care.",
    "Build started failing using Python:3.8 Docker image on apt-get update and install with GPG error: bookworm InRelease is not signed": "Why this happened?\nThe Python docker images have been updated recently to use Debian 12 bookworm version which was released on 10 June 2023 instead of Debian 10 buster.\nSources:\nGitHub > docker-library/python > Commit > add bookworm, remove buster\nWikipedia > Debian version history > Release table\nWhat is the root cause?\nIt is Docker with libseccomp so a newer syscall used in Debian Bookworm packages/libs is being blocked. libseccomp lets you configure allowed syscalls for a process. Docker sets a default seccomp profile for all containers such that only certain syscalls are allowed and everything else is blocked (so, newer syscalls that are not yet known to libseccomp or docker are blocked).\nSource: python:3.9 - Failed run apt update from the last version of the image #837\nPossible Solutions:\nEither\nAdd the following in the Dockerfile:\nRUN mv -i /etc/apt/trusted.gpg.d/debian-archive-*.asc  /root/\nRUN ln -s /usr/share/keyrings/debian-archive-* /etc/apt/trusted.gpg.d/\nOr\nUse any of the bullseye image (e.g., python:3.8-slim-bullseye).\nOr\nUpdate libseccomp and docker on the host running the containers.",
    "how to correctly use system user in docker container": "This sort of error will happen when the uid/gid does not exist in the /etc/passwd or /etc/group file inside the container. There are various ways to work around that. One is to directly map these files from your host into the container with something like:\n$ docker run -it --rm --user=999:998 \\\n  -v /etc/passwd:/etc/passwd:ro -v /etc/group:/etc/group:ro \\\n  my-image:latest bash\nI'm not a fan of that solution since files inside the container filesystem may now have the wrong ownership, leading to potential security holes and errors.\nTypically, the reason people want to change the uid/gid inside the container is because they are mounting files from the host into the container as a host volume and want permissions to be seamless across the two. In that case, my solution is to start the container as root and use an entrypoint that calls a script like:\nif [ -n \"$opt_u\" ]; then\n  OLD_UID=$(getent passwd \"${opt_u}\" | cut -f3 -d:)\n  NEW_UID=$(stat -c \"%u\" \"$1\")\n  if [ \"$OLD_UID\" != \"$NEW_UID\" ]; then\n    echo \"Changing UID of $opt_u from $OLD_UID to $NEW_UID\"\n    usermod -u \"$NEW_UID\" -o \"$opt_u\"\n    if [ -n \"$opt_r\" ]; then\n      find / -xdev -user \"$OLD_UID\" -exec chown -h \"$opt_u\" {} \\;\n    fi\n  fi\nfi\nThe above is from a fix-perms script that I include in my base image. What's happening there is the uid of the user inside the container is compared to the uid of the file or directory that is mounted into the container (as a volume). When those id's do not match, the user inside the container is modified to have the same uid as the volume, and any files inside the container with the old uid are updated. The last step of my entrypoint is to call something like:\nexec gosu app_user \"$@\"\nWhich is a bit like an su command to run the \"CMD\" value as the app_user, but with some exec logic that replaces pid 1 with the \"CMD\" process to better handle signals. I then run it with a command like:\n$ docker run -it --rm --user=0:0 -v /host/vol:/container/vol \\\n  -e RUN_AS app_user --entrypoint /entrypoint.sh \\\n  my-image:latest bash\nHave a look at the base image repo I've linked to, including the example with nginx that shows how these pieces fit together, and avoids the need to run containers in production as root (assuming production has known uid/gid's that can be baked into the image, or that you do not mount host volumes in production).",
    "Docker environmental variables from a file": "Your best options is to use either the -e flag, or the --env-file of the docker run command.\nThe -e flag allows you to specify key/value pairs of env variable,\nfor example:\ndocker run -e ENVIRONMENT=PROD\nYou can use several time the -e flag to define multiple env\nvariables. For example, the docker registry itself is configurable with -e flags, see: https://docs.docker.com/registry/deploying/#running-a-domain-registry\nThe --env-file allow you to specify a file. But each line of the file must be of type VAR=VAL\nFull documentation:\nhttps://docs.docker.com/engine/reference/commandline/run/#set-environment-variables-e-env-env-file",
    "Error while trying to ssh a docker container : System is booting up": "Solution:\nPlease edit your dockerfile like this:\nFROM centos\nRUN yum -y install openssh-server\nRUN useradd remote_user && \\\n    echo remote_user:1234 | chpasswd && \\\n    mkdir /home/remote_user/.ssh && \\\n    chmod 700 /home/remote_user/.ssh\nCOPY remote-key.pub /home/remote_user/.ssh/authorized_keys\nRUN chown remote_user:remote_user -R /home/remote_user/.ssh && \\\n    chmod 600 /home/remote_user/.ssh/authorized_keys\nRUN /usr/bin/ssh-keygen -A\nEXPOSE 22\nRUN rm -rf /run/nologin\nCMD /usr/sbin/sshd -D",
    "Docker RUN fails with \"returned a non-zero code: 6\"": "The exit code 6 means that \"Host public key is unknown. sshpass exits without confirming the new key.\"\nSo either you populate before that the ~/.ssh/known_hostswith the fingerprint of the host, or just ignore the check of the host public key by adding the StrictHostKeyChecking=no option to the scp.\nThe updated line would look like that:\nRUN sshpass -p userPassword scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -r user@server:~/data/* ./",
    "Docker: how to manage development and production settings?": "You could mount your custom nginx.conf into the container in development via e.g. --volume ./nginx/nginx.conf:/etc/nginx/nginx.conf and simply omit this parameter to docker run in production.\nIf using docker-compose, the two options I would recommend are:\nEmploy the limited support for environment variable interpolation and add something like the following under volumes in your container definition: ./nginx/nginx.${APP_ENV}.conf:/etc/nginx/nginx.conf\nUse a separate YAML file for production overrides.",
    "Copying node_modules into a dockerfile vs installing them": "I'd almost always install Node packages from inside the container rather than COPYing them from the host (probably via RUN npm ci if I was using npm).\nIf the host environment doesn't exactly match the container environment, COPYing the host's node_modules directory may not work well (or at all). The most obvious case of this using a MacOS or Windows host with a Linux container, where if there are any C extensions or other binaries they just won't work. It's also conceivable that there would be if the Node versions don't match exactly. Finally, and individual developer might have npm installed an additional package or a different version, and the image would vary based on who's building it.\nAlso consider the approach of using a multi-stage build to have both development and production versions of node_modules; that way you do not include build-only tools like the tsc Typescript compiler in the final image. If you have two different versions of node_modules then you can't COPY a single tree from the host, you must install it in the Dockerfile.\nFROM node AS build\nWORKDIR /app\nCOPY package*.json .\nRUN npm ci\nCOPY . .\nRUN npm install\n\nFROM node\nWORKDIR /app\nCOPY package*.json .\nENV NODE_ENV=production\nRUN npm ci\nCOPY --from=build /app/build /app/build\nCMD [\"node\", \"/app/build/index.js\"]",
    "docker inside docker container": "Update\nThanks to https://stackoverflow.com/a/38016704/372019 I want to show another approach.\nInstead of mounting the host's docker binary, you should copy or install a container specific release of the docker binary. Since you're only using it in a client mode, you won't need to install it as a system service. You still need to mount the Docker socket into the container so that you can easily communicate with the host's Docker engine.\nAssuming that you got a base image with a working Docker binary (e.g. the official docker image), the example now looks like this:\ndocker run\\\n  -v /var/run/docker.sock:/var/run/docker.sock\\\n  docker:1.12 docker info\nWithout actually answering your question I'd suggest you to read Using Docker-in-Docker for your CI or testing environment? Think twice.\nIt explains why running docker-in-docker should be replaced with a setup where Docker containers run as siblings of the \"outer\" or \"base\" container. The article also links to the original https://github.com/jpetazzo/dind project where you can find working examples how to run Docker in Docker - in case you still want to have docker-in-docker.\nAn example how to enable a container to access the host's Docker daemon look like this:\ndocker run\\\n  -v /var/run/docker.sock:/var/run/docker.sock\\\n  -v /usr/bin/docker:/usr/bin/docker\\\n  busybox:latest /usr/bin/docker info",
    "Running Docker pull command in Dockerfile": "You should not pull from a Dockerfile.\nYou simply can start your Dockerfile with:\nFROM docker-oracle-xe-11g\nAnd add in it any Oracle config file which you would need in your own Oracle image.\nThe docker-oracle-xe-11g Dockerfile is already based on ubuntu.\nFROM ubuntu:14.04.1",
    "How to modify the `core_pattern` when building docker image": "It's not possible to have different core_pattern in the host and in the container at the same time, as docker is sharing the kernel with its host.\nHowever, you can run the container in privileged mode and change core_pattern from inside a container during startup/runtime (modify core_pattern in CMD section or execute os command from inside).\nBut keep in mind that this setting will not be automatically restored after the container finished (until you do it programmatically).",
    "Docker WORKDIR - on my machine or the container?": "It is inside the container.\nTaken for the Dockerfile reference site https://docs.docker.com/engine/reference/builder/#workdir\nThe WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile. If the WORKDIR doesn\u2019t exist, it will be created even if it\u2019s not used in any subsequent Dockerfile instruction.\nSo rather than adding RUN cd && ... you could do:\nWORKDIR /path/to/dir\nRUN command",
    "How to append to path in dockerfile or docker run": "To append the containers $PATH try something along these lines in the Dockerfile:\nENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATH\nResources:\nUpdating PATH environment variable permanently in Docker container\nIn a Dockerfile, How to update PATH environment variable?\nhttps://docs.docker.com/engine/reference/builder/#env",
    "Building Dockerfile fails when touching a file after a mkdir": "Looking at https://registry.hub.docker.com/u/library/jenkins/, it seems that /var/jenkins_home is a volume. You can only create files there while the container is running, presumably with a volume mapping like\ndocker run ... -v /your/jenkins/home:/var/jenkins_home ...\nThe docker build process knows nothing about shared volumes.",
    "nobody & nonroot user in distroless images": "There are 2 type of distroless images which can be used in production:\nwith latest tag\nThis image say, gcr.io/distroless/base by default has \"Config.User: 0\" and \"Config.WorkingDir: /\" config in it and if you don't use USER for switching the user to nonroot user which is defined in it or it will start container with root user.\nwith nonroot tag\nThis image say, gcr.io/distroless/base:nonroot by default has \"Config.User: 65532\" and \"Config.WorkingDir: /home/nonroot\" config in it and there is no need to use USER for changing user to non-root user.\nPS: maybe you need to change ownership of copied files in multistage build to nonroot user.\nnobody user\nThe purpose of nobody user is not related to distroless images and it's about Linux itself which described here very well",
    "What happens to a Docker Container when HEALTHCHECK fails": "When running HEALTHCKECKS you can specify:\n--interval=DURATION (default 30s)\n--timeout=DURATION (default 30s)\n--retries=N (default 3)\nAnd the container can have three states:\nstarting \u2013 Initial status when the container is still starting.\nhealthy \u2013 When the command succeeds.\nunhealthy \u2013 When a single run of the HEALTHCHECK takes longer than the specified timeout. When this happens it will run retries and will be declared \"unhealthy\" if it still fails.\nWhen the check fails for a specified number of times in a row, the failed container will:\nstay in \"unhealthy\" state if it is in standalone mode\nrestart if it is in Swarm mode\nOtherwise it will exit with error code 0 which means it is considered \"healthy\".\nI hope it makes things more clear.",
    "microdnf update command installs new packages instead of just updating existing packages": "I had the same or a very similar problem. Found a command-line flag that helped to lower the number of additionally installed packages. If you add install_weak_deps=0, it should help with these additional packages.\nmicrodnf upgrade \\\n  --refresh \\\n  --best \\\n  --nodocs \\\n  --noplugins \\\n  --setopt=install_weak_deps=0",
    "\\Dockerfile: The system cannot find the file specified": "The command docker -t build <my docker file name> . is being misused. It should be:\ndocker build -t <image-name> -f dockerFile .\nwhere dockerFile is the name you gave to the Dockerfile.\nThe -t option specifies the tag or name to give to the Docker image.\nThe -f must be used, if you name the Dockerfile something other than Dockerfile\nThe . specifies the docker build context.",
    "Docker - no such file or directory": "With that sort of corruption, I'd give a full docker wipe a try, rm -rf /var/lib/docker/*. Before doing that, backup any data (volumes), then shutdown docker, and you'll need to pull or rebuild all your images again. If there are still problems with aufs, try changing the filesystem driver, e.g. changing to dockerd -s overlay2 in your service startup.\nIt doesn't hurt to check for common issues, like running out of disk space or old version of the application, first.",
    "Run py.test in a docker container as a service": "I just enabled it on one of my projects recently. I use a multistage build. At present I put tests in the same folder as the source test_*.py. From my experience with this, it doesn't feel natural, I prefer tests to be in its own folder that is excluded by default.\nFROM python:3.7.6 AS build\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip3 install --compile -r requirements.txt && rm -rf /root/.cache\nCOPY src /app\n# TODO precompile\n\n# Build stage test - run tests\nFROM build AS test\nRUN pip3 install pytest pytest-cov && rm -rf /root/.cache\nRUN pytest --doctest-modules \\\n  --junitxml=xunit-reports/xunit-result-all.xml \\\n  --cov \\\n  --cov-report=xml:coverage-reports/coverage.xml \\\n  --cov-report=html:coverage-reports/\n\n# Build stage 3 - Complete the build setting the executable\nFROM build AS final\nCMD [ \"python\", \"./service.py\" ]\nIn order to exclude the test files from coverage. .coveragerc must be present.\n[run]\nomit = test_*\nThe test target runs the required tests and generates coverage and execution reports. These are NOT suitable for Azure DevOps and SonarQube. To make it suitable\nsed -i~ 's#/app#$(Build.SourcesDirectory)/app#' $(Pipeline.Workspace)/b/coverage-reports/coverage.xml\nTo run tests\n#!/usr/bin/env bash\nset -e\nDOCKER_BUILDKIT=1 docker build . --target test --progress plain",
    "How to forward all ports in docker container": "You can expose a range of ports using the -p option, for example:\ndocker run -p 2000-5000:2000-5000 -v /host/:/host appimage\nSee the docker run reference documentation for more details.",
    "Install mysql in dockerfile?": "The problem is that you've never started the database - you need to explicitly start services in most Docker images. But if you want to run two processes in Docker (the DB and your python program), things get a little more complex. You either have to use a process manager like supervisor, or be a bit cleverer in your start-up script.\nTo see what I mean, create the following script, and call it cmd.sh:\n#!/bin/bash\n\nmysqld &\npython main.py\nAdd it to the Dockerfile:\nFROM ubuntu:saucy\n\n# Install required packages\nRUN apt-get update\nRUN DEBIAN_FRONTEND=noninteractive apt-get -y install python\nRUN DEBIAN_FRONTEND=noninteractive apt-get -y install mysql-server python-mysqldb\n\n# Add our python app code to the image\nRUN mkdir -p /app\nADD . /app\nWORKDIR /app\n\n# Set the default command to execute\nCOPY cmd.sh /cmd.sh\nRUN chmod +x /cmd.sh\nCMD cmd.sh\nNow build and try again. (Apologies if this doesn't work, it's off the top of my head and I haven't tested it).\nNote that this is not a good solution; mysql will not be getting signals proxied to it, so probably won't shutdown properly when the container stops. You could fix this by using a process manager like supervisor, but the easiest and best solution is to use separate containers. You can find stock containers for mysql and also for python, which would save you a lot of trouble. To do this:\nTake the mysql installation stuff out of the Dockerfile\nChange localhost in your python code to mysql or whatever you want to call your MySQL container.\nStart a MySQL container with something like docker run -d --name mysql mysql\nStart your container and link to the mysql container e.g: docker run myapp --link mysql:mysql",
    "Make Docker build stop after if RUN fails in multi-stage build": "Your command, run-tests.sh, needs to exit with a non-zero exit code and docker will stop building. In this case, that has happened:\nThe command '/bin/bash -c cd test; ./run-tests.sh' returned a non-zero code: 1\nWhatever you run to call docker build needs to handle that exit code and stop running at that point. Docker's behavior is to give you an exit code to indicate the failure:\n$ cat df.fail\nFROM busybox\nRUN exit 1\nRUN echo still running\n\n$ docker build -f df.fail .\nSending build context to Docker daemon  23.04kB\nStep 1/3 : FROM busybox\n ---> 59788edf1f3e\nStep 2/3 : RUN exit 1\n ---> Running in 70d90fb88d6e\nThe command '/bin/sh -c exit 1' returned a non-zero code: 1\n\n$ echo $?\n1\nFrom the above example, you can see that docker does stop as soon as the command returns a non-zero exit code, it does not run the echo still running line, and there's a non-zero return code from docker build itself that you can handle with whatever you use to run the build.",
    "Docker not updating changes in directory": "In your docker file, you are using\nCOPY . .\nThis mean that, when you build your docker, you copy your current folder to the default folder of your container. Probably /root\nBut this copy isn't executed every time you RUN the container or START it, it's only when you BUILD.\nTo be able to see every change you make in real time without re BUILD, you need to create a volume, wich will be a link between your host and your container. Every content changing on the host or the container will be shared to the other.\nNote that in your dockerfile, declaring a VOLUME won't actually change anything, it's just an information. To actually make a volume you need to add -v /host/path:/container/path in your docker run command line.",
    "What are Docker COPY's rules about symlinks / how can I preserve symlinks?": "This works if you try to copy the entire directory as a unit, rather than trying to copy the files in the directory:\nCOPY ./ /foo/bar/\nNote that there are some subtleties around copying directories: the Dockerfile COPY documentation notes that, if you COPY a directory,\nNOTE: The directory itself is not copied, just its contents.\nThis is fine for your case where you're trying to copy the entire build context. If you have a subdirectory you're trying to copy, you need to make sure the subdirectory name is also on the right-hand side of COPY and that the directory name ends with /.",
    "Docker Buildx Cannot Pull From Local for Inherited Image": "There are a few different buildx drivers, and they each have tradeoffs.\nFirst is the docker driver. This is the driver for the default builder instance if you change nothing else. It's build-in to to the docker engine and should have visibility to the other images on the host. The goal is to be similar to the classic build process.\nThe second is docker-container and is the default if you create a new builder instance with docker buildx create. This is needed for specific functionality like the multi-platform images and exporting the cache. But since it's running inside a container, you won't see the other images on the docker host.\nOne big issue when trying to use the docker host for multi-architecture images is that the docker engine itself doesn't support multi-architecture images. It will only pull one of the architectures from a registry, so your image becomes a single architecture that likely can't be used in a multi-architecture build.\nThe easiest fix is to use a registry for your images. This supports the multi-architecture image formats which you can't do on a docker host. And this is portable when you run the build on another node.\nThere are other options in the buildx documentation to cache from/to other locations. But when dealing with a multi-arch base image, you'll find the external registry is much easier, and likely the one that actually works. Keep in mind this doesn't have to be Docker Hub, you can run you own registry server on the same host where you run your builds.\nSide note: buildx/buildkit also benefits from having a persistent volume if you happen to run ephemeral builders (e.g. using some kind of DinD on a CI server). Buildkit can be configured to automatically garbage collect this cache to avoid the storage problems of the past. And with that cache, you avoid the need to download the image layers on every build from the external registry.",
    "Run docker image with docker-compose": "you should build the image with this name: (registryName:RegistryPort)/imagename:version\n$ docker build -t myRegistry.example.com:5000/myApp:latest .\n$ docker build -t myRegistry.example.com:5000/myDb:latest .\nNow add these lines to the docker-compose file :\nMyapp:                                                    \n  image: myRegistry.example.com:5000/myApp:latest \n\nMyDb:                        \n  image: myRegistry.example.com:5000/myDb:latest\nAnd then push it :\n$ docker push myRegistry.example.com:5000/myApp:latest\n$ docker push myRegistry.example.com:5000/myDb:latest\nYour mate should now be able to pull it now\n$ docker pull myRegistry.example.com:5000/myApp:latest\n$ docker pull myRegistry.example.com:5000/myDb:latest",
    "Make docker container run forever while being able to gracefully stop": "You could consider using (with docker 1.9+) STOPSIGNAL in your Dockerfile.\nThe STOPSIGNAL instruction sets the system call signal that will be sent to the container to exit.\nThis signal can be a valid unsigned number that matches a position in the kernel\u2019s syscall table, for instance 9, or a signal name in the format SIGNAME, for instance SIGKILL.\nBut for a script managing such a signal, see \"Trapping signals in Docker containers\" and its program.sh to orchestrate other non-PID1 processes.",
    "Store and Restore Inherited Dockerfile USER setting": "If you are author of parent image you can do this like this:\nENV serviceuser=foo\nRUN useradd $serviceuser\nUSER $serviceuser\nchild image:\nUSER root\nRUN apt-get install -y cool-stuff\nUSER $serviceuser",
    "docker logs <C> returns nothing": "Docker containers exit when their main process finishes. That is why you don't get any logs.\nThe docker logs command batch-retrieves logs present at the time of execution.\n(see: https://docs.docker.com/engine/reference/commandline/logs/)\nAn example:\nThe following will create a new container and start it:\ndocker run -d --name=my_test_container alpine ping -c 20 127.0.0.1\n[----run----]   [--------name--------] [image][-----command------]\nTry to use the following, before ping command stops:\ndocker logs my_test_container\ndocker logs --follow my_test_container\nThe first one shows what has been printed out by ping (until then) and the second one gives you logs as ping prints out new lines.\nAfter 20 ping requests, the ping command finishes and the container stops.\nubuntu@ubuntu:~$ docker container ls -a\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                     PORTS               NAMES\ne8f371000146        alpine              \"ping -c 20 127.0.0.1\"   29 seconds ago      Exited (0) 9 seconds ago                       my_test_container",
    "Who runs the healthcheck? Compose or the container itself?": "Dockerfiles provides a way to perform docker container health check as well as docker compose which allows to set healthcheck for the services that compose the application from the docker-compose.yml file, starting from version 2.1.\nIn both cases, from Dockerfiles or docker compose, the health checks are performed by the Docker daemon which invokes this command every 30 seconds and it determines if a container is healthy.\nIn particular, the healthcheck result is displayed in the STATUS column of the docker ps command and may have the results:\n0 healthy container\n1 unhealthy container\nPlease, refer to the official documentation https://docs.docker.com/engine/reference/builder/#healthcheck.",
    "pnpm workspace:* dependencies": "Was getting the same error while shifting from yarn to pnpm! Resolved it by just adding pnpm-workspace.yaml with following contents:\npackages:\n  - \"apps/*\"\n  - \"packages/*\"\nHope that helps!",
    "NuGet in Docker: Error NU1301: Unable to load the service index for source - Sequence contains no elements": "I was able to resolve it by adding a Personal Access Token (PAT) as the password for the artifact source.\nExample:\n# access token arg is passed in by build process                \nARG ACCESS_TOKEN=\"your PAT\"\nARG ARTIFACTS_ENDPOINT=\"https://yoursource/v3/index.json\"\n\n# Configure the environment variables\nENV NUGET_CREDENTIALPROVIDER_SESSIONTOKENCACHE_ENABLED true\nENV VSS_NUGET_EXTERNAL_FEED_ENDPOINTS \"{\\\"endpointCredentials\\\": [{\\\"endpoint\\\":\\\"${ARTIFACTS_ENDPOINT}\\\", \\\"password\\\":\\\"${ACCESS_TOKEN}\\\"}]}\"\nI also needed terminal to be running with Admin privileges.",
    "Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=os&infra=container [closed]": "You should use next commands inside your docker container\nsed -i 's/mirrorlist/#mirrorlist/g' /etc/yum.repos.d/CentOS-*\nsed -i 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-*",
    "How to use the ARG instruction of Dockerfile for Windows image": "As @matt9 suggested\nUse $env:FirefoxVersion in powershell\nUse %FirefoxVersion% in cmd.exe\nFROM microsoft/windowsservercore:ltsc2016\nARG FirefoxVersion\n#if using powershell\nSHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop';\"]\nRUN Write-Host Powershell: $env:FirefoxVersion\n#if using CMD\nSHELL [\"cmd\", \"/S\", \"/C\"]\nRUN echo cmd.exe: %FirefoxVersion%\nBuild: docker build -t myimage --build-arg FirefoxVersion=61.0.1 .\nResult\nPowershell: 61.0.1\ncmd.exe: 61.0.1",
    "Docker so slow while installing pip requirements": "Probably this is because PyPI wheels don\u2019t work on Alpine. Instead of using precompile files Alpine downloads the source code and compile it. Try to use python:3.7-slim image instead:\n# Pull base image\nFROM python:3.7-slim\n\n# Set environment variables\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\n# Set work directory\nWORKDIR /code\n\n# Install dependencies\nCOPY requirements.txt /code/\nRUN pip install -r requirements.txt\n\n# Copy project\nCOPY . /code/\nCheck this article for more details: Alpine makes Python Docker builds 50\u00d7 slower.",
    "Visual Studio with docker file support - no such file or directory": "This is my fault. I should run this command from root project directory:\ndocker build -f \"DockerTest/Dockerfile\" .",
    "Pass command line args to Java app (Spring Boot) running in Docker": "You can provide all command line arguments just after name of your docker image in run command.\nExample:\ndocker run -p 8080:8080 test-image --recipient=\"World\"--greeting=\"Hello\"",
    "python based Dockerfile throws locale.Error: unsupported locale setting": "What I would do for Debian based docker image:\nFROM python:3.7.5\n\nRUN apt-get update && \\\n    apt-get install -y locales && \\\n    sed -i -e 's/# ru_RU.UTF-8 UTF-8/ru_RU.UTF-8 UTF-8/' /etc/locale.gen && \\\n    dpkg-reconfigure --frontend=noninteractive locales\n\nENV LANG ru_RU.UTF-8\nENV LC_ALL ru_RU.UTF-8\nand then in python:\nimport locale\n\nlocale.setlocale(locale.LC_ALL,'ru_RU.UTF-8')",
    "Cron and Crontab files not executed in Docker": "Cron (at least in Debian) does not execute crontabs with more than 1 hardlink, see bug 647193. As Docker uses overlays, it results with more than one link to the file, so you have to touch it in your startup script, so the link is severed:\ntouch /etc/crontab /etc/cron.*/*",
    "Docker build failed to compute cache key": "Thanks to BMitch, I found my problem. The problem is the dockerignore file contains some pattern that must not match with COPY files name.\nMy problem is the patterns inside the .dockerignore file is matched wrongly with bin/app.publish. To resolve my problem I just change the pattern in dockerignore.\nSpecifically, remove the line **/bin for .dockerignore.",
    "Docker environment variables in multi-stage builds": "So this is not a multi-stage issue.\nIt appears ENV variables are only used when running containers (docker-compose up). Not at build time (docker-compose build). So you have to use arguments:\n.env:\nTEST=11111\ndocker-compose.yaml:\nversion: '3'\nservices:\n  test:\n    build:\n      context: .\n      args:\n        TEST: ${TEST}\nDockerfile:\nFROM nginx:alpine\nARG TEST\nENV TEST ${TEST}\nCMD [\"sh\", \"-c\", \"echo $TEST\"]\ntest command:\ndocker rmi test_test:latest ; docker-compose build && docker run -it --rm test_test:latest\nSeriously the documentation is somewhat lacking.\nReference: https://github.com/docker/compose/issues/1837",
    "Bust cache bust within Dockerfile without providing external build args": "Update: Reviewing this one, it looks like you injected the cache busting option incorrectly in two ways:\nENV is not an ARG\nThe $(x) syntax is not a variable expansion, you need curly brackets (${}), not parenthesis ($()).\nTo break the cache on the next run line, the syntax is:\nARG CACHE_BUST\nRUN echo \"command with external dependencies\"\nAnd then build with:\ndocker build --build-arg CACHE_BUST=$(date +%s) .\nWhy does that work? Because during the build, the values for ARG are injected into RUN commands as environment variables. Changing an environment variable results in a cache miss on the new build.\nTo bust the cache, one of the inputs needs to change. If the command being run is the same, the cache will be reused even if the command has external dependencies that have changed, since docker cannot see those external dependencies.\nOptions to work around this include:\nPassing a build arg that changes (e.g. setting it to a date stamp).\nChanging a file that gets included into the image with COPY or ADD.\nRunning your build with the --no-cache option.\nSince you do not want to do option 1, there is a way to do option 3 on a specific line, but only if you can split up your Dockerfile into 2 parts. The first Dockerfile has all the lines as you have today up to the point you want to break the cache. Then the second Dockerfile has a FROM line to depend on the first Dockerfile, and you build that with the --no-cache option. E.g.\nDockerfile1:\nFROM base\nRUN normal steps\nDockerfile2\nFROM intermediate\nRUN curl external.jar>file.jar\nRUN other lines that cannot be cached\nCMD your cmd\nThen build with:\ndocker build -f Dockerfile1 -t intermediate .\ndocker build -f Dockerfile2 -t final --no-cache .\nThe only other option I can think of is to make a new frontend with BuildKit that allows you to inject an explicit cache break, or unique variable that results in a cache break.",
    "CMD doesn't run after ENTRYPOINT in Dockerfile": "As documented in https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact, if you combine the \"shell form\" of CMD and ENTRYPOINT, the CMD specification is ignored:\nSo you should rather use the \"exec form\" and write something like this:\n\u2026\nENTRYPOINT [\"/usr/bin/chamber\", \"exec\", \"${ENV}_${SERVICE_NAME}\", \"-r\", \"1\", \"--\", \"./entrypoint.sh\"]\nCMD [\"java -jar\", \"-Dspring.profiles.active=docker\", \"target/my.jar\"]\nHowever this won't work as is, because the ${ENV} and ${SERVICE_NAME} won't be expanded (as a shell would be required).\nSo the simplest, proper solution to apply here is to refactor your entrypoint.sh, or if ever you don't want to change it and still rely on environment variables with an \"exec form\" ENTRYPOINT, you could write instead:\n\u2026\nRUN chmod a+x entrypoint1.sh\nENTRYPOINT [\"./entrypoint1.sh\"]\nCMD [\"java -jar\", \"-Dspring.profiles.active=docker\", \"target/my.jar\"]\nwith a file\nentrypoint1.sh\n#!/bin/bash\nexec /usr/bin/chamber exec ${ENV}_${SERVICE_NAME} -r 1 -- ./entrypoint.sh \"$@\"",
    "How can I edit an existing docker image metadata?": "Its a bit hacky, but works:\nSave the image to a tar.gz file:\n$ docker save [image] > [targetfile.tar.gz]\nExtract the tar file to get access to the raw image data:\ntar -xvzf [targetfile.tar.gz]\nLookup the image metadata file in the manifest.json file: There should be a key like .Config which contains a [HEX] number. There should be an exact [HEX].json in the root of the extracted folder.\nThis is the file containing the image metadata. Edit as you like.\nPack the extracted files back into an new.tar.gz-archive\nUse cat [new.tar.gz] | docker load to re-import the modified image\nUse docker inspect [image] to verify your metadata changes have been applied\nEDIT: This has been wrapped into a handy script: https://github.com/gdraheim/docker-copyedit",
    "Docker: reload a nodejs app with nodemon": "Ah, the problem seems to be with docker-compose.yml volumes param. Changing it to\nvolumes:\n  - .:/opt/app\nmakes it work. It tells docker to mount the application folder . on the host to the /opt/app in the container.\nThen after doing docker-compose up, the server restarts in case of file changes.",
    "Putting a python script into a docker container": "CMD SOLUTION\nI would recommend switching from Entrypoint to CMD\nCMD [ \"python\", \"./my_script.py\" ]\nThis method can be seen in depth here: https://runnable.com/docker/python/dockerize-your-python-application\nSome more complexity (flags etc) can also be handled with CMD as can be seen here : how to pass command line arguments to a python script running in docker\nENTRYPOINT SOLUTION\nENTRYPOINT [\"python\", \"app.py\"]\nThis style of solution is explained in depth here: https://lostechies.com/gabrielschenker/2016/08/21/container-entrypoint/\nThe difference between the two (if you're curious and don't know)\nCMD commands can be overwritten from the command line. CMD is effectively the default value of your container's command.\nENTRYPOINT commands are not overwritten from the command line.\nCMD and ENTRYPOINT are similar, but I prefer command because it enables me to change the flags or command at run time if preferred, while keeping the same dockerfile that can be run without a command if desired.\nHere is a longer form discussion of the difference: http://goinbigdata.com/docker-run-vs-cmd-vs-entrypoint/",
    "Running redis on nodejs Docker image": "The best solution would be to use docker compose. With this you would create a redis container, link to it then start your node.js app. First thing would be to install docker compose detailed here - (https://docs.docker.com/compose/install/).\nOnce you have it up and running, You should create a docker-compose.yml in the same folder as your app's dockerfile. It should contain the following\nversion: '3'\nservices:\n  myapp:\n    build: .  \n    ports:\n     - \"3011:3011\"\n    links:\n     - redis:redis\n  redis:\n    image: \"redis:alpine\"\nThen redis will be accessible from your node.js app but instead of localhost:6379 you would use redis:6379 to access the redis instance.\nTo start your app you would run docker-compose up, in your terminal. Best practice would be to use a network instead of links but this was made for simplicity.\nThis can also be done as desired, having both redis and node.js on the same image, the following Dockerfile should work, it is based off what is in the question:\nFROM node:carbon\n\nRUN wget http://download.redis.io/redis-stable.tar.gz && \\\n    tar xvzf redis-stable.tar.gz && \\\n    cd redis-stable && \\\n    make && \\\n    mv src/redis-server /usr/bin/ && \\\n    cd .. && \\\n    rm -r redis-stable && \\\n    npm install -g concurrently   \n\nEXPOSE 6379\n\nWORKDIR /app\n\nCOPY package.json /app\n\nRUN npm install\n\nCOPY . /app\n\nEXPOSE 3011\n\nEXPOSE 6379\n\nCMD concurrently \"/usr/bin/redis-server --bind '0.0.0.0'\" \"sleep 5s; node /app/src/server.js\" \nThis second method is really bad practice and I have used concurrently instead of supervisor or similar tool for simplicity. The sleep in the CMD is to allow redis to start before the app is actually launched, you should adjust it to what suits you best. Hope this helps and that you use the first method as it is much better practice",
    "Using Docker Buildkit --mount=type=cache for caching Nuget packages for .NET 5 dockerfile": "The key is using the same --mount=type=cache argument in all of the dockerfile RUN commands that need access to the same package cache (e.g. docker restore, docker build, docker publish).\nHere's a short dockerfile example showing the same --mount=type=cache with the same id spread across separate dotnet restore/build/publish invocations. Separating the calls isn't always necessary as build will restore by default and publish will do both, but this way shows sharing the same cache across multiple commands. The cache mount declarations only appear in the dockerfile itself and don't require arguments in docker build.\nThe example also shows how you might use the BuildKit --mount=type=secret argument to pass in a NuGet.Config file that may be configured to access e.g. a private nuget feed. By default, secret files passed in this way appear in /run/secrets/<secret-id>, but you can change where they go via the target attribute in the docker build command. They only exist during the RUN invocation and don't remain in the final image.\n# syntax=docker/dockerfile:1.2\nFROM my-dotnet-sdk-image as builder\nWORKDIR \"/src\"\nCOPY \"path/to/project/src\" .\n\nRUN --mount=type=cache,id=nuget,target=/root/.nuget/packages \\\n    --mount=type=secret,id=nugetconfig \\\n    dotnet restore \"MyProject.csproj\" \\\n    --configfile /run/secrets/nugetconfig \\\n    --runtime linux-x64\n\nRUN --mount=type=cache,id=nuget,target=/root/.nuget/packages \\\n    dotnet build \"MyProject.csproj\" \\\n    --no-restore \\\n    --configuration Release \\\n    --framework netcoreapp3.1 \\\n    --runtime linux-x64\n\nRUN --mount=type=cache,id=nuget,target=/root/.nuget/packages \\\n    dotnet publish \"MyProject.csproj\" \\\n    --no-restore \\\n    --no-build \\\n    -p:PublishReadyToRun=true \\\n    -p:PublishReadyToRunShowWarnings=true \\\n    -p:TieredCompilation=false \\\n    -p:TieredCompilationQuickJit=false \\\n    --configuration Release \\\n    --framework netcoreapp3.1 \\\n    --runtime linux-x64\nA sample docker build command to pass in the nugetconfig file for a private feed might be:\ndocker build --secret id=nugetconfig,src=path/to/nuget.config -t my-dotnet-image .\nFor that command, the environment variable DOCKER_BUILDKIT=1 needs to be set.\nAlternatively, you can use buildx:\ndocker buildx build --secret id=nugetconfig,src=path/to/nuget.config -t my-dotnet-image .",
    "Why doesn't the cron service in Dockerfile run?": "Having started crond with supervisor, your cron jobs should be executed. Here are the troubleshooting steps you can take to make sure cron is running\nIs the cron daemon running in the container? Login to the container and run ps a | grep cron to find out. Use docker exec -ti CONTAINERID  /bin/bash to login to the container.\nIs supervisord running?\nIn my setup for instance, the following supervisor configuration works without a problem. The image is ubuntu:14.04. I have CMD [\"/usr/bin/supervisord\"] in the Dockerfile.\n[supervisord]\n nodaemon=true\n[program:crond]\n command = /usr/sbin/cron\n user = root\n autostart = true\nTry another simple cron job to findout whether the problem is your cron entry or the cron daemon. Add this when logged in to the container with crontab -e :\n* * * * * echo \"hi there\" >> /tmp/test\nCheck the container logs for any further information on cron:\ndocker logs CONTAINERID | grep -i cron\nThese are just a few troubleshooting tips you can follow.",
    "What is the programming language used in dockerfile and docker-compose files": "Docker is written in the GO language\nA Dockerfile is just a text file. It is a script that contains collections of commands and instructions that will be automatically executed in sequence in the docker environment for building a new docker image.\nA docker-compose.yml file is used for docker-compose, if you are using that feature of Docker.",
    "how to pass argument to dockerfile from a file": "Then you can use --build-arg, it will pass parameters with --build-arg key=value to dockerfile when build, refer to this.\nYou just need to use sed to get from your env file & combine them to the format --build-arg key=value when build the dockerfile, example as next:\nDockefile:\nFROM ubuntu:16.04\n\nARG BuildMode\nENV BuildMode=${BuildMode}\n\nRUN echo $BuildMode\ndocker.env:\nBuildMode=\"release\"\nCommand:\ndocker build -t abc:1 $(cat docker.env | sed 's@^@--build-arg @g' | paste -s -d \" \") . --no-cache\nOutput:\nshubuntu1@shubuntu1:~/1$ docker build -t abc:1 $(cat docker.env | sed 's@^@--build-arg @g' | paste -s -d \" \") . --no-cache\nSending build context to Docker daemon  3.072kB\nStep 1/4 : FROM ubuntu:16.04\n ---> 13c9f1285025\nStep 2/4 : ARG BuildMode\n ---> Running in 3bc49fbb0af4\nRemoving intermediate container 3bc49fbb0af4\nStep 3/4 : ENV BuildMode=${BuildMode}\n ---> Running in 4c253fba0b36\nRemoving intermediate container 4c253fba0b36\n ---> c70f7f535d1f\nStep 4/4 : RUN echo $BuildMode\n ---> Running in 5fef72f28975\n\"release\"\nRemoving intermediate container 5fef72f28975\n ---> 4b5555223b5b\nSuccessfully built 4b5555223b5b\nSuccessfully tagged abc:1",
    "dockerize a wpf application and use it": "You cannot run a WPF application in docker.\nHere is a snippet of the Microsoft docs\nDocker is for server applications\u2014Web sites, APIs, messaging solutions and other components that run in the background. You can\u2019t run desktop apps in Docker because there\u2019s no UI integration between the Docker platform and the Windows host. That rules out running Windows Forms or Windows Presentation Foundation (WPF) apps in containers (although you could use Docker to package and distribute those desktop apps), but Windows Communication Foundation (WCF), .NET console apps and all flavors of ASP.NET are great candidates.\nCheck out the source",
    "How to run Redis on docker with a different configuration file?": "The run command:\ndocker run --name my-redis -p 0.0.0.0:6379:6379 -d ouruser/redis redis-server --appendonly yes\nOverrides the CMD defined in the Dockerfile with redis-server --appendonly yes, so your conf file will be being ignored. Just add the path to the conf file into your run command:\ndocker run --name my-redis -p 0.0.0.0:6379:6379 -d ouruser/redis redis-server /usr/local/etc/redis/redis.conf --appendonly yes\nAlternatively, set up an entrypoint script or add --appendonly yes to the CMD instruction.",
    "Docker --ssh default Permission denied (publickey)": "Docker is not copying the file from ~/.ssh/.\nWhen using the default configuration --ssh default you need to add your keys to your local SSH agent.\nYou can check ssh-add -L locally to see if the public keys are visible to the agent.\nIf they are not, try to run ssh-add -K.\nReferences:\nhttps://medium.com/@tonistiigi/build-secrets-and-ssh-forwarding-in-docker-18-09-ae8161d066\nhttps://apple.stackexchange.com/questions/254468/macos-sierra-doesn-t-seem-to-remember-ssh-keys-between-reboots",
    "How to run two commands on Dockerfile? [duplicate]": "Try creating a script like this:\n#!/bin/sh\nnginx -g 'daemon off;' & \ngulp watch-dev\nAnd then execute it in your CMD:\nCMD /bin/my-script.sh\nAlso, notice your last line would not have worked:\nCMD [\"gulp watch-dev\"]\nIt needed to be either:\nCMD gulp watch-dev\nor:\nCMD [\"gulp\", \"watch-dev\"]\nAlso, notice that RUN is for executing a command that will change your image state (like RUN apt install curl), not for executing a program that needs to be running when you run your container. From the docs:\nThe RUN instruction will execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the Dockerfile.",
    "Docker - Build Arg in Source File": "According to the docs, the first instruction needs to be FROM (or technically a parser directive, but not relevant here) so this approach likely isn't going to work. Probably some shell wrapper around docker build... with some sed command or something to insert the correct version, or a template of some kind.\nGareth Rushgrove had a nice talk at DockerCon16 on image build tooling that might be interesting.\nUpdate (7/2/17): This is now possible to achieve since v17.06.",
    "How to kill used port of docker": "To expand on Robert Moskal's answer, you'll need to kill whatever's already on that port:\nkill all the containers again\nif you're on Linux, kill the process running on your port with fuser -k 9042/tcp\nif above steps don't work, reboot your computer and try again.\nHappy hunting!",
    "UserWarning: Supervisord is running as root and it is searching for its configuration file in default locations": "By design:\ndocker containers run as root if no USER is specified\nsupervisor does not allow running the daemon as root, without specifying it explicitly in the config files.\nSo you can either run supervisor as a user other than root or just add the user=root directive into the configs.\n[supervisord]\nnodaemon=true\nuser=root\n\n\n[program:uwsgi]\ncommand = /usr/local/bin/uwsgi --ini /trell-ds-framework/uwsgi.ini\nstdout_logfile=/dev/stdout\nstdout_logfile_maxbytes=0\nstderr_logfile=/dev/stderr\nstderr_logfile_maxbytes=0\nuser=root\n\n[program:nginx]\ncommand = /usr/sbin/nginx -g \"daemon off;\"\nstdout_logfile=/dev/stdout\nstdout_logfile_maxbytes=0\nstderr_logfile=/dev/stderr\nstderr_logfile_maxbytes=0\ndaemon=off\nuser=root ;here too if you want to",
    "How to build a docker image for M1 Mac Silicon or AMD conditionally from Dockerfile?": "I believe you can use the --platform parameter on docker buildx build or docker build to set platform(s) to build the image which will be used within any FROM calls within the Dockerfile if nothing else is specified (see Dockerfile FROM), as mentioned in the documentation.\nYou can then use the TARGETPLATFORM variable within your Dockerfile to get what platform it's being built for if needed. If you want to change the default platform to build for, you can set the DOCKER_DEFAULT_PLATFORM environment variable.",
    "What is the default user for Linux Alpine?": "I believe USER root is correct. The default docker user is typically root.\nThe closest reference I could find is in the Docker docs for the USER command:\nhttps://docs.docker.com/engine/reference/builder/#user\nWarning: When the user doesn\u2019t have a primary group then the image (or the next instructions) will be run with the root group.\nHowever, it very easy to find for yourself, using a simple Dockerfile:\nFROM python:3.6-alpine\nRUN whoami\nWill output (either ifdocker is started as root or as a Docker previliged user):\nStep 2/2 : RUN whoami #\n ---> Running in 3a8d159404cd\nroot",
    "Dockerfile with copy command and relative path": "Reference: Allow Dockerfile from outside build-context\nYou can try this way\n$ cd project-console\n$ docker build -f ../project-test/Dockerfile .\nUpdate:\nBy using docker-compose\nbuild:\n  context: ../\n  dockerfile: project-test/Dockerfile\n../ will be set as the context, it should include project-console and project-test in your case. So that you can COPY project-console/*.csproj in Dockerfile.",
    "Can we include git commands in docker image?": "Yes, you can. Let me recommend some things about that.\nDefine a git token in github associated to a generic user. I like to give only read permissions to that user.\nDeclare some ARGs related to git in your Dockerfile, so you can customize your build.\nAdd Git installation to your Dockerfile.\nDo git clone cloning only needed folders, not the whole repo.\nSo, your Dockerfile could be, for example for debian/ubuntu:\nFROM <your linux distribution>\nARG GIT_USER\nARG GIT_TOKEN\nARG GIT_COMMIT\nARG SW_VERSION\nRUN apt-get update\nRUN apt-get -y install git\nRUN git clone -n https://${GIT_USER}:${GIT_TOKEN}@github.com/<your git>/your_git.git --depth 1 && cd <your_git> && git checkout ${GIT_COMMIT} <dir_you_want_to_checkout>\nCOPY myapp/ /app\nCMD /app/entry.pl /<your_git>/<dir_you_want_to_checkout>/...\nThen, you can build as you know with:\ndocker build -t --build-arg GIT_USER=<your_user> --build-arg GIT_TOKEN=d0e4467c63... --build-arg GIT_COMMIT=a14dc9f454... <your_image_name> .\nGood luck.",
    "is it a good practice to put a war file image into docker containers?": "One of the main reasons for building a docker image for your application is to provide an artefact that people can run without installing and managing external software dependencies (like an app server to run a war file).\nFrom a container user perspective, it makes no difference whether you package your code in a jar or a war file or as a fortran binary. All they do is run a container image.\nFrom your perspective, of doing the Docker build and config management, the packaging of a jar file and copying into a container would be a simpler solution than trying to setup and configure an app server for packaging, then deploying each release into the app server. That being said, users would probably prefer the more complete app server solution, because that may be easier for them the run the application.\nSee Ohmens answer for some more technical components of the java build process",
    "Dockerfile - What are the intermediate containers doing exactly?": "My guess is that the /etc/kafka-connect/jars directory is declared as a VOLUME in that image's Dockerfile.\nAnd the output of the docker inspect command confirms my guess:\n$ docker image inspect confluentinc/cp-kafka-connect:4.0.0 --format '{{.Config.Volumes}}'\nmap[/etc/kafka-connect/secrets:{} /etc/kafka/secrets:{} /var/lib/kafka/data:{} /etc/kafka-connect/jars:{}]\nCiting from The Dockerfile Specification:\nIf any build steps change the data within the volume after it has been declared, those changes will be discarded.\nSo, here are the details about your problem:\nThe base image declares VOLUME /etc/kafka-connect/jars.\nIn step 3 of your Dockerfile, you changed the contents of that directory. That's why the ls command in this step works normally.\nThen these changes are discarded.\nThe solution is to put the jar files on your host, and bind-mount the host directory to the container when running the container. As below:\ndocker run -v /path/contains/jar/files:/etc/kafka-connect/jars <IMAGE>",
    "Spring Boot in Docker": "You can use docker run Using Spring Profiles. Running your freshly minted Docker image with Spring profiles is as easy as passing an environment variable to the Docker run command\n$ docker run -e \"SPRING_PROFILES_ACTIVE=prod\" -p 8080:8080 -t springio/gs-spring-boot-docker\nYou can also debug the application in a Docker container. To debug the application JPDA Transport can can be used. So we\u2019ll treat the container like a remote server. To enable this feature pass a java agent settings in JAVA_OPTS variable and map agent\u2019s port to localhost during a container run.\n$ docker run -e \"JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,address=5005,server=y,suspend=n\" -p 8080:8080 -p 5005:5005 -t springio/gs-spring-boot-docker\nResource Link: Spring Boot with Docker\nUsing spring profile with docker for nightly and dev build:\nSimply set the environment varialbe SPRING_PROFILES_ACTIVE when starting the container. This will switch the active of the Spring Application.\nThe following two lines will start the latest Planets dev build on port 8081 and the nightly build on port 8080.\ndocker run -d -p 8080:8080 -e \\\"SPRING_PROFILES_ACTIVE=nightly\\\" --name nightly-planets-server planets/server:nightly\ndocker run -d -p 8081:8080 -e \\\"SPRING_PROFILES_ACTIVE=dev\\\" --name dev-planets-server planets/server:latest\nThis can be done automatically from a CI system. The dev server contains the latest build and nightly will be deployed once a day...",
    "How does escape character work in a Dockerfile?": "Use the escape directive on Windows to avoid these headaches, e.g.:\n# escape=`\n\nFROM microsoft/nanoserver\nCOPY testfile.txt c:\\\nRUN dir c:\\\nIn your case, the second slash is escaping the newline. Therefore two lines are running together to form: COPY testfile.txt c:\\RUN dir c:. I understand you're thinking the first slash should escape the second, but that's not how the parser behaves according to the documentation.",
    "Docker : failed to compute cache key": "In my case I missed the folder entry in .dockerignore file. Do something like that.\n**/*\n!docker-images\n!configs\n!main",
    "Dockerfile build ARG in COPY --from=": "At last check, you can't use a build arg there, but you can use it in a top level from line using the multi-stage syntax. Then, you also need to define the build arg at the top level:\nARG BUILD_CONFIG=dev\nFROM test/test-library:${BUILD_CONFIG} as test-library\n\nFROM node:12.15:0 as prod\nARG BUILD_CONFIG\nRUN echo ${BUILD_CONFIG}\n\nCOPY --from=test-library /work/dist /work/library/dist\nCMD[ \"bash\" ]",
    "Problem with connecting mongo express service in docker": "In your docker-compose file, you call the mongo service mongo. That's the name you can address it as on the docker network. In your connection string, you've said\nmongodb://root:pass@mongodb:27017/\"\nIt should be\nmongo://root:pass@mongo:27017/\"\nSince the connection string is built using the environment variables in your docker-compose file, the thing to change is\n- ME_CONFIG_MONGODB_SERVER=mongodb\nto\n- ME_CONFIG_MONGODB_SERVER=mongo",
    "install PyTorch CPU-only in Dockerfile": "Installing it via pip should work:\nRUN pip3 install torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html",
    "Build a Docker image using docker build and a tmpfs?": "With BuildKit, you can use experimental features to mount a tmpfs filesystem for a single RUN line. This filesystem will not exist outside of the context of that RUN line, just as a tmpfs does not exist when a container has been stopped or deleted, so you'll need to copy any artifacts back into the container filesystem at the end of your build.\nFor BuildKit, you need at least 18.09, and you can enable it by either:\n    export DOCKER_BUILDKIT=1\nfor a single shell, or to change the default on the host you can update /etc/docker/daemon.json with:\n    {\n      \"features\": {\"buildkit\": true}\n    }\nWith BuildKit enabled, the Dockerfile would look like:\n    # syntax=docker/dockerfile:experimental\n    FROM your_base_image\n    COPY src /src\n    RUN --mount=type=tmpfs,target=/build \\\n        cp -r /src/. /build/ \\\n     && cd /build \\\n     && make your_project \\\n     && cp /build/result.bin /result.bin\n    ...\nNote that BuildKit is rather new, won't be supported in most cloud build environments, and isn't supported from docker-compose in older versions either. To see more on these experimental features, see: https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/reference.md\nDocker-compose:\nThe support for BuildKit within docker-compose was released in v1.25.0 In order to enable it, do the following configurations:\n    export DOCKER_BUILDKIT=1 # or configure in daemon.json\n    export COMPOSE_DOCKER_CLI_BUILD=1\nWith those variables set in your shell, you can now run docker-compose build using BuildKit.\nIn windows you can execute in your console:\n    setx DOCKER_BUILDKIT 1 # or configure in daemon.json\n    setx COMPOSE_DOCKER_CLI_BUILD 1\nafter will need restart your console",
    "Does LABEL add new layer to docker image or apply to all layers, and does it matter where LABEL is place in dockerfile?": "I've performed a few tests using a very simple Dockerfile, and adding a LABEL doesn't add any layers, but it does change the image's sha256 id.\nAny change to the label's value will change the sha256 and even moving it to a different part of the Dockerfile will too, but the layers remain unaffected by the label's presence or value.\nAs for whether a LABEL \"applies to all layers\", I suppose we could conclude that the label is applicable to the overall image but has no effect on its layers.",
    "can't find a suitable configuration file in this directory or any docker compose (docker-compose section not getting added in solution)": "Not really an expert on VS 2017, but you would need to create a docker-compose.yml file inside of the project.\nMore information can be found here.\nversion: '3.5'\nservices:\n  client:\n    container_name: WebApplication \n    build:\n      dockerfile: Dockerfile\n    volumes:\n        # Any volumes you would mount goes here\n    ports:\n        # Ports that need to be mapped from container to host goes here\n    environment:\n        # Any environment variables\nEdit: In order for VS 2017 to automatically create docker-compose.yml, you will need to right-click on the Project and select Add > Orchestrator Support (15.8 and greater) or Add > Docker Project Support (below 15.8).",
    "Transfer virtualenv to docker image": "We had run into this problem earlier and here are a few things we considered:\nConsider building base images that have common packages installed. The app containers can then use the one of these base containers and install the differential.\nCache the Pip packages on a local path that can be mounted on the container. That'll save the time to download the packages.\nDepending on the complexity of your project one may suit better than the other - you may also consider a hybrid approach to find maximum optimization.",
    "How to fix read-only error when adding host in Docker? [duplicate]": "From the docker docs page on /etc/hosts, they say docker itself may update the file.\nSince Docker may live update the container\u2019s /etc/hosts file, there may be situations when processes inside the container can end up reading an empty or incomplete /etc/hosts file. In most cases, retrying the read again should fix the problem.\nSo because docker may rewrite the file, any changes you make from inside the image/container would be lost. To prevent this, they don't let you change it at all, so you get the read-only FS error.",
    "why is my docker container ASP.NET core app not available after ending debugging in Visual Studio": "During the debugging, if you run this command:\ndocker exec -it containerName bash -c 'pidof dotnet'\nYou will noticed, that the dotnet process is running, and when you stop the debugging and run it again, you are going to see that, the process was finalized.\nIf you want to start your application in the container, without run the debugger again, you just need to run the start the dotnet process inside the container.\nYou could do that, running a script like this:\n#Set these 3 variables\n$containerName = \"MyContainer\"\n$appDll = \"myApp.dll\"\n$appDirectory = \"/app/bin/debug/netcoreapp3.1\"\n\n$args = \"/c docker exec -it $containerName bash -c 'cd $appDirectory;dotnet $appDll'\"\nStart-Process -FilePath \"powershell\" -ArgumentList $args -NoNewWindow\nYou can check if it worked, by running this script again:\ndocker exec -it containerName bash -c 'pidof dotnet'",
    "How to use the (short) git commit hash to tag image versions in Docker Compose file": "The build ARGS section illustrates jonrsharpe's comment\nYou need to set an environment variable first, and declare the ARGS in your docker-compose.yml\nARG commit\n...\nimage: \"myapp:${commit}\"\nSee \"variable substitution\" and also The \u201cenv_file\u201d configuration option\nYour configuration options can contain environment variables.\nCompose uses the variable values from the shell environment in which docker-compose is run.\nAny hope of running a command directly in the docker-compose.yml file was ruled out with docker/compose issue 4081.",
    "Can't connect to mongodb in the docker container": "Use host.docker.internal with exposed port : host.docker.internal:27017",
    "install mongoose in docker container": "This is just an output from node-gyp. You can ignore this messages if you don't use the MongoDB Enterprise with Kerberos Authentication.\nNevertheless the docker build command will run successfully and mongoose will also work.\nThe output above is just about some pragam directives. The pragma statement was introduced with ANSI-C to define compiler options.\nFor example have a look at:\n../lib/kerberosgss.c: In function 'authenticate_gss_client_wrap':\n../lib/kerberosgss.c:348:19: warning: variable 'server_conf_flags' set but not used [-Wunused-but-set-variable]\nchar buf[4096], server_conf_flags;\nThis just tells you, that the variable server_conf_flags defined in lib/kerberosgss.c:348:19 is not used anywhere. If you look at the source on github, this is not a problem.\nEach C-compiler handles these pragam directives slightly different which is intentionally. Maybe on your local machine you have got a different C-compiler or a completely different OS?\nSo this is nothing to worry about.",
    "Docker Compose: Stop \"depends_on\" service when \"parent\" finishes": "I do not believe this is a built in feature of docker-compose. But you can easily script a solution, e.g.:\ndocker-compose up -d db\ndocker-compose up test-backend\ndocker-compose down db\nOutside of docker-compose, there's also the docker container wait command that lets you hang until a container exits. You can lookup the container name with a docker container ls and filters on a label or other unique value, or you can rely on the predictable container names from compose ($project_$service_$replica). The docker container wait command would allow you to spin up the entire project in a detached state and then wait for one container to exit before proceeding to stop it, e.g.:\ndocker-compose up -d\ndocker container wait $(basename $(pwd))_test-backend_1\ndocker-compose down db",
    "supervisor.sock refused connection in docker container": "It's also worth checking that supervisor is actually running\nsupervisord\nRunning this will try and start supervisor, or throws an error if it is already running. For me, it's always a stale connection which breaks supervisor",
    "Run dotnet 1.1 using docker": "I changed my dockerfile to have the following COPY statement:\nCOPY out ./\nThat then made the entrypoint to work because it was then able to find out myApp.dll. I think the message could be improved here, but that's me assuming that's what happened",
    "Docker-Compose Issue with container name": "No, you can't have two containers with the same name. You'll have to pick a different name for the container_name field. The previous container needs to be removed before you can re-use the name.\nIf you wanted Compose to treat the container as if it had created it, you have to set the container labels as Compose does. The easiest way to find these would be to have compose create a container (probably by removing the container_name field), then using docker inspect to view the labels.",
    "Can Docker help build executable that work in different platform": "You still need to compile source code on different platforms. The point of the docker is to automate building and testing the code on every platform, so that you can just work on the code, and let it build and test on every platform.\nYou do have to set up the dockers and test scripts and get the code working cross-platform in the first place. But after that is done, you can basically not worry about any other platform unless you actually break it.",
    "How can I improve build time of Angular 5 project using Docker?": "[TL;DR]\nUse volumes to store node_modules and .npm\nParallelize parts of your process (e.g. tests)\nBe careful when using relative paths\nDo not copy your entire project with COPY . . . Relative path issues and possible information leaks.\nCreate a separate image containing only core dependencies for building and testing (e.g. npm, java, chrome-driver, libgconf2).\nConfigure pipelines to use this image\nLet the CI clone the repo and copy your project into the container for building and testing\nArchive built files (e.g. dist) and tag based on failure rates\nCreate a new image with just enough things to run your built files.\n[LONG VERSION]\nThere is a good chance that your npm dependencies are being re-downloaded and/or your docker images are being rebuilt for every build you run.\nRather than copying files into a docker image, it would be better to mount volumes for modules and cache so that additional dependencies included later doesn't need to be downloaded again. Typical directories that you should consider creating volumes for are npm_modules (one for global and one for local) and .npm (cache).\nYour package.json is being copied into root / and the same package.json is being copied into /web with COPY . ..\nThe initial run of npm i is installing into / and you're running it again for /web. You're downloading dependencies twice but are the modules in / going to be used for anything? Regardless, you appear to be using the same package.json in both npm i and ng build, so the same thing is being done twice, ( [EDIT] - It would seem that ng build doesn't redownload packages) but node_modules isn't available in / so the npm i command creates another one and re-downloads all packages.\nYou create a web directory in root / but there are other commands instructing to relative paths ./web. Are you certain that things are running in the right places? There is no guarantee that programs would be looking in the directories you want them to if you use relative paths. While it may appear to work for this image, the same practice will not be consistent across other images that may have different initial work directories.\n[may or may not be relevant information]\nAlthough I'm not using Bitbucket for automating builds, I faced a similar issue when running Jenkins pipelines. Jenkins placed the project in a different directory so that every time it runs, all the dependencies would be downloaded again. I initially thought the project would be in /home/agent/project but it was actually placed elsewhere. I found the directory where the project was copied to by using the pwd and npm cache verify command in a build step, then mounted the volumes to the correct places. You can view the output in the logs generated on builds.\nYou can view the output by expanding the section within the pipelines page.\nIf the image is being rebuilt on every run, build your image separately then push the image to a registry. Configure the pipeline file to use your image instead. You should try to use already available base images whenever possible unless there are other dependencies you need that are unavailable in the base image (things like alpine's apk packages and not npm. npm dependencies can be stored in volumes). If you're going to use a public registry, do not store any files that may contain sensitive data. Configure your pipeline so that things are mounted with volumes and/or uses secrets.\nA basic restructure on the test and build steps.\n       Image on Docker Hub\n              |\n              |\n           ---|-------------------------------------|\n           |                       |                |\n           V                       V                |\nCommit -- build (no test) ---> e2e tests (no build)-]--+--> archive build --> (deploy/merge/etc)\n                         |           _______________|  ^\n                         |           v                 |\n                         |-> unit tests (no build)---->|\nYou don't need to follow it entirely, but it should give you an idea on how you could use parallel steps to separate things and improve completion times.",
    "Loop/Iterate in Dockerfile": "In your case I would prepare a new plugins folder together with a (possibly generated or handcrafted) script that installs them. Add the real plugins folder to .dockerignore. Then you copy in the new folder and run the install script. This also reduces the size of the context you upload to docker before the build starts. You are hand picking the dependencies anyway, so doing the work beforehand (before you build) should work fine.\nIn the build system you you should do something like:\ncollect_plugins.sh # Creates the new plugin folder and install script\ndocker build <params>\nThe two layers could then be:\nCOPY plugins_selected/ /usr/lib/myapp/plugins/\nRUN  /usr/lib/myapp/plugins/install.sh\nIf you prepare the context you are sending to docker it will make the Dockerfile a lot more simple (a good thing). We are simply solving the problem before build.\nNormally dependencies are fetched over the network using a package manager or simply by downloading them over http(s). Copying them into the build context like you are doing is not necessarily wrong, but it gets a bit more awkward.\nLet's look at how docker processes the Dockerfile (slightly simiplified).\nWhen you build, docker will upload all the files in the context to docker engine (except the paths mentioned in .dockerignore) and start processing the Dockerfile. The purpose is to produce filesystem layers that will represent the final image.\nWhen you do operations like RUN, docker actually starts a container to execute the command(s) then adds the resulting layer to the image. The only thing that will actually run in production is what you specify in CMD and or ENTRYPOINT at the end of the Dockerfile.\nTry to create as few layers as possible.\nThe dockerfile best practices guide covers the basics.",
    "Handle EF Core migrations with container orchestration": "I'm not sure, if this is best practice, but you can trigger schema upgrade from your C# code. You must add following code to Configure method at Startup.cs:\nusing (var serviceScope = app.ApplicationServices\n    .GetRequiredService<IServiceScopeFactory>()\n    .CreateScope())\n{\n    using (var context = serviceScope.ServiceProvider.GetService<EasyAdminContext>())\n    {\n        context.Database.Migrate();\n    }\n}\nThis way the database will be updated when the application starts",
    "How to shrink size of Docker image with NodeJs": "You can certainly use my alpine-ng image if you like.\nYou can also check out the dockerfile, if you want to try and modify it in some way.\nI regret to inform you that even based on alpine, it is still 610MB. An improvement to be sure, but there is no getting around the fact that the angular compiler is grossly huge.",
    "Docker - How to get the name (user/repo:tag) of the base image used to build another image": "When you just have an image and want to recreate the Dockerfile, you can use dockerfile-from-image from\nhttps://github.com/CenturyLinkLabs/dockerfile-from-image/blob/master/dockerfile-from-image.rb\nit is some Ruby code (in a container, of course!) that find all the commands used.\nAnd, yes f35... is the id of DEBIAN:WHEEZY, as\ndocker run -v /var/run/docker.sock:/var/run/docker.sock   centurylink/dockerfile-from-image debian:wheezy\nshows\nADD file:f35a56605b9a065a14a18d0e36fdf55c1c381d3521b4fa7f11173f0025d36839 in /\nCMD [\"/bin/bash\"] `",
    "Create a user in a Dockerfile : Option d is ambiguous": "It should be RUN adduser --disabled-login myuser (or --disabled-password)",
    "Heroku run Docker image with port parameter": "The question is quite old now, but still I will write my answer here if it can be of some help to others.\nI have spring-boot App along with swagger-ui Dockerized and deployed on Heroku.\nThis is my application.yml looks like:\nserver:\n  port: ${PORT:8080}\n  forward-headers-strategy: framework\n  servlet:\n   contextPath: /my-app\n\nspringdoc:\n  swagger-ui:\n    path: '/swagger-ui.html'\nBelow is my DockerFile configuration.\nFROM maven:3.5-jdk-8 as maven_build\nWORKDIR /app\n\nCOPY pom.xml .\nRUN mvn clean package -Dmaven.main.skip -Dmaven.test.skip && rm -r target\n\nCOPY src ./src\nRUN mvn package spring-boot:repackage\n\n########run stage########\nFROM openjdk:8-jdk-alpine\nWORKDIR /app\nRUN apk add --no-cache bash\n\n\nCOPY --from=maven_build /app/target/springapp-1.1.1.jar ./\n\n#run the app\n# 256m was necessary for me, as I am using free version so Heroku was giving me memory quota limit exception therefore, I restricted the limit to 256m\nENV JAVA_OPTS \"-Xmx256m\"\nENTRYPOINT  [\"java\",\"${JAVA_OPTS}\", \"-jar\",\"-Dserver.port=${PORT}\", \"springapp-1.1.1.jar\"]\nThe commands I used to create the heroku app:\nheroku create\nheroku stack:set container\nThe commands I used to build image and deploy:\ndocker build -t app-image .\nheroku container:push web\nheroku container:release web\nFinally make sure on Heroku Dashboard the dyno information looks like this:\nweb java \\$\\{JAVA_OPTS\\} -jar -Dserver.port\\=\\$\\{PORT\\} springapp-1.1.1.jar\nAfter all these steps, I was able to access the swagger-ui via\nhttps://testapp.herokuapp.com/my-app/swagger-ui.html",
    "Docker: COPY failed: CreateFile, looking for file in strange location": "Seems to me the standard Dockerfile that comes with a new solution is bugged :/\nI moved the Dockerfile up to the solution folder, from PowerShell:\nmv Dockerfile ../Dockerfile\nRunning the docker build command from there did the trick for me...",
    "Deploy docker images to heroku": "Login in the Docker Registry before pushing\nheroku container:login",
    "How to start Nginx server within alpine image using rc-service command": "After debugging and lots of trial and error, I found a perfect solution at least for me, David Maze answer is very helpful, But I need to be able to restart the nginx service within the container.\nwhatever when I start my container using the following command :\ndocker run -it -p 80:80 -p 443:443 alpine:3.12.0\nwe access the container shell, and we need to download nginx server , and openrc to be able to use rc-service command line.\n/ # apk update\n/ # apk add nginx openrc\n#ANSWER 1\nnow we'll test If there is an error in nginx server by using the following command :\n/ # nginx -t\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok\nnginx: [emerg] open() \"/run/nginx/nginx.pid\" failed (2: No such file or directory)\nnginx: configuration file /etc/nginx/nginx.conf test failed\nas you can see from the output that we get, It tells you that a missing file or directory shall be created, so let's create that directory :\n/ # ls -la run/\ntotal 8\ndrwxr-xr-x    2 root     root          4096 Dec 16 10:31 .\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:12 ..\n\n/ # mkdir /run/nginx/\nand we give that directory that we created some permissions :\n/ # chown -R nginx:nginx /run/nginx/\n/ # chmod 775 /run/nginx/\n/ # ls -la /run/\ntotal 12\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:15 .\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:12 ..\ndrwxrwxr-x    2 nginx    nginx         4096 Jan 16 08:15 nginx\nnow we are good with nginx :\n/ # nginx -t\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok\nnginx: configuration file /etc/nginx/nginx.conf test is successful\nLet's test our nginx service is it started or not with rc-service command :\n/ # rc-service nginx status\n * You are attempting to run an openrc service on a\n * system which openrc did not boot.\n * You may be inside a chroot or you may have used\n * another initialization system to boot this system.\n * In this situation, you will get unpredictable results!\n * If you really want to do this, issue the following command:\n * touch /run/openrc/softlevel\nSo from the above output we see that we have two problems, openrc did not boot, and there is a missing file softlevel :\n/ # ls -la /run/\ntotal 12\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:15 .\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:12 ..\ndrwxrwxr-x    2 nginx    nginx         4096 Jan 16 08:22 nginx\nLet's start by booting our system with openrc simply by typing it itself :\n/ # openrc\n * Caching service dependencies ...\nService `hwdrivers' needs non existent service `dev'\n\n/ # ls -la /run/\ntotal 16\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:29 .\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:12 ..\ndrwxrwxr-x    2 nginx    nginx         4096 Jan 16 08:22 nginx\ndrwxr-xr-x   14 root     root          4096 Jan 16 08:29 openrc\n\n/ # ls -la /run/openrc/\ntotal 64\ndrwxr-xr-x   14 root     root          4096 Jan 16 08:29 .\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:29 ..\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 daemons\n-rw-r--r--    1 root     root            11 Jan 16 08:29 depconfig\n-rw-r--r--    1 root     root          2895 Jan 16 08:29 deptree\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 exclusive\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 failed\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 hotplugged\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 inactive\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 options\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 scheduled\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 started\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 starting\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 stopping\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 tmp\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 wasinactive\nnow we create the missing file :\n/ # touch /run/openrc/softlevel\nnow our rc-service command works perfectly :\n/ # rc-service nginx status\n * status: stopped\nlet's start our server :\n / # rc-service nginx start\n * Starting nginx ...          [ ok ]\ncheck if it is started or not :\n/ # rc-service nginx status\n * status: started\n#ANSWER 2\nOr you can simply call those two command lines instead :\n/ # openrc\n * Caching service dependencies ...\nService `hwdrivers' needs non existent service `dev'    [ ok ]\n\n/ # touch /run/openrc/softlevel\nNow you can start your nginx server :)\n/ # rc-service nginx status\n * status: stopped\n\n/ # rc-service nginx start\n * /run/nginx: creating directory\n * /run/nginx: correcting owner\n * Starting nginx ...         [ ok ]\n\n/ # rc-service nginx status\n * status: started\nHope I was clear.",
    "Docker: Installing python cryptography on alpine linux distribution": "Alpine is a headache distro for most Python packages that ship C/C++ extensions (code written in C/C++ that is compiled to a shared object and loaded in Python via a foreign function library). The reason for that is that is PEP 513 which portability definition between Linux distros, manylinux1, is based on glibc/glibcxx. Since Alpine uses musl libc, no manylinux1 compatible wheel can be installed on Alpine. So when you issue pip install cryptography, the wheel with the compiled extensions is filtered and pip tries to build the package with all the C extensions from source.\ninstalling with the system package manager\n\nThis is the preferred way and was mentioned by @GracefulRestart in the comments; use it if you don't need the bleeding edge version of the package. Install it with apk:\n$ apk add py-cryptography\ninstalling with pip\n\nShould you need the bleeding edge version, you can try building it from source by installing with pip.\nPreparing the build environment\n\nYou will need the compiler and libraries with header files: musl, OpenSSL, libffi and Python itself:\n$ apk add gcc musl-dev libffi-dev openssl-dev python3-dev\n\nBuilding\n\n$ pip install pkgname\nhides the build log by default. To see the complete build log, add -vvv to increase verbosity. (Optional) Also, you can explicitly prohibit installing manylinux1 wheels by adding -\n-no-binary=pkgname\nso the build from source will be enforced.\n$ pip install cryptography -vvv --no-binary=cryptography",
    "Run python mysql client on slim python 3.6 docker image": "I'm using python:3.7-slim and using the following command\nRUN apt-get -y install default-libmysqlclient-dev",
    "Docker compose up keep replace existing container": "Docker compose associates the container with the project name (default directory name) and the service name or container_name if specified. Thus in case both branches have the compose file under the same directory name, and thus the compose files will be interpreted as refering to the same container, which will lead to the container being recreated.\nTo avoid this situation, you can the --project-name option to override the default one (directory name).\ndocker-compose --project-name branch1 up -d\ndocker-compose --project-name branch2 up -d\nIn this case both containers will be created.\nBut note that if both compose files have the same container_name set, there will be a conflict and the second container creation will fail. To avoid that, either use different container names, or remove the container_name property, to get the default container name which is <project_name>_<service_name>_1",
    "Getting an \"operation not supported\" error when trying to RUN something while building a docker image": "Well, turns out that if your machine had a kernel update but you didn't restart yet then docker freaks out. Restarting the machine fixed that. Oops.",
    "Next.JS v13 in Docker does not respect path alias but works locally": "Found the issue.\nThe solution was to update next.config.js to set the path alias in webpack. This should be the same as your tsconfig.json.\nconst nextConfig = {\n  output: 'standalone',\n  webpack: (config, { isServer }) => {\n    config.resolve.alias['~'] = path.join(__dirname, 'src');\n    return config;\n  },\n};\nHere's my hypothesis: in Next.js 13, they moved to turbopack but still use Webpack as a bundler. Webpack runs at a different phase than the TS compilation phase, so it doesn't take into account tsconfig.json.",
    "failed to start daemon: pid file found, ensure docker is not running or delete /var/run/docker.pid": "Delete the .pid file using the below Linux command,\nrm /var/run/docker.pid\nNow the pid file will get deleted and the docker daemon can be launched newly.",
    "Dockerfile vs Docker image": "Using a Dockerfile:\nYou have an 'audit log' that describes how the image is built. For me this is fundamental if it is going to be used in a production pipeline where more people are working and maintainability should be a priority.\nYou can automate the building process of your image, being an easy way of updating the container with system updates, or if it has to take part in a continuous delivery pipeline.\nIt is a cleaner way of create the layers of your container (each Dockerfile command is a different layer)\nChanging a container and committing the changes is great for testing purposes and for fast development for a conceptual test. But if you plan to use the result image for some time, I would definitely use Dockerfiles.\nApart from this, if you have to modify a file and doing it using bash tools (awk, sed...) results very tedious, you can add any file you wish from outside during the building process.",
    "Unable to start postgres docker container from docker-compose": "The problem caused because the volume which your compose created to store your database still keep old data which initiated by PostgreSQL 9.6. That volume name is postgres-data which created when you use named volume on your docker-compose.yml. So simply to get rid of this, you can use some ways below:\nUsing docker-compose command:\nRun docker-compose down -v, this will stop all your container inside that compose and remove all named volume inside that compose.\nYou could take a look at docker-compose down command\nUsing docker volume command:\nRun docker volume ls to get list of current volumes on your machine, I think you will see your volume on that list too:\nDRIVER              VOLUME NAME\nlocal               postgres-data\nRun docker volume rm postgres-data to remove that volume, if your container still running and you couldn't remove it then you can use -f to force remove it\nHope that helps!",
    "Dockerfile build remove source code from final image": "You can do a multi-stage build and copy the artifacts on a new image from the previous one. Also install any required runtime dependencies (if any).\nFROM alpine AS builder\n\nRUN apk add --no-cache <build_dependencies>\n\nCOPY source_code /tmp/source_code\n\nRUN make -C /tmp/source_code && \\\n        mkdir /libraries/\n        cp /tmp/lib/* /libraries/\n        rm -rf /tmp/*\n\nFROM alpine\n\nRUN apk add --no-cache <runtime_dependencies>\n\nCOPY --from=builder /libraries/ /libraries/",
    "How to run cron job in docker container?": "Crontab requires additional field: user, who runs the command:\n* * * * * root python3 /code/populatePDBbackground.py >> /var/log/cron.log\n# Empty line\nThe Dockerfile is:\nFROM python:3\nRUN apt-get -y update && apt-get -y upgrade\nRUN apt-get install -y cron postgresql-client\nRUN touch /var/log/cron.log\nRUN mkdir /code\nWORKDIR /code\nADD . /code/\nCOPY crontab /etc/cron.d/cjob\nRUN chmod 0644 /etc/cron.d/cjob\nENV PYTHONUNBUFFERED 1\nCMD cron -f\nTest python script populatePDBbackground.py is:\nfrom datetime import datetime\n\nprint('Script has been started at {}'.format(datetime.now()))\nAnd finally we get:\n$ docker run -d b3fa191e8822\nb8e768b4159637673f3dc4d1d91557b374670f4a46c921e0b02ea7028f40e105\n\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES\nb8e768b41596        b3fa191e8822        \"/bin/sh -c 'cron -f'\"   4 seconds ago       Up 3 seconds                            cocky_beaver\n\n$ docker exec -ti b8e768b41596 bash\nroot@b8e768b41596:/code# tail -f /var/log/cron.log\nScript has been started at 2019-03-13 00:06:01.095013\nScript has been started at 2019-03-13 00:07:01.253030\nScript has been started at 2019-03-13 00:08:01.273926",
    "\"Exec format error\" with docker run command": "The \"Exec format error\" was simply because I was copying the binary file built on OSX/MacOS into the Docker image and trying to run that binary file in the Linux container. That don't work.\nHere is the Dockerfile that worked for me:\nFROM golang:latest\n\nRUN mkdir -p /app\n\nWORKDIR /app\n\nCOPY . .\n\nENV GOPATH /app\n\nRUN go install huru\n\nENTRYPOINT /app/bin/huru\nand my project structure like so on my host fs:\n$GOPATH/\n      src/\n        huru/\n      .dockerignore\n      Dockerfile\nI run:\ndocker build -t foo .\ndocker run foo\nmy .dockerignore file contains:\n.vscode\nbin\npkg",
    "Dockerfile: create ENV variable that a USER can see?": "Any user can see the environment variables:\n$ cat Dockerfile\nFROM debian\n\nENV foo bar\nRUN groupadd -r am && useradd -r -g am am\nUSER am\n$ docker build -t test .\n...\n$ docker run test bash -c 'echo $foo'\nbar\nSo that's not what the problem is. It may be that your process forked a new environment, but I can't be sure as you haven't shared how you're checking the value.",
    "standard_init_linux.go:211: exec user process caused \"no such file or directory\"?": "The \"shebang\" line at the start of a script says what interpreter to use to run it. In your case, your script has specified #!/bin/bash, but Alpine-based Docker images don't typically include GNU bash; instead, they have a more minimal /bin/sh that includes just the functionality in the POSIX shell specification.\nYour script isn't using any of the non-standard bash extensions, so you can just change the start of the script to\n#!/bin/sh",
    "Getting ssh-keygen in Alpine docker": "Thanks to @PrasadK - which nudged me along, the answer to Node- Red new Projects feature since version 0.18.3 - in order to have a remote repo - using this function in Node-Red Projects, the underlying docker image requires ssh-keygen. Do this in the Dockerfile with:\n......\nRUN   apk update && \\\n      apk add --no-cache \\\n      openssh-keygen\n......",
    "How to create an empty file in a scratch container?": "There are no commands in scratch to run. The only options you have from scratch are COPY and ADD. And you can only copy or add files from the context (unless you want to ADD from a remote url which I wouldn't recommend). So you are left to create an empty file in your context and copy that.\nAnd then docker introduced multi stage builds, which let's you use another build as your context. So you can make an empty file in one stage and copy it to the other.\nFROM busybox AS build-env\nRUN touch /empty\n\nFROM scratch\nCOPY --from=build-env /empty /.emptyfile",
    "Building Docker image for Node application using Yarn dependency": "You should first run yarn install to generate a yarn lockfile (yarn.lock) before building the image. Then make sure to copy it along with the package.json. Your dockerfile should look like this :\nFROM node:7 \nWORKDIR /app \nCOPY package.json /app \nCOPY yarn.lock /app\nRUN yarn install \nCOPY . /app \nCMD npm run develop \nEXPOSE 8000\nWith this all dependencies should install successfully when building your image",
    "RUN echo -e \"deb http ... \" prepares a wrong contents of the target file": "Try using the bash shell for the echo command instead. Docker by default uses the sh shell and for some reason it doesn't like the -e characters and had echo'ed it as part of its input.\nTo fix this;\nUpdate to:\nRUN bash -c 'echo -e \"deb http://nginx.org/packages/mainline/ubuntu/ xenial nginx\\ndeb-src http://nginx.org/packages/mainline/ubuntu/ xenial nginx\" | tee /etc/apt/sources.list.d/nginx.list'\nOutputs:\nroot@87944d07f493:/etc/apt/sources.list.d# cat nginx.list deb http://nginx.org/packages/mainline/ubuntu/ xenial nginx deb-src http://nginx.org/packages/mainline/ubuntu/ xenial nginx\nYou can replicate this \"docker problem\" by switching to sh on your current terminal session and then do the same echo command again.",
    "Docker Official Tomcat Image Modify Server.xml and add jar to lib folder": "The OP is old, but I came across it on google searching for a way to update the server.xml from the official docker image for tomcat. I would like to add my solution.\nUse Case: My company has static html files that are generated from another application (running under Websphere Liberty) and saved to a shared directory. The shared directory is also mounted in our Tomcat container, but is not mounted in any of the web application directories, so tomcat does not have access to it by default. The official Tomcat docker image contains a default server.xml file, so I need to update this file to specify additional directories where Tomcat can search for content.\nInitial Thoughts: Initially, I thought it would be best to write a bash script that simply applied my changes to the server.xml without overwriting the entire file. After contemplating this, I came to the conclusion if the default server.xml file changed (due to an image update) then unexpected behavior could occur which would require updates to the bash script, therefore, I dropped this idea.\nSolution: Make a copy of the image's server.xml file, make my changes and add a COPY directive in the Dockerfile to overwrite the default server.xml with my changes. If the image is updated and the default server.xml file changes, then I will need to update my copy as well, but I would much rather do that than introduce more code to our repo.\nDockerfile:\nFROM tomcat:7-jre8\nBuild and run official Tomcat image:\ndocker build -t tomcat:official .\nCopy the server.xml to the local filesystem:\ndocker run tomcat:official tar -c -C /usr/local/tomcat/conf server.xml | tar x\nMake changes to the local copy of the server.xml file and make sure that it's in the same directory as the Dockerfile.\nIn my case, I had to add the following line between the \"Host\" tags\n<Context docBase=\"/mnt/html/Auto/Reports\" path=\"/Auto/Reports\" />\nUpdate the Dockerfile:\nFROM tomcat:7-jre8\nCOPY server.xml /usr/local/tomcat/conf/\nNotes: My company is currently using the 'tomcat:7-jre8' version of the Tomcat image, but a list of all versions can be found here.\nHopfully this helps someone else.",
    "DockerFile one-line vs multi-line instruction [duplicate]": "In this specific case it is important to put apt-get update and apt-get install together. More broadly, fewer layers is considered \"better\" but it almost never has a perceptible difference.\nIn practice I tend to group together \"related\" commands into the same RUN command. If I need to configure and install a package from source, that can get grouped together, and even if I change make arguments I don't mind re-running configure. If I need to configure and install three packages, they'd go into separate RUN lines.\nThe important difference in this specific apt-get example is around layer caching. Let's say your Dockerfile has\nFROM ubuntu:18.04\nRUN apt-get update\nRUN apt-get install package-a\nIf you run docker build a second time, it will decide it's already run all three of these commands and the input hasn't changed, so it will run very quickly and you'll get an identical image out.\nNow you come back a day or two later and realize you were missing something, so you change\nFROM ubuntu:18.04\nRUN apt-get update\nRUN apt-get install package-a package-b\nWhen you run docker build again, Docker decides it's already run apt-get update and can jump straight to the apt-get install line. In this specific case you'll have trouble: Debian and Ubuntu update their repositories fairly frequently, and when they do the old versions of packages get deleted. So your apt-get update from two days ago points at a package that no longer exists, and your build will fail.\nYou'll avoid this specific problem by always putting the two apt-get commands together in the same docker run line\nFROM ubuntu:18.04\nRUN apt-get update \\\n && DEBIAN_FRONTEND=noninteractive \\\n    apt-get install --assume-yes --no-install-recommends \\\n      package-a \\\n      package-b",
    "Dockerfile doesn't seem to run while debugging in Visual Studio 2019": "This is apparently by design as a \"Fast mode\" optimization in Visual Studio 2019. See the documentation for debugging in containers here.\nWhat it states is that \"Fast mode\" is the default behavior when debugging containers in VS 2019. In this mode, only the first stage (base) of a multi-stage build is built according to the Dockerfile. VS then handles the rest on the host machine, ignoring the Dockerfile, and shares the output to the container by using volume mounting. This means that any custom steps you add to other stages will be ignored when using the Debug configuration in VS 2019. (The reason given for this non-obvious, and therefore potentially frustrating, optimization is that builds are much slower in a container than on the local machine.) Note that this optimization only happens when using the Debug configuration. The Release configuration will use the entire Dockerfile.\nYour options are:\nPlace your custom steps in the first (base) step of the Dockerfile.\nor\nDisable this optimization by editing the project file like this:\n<PropertyGroup>\n   <ContainerDevelopmentMode>Regular</ContainerDevelopmentMode>\n</PropertyGroup>\nAlso keep in mind that it will try to reuse a previously built container if possible, so you may need to perform a Clean or Rebuild in order to force the build to create a new version of the container.\nGood luck!\n** EDIT **\nIt seems that there is an issue when trying to use the ContainerDevelopmentMode flag after Container Orchestration Support (in this case, Docker Compose) is added. See this issue. It is suggested in the issue discussion that this flag could be used on the docker-compose.dcproj file, but there is a bug (still not fixed) that keeps that approach from working.\nA third option, hinted at in my previous answer but not made explicit, would be:\nSwitch your solution configuration from Debug to Release.\nThis works, but clearly isn't ideal when you're trying to debug your application.",
    "Difference between --cache-to/from and --mount type=cache in docker buildx build": "They are solving two different problems.\n--cache-to/from is used to store the result of a build step and reuse it in future builds, avoiding the need to run the command again. This is stored in a persistent location outside of the builder, like on a registry, so that other builders can skip already completed steps of a build even if the image wasn't built on the local system.\n--mount type=cache creates a mount inside the temporary container that's executed in a RUN step. This mount is reused in later executions of the build when the step itself is not cached. This is useful when a step pulls down a lot of external dependencies that do not need to be in the image and can safely be reused between builds. The storage of the mount cache is local to the builder and is an empty directory on first use.",
    "Conditionally mount volumes in docker-compose for several conditions": "You can set defaults for environment variable in a .env-file shipped alongside with a docker-compose.yml [1].\nBy setting your environment variables to /dev/null by default and then handling this case in the containerized application, you should be able to achieve what you need.\nExample\n$ tree -a\n.\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 .env\n\u2514\u2500\u2500 run.sh\ndocker-compose.yml\nversion: \"3\"\n\nservices:\n  test:\n    build: .\n    environment:\n      - VOL_DST=${VOL_DST}\n    volumes:\n      - \"${VOL_SRC}:${VOL_DST}\"\nDockerfile\nFROM alpine\nCOPY run.sh /run.sh\nENTRYPOINT [\"/run.sh\"]\n.env\nVOL_SRC=/dev/null\nVOL_DST=/volume\nrun.sh\n#!/usr/bin/env sh\nset -euo pipefail\n\nif [ ! -d ${VOL_DST} ]; then\n  echo \"${VOL_DST} not mounted\"\nelse\n  echo \"${VOL_DST} mounted\"\nfi\nTesting\nEnvironment variable VOL_SRC not defined:\n$ docker-compose up\nStarting test_test_1 ... done\nAttaching to test_test_1\ntest_1  | /volume not mounted\ntest_test_1 exited with code 0\nEnvironment variable VOL_SRC defined:\n$ VOL_SRC=\"./\" docker-compose up\nRecreating test_test_1 ... done\nAttaching to test_test_1\ntest_1  | /volume mounted\n[1] https://docs.docker.com/compose/environment-variables/#the-env-file",
    "How to navigate up one folder in a dockerfile": "TL;DR\nrun it from the root directory:\ndocker build . -f ./path/to/dockerfile\nthe long answer:\nin dockerfile you cant really go up.\nwhy\nwhen the docker daemon is building you image, it uses 2 parameters:\nyour Dockerfile\nthe context\nthe context is what you refer to as . in the dockerfile. (for example as COPY . /app) both of them affect the final image - the dockerfile determines what is going to happen. the context tells docker on which files it should perform the operations you've specified in that dockerfile.\nthats how the docs put it:\nA build\u2019s context is the set of files located in the specified PATH or URL. The build process can refer to any of the files in the context. For example, your build can use a COPY instruction to reference a file in the context.\nso, usually the context is the directory where the Dockerfile is placed. my suggestion is to leave it where it belongs. name your dockerfiles after their role (Dockerfile.dev,Dockerfile.prod, etc) thats ok to have a few of them in the same dir.\nthe context can still be changed:\nafter all, you are the one that specify the context. since the docker build command accepts the context and the dockerfile path. when i run:\ndocker build .\ni am actually giving it the context of my current directory, (ive omitted the dockerfile path so it defaults to PATH/Dockerfile)\nso if you have a dockerfile in dockerfiles/Dockerfile.dev, you shoul place youself in the directory you want as context, and you run:\ndocker build . -f dockerfiles/Dockerfile.dev\nsame applies to docker-compose build section (you specify there a context and the dockerfile path)\nhope that made sense.",
    "Docker variable expansion in compose entrypoint": "Variables defined in the compose environment section are passed to the container, but not used by docker-compose itself to parse your yml file. The variables in the yml file are expanded with your host shell's environment (the shell where you run the docker-compose up command from) and/or the .env file contents.\nSince you are running the entrypoint with the shell syntax, you can have the shell inside the container expand the variables instead of having docker-compose do it, by escaping the variables:\nentrypoint: \"java $${JAVA_OPTS} -Xmx$${javaMemoryLimit} -jar /app.jar\"\nYou may need to add a /bin/sh to parse those variables:\nentrypoint: \"/bin/sh -c \\\"java $${JAVA_OPTS} -Xmx$${javaMemoryLimit} -jar /app.jar\\\"\"",
    "How happens when Linux distributions are different between the docker host and the docker image?": "Docker does not use LXC (not since Docker 0.9) but libcontainer (now runc), a built-in execution driver which manipulates namespaces, control groups, capabilities, apparmor profiles, network interfaces and firewalling rules \u2013 all in a consistent and predictable way, and without depending on LXC or any other userland package.\nA docker image represents a set of files winch will run as a container in their own memory and disk and user space, while accessing the host kernel.\nThis differs from a VM, which does not access the host kernel but includes its own hardware/software stack through its hypervisor.\nA container has just to set limits (disk, memory, cpu) in the host. An actual VM has to build an entire new host.\nThat docker image (group of files) can be anything, as long as:\nit does not depends on host libraries (since it is isolated in its own disk space, it does not have access to hosts files, unless volumes are mounted)\nit does only system calls: see \"What is meant by shared kernel in Docker?\"\nThat means an image can be anything: another linux distro, or even a single executable file. Any executable compile in go (https://golang.org/) for instance, could be packaged in its own docker image without any linux distro:\nFROM scratch\nCOPY my_go_exe /\nENTRYPOINT /my_go_exe\nscratch is the \"empty\" image, and a go executable is statically linked, so it is self-contained and only depends on system calls to the kernel.",
    "Serving multiple tensorflow models using docker": "I ran into this double slash issue for git bash on windows.\nAs such I am passing the argument, mentioned by @KrisR89, in via command in the docker-compose.\nThe new docker-compose looks like this and works with the supplied dockerfile:\nversion: '3'\n\nservices:\n  serving:\n    build: .\n    image: testing-models\n    container_name: tf\n    command: --model_config_file=/config/config.conf",
    "Configure dockerfile with postgres": "When building your docker image postgres is not running. Database is started when container is starting, any sql files can be executed after that. Easiest solution is to put your sql files into special directory:\nFROM postgres:9.4\nCOPY *.sql /docker-entrypoint-initdb.d/\nWhen booting startup script will execute all files from this dir. You can read about this in docs https://hub.docker.com/_/postgres/ in section How to extend this image.\nAlso, if you need different user you should set environment variables POSTGRES_USER and POSTGRES_PASSWORD. It's easier then using custom scripts for creating user.",
    "How to copy azure pipeline artifacts to a docker image which is microsoft dotnetcore runtime image": "Publishing to $(Build.ArtifactStagingDirectory) is fine. There are several ways to do it, so here's one:\nAdd a Docker task after the publish, configured to \"Build\"\nSet the \"Build Context\" in the task to $(Build.ArtifactStagingDirectory). That's the root path Docker will use for commands like COPY in a Dockerfile.\nCommit a Dockerfile to your repo, and set the Dockerfile path in the task to match its location\nSet up the Dockerfile like this (I'm assuming .NET Core 2.2 here):\nFROM mcr.microsoft.com/dotnet/core/aspnet:2.2\nWORKDIR /app\nCOPY . .\nENTRYPOINT [\"dotnet\", \"myAppNameHere.dll\"]\nBecause you've set the Docker Build Context to $(Build.ArtifactStagingDirectory), where your app has been published, the COPY command will use that as a \"current working directory.\" The translation of the COPY is \"copy everything in $(Build.ArtifactStagingDirectory) to the /app folder inside the container.\"\nThat'll get you a basic Docker container that simply contains your pre-built and published app files.",
    "How to use an environment variable from a docker-compose.yml in a Dockerfile?": "There are two different time frames to understand. Building an image is separate from running the container. The first part uses the Dockerfile to create your image. And the second part takes the resulting image and all of the settings (e.g. environment variables) to create a container.\nInside the Dockerfile, the RUN line occurs at build time. If you want to pass a parameter into the build, you can use a build arg. Your Dockerfile would look like:\nFROM node:7\nARG GITLAB_USER=default_user_name\n# ENV is optional, without it the variable only exists at build time\n# ENV GITLAB_USER=${GITLAB_USER}\nRUN echo '${GITLAB_USER}' \nAnd your docker-compose.yml file would look like:\nversion: '2'\nservices:\n  myapp:\n    build:\n      context: .\n      args:\n      - GITLAB_USER=${GITLAB_USER}\nThe ${GITLAB_USER} value inside the yml file will be replaced with the value set inside your .env file.",
    "How make Docker find referenced projects on build?": "Is complicated to explaind, why is not posible to build Dockerfile from same project space, then I read this article [blog]: https://josiahmortenson.dev/blog/2020-06-08-aspnetcore-docker-https and finaly understud how I need to run the build.\nIn my case:\n/src\n .sln\n /project1\n   project1.csproj\n   Dockerfile\n /project2\n   project2.csproj\n   Dockerfile\n /reference1\n   reference1.csproj\n /reference2\n   reference2.csproj\nIs necesary to move to /src level for run command\ndocker build -t {nametagedcontainer} -f project1/Dockerfile .\nFinally\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS c\nWORKDIR /app   \n\n    \nCOPY [\"project1/project1.csproj\", \"project1/\"]\nCOPY [\"reference1/reference1.csproj\", \"reference1/\"]\n#others reference if necesary\n\n\n\nRUN dotnet restore \"project1/project1.csproj\" \n\nCOPY . .\nWORKDIR \"/app/project1\"\nRUN dotnet build \"project1.csproj\" -c Release -o /app/build\n\nfrom c as publish\nRUN dotnet publish \"project1.csproj\" -c Release -o /app/publish\n \n\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 AS base\nWORKDIR /app \nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"{dllname}.dll\"]",
    "How to pass Java_opts before an executable to entrypoint in dockerfile?": "You can declare environment variables with ENV:\n...\nADD --chown=daemon:daemon UserPrefExporter.sc /opt/docker\nUSER daemon\nENV JAVA_OPTS=\"-Xmx<limit>\"\nENTRYPOINT [\"./amm\", \"-h\", \"amm_home\", \"UserPrefExporter.sc\"]",
    "Docker run container on different port": "Please try:\ndocker run -p 70:80 kitematic/hello-world-nginx\nbinding port is probably mistyped in the command you provided.",
    "What is the location of config file in Docker 1.9?": "It sounds like you have moved from the CentOS distributed docker to the docker.com docker-engine packages as CentOS hasn't moved to 1.9 yet.\nThe CentOS packages will make use of the /etc/sysconfig standard. Dockers packages will not. You will also miss out on the docker-storage-setup program RedHat built to deal with their unique storage requirements.\nThe systemd config in /usr/lib/systemd/system/docker.service will show you the difference:\nDocker\n[Service]\nType=notify\nExecStart=/usr/bin/docker daemon -H fd://\nMountFlags=slave\nLimitNOFILE=1048576\nLimitNPROC=1048576\nCentOS\n[Service]\nType=notify\nEnvironmentFile=-/etc/sysconfig/docker\nEnvironmentFile=-/etc/sysconfig/docker-storage\nEnvironmentFile=-/etc/sysconfig/docker-network\nEnvironment=GOTRACEBACK=crash\nExecStart=/usr/bin/docker daemon $OPTIONS \\\n          $DOCKER_STORAGE_OPTIONS \\\n          $DOCKER_NETWORK_OPTIONS \\\n          $ADD_REGISTRY \\\n          $BLOCK_REGISTRY \\\n          $INSECURE_REGISTRY\nLimitNOFILE=1048576\nLimitNPROC=1048576\nLimitCORE=infinity\nMountFlags=slave\nTimeoutStartSec=1min\nRestart=on-failure\nSo then, how do I configure the docker-engine installed docker service?\nDocker have a page on setting up the systemd config. You can go either CentOS or Docker with your config\nDocker - edit the docker-engine systemd config file /usr/lib/systemd/system/docker.service with your required options\nCentos - copy the EnvironmentFile setup and then configure your options in /etc/sysconfig/docker.",
    "How to COPY files of current directory to folder in Dockerfile": "you can start a container and check.\n$ docker run -ti --rm <DOCKER_IMAGE> sh\n$ ls -l /this/folder\nIf your docker image has ENTRYPOINT setting, then run below command:\n$ docker run -ti --rm --entrypoint sh <DOCKER_IMAGE>\n$ ls -l /this/folder",
    "How to cache Python dependencies properly": "The --download-cache option was removed in pip version 8, because it's now using cache by default. So you don't need to specify this option at all. I'm not sure what the purpose of the pip download -d <dir> option is, but apparently it's not creating a cache in the destination directory. You can just leave out the -d <dir> option too. The following Dockerfile works:\nFROM python:3.7\nCOPY constraints.txt requirements.txt ./\nRUN pip3 download -d .pipcache -r requirements.txt -c constraints.txt\nCOPY test.txt ./\nRUN pip3 install -r requirements.txt -c constraints.txt\nIf you add --cache-dir <dir> to both the download and install commands, it will work as well. So the following Dockerfile also works:\nFROM python:3.7\nCOPY constraints.txt requirements.txt ./\nRUN pip3 download --cache-dir ./tmp/pipcache -r requirements.txt -c constraints.txt\nCOPY test.txt ./\nRUN pip3 install --cache-dir ./tmp/pipcache -r requirements.txt -c constraints.txt\nExample output (with only pep8 and pylint in the requirements.txt):\nFirst run:\nSending build context to Docker daemon  5.632kB\nStep 1/5 : FROM python:3.7\n ---> a4cc999cf2aa\nStep 2/5 : COPY constraints.txt requirements.txt ./\n ---> 411eaa3d36ff\nStep 3/5 : RUN pip3 download -r requirements.txt -c constraints.txt\n ---> Running in 6b489df74137\nCollecting pep8==1.7.1 (from -c constraints.txt (line 17))\n  Downloading https://files.pythonhosted.org/packages/42/3f/669429ce58de2c22d8d2c542752e137ec4b9885fff398d3eceb1a7f5acb4/pep8-1.7.1-py2.py3-none-any.whl (41kB)\n  Saved /pep8-1.7.1-py2.py3-none-any.whl\nCollecting pylint==2.3.1 (from -c constraints.txt (line 22))\n  Downloading https://files.pythonhosted.org/packages/60/c2/b3f73f4ac008bef6e75bca4992f3963b3f85942e0277237721ef1c151f0d/pylint-2.3.1-py3-none-any.whl (765kB)\n  Saved /pylint-2.3.1-py3-none-any.whl\nCollecting mccabe==0.6.1 (from -c constraints.txt (line 14))\n  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n  Saved /mccabe-0.6.1-py2.py3-none-any.whl\nCollecting astroid==2.2.5 (from -c constraints.txt (line 2))\n  Downloading https://files.pythonhosted.org/packages/d5/ad/7221a62a2dbce5c3b8c57fd18e1052c7331adc19b3f27f1561aa6e620db2/astroid-2.2.5-py3-none-any.whl (193kB)\n  Saved /astroid-2.2.5-py3-none-any.whl\nCollecting isort==4.3.19 (from -c constraints.txt (line 10))\n  Downloading https://files.pythonhosted.org/packages/ae/ae/5ef4b57e15489754b73dc908b656b02ab0e6d37b190ac78dd498be8b577d/isort-4.3.19-py2.py3-none-any.whl (42kB)\n  Saved /isort-4.3.19-py2.py3-none-any.whl\nCollecting lazy-object-proxy==1.4.1 (from -c constraints.txt (line 12))\n  Downloading https://files.pythonhosted.org/packages/43/a5/1b19b094ad19bce55b5b6d434020f5537b424fd2b3cff0fbef23d7bb5a95/lazy_object_proxy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (49kB)\n  Saved /lazy_object_proxy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl\nCollecting wrapt==1.11.1 (from -c constraints.txt (line 39))\n  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n  Saved /wrapt-1.11.1.tar.gz\nCollecting six==1.12.0 (from -c constraints.txt (line 28))\n  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n  Saved /six-1.12.0-py2.py3-none-any.whl\nCollecting typed-ast==1.3.5 (from -c constraints.txt (line 37))\n  Downloading https://files.pythonhosted.org/packages/17/9e/00918af7bdd616decb5b7ad06a9cd0a4a247d2fccaa630ab448a57e68b98/typed_ast-1.3.5-cp37-cp37m-manylinux1_x86_64.whl (736kB)\n  Saved /typed_ast-1.3.5-cp37-cp37m-manylinux1_x86_64.whl\nSuccessfully downloaded pep8 pylint mccabe astroid isort lazy-object-proxy wrapt six typed-ast\nRemoving intermediate container 6b489df74137\n ---> 8ac3be432c58\nStep 4/5 : COPY test.txt ./\n ---> 5cac20851967\nStep 5/5 : RUN pip3 install -r requirements.txt -c constraints.txt\n ---> Running in 394847f09e9b\nCollecting pep8==1.7.1 (from -c constraints.txt (line 17))\n  Using cached https://files.pythonhosted.org/packages/42/3f/669429ce58de2c22d8d2c542752e137ec4b9885fff398d3eceb1a7f5acb4/pep8-1.7.1-py2.py3-none-any.whl\nCollecting pylint==2.3.1 (from -c constraints.txt (line 22))\n  Using cached https://files.pythonhosted.org/packages/60/c2/b3f73f4ac008bef6e75bca4992f3963b3f85942e0277237721ef1c151f0d/pylint-2.3.1-py3-none-any.whl\nCollecting astroid==2.2.5 (from -c constraints.txt (line 2))\n  Using cached https://files.pythonhosted.org/packages/d5/ad/7221a62a2dbce5c3b8c57fd18e1052c7331adc19b3f27f1561aa6e620db2/astroid-2.2.5-py3-none-any.whl\nCollecting mccabe==0.6.1 (from -c constraints.txt (line 14))\n  Using cached https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\nCollecting isort==4.3.19 (from -c constraints.txt (line 10))\n  Using cached https://files.pythonhosted.org/packages/ae/ae/5ef4b57e15489754b73dc908b656b02ab0e6d37b190ac78dd498be8b577d/isort-4.3.19-py2.py3-none-any.whl\nCollecting lazy-object-proxy==1.4.1 (from -c constraints.txt (line 12))\n  Using cached https://files.pythonhosted.org/packages/43/a5/1b19b094ad19bce55b5b6d434020f5537b424fd2b3cff0fbef23d7bb5a95/lazy_object_proxy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl\nCollecting six==1.12.0 (from -c constraints.txt (line 28))\n  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\nCollecting wrapt==1.11.1 (from -c constraints.txt (line 39))\n  Using cached https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\nCollecting typed-ast==1.3.5 (from -c constraints.txt (line 37))\n  Using cached https://files.pythonhosted.org/packages/17/9e/00918af7bdd616decb5b7ad06a9cd0a4a247d2fccaa630ab448a57e68b98/typed_ast-1.3.5-cp37-cp37m-manylinux1_x86_64.whl\nBuilding wheels for collected packages: wrapt\n  Building wheel for wrapt (setup.py): started\n  Building wheel for wrapt (setup.py): finished with status 'done'\n  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\nSuccessfully built wrapt\nInstalling collected packages: lazy-object-proxy, six, wrapt, typed-ast, astroid, isort, mccabe, pep8, pylint\nSuccessfully installed astroid-2.2.5 isort-4.3.19 lazy-object-proxy-1.4.1 mccabe-0.6.1 pep8-1.7.1 pylint-2.3.1 six-1.12.0 typed-ast-1.3.5 wrapt-1.11.1\nRemoving intermediate container 394847f09e9b\n ---> 68e65a214a32\nSuccessfully built 68e65a214a32\nSuccessfully tagged test:latest\nSecond run (after changing test.txt to trigger a rebuild of Docker layers 4 and 5):\nSending build context to Docker daemon  5.632kB\nStep 1/5 : FROM python:3.7\n ---> a4cc999cf2aa\nStep 2/5 : COPY constraints.txt requirements.txt ./\n ---> Using cache\n ---> 411eaa3d36ff\nStep 3/5 : RUN pip3 download -r requirements.txt -c constraints.txt\n ---> Using cache\n ---> 8ac3be432c58\nStep 4/5 : COPY test.txt ./\n ---> 7ab5814153b7\nStep 5/5 : RUN pip3 install -r requirements.txt -c constraints.txt\n ---> Running in 501da787ab07\nCollecting pep8==1.7.1 (from -c constraints.txt (line 17))\n  Using cached https://files.pythonhosted.org/packages/42/3f/669429ce58de2c22d8d2c542752e137ec4b9885fff398d3eceb1a7f5acb4/pep8-1.7.1-py2.py3-none-any.whl\nCollecting pylint==2.3.1 (from -c constraints.txt (line 22))\n  Using cached https://files.pythonhosted.org/packages/60/c2/b3f73f4ac008bef6e75bca4992f3963b3f85942e0277237721ef1c151f0d/pylint-2.3.1-py3-none-any.whl\nCollecting astroid==2.2.5 (from -c constraints.txt (line 2))\n  Using cached https://files.pythonhosted.org/packages/d5/ad/7221a62a2dbce5c3b8c57fd18e1052c7331adc19b3f27f1561aa6e620db2/astroid-2.2.5-py3-none-any.whl\nCollecting mccabe==0.6.1 (from -c constraints.txt (line 14))\n  Using cached https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\nCollecting isort==4.3.19 (from -c constraints.txt (line 10))\n  Using cached https://files.pythonhosted.org/packages/ae/ae/5ef4b57e15489754b73dc908b656b02ab0e6d37b190ac78dd498be8b577d/isort-4.3.19-py2.py3-none-any.whl\nCollecting typed-ast==1.3.5 (from -c constraints.txt (line 37))\n  Using cached https://files.pythonhosted.org/packages/17/9e/00918af7bdd616decb5b7ad06a9cd0a4a247d2fccaa630ab448a57e68b98/typed_ast-1.3.5-cp37-cp37m-manylinux1_x86_64.whl\nCollecting six==1.12.0 (from -c constraints.txt (line 28))\n  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\nCollecting wrapt==1.11.1 (from -c constraints.txt (line 39))\n  Using cached https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\nCollecting lazy-object-proxy==1.4.1 (from -c constraints.txt (line 12))\n  Using cached https://files.pythonhosted.org/packages/43/a5/1b19b094ad19bce55b5b6d434020f5537b424fd2b3cff0fbef23d7bb5a95/lazy_object_proxy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl\nBuilding wheels for collected packages: wrapt\n  Building wheel for wrapt (setup.py): started\n  Building wheel for wrapt (setup.py): finished with status 'done'\n  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\nSuccessfully built wrapt\nInstalling collected packages: typed-ast, six, wrapt, lazy-object-proxy, astroid, isort, mccabe, pep8, pylint\nSuccessfully installed astroid-2.2.5 isort-4.3.19 lazy-object-proxy-1.4.1 mccabe-0.6.1 pep8-1.7.1 pylint-2.3.1 six-1.12.0 typed-ast-1.3.5 wrapt-1.11.1\nRemoving intermediate container 501da787ab07\n ---> b377fe561e97\nSuccessfully built b377fe561e97\nSuccessfully tagged test:latest\nNB: The official documentation was quite helpful.",
    "how to reload .bashrc in dockerfile": "Each command in a Dockerfile creates a new temporary container, but without tty (issue 1870, discussed in PR 4955, but closed in favor of PR 4882).\nThe lack of tty during docker builds triggers the ttyname failed: inappropriate ioctl for device error message.\nWhat you can try instead is running a wrapper script which in it will source the .bashrc.\nDockerfile:\nCOPY myscript /path/to/myscript\nRUN /path/to/myscript\nmyscript:\n#!/bin/bash\nsource /path/to/.bashrc\n# rest of the commands    \nAbderrahim points out in the comments:\nIn my case it was for nvm: it adds an init script to .bashrc therefore it wasn't usable in the Dockerfile context.\nEnded up making an install script with it's dependent command.",
    "How to download a file from URL using Dockerfile": "I recommend using ADD, as @David Maze commented and as @nicobo commented on the first answer.\nI think that this is the best answer for many of us, since it does not force download of wget or similar into the Docker image. Here is the example I just used for CMake:\nADD https://github.com/Kitware/CMake/releases/download/v3.27.6/cmake-3.27.6-linux-x86_64.sh /tmp/cmake.sh\n\nRUN mkdir /opt/cmake && bash /tmp/cmake.sh --prefix=/opt/cmake --skip-license && rm /tmp/cmake.sh",
    "Can't send request during build time in next.js with docker?": "There are 2 solutions I can think of.\nApparently, the Next.js build fails because the service it is querying is not running. Thus why not build and start the service it depends on explicitly and build the rest like this.\ndocker-compose build some_services\ndocker-compose up -d some_services\ndocker-compose build the_rest\nThis way the Next.js app will be able to make the request. Please keep in mind that You still need to configure the ports and networks correctly. Pretty sure this will resolve the issue.\nA more 'fancy' solution would be using build-time networks which are added in the later versions, 3.4+ if I am not mistaken.\ndocker-compose.yml\n\nbuild:\n    context: ./service_directory\n    network: some_network\n   \nFor more details please see Docker-compose network",
    "Dockerfile HOSTNAME Instruction for docker build like docker run -h": "You can use docker-compose to build your image and assign the hostname, e.g.\nversion: '3'\nservices:\n  all:\n    image: testimage\n    container_name: myname\n    hostname: myhost\n    build:\n      context: .\nThe run as: docker-compose --build up.",
    "How to create docker-compose.yml file while using jib?": "Answer recommended by Google Cloud Collective",
    "What is the best approach for adding cron jobs (scheduled tasks) for a particular service in docker-compose": "You can try the following Opensource tool for scheduling crons in the docker-compose.\nhttps://github.com/mcuadros/ofelia\neg:\n [job-service-run \"service-executed-on-new-container\"]\n schedule = 0,20,40 * * * *\n image = ubuntu\n network = swarm_network\n command =  touch /tmp/example\nIn case you are planning to utilize the image in any of the cloud platforms.\nFor Eg.\nAWS: You can also have a look at ECS scheduler\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduled_tasks.html\nGCP: Kubernetes Engine CronScheduler\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/cronjobs",
    "pull access denied for oracle/serverjre": "For some time now, it's needed not only to login with the Oracle account, but also to accept the license and user agreement. So you cannot do it only from command line. You must go to the Oracle container registry:\nhttps://container-registry.oracle.com/\nThen select Java repository, then select serverjre, then signin:\nAnd accept the license:\nOnce you have done that, you'll be able to pull the docker image, but as other have said, you'll need to change the registry that is set inside the Dockerfile:\n#FROM oracle/serverjre:8\nFROM container-registry.oracle.com/java/serverjre:8\nAnd afterwards, before running the build, you must do a docker login\ndocker login container-registry.oracle.com\nusername:<SSO USERNAME>\npassword:<SSO PASSWORD>\nAt this point, you'll be able to pull the image.",
    "Using VNCserver + GUI application + Virtual Display in Docker container": "I managed to found the solution:\nChanged the script in Attempt 3 above as follows worked\n!/bin/bash\n\nXvfb :1 -screen 0 800x600x16 &\n/usr/bin/x11vnc -display :1.0 -usepw &\nDISPLAY=:1.0\nexport DISPLAY\nfirefox\nCheers.",
    "Issue - Building Docker Image - as linux/amd64 on Macbook M1 Chip": "Actually I had to delete older images that would match the image build which end up pushing the older version ARM, not AMD.\nEverything is working as expected with the steps above (Just make sure to clean your local stored images)",
    "what npm command creates a dist folder in nodejs?": "Usually the dist/ folder is created by the build tool (webpack, vite, parcel, etc). So there's no command that just says \"create this directory.\" If you're using create-react-app then it would be npm run build but I believe CRA is actually outputting to a build/ directory and not dist/.\nReally it just depends on what you're wanting to do exactly. Are you looking for a command to build the application for the docker file? Are you looking to just have a path? etc.",
    "Dockerfile - add node.js via use COPY --from": "For node and npm to work you have to copy two directories in your Dockerfile:\n# Get NodeJS\nCOPY --from=node:20-slim /usr/local/bin /usr/local/bin\n# Get npm\nCOPY --from=node:20-slim /usr/local/lib/node_modules /usr/local/lib/node_modules",
    "Dockerfile - Hide --build-args from showing up in the build time": "Update\nYou know, I was focusing on the wrong part of your question. You shouldn't be using a username and password at all. You should be using access keys, which permit read-only access to private repositories.\nOnce you've created an ssh key and added the public component to your repository, you can then drop the private key into your image:\nRUN mkdir -m 700 -p /root/.ssh\nCOPY my_access_key /root/.ssh/id_rsa\nRUN chmod 700 /root/.ssh/id_rsa\nAnd now you can use that key when installing your Python project:\nRUN pip install git+ssh://git@bitbucket.org/you/yourproject.repo\n(Original answer follows)\nYou would generally not bake credentials into an image like this. In addition to the problem you've already discovered, it makes your image less useful because you would need to rebuild it every time your credentials changed, or if more than one person wanted to be able to use it.\nCredentials are more generally provided at runtime via one of various mechanisms:\nEnvironment variables: you can place your credentials in a file, e.g.:\nUSERNAME=myname\nPASSWORD=secret\nAnd then include that on the docker run command line:\ndocker run --env-file myenvfile.env ...\nThe USERNAME and PASSWORD environment variables will be available to processes in your container.\nBind mounts: you can place your credentials in a file, and then expose that file inside your container as a bind mount using the -v option to docker run:\ndocker run -v /path/to/myfile:/path/inside/container ...\nThis would expose the file as /path/inside/container inside your container.\nDocker secrets: If you're running Docker in swarm mode, you can expose your credentials as docker secrets.",
    "Docker: Entrypoint's override involve CMD specification?": "This is one of the exception cases when inheriting values from a previous image. If the parent image defines a CMD, and your image defines an ENTRYPOINT, then the value of CMD is nulled out. In all other scenarios, you should see ENTRYPOINT and CMD inherited from parent images unchanged. For the logic behind this decision, please see issue 5147.",
    "Difference between && and `set -ex` in Dockerfiles": "This isn't specific to Docker; it's just regular shell syntax used in the RUN command. set -e causes the script to exit if any command fails, while && only runs its right-hand command if the left-hand command does not fail. So in both\nset -e\nfoo\nbar\nand\nfoo && bar\nbar will only run if foo succeeds.\nSo, the two are identical if the entire script consists of a single list command ... && ... && ... where a command only runs if every previous command succeeds. An example of how they would differ:\nset -e\necho one\nfalse\necho two\necho three\nHere, echo two and echo three would never run. But in\necho one && false && echo two\necho three\nthe echo three would still run, because only echo two was \"guarded\" by the && preceding it.",
    "Docker build not using layer cache": "I found the answer by the way :) The Dockerfile itself was ok except one problem I could then find via docker history Which shows the real shell command which are executed by docker.\nThe Problem was that ARG BUILD_VERSION lead to docker adds to every run command the environment variable like /bin/sh -c \"ARG=123 ./sbt ...\". This leads to a different call signature and a different hash every time the arg changes and therefore the run command is not applied from cache. To fix this issue just move the ARG down to the first RUN command which needs it.\nFROM openjdk:8 as workspace\n\nWORKDIR /build\n\nCOPY ./sbt ./sbt\nCOPY ./sbt-dist ./sbt-dist\nCOPY ./build.sbt ./build.sbt\nCOPY ./project/build.properties ./project/build.properties\nCOPY ./project/plugins.sbt ./project/plugins.sbt\n\nRUN ./sbt -sbt-dir ./sbt-dir -ivy ./ivy update\n\nCOPY ./ ./\n\n# Embedded postgres need to be run as non-root user\nRUN useradd -ms /bin/bash runner\nRUN chown -R runner /build\nUSER runner\n\nRUN ./sbt -sbt-dir ./sbt-dir -ivy ./ivy clean test\n\nARG BUILD_VERSION\nRUN ./sbt -sbt-dir ./sbt-dir -ivy ./ivy docker:stage -Ddocker.image.version=\"${BUILD_VERSION}\"",
    "What Docker base image (`FROM`) for Java Spring Boot?": "There's a nice documentation on how to integrate Spring-Boot with Docker: https://spring.io/guides/gs/spring-boot-docker/\nBasically you define your dockerfile in src/main/docker/Dockerfile and configure the docker-maven-plugin like this:\n<build>\n<plugins>\n    <plugin>\n        <groupId>com.spotify</groupId>\n        <artifactId>docker-maven-plugin</artifactId>\n        <version>0.4.11</version>\n        <configuration>\n            <imageName>${docker.image.prefix}/${project.artifactId}</imageName>\n            <dockerDirectory>src/main/docker</dockerDirectory>\n            <resources>\n                <resource>\n                    <targetPath>/</targetPath>\n                    <directory>${project.build.directory}</directory>\n                    <include>${project.build.finalName}.jar</include>\n                </resource>\n            </resources>\n        </configuration>\n    </plugin>\n</plugins>\nDockerfile:\nFROM frolvlad/alpine-oraclejre8:slim\nVOLUME /tmp\nADD gs-spring-boot-docker-0.1.0.jar app.jar\nRUN sh -c 'touch /app.jar'\nENV JAVA_OPTS=\"\"\nENTRYPOINT [ \"sh\", \"-c\", \"java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /app.jar\" ]\nNote that in this example FROM frolvlad/alpine-oraclejre8:slim is a small-footprinted image which is based on Alpine Linux.\nYou should also be able to use the standard Java 8 image (which is based on Debian and might have an increased footprint) as well. An extensive list of available Java Baseimages can be found here: https://github.com/docker-library/docs/tree/master/openjdk.",
    "Does a RHEL7 docker container need subscription?": "On the Docker hub, you can find some Red Hat docker images , like\nhttps://hub.docker.com/r/richxsl/rhel6.5/\nor\nhttps://hub.docker.com/r/lionelman45/rhel7/\nbut in order to update them, you will need a valid subscription\nYou will find Red Hat docker images on the Red Hat site, at\nhttps://access.redhat.com/containers\nthis article summarizes what you need in order to build a Red hat docker image\nhttp://cloudgeekz.com/625/howto-create-a-docker-image-for-rhel.html\nit begins with\nPre-requisites\nAccess to RHEL package repository.",
    "Specifying JVM Options in docker-compose File": "Switching the environment declaration from sequence style to value-map style allows to use the YAML multiline string operator '>'. It will merge all lines to a single line.\nversion: '3.1'\nservices:\n  service:\n    image: registry.gitlab.com/project/service/${BRANCH}:${TAG}\n    container_name: serviceApp\n    env_file: docker-compose.env\n    environment:\n      JVM_OPTS: >\n        -XX:NativeMemoryTracking=summary\n        -XX:+StartAttachListener\n        -XX:+UseSerialGC\n        -Xss512k\n        -Dcom.sun.management.jmxremote.rmi.port=8088\n        -Dcom.sun.management.jmxremote=true\n        -Dcom.sun.management.jmxremote.port=8088\n        -Dcom.sun.management.jmxremote.ssl=false\n        -Dcom.sun.management.jmxremote.authenticate=false\n        -Dcom.sun.management.jmxremote.local.only=false\n        -Djava.rmi.server.hostname=localhost\n\n    ports:\n        - 8088:8088\n    networks:\n        - services\n    working_dir: /opt/app\n    command: [\"java\",\"-Djava.security.egd=file:/dev/./urandom\",\"-jar\",\"/service.jar\"\"]\n\nnetworks:\n  services:\n    external:\n    name: services",
    "What is the base.Docker file in VSCode's dev containers for?": "base.Dockerfile is the \"recipe\" used by Microsoft to generate the image used in Dockerfile. In this case, this is mcr.microsoft.com/vscode/devcontainers/jekyll:0-${VARIANT}, specied in the FROM section at the top of Dockerfile.\nThey state that if you want to customize the image, that you can replace the FROM with the contents of base.Dockerfile. This is described here: https://github.com/microsoft/vscode-dev-containers/tree/v0.205.2/containers/jekyll#using-this-definition",
    "Can we have a docker container without a shell?": "Yes, you can create a container from scratch, which does not contain anything even bash, it will only contain binaries that you copy during build time, otherwise, it will be empty.\nFROM scratch\nCOPY hello /\nCMD [\"/hello\"]\nYou can use Docker\u2019s reserved, minimal image, scratch, as a starting point for building containers. Using the scratch \u201cimage\u201d signals to the build process that you want the next command in the Dockerfile to be the first filesystem layer in your image.\nWhile scratch appears in Docker\u2019s repository on the hub, you can\u2019t pull it, run it, or tag any image with the name scratch. Instead, you can refer to it in your Dockerfile. For example, to create a minimal container using scratch:\nscratch-docker-image\nUsing this as a base image, you can create your custom image, for example you only node runtime not thing more, then you try form scratch-node.\nFROM node as builder\n\nWORKDIR /app\n\nCOPY package.json package-lock.json index.js ./\n\nRUN npm install --prod\n\nFROM astefanutti/scratch-node\n\nCOPY --from=builder /app /\n\nENTRYPOINT [\"node\", \"index.js\"]",
    "Understanding Docker user/uid creation": "Absent user namespace remapping, there are only two things that matter:\nWhat the numeric user ID is; and\nWhat's in the /etc/passwd file.\nRemember that each container and the host have separate filesystems, so each of these things could have separate /etc/passwd files.\nWhat happens if I run the same image multiple times i.e multiple containers? Will the same uid be assigned to all processes?\nYes, because each container gets a copy of the same /etc/passwd file from the image.\nWhat happens if I add same above command in another image and run that image as container? - will I get new uid or same uid as the previous one?\nIt depends on what adduser actually does; it could be the same or different.\nHow the uid increment happens in Container in relation with the host machine.\nThey're completely and totally independent.\nAlso remember that you can docker push/docker pull a built image to run it on a different host. That will bring the image's /etc/passwd file along with it, but the host environment could be totally different. Correspondingly, it's not a best practice to try to match some specific host's uid mapping in a Dockerfile, because it will be wrong if you try to run the same image anywhere else.",
    "docker error in windows container read tcp : wsarecv: An existing connection was forcibly closed by the remote host": "I am not sure exactly why this one worked, as I was trying to do a pull of a couple microsoft images. But in Settings > General > Expose daemon on tcp://localhost:2375 without TLS, worked for me. Following that I reverted the change but nice to have that on in the back-pocket. Might be related to firewall settings in Windows. I am using Win 10 Professional.",
    "Dockerize existing Django project": "This question is too broad. What happens with the Dockerfile you've created?\nYou don't need docker compose unless you have multiple containers that need to interact.\nSome general observations from your current Dockerfile:\nIt would be better to collapse the pip install commands into a single statement. In docker, each statement creates a file system layer, and the layers in between the pip install commmands probably serve no useful purpose.\nIt's better to declare dependencies in setup.py or a requirements.txt file (pip install -r requirements.txt), with fixed version numbers (foopackage==0.0.1) to ensure a repeatable build.\nI'd recommend packaging your Django app into a python package and installing it with pip (cd /code/; pip install .) rather than directly adding the code directory.\nYou're missing a statement (CMD or ENTRYPOINT) to execute the app. See https://docs.docker.com/engine/reference/builder/#cmd",
    "Docker COPY all files and folders except some files/folders": "You have two choices:\nList all directories you want to copy directly:\nCOPY [\"foldera\", \"folderc\", \"folderd\", ..., \"/dstPath]\nTry to exclude some paths but also make sure that all paths patterns are not including the path we want to exclude:\nCOPY [\"folder[^b]*\", \"file*\", \"/dstPath\"]\nAlso you can read more about available solutions in this issue: https://github.com/moby/moby/issues/15771",
    "Docker: bind `/uploads` directory to Amazon S3 Storage": "The S3 storage link that your posted is for Docker Registry setup and not for Docker volumes. What you need is to map a folder on your hard drive to the container. So let's assume you map /var/www/uploads on host to your uploads inside the container.\nNow what you want is /var/www/uploads to be actually mounted as an S3 backed folder. For this Amazon had launched an AWS Storage Gateway. You can use that to create a S3 backed folder on your system. Below is an article from Amazon that details how to configure the same\nhttp://docs.aws.amazon.com/storagegateway/latest/userguide/ManagingLocalStorage-common.html",
    "Disable Dockerfile POP-UP Assistance in Visual Studio Code": "This is handled by the Parameter Hints functionality of VSCode rather than the Docker extension itself.\nYou can turn these off by unchecking the Editor \u203a Parameter Hints setting in your VSCode settings file.\nYou can still access the hint using the keyboard:\nFor Linux and Windows:\nCtrl + Shift + Space\nFor MacOS:\n\u21e7 + \u2318 + Space\nSee also this related answer.",
    "Copy a file from container to host during build process": "I wouldn't run the tests during build, this will only increase the size of your image. I would recommend you to build the image and then run it mounting a host volume into the container and changing the working directory to the mount point.\ndocker run -v `pwd`/results:/results -w /results -t IMAGE test_script",
    "How Docker selects an image's os/arch": "I'm wondering what image I'll get if I just do a \"docker pull python\"\nFrom \"Leverage multi-CPU architecture support\"\nMost of the Docker Official Images on Docker Hub provide a variety of architectures.\nFor example, the busybox image supports amd64, arm32v5, arm32v6, arm32v7, arm64v8, i386, ppc64le, and s390x.\nWhen running this image on an x86_64 / amd64 machine, the amd64 variant is pulled and run.\nSo it depends on the OS used when you do your docker pull.\nand how I can choose the OS and architecture myself.\nThe same page adds:\nYou can also run images targeted for a different architecture on Docker Desktop.\nYou can run the images using the SHA tag, and verify the architecture.\nFor example, when you run the following on a macOS:\ndocker run --rm docker.io/username/demo:latest@sha256:2b77acdfea5dc5baa489ffab2a0b4a387666d1d526490e31845eb64e3e73ed20 uname -m\naarch64\ndocker run --rm docker.io/username/demo:latest@sha256:723c22f366ae44e419d12706453a544ae92711ae52f510e226f6467d8228d191 uname -m\narmv71\nIn the above example, uname -m returns aarch64 and armv7l as expected, even when running the commands on a native macOS or Windows developer machine",
    "Is sklearn compatible with Linux-alpine?": "UPDATE: Since 2020 there is an official sklearn alpine package, which can easily be installed via:\napk add py3-scikit-learn\nhttps://pkgs.alpinelinux.org/package/edge/community/x86/py3-scikit-learn\nI would even recommend this way, instead of using pip. Some people (including me) encountered problems while trying the pip-way for scipy and/or sklearn:\nScipy error in python:3.8-alpine3.11 - No lapack/blas resources found",
    "Apache with Docker Alpine Linux": "It is true that Apache uses SIGWINCH to trigger a graceful shutdown:\ndocker kill ----signal=SIGWINCH apache\ndocker-library/httpd issue 9 mentions\nEven just dropping \"-t\" should remove the sending of SIGWINCH when the window resizes.\nActually, you need just -d: see PR669.\nIn your case, you already running the image with -dit, so check if just keeping -d might help.\nThe original issue (on httpd side, not docker) is described in bug id 1212224.\nThe OP Sebi2020 confirms in the comments:\nif I don't connect a tty the signal isn't send\nSo if possible, avoid the -t and, if needed, add a docker exec -t session if you need tty.",
    "Caching Jar dependencies for Maven-based Docker builds": "There is a new instruction regarding this topic: https://github.com/carlossg/docker-maven#packaging-a-local-repository-with-the-image\nThe $MAVEN_CONFIG dir (default to /root/.m2) is configured as a volume so anything copied there in a Dockerfile at build time is lost. For that the dir /usr/share/maven/ref/ is created, and anything in there will be copied on container startup to $MAVEN_CONFIG.\nTo create a pre-packaged repository, create a pom.xml with the dependencies you need and use this in your Dockerfile. /usr/share/maven/ref/settings-docker.xml is a settings file that changes the local repository to /usr/share/maven/ref/repository, but you can use your own settings file as long as it uses /usr/share/maven/ref/repository as local repo.",
    "chown: changing ownership of '/var/lib/mysql/': Operation not permitted": "The MariaDB images on DockerHub don't follow good practice of not requiring to be run as root user.\nYou should instead use the MariaDB images provided by OpenShift. Eg:\ncentos/mariadb-102-centos7\nSee:\nhttps://github.com/sclorg/mariadb-container\nThere should be an ability to select MariaDB from the service catalog browser in the OpenShift web console, or use the mariadb template from the command line.",
    "Spring Boot can't read application.properties in Docker": "You have to add the application.properties file in the docker /app/ directory. Ur docker directory structure will be\napp\n   -main.jar\n   -application.properties\nYou can do so by using ADD /ur/local/location/application.properties /app/application.properties\nThen better write this command in your docker file\nENTRYPOINT [\"java\" ,\"-Djava.security.egd=file:/dev/./urandom --spring.config.location=classpath:file:/app/application-properties\",\"-jar\",\"/app/main.jar\"]\nYour whole dockerFile should look like this:\nFROM java:8-jre\nVOLUME /tmp\nCOPY ./mail.jar /app/mail.jar\nADD /ur/local/location/application.properties /app/application.properties\nENTRYPOINT [\"java\" ,\"-Djava.security.egd=file:/dev/./urandom --spring.config.location=classpath:file:/app/application-properties\",\"-jar\",\"/app/main.jar\"]\nEXPOSE 8080",
    "standard_init_linux.go:190: exec user process caused \"no such file or directory\" Docker with go basic web app": "File not found can mean the file is missing, a script missing the interpreter, or an executable missing a library. In this case, the net import brings in libc by default, as a dynamic linked binary. You should be able to see that with ldd on your binary.\nTo fix it, you'll need to pass some extra flags:\nCGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -tags netgo -ldflags '-w' -o mybin *.go\nThe above is from: https://medium.com/@diogok/on-golang-static-binaries-cross-compiling-and-plugins-1aed33499671",
    "How to install hadolint on Ubuntu [closed]": "This code works and installs the prebuilt binary file :\nwget -O /bin/hadolint https://github.com/hadolint/hadolint/releases/download/v2.10.0/hadolint-Linux-x86_64\nI didn't know these basic commands since I am new to Ubuntu",
    "docker build failed after pip installed requirements with exit code 137": "I was getting the below error on my windows machine:\nKilled ERROR: Service 'kafka-asr' failed to build: The command '/bin/sh -c pip install --no-cache-dir -r requirements.txt' returned a non-zero code: 137\nAfter increasing the docker memory the error got resolved.\nRight click on docker -> setting -> Advance\nI increased the memory 2304 to 2816 and clicked on Apply.",
    "Permission denied in Docker build": "For me, it was the issue with the missing permission for the Docker\nI fixed the issue with the following command.\nsudo chmod -R g+rw \"$HOME/.docker\"\nFYI: I am using a Mac M1 machine.",
    "How to build a Docker image on a specific architecture with Docker Hub?": "I solved my own issue after a bit of research... First, I was making a stupid mistake and second, I was forgetting a very important thing. Here's how I fixed my issues:\nThe Stupid Mistake\nAlthough I specified different Dockerfiles for each automated build, I also had a build hook which was overwriting the docker build command and it was defaulting to Dockerfile for all builds instead of picking the right file.\nFixed build hook file:\n#!/bin/bash\n\ndocker build \\\n    --file \"${DOCKERFILE_PATH}\" \\\n    --build-arg BUILD_DATE=\"$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\" \\\n    --build-arg VCS_REF=\"$(git rev-parse --short HEAD)\" \\\n    --tag \"$IMAGE_NAME\" \\\n    .\nThe Important Thing\nLike @JanGaraj mentioned on his answer, Docker Hub runs on amd64 so it can't run binaries for other architectures. How does one build multi-arch images with Docker Hub Automated Builds? With the help of qemu-user-static and more hooks. I found the answer on this GitHub issue but I'll post here the complete answer to my specific use case:\nMy sample project tree:\n.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 Dockerfile.aarch64\n\u251c\u2500\u2500 Dockerfile.armhf\n\u2514\u2500\u2500 hooks\n    \u251c\u2500\u2500 build\n    \u251c\u2500\u2500 post_checkout\n    \u2514\u2500\u2500 pre_build\nThe post_checkout hook file:\n#!/bin/bash\n\nBUILD_ARCH=$(echo \"${DOCKERFILE_PATH}\" | cut -d '.' -f 2)\n\n[ \"${BUILD_ARCH}\" == \"Dockerfile\" ] && \\\n    { echo 'qemu-user-static: Download not required for current arch'; exit 0; }\n\nQEMU_USER_STATIC_ARCH=$([ \"${BUILD_ARCH}\" == \"armhf\" ] && echo \"${BUILD_ARCH::-2}\" || echo \"${BUILD_ARCH}\")\nQEMU_USER_STATIC_DOWNLOAD_URL=\"https://github.com/multiarch/qemu-user-static/releases/download\"\nQEMU_USER_STATIC_LATEST_TAG=$(curl -s https://api.github.com/repos/multiarch/qemu-user-static/tags \\\n    | grep 'name.*v[0-9]' \\\n    | head -n 1 \\\n    | cut -d '\"' -f 4)\n\ncurl -SL \"${QEMU_USER_STATIC_DOWNLOAD_URL}/${QEMU_USER_STATIC_LATEST_TAG}/x86_64_qemu-${QEMU_USER_STATIC_ARCH}-static.tar.gz\" \\\n    | tar xzv\nThe pre_build hook file:\n#!/bin/bash\n\nBUILD_ARCH=$(echo \"${DOCKERFILE_PATH}\" | cut -d '.' -f 2)\n\n[ \"${BUILD_ARCH}\" == \"Dockerfile\" ] && \\\n    { echo 'qemu-user-static: Registration not required for current arch'; exit 0; }\n\ndocker run --rm --privileged multiarch/qemu-user-static:register --reset\nThe Dockerfile file:\nFROM amd64/alpine:3.8\n(...)\nThe Dockerfile.aarch64 file:\nFROM arm64v8/alpine:3.8\nCOPY qemu-aarch64-static /usr/bin/\n(...)\nThe Dockerfile.armhf file:\nFROM arm32v6/alpine:3.8\nCOPY qemu-arm-static /usr/bin/\n(...)\nThat's it!",
    "Dockerfile returns npm not found on build": "You need to explicitly install node / npm in your container before running npm install. Add this to your Dockerfile.\nRUN apt-get update && apt-get install -y curl\nRUN curl -sL https://deb.nodesource.com/setup_8.x | bash -\nRUN apt-get update && apt-get install -y nodejs",
    "Docker Python File Input Selector": "You need to run the container with docker run -ti image to make sure that it runs in interactive mode with the terminal attached.\nRunning X11 GUI applications is a bit more tricky since you need to give the container access to your display. This blog post describes the process in more details.",
    "How to define OpenJDK 8 in CentOS based Dockerfile?": "Seems to be as easy as this:\nFROM centos\n\nRUN yum install -y \\\n       java-1.8.0-openjdk \\\n       java-1.8.0-openjdk-devel\n\nENV JAVA_HOME /etc/alternatives/jre\n.\n.",
    "Python + Docker + No module found": "At first sight, the error you obtain in the logs\n[\u2026] Traceback (most recent call last):\napp6_1 | File \"/code/bank_transactions/main_transactions.py\", line 5, in <module>\napp6_1 | from bank_transactions import constants\napp6_1 | ModuleNotFoundError: No module named 'bank_transactions'\nsuggests the file main_transactions.py is indeed parsed, which in turn imports the constants.py file, which fails.\nActually, this is related to the way import walks directories to find python packages and I'm pretty sure your error should vanish by setting the PYTHONPATH environment variable:\nFROM python:3.6\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nWORKDIR /code\n\nCOPY . /code/\n\nRUN pip install -r requirements.txt \n\nENV PYTHONPATH /code\n\nCMD [ \"python\", \"/code/bank_transactions/main_transactions.py\" ]\nFor additional details, see this other StackOverflow question (which dealt with Python2 \u2212 cf. the ImportError instead of ModuleNotFoundError with Python3 \u2212 but the fix should be the same).",
    "How to run script file(.sh file) inside Dockerfile? [closed]": "Steps as follows :\nCopy '.sh' file to container\nCOPY install.sh .\nExecuting '.sh' file\nRUN ./install.sh\n'install.sh' file should be in current working directory else you can specify path.",
    "linux source command not working when building Dockerfile": "From the docker builder reference, each RUN command is run independently. So doing RUN source /usr/local/rvm/scripts/rvm does not have any effect on the next RUN command.\nTry changing the operations which require the given source file as follows\n  RUN /bin/bash -c \"source /usr/local/rvm/scripts/rvm ; gem install rails\"",
    "How to export my local docker image to a tar and the load on another computer": "The best way is use save/load commands because the CMD are saved. Using import/export commands the CMD is not saved.\nSave to the disk your docker image:\ndocker save --output=\"image_name.tar\" id_image\nLoad your docker image from the disc:\ndocker load --input image_name.tar\nif after list images the repository and tag are < none >, you can rename your image setting new repository:tag\nDocker tag new_repository:new_tag",
    "Docker Images Hierarchy": "docker tree was deprecated before any good replacement was proposed (see the debate in PR 5001)\nThis is currently externalized to justone/dockviz.\n alias dockviz=\"docker run --rm -v /var/run/docker.sock:/var/run/docker.sock nate/dockviz\"\nImage info is visualized with lines indicating parent images:\ndockviz images -d | dot -Tpng -o images.png\nas a tree in the terminal:\n$ dockviz images -t\n\u2514\u2500511136ea3c5a Virtual Size: 0.0 B\n  |\u2500f10ebce2c0e1 Virtual Size: 103.7 MB\n  | \u2514\u250082cdea7ab5b5 Virtual Size: 103.9 MB\n  ...",
    "Dockerfile not executing second stage": "Buildkit uses a dependency graph. It looks at the target stage, which by default is the last one:\nFROM base as final\nRUN echo \"3\"\nFrom there it sees that base is needed to build this stage so it pulls in the base stage:\nFROM alpine as base\nRUN echo \"1\"\nAnd from there it's done, it's not needed to build the mid stage to create your target image. There's no dependencies in the FROM or a COPY --from that would require it. This behavior differs from the classic docker build which performed steps in order until the target stage was reached, and is one of the reasons buildkit is much faster.",
    "How to inject Docker container build timestamp in container?": "The output of each RUN step during a build is the changes to the filesystem. So you can output the date to a file in your image. And the logs from the container are just the stdout from commands you run. So you can cat out the date inside your entrypoint.\nIn code, you'd have at the end of your Dockerfile:\nRUN date >/build-date.txt\nAnd inside an entrypoint script:\n#!/bin/sh\n#.... Initialization steps\necho Image built: $(cat /build-date.txt)\n#.... More initialization steps\n# run the command\nexec \"$@\"",
    "Why can't I use the build arg again after FROM in a Dockerfile?": "Why can't I use the FROM_IMAGE build arg twice, on and after a FROM line?\nThere is a real difference depending on where you put ARG related to FROM line:\nany ARG before the first FROM can be used in any FROM line\nany ARG within a build stage (after a FROM) can be used in that build stage\nThis is related to build stages mechanics and some reference of actual behavior can be found here: https://github.com/docker/cli/pull/333, and a discussion on why documentation and build mechanics is a bit confusing on ARG usage is here: https://github.com/moby/moby/issues/34129",
    "Unable to change directories while building docker Image using Dockerfile": "use WORKDIR\nhttps://docs.docker.com/engine/reference/builder/#workdir\nor do all in one RUN\nyour cd is \"forgotten\" when you are in another RUN\nBy the way, group your RUN, as indicated in the Dockerfile best practices\nhttps://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/",
    "How can I grab exposed port from inspecting docker container?": "Execute the command: docker inspect --format=\"{{json .Config.ExposedPorts }}\" src_python_1\nResult: {\"8000/tcp\":{}}\nProof (using docker ps):\ne5e917b59e15        src_python:latest   \"start-server\"         22 hours ago        Up 22 hours         0.0.0.0:8000->8000/tcp                                        src_python_1",
    "Nuget package restore error in Docker Compose build": "SOLVED:\nIt turns out to be a networking issue. I am behind a corporate firewall at work that leverages TLS packet inspection to break apart SSL traffic. The build process while debugging runs as \"me\" on my local machine, however, the release build (docker-compose) actually pulls down a aspnetcore-build docker image, copies your code to the docker container, then runs dotnet restore to get fresh nuget packages for your docker image. These actions can be found in the Docker File in your project. This \"dotnet restore\" inside the container, runs under a different security context, and therefore was getting hung up. We traced the network traffic which was hard for me to get to because of how docker networking works. Fiddler was not catching the traffic. Using wireshark, we were able to catch it from a device level and see the drop. The reason it continued to fail from my home network was due to the configuration with our hypervisor & networking.\nRESOLUTIONS:\nAdd a firewall rule for https://api.nuget.org/v3/index.json (Preferred) OR Build the image from VSTS in the cloud OR Build from a different network.\nPS4 please post back if you are able to resolve this the same way? Having spent 3 days on this, I'm curious about your status.",
    "How to access the Host's machine's localhost 127.0.0.1 from docker container": "its not working, its looks like inside container its trying to connect localhost of container.\nYes, that is the all idea behind the isolation provided by container, even docker build (which builds one container per Dockerfile line, and commits it in an intermediate image).\nAs commented by the OP dhairya, and mentioned in the documentation I referred in my comments: \"Docker container networking\" and docker build, you can set to host the networking mode for the RUN instructions during build: then localhost would refer to the host localhost.\n docker build --network=\"host\"\nThis is since API 1.25+ only, in docker v.1.13.0-rc5 (January 2017)\nPOST /build accepts networkmode parameter to specify network used during build.\nBut if you don't need Git in your actual built image (for its main process to run), it would be easier to\nclone the repo locally (directly on the host) before the docker build.\nYou need to clone it where your Dockerfile is, as it must be relative to the source directory that is being built (the context of the build).\nuse a COPY directive in the Dockerfile, to copy the host folder representing the checked out Git repo.\nNote: add .git/: to your .dockerignore (file in the same folder as your Dockerfile), in order to not copy the repo_dir/.git folder of that cloned repo, if you don't have git in your target image.\nCOPY repo_dir .\n(Here '.' represent the current WORKDIR within the image being built)",
    "Using ARG in FROM in dockerfile": "Using the docs for reference, if you want to use ARG in FROM, ARG must be the first line in your Dockerfile (as reminded by koby.el's answer) and FROM must come immediately after. See this section for details.\nThis minimal Dockerfile works:\nARG url=docker-local.artifactory.com/projectA \nFROM $url\nBuilt using this command with a build arg:\ndocker build -t from --build-arg url=alpine:3.9 .\n[+] Building 0.1s (5/5) FINISHED\n => [internal] load build definition from Dockerfile                                                               0.0s\n => => transferring dockerfile: 116B                                                                               0.0s\n => [internal] load .dockerignore                                                                                  0.0s\n => => transferring context: 2B                                                                                    0.0s\n => [internal] load metadata for docker.io/library/alpine:3.9                                                      0.0s\n => CACHED [1/1] FROM docker.io/library/alpine:3.9                                                                 0.0s\n => exporting to image                                                                                             0.0s\n => => exporting layers                                                                                            0.0s\n => => writing image sha256:352159a49b502edb1c17a3ad142b320155bd541830000c02093b79f4058a3bd1                       0.0s\n => => naming to docker.io/library/from\nThe docs also show an example if you want to re-use the ARG value after the first FROM command:\nARG url=docker-local.artifactory.com/projectA \nFROM $url\nARG url\nRUN echo $url",
    "Two docker containers cannot communicate": "Do not use localhost to communicate between containers. Networking is one of the namespaces in docker, so localhost inside of a container only connects to that container, not to your external host, and not to another container. In this case, use the service name, graph-db, instead of localhost, in your app to connect to the db.",
    "error while creating mount source path mkdir /host_mnt/d: file exists": "I also experienced this issue (using Docker Desktop for Windows). For me, I simply issued a restart for Docker by using the following steps:\nLocate the Docker Desktop Icon (in the Windows Task Bar)\nRight-Click the Docker Desktop Icon and choose \"Restart...\"\nClick the \"Restart\" button on the confirmation dialog that appears\nWhen Docker Desktop had restarted, I then:\nChecked for running containers using docker-compose ps\nStopped any running containers using docker-compose down\nStarted the containers docker-compose up\nAnd Voila! All containers restarted successfully with no mount errors.",
    "running two nodejs apps in one docker image": "I recommend using pm2 as the entrypoint process which will handle all your NodeJS applications within docker image. The advantage of this is that pm2 can bahave as a proper process manager which is essential in docker. Other helpful features are load balancing, restarting applications which consume too much memory or just die for whatever reason, and log management.\nHere's a Dockerfile I've been using for some time now:\n#A lightweight node image\nFROM mhart/alpine-node:6.5.0\n\n#PM2 will be used as PID 1 process\nRUN npm install -g pm2@1.1.3\n\n# Copy package json files for services\n\nCOPY app1/package.json /var/www/app1/package.json\nCOPY app2/package.json /var/www/app2/package.json\n\n# Set up working dir\nWORKDIR /var/www\n\n# Install packages\nRUN npm config set loglevel warn \\\n# To mitigate issues with npm saturating the network interface we limit the number of concurrent connections\n    && npm config set maxsockets 5 \\\n    && npm config set only production \\\n    && npm config set progress false \\\n    && cd ./app1 \\\n    && npm i \\\n    && cd ../app2 \\\n    && npm i\n\n\n# Copy source files\nCOPY . ./\n\n# Expose ports\nEXPOSE 3000\nEXPOSE 3001\n\n# Start PM2 as PID 1 process\nENTRYPOINT [\"pm2\", \"--no-daemon\", \"start\"]\n\n# Actual script to start can be overridden from `docker run`\nCMD [\"process.json\"]\nprocess.json file in the CMD is described here",
    "Difference between nodejs v0.12 and v5.x distributions": "You should definitely not use any of the v0.x versions of Node.js as support for them are set to expire in 2016.\nYou should use either v4 (code name argon) which is the next LTS (long term support) version of Node.js or v5 which is the latest stable version.\nAlso, Node.js has an official Docker Image:\nFROM node:5",
    "How to set mysql username in dockerfile": "If you take a look at the official Docker MySQL image Dockerfile, you will discover how they did it using debconf-set-selections.\nThe relevant instructions are:\nRUN { \\\n        echo mysql-community-server mysql-community-server/data-dir select ''; \\\n        echo mysql-community-server mysql-community-server/root-pass password ''; \\\n        echo mysql-community-server mysql-community-server/re-root-pass password ''; \\\n        echo mysql-community-server mysql-community-server/remove-test-db select false; \\\n    } | debconf-set-selections \\\n    && apt-get update && apt-get install -y mysql-server\ndebconf-set-selections is a tool that allows you to prepare the answers for the questions that will be asked during the later installation.",
    "\"docker build\" requires exactly 1 argument [duplicate]": "You should provide the context, current directory for instance: ..\ndocker build -f C://Users/XXXX/XXXX/XXXX/XXXX/XXXX/Dockerfile -t something .",
    "How to set JVM settings in Dockerfile": "You can simply set the JAVA_OPTS value to the one you need at build time, in your Dockerfile :\nENV JAVA_OPTS=\"-DTOMCAT=Y -DOracle.server=1234 [...]\"\nYou may also simply set it a runtime if you don't modify the CMD from the official tomcat image:\n$ docker run -e JAVA_OPTS=\"-DTOMCAT=Y -DOracle.server=1234 [...]\" your_image:your_tag \nSee: https://github.com/docker-library/tomcat/issues/8\nConsidering the options you're providing in your example, it would be better to opt for the second version (host, port and password information should not be left in a Docker image, from a security standpoint).\nIf you're only providing minimal requirements, resource-wise, for your application, this could live in the Dockerfile.",
    "How do I setup only python 2.7 in a docker container?": "You can use python base image\nFROM python:2.7\nThis base image with have python pre-configured and you don't need to install python seperately. Hope it helps.\nHere is the list of available image\nFor quick reference please check https://blog.realkinetic.com/building-minimal-docker-containers-for-python-applications-37d0272c52f3",
    "Pass ENV in docker run command": "It looks like you might be confusing the image build with the container run. If the difference between the two isn't immediately clear, I'd recommend reviewing some other questions and docs like:\nIn Docker, what's the difference between a container and an image? https://docs.docker.com/develop/develop-images/dockerfile_best-practices/\nRUN echo \"${FILENAME} ${animals}\" > ./entrypoint.sh\nWith the above, the variables will be expanded during the image build. The entrypoint.sh will not contain ${FILENAME} ${animals}. Instead, it will contain\nfile_to_run.zip turtle, monkey, goose\nAfter the build, the docker run command will create a container from that image and run the above script with the environment variables defined but never used since the script already has the variables expanded. To prevent the variable expansion, you need to escape the $ or use single quotes to prevent the expansion, e.g.\nRUN echo \"\\${FILENAME} \\${animals}\" > ./entrypoint.sh\nor\nRUN echo '${FILENAME} ${animals}' > ./entrypoint.sh\nI would also recommend being explicit with a #!/bin/ash at the top of this script. Then when you run the script, do not override the command with parameters after the image name. Instead set the environment variables with the appropriate flag to run:\ndocker run -it -e animals=\"mouse,rat,kangaroo\" image ",
    "Fix umask for future RUN commands in dockerfile": "Docker creates a new, minimal sh environment for every RUN step.\nThe umask is set to 0022 in runc by default when a container is started. A umask configuration option has been exposed in runc but unfortunately that is not configurable from Docker yet.\nFor now, the umask command (or the process setting the umask) will need to be chained in each RUN step where it is needed, while the subsequent chained commands run under the same shell process.\nRUN set -uex; \\\n    umask 0002; \\\n    do_something; \\\n    do_otherthing;\nRUN set -uex; \\\n    umask 0002; \\\n    do_nextthing; \\\n    do_subsequentthing;",
    "Error unknown time zone America/Los_Angeles in time.LoadLocation": "For anyone looking for an answer, this helped me.\nadding these two line to docker file , (final if it's a 2 stage build)\nADD https://github.com/golang/go/raw/master/lib/time/zoneinfo.zip /zoneinfo.zip\nENV ZONEINFO /zoneinfo.zip",
    "Adding default external network in docker-compose": "First of all, check your version of the file. For version 3.6, the following examples can meet your needs:\nExample 1:\nversion: '3.6'\nservices:\n  webserver:\n...\n#add existing database network \nnetworks:\n  default:\n    external:\n      name: proxy_host\nExample 2:\nversion: '3.6'\nservices:\n  webserver:\n...\n#add existing database network \nnetworks:\n  default:\n    name: proxy_host\n    external: true\nExample 3: This configuration creates new networks.\nversion: '3.6'\nservices:\n  webserver:\n    networks:\n      - proxy_host\n      - database_host\n...\nnetworks:\n  proxy_host: {}\n  database_host: {}",
    "Routing doesn't work after Dockerizing Angular app": "This is happening because the base nginx image will try to serve the requests from the docroot, so it will try to find a login.html when you send a request to /login.\nTo avoid this, you need nginx to serve the index.html no matter the request, and thus letting angular take care of the routes.\nTo do so, you will need to change your nginx.conf, which is currently in the image, to include the following :\ntry_files $uri $uri/ /index.html;\nInstead of the default being:\ntry_files $uri $uri/ =404;\nYou can do so in many ways, but I guess the best approach is to have an extra command in your Dockerfile, that copies over the nginx configuration like so :\nCOPY nginx/default.conf /etc/nginx/conf.d/default.conf\nAnd have that nginx/default.conf file in your directory (containing the default nginx configuration) with the command specified above replaced.\nEDIT\nThere is already images that does exactly that, so you can just use an image other than the official one, that is made specifically for that, and it should work fine: example",
    "Mounting Maven Repository to Docker": "I finally found the solution for mounting my local maven repository in docker. I changed my solution; I am mounting it in the run phase instead of build phase. This is my Dockerfile:\nFROM ubuntu\nMAINTAINER Zeinab Abbasimazar\nADD gwr $HOME\nRUN apt-get update; \\\n    apt-get install -y --no-install-recommends apt-utils; \\\n    apt-get install -y wget unzip curl maven git; \\\n    echo \\\n    \"<settings xmlns='http://maven.apache.org/SETTINGS/1.0.0\\' \\\n    xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance' \\\n    xsi:schemaLocation='http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd'> \\\n        <localRepository>/root/.m2/repository</localRepository> \\\n        <interactiveMode>true</interactiveMode> \\\n        <usePluginRegistry>false</usePluginRegistry> \\\n        <offline>false</offline> \\\n    </settings>\" \\\n    > /usr/share/maven/conf/settings.xml; \\\n    mkdir /root/.m2/; \\\n    echo \\\n    \"<settings xmlns='http://maven.apache.org/SETTINGS/1.0.0\\' \\\n    xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance' \\\n    xsi:schemaLocation='http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd'> \\\n        <localRepository>/root/.m2/repository</localRepository> \\\n        <interactiveMode>true</interactiveMode> \\\n        <usePluginRegistry>false</usePluginRegistry> \\\n        <offline>false</offline> \\\n    </settings>\" \\\n    > /root/.m2/settings.xml\nWORKDIR .\nCMD mvn  -X clean install -pl components -P profile\nAt first, I build the image using above Dockerfile:\nsudo docker build -t imageName:imageTag .\nThen, I run a container as below:\nsudo docker run -d -v /home/zeinab/.m2/:/root/.m2/ --name containerName imageName:imageTag",
    "Pass variables from .env file to dockerfile through docker-compose": "I'm posting a new answer to highlight the various assumptions related to the OP's question, in particular, the fact that there's a subtle difference between the \".env\" unique filename and *.env files (arguments for env_file:).\nBut apart from this subtlety, the process to pass arguments from docker-compose.yml to docker build -f Dockerfile . and/or docker run -e \u2026 is easy, as shown by the comprehensive example below.\nMinimal working example\nLet's consider the following files in a given directory, say ./docker.\nFile docker-compose.yml:\nservices:\n  demo-1:\n    image: demo-${ENV_NUM}\n    build:\n      context: .\n      args:\n        ARG1: \"demo-1/${ARG1}\"\n        ARG3: \"demo-1/${ARG3}\"\n  demo-2:\n    image: demo-2${ENV_FILE_NUM}\n    build:\n      context: .\n      args:\n        ARG1: \"demo-2/${ARG1}\"\n        ARG3: \"demo-2/${ARG3}\"\n    env_file:\n      - var.env\nRemark: even if we use a build: field, it appears to be a good idea to also add an image: field to automatically tag the built image; but note that these image names must be pairwise different.\nFile .env:\nKEY=\"some value\"\nENV_NUM=1\nARG1=.env/ARG1\nARG2=.env/ARG2\nARG3=.env/ARG3\nFile var.env:\nENV_FILE_NUM=\"some number\"\nARG1=var.env/ARG1\nARG2=var.env/ARG2\nARG3=var.env/ARG3\nARG4=var.env/ARG4\nFile Dockerfile:\nFROM debian:10\n\n# Read build arguments (default value if omitted at CLI)\nARG ARG1=\"default 1\"\nARG ARG2=\"default 2\"\nARG ARG3=\"default 3\"\n\n# the build args are exported at build time\nRUN echo \"ARG1=${ARG1}\" | tee /root/arg1.txt\nRUN echo \"ARG2=${ARG2}\" | tee /root/arg2.txt\nRUN echo \"ARG3=${ARG3}\" | tee /root/arg3.txt\n\n# Export part of these args at runtime also\nENV ARG1=\"${ARG1}\"\nENV ARG2=\"${ARG2}\"\n\n# exec-form is mandatory for ENTRYPOINT/CMD\nCMD [\"/bin/bash\", \"-c\", \"echo ARG1=\\\"${ARG1}\\\" ARG2=\\\"${ARG2}\\\" ARG3=\\\"${ARG3}\\\"; echo while at build time:; cat /root/arg{1,2,3}.txt\"]\nExperiment session 1\nFirst, as suggested by @SergioSantiago in the comments, a very handy command to preview the effective docker-compose.yml file after interpolation is docker-compose config:\n$ docker-compose config\n\nWARN[0000] The \"ENV_FILE_NUM\" variable is not set. Defaulting to a blank string. \nname: docker\nservices:\n  demo-1:\n    build:\n      context: /home/debian/docker\n      dockerfile: Dockerfile\n      args:\n        ARG1: demo-1/.env/ARG1\n        ARG3: demo-1/.env/ARG3\n    image: demo-1\n    networks:\n      default: null\n  demo-2:\n    build:\n      context: /home/debian/docker\n      dockerfile: Dockerfile\n      args:\n        ARG1: demo-2/.env/ARG1\n        ARG3: demo-2/.env/ARG3\n    environment:\n      ARG1: var.env/ARG1\n      ARG2: var.env/ARG2\n      ARG3: var.env/ARG3\n      ARG4: var.env/ARG4\n      ENV_FILE_NUM: some number\n    image: demo-2\n    networks:\n      default: null\nnetworks:\n  default:\n    name: docker_default\nHere, as indicated by the warning, we see there's an issue for interpolating ENV_FILE_NUM despite the fact this variable is mentioned by var.env. The reason is that env_files lines just add new environment variables for the underlying docker run -e \u2026 command, but don't interpolate anything in the docker-compose.yml.\nContrarily, one can notice that the value ARG1=.env/ARG1 taken from \".env\" is interpolated within the args: field of docker-compose.yml, cf. the output line:\nargs:\n  ARG1: demo-1/.env/ARG1\n  \u2026\nThis very distinct semantics of \".env\" vs. env_files is described in this page of the official documentation.\nExperiment session 2\nNext, let us run:\n$ docker-compose up --build\n                                                                                         \nWARN[0000] The \"ENV_FILE_NUM\" variable is not set. Defaulting to a blank string.                                                                     \n[+] Building 10.4s (13/13) FINISHED\n => [demo-1 internal] load build definition from Dockerfile\n => => transferring dockerfile: 609B\n => [demo-2 internal] load build definition from Dockerfile\n => => transferring dockerfile: 609B\n => [demo-1 internal] load .dockerignore\n => => transferring context: 2B\n => [demo-2 internal] load .dockerignore\n => => transferring context: 2B\n => [demo-2 internal] load metadata for docker.io/library/debian:10\n => [demo-2 1/4] FROM docker.io/library/debian:10@sha256:ebe4b9831fb22dfa778de4ffcb8ea0ad69b5d782d4e86cab14cc1fded5d8e761\n => => resolve docker.io/library/debian:10@sha256:ebe4b9831fb22dfa778de4ffcb8ea0ad69b5d782d4e86cab14cc1fded5d8e761\n => => sha256:85bed84afb9a834cf090b55d2e584abd55b4792d93b750db896f486680638344 50.44MB / 50.44MB\n => => sha256:ebe4b9831fb22dfa778de4ffcb8ea0ad69b5d782d4e86cab14cc1fded5d8e761 1.85kB / 1.85kB\n => => sha256:40dd1c1b1c36eac161ab63b6ce3a57d56ad79a667a37717a31721bac3f30aaf9 529B / 529B\n => => sha256:26a2b081e03207d26a105340161109ba0f00e857cbb0ff85aaeeeadd46b709c5 1.46kB / 1.46kB\n => => extracting sha256:85bed84afb9a834cf090b55d2e584abd55b4792d93b750db896f486680638344\n => [demo-2 2/4] RUN echo \"ARG1=demo-2/.env/ARG1\" | tee /root/arg1.txt\n => [demo-1 2/4] RUN echo \"ARG1=demo-1/.env/ARG1\" | tee /root/arg1.txt\n => [demo-1 3/4] RUN echo \"ARG2=default 2\" | tee /root/arg2.txt\n => [demo-2 3/4] RUN echo \"ARG2=default 2\" | tee /root/arg2.txt\n => [demo-2 4/4] RUN echo \"ARG3=demo-2/.env/ARG3\" | tee /root/arg3.txt\n => [demo-1 4/4] RUN echo \"ARG3=demo-1/.env/ARG3\" | tee /root/arg3.txt\n => [demo-2] exporting to image\n => => exporting layers\n => => writing image sha256:553f294a410ceeb3c0ac9d252d443710c804d3f7437ad7fffa586967517f5e7a\n => => naming to docker.io/library/demo-1\n => => writing image sha256:84bb2bd0ffae67ffed0e74efbf9253b6d634a6f37c6f99bc4eedea81846a9352\n => => naming to docker.io/library/demo-2\n                                     \nUse 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n[+] Running 3/3              \n \u283f Network docker_default     Created\n \u283f Container docker-demo-1-1  Created\n \u283f Container docker-demo-2-1  Created\nAttaching to docker-demo-1-1, docker-demo-2-1\n\ndocker-demo-1-1  | ARG1=demo-1/.env/ARG1 ARG2=default 2 ARG3=\ndocker-demo-1-1  | while at build time:\ndocker-demo-1-1  | ARG1=demo-1/.env/ARG1\ndocker-demo-1-1  | ARG2=default 2\ndocker-demo-1-1  | ARG3=demo-1/.env/ARG3\n\ndocker-demo-2-1  | ARG1=var.env/ARG1 ARG2=var.env/ARG2 ARG3=var.env/ARG3\ndocker-demo-2-1  | while at build time:\ndocker-demo-2-1  | ARG1=demo-2/.env/ARG1\ndocker-demo-2-1  | ARG2=default 2\ndocker-demo-2-1  | ARG3=demo-2/.env/ARG3\n\ndocker-demo-1-1 exited with code 0\ndocker-demo-2-1 exited with code 0\nHere, we can see again that the \".env\" values and those of file_env: [ filename.env ] play different roles that don't overlap.\nFurthermore:\nGiven the absence of a Dockerfile command line ENV ARG3=\"${ARG3}\", the value of build-arg ARG3 is not propagated at runtime (see the ARG3= line in the output above).\nBut the value can be exported at runtime anyway if it is defined/overidden in the environment: or env_file: sections in the docker-compose.yml file (see the ARG3=var.env/ARG3 line in the output above).\nFor more details, see the documentation of the ARG directive.\nRemarks on the docker-compose --env-file option use-case\nAs mentioned by the OP, docker-compose also enjoys a useful CLI option --env-file (which is precisely named the same way as the very different env-file: field, which is unfortunate, but nevermind).\nThis option allows for the following use-case (excerpt from OP's code):\nFile docker-compose.yml:\nservices:\n  home:\n    image: home-${ENV_NUM}\n    build:\n      args:\n        ARG1: \"${ARG1}\"\n      ...\n    labels:\n      - traefik.http.routers.home.rule=Host(`${DOMAIN}`)\n      ...\n    env_file:\n      - ${ENV}\nFile prod.env:\nDOMAIN = 'actualdomain.com'\nENV = 'prod.env'\nENV_NUM = 1\nARG1 = 'value 1'\nFile dev.env:\nDOMAIN = 'localhost'\nENV = 'dev.env'\nENV_NUM = 0\nARG1 = 'value 1'\nThen run:\ndocker-compose --env-file prod.env build,\nor docker-compose --env-file dev.env build\nAs an aside, even if most of this answer up to now, amounted to illustrating that the \".env\" filename and env_file: files enjoy a very different semantics\u2026 it is true that they can also be combined \"nicely\" this way, as suggested by the OP, to achieve this use case.\nNote in passing that the docker-compose config is also applicable to \"debug\" the Compose specification:\ndocker-compose --env-file prod.env config,\nor docker-compose --env-file dev.env config.\nNow regarding the last question:\nGetting the values from the prod.env or dev.env files to docker-compose is not the issue. The issue is getting those values to the Dockerfile.\nit can first be noted that there are two different cases:\nEither the two different deployment environments (prod.env and dev.env) can share the same image, so that the difference only lies in the runtime environment variables (not the docker build args).\nOr, depending on the file passed for --env-file, the images should be different (and then a docker-compose --env-file \u2026 build is indeed necessary).\nIt appears that most of the time, case 1. can be achieved (and it is also the case in the question's configuration, because the ARG1 values are the same in prod.env and dev.env) and can be viewed as more-interesting for the sake of reproducibility (because we are sure that the \"prod\" image will be the same as the \"dev\" image).\nYet, sometimes it's impossible to do so and we are \"in case 2.\", e.g. if the Dockerfile has a specific step, maybe related to tests or so, that has to be enabled (resp. disabled) in production mode.\nSo now, let us assume we're in case 2. How can we pass \"everything\" from the --env-file to the Dockerfile? There is only one solution, namely, extending the args: map of the docker-compose.yml and include each variable you are interested in, for example:\nservices:\n  home:\n    image: home-${ENV_NUM}\n    build: \n      context: .\\home\n      args:\n        DOMAIN: \"${DOMAIN}\"\n        ENV_NUM: \"${ENV_NUM}\"\n        ARG1: \"${ARG1}\"\n    networks:\n      - demo-net\n    env_file:\n      - ${ENV}\n    labels:\n      - traefik.enable=true\n      - traefik.http.routers.home.rule=Host(`${DOMAIN}`)\n      - traefik.http.routers.home.entrypoints=web\n    volumes:\n      - g:\\:c:\\sharedrive\nEven if there is no other solution to pass arguments at build time (from docker-compose to the underlying docker build -f Dockerfile \u2026), this has the advantage of being \"declarative\" (only the variables mentioned in args: will be actually passed to the Dockerfile).\nDrawback?\nThe only drawback I see is that you may have unneeded extra environment variables at runtime (from docker-compose to the underlying docker run -e \u2026), such as ENV=prod.env.\nIf this is an issue, you might want to split your \".env\" files like this:\nFile prod.env:\nDOMAIN = 'actualdomain.com'\nENV = 'prod-run.env'\nENV_NUM = 1\nARG1 = 'value 1'\nFile prod-run.env:\nDOMAIN = 'actualdomain.com'\nENV_NUM = 1\n(assuming you only want to export these two environment variables at runtime).\nOr alternatively, to better follow the usual Do-not-Repeat-Yourself rule, remove prod-run.env, then pass these values as docker-compose build arguments as mentioned previously:\nargs:\n  DOMAIN: \"${DOMAIN}\"\n  ENV_NUM: \"${ENV_NUM}\"\nand write in the Dockerfile:\nARG DOMAIN\nARG ENV_NUM\n\n# ... and in the end:\n\nENV DOMAIN=\"${DOMAIN}\"\nENV ENV_NUM=\"${ENV_NUM}\"\nI already gave an example of these Dockerfile directives in section \"Experiment session 2\".\n(Sorry for the significant length of this answer BTW :)",
    "Not understanding how docker build --secret is supposed to be used": "Short answer\nCould you try to combine 2 RUN coverage with RUN --mount ... ?\nI think it will be something like that :\nRUN --mount=type=secret,id=mysecret \\\n  export MYSECRET=$(cat /run/secrets/mysecret) \\\n  && coverage run --omit='src/manage.py,src/config/*,*/.venv/*,*/*__init__.py,*/tests.py,*/admin.py' src/manage.py test src \\\n  && coverage report\nDetail\nThis --secret flag persists only in one docker layer.\nThe final image built will not have the secret file\nDocumentation :\nhttps://docs.docker.com/engine/reference/builder/#run---mounttypesecret\nBlog about it:\nhttps://pythonspeed.com/articles/docker-buildkit/\neach RUN creates one intermediate layer.\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/\nSo, according to my understanding,\nRUN --mount=type=secret,id=mysecret export MYSECRET=$(cat /run/secrets/mysecret)\nwill not persist both in your final image and in your last RUN.\nEDIT 1\nRUN is executed by /bin/sh not /bin/bash so you have to instanciate MYSECRET and export it in 2 steps :\n/bin/sh -c [your RUN instructions]\nRUN --mount=type=secret,id=mysecret \\\n    MYSECRET=$(cat /run/secrets/mysecret) \\\n    && export MYSECRET \\\n    && coverage run --omit='src/manage.py,src/config/*,*/.venv/*,*/*__init__.py,*/tests.py,*/admin.py' src/manage.py test src \\\n    && coverage report\nHere is my working example :\ntest.py\nprint MYSECRET value\nimport os\n\nprint(os.environ[\"MYSECRET\"])\nmysecret.txt\ncontains MYSECRET value\nHello\nDockerfile\nbased on python:3.9 image\nFROM python:3.9\n\nCOPY test.py /tmp/test.py\n\nRUN --mount=type=secret,id=mysecret \\\n  MYSECRET=$(cat /run/secrets/mysecret) \\\n  && export MYSECRET \\\n  && python /tmp/test.py\nBuild output\n...\n#7 [3/3] RUN --mount=type=secret,id=mysecret     MYSECRET=$(cat /run/secrets/mysecret)     && export MYSECRET     && python /tmp/test.py\n#7 sha256:f0134fcb389100e7094a47f437f8630e67da83447daaf617756c1d8432163bae\n#7 0.374 Hello\n#7 DONE 0.4s\n...",
    "speed up docker healthcheck containers": "Edit 2023-07-17\nThere is now support implemented for setting a start-interval to check in a shorter interval during the start-period\nThe feature is expected to be released in version 25 of the docker engine.\nExample:\nHEALTHCHECK --interval=5m --start-period=3m --start-interval=10s \\\n  CMD curl -f http://localhost/ || exit 1\nIn this example, there is a start-interval of 10s, which means the first healthcheck is done 10s after the container has been started. After that, the healthcheck is repeated every 10s, until either the health state switches to healthy, or the start-period is over (3m). After that, it proceeds the healthchecks at the regular interval (5m).\nDocker Docs: https://docs.docker.com/engine/reference/builder/#healthcheck\nRelated pull request: https://github.com/moby/moby/pull/40894\nOriginal Answer as of 2021\nThere is currently no built in way to decrease the time, until the first healthcheck is performed. Docker always waits a full interval between the container start and the first healthcheck. The start-period option just defines a grace time, that allow healthchecks to fail without marking the container as unhealthy. This will only be meaningful, if the interval is lower than the start-period.\nThere is a feature request to add an option, that decreases the interval while the container is starting, to get the container faster into a healthy state: https://github.com/moby/moby/issues/33410",
    "How to install tshark on Docker?": "RUN DEBIAN_FRONTEND=noninteractive apt-get install -y tshark\nWill install without needing user interaction.",
    "Docker container works from Dockerfile but get next: not found from docker-compose container": "This is happening because your local filesystem is being mounted over what is in the docker container. Your docker container does build the node modules in the builder stage, but I'm guessing you don't have the node modules available in your local file system.\nTo see if this is what is happening, on your local file system, you can do a yarn install. Then try running your container via docker again. I'm predicting that this will work, as yarn will have installed next locally, and it is actually your local file system's node modules that will be run in the docker container.\nOne way to fix this is to volume mount everything except the node modules folder. Details on how to do that: Add a volume to Docker, but exclude a sub-folder\nSo in your case, I believe you can add a line to your compose file:\nfrontend:\n    ...\n    volumes:\n      - ./frontend:/app\n      - ./frontend/node_modules # <-- try adding this!\n    ...\nThat should allow the docker container's node_modules to not be overwritten by any volume mount.",
    "dockerfile copy from with variable": "You can do what you want, but you need to have a FROM statement identifying the image you want to copy from. Something like this\nARG MY_VERSION\nFROM my-image:$MY_VERSION as source\nFROM scratch as final\nCOPY --from=source /src /dst\nReplace scratch with the base image you want to use for your new image.\nHere's an example to show that it works. Dockerfile:\nARG MY_VERSION\nFROM ubuntu:$MY_VERSION as source\nFROM alpine:latest\nCOPY --from=source /etc/os-release /\nBuild and run with\ndocker build --build-arg MY_VERSION=20.04 -t test .\ndocker run --rm test cat /os-release\nThe output shows\nNAME=\"Ubuntu\"\nVERSION=\"20.04.2 LTS (Focal Fossa)\"\nID=ubuntu\n...\nwhich shows that it has copied a file from the Ubuntu 20.04 image to an Alpine image.",
    "What exactly does \"-Djava.security.egd=file:/dev/./urandom\" do when containerizing a Spring Boot application": "The purpose of that security property is to speed up tomcat startup. By default the library used to generate random number in JVM on Unix systems relies on /dev/random. On docker containers there isn't enough entropy to support /dev/random. See Not enough entropy to support /dev/random in docker containers running in boot2docker. The random number generator is used for session ID generation. Changing it to /dev/urandom will make the startup process faster.\nSimilar question Slow startup on Tomcat 7.0.57 because of SecureRandom",
    "How to install kerberos client in docker?": "You need the -y parameter for the apt\nFROM node:latest\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN apt-get -qq update && \\\n    apt-get -yqq install krb5-user libpam-krb5 && \\\n    apt-get -yqq clean\n\nCOPY / ./\n\nEXPOSE 3000\n\nCMD [\"npm\", \"start\"]\nAnd pay attention, that each RUN directive creates one additional layer in the image. That means, your clean command will create a new layer, but all package cache will remain in other layers. So reducing the amount of these directives will be nice. It would help you to shrink the image size.",
    "How does Docker run a command without invoking a command shell?": "If I understand your question correctly, you're asking how something can be run (specifically in the context of docker) without invoking a command shell.\nThe way things are run in the linux kernel are usually using the exec family of system calls.\nYou pass it the path to the executable you want to run and the arguments that need to be passed to it via an execl call for example.\nThis is actually what your shell (sh, bash, ksh, zsh) does under the hood anyway. You can observe this yourself if you run something like strace -f bash -c \"cat /tmp/foo\"\nIn the output of that command you'll see something like this:\nexecve(\"/bin/cat\", [\"cat\", \"/tmp/foo\"], [/* 66 vars */]) = 0\nWhat's really going on is that bash looks up cat in $PATH, it then finds that cat is actually an executable binary available at /bin/cat. It then simply invokes it via execve. and the correct arguments as you can see above.\nYou can trivially write a C program that does the same thing as well. This is what such a program would look like:\n#include<unistd.h>\n\nint main() {\n\n    execl(\"/bin/cat\", \"/bin/cat\", \"/tmp/foo\", (char *)NULL);\n\n    return 0;\n}\nEvery language provides its own way of interfacing with these system calls. C does, Python does and Go, which is what's used to write Docker for the most part, does as well. A RUN instruction in the docker likely translates to one of these exec calls when you hit docker build. You can run an strace -f docker build and then grep for exec calls in the log to see how the magic happens.\nThe only difference between running something via a shell and directly is that you lose out on all the fancy stuff your shell will do for you, such as variable expansion, executable searching etc.",
    "What does the Docker sleep command do? [closed]": "docker container run -d --name mycontainer myimage:mytag sleep infinity\nThe last part after the image name (i.e. sleep infinity) is not a docker command but a command sent to the container to override its default command (set in the Dockerfile).\nAn extract from the documentation (you can get it typing man sleep in your terminal, it may vary depending on the implementation)\nPause for NUMBER seconds. SUFFIX may be 's' for seconds (the default), 'm' for minutes, 'h' for hours or 'd' for days\nTo my surprise, the parameter infinity is not documented in my implementation but is still accepted. It's quite easy to understand: it pauses indefinitely (i.e. until the command is stopped/killed). In your own example above, it will pause for one day.\nWhat is the usual reason to use sleep as a command to run a docker container?\nA docker container will live until the command it runs finishes. This command is normally set in the Dockerfile used to build the image (in a CMD stanza) and can be overridden on the command line (as in the above examples).\nA number of base images (like base OS for debian, ubuntu, centos....) will run a shell as the default command (bash or sh in general). If you try to spawn a container from that image using its default command, it will live until the shell exits.\nWhen running such an image interactively (i.e. with docker container run -it .....), it will run until you end your shell session. But if you want to launch it in the background (i.e. with docker container run -d ...) it will exit immediately leaving you with a stopped container.\nIn this case, you can \"fake\" a long running service by overriding the default command with a long running command that basically does nothing but wait for the container to stop. Two widely used commands for this are sleep infinity (or whatever period suiting your needs) and tail -f /dev/null\nAfter you launched a container like this you can use it to test whatever you need. The most common way is to run an interactive shell against it:\n# command will depend on shells available in your image\ndocker exec -it mycontainer [bash|sh|zsh|ash|...]\nOnce you are done with your experimentation/test, you can stop and recycle your container\ndocker container stop mycontainer\ndocker container rm mycontainer",
    "Couldn't connect to Docker Aerospike from host": "The IP 172.17.0.2 is only accessible within Docker (therefore you can use another container to connect). In case you want to connect from your host you need to map the respective port.\ndocker run -d --name aerospike -p 3000:3000 aerospike/aerospike-server\nAfterwards you can use:\nAerospikeClient client = new AerospikeClient(\"localhost\", 3000);",
    "Docker ONBUILD COPY doesn't seem to copy files": "You have to use COPY to build your image. Use ONBUILD if your image is kind of template to build other images.\nSee Docker documentation:\nThe ONBUILD instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build. The trigger will be executed in the context of the downstream build, as if it had been inserted immediately after the FROM instruction in the downstream Dockerfile.",
    "Parse a variable with the result of a command in DockerFile": "I had same issue and found way to set environment variable as result of function by using RUN command in dockerfile.\nFor example i need to set SECRET_KEY_BASE for Rails app just once without changing as would when i run:\ndocker run  -e SECRET_KEY_BASE=\"$(openssl rand -hex 64)\"\nInstead it i write to Dockerfile string like:\nRUN bash -l -c 'echo export SECRET_KEY_BASE=\"$(openssl rand -hex 64)\" >> /etc/bash.bashrc'\nand my env variable available from root, even after bash login. or may be\nRUN /bin/bash -l -c 'echo export SECRET_KEY_BASE=\"$(openssl rand -hex 64)\" > /etc/profile.d/docker_init.sh'\nthen it variable available in CMD and ENTRYPOINT commands\nDocker cache it as layer and change only if you change some strings before it.\nYou also can try different ways to set environment variable.",
    "Running a longer command from docker": "I was fiddling with crossbuild* and was wondering about how to use here documents to pipe commands to a Docker container. Here's the solution.\n$ docker run --rm --interactive --volume $(pwd):/workdir --env CROSS_TRIPLE=x86_64-apple-darwin multiarch/crossbuild /bin/bash -s <<EOF\nmkdir build && cd build\ncmake ..\nmake\nEOF\nQuick rundown of what's happening.\n--rm tells Docker to remove the container when it finished execution, otherwise it would show up in the output docker ps -a (not mandatory to use of course)\n--interactive, -i is needed, otherwise /bin/bash would not run in an interactive environment and would not accept the here document from stdin as its input\nabout the -s flag passed to /bin/bash\nIf the -s option is present, or if no arguments remain after option processing, then commands are read from the standard input.\n--volume $(pwd):/workdir, just -v will mount the current working directory on the host to /workdir in the container\n--env CROSS_TRIPLE=x86_64-apple-darwin, or simple -e tells the crossbuild container about the target platform and architecture (the container's entry point is /usr/bin/crossbuild, which is a shell script and based on the environment variable it's symlink the right toolchain components to the right places for the cross compilation to work)\nmultiarch/crossbuild the name of the Docker container to run (available in Docker Hub)\nThe commands can be fed to Docker like this as well.\n$ cat a.sh\nmkdir build && cd build\ncmake ..\nmake\n$ docker run --rm -i -v $(pwd):/workdir -e CROSS_TRIPLE=x86_64-apple-darwin multiarch/crossbuild /bin/bash -s < a.sh\nHoped this helps.\nUpdate\nActually it seems you don't even need to use /bin/bash -s, it can be ommited, at least for the crossbuild container, YMMV.\n*Linux based container used to produce multi-arch binaries: Linux, Windows and OS X, very cool.",
    "Dockerfile COPY and keep folder structure": "After a lot of searching around, I managed to find a solution that works for me\nI ended up creating a tar of all the package.json files\ntar --exclude node_modules -cf package.json.tar  */package.json\nAnd using the docker add command to unpack that tar inside the image\nADD package.json.tar .\nThe add command will detect that the input is a tar file, and will extract it inside the image.\nThe down side of using this approach is you have to run the tar command sometime before you build the image.",
    "Installing python in Dockerfile without using python image as base": "just add this with any other thing you want to apt-get install:\nRUN apt-get update && apt-get install -y \\\n    python3.6 &&\\\n    python3-pip &&\\\nin alpine it should be something like:\nRUN apk add --update --no-cache python3 && ln -sf python3 /usr/bin/python &&\\\n    python3 -m ensurepip &&\\\n    pip3 install --no-cache --upgrade pip setuptools &&\\",
    "Go server empty response in Docker container": "Don't use localhost (basically an alias to 127.0.0.1) as your server address within a Docker container. If you do this only 'localhost' (i.e. any service within the Docker container's network) can reach it.\nDrop the hostname to ensure it can be accessed outside the container:\n// Addr:         \"localhost:\" + port, // unreachable outside container\nAddr:         \":\" + port, // i.e. \":3000\" - is accessible outside the container",
    "Build 2 images with Docker from a single Dockerfile while using different base images": "You could use build-time configuration with ARG.\nFROM instructions support variables that are declared by any ARG instructions that occur before the first FROM. An ARG declared before a FROM is outside of a build stage, so it can\u2019t be used in any instruction after a FROM. To use the default value of an ARG declared before the first FROM use an ARG instruction without a value inside of a build stage:\nDockerfile:\nARG imagename\nFROM $imagename\nRUN echo $imagename\nRun with two different base images:\ndocker build --build-arg imagename=alpine .\noutputs:\nStep 1/3 : ARG imagename\nStep 2/3 : FROM $imagename\nlatest: Pulling from library/alpine\nff3a5c916c92: Pull complete \nDigest: sha256:e1871801d30885a610511c867de0d6baca7ed4e6a2573d506bbec7fd3b03873f\nStatus: Downloaded newer image for alpine:latest\n ---> 3fd9065eaf02\nStep 3/3 : RUN echo $imagename\n ---> Running in 96b45ef959c3\n\nRemoving intermediate container 96b45ef959c3\n ---> 779bfc103e9e\nSuccessfully built 779bfc103e9e\nAlternatively:\ndocker build --build-arg imagename=busybox .\nresults in:\nStep 1/3 : ARG imagename\nStep 2/3 : FROM $imagename\nlatest: Pulling from library/busybox\n07a152489297: Pull complete \nDigest: sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47\nStatus: Downloaded newer image for busybox:latest\n ---> 8c811b4aec35\nStep 3/3 : RUN echo $imagename\n ---> Running in 6027fe4f5b7b\n\nRemoving intermediate container 6027fe4f5b7b\n ---> 28640f123967\nSuccessfully built 28640f123967\nSee also this blogpost for more ideas.",
    "Docker creating multiple images": "Images are immutable, so any change you make results in a new image being created. Since your compose file specifies the build command, it will rerun the build command when you start the containers. If any files you are including with a COPY or ADD change, then the existing image cache is no longer used and it will build a new image without deleting the old image.\nNote, I'd recommend naming your image in the compose file so it's clear which image is being rebuilt. And you can watch the compose build output to the the first step that doesn't report using the cache to see what is changing. If I was to guess, the line that breaks your cache and causes a new image is this one in nodejs:\nCOPY . /usr/src/app\nIf the files being changed and causing the rebuild are not needed in your container, then use a .dockerignore file to exclude the unnecessary files.",
    "Docker: cannot open port from container to host": "Please check the program in container is listening on interface 0.0.0.0.\nIn container, run command:\nss -lntp\nIf it appears like:\nLISTEN  0   128   127.0.0.1:5000  *:*\nthat means your web app only listen at localhost so container host cannot access to your web app. You should make your server listen at 0.0.0.0 interface by changing your web app build setting.\nFor example if your server is nodejs app:\nvar app = connect().use(connect.static('public')).listen(5000, \"0.0.0.0\");\nIf your server is web pack:\n \"scripts\": {\n    \"dev\": \"webpack-dev-server --host 0.0.0.0 --port 5000 --progress\"\n  }",
    "Does docker reuse images when multiple containers run on the same host?": "Dockers Understand images, containers, and storage drivers details most of this.\nFrom Docker 1.10 onwards, all the layers that make up an image have an SHA256 secure content hash associated with them at build time. This hash is consistent across hosts and builds, as long as the content of the layer is the same.\nIf any number of images share a layer, only the 1 copy of that layer will be stored and used by all images on that instance of the Docker engine.\nA tag like debian can refer to multiple SHA256 image hash's over time as new releases come out. Two images that are built with FROM debian don't necessarily share layers, only if the SHA256 hash's match.\nAnything that runs the Docker Engine underneath will use this storage setup.\nThis sharing also works in the Docker Registry (>2.2 for the best results). If you were to push images with layers that already exist on that registry, the existing layers are skipped. Same with pulling layers to your local engine.",
    "How can I set $PS1 with Dockerfile?": "What is happening here is that PS1 is being redefined by the file ~/.bashrc that is in your image and automatically sourced on start up of your container (it could be on another file - I am not sure if PS1 always get defined in ~/.bashrc on all linux distros).\nAssuming it is defined in ~/.bashrc, then you could write in your Dockerfile a RUN command that could look like:\nRUN echo PS1=\\\"\\\\h:\\\\W \\\\u$ \\\" >> ~/.bashrc\nEt voila!",
    "ArchLinux docker CI-failed to initialise alpm library :: returned a non-zero code: 255": "This workaround has worked for me. It requires patching glibc to an older version.\nRUN patched_glibc=glibc-linux4-2.33-4-x86_64.pkg.tar.zst && \\\ncurl -LO \"https://repo.archlinuxcn.org/x86_64/$patched_glibc\" && \\\nbsdtar -C / -xvf \"$patched_glibc\"\nhttps://github.com/qutebrowser/qutebrowser/commit/478e4de7bd1f26bebdcdc166d5369b2b5142c3e2",
    "Does cleaning the user yarn cache will affect docker image?": "yarn Version 1\nYou can safely remove cache files, it will not affect your application. There's even a dedicated command for that:\n$ yarn cache clean\nyarn Version 2\nWith Plug'n'Play, however, clearing the cache will very likely break your application because dependencies are no longer placed in node_modules. Here's what the documentation says:\nIn this install mode (now the default starting from Yarn v2), Yarn generates a single .pnp.js file instead of the usual node_modules. Instead of containing the source code of the installed packages, the .pnp.js file contains a map linking a package name and version to a location on the disk, and another map linking a package name and version to its set of dependencies. Thanks to this efficient system, Yarn can tell Node exactly where to look for files being required - regardless of who asks for them!\nThe location on the disk is cache.\nYou can get the old behavior back by placing this in your .yarnrc.yml file.\nnodeLinker: node-modules\nRead more about Plug'n'Play here",
    "Password does not match for user \"postgres\"": "The Postgres password is set on the first initialization if the db (when the var/lib/postgresql/data folder is empty). Since you probably ran docker-compose before with a different password, the new one will not take effect. To fix this, remove the volume in your host machine and have docker-compose automatically recreate it next time you run docker-compose up. Note that removing this volume will delete all data stored in PostgreSQL",
    "How to set username and password for our own docker private registry?": "How to set username and password for our own docker private registry?\nThere are couple ways to implement basic auth in DTR. The simplest way is to put the DTR behind a web proxy and use the basic auth mechanism provided by the web proxy.\nTo enable basic auth in DTR directly? This is how.\nCreate a password file containing username and password: mkdir auth && docker run --entrypoint htpasswd registry:2 -Bbn your-username your-password > auth/htpasswd.\nStop DTR: docker container stop registry.\nStart DTR again with basic authentication, see commands below.\nNote: You must configure TLS first for authentication to work.\ndocker run -d \\\n  -p 5000:5000 \\\n  --restart=always \\\n  --name registry \\\n  -v `pwd`/auth:/auth \\\n  -e \"REGISTRY_AUTH=htpasswd\" \\\n  -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\\n  -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\\n  -v `pwd`/certs:/certs \\\n  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\\n  -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\\n  registry:2\nHow to use them when we want to use the image from that repository?\nBefore pulling images, you need first to login to the DTR:\ndocker login your-domain.com:5000\nAnd fill in the username and password from the first step.",
    "How to get version value of package.json inside of Dockerfile?": "If you just need the version from inside of your node app.. require('./package.json').version will do the trick.\nOtherwise, since you're already building your own container, why not make it easier on yourself and install jq? Then you can run VERSION=$(jq .version package.json -r).\nEither way though, you cannot simply export a variable from a RUN command for use in another stage. There is a common workaround though:\nFROM node:8-alpine\nRUN apk update && apk add jq\nCOPY package.json .\nCOPY server.js .\nRUN jq .version package.json -r > /root/version.txt\nCMD VERSION=$(cat /root/version.txt) node server.js\nResults from docker build & run:\n{ NODE_VERSION: '8.11.1',\n  YARN_VERSION: '1.5.1',\n  HOSTNAME: 'xxxxx',\n  SHLVL: '1',\n  HOME: '/root',\n  VERSION: '1.0.0',\n  PATH: '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n  PWD: '/' }",
    "Conditional Block in Docker": "If you don't want to use all those RUN if statements, you can instead create a bash script with the setup procedure and call it from the Dockerfile. For example:\nFROM centos:centos7\nMAINTAINER Someone <someone@email.com>\n\nARG BUILD_TOOL\n\nCOPY setup.sh /setup.sh\n\nRUN ./setup.sh\n\nRUN rm /setup.sh\nAnd the setup.sh file (don't forget to make it executable):\nif [ \"${BUILD_TOOL}\" = \"MAVEN\" ]; then\n    echo \"Step 1 of MAVEN setup\";\n    echo \"(...)\";\n    echo \"Done MAVEN setup\";\nelif [ \"${BUILD_TOOL}\" = \"ANT\" ]; then\n    echo \"Step 1 of ANT setup\";\n    echo \"(...)\";\n    echo \"Done ANT setup\";\nfi\nYou can then build it using docker build --build-arg BUILD_TOOL=MAVEN . (or ANT).\nNote that I used a shell script here, but if you have other interpreters available (ex: python or ruby), you can also use them to write the setup script.",
    "Gracefully Stopping Docker Containers": "Finally solved the problem.\nTcsh shell doesn't receive most of the signals like SIGTERM which is the signal sent by docker when stopping the container.\nSo I changed the script to use bash shell and whenever I want to run a tcsh command I just do it like this:\n/bin/tcsh ./my-command\nSo, my docker-entrypoint.sh is like this:\n#!/bin/bash\n\n# SIGTERM-handler this funciton will be executed when the container receives the SIGTERM signal (when stopping)\nterm_handler(){\n   echo \"***Stopping\"\n   /bin/tcsh ./my-cleanup-command\n   exit 0\n}\n\n# Setup signal handlers\ntrap 'term_handler' SIGTERM\n\necho \"***Starting\"\n/bin/tcsh ./my-command\n\n# Running something in foreground, otherwise the container will stop\nwhile true\ndo\n   #sleep 1000 - Doesn't work with sleep. Not sure why.\n   tail -f /dev/null & wait ${!}\ndone",
    "How to get all available environment-variables of docker image?": "docker image inspect {image_name} has a section called \"Env\" where you can see this info.",
    "How to create a solr core using docker-solrs image extension mechanism?": "Provide precreate-core file location which is to be executed, so edit create-a12core.sh as given below\n #!/bin/bash\n /opt/docker-solr/scripts/precreate-core  A12Core /A12Core\nTested and Works !!!",
    "Docker user namespacing map user in container to host": "No, if you want to use user-namespaces, there's currently no way to remap the user for bind-mounted directories and files.\nBasically, the issue you're running into is the exact goal of user namespaces; preventing a privileged user inside a container to get access to files on the host. This protects you from a process being able to escape the container from doing damage on the host.\nHowever it looks like your goal is to give the postgres user access to files on your host, which are owned by a local user. For that situation, there may be another approach; run the container with the same uid:gid as the user that owns the files on the host. This may require changes to the image (because some parts in the images are currently created with the postgres user, which currently has a different uid:gid (also see some information about this in this answer)\nCurrently, doing so with the postgres official image requires some manual changes, but a pull request was recently merged for the official postgres image that makes this work out of the box (see docker-entrypoint.sh#L30-L41)\nYou can find details on the pull request; https://github.com/docker-library/postgres/pull/253, and the associated documentation; https://github.com/docker-library/docs/pull/802. This change should be available soon, but in the meantime you can create a custom image that extends the official PostgreSQL image.",
    "How to install a specific version of Java 8 using Dockerfile": "As most PPA packages pack the latest stable version, I would recommend installing Java manually from Oracle, just like in this answer.\nYou can do all the work in the script too, the steps are:\nget the tarball with wget,\nuntar it with tar -xz,\nuse update-alternatives to set is as default",
    "How do I use command substition in Dockerfile": "What went wrong\nThe $( ... ) command substitution you attempted is for Bash, whereas the Dockerfile is not Bash. So docker doesn't know what to do with that, it's just plain text to docker, docker just spews out what you wrote as-is.\nRecommendation\nTo avoid hard-coding values into a Dockerfile, and instead, to dynamically change a setting or custom variable as PYTHONPATH during the build, perhaps the ARG ... , --build-arg docker features might be most helpful, in combination with ENV ... to ensure it persists.\nWithin your Dockerfile:\nARG PYTHON_PATH_ARG\n\nENV PYTHONPATH ${PYTHON_PATH_ARG}\nIn Bash where you build your container:\npython_path=\"/usr/local$(python3 -c 'from distutils import sysconfig; print(sysconfig.get_python_lib())')\"\n\ndocker build --build-arg PYTHON_PATH_ARG=$python_path .\nExplanation\nAccording to documentation, ARG:\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.\nSo, in Bash we first:\npython_path=\"/usr/local$(python3 -c 'from distutils import sysconfig; print(sysconfig.get_python_lib())')\"\n$(...) Bash command substitution is used to dynamically put together a Python path value\nthis value is stored temporarily in a Bash variable $python_path for clarity\ndocker build --build-arg PYTHON_PATH_ARG=$python_path .\nBash variable $python_path value is passed to docker's --build-arg PYTHON_PATH_ARG\nWithin the Dockerfile:\nARG PYTHON_PATH_ARG\nso PYTHON_PATH_ARG stores the value from --build-arg PYTHON_PATH_ARG...\nARG variables are not equivalent to ENV variables, so we couldn't merely do ARG PYTHONPATH and be done with it. According to documentation about Using arg variables:\nARG variables are not persisted into the built image as ENV variables are.\nSo finally:\nENV PYTHONPATH ${PYTHON_PATH_ARG}\nWe use Dockerfile's ${...} convention to get the value of PYTHON_PATH_ARG, and save it to your originally named PYTHONPATH environment variable\nDifferences from original code\nYou originally wrote:\nENV PYTHONPATH /usr/local/$(python3 -c 'from distutils import sysconfig; print(sysconfig.get_python_lib())')\nI re-wrote the Python path finding portion as a Bash command, and tested on my machine:\n$ python_path=\"/usr/local/$(python3 -c 'from distutils import sysconfig; print(sysconfig.get_python_lib())')\"\n\n$ echo $python_path\n/usr/local//usr/lib/python3/dist-packages\nNotice there is a double forward slash ... local//usr ... , not sure if that will break anything for you, depends on how you use it in your code.\nInstead, I changed it to:\n$ python_path=\"/usr/local$(python3 -c 'from distutils import sysconfig; print(sysconfig.get_python_lib())')\"\nResult:\n$ echo $python_path\n/usr/local/usr/lib/python3/dist-packages\nSo this new code will have no double forward slashes.",
    "libGL error: failed to load driver swrast in docker container": "Figured it out. I had to build the gui with hardware accelerated OpenGL support. Theres a repo (https://github.com/gklingler/docker3d) that contains docker images with nvidia or other graphics drivers support.\nThe other catch was, it didn't work for me unless the host and the container had the exact same driver. In order to resolve this, you can run the following shell script if you're running on linux:\n#!/bin/bash\nversion=\"$(glxinfo | grep \"OpenGL version string\" | rev | cut -d\" \" -f1 | rev)\"\nwget http://us.download.nvidia.com/XFree86/Linux-x86_64/\"$version\"/NVIDIA-Linux-x86_64-\"$version\".run\nmv NVIDIA-Linux-x86_64-\"$version\".run NVIDIA-DRIVER.run",
    "Dockerfile FROM Instruction": "#Note: image1 and image2 can be same\n\nFROM image1\n.. any commands for image1\nFROM image2\n.. any commands for image2\nIt will create two images. It will return latest image id after the build(as the doc says). So this usage is possible(I didn't see that usage yet.), but in my opinion it can be used on exceptional cases. It doesn't seem a nice usage to build two different images and reaching first image id.\nMay be your requirement is building mass applications and able to build once a time together. So it's up to your requirement. Do you really need this usage is the main question.",
    "Self-hosted alternative to hub.docker.com?": "You need to seperately setup the registry and the build server separately. This way when you make a push to GitLab, it notifies the build system (via a POST) and builds the image. After the build is complete, the final image gets pushed to the registry (either self-hosted or to hub.docker.com).\nSetting up the Registry\nFirst make sure that you have docker installed.\nThen run the following command, which will start an instance of the registry.\nsudo docker run --restart='always' -d -p 5000:5000 --name=registry \\\n-e GUNICORN_OPTS=[\"--preload\"] \\\n-v /srv/registry:/tmp/registry \\\nregistry\nTo expose a Web UI for the above registry, run the following. (Replace with the IP of the registry)\nsudo docker run  -d -P --restart='always' \\\n-e ENV_DOCKER_REGISTRY_HOST=<REGISTRY_IP> \\\n-e ENV_DOCKER_REGISTRY_PORT=5000 \\\nkonradkleine/docker-registry-frontend\nSetting up the Build Server\nThe ubiquitous Jenkins build server can fill in this gap.\nYou'll need to install the GitLab CI plugin (for Jenkins) which partially emulates the GitLab CI API. Note than you need to also configure the CI plugin after installation from \"Manage Jenkins\" -> \"Configure System\". Note that the private token functionality is not implemented. So enter something random in that field.\nNow you can configure your GitLab repo to fire up a CI event after a PUSH to the repo using Services -> GitLab CI.\nPlease Note: I have tried this out on GitLab v7.7.2. AFAIK the newer GitLab release has interated the earlier seperate GitLab CI.\nOn the jenkins server, create a new freestyle project or edit an existing project. Then check Build on Push Events.\nNow for the final step, execute the following code snippet as a shell script. Note that you will need to start your docker daemon with the insecure registry option. Refer: https://docs.docker.com/registry/insecure/\n# Build and push image\ncd $WORKSPACE\ndocker build -t <REGISTRY_IP>:5000/<PROJECT_NAME>:latest .\ndocker push <REGISTRY_IP>:5000/<PROJECT_NAME>:latest\nAlternatively\nHave a look at tarzan. It works quite similar to docker hub but it needs to be triggered from a GitHub event (not GitLab). Also because I haven't tried it out, I can't vouch for it.\nI suspect that even though tarzan is said to work only with GitHub, it might also work with GitLab.",
    "Advantages of a Dockerfile": "Dockerfile is used for automation of work by specifying all step that we want on docker image.\nA Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Using docker build users can create an automated build that executes several command-line instructions in succession.\nyes , we can create Docker images but every time when we want to make any change then you have to change manually and test and push it .\nor if you use Dockerfile with dockerhub then it will rebuild automatically and make change on every modification and if something wrong then rebuild will fail.\nAdvantages of Dockerfile\nDockerfile is automated script of Docker images\nmanual image creation will become complicated when you want to test same setup on different OS flavor then you have to create image for all flavor but by small changing in dockerfile you can create images for different flavor\nit have simple syntax for image and do many change automatically that will take more time while doing manually.\nDockerfile have systematic step that can be understand by others easily and easy to know what exact configuration changed in base image.\nAdvantage of Dockerfile with dockerhub\nDocker Hub provide private repository for Dockerfile.\nDockerfile can share among team and organization.\nAutomatic image builds\nWebhooks that are attached to your repositories that allow you to trigger an event when an image or updated image is pushed to the repository\nwe can put Dockerfile on Github or Bitbucket\nDifference between committed image and Dockerfile image\nCommitted image : it commit a container\u2019s file changes or settings into a new image.\nUsage:  docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]\nCreate a new image from a container's changes\n\n  -a, --author=       Author (e.g., \"John Hannibal Smith <hannibal@a-team.com>\")\n  -c, --change=[]     Apply Dockerfile instruction to the created image\n  --help=false        Print usage\n  -m, --message=      Commit message\n  -p, --pause=true    Pause container during commit\nIt is good option to debug container and export changed setting into another image.but docker suggest to use dockerfile see here or we can say commit is versioning of docker or backup of image.\nThe commit operation will not include any data contained in volumes mounted inside the container.\nBy default, the container being committed and its processes will be paused while the image is committed. This reduces the likelihood of encountering data corruption during the process of creating the commit. If this behavior is undesired, set the \u2018p\u2019 option to false.\nDockerfile based image:\nit always use base image for creating new image. let suppose if you made any change in dockerfile then it will apply all dockerfile steps on fresh image and create new image. but commit use same image.\nmy point of view we have to use dockerfile that have all step that we want on image but if we create image from commit then we have to document all change that we made that may be needed if we want to create new image and we can say dockerfile is a documentation of image.",
    "Docker Compose: executable file not found in $PATH: unknown": "As pointed out in @derpirscher's comment and mine, one of the issues was the permission of your script(s) and the way they should be called as the ENTRYPOINT (not CMD).\nConsider this alternative code for your Dockerfile :\nFROM node:16\n\nWORKDIR /usr/src/app\n\nCOPY package*.json ./\nCOPY wait-for-it.sh ./\nCOPY docker-deploy.sh ./\n\n# Use a single RUN command to avoid creating multiple RUN layers\nRUN chmod +x wait-for-it.sh \\\n  && chmod +x docker-deploy.sh \\\n  && npm install --legacy-peer-deps\n\nCOPY . .\n\nRUN npm run build\n\nENTRYPOINT [\"./docker-deploy.sh\"]\ndocker-deploy.sh script :\n#!/bin/sh\n\n# call wait-for-it with args and then start node if it succeeds\nexec ./wait-for-it.sh -h \"${DB_HOST}\" -p \"${DB_PORT}\" -t 300 -s -- node start\nSee this other SO question for more context on the need for the exec builtin in a Docker shell entrypoint.\nAlso, note that the fact this exec ... command line is written inside a shell script (not directly in an ENTRYPOINT / CMD exec form) is a key ingredient for using the parameter expansion.\nIn other words: in the revision 2 of your question, the \"${DB_HOST}:${DB_PORT}\" argument was understood literally because no shell interpolation occurs in an ENTRYPOINT / CMD exec form.\nRegarding the docker-compose.yml :\n# version: '3.7'\n# In the Docker Compose specification, \"version:\" is now deprecated.\n\nservices:\n  my-service:\n    build: .\n    # Add \"image:\" for readability\n    image: some-optional-fresh-tag-name\n    # Pass environment values to the entrypoint\n    environment:\n      DB_HOST: postgres\n      DB_PORT: ${DB_PORT}\n      # etc.\n    # Add network spec to make it explicit what services can communicate together\n    networks:\n      - postgres-network\n    # Add \"depends_on:\" to improve \"docker-run scheduling\":\n    depends_on:\n      - postgres\n\n  postgres:\n    # container_name: postgres # unneeded\n    image: postgres:14.3\n    environment:\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n      POSTGRES_USER: ${DB_USER}\n      POSTGRES_DB: my-service-db\n      PG_DATA: /var/lib/postgresql2/data\n    volumes:\n      - pgdata:/var/lib/postgresql2/data\n    networks:\n      - postgres-network\n    # ports:\n    #   - ${DB_PORT}:${DB_PORT}\n    # Rather remove this line in prod, which is a typical weakness, see (\u00a7)\n\nnetworks:\n  postgres-network:\n    driver: bridge\n\nvolumes:\n  pgdata:\n    # let's be more explicit\n    driver: local\nNote that in this Docker setting, the wait-for-it host should be postgres (the Docker service name of your database), not 0.0.0.0 nor localhost. Because the wait-for-it script acts as a client that tries to connect to the specified web service in the ambient docker-compose network.\nFor a bit more details on the difference between 0.0.0.0 (a server-side, catch-all special IP) and localhost in a Docker context, see e.g. this other SO answer of mine.\n(\u00a7): last but not least, the ports: [ \"${DB_PORT}:${DB_PORT}\" ] lines should rather be removed because they are not necessary for the Compose services to communicate (the services just need to belong to a common Compose network and use the other Compose services' hostname), while exposing one such port directly on the host increases the attack surface.\nLast but not least:\nTo follow-up this comment of mine, suggesting to run ls -l docker-deploy.sh; file docker-deploy.sh in your myapp/ directory as a debugging step (BTW: feel free to do this later on then comment for the record):\nAssuming there might be an unexpected bug in Docker similar to this one as pointed by @Lety:\nI'd suggest to just replacing (in the Dockerfile) the line\nRUN chmod +x wait-for-it.sh \\\n  && chmod +x docker-deploy.sh \\\n  && npm install --legacy-peer-deps\nwith\nRUN npm install --legacy-peer-deps\nand running directly in a terminal on the host machine:\ncd myapp/\nchmod -v 755 docker-deploy.sh\nchmod -v 755 wait-for-it.sh\n\ndocker-compose --env-file .env up --build\nIf this does not work, here is another useful information you may want to provide: what is your OS, and what is your Docker package name? (e.g. docker-ce or podman\u2026)",
    "Docker Error response from daemon: OCI runtime create failed container_linux.go:380: starting container process caused": "It looks like you're trying to replace the application code in the image with different code using Docker bind mounts. Docker's general model is that a container runs a self-contained image; you shouldn't need to separately install the application code on the host.\nIn particular, these two volumes: blocks cause the error you're seeing, and can safely be removed:\nservices:\n  react-app:\n    build:             # <-- This block builds an image with the code\n      context: ./\n      dockerfile: ./client/Dockerfile\n    # volumes:         # <-- So delete this block\n    #   - ./client:/usr/src/client:ro\n    #   - /usr/src/client/node_modules\n  node-app:\n    build: \n      context: ./\n      dockerfile: ./server/Dockerfile\n    # volumes:         # <-- And this one\n    #   - ./server:/usr/src/server:ro\n    #   - /usr/src/server/node_modules\nMechanically, the first volumes: line replaces the image's code with different code from the host, but with a read-only mount. The second volumes: line then further tries to replace the node_modules directory with an old copy from an anonymous volume. This will create the node_modules directory if it doesn't exist yet; but the parent directory is a read-only volume mount, resulting in the error you're seeing.",
    "Why is a folder created by WORKDIR owned by root instead of USER": "I failed to find detail documents for this, but I'm interested on this, so I just had a look for docker source code, I guess we can get the clue from sourcecode:\nmoby/builder/dockerfile/dispatcher.go (Line 299):\n// Set the working directory for future RUN/CMD/etc statements.\n//\nfunc dispatchWorkdir(d dispatchRequest, c *instructions.WorkdirCommand) error {\n    ......\n    if err := d.builder.docker.ContainerCreateWorkdir(containerID); err != nil {\n        return err\n    }\n\n    return d.builder.commitContainer(d.state, containerID, runConfigWithCommentCmd)\n}\nAbove, we can see it will call ContainerCreateWorkdir, next is the code:\nmoby/daemon/workdir.go:\nfunc (daemon *Daemon) ContainerCreateWorkdir(cID string) error {\n    ......\n    return container.SetupWorkingDirectory(daemon.idMapping.RootPair())\n}\nAbove, we can see it call SetupWorkingDirectory, next is the code:\nmoby/container/container.go (Line 259):\nfunc (container *Container) SetupWorkingDirectory(rootIdentity idtools.Identity) error {\n    ......\n    if err := idtools.MkdirAllAndChownNew(pth, 0755, rootIdentity); err != nil {\n        pthInfo, err2 := os.Stat(pth)\n        if err2 == nil && pthInfo != nil && !pthInfo.IsDir() {\n            return errors.Errorf(\"Cannot mkdir: %s is not a directory\", container.Config.WorkingDir)\n        }\n\n        return err\n    }\n\n    return nil\n}\nAbove, we can see it call MkdirAllAndChownNew(pth, 0755, rootIdentity), next is the code:\nmoby/pkg/idtools/idtools.go (Line 54):\n// MkdirAllAndChownNew creates a directory (include any along the path) and then modifies\n// ownership ONLY of newly created directories to the requested uid/gid. If the\n// directories along the path exist, no change of ownership will be performed\nfunc MkdirAllAndChownNew(path string, mode os.FileMode, owner Identity) error {\n    return mkdirAs(path, mode, owner, true, false)\n}\nAbove will setup folder in intermediate build container & also change the ownership of the folder with rootIdentity.\nFinally, what is rootIdentity here?\nIt's passed here as daemon.idMapping.RootPair(), next is the declare:\nmoby/pkg/idtools/idtools.go (Line 151):\n// RootPair returns a uid and gid pair for the root user. The error is ignored\n// because a root user always exists, and the defaults are correct when the uid\n// and gid maps are empty.\nfunc (i *IdentityMapping) RootPair() Identity {\n    uid, gid, _ := GetRootUIDGID(i.uids, i.gids)\n    return Identity{UID: uid, GID: gid}\n}\nSee the function desc:\nRootPair returns a uid and gid pair for the root user\nYou can continue to see what GetRootUIDGID is, but I think it's enough now from the function desc. It will finally use change the ownership of WORKDIR to root.\nAnd, additional to see what USER do?\n__moby/builder/dockerfile/dispatcher.go (Line 543):__\n\n// USER foo\n//\n// Set the user to 'foo' for future commands and when running the\n// ENTRYPOINT/CMD at container run time.\n//\nfunc dispatchUser(d dispatchRequest, c *instructions.UserCommand) error {\n    d.state.runConfig.User = c.User\n    return d.builder.commit(d.state, fmt.Sprintf(\"USER %v\", c.User))\n}\nAbove, just set user to run config and directly commit for further command, but did nothing related to WORKDIR setup.\nAnd, if you want to change the ownership, I guess you will have to do it by yourself use chown either in RUN or ENTRYPOINT/CMD.",
    "\"ModuleNotFoundError: No module named <package>\" in my Docker container": "Inside the container, when I pip install bugsnag, I get the following:\nroot@af08af24a458:/app# pip install bugsnag\nRequirement already satisfied: bugsnag in /usr/local/lib/python2.7/dist-packages\nRequirement already satisfied: webob in /usr/local/lib/python2.7/dist-packages (from bugsnag)\nRequirement already satisfied: six<2,>=1.9 in /usr/local/lib/python2.7/dist-packages (from bugsnag)\nYou probably see the problem here. You're installing the package for python2.7, which is the OS default, instead of python3.6, which is what you're trying to use.\nCheck out this answer for help resolving this issue: \"ModuleNotFoundError: No module named <package>\" in my Docker container\nAlternatively, this is a problem virtualenv and similar tools are meant to solve, you could look into that as well.",
    "Templating config file with docker": "Found a solution using simple bash script, so I didn't even alter the template.\n#!/bin/sh\n\ncat > /etc/sphinxsearch/sphinx.conf\n\nfor LOCALE in ru en de ;\ndo\n    sed \"s/{{ locale }}/${LOCALE}/g\" ./template/index.conf.template >> /etc/sphinxsearch/sphinx.conf\ndone",
    "Extending docker official postgres image": "If you define a new ENTRYPOINT in your Dockerfile it will override the inherited ENTRYPOINT. So in that case, Postgres will not be able to be initialized automatically (unless yo write the same ENTRYPOINT).\nhttps://docs.docker.com/engine/reference/builder/#entrypoint\nAlso, the official postgres image let you add .sql/.sh files in the /docker-entrypoint-initdb.d folder, so they can be executed once the database is initialized.\nFinally, if you don't want that Postgres remove your data, you can mount a volume between the /var/lib/postgresql/data folder and a local folder in each docker run ... command to persist them.",
    "Restart Docker container during build process": "OK, looks like you're trying to install VisualStudio 2019. That's how I solved the problem. The first approach is to use multi-stage build as stated above:\nFROM mcr.microsoft.com/windows/servercore:1809 as baseimage\nRUN powershell -NoProfile -ExecutionPolicy Bypass -Command \\    \n     $Env:chocolateyVersion = '0.10.15' ; \\\n     $Env:chocolateyUseWindowsCompression = 'false' ; \\\n     \"[Net.ServicePointManager]::SecurityProtocol = \\\"tls12, tls11, tls\\\"; iex ((New-Object System.Net.WebClient).DownloadString('http://chocolatey.org/install.ps1'))\" && SET \"PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin\"\n\n# suppress the \"restart required\" error code (3010)\nRUN choco install -y --ignore-package-exit-codes=3010 dotnetfx\n\n# emulate the required restart after installing dotnetfx\nFROM baseimage\nRUN choco install -y visualstudio2019buildtools --package-parameters \\\n    \"--norestart --add Microsoft.VisualStudio.Component.VC.Tools.x86.x64\"\nThe problem with that approach is that dotnetfx package seems to be broken - some other packages fail to install due to the missing 'alink.dll' library. Also, I didn't check that --ignore-package-exit-codes=3010 suppresses only one error or all errors (choco doc says nothing about the possibility to specify the exact code).\nThe second approach is to install visual studio from the MS website (works perfectly):\nFROM mcr.microsoft.com/windows/servercore:1809\n\nRUN powershell -NoProfile -ExecutionPolicy Bypass -Command \\\n    Invoke-WebRequest \"https://aka.ms/vs/16/release/vs_community.exe\" \\\n    -OutFile \"%TEMP%\\vs_community.exe\" -UseBasicParsing\n\nRUN \"%TEMP%\\vs_community.exe\"  --quiet --wait --norestart --noUpdateInstaller \\\n    --add Microsoft.VisualStudio.Component.VC.Tools.x86.x64 \\\n    --add Microsoft.VisualStudio.Component.Windows10SDK.18362\nNote that components might be different in you case.",
    "How to configure proxy for NuGet while using Docker?": "Check my answer here.\nBasically, you should instruct the docker build environment to use the proxy by adding build arguments like:\ndocker build --build-arg HTTP_PROXY=<proxy URL> --build-arg HTTPS_PROXY=<proxy URL> -t <application name>",
    "How to COPY library files between stages of a multi-stage Docker build while preserving symlinks?": "This is more of a workaround than an answer.\nYou could tar the files, copy the tarball to the second container and then untar them.\nTar maintains symbolic links by default.",
    "defining multiple services on Dockerfile with single CMD": "It is best practice to run a single service in each container, but it is not always practical. I run CMD [\"sh\", \"-c\", \"/usr/sbin/crond && php-fpm\"] so I can use the laravel scheduler. So in answer to the question, yes the above does work.",
    "How to Containerize a Vue.js app?": "In a default vue-cli setup, npm start (the command you are using) runs npm run dev.\nAnd, again, by default, npm run dev binds to localhost only.\nAdd --host 0.0.0.0 to your webpack-dev-server line in package.json so you can access it from outside the docker container:\nFrom something like:\n  \"scripts\": {\n    \"dev\": \"webpack-dev-server --inline --progress --config build/webpack.dev.conf.js\",\nTo something like (add --host 0.0.0.0):\n    \"dev\": \"webpack-dev-server --inline --progress --config build/webpack.dev.conf.js --host 0.0.0.0\",\nNote: I'm assuming, because you used CMD [\"npm\", \"start\"], you are creating a container for development or debugging purposes. If you are targeting production, you should really consider generating the bundle (npm run build) and serving the generated files directly on a HTTP server like nginx (which could be created in a docker as well).",
    "Run SQL scripts inside Dockerfile": "======================== Correct Answer Start ========================\nAccording to the Postgres image documentation you can extend the original image by running a script or an SQL file.\nIf you would like to do additional initialization in an image derived from this one, add one or more *.sql, *.sql.gz, or *.sh scripts under /docker-entrypoint-initdb.d (creating the directory if necessary). After the entrypoint calls initdb to create the default postgres user and database, it will run any *.sql files and source any *.sh scripts found in that directory to do further initialization before starting the service.\nFor your Dockerfile, you could do something like this.\nFROM postgres:9.6.5\n\nENV POSTGRES_USER postgres\nENV POSTGRES_PASSWORD xxx\nENV POSTGRES_DB postgres\n\nADD create-role.sh /docker-entrypoint-initdb.d\nADD localDump.sql /docker-entrypoint-initdb.d\nYou will also need the create-role.sh script to execute the plsql command\n#!/bin/bash\nset -e\n\npsql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" <<-EOSQL\n    CREATE ROLE read_only;\nEOSQL\nNote that I'm not a SQL DBA, the plsql command is just an example.\n======================== Correct Answer End ========================\nOriginal answer left for posterity :\nCan you use psql ? Something like that.\nFROM postgres:9.6.5\n\nENV POSTGRES_USER postgres\nENV POSTGRES_PASSWORD xxx\nENV POSTGRES_DB postgres\n\nRUN psql -U postgres -d database_name -c \"SQL_QUERY\"\n\nADD localDump.sql /docker-entrypoint-initdb.d\nYou should find how to run SQL Query for Postgres from bash, and then add a RUN instruction in your Dockerfile.",
    "What's the difference between RUN and bash script in a dockerfile?": "The primary difference is that when you COPY the bash script into the image it will be available for inspection in the running container, whereas the RUN command is a little more opaque. Putting your commands in a file like that is arguably more manageable for other reasons: changes in your VCS history will be a little more clear, and for longer or more complex scripts you will probably find it easier to format things cleanly with the script in a separate file rather than embedded in your Dockerfile in a RUN command.\nOtherwise the result is the same (in both cases, you are executing the same set of commands), although the COPY and RUN will result in an extra image layer (vs. just the RUN by itself).",
    "How can I install python modules in a docker image?": "Yes, the best thing is to build your image in such a way it has the python modules are in there.\nHere is an example. I build an image with the build dependencies:\n$ docker build -t oz123/alpine-test-mycoolapp:0.5 - < Image\nSending build context to Docker daemon  2.56 kB\nStep 1 : FROM alpine:3.5\n ---> 88e169ea8f46\nStep 2 : ENV MB_VERSION 3.1.4\n ---> Running in 4587d36fa4ae\n ---> b7c55df49803\nRemoving intermediate container 4587d36fa4ae\nStep 3 : ENV CFLAGS -O2\n ---> Running in 19fe06dcc314\n ---> 31f6a4f27d4b\nRemoving intermediate container 19fe06dcc314\nStep 4 : RUN apk add --no-cache python3 py3-pip gcc python3-dev py3-cffi    file git curl autoconf automake py3-cryptography linux-headers musl-dev libffi-dev openssl-dev build-base\n ---> Running in f01b60b1b5b9\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.5/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.5/community/x86_64/APKINDEX.tar.gz\n(1/57) Upgrading musl (1.1.15-r5 -> 1.1.15-r6)\n(2/57) Upgrading zlib (1.2.8-r2 -> 1.2.11-r0)\n(3/57) Installing m4 (1.4.17-r1)\n(4/57) Installing perl (5.24.0-r0)\n(5/57) Installing autoconf (2.69-r0)\n(6/57) Installing automake (1.15-r0)\n(7/57) Installing binutils-libs (2.27-r1)\n...\nNote, I am installing Python's pip inside the image, so later I can download packages from pypi. Packages like numpy might require a C compiler and tool chain, so I am installing these too.\nAfter building the packages which require the build tools chain I remove the tool chain packages:\nRUN apk del file pkgconf autoconf m4 automake perl g++ libstdc++\nAfter you have your base image, you can run your application code in an image building on top of it:\n$ cat Dockerfile\n\n    FROM oz123/alpine-test-mycoolapp\n    ADD . /code\n    WORKDIR /code\n    RUN pip3 install -r requirements.txt -r requirements_dev.txt\n    RUN pip3 install -e .\n    RUN make clean\n    CMD [\"pytest\", \"-vv\", \"-s\"]\nI simply run this with docker.",
    "Unable to display GUI application from Windows container : Windows 2019 Server": "The short answer here is no. Windows containers don't have the underlying GUI components available, so you can't run any desktop application on Windows containers.\nThe longer answer is that this depends on what you're trying to achieve. The actual components for the GUI (GUI APIs) are present on the Server Core and Server image (not on the Nano Server one). However, the GUI itself is not present. What that means is you can run an application that depends on the GUI APIs, for UI test automation, for example. I blogged about this topic a while ago here: https://techcommunity.microsoft.com/t5/containers/nano-server-x-server-core-x-server-which-base-image-is-the-right/ba-p/2835785",
    "Why does the ASP.NET Core Multi-Stage Dockerfile use 4 Stages": "The file effectively equivalent to below\nFROM microsoft/aspnetcore-build:2.0 AS build\nWORKDIR /src\nCOPY WebApplication1.sln ./\nCOPY WebApplication1/WebApplication1.csproj WebApplication1/\nRUN dotnet restore\nCOPY . .\nWORKDIR /src/WebApplication1\nRUN dotnet build -c Release -o /app\nRUN dotnet publish -c Release -o /app\n\nFROM microsoft/aspnetcore:2.0 AS base\nEXPOSE 80\nWORKDIR /app\nCOPY --from=build /app .\nENTRYPOINT [\"dotnet\", \"WebApplication1.dll\"]\nNow the reason they may have chosen 4 build stage might be any of the two\nPresentational\nFuture changes\nSo it may be that it depicts a\nbase -> build -> publish -> deploy the build\nThe size with 2 build stage would also the same as this one. So there is no obvious difference between the 2 stage and the 4 stage. It becomes a matter preference, representation and all. Nothing technologically different",
    "Error CTC1014 Docker command failed with exit code 1 with dotnet core api": "I ran into the very same error (CTC1014) in the past. I found out that my project was being run in \"Release\" mode, when it should have been running in \"Debug\" mode. So I would like to suggest this as a workaround.\nHere's how you change it in VS2019\nOf course, running your application in Release mode shouldn't be a problem. So I assumed it must have been related to some optimization employed by most release build configurations, in conflict with Docker. Not exactly sure which one, though.\nCheers! o/",
    "How to correctly initialize git submodules in Dockerfile for Docker Cloud": "See the answer I gave here: https://stackoverflow.com/a/59640438/1021344\nReproduced here for simplicity: You need to use hooks: https://docs.docker.com/docker-hub/builds/advanced/#custom-build-phase-hooks\nTL;DR: Place this in hooks/post_checkout:\n#!/bin/bash\n# Docker hub does a recursive clone, then checks the branch out,\n# so when a PR adds a submodule (or updates it), it fails.\ngit submodule update --init",
    "How do I run Elixir Phoenix Docker in production mode using example from Phoenix Guides?": "There is a good sample repo is configured for production-ready build and deploy cycle. It contains an ansible setup that will maintain docker image, build phoenix app in docker image, and do automated versioned releases on your production server.\nAnd I recommend you to read the blog post guide",
    "Couldn't use data file .coverage: unable to open database file": "Problem\nI ran into the same error when running pytest with coverage using docker-compose in the github-hosted ubuntu-latest image in GitHub Actions. This is an instance of the Docker host file system owner matching problem.\nIn short, the user on the host (the github action runner) and the user in on the container (where my pytest suite runs) have different UIDs. The mounted directory app is owned by the user on the host. When the user in the container attempts to write to the app/.coverage, permission is denied (since this user is not the owner).\nThe fix\nIn my case, I solved the issue by matching the UID of my docker image's default user with that of the github actions runner user, 1001. I added this to my Dockerfile to accomplish this:\n# Make the default user have the same UID as the github actions \"runner\" user.\n# This to avoid permission issues when mounting volumes.\nUSER root\nRUN usermod --uid 1001 <image_default_user>\nUSER <image_default_user>\nAll relevant files\napp/test_utils/docker-compose.yml:\nversion: \"3.9\"\n\nservices:\n  app:\n    build:\n      context: ../\n      dockerfile: ./test_utils/Dockerfile\n    container_name: app\n    volumes:\n      - ..:/app\napp/test_utils/Dockerfile:\nFROM <my base image>\n\n# Make the default user have the same UID as the github actions \"runner\" user.\n# This to avoid permission issues when mounting volumes, see\nUSER root\nRUN usermod --uid 1001 <image_default_user>\nUSER <image_default_user>\n\nCOPY . /app\nWORKDIR /app\n\nRUN pip3 install -r requirements.txt -r requirements_test.txt\napp/.github/unittests.yml:\nname: Run unit tests, Report coverage\n\non:\n  pull_request:\n    paths:\n      - app/*\n      - .github/workflows/unittests.yml\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checks out the repo\n        uses: actions/checkout@v2\n\n      - name: Build docker image\n        run: docker-compose -f test_utils/docker-compose.yml build\n\n      - name: Run unit tests & produce coverage report\n        # Adapted from the docker example in\n        # https://github.com/MishaKav/pytest-coverage-comment?tab=readme-ov-file#example-usage\n        run: |\n          docker-compose \\\n            -f test_utils/docker-compose.yml \\\n            run app \\\n            pytest \\\n              --cov-report=term-missing:skip-covered \\\n              --junitxml=/app/pytest.xml \\\n              --cov=/app \\\n              /app \\\n            | tee pytest-coverage.txt\n\n      - name: Pytest coverage comment\n        uses: MishaKav/pytest-coverage-comment@main\n        with:\n          pytest-coverage-path: pytest-coverage.txt\n          junitxml-path: pytest.xml",
    "Override ENV variable in base docker image": "If you cannot build another image, as described in \"Dockerfile Overriding ENV variable\", you at least can modify it when starting the container with docker run -e\nSee \"ENV (environment variables)\"\nthe operator can set any environment variable in the container by using one or more -e flags, even overriding those mentioned above, or already defined by the developer with a Dockerfile ENV\n$ docker run -e \"deep=purple\" -e today --rm alpine env\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nHOSTNAME=d2219b854598\ndeep=purple   <=============",
    "mkdir: cannot create directory '/ffa_app': Permission denied": "The user set by the base image is jboss, so you have 2 options:\ncreate and work in the user's home folder mkdir -p ~/ffa_app\nset USER root at the top of your Dockerfile, after the FROM statement\nNeedless to say, I'd recommend sticking to a user with lower privileges.",
    "Docker COPY and keep directory": "You should consider ADD instead of COPY: see Dockerfile ADD\nIf <src> is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory.\nThat means you can wrap your docker build step into a script which would first tar -cvf dirs.tar dir1 dir2 dir3\nYour Dockerfile can then ADD dirs.tar: you will find your folders in your image.\nSee also Dockerfile Best Practices: ADD or COPY.",
    "difference between building a docker from ubuntu base image and python base image?": "With ubuntu you can run a django app. you just have to install the dependencies for it (with instructions in your Dockerfile for example).\nIn your Dockerfile, add something like :\nRUN apt-get install python\nRUN apt-get install django\nYou may also have to replace some commands by their equivalent if they're not implemented in the new base image (replace apt-get by pacman if you use archlinux instead of ubuntu for example).\nBut if you use django, you also can install and use pip.",
    "qemu-x86_64: Could not open '/lib/ld-musl-x86_64.so.1': No such file or directory": "It seems azul/zulu-openjdk-alpine:11 doesn't have arm64 image needed to run on Apple silicon architecture.\nTry updating to jdk 17 image with arm64 support https://hub.docker.com/r/azul/zulu-openjdk-alpine/",
    "Where docker named volumes are stored? [duplicate]": "You can inspect docker volumes and see detailed informations.\nSee docker volume reference\nEdit (commented suggestion):\nAlternatively (for exact answer) see the already answered question.",
    "X11 forwarding of GUI app in Docker container": "To be able to communicate with the X server, the user that runs the app has to be allowed to communicate with the X server. So I think you have two options:\nAllow the user you have in the container to connect to the X server. If your app is run with user root inside the container, you can use:\n$ xhost +SI:localuser:root\n(I don't know the security implications of this, but root should be able to connect either way...)\nAdd a user in the container that matches your user session. If the user you are using in the host system has UID = 1000, you can create a dummy user inside the container:\n$ useradd -u 1000 my_user\nAnd then use that user to run your app inside the container. This doesn't require any change in the accepted hosts (as user 1000 is already capable of connection).\nLooking at the two options, the second seems better, because it does not require any change in the host system, and if you need to use this container in other systems that the main user could not match UID=1000, you can make the container receive the correct uid from an env var, and then set up the correct user (useradd + chown program files).",
    "How do I build docker images without docker?": "Part of the problem could be that you're missing the --privileged flag, but in general, your questions can probably be answered here: https://hub.docker.com/_/docker/\nAnd you might take the time to read the blog linked there detailing some of the pitfalls of using docker-in-docker.",
    "Using docker image to store data files": "You're trying to address a configuration management problem with an application management approach. For shared configurations, you generally want to take the \"centralized location approach\". Basic examples of this would be a git repository or an S3 bucket. Both solutions have native document storage and can be appropriately shared between services with fine-grained access control.\nA docker image isn't the docker approach to store/share configuration. By having an image, you can basically do two things:\n1. Base other images off your initial image\n2. Run a container\nGiven that your image here is just Scratch and files, there are no executables and nothing to run. The hello world example copied a directory with an actual script in it, which is why they ended up with a runnable container.\nAs a base image, configuration doesn't make sense coming before things like dependencies.\nIf you really want to use a docker tool for this, you're looking for docker volumes. That can persist a file system and is easy to share around different containers. https://docs.docker.com/engine/userguide/containers/dockervolumes/",
    "Dockerfile COPY wildcard to only match files, not folders": "That's a tricky one. Given what you said about not being able to .dockerignore the directories, I think I'd probably either:\nlist out all the files explicitly, or\nuse something to generate the Dockerfile from a template .. this is kind of just a special case of the first option",
    "Extracting gz in Dockerfile": "The ADD directive will unpack the .gz automatically on build\nADD target/test_app.tar.gz /service/app_lib\n# Clean up\nRUN rm -rf /service/app_lib/conf \\\n           /service/app_lib/bin",
    "ERROR: Cannot find \"/config/config.json\". Have you run \"sequelize init\"?": "You can create a .sequelizerc config file for Sequlize CLI command for your project which tells Sequlize where to look for config files.\n    var path = require('path')\n\nmodule.exports = {\n  'config':          path.resolve('server', 'config', 'database.json'),\n  'migrations-path': path.resolve('server', 'migrations'),\n  'models-path':     path.resolve('server', 'models'),\n  'seeders-path':    path.resolve('server', 'seeders'),\n}",
    "error NETSDK1064: Package DnsClient, 1.2.0 was not found": "When solutions like using --no-cache are not restoring at all work, it usually indicates that artifact from the host machine are getting copied into the build container. This can cause problems when the host machine and build container have different RIDs. This can commonly occur on Windows machines when running linux containers. The .NET Core Docker Samples suggest using a .dockerignore file to avoid coping the bin and obj directories with your Dockerfiles.",
    "How to start another bash in Dockerfile": "To expand on @user2915097's answer here is a working example using devtoolset-7 and rh-python36 instead of devtoolset-1.1\nFROM centos:7\n\n# Default version of GCC and Python\nRUN gcc --version && python --version\n\n# Install some developer style software collections with intent to\n# use newer version of GCC and Python than the OS provided\nRUN yum install -y centos-release-scl && yum install -y devtoolset-7 rh-python36\n\n# Yum installed packages but the default OS-provided version is still used.\nRUN gcc --version && python --version\n\n# Okay, change our shell to specifically use our software collections.\n# (default was SHELL [ \"/bin/sh\", \"-c\" ])\n# https://docs.docker.com/engine/reference/builder/#shell\n#\n# See also `scl` man page for enabling multiple packages if desired:\n# https://linux.die.net/man/1/scl\nSHELL [ \"/usr/bin/scl\", \"enable\", \"devtoolset-7\", \"rh-python36\" ]\n\n# Switching to a different shell has brought the new versions into scope.\nRUN gcc --version && python --version",
    "Dockerfile: COPY folder if it exists (conditional COPY)": "TL;DR;\nCOPY ./migration[s]/ /app/migrations/\nMore detailed answer\nWe can use glob patterns to achieve such behavior, Docker will not fail if it won't find any valid source, there are 3-most used globs:\n? - any character, doesn't match empty character\n* - any characters sequence of any length, matches empty character\n[abc] - sequence of characters to match, just like ?, but it matches only characters defined in brackets\nSo there are 3 ways to solve this\nCOPY ./migration?/ /app/migrations/ - will also match migrationZ, migration2 and so on..\nCOPY ./migrations*/ /app/migrations/ - will also match migrations-cool, migrations-old\nCOPY ./migration[s]/ /app/migrations/ - will match only migrations, because we are using glob that is saying match any character from 1-item sequence [s] and it just can't anything except letter \"s\"\nMore about globs: https://en.wikipedia.org/wiki/Glob_(programming)",
    "safe way to use build-time argument in Docker": "With docker 18.09+, that will be: docker build --secret id=mysecret,src=/secret/file (using buildkit).\nSee PR 1288, announced in this tweet.\n--secret is now guarded by API version 1.39.\nExample:\nprintf \"hello secret\" > ./mysecret.txt\n\nexport DOCKER_BUILDKIT=1\n\ndocker build --no-cache --progress=plain --secret id=mysecret,src=$(pwd)/mysecret.txt -f - . <<EOF\n# syntax = tonistiigi/dockerfile:secrets20180808\nFROM busybox\nRUN --mount=type=secret,id=mysecret cat /run/secrets/mysecret\nRUN --mount=type=secret,id=mysecret,dst=/foobar cat /foobar\nEOF\nIt sure would help if you could please extend this Dockerfile with an example of making the secret available in a Docker variable so it can be used, like to compose a URL for an artifact repository that requires credentials.\nI tried this but it did not work: RUN export MYSECRET=$(cat /foobar)\nThe issue with setting the secret as an environment variable inside the Dockerfile (e.g., RUN export MYSECRET=$(cat /foobar)) is that environment variables do not persist between RUN instructions by default.\nAnd using ENV to store the secret would bake it into the image layers, making it visible to anyone who inspects the image: see docker inspect and \"How to get ENV variable when doing docker inspect\"\nThe recommended approach is to use the secret in the same RUN instruction where it is read. That way, the secret is never stored as a persistent environment variable or in a layer\u2014only used transiently during that particular build step.\nFor example, if you want to use the secret to form a URL and fetch some artifact, you could do this:\n# syntax = tonistiigi/dockerfile:secrets20180808\nFROM busybox\n\nRUN --mount=type=secret,id=mysecret \\\n    sh -c 'MYSECRET=$(cat /run/secrets/mysecret) && \\\n           echo \"Secret read, now using it in a command\" && \\\n           curl -fSL \"https://myrepo.com/artifact?token=${MYSECRET}\" -o /artifact.tar.gz'\nMYSECRET is defined and used within the same sh -c command sequence. Once the RUN step completes, MYSECRET and the secret file are gone.",
    "docker-compose to create replication in mongoDB": "Here is how I did this using a 4th container to initialize the replica set after 3 mongodb containers are running:\nDocker-compose file\n    version: '3'\n    services:\n    \n      mongodb1:\n        image: mongo:latest\n        networks:\n          - alphanetwork\n        volumes:\n          - data1:/data/db\n          - config1:/data/configdb\n        ports:\n          - 30001:27017\n        entrypoint: [ \"/usr/bin/mongod\", \"--bind_ip_all\", \"--replSet\", \"rs0\" ]\n    \n      mongodb2:\n        image: mongo:latest\n        networks:\n          - alphanetwork\n        ports:\n          - 30002:27017\n        entrypoint: [ \"/usr/bin/mongod\", \"--bind_ip_all\", \"--replSet\", \"rs0\" ]\n    \n      mongodb3:\n        image: mongo:latest\n        networks:\n          - alphanetwork\n        ports:\n          - 30003:27017\n        entrypoint: [ \"/usr/bin/mongod\", \"--bind_ip_all\", \"--replSet\", \"rs0\" ]\n    \n      mongoclient:\n        image: mongo\n        networks:\n          - alphanetwork\n        depends_on:\n          - mongodb1\n          - mongodb2\n          - mongodb3\n        volumes:\n          - ./deployment_scripts:/deployment_scripts\n        entrypoint:\n          - /deployment_scripts/initiate_replica.sh\n    \n    networks:\n      alphanetwork:\n    \n    volumes:\n      data1:\n      config1:\ninitiate_replica.sh\n    #!/bin/bash\n    \n    echo \"Starting replica set initialize\"\n    until mongo --host mongodb1 --eval \"print(\\\"waited for connection\\\")\"\n    do\n        sleep 2\n    done\n    echo \"Connection finished\"\n    echo \"Creating replica set\"\n    mongo --host mongodb1 <<EOF\n    rs.initiate(\n      {\n        _id : 'rs0',\n        members: [\n          { _id : 0, host : \"mongodb1:27017\" },\n          { _id : 1, host : \"mongodb2:27017\" },\n          { _id : 2, host : \"mongodb3:27017\" }\n        ]\n      }\n    )\n    EOF\n    echo \"replica set created\"\nAnd don't forget to chmod +x ./deployment_scripts/initiate_replica.sh to give docker the permission to execute it.",
    "Possible to add kaniko to alpine image or add jq to kaniko image": "Official Kaniko Docker image is built from scratch using standalone Go binaries (see Dockerfile from Kaniko's GitHub repository). You can re-use the same binaries from official image and copy them in your image such as:\n# Use this FROM instruction as shortcut to use --copy=from kaniko below\n# It's also possible to use directly COPY --from=gcr.io/kaniko-project/executor\nFROM gcr.io/kaniko-project/executor AS kaniko\n\nFROM alpine:3.14.2\n\nRUN apk --update add \\\n  bash \\\n  curl \\\n  git \\\n  jq \\\n  npm\nRUN curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.21.4/bin/linux/amd64/kubectl\nRUN chmod u+x kubectl && mv kubectl /bin/kubectl\n\n#\n# Add kaniko to this image by re-using binaries and steps from official image\n#\nCOPY --from=kaniko /kaniko/executor /kaniko/executor\nCOPY --from=kaniko /kaniko/docker-credential-gcr /kaniko/docker-credential-gcr\nCOPY --from=kaniko /kaniko/docker-credential-ecr-login /kaniko/docker-credential-ecr-login\nCOPY --from=kaniko /kaniko/docker-credential-acr-env /kaniko/docker-credential-acr-env\nCOPY --from=kaniko /etc/nsswitch.conf /etc/nsswitch.conf\nCOPY --from=kaniko /kaniko/.docker /kaniko/.docker\n\nENV PATH $PATH:/usr/local/bin:/kaniko\nENV DOCKER_CONFIG /kaniko/.docker/\nENV DOCKER_CREDENTIAL_GCR_CONFIG /kaniko/.config/gcloud/docker_credential_gcr_config.json\nEDIT: for the debug image, Dockerfile would be:\nFROM gcr.io/kaniko-project/executor:debug AS kaniko\n\nFROM alpine:3.14.2\n\nRUN apk --update add \\\n  bash \\\n  curl \\\n  git \\\n  jq \\\n  npm\nRUN curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.21.4/bin/linux/amd64/kubectl\nRUN chmod u+x kubectl && mv kubectl /bin/kubectl\n\n#\n# Add kaniko to this image by re-using binaries and steps from official image\n#\nCOPY --from=kaniko /kaniko/ /kaniko/\nCOPY --from=kaniko /kaniko/warmer /kaniko/warmer\nCOPY --from=kaniko /kaniko/docker-credential-gcr /kaniko/docker-credential-gcr\nCOPY --from=kaniko /kaniko/docker-credential-ecr-login /kaniko/docker-credential-ecr-login\nCOPY --from=kaniko /kaniko/docker-credential-acr /kaniko/docker-credential-acr\nCOPY --from=kaniko /kaniko/.docker /kaniko/.docker\nCOPY --from=busybox:1.32.0 /bin /busybox\n\nENV PATH $PATH:/usr/local/bin:/kaniko:/busybox\nENV DOCKER_CONFIG /kaniko/.docker/\nENV DOCKER_CREDENTIAL_GCR_CONFIG /kaniko/.config/gcloud/docker_credential_gcr_config.json\nNote that you need to use gcr.io/kaniko-project/executor:debug (for latest version) or gcr.io/kaniko-project/executor:v1.6.0-debug as source (or another tag)\nTested building a small image, seems to work fine:\n# Built above example with docker build . -t kaniko-alpine\n# And ran container with docker run -it kaniko-alpine sh\necho \"FROM alpine\" > Dockerfile\necho \"RUN echo hello\" >> Dockerfile\necho \"COPY Dockerfile Dockerfile\" >> Dockerfile\n\nexecutor version\nexecutor -c . --no-push\n\n# Output like:\n#\n# Kaniko version :  v1.6.0\n#\n# INFO[0000] Retrieving image manifest alpine             \n# INFO[0000] Retrieving image alpine from registry index.docker.io \n# INFO[0000] GET KEYCHAIN                                 \n# [...] \n# INFO[0001] RUN echo hello                               \n# INFO[0001] Taking snapshot of full filesystem...        \n# INFO[0001] cmd: /bin/sh                                 \n# INFO[0001] args: [-c echo hello]                        \n# INFO[0001] Running: [/bin/sh -c echo hello]             \n# [...]\nNote that using Kaniko binaries outside of their official image is not recommended, even though it may still work fine:\nkaniko is meant to be run as an image: gcr.io/kaniko-project/executor. We do not recommend running the kaniko executor binary in another image, as it might not work.",
    "ModuleNotFoundError in Docker": "So I finally fixed the issue. For those who may be wondering how was it that I fixed it. You need to define a PYTHONPATH environment variable either in the Dockerfile or docker-compose.yml.",
    "npm run doesn't pass --configuration to build task": "Finally I found the solution of my problem in this post.\nMy sintax of npm run wasn't correct. The correct way to pass params to a npm command is adding -- operator.\nIn my case I need to change\nRUN npm run ng build --output-path=dist --configuration=$ENVIROMENT --verbose\nto\nRUN npm run ng build -- --output-path=dist --configuration=$ENVIROMENT --verbose",
    "Docker: Dockerfile vs docker-compose.yml": "You could put this docker-compose.yaml next to your Dockerfile:\nversion: '2'\nservices:\n  docker-whale:\n    image: docker-whale\n    build: .\nAnd then execute the following commands:\n# build docker image\ndocker-compose build\n\n# bring up one docker container\ndocker-compose up -d\n\n# scale up to three containers\ndocker-compose scale docker-whale=3\n\n# overview about these containers\ndocker-compose ps\n\n# view combined logs of all containers\n# use <ctrl-c> to stop viewing\ndocker-compose logs --follow\n\n# take down all containers\ndocker-compose down",
    "Docker-compose with nginx reverse, a website and a restful api?": "I wrote a tutorial specifically about reverse proxies with nginx and docker.\nCreate An Nginx Reverse Proxy With Docker\nYou'd basically have 3 containers and two without exposed ports that would be communicated through a docker network and each attached to the network.\nBash Method:\ndocker create my-network;\n# docker run -it -p 80:80 --network=my-network ...\nor\nDocker Compose Method:\nFile: docker-compose.yml\nversion: '3'\nservices:\n   backend:\n      networks:\n         - my-network\n   ...\n   frontend:\n      networks:\n         - my-network\n   proxy:\n      networks:\n         - my-network\nnetworks:\n   my-network:\nA - Nginx Container Proxy - MAPPED 80/80\nB - REST API - Internally Serving 80 - given the name backend\nC - Website - Internally Serving 80 - given the name frontend\nIn container A you would just have an nginx conf file that points to the different services via specific routes:\nFile: /etc/nginx/conf.d/default.conf\nserver {\n    listen       80;\n    server_name  localhost;\n    #charset koi8-r;\n    #access_log  /var/log/nginx/host.access.log  main;\n    location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n        proxy_pass http://frontend;\n    }\n    location /api {\n        proxy_pass http://backend:5000/;\n    }\n    //...\n}\nThis makes it so that when you visit:\nhttp://yourwebsite.com/api = backend\nhttp://yourwebsite.com = frontend\nLet me know if you have questions, I've built this a few times, and even added SSL to the proxy container.\nThis is great if you're going to test one service for local development, but for production (depending on your hosting provider) it would be a different story and they may manage it themselves with their own proxy and load balancer.\n===================== UPDATE 1: =====================\nThis is to simulate both backend, frontend, a proxy and a mysql container in docker compose.\nThere are four files you'll need in the main project directory to get this to work.\nFiles:\n- backend.html\n- frontend.html\n- default.conf\n- docker-compose.yml\nFile: ./backend.html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Backend API</title>\n</head>\n<body>\n    <h1>Backend API</h1>\n</body>\n</html>\nFile: ./frontend.html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Frontend / Website</title>\n</head>\n<body>\n    <h1>Frontend / Website</h1>\n</body>\n</html>\nTo configure the proxy nginx to point the right containers on the network.\nFile: ./default.conf\n# This is a default site configuration which will simply return 404, preventing\n# chance access to any other virtualhost.\n\nserver {\n    listen 80 default_server;\n    listen [::]:80 default_server;\n\n    # Frontend\n    location / {\n        proxy_pass http://frontend-name; # same name as network alias\n    }\n\n    # Backend\n    location /api {\n        proxy_pass http://backend-name/;  # <--- note this has an extra /\n    }\n\n    # You may need this to prevent return 404 recursion.\n    location = /404.html {\n        internal;\n    }\n}\nFile: ./docker-compose.yml\nversion: '3.5'\nservices:\n    frontend:\n        image: nginx:alpine\n        volumes:\n            - $PWD/frontend.html:/usr/share/nginx/html/index.html\n        networks:\n            my-network-name:\n                aliases:\n                    - frontend-name\n    backend:\n        depends_on:\n            - mysql-database\n        image: nginx:alpine\n        volumes:\n            - $PWD/backend.html:/usr/share/nginx/html/index.html\n        networks:\n            my-network-name:\n                aliases:\n                    - backend-name\n    nginx-proxy:\n        depends_on:\n            - frontend\n            - backend\n        image: nginx:alpine\n        volumes: \n            - $PWD/default.conf:/etc/nginx/conf.d/default.conf\n        networks:\n            my-network-name:\n                aliases:\n                    - proxy-name\n        ports:\n            - 1234:80\n    mysql-database:\n        image: mysql\n        command: --default-authentication-plugin=mysql_native_password\n        restart: always\n        environment:\n            MYSQL_DATABASE: 'root'\n            MYSQL_ROOT_PASSWORD: 'secret'\n        ports:\n            - '3306:3306'\n        networks:\n            my-network-name:\n                aliases:\n                    - mysql-name\nnetworks:\n    my-network-name:\nCreate those files and then run:\ndocker-compose -d up;\nThen visit:\nFrontend - http://localhost:1234\nBackend - http://localhost:1234/api\nYou'll see both routes now communicate with their respective services. You can also see that the fronend and backend don't have exposed ports. That is because nginx in them default port 80 and we gave them aliases within our network my-network-name) to refer to them.\nAdditionally I added a mysql container that does have exposed ports, but you could not expose them and just have the backend communicate to the host: mysql-name on port 3306.\nIf you to walkthrough the process a bit more to understand how things work, before jumping into docker-compose, I would really recommend checking out my tutorial in the link above.\nHope this helps.\n===================== UPDATE 2: =====================\nHere's a diagram:",
    "Change ImageMagick policy on a Dockerfile": "This change to your Dockerfile works\nARG imagemagic_config=/etc/ImageMagick-6/policy.xml\n\nRUN if [ -f $imagemagic_config ] ; then sed -i 's/<policy domain=\"coder\" rights=\"none\" pattern=\"PDF\" \\/>/<policy domain=\"coder\" rights=\"read|write\" pattern=\"PDF\" \\/>/g' $imagemagic_config ; else echo did not see file $imagemagic_config ; fi",
    "Docker Image > 1GB in size from python:3.8.3-alpine": "welcome to Docker! It can be quite the thing to wrap one's head around, especially when beginning, but you're asking really valid questions that are all pertinent\nReducing Size\nHow to\nA great place to start is Docker's own Dockerfile best practices page:\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/\nThey explain neatly how your each directve (COPY, RUN, ENV, etc) all create additional layers, increasing your containers size. Importantly, they show how to reduce your image size by minimising the different directives. They key to alot of minimisation is chaining commands in RUN statements with the use of &&.\nSomething else I note in your Dockerfile is one specific line:\nCOPY . $APP_HOME\nNow, depending on how you build your container (Specifically, what folder you pass to Docker as the context), this will copy EVERYTHING in that it has available to it. Chances are, this will be bringing in your venv folder etc if you have one. I feel that this may be the largest perpetrator of size for you. You can mitigate this by adding an explicit COPY in, or using a .dockerignore file.\nI built your image (Without any source code, and without copying in entrypoint.sh), and it came out to 710MB as a base. It could be a good idea to check the size of your source code, and see if anything else is getting in there. After I re-arranged some of the commands to reuse directives, the image was 484MB, which is considerably smaller! If you get stuck, I can pop it into a gist on Github for you and walk you through it, however, the Docker documentation should hopefully get you going\nWhy?\nWell, larger applications / images aren't inherently bad, but with any increase in data, some operations may be slower.\nWhen I say operations, I tend to mean pulling images from a registry, or pushing them to publish. It will take longer to transfer 1GB than it will 50MB.\nThere's also a consideration to be made when you scale your containers. While the image size does not necessarily correlate directly to how much disk you will use when you start a container, it will certainly increase the requirements for the machine you're running on, and limit others on smaller devices\nDocker\nThe advantages of using Docker are widespread, and I can't cover them all here without submitting my writing for thesis defence ;-)\nBut it mainly boils down to the following points:\nAlot of providers support running your applications in docker\nDockerfiles help you to build your application in a consisten environment, meaning you dont have to configure each host your app runs on, or worry about version clashes\nContainers let you develop and run your application in a consistent (And the same) environment\nContainers usually provide really nice networking capabilities. An example you will have encountered is within docker compose, you can reach other containers simply through their hostname\nNginx\nYou've set things up well there, from what I can gather! I imagine nginx is 'telling you' (Via the logs?) to navigate to 0.0.0.0 because that is what it will have bound to in the container. Now, you've forwarded traffic from 1337:80. Docker follows the format of host:container, so this means that traffic on localhost:1337 will be directed to the containers port 80. You may need to swap this around based on your nginx configuration, but rest assured you will be able to navigate to localhost in your browser and see your website once everything is set up\nLet me know if you need help with any of the above, or want more resources to aid you. Happy to correspond and walk you through anything anytime given we seem to be in the same timezone \ud83e\udd19",
    "Add sudo permission (without password ) to user by command line": "First, you are not suggested to use sudo in docker. You could well design your behavior using USER + gosu.\nBut, if you insist for some uncontrolled reason, just add next line after you setup normal user:\nRUN echo '%sudo ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\nSo for your scenario, the workable one is:\nFROM ubuntu:bionic\n\nENV DEBIAN_FRONTEND noninteractive\n\n# Get the basic stuff\nRUN apt-get update && \\\n    apt-get -y upgrade && \\\n    apt-get install -y \\\n    sudo\n\n# Create ubuntu user with sudo privileges\nRUN useradd -ms /bin/bash ubuntu && \\\n    usermod -aG sudo ubuntu\n# New added for disable sudo password\nRUN echo '%sudo ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\n\n# Set as default user\nUSER ubuntu\nWORKDIR /home/ubuntu\n\nENV DEBIAN_FRONTEND teletype\n\nCMD [\"/bin/bash\"]\nTest the effect:\n$ docker build -t abc:1 .\nSending build context to Docker daemon  2.048kB\nStep 1/9 : FROM ubuntu:bionic\n......\nSuccessfully built b3aa0793765f\nSuccessfully tagged abc:1\n\n$ docker run --rm abc:1 cat /etc/sudoers\ncat: /etc/sudoers: Permission denied\n\n$ docker run --rm abc:1 sudo cat /etc/sudoers\n#\n# This file MUST be edited with the 'visudo' command as root.\n#\n# Please consider adding local content in /etc/sudoers.d/ instead of\n# directly modifying this file.\n#\n# See the man page for details on how to write a sudoers file.\n#\nDefaults        env_reset\n......\n#includedir /etc/sudoers.d\n%sudo ALL=(ALL) NOPASSWD:ALL\nYou could see with sudo, we could already execute a root-needed command.",
    "How to keep a docker container run for ever": "From HOW TO KEEP DOCKER CONTAINERS RUNNING, we can know that docker containers, when run in detached mode (the most common -d option), are designed to shut down immediately after the initial entrypoint command (program that should be run when container is built from image) is no longer running in the foreground. So to keep docker container run even the inside program has done\njust add CMD tail -f /dev/null as last line to dockerfile\nWhat's more important is that we should understand what's docker is intended for and how to use it properly. Try to use docker as environment foundation for applications in host machine is not a good choice, docker is designed to run applications environment-independent. Applications should be placed into docker image via docker build and run in docker container in runtime.",
    "Can't set environment variables with docker run -e or --env-file option": "Rearrange the docker run command, as the default entrypoint for node base docker image, is node, so the container considers --env-file .env as an argument to node process.\ndocker run -d -p 3000:3000  --env-file .env chatapp-back\nAlso, you can verify this before running the main process.\ndocker run -it -p 3000:3000 --env-file .env chatapp-back -e \"console.log(process.env)\"",
    "Unsupported config option for services.web: 'dockerfile'": "The best way to troubleshoot these type of issues is checking the docker-compose reference for the specific version - https://docs.docker.com/compose/compose-file/.\nWithout testing, the issue is because you need to put dockerfile under build.\nOld:\nweb:\n  dockerfile: Dockerfile\nNew:\nweb:\n  build:\n    context: .\n    dockerfile: Dockerfile\nSince you are using the standard Dockerfile filename, you could also use:\nbuild:\n  context: .\nFrom reference page:\nDOCKERFILE Alternate Dockerfile.\nCompose uses an alternate file to build with. A build path must also be specified.\nbuild:\n  context: .\n  dockerfile: Dockerfile-alternate",
    "Install and using pip and virtualenv in Docker": "On Debian-based platforms, including Ubuntu, the command installed by python3-pip is called pip3 in order for it to peacefully coexist with any system-installed Python 2 and its pip.\nSomewhat similarly, the virtualenv command is not installed by the package python3-virtualenv; to get that, you need apt-get install -y virtualenv.\nNote that venv is included in the Python 3 standard library, so you don't really need to install anything at all.\npython3 -m venv newenv\nWhy would you want a virtualenv inside Docker anyway, though? (There are situations where it makes sense but in the vast majority of cases, you want the Docker container to be as simple as possible, which means, install everything as root, and rebuild the whole container if something needs to be updated.)\nAs an aside, you generally want to minimize the number of RUN statements. Making many layers while debugging is perhaps defensible, but layers which do nothing are definitely just wasteful. Perhaps also discover that apt-get can install more than one package at a time.\nRUN apt-get update -y && \\\n    apt-get install -y python3 python3-pip && \\\n    ...\nThe && causes the entire RUN sequence to fail as soon as one of the commands fails.",
    "Dockerfile CMD `command not found`": "You are using wrong quotes. It should be:\nCMD [\"bash\", \"npm run lint\"]",
    "Scripts in the /docker-entrypoint-initdb.d folder are ignored": "Postgres only initializes the database if no database is found, when the container starts. Since you have a volume mapping on the database directory, chances are that a database already exists.\nIf you delete the db_data volume and start the container, postgres will see that there isn't a database and then it'll initialize one for you using the scripts in docker-entrypoint-initdb.d.",
    "EACCES: permission denied, open \"file-path\"": "It is clear that node_modules folder in container is built by root user during the step npm install, therefore has root as user. This is the reason we don't have access to that folder when we set up our node user. To resolve this what we have to do is firstly using the root user we have to give permission to the node user while copying files from local directory to image and then later set up node as the user as shown below:\nCOPY --chown=node:node package.json .\nRUN npm install\n\nCOPY --chown=node:node . .\nUSER node",
    "How do I set up my Dockerfile to use cpanm to install a specific version of a Perl module?": "For anyone who's searching for this same answer in the future, another option can be found here in the documentation for cpanm:\ncpanm Plack@0.9990\nIf you have a long list of modules, consider feeding a cpanfile into cpanm rather than listing them all in the Dockerfile.\nThe easiest way to specify a particular version number for a module in a cpanfile is like this:\nrequires 'Text::ParseWords', '==3.1';\nThe syntax for requesting the latest version of a module is this:\nrequires 'Text::ParseWords';\nRequesting a minimum version: (note the lack of '==')\nrequires 'Text::ParseWords', '3.1';\nThe syntax for requesting specific versions in other ways is fairly well-documented here.\nAnother great write-up of the use of cpanm and a cpanfile can be found in Installation of cpan modules by cpanm and cpanfile.",
    "Call From quickstart.cloudera/172.17.0.2 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused": "Port 8020 is for the hdfs-namenode service, so my guess is that service not started or has failed.\nCan you try to restart it?\ncommand: sudo  service hadoop-hdfs-namenode restart\nYou can also check the status of the namenode service.\nCommand: sudo  service hadoop-hdfs-namenode status\nAlso, check the hadoop-hdfs-datanode service as it may also need to be restarted.\ncommand: sudo  service hadoop-hdfs-datanode restart\nIf you still get the error then check the NameNode logs in /var/log/hadoop-hdfs and add it to your question for further analysis.",
    "$GOPATH/go.mod exists but should not when building docker container, but works if I manually run commands": "I have same problem. You need set WORKDIR /go/delivery",
    "How do I run TypeScript `tsc` before `COPY` in Dockerfile.template?": "You can use a multi-stage build for this. The first stage includes all of the development dependencies, including tsc; the second stage only includes the files that are needed to run the built application.\n(I'm not familiar with the specific build environment you're using so this will be in terms of the standard node image.)\n# First stage: compile things.\nFROM node:12 AS build\nWORKDIR /usr/src/app\n\n# (Install OS dependencies; include -dev packages if needed.)\n\n# Install the Javascript dependencies, including all devDependencies.\nCOPY package.json .\nRUN npm install\n\n# Copy the rest of the application in and build it.\nCOPY . .\n# RUN npm build\nRUN npx tsc -p ./tsconfig.json\n\n# Now /usr/src/app/dist has the built files.\n\n# Second stage: run things.\nFROM node:12\nWORKDIR /usr/src/app\n\n# (Install OS dependencies; just libraries.)\n\n# Install the Javascript dependencies, only runtime libraries.\nCOPY package.json .\nRUN npm install --production\n\n# Copy the dist tree from the first stage.\nCOPY --from=build /usr/src/app/dist dist\n\n# Run the built application when the container starts.\nEXPOSE 3000\nCMD [\"npm\", \"run\", \"serve\"]",
    "Cloning a git repo in Dockerfile and working off it": "You can do it by three ways.\nHere is the Dockerfile.\nFROM node:alpine\nRUN apk add --no-cache git\nRUN apk add --no-cache openssh\nWORKDIR /data\nRUN git clone https://github.com/jahio/hello-world-node-express.git /data/app\nWORKDIR /data/app\nEXPOSE 3000\nBuild:\ndocker build -t node-test .\nUpdate:\nDamn, I am crazy about docker :D Another solution easiest and good\nCreate an empty directory in host and container and mount that one\n/home/adiii/Desktop/container_Data:/to_host\nCopy the cloned repo to to_host at the entry point with -u flat so only will new file will be paste and host data will be persistent.\nand entrypoint.sh\n#!/bin/ash\ncp -r -u /data/app /to_host && /bin/ash\ndockerfile update section.\nADD entrypoint.sh /usr/bin/entrypoint.sh\nRUN chmod +x /usr/bin/entrypoint.sh\nRUN WORKDIR /\nRUN mkdir -p to_host\n\n# so we will put the code in to_host after container bootup boom im crazy about docker so no need to make it complex..simple easy :D\n\nENTRYPOINT [ \"/usr/bin/entrypoint.sh\" ]\n1: using docker volume\nCreate volume named code\ndocker volume create code\nNow run that container with mounting this volume.\ndocker run -p 3000:3000 -v myvol:/data/app --rm -it node-test ash\nNow terminate the container or stopping it will data still preserved in volume.\nYou can find if OS is Linux.\n/var/lib/docker/volumes/code/_data\nyou will see three\napp.js  node_modules  package.json\n2: using bash see comments in the script\n  #!/bin/bash\nimage_name=node-test\ncontainer_name=git_code\n\n\n# for first time use first_time\nif [ $1 == \"first_time\" ] ; then\n# remove if exist\ndocker rm -f $container_name\n#run contianer for first time to copy code\ndocker run --name $container_name -dit $image_name ash\nfi\n# check if running\nif docker inspect -f '{{.State.Running}}' $container_name ; then\n# copy code from container to /home/adiii/desktop\ndocker cp $container_name:/data/app /home/adil/Desktop/app\nfi\n\n# for normal runing using run \nif [ $1 == \"run\" ]; then\n# remove old container if running\ndocker rm -f $container_name\ndocker run --name $container_name -v /home/adil/Desktop/app:/data/app -dit $image_name\nfi\nNow run the command in the container\ndocker exec -it git_code ash\n3: By mounting the empty directory of the host with code directory of the container at the runtime. So when you run next time with mount directory it will contain your update code which you made any change from host OS. But make sure the permission of that directory after container run and terminated data will be there but the behaviour of this method is not constant.\ndocker run -p 3000:3000 -v /home/adiii/code/app:/data/app --rm -it node-test ash\nHere /home/adiii/code/app is an empty directory of host and after termination of containers its still have cloned code but I said its behavior varies.",
    "Unable to build docker from root": "Move your dockerfile out of your root directory.\nThere are a whole lot of reasons to do this, not the least of which is permissions issues. You should nearly never be using your root directory as your working directory.",
    "\"CMD ['/home/user/script.sh']\" in docker file doesn't work with docker-compose": "I simplified your example a bit\nsee https://github.com/BITPlan/docker-stackoverflowanswers/tree/master/33229581\nand used:\ndocker-compose.yml\nweb:\n  build: .\n  ports:\n   - \"8888:8888\"\nfind .\n.\n./docker-compose.yml\n./Dockerfile\n./myproject\ndocker build & run\ndocker-compose build\ndocker-compose run web\nand of course I get\n/bin/sh: 1: [/home/root/myproject/uwsgi.sh,: not found\nassuming this is since there is no uwsgi.sh in the directory myproject.\nIf I add uwsgi.sh with\necho 'echo $0 is there and called with params $@!' > myproject/uwsgi.sh\nchmod +x myproject/uwsgi.sh\nand test it with\ndocker-compose run web /bin/bash\nls\ncat uwsgi.sh\n./uwsgi.sh start\nit's there and behaves as expected:\nroot@9f06f8ff8c3b:/home/root/myproject# ls\nuwsgi.sh\nroot@9f06f8ff8c3b:/home/root/myproject# cat uwsgi.sh \necho $0 is there and called with params $@!\nroot@9f06f8ff8c3b:/home/root/myproject# ./uwsgi.sh start\n./uwsgi.sh is there and called with params start!\n root@9f06f8ff8c3b:/home/root/myproject# \nbut for docker-compose run web I still get\n/bin/sh: 1: [/home/root/myproject/uwsgi.sh,: not found\nIf I add a single blank to the Dockerfile CMD line:\nCMD [ '/home/root/myproject/uwsgi.sh', 'start' ] \nthe result is: /bin/sh: 1: [: /home/root/myproject/uwsgi.sh,: unexpected operator\nwhich brings us closer. As a next step I am leaving out the \"start\" parameter.\nCMD [ '/home/root/myproject/uwsgi.sh' ] \nthis now leads to no output at all ...\nIf i change the CMD line to:\nCMD [ \"/home/root/myproject/uwsgi.sh\", \"start\" ] \nI get\n Cannot start container \n 0b9da138c43ef308ad70da4a7718cb96fbfdf6cda113e2ae0ce5e24de06f07cd: [8]  \n System   error: exec format error\nand now you could continue in the Edison like approach:\nI have not failed. I've just found 10,000 ways that won't work. until you find\nCMD [ \"/bin/bash\", \"/home/root/myproject/uwsgi.sh\", \"start\" ]\nwhich brings you closer with the result:\n /home/root/myproject/uwsgi.sh is there and called with params start!\nCMD expects an executable as the first parameter e.g.\nhttps://docs.docker.com/compose/\nhas\nCMD python app.py\nas an example. To run your shell script you'll need a shell like bash. See also https://stackoverflow.com/a/33219131/1497139",
    "docker-compose yaml - option to pass the 'ulimit' parameters 'rtprio' and 'memlock'": "There's a per-service dictionary called ulimits:.\nversion: '3'\nservices:\n  my_proj:\n    image: image/my_image\n    ulimits:\n      rtprio: 95\n      memlock: -1\n    ...\nNote that Docker Compose works better with non-interactive services that stay running; I would use it to launch your service proper and not necessarily to get an interactive shell in a temporary container.",
    "Where to see the Dockerfile for a docker image?": "Use\ndocker history --no-trunc IMAGE_NAME_OR_ID\nThis will show all commands run in the image building process in reverse order. It's not exactly a Dockerfile, but you can find all essential content.",
    "Creating a table in single user mode in postgres": "@a_horse_with_no_name got me on the right track with his comment. I decided to ditch the single user mode even if it was \"recommended\". Instead I start postgres with pg_ctl, load some sql files containing my table creations, and stop the server with pg_ctl.\nMy shell script looks like this:\n#!/bin/bash\necho \"******CREATING DOCKER DATABASE******\"\n\necho \"starting postgres\"\ngosu postgres pg_ctl -w start\n\necho \"bootstrapping the postgres db\"\ngosu postgres psql -h localhost -p 5432 -U postgres -a -f /db/bootstrap.sql\n\necho \"initializing tables\"\ngosu postgres psql -h localhost -p 5432 -U postgres -d orpheus -a -f /db/setup.sql\n\necho \"stopping postgres\"\ngosu postgres pg_ctl stop\n\necho \"stopped postgres\"\n\n\necho \"\"\necho \"******DOCKER DATABASE CREATED******\"",
    "Cannot change permissions for 'RUN chmod +x /app-entrypoint.sh' in Dockerfile": "The COPY step will create the file with the uid/gid of 0:0 (root:root) within the / directory where normal users have no access. And the selected base image is configured to run as uid 1001. Probably the easiest is to switch back to root temporarily to run that step.\nFROM docker.io/bitnami/jasperreports:7-debian-10\nUSER root\nCOPY custom-entrypoint.sh /app-entrypoint.sh\nUSER 1001\nRUN  chmod +x /app-entrypoint.sh\nAlternatively, you can copy the script to a directory where the user has access with the right ownership on the file. Without pulling the image, I suspect /opt/bitnami/scripts may have different permissions:\nFROM docker.io/bitnami/jasperreports:7-debian-10\nCOPY --chown=1001 custom-entrypoint.sh /opt/bitnami/scripts/app-entrypoint.sh\nRUN  chmod +x /opt/bitnami/scripts/app-entrypoint.sh",
    "Unable to run docker image due to libGl error": "I was able to make the docker container run by making following changes to the dockerfile\nFROM python:3.6.8\nCOPY . /app\nWORKDIR /app\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update -y\nRUN apt install libgl1-mesa-glx -y\nRUN apt-get install 'ffmpeg'\\\n    'libsm6'\\\n    'libxext6'  -y\nRUN pip3 install --upgrade pip\n\nRUN pip3 install opencv-python==4.3.0.38\nRUN pip3 install -r requirements.txt\nEXPOSE 80\nCMD [\"python3\", \"server.py\"]\nThe lines required for resolving the libGl error\nRUN apt install libgl1-mesa-glx -y\nRUN apt-get install 'ffmpeg'\\\n    'libsm6'\\\n    'libxext6'  -y\nwere not able to run without updating the ubuntu environment. Moreover creating the docker image as noninteractive helped to skip any interactive command line inputs",
    "Docker Image Timestamp Issue": "That's from Google Jib. For reproducibility they don't set a date, or they explicitly set the date to the zero value, which is the epoch in 1970.\nThere is a FAQ entry for this: https://github.com/GoogleContainerTools/jib/blob/master/docs/faq.md#why-is-my-image-created-48-years-ago\nFor reproducibility purposes, Jib sets the creation time of the container images to the Unix epoch (00:00:00, January 1st, 1970 in UTC). If you would like to use a different timestamp, set the jib.container.creationTime / <container><creationTime> parameter to an ISO 8601 date-time. You may also use the value USE_CURRENT_TIMESTAMP to set the creation time to the actual build time, but this sacrifices reproducibility since the timestamp will change with every build.\nSetting creationTime parameter\nMaven:\n<configuration>\n  <container>\n    <creationTime>2019-07-15T10:15:30+09:00</creationTime>\n  </container>\n</configuration>\nGradle:\njib.container.creationTime = '2019-07-15T10:15:30+09:00'",
    "dotnet build with version is not working in docker": "Yes, it should be a trivial task but I also ran into a few issues before I finally could do it. Spent a few hours to solve the issue, hope you can save time. I recommend to read this post it helped.\nMy final solution was to use a Docker ARG, you must be declare it before the first FROM:\n#Declare it at the beginning of the Dockerfile\nARG BUILD_VERSION=1.0.0\n...\n#Publish your project with \"-p:Version=${BUILD_VERSION}\" it works also with \"dotnet build\"\nRUN dotnet publish \"<xy.csproj>\" -c Release -o /app/publish --no-restore -p:Version=${BUILD_VERSION}\nOne very important thing to Note: is if you are using multistage build (multiple FROM in your file) you have to \"re-declare\" the ARG in that stage. See a similar question here.\nFinally you can call your Docker build with:\ndocker build --build-arg BUILD_VERSION=\"1.1.1.1\"",
    "How to automatically delete intermediate stage Docker containers from my system?": "You should build it like this:\ndocker build --rm -t kube-test-app .\nIf dockerfile is in direcory were you located or specify path to dockerfile\ndocker build --rm -t kube-test-app -f path/to/dockerfile .\n-t is tag, name of your builded docker image\nFor remove all except images and runing containers use docker system prune\nFor remove image docker rmi -f image_name",
    "URL Rewrite 2.0 installation fails on Docker": "I finally figured it out thanks to this article. Using PowerShell to run msiexec with the appropriate switches works. Oddly, it threw \"Unable to connect to the remote server\" when trying to also download the MSI using PowerShell, so I resorted to using ADD.\nHere's the relevant portion of my Dockerfile:\nWORKDIR /install\nADD https://download.microsoft.com/download/C/9/E/C9E8180D-4E51-40A6-A9BF-776990D8BCA9/rewrite_amd64.msi rewrite_amd64.msi\nRUN Write-Host 'Installing URL Rewrite' ; \\\n    Start-Process msiexec.exe -ArgumentList '/i', 'rewrite_amd64.msi', '/quiet', '/norestart' -NoNewWindow -Wait",
    "docker build purely from command line": "Update for 2017-05-05: Docker just released 17.05.0-ce with this PR #31236 included. Now the above command creates an image:\n$ docker build -t test-no-df -f - . <<EOF\nFROM busybox:latest\nCMD echo just a test\nEOF\nSending build context to Docker daemon  23.34MB\nStep 1/2 : FROM busybox:latest\n ---> 00f017a8c2a6\nStep 2/2 : CMD echo just a test\n ---> Running in 45fde3938660\n ---> d6371335f982\nRemoving intermediate container 45fde3938660\nSuccessfully built d6371335f982\nSuccessfully tagged test-no-df:latest\nThe same can be achieved in a single line with:\n$ printf 'FROM busybox:latest\\nCMD echo just a test' | docker build -t test-no-df -f - .\nOriginal Response\ndocker build requires the Dockerfile to be an actual file. You can use a different filename with:\ndocker build -f Dockerfile.temp .\nThey allow the build context (aka the . or current directory) to be passed by standard input, but attempting to pass a Dockerfile with this syntax will fail:\n$ docker build -t test-no-df -f - . <<EOF\nFROM busybox:latest\nCMD echo just a test\nEOF\n\nunable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /home/bmitch/data/docker/test/-: no such file or directory",
    "Docker multi-stage builds: how to set WORKDIR globally for every stage?": "A FROM line inherits almost all of the settings from its base image, including its WORKDIR. Since your later stages derive from the first stage you only need to include WORKDIR in the base stage.\nFROM base AS builder\nWORKDIR /app\n\nFROM builder AS test\n# already in /app directory\n\nFROM builder AS deploy\n# already in /app directory",
    "Why copy package.json and install dependencies before copying the rest of the project within docker? [duplicate]": "It's done because of Docker layer caching. If you run docker build when none of the copied files have changed, the image layers will just be read from the cache. If a COPY file has changed, all layers after that in the image will be rebuilt.\nRUN yarn install is an expensive operation and we don't want to need to execute it again when any random source files in the project change. This way, it's only re-executed if the package.json or package-lock.json files have changed.",
    "How to log docker healthcheck status in docker logs?": "OK, this is super hacky and I'm not proud of it. The issue is that the healthcheck runs in a different process than your main process, so it's hard to write to the main process' stdout.\nBut you can exploit that the main process (usually) runs as process #1 and write to /proc/1/fd/1 which is that process' stdout. Something like this\nFROM ubuntu\nHEALTHCHECK --interval=1s --timeout=30s --retries=3 CMD echo {'health': 'healthy'} | tee /proc/1/fd/1\nCMD tail -f /dev/null\nThe tail -f /dev/null is just a dummy command that keeps the container running.",
    "Adding SSL certificate when using Google Jib and Kubernetes": "Answer recommended by Google Cloud Collective",
    "public key is not available: NO_PUBKEY F76221572C52609D": "There are several issues here:\n1) W: GPG error: https://apt.dockerproject.org debian-jessie InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY F76221572C52609D W: There is no public key available for the following key IDs: AA8E81B4331F7F50\nSolution:\nMove the keyserver add actions to the place before RUN echo 'deb http://deb.debian.org/debian jessie-backports main' > /etc/apt/sources.list.d/jessie-backports.list, meanwhile add AA8E81B4331F7F50 also as next:\nRUN apt-get install -y --no-install-recommends apt-transport-https ca-certificates\nRUN apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\nRUN apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys AA8E81B4331F7F50\n2) W: Failed to fetch http://deb.debian.org/debian/dists/jessie-backports/main/binary-amd64/Packages 404 Not Found\nE: Some index files failed to download. They have been ignored, or old ones used instead.\nSolution:\nmicrosoft/aspnetcore-build:1.0.1 base on debian8, and you want to use openjdk8 which was default not in apt repository. So you use deb http://deb.debian.org/debian jessie-backports main.\nUnfortunately, if you check http://ftp.debian.org/debian/dists/, you will find jessie-backports had been removed. So you had to switch to archived url like next (Comment the old url, just use the url next):\n#RUN echo 'deb http://deb.debian.org/debian jessie-backports main' > /etc/apt/sources.list.d/jessie-backports.list\nRUN echo 'deb http://archive.debian.org/debian jessie-backports main' > /etc/apt/sources.list.d/jessie-backports.list\nMeanwhile, you had to add next after doing above to resolve release-file-expired-problem:\nRUN echo \"Acquire::Check-Valid-Until \\\"false\\\";\" > /etc/apt/apt.conf.d/100disablechecks\n3) ENV JAVA_VERSION 8u111\nENV JAVA_DEBIAN_VERSION 8u111-b14-2~bpo8+1\nSolution:\nNot sure how you get this version, but in fact after change to archive jessie backports, what you could get is something like next:\nroot@2ecaeffec483:/etc/apt# apt-cache policy openjdk-8-jdk\nopenjdk-8-jdk:\n  Installed: (none)\n  Candidate: 8u171-b11-1~bpo8+1\n  Version table:\n     8u171-b11-1~bpo8+1 0\n        100 http://archive.debian.org/debian/ jessie-backports/main amd64 Packages\nSo, you had to change to next:\nENV JAVA_VERSION 8u171\nENV JAVA_DEBIAN_VERSION 8u171-b11-1~bpo8+1",
    "Install certificate in dotnet core docker container": "Is your Docker container running on Linux?\nI assume that it is. Then your base image should be microsoft/aspnetcore, which is based on Ubuntu.\nYou should add this in your DOCKERFILE:\nCOPY ca_bundle.crt /usr/local/share/ca-certificates/your_ca.crt\nRUN update-ca-certificates\nFirst line copies your CA bundle into the image, the second line updates the CA list.\nThe CA bundle (the list of authorities that signed your certificate) can be extracted from PFX, just Google for it. This is the first link I found.\nIf your container is running on Windows, then Powershell command should work as-is (I'm not sure about that)",
    "Docker container exits when using -it option": "This behaviour is caused by Apache and it is not an issue with Docker. Apache is designed to shut down gracefully when it receives the SIGWINCH signal. When running the container interactively, the SIGWINCH signal is passed from the host to the container, effectively signalling Apache to shut down gracefully. On some hosts the container may exit immediately after it is started. On other hosts the container may stay running until the terminal window is resized.\nIt is possible to confirm that this is the source of the issue after the container exits by reviewing the Apache log file as follows:\n# Run container interactively:\ndocker run -it <image-id>\n\n# Get the ID of the container after it exits:\ndocker ps -a\n\n# Copy the Apache log file from the container to the host:\ndocker cp <container-id>:/var/log/apache2/error.log .\n\n# Use any text editor to review the log file:\nvim error.log\n\n# The last line in the log file should contain the following:\nAH00492: caught SIGWINCH, shutting down gracefully\nSources:\nhttps://bz.apache.org/bugzilla/show_bug.cgi?id=50669\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1212224\nhttps://github.com/docker-library/httpd/issues/9",
    "Volumes and docker-compose": "Aug. 2022:\nbrandt points out in the comments to the updated docker-compose documentation.\nNote August 2017: with docker-compose version 3, regarding volumes:\nThe top-level volumes key defines a named volume and references it from each service\u2019s volumes list.\nThis replaces volumes_from in earlier versions of the Compose file format. See Use volumes and Volume Plugins for general information on volumes.\nExample:\nversion: \"3.2\"\nservices:\n  web:\n    image: nginx:alpine\n    volumes:\n      - type: volume\n        source: mydata\n        target: /data\n        volume:\n          nocopy: true\n      - type: bind\n        source: ./static\n        target: /opt/app/static\n\n  db:\n    image: postgres:latest\n    volumes:\n      - \"/var/run/postgres/postgres.sock:/var/run/postgres/postgres.sock\"\n      - \"dbdata:/var/lib/postgresql/data\"\n\nvolumes:\n  mydata:\n  dbdata:\nThis example shows a named volume (mydata) being used by the web service, and a bind mount defined for a single service (first path under db service volumes).\nThe db service also uses a named volume called dbdata (second path under db service volumes), but defines it using the old string format for mounting a named volume.\nNamed volumes must be listed under the top-level volumes key, as shown.\nFebruary 2016:\nThe docs/compose-file.md mentions:\nMount all of the volumes from another service or container, optionally specifying read-only access(ro) or read-write(rw).\n(If no access level is specified, then read-write will be used.)\nvolumes_from:\n - service_name\n - service_name:ro\n - container:container_name\n - container:container_name:rw\nFor instance (from this issue or this one)\nversion: \"2\"\n\nservices:\n...\n  db:\n    image: mongo:3.0.8\n    volumes_from:\n      - dbdata\n    networks:\n      - back\n    links:\n      - dbdata\n\n dbdata:\n    image: busybox\n    volumes:\n      - /data/db",
    "How can I fix the 'stream terminated by RST_STREAM' error when building a Docker image for a Vue app?": "This issue has to do with the \"Dockerfile\" text encoding, change it to UTF-8, you can do that in notepad++ or any other editor like VS Code. This fixed the issue for me.",
    "Unknown Instruction ln Docker File in RUN": "You need to add an &&\\ to link RUN commands togethers\nOtherwise Dockerfile won't be able to interpret ln as a Dockerfile command (like RUN, COPY, ADD, ...)\nRUN mv /maven /opt/maven &&\\    <============== missing\n    ln -s /opt/maven/bin/mvn /usr/bin/mvn && \\\nOr at least add a second run\nRUN mv /maven /opt/maven \nRUN ln -s /opt/maven/bin/mvn /usr/bin/mvn && \\\n ^  ...\n |\n --- missing",
    "How can I run a docker container on localhost over the default IP?": "The default network is bridged. The 0.0.0.0:49166->443 shows a port mapping of exposed ports in the container to high level ports on your host because of the -P option. You can manually map specific ports by changing that flag to something like -p 8080:80 -p 443:443 to have port 8080 and 443 on your host map into the container.\nYou can also change the default network to be your host network as you've requested. This removes some of the isolation and protections provided by the container, and limits your ability to configure integrations between containers, which is why it is not the default option. That syntax would be:\ndocker run --name nginx1 --net=host -d nginx\nEdit: from your comments and a reread I see you're also asking about where the 10.0.75.2 ip address comes from. This is based on how you launch the docker daemon. That IP binding is assigned when you pass the --ip flag to the daemon documentation here. If you're running docker in a vm with docker-machine, I'd expect this to be the IP of your vm.",
    "Run dotnet tests in docker which use testcontainers": "I could manage to do it, with two major differences:\nThe tests do not run on the docker image, but rather on the docker container.\nI am using docker compose now.\ndocker-compose-tests.yml:\nversion: '3.4'\n\nservices:\n  myproject.authentication.api.tests: # docker compose -f docker-compose-tests.yml up myproject.authentication.api.tests\n    build:\n      context: .\n      dockerfile: Authentication.Api/MyProject.Authentication.Api/Dockerfile\n      target: build\n    command: >\n        sh -cx \"\n                dotnet test /src/Authentication.Api/MyProject.Authentication.Api.IntegrationTests/MyProject.Authentication.Api.IntegrationTests.csproj -c Release --results-directory /testresults --logger \\\"trx;LogFileName=testresults_authentication_api_it.trx\\\" /p:CollectCoverage=true /p:CoverletOutputFormat=json%2cCobertura /p:CoverletOutput=/testresults/coverage/ -p:MergeWith=/testresults/coverage/coverage.json\"\n    environment:\n      - TESTCONTAINERS_HOST_OVERRIDE=host.docker.internal # Needed in Docker Desktop (Windows), needs to be removed on linux hosts. Can be done with a override compose file.\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - coverage:/testresults/coverage\n    container_name: myproject.authentication.api.tests\n(\"sh\" command is useful if more test projects are expected to run.)\nAuthentication.Api/MyProject.Authentication.Api/Dockerfile:\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS build\nWORKDIR /src\nCOPY [\"Authentication.Api/MyProject.Authentication.Api/MyProject.Authentication.Api.csproj\", \"Authentication.Api/MyProject.Authentication.Api/\"]\nCOPY [\"Authentication.Api/MyProject.Authentication.Api.IntegrationTests/MyProject.Authentication.Api.IntegrationTests.csproj\", \"Authentication.Api/MyProject.Authentication.Api.IntegrationTests/\"]\nRUN dotnet restore \"Authentication.Api/MyProject.Authentication.Api/MyProject.Authentication.Api.csproj\"\nRUN dotnet restore \"Authentication.Api/MyProject.Authentication.Api.IntegrationTests/MyProject.Authentication.Api.IntegrationTests.csproj\"\nCOPY . .\n\nWORKDIR \"/src/Authentication.Api/MyProject.Authentication.Api\"\nRUN dotnet build \"MyProject.Authentication.Api.csproj\" -c Release -o /app/build\n\nWORKDIR \"/src/Authentication.Api/MyProject.Authentication.Api.IntegrationTests\"\nRUN dotnet build -c Release\nAuthentication.Api/MyProject.Authentication.Api.IntegrationTests/Factory/CustomWebApplicationFactory.cs: same as in the question.\n{shared library path}/MsSqlDatabaseProvider.cs:\npublic class MsSqlDatabaseProvider\n{\n    private const string DbImage = \"mcr.microsoft.com/mssql/server:2019-latest\";\n    private const string DbUsername = \"sa\";\n    private const string DbPassword = \"my_dummy_password#123\";\n    private const ushort MssqlContainerPort = 1433;\n\n\n    public readonly TestcontainerDatabase Database;\n\n    public MsSqlDatabaseProvider() =>\n        Database = new TestcontainersBuilder<MsSqlTestcontainer>()\n            .WithDatabase(new MsSqlTestcontainerConfiguration\n            {\n                Password = DbPassword,\n            })\n            .WithImage(DbImage)\n            .WithCleanUp(true)\n            .WithPortBinding(MssqlContainerPort, true)\n            .WithEnvironment(\"ACCEPT_EULA\", \"Y\")\n            .WithEnvironment(\"MSSQL_SA_PASSWORD\", DbPassword)\n            .WithWaitStrategy(Wait.ForUnixContainer().UntilCommandIsCompleted(\"/opt/mssql-tools/bin/sqlcmd\", \"-S\", $\"localhost,{MssqlContainerPort}\", \"-U\", DbUsername, \"-P\", DbPassword))\n            .Build();\n}\nAnd I can run the tests in docker with docker compose -f docker-compose-tests.yml up myproject.authentication.api.tests.",
    "Dynamic Docker base image": "For that, you need to define Global ARGs and better to have some default value and override it during build time.\nARG sample_TAG=test\nFROM maven:3.6.1-jdk-8 as maven-build\nARG sample_TAG\nWORKDIR /apps/sample-google\nRUN echo \"image tag is ${sample_TAG}\"\nFROM $sample_TAG\nVOLUME /apps\nRUN mkdir /apps/sample-google",
    "Multistage dockerfile skip stages": "Skipping stages only works with BuildKit. See the discussion here and article here.\nAs for the error you're getting, you should be getting it with or without BuildKit as you cannot use build arguments in COPY instruction. The difference being that with BuildKit Docker will refuse to even start the build and without it the build will fail on COPY instruction.\nWhat you need to do is to create an additional alias for the image you want to copy from using the fact that FROM instruction resolves the build args:\nARG GIT_TOKEN=abc:1a2b3\nARG EXECUTION_ENV=local\n\n# get dependencies from github\nFROM alpine/git as gitclone-ci\nWORKDIR /usr/src/\nRUN git clone https://{GIT_USER_TOKEN}@github.com/something.git \\\n    && git clone https://{GIT_USER_TOKEN}@github.com/somethingelse.git\n\n## in local dependencies are already available in the parent folder\nFROM alpine/git as gitclone-local\nWORKDIR /usr/src/\nCOPY ../something /usr/src/something \nCOPY ../somethingelse /usr/src/somethingelse\n\nFROM gitclone-${EXECUTION_ENV} as intermediate\n\nFROM node:latest as builder\nWORKDIR /usr/src\nCOPY --from=intermediate /usr/src .\nCOPY package* ./\nCOPY src/ src/\nRUN [\"npm\", \"install\"]",
    "How to craft a Dockerfile for an image with NVIDIA driver/CUDA (support for tensorflow-gpu) and Python3 with pip?": "you can use this :\nFROM nvidia/driver:418.40.04-ubuntu18.04\nRUN apt-get -y update \\\n    && apt-get install -y software-properties-common \\\n    && apt-get -y update \\\n    && add-apt-repository universe\nRUN apt-get -y update\nRUN apt-get -y install python3\nRUN apt-get -y install python3-pip",
    "config alpine's proxy in dockerfile": "It seems like you're required to set http_proxy in your Dockerfile. If you do (e.g. for a specific, temporary reason - say you're building your container behind a corporate proxy) and subsequently don't need it anymore I'd suggest something like the following:\nRUN export \\\n  http_proxy=\"http://some.custom.proxy:8080/\u201d \\\n  https_proxy=\"https://some.custom.proxy:8080/\" \\\n  \\\n  && < E.G. pip install requirements.txt> \\\n  \\\n  && unset http_proxy https_proxy\nYou can also use a more permanent solution in your Dockerfile by invoking ENV, but be aware that these are persisted and can lead to problems further down the road if you push/deploy your images somewhere else - Reference.",
    "How to install gulp on a docker with docker-compose": "You need to run npm install gulp AFTER WORKDIR /app, so that gulp is installed locally in node_modules/gulp. But you already did that and having the same error. It's because in your docker-compose-dev.yml, you are mounting host directory as /app volume inside docker container. So local changes in /app directory is lost when you are running the container.\nYou can either remove volumes from docker-compose-dev.yml or run npm install gulp in host machine.",
    "Docker: Expose a range of ports": "Here is a similar question that caters your requirement as well. Docker expose all ports or range of ports from 7000 to 8000\nTo summarize here, Yes it is possible since Docker version 1.5. You can expose port range when executing docker run command like this:\ndocker run -p 192.168.0.10:8000-9000:8000-9000\nor\ndocker run -p 8000-9000:8000-9000\nI have verified that its working fine on my machine using Docker version 1.6.",
    "Is it possible to add pgvector extension on top of postgres:15.3-alpine images": "This extension of the Alpine docker image worked for me:\n$ cat Dockerfile\nFROM postgres:14.4-alpine AS pgvector-builder\nRUN apk add git\nRUN apk add build-base\nRUN apk add clang\nRUN apk add llvm13-dev\nWORKDIR /home\nRUN git clone --branch v0.4.4 https://github.com/pgvector/pgvector.git\nWORKDIR /home/pgvector\nRUN make\nRUN make install\n\nFROM postgres:14.4-alpine\nCOPY --from=pgvector-builder /usr/local/lib/postgresql/bitcode/vector.index.bc /usr/local/lib/postgresql/bitcode/vector.index.bc\nCOPY --from=pgvector-builder /usr/local/lib/postgresql/vector.so /usr/local/lib/postgresql/vector.so\nCOPY --from=pgvector-builder /usr/local/share/postgresql/extension /usr/local/share/postgresql/extension\n\n$ docker build -t postgres:14.4-alpine-pgvector .",
    "How can I correctly specify the platform for my dockerfile?": "On Mac, I add this line to the .zprofile and that takes care of everything at once. Adding it to the .zprofile, for me, ensures the DOCKER_DEFAULT_PLATFORM is present in the environ on accessing the terminal.\nexport DOCKER_DEFAULT_PLATFORM=linux/amd64",
    "Docker set user password non-interactively": "Instead of using passwd, there is another utility for the: chpasswd. I've resolved this by using the following command in my Dockerfile (after creation of the user):\nRUN echo \"${USER}:pass\" | chpasswd\nworks like a charm!",
    "ModuleNotFoundError and import errors in Docker container": "I've just come across another thread on StackOverflow which seems to have resolved my issue. I can leave the import statements as I indicated above in my question, and by setting the PYTHONPATH in the docker container correctly, I am able to get the imports working correctly in docker.\nHow do you add a path to PYTHONPATH in a Dockerfile\nMy updated (working) Dockerfile is as follows:\nFROM python:3.8.5-alpine\n\nRUN pip install pipenv\nCOPY Pipfile /usr/src/\nWORKDIR /usr/src\nRUN pipenv lock --requirements > requirements.txt\nRUN pip install -r requirements.txt\n\nCOPY app /usr/src/app\n\nENV PYTHONPATH \"${PYTHONPATH}:/usr/src/app\"\n\nCMD [\"python\", \"app/api.py\"]",
    "What causes a cache invalidation when building a Dockerfile?": "Let's focus on your original problem (regarding apt-get update) to make things easier. The following example is not based on any best practices. It just illustrates the point you are trying to understand.\nSuppose you have the following Dockerfile:\nFROM ubuntu:18.04\n\nRUN apt-get update\nRUN apt-get install -y nginx\nYou build a first image using docker build -t myimage:latest .\nWhat happens is:\nThe ubuntu image is pulled if it does not exist\nA layer is created and cached to run apt-get update\nA layer is created an cached to run apt install -y nginx\nNow suppose you modify your Docker file to be\nFROM ubuntu:18.04\n\nRUN apt-get update\nRUN apt-get install -y nginx openssl\nand you run a build again with the same command as before. What happens is:\nThere is already an ubuntu image locally so it will not be pulled (unless your force with --pull)\nA layer was already created with command apt-get update against the existing local image so it uses the cached one\nThe next command has changed so a new layer is created to install nginx and openssl. Since apt database was created in the preceding layer and taken from cache, if a new nginx and/or openssl version was released since then, you will not see them and you will install the outdated ones.\nDoes this help you to grasp the concept of cached layers ?\nIn this particular example, the best handling is to do everything in a single layer making sure you cleanup after yourself:\nFROM ubuntu:18.04\n\nRUN apt-get update  \\\n    && apt-get install -y nginx openssl \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*",
    "Dockerfile setup for both production and development?": "A good aproach is to use Docker's multi-stage builds, since they will allow you to build your artifact inside an image that contains your dev dependencies and only use your artifact and runtime dependencies in the final image.\nI'd generally advise against using different Dockerfiles for different environments, this should normally be achieved using configuration parameters (environment variables are a good solution).\nHaving a faster development feedback cycle depends IMO heavily on the used language. Many people will develop using an IDE, in a more classic fashion and only build the image for integration testing and so on (in my experience this is for example often the case for Java developers). Other interpreted languages might indeed profit from mounting your sources into a development environment image. In this case, you might incorperate this image into your multi-stage build.",
    "Where is the docker file located?": "The Dockerfile isn't on your machine. Once the image is built, it exists independently of the Dockerfile, just like a compiled C program doesn't need its source code kept around to function. You can partially recover the Dockerfile via the docker history command, but this won't show you files added with ADD or COPY, and various \"image squishing\" programs can further obfuscate things. The recommended way to get an image's Dockerfile is to go to the repository from which the image was built (hopefully linked from the image's Docker Hub page). If there's no public repository, you're out of luck.",
    "Docker COPY behaving inconsistently": "Generally, COPY instruction copy files and/or directories from \"/app\" and adds them to the container at path \"/www/\". If you want to have a \"app\" directory inside of \"/www/\" then your COPY instruction should looks like:\nCOPY /app /www/app/\n\nYou can read more about COPY instruction in documentation. Here I paste explanation of this behaviour from it:\nIf is a directory, the entire contents of the directory are copied, including filesystem metadata. Note:The directory itself is not copied, just its contents.\n\n\nRegarding to your update:\nCOPY . /target and COPY app /target behaves the same. It takes entire contents from source directory ( from app or from . directory ) and copy it to the /target directory.\n\nYou can see slightly different behaviour when you use wildcards, eg. COPY app/* /www/. It copy all files from app/ to /www/ but then it treat every single directory from app/ like a source and copy its contents to /www/.\n\nWhy in Docker COPY is not implemented in the same way like it is in UNIX's cp command? I don't know, but if you want you can create pull request with own implementation :)",
    "Dockerfile, persist data with VOLUME": "What is not very obvious is that you are creating a brand new container every time you do a \"docker run\". Each new container would then have a fresh volume.\nSo your data is being persisted, but you're not reading the data from the container you wrote it to.\nExample to illustrate the problem\nSample Dockerfile\nFROM ubuntu\nVOLUME /data\nbuilt as normal\n$ docker build . -t myimage\nSending build context to Docker daemon 2.048 kB\nStep 1 : FROM ubuntu\n ---> bd3d4369aebc\nStep 2 : VOLUME /data\n ---> Running in db84d80841de\n ---> 7c94335543b8\nNow run it twice\n$ docker run -ti myimage echo hello world\n$ docker run -ti myimage echo hello world\nAnd take a look at the volumes\n$ docker volume ls\nDRIVER              VOLUME NAME\nlocal               078820609d31f814cd5704cf419c3f579af30672411c476c4972a4aad3a3916c\nlocal               cad0604d02467a02f2148a77992b1429bb655dba8137351d392b77a25f30192b\nThe \"docker rm\" command has a special \"-v\" option that will cleanup any volumes associated with containers.\n$ docker rm -v $(docker ps -qa)\nHow to use a data container\nUsing the same docker image, built in the previous example create a container whose sole purpose is to persist data via it's volume\n$ docker create --name mydata myimage\nLaunch another container that saves some data into the \"/data\" volume\n$ docker run -it --rm --volumes-from mydata myimage bash\nroot@a1227abdc212:/# echo hello world > /data/helloworld.txt\nroot@a1227abdc212:/# exit\nLaunch a second container that retrieves the data\n$ docker run -it --rm --volumes-from mydata myimage cat /data/helloworld.txt\nhello world\nCleanup, simply remove the container and specify the \"-v\" option to ensure its volume is cleaned up.\n$ docker rm -v mydata\nNotes:\nThe \"volumes-from\" parameter means all data is saved into the underlying volume associated with the \"mydata\" container\nWhen running the containers the \"rm\" option will ensure they are automatically removed, useful for once-off containers.",
    "Docker: Best practice for development and production environment": "So the way I handle it is I have 2 Docker files (Dockerfile and Dockerfile.dev).\nIn the Dockerfile.dev I have:\nFROM node:6\n\n# Update the repository\nRUN apt-get update\n\n# useful tools if need to ssh in or used by other tools\nRUN apt-get install -y curl net-tools jq\n\n# app location\nENV ROOT /usr/src/app\n\nCOPY package.json /usr/src/app/\n\n# copy over private npm repo access file\nADD .npmrc /usr/src/app/.npmrc\n\n# set working directory\nWORKDIR ${ROOT}\n\n# install packages\nRUN npm install\n\n# copy all other files over\nCOPY . ${ROOT}\n\n# start it up\nCMD [ \"npm\", \"run\", \"start\" ]\n\n# what port should I have\nEXPOSE 3000\nMy NPM scripts look like this\n\"scripts\": {\n    ....\n    \"start\": \"node_modules/.bin/supervisor -e js,json --watch './src/' --no-restart-on error ./index.js\",\n    \"start-production\": \"node index.js\",\n    ....\n},\nYou will notice it uses supervisor for start so any changes to any file under src will cause it to restart the server without requiring a restart to docker.\nLast is the docker compose.\ndev:\n  build: .\n  dockerfile: Dockerfile.dev\n  volumes:\n    - \"./src:/usr/src/app/src\"\n    - \"./node_modules:/usr/src/node_modules\"\n  ports:\n    - \"3000:3000\"\n\nprod:\n  build: .\n  dockerfile: Dockerfile\n  ports:\n    - \"3000:3000\"\nSo you see in a dev mode it loads and mounts the current directory's src folder to the container at /usr/src/app/src and also the node_modules directory to the /usr/src/node_modules.\nThis makes it so that I can make changes locally and save, the volume will update the container's file, then supervisor will see that change and restart the server.\n** Note as it doesn't watch the node_modules folder you have to change another file in the src directory to do the restart **",
    "Shared folder in Docker. With Windows. Not only \"C/user/\" path": "This question and this question have a similar root problem, mounting a non C:/ drive folder in boot2docker. I wrote an in-depth answer to the other question that provide the same information that is in the first half of @VonC's answer.\nFrom Docker Docs:\nAll other paths come from your virtual machine\u2019s filesystem. [...] In the case of VirtualBox you need to make the host folder available as a shared folder in VirtualBox. Then, you can mount it using the Docker -v flag.\nTo get your folder mounted in a container:\nThis mounts your entire D:\\ drive, you can simply change the file paths to be more granular and specific.\nShare the directory with VBox:\nThis only needs to be done once.\nIn windows CMD:\nVBoxManage sharedfolder add \"boot2docker-vm\" --name \"d-share\" --hostpath \"D:\\\"\nMount the shared directory in your VM:\nThis will need to be done each time you restart the VM.\nIn the Boot2Docker VM terminal:\nmount -t vboxsf -o uid=1000,gid=50 d-share /d\nTo see sources and explanation for how this works see my full answer to the other similar question\nAfter this you can use the -v/--volume flag in Docker to mount this folder or any sub-folders or files into containers. If you mounted your whole D:\\ drive you can use that exact docker run command from your question and it should now work. If you mounted a specific part of you drive you will have to change the paths to match.\nTo edit in windows, run in docker:\nAlso from Docker Docs:\nMounting a host directory can be useful for testing. For example, you can mount source code inside a container. Then, change the source code and see its effect on the application in real time.\nAs a VBox shared directory you should be able to see changes made from the Windows side reflected in the boot2docker vm.\nYou may need to restart containers to see the changes actually appear, this depends on how the program running inside the container, in your case ruby, uses the files. If the files are compiled into an app when the container starts, for example, you will definitely need to restart the container to see the changes.\nNote:\nBeware the CR LF vs. LF line ending difference when writing files in Windows and reading them in Linux. Make sure your text editor is saving files with Unix line endings or else you may start to see errors caused by '^M' appended to the end of all your lines.",
    "Installing Google Chrome on Ubuntu via Dockerfile hitting Geographic Area [duplicate]": "I solved that issue by configuring the timezone before installing Chrome\nENV TZ=Europe/Madrid \nRUN echo \"Preparing geographic area ...\"\nRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone",
    "Setting security_opt in docker-compose.yaml doesnt work": "Answering \"How can i check security_opt settings been applied?\"\n$ docker inspect [CONTAINER-NAME] --format '{{ .Id }}: SecurityOpt={{ .HostConfig.SecurityOpt }}'\nTo do it for all running containers:\n$ docker ps --quiet --all | xargs docker inspect --format '{{ .Id }}: SecurityOpt={{ .HostConfig.SecurityOpt }}'",
    "How to copy files from a docker image - dockerfile cmd": "Copying files from the image to the host at build-time is not supported.\nThis can easily be achieved during run-time using volumes.\nHowever, if you really want to work-around this by all means, you can have a look in the custom build outputs documentation, that introduced support for this kind of activity.\n\nHere is a simple example inspired from the official documentation:\nDockerfile\nFROM alpine AS stage-a\nRUN mkdir -p /opt/temp/\nRUN touch /opt/temp/file-created-at-build-time\nRUN echo \"Content added at build-time\" > /opt/temp/file-created-at-build-time\n\nFROM scratch as custom-exporter\nCOPY --from=stage-a /opt/temp/file-created-at-build-time .\nFor this to work, you need to launch the build command using these arguments:\nDOCKER_BUILDKIT=1 docker build --output out .\nThis will create on your host, aside the Dockerfile, a directory out with the file you need:\n.\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 out\n    \u2514\u2500\u2500 file-created-at-build-time\ncat out/file-created-at-build-time \nContent added at build-time",
    "pass external config file to docker container at runtime": "I'd recommend instead of passing application.conf file with overrides, go with overrides based on environment variables, because of reasons like:\nMounting volume with configuration file might be tricky from deployment tools perspective;\nNot all configurations management tools (for instance HashiCorp Consul) provide HOCON support, but managing environment variables is almost a standard. Especially if it contains secrets which needs to be protected;\nSo, you can do next: In your application conf set overrides via environment variable:\nfoo=default\nfoo={?FOO}\nAnd run application docker container with specific override:\ndocker run ...\n -e foo=bar \\\n ...\nPlease, see for more details:\nHOCON Optional system or env variable overrides: https://github.com/lightbend/config#optional-system-or-env-variable-overrides\nRun docker container with environment variables: https://docs.docker.com/engine/reference/run/#env-environment-variables\nHope this helps!",
    "ENTRYPOINT: first run command as root, then start shell for non-root user": "You start your container as root. This runs your entrypoint as root. Perform all the steps you need, then make the last step look like:\nexec gosu username /bin/bash\nTo launch /bin/bash as the user username. You can find gosu in this github repo. It has the advantage of running an su command with an implicit exec which avoids leaving the parent process around which can break signal handling.\nIf you make /bin/bash the value of CMD, you can make this more flexible with:\nexec gosu username \"$@\"\nMake sure to use the JSON syntax for ENTRYPOINT and CMD to avoid issues with the merged commands and cli args.\nThis is preferable over sudo since it avoids any option to go back from the user to root.",
    "docker ERROR: for nginx Cannot start service nginx: driver failed programming external connectivity on": "You can't allocate port 9010 of your host for both services. This is what you're doing in the ports section of declaration of service nginx and web.\nMoreover, by default nginx will listen to port 80 and 443 for https.\nYou can keep it like that and publish to a different port on your host. See how to use port keyword in docker-compose :\nhttps://docs.docker.com/compose/compose-file/#ports\nMaybe you want something more like that:\nversion: '3'\n\nservices:\n  nginx:\n    image: nginx:latest\n    ports:\n      - \"10080:80\"\n      - \"10443:443\"\n    volumes:\n      - .:/app\n      - ./config/nginx:/etc/nginx/conf.d\n      - ./static_cdn:/static\n   depends_on:\n      - web\n\n  web:\n    build: .\n    command: ./start.sh\n    container_name: \"web-app\"\n    volumes:\n      - .:/app\n      - ./static_cdn:/static\n    expose:\n      - \"9010\"\n    depends_on:\n      - db\n\n  db: \n    image: postgres\ncontents of config/nginx/nginx.conf\nupstream web {\n  ip_hash;\n  server web-app:9010;\n}\n\nserver {\n    location /static {\n        autoindex on;\n        alias /static/\n     }\n\nlocation / {\n    proxy_pass http://web;\n}\n\nlisten 80;\nserver_name localhost;\n}\nConcerning your last question, you could go for an official Python image from the Docker hub Python repository or start from any other base image like debian:jessie-slim from Debian official repository or keep the Ubuntu 18.04 image",
    "Validate yaml schema with golang (semantic check)": "Here is what you could try.\nModel a struct after the shape of the expected yaml data:\ntype Config struct {\n        Version struct {\n                Required bool\n        }\n        ID struct {\n                Required bool\n                Pattern string\n        }\n        ReleaseVersion struct {\n                Required bool\n        }\n        Type interface{}\n        Builds struct {\n                Type []interface{} `yaml:\"type\"`\n                Sequence struct {\n                        Type string\n                }\n                Mapping struct {\n                        Name map[string]interface{}\n                        Params struct {\n                                Type string `yaml:\"type\"`\n                                Mapping struct {\n                                        To map[string]string `yaml:\"=\"`\n                                }\n                        }\n                } `yaml:\"mapping\"`              \n        }\n}\nThe yaml flag yaml:\"somefield\" is added to label the field name of the yaml the data we're interested in.\nAlso many fields with unknown/undetermined type can be declared as empty interface (interface{}) or if you want to \"enforce\" that the underlying form is a key-value object you can either declare it as a map[string]interface{} or another struct.\nWe then unmarshal the yaml data into the struct:\ncfg := Config{}\nerr := yaml.Unmarshal([]byte(data), &cfg)\nif err != nil {\n        log.Fatalf(\"error: %v\", err)\n}\nSince we have modeled fields as either anonymous structs or maps, we can test if a particular field has a \"key-value\" value by checking its equality to nil.\n// Mapping is a key value object\nif (Mapping != nil) {\n        // Mapping is a key-value object, since it's not nil.\n}\n\n\n// type any is and key value\n// Mapping.To is declared with map[string]string type\n// so if it's not nil we can say there's a map there.\nif (Mapping.To != nil) {\n        // Mapping.To is a map\n}\nIn marshaling/unmarshaling, maps and structs are pretty interchangeable. The benefit of a struct is you can predefine the field's names ahead of time while unmarshaling to a map it won't be clear to you what the keys are.",
    "Changing /proc/sys/kernel/core_pattern file inside docker container": "The kernel does not support per-container patterns. There is a patch for this, but it is unlikely to go in any time soon. The basic problem is that core patterns support piping to a dedicated process which is spawned for this purpose. But the code spawning it does not know how to handle containers just yet. For some reason a simplified pattern handling which requires a target file was not deemed acceptable.",
    "How do I write a dockerfile to execute a simple bash script?": "You need to make the script part of the container. To do that, you need to copy the script inside using the COPY command in the Docker file, e.g. like this\nFROM ubuntu:14.04\nCOPY run_netcat_webserver.sh /some/path/run_netcat_webserver.sh\nCMD /some/path/run_netcat_webserver.sh\nThe /some/path is a path of your choice inside the container. Since you don't need to care much about users inside the container, it can be even just /.\nAnother option is to provide the script externally, via mounted volume. Example:\nFROM ubuntu:14.04\nVOLUME /scripts\nCMD /scripts/run_netcat_webserver.sh\nThen, when you run the container, you specify what directory will be mounted as /scripts. Let's suppose that your script is in /tmp, then you run the container as\ndocker run --volume=/tmp:/scripts (rest of the command options and arguments)\nThis would cause that your (host) directory /tmp would be mounted under the /scripts directory of the container.",
    "mkdir is not executing in Dockerfile/build": "There's an interesting character between RUN and mkdir in your Dockerfile. Replacing it with a space makes your Dockerfile build.\ndocker build output with yours:\nSending build context to Docker daemon 2.048 kB\nStep 1 : FROM ubuntu:14.04\n ---> c4bea91afef3\nStep 2 : RUN\u2002MKDIR\nUnknown instruction: RUN\u2002MKDIR\ndocker build output with fixed:\nSending build context to Docker daemon 2.048 kB\nStep 1 : FROM ubuntu:14.04\n ---> c4bea91afef3\nStep 2 : RUN mkdir -p /home/developer\n ---> Using cache\n ---> 1ac57f7c9ccd\nSuccessfully built 1ac57f7c9ccd",
    "How to write a Dockerfile which I can start a service and run a shell and also accept arguments for the shell?": "You have to use supervisord inside a Docker container able to use more complex shell syntax when you creating containers.\nDocker documentation about supervisord: https://docs.docker.com/engine/articles/using_supervisord/\nYOU CAN use more complex shell syntax (that you want to use) when you create a new container with $ docker run command, however this will not work within systemd service files (due to limitation in systemd) and docker-compose .yml files and the Dockerfiles also.\nFirst, you have to install supervisord in your Dockerfile:\nRUN apt-get -y update && apt-get -y dist-upgrade \\\n    && apt-get -y install \\\n        supervisor\nRUN mkdir -p /var/log/supervisord\nThan place this at the end of the Dockerfile:\nCOPY etc/supervisor/conf.d/supervisord.conf /etc/supervisor/conf.d/\nCMD [\"/usr/bin/supervisord\", \"-c\", \"/etc/supervisor/supervisord.conf\"]\nCreate a file in etc/supervisor/conf.d/supervisord.conf next to your Dockerfile:\n[unix_http_server]\nfile=/var/run/supervisord.sock\nchmod=0777\nchown=root:root\nusername=root\n\n[supervisord]\nnodaemon=true\nuser=root\nenvironment=HOME=\"/root\",USER=\"root\"\nlogfile=/var/log/supervisord/supervisord.log\npidfile=/var/run/supervisord.pid\nchildlogdir=/var/log/supervisord\nlogfile_maxbytes=10MB\nloglevel=info\n\n[program:keepalive]\ncommand=/bin/bash -c 'echo Keep Alive service started... && tail -f /dev/null'\nautostart=true\nautorestart=true\nstdout_events_enabled=true\nstderr_events_enabled=true\nstdout_logfile=/var/log/supervisord/keepalive-stdout.log\nstdout_logfile_maxbytes=1MB\nstderr_logfile=/var/log/supervisord/keepalive-stderr.log\nstderr_logfile_maxbytes=1MB\n\n[program:dcheck]\ncommand=/bin/bash -c 'chmod +x /root/dcheck/repo/dcheck.sh && cd /root/dcheck/repo && ./dcheck.sh'\nautostart=true\nautorestart=true\nstdout_events_enabled=true\nstderr_events_enabled=true\nstdout_logfile=/var/log/supervisord/dcheck-stdout.log\nstdout_logfile_maxbytes=10MB\nstderr_logfile=/var/log/supervisord/dcheck-stderr.log\nstderr_logfile_maxbytes=1MB\nThis is a more complex supervisord.conf and probably you don't need many of the commands here, plus you have to change the file locations to your needs. However you can see how to create log files from the bash output of the script.\nLater on you have to docker exec in that container and you can watch real-time the log with:\ndocker exec -it your_running_container /bin/bash -c 'tail -f /var/log/supervisord/dcheck-stdout.log'\nYou have the option to show subprocess log in the main supervisord log with loglevel=debug, however this is full of timestamps and comments, not the pure bash output like when you run the script directly.\nAs you can see in my scipt, I keeping alive the container with tail -f /dev/null, however this is a bad practice. The .sh script should keep alive your container on their own.\nWhen you sending your scipt to ENTRYPOINT as ENTRYPOINT [\"sudo service docker start\", \"&&\", \"/home/user/che/bin/che.sh run\"], you want to change the default docker ENTRYPOINT from /bin/sh -c to sudo (also, use full location names).\nThere are two ways to change docker ENTRYPOINT in Dockerfile. One is to place this in the head section of your Dockerfile:\nRUN ln -sf /bin/bash /bin/sh && ln -sf /bin/bash /bin/sh.distrib\nOr place this at the bottom:\nENTRYPOINT ['/bin/bash', '-c']\nAfter when you send any CMD to this Dockerfile, it will be run by /bin/bash -c command.\nOne more thing to note is that the first command takes PID1, so if you want to run the .sh script without tail -f /dev/null in my supervisord script, it will take PID1 process place and CTRL+C command will not gonna work. You have to shut down the container from another shell instance.\nBut if you run the command with:\n[program:dcheck]\ncommand=/bin/bash -c 'echo pid1 > /dev/null && chmod +x /root/dcheck/repo/dcheck.sh && cd /root/dcheck/repo && ./dcheck.sh'\necho pid1 > /dev/null will take PID1 and SIGTERM, SIGKILL and SIGINT will work again with your shell script.\nI try to stay away running Docker with --privileged flag. You have many more options to get away on the limitations.\nI don't know anything about your stack, but generally good idea to not dockerise Docker in a Container. Is there a specific reason why sudo service docker start is in your Dockerfile?\nI don't know anything about this container, is it have to be alive? Because if doesn't, there is a more simple solution, only running the container when it has to process something from the command line. Place this file on the host with the name of run let's say in /home/hostuser folder and give it chmod +x run:\n#!/bin/bash\ndocker run --rm -it -v /home/hostuser/your_host_shared_folder/:/root/your_container_shared_folder/:rw your_docker_image \"echo pid1 > /dev/null && chmod +x /root/script.sh && cd  /root && ./script.sh\"\nIn this case, ENTRYPOINT is preferred to be ENTRYPOINT ['/bin/bash', '-c'].\nRun this script on the host with:\n$ cd /home/hostuser\n$ ./run -flag1 -flag2 args1 args2 args3",
    "docker-compose - Expose linked service port": "You don't need to: an EXPOSE port from one service is directly visible from another (linking to the first).\nNo port mapping necessary (as you do for 9000 from SonarQube and 3306)\nPort mapping is necessary for accessing a container from the host.\nBut from container to a (linked) container (both managed by the same docker daemon), any port declared in EXPOSE in its Dockerfile is directly accessible.\nI want to expose both ports to my localhost. I need access to both ports from my machine, as I SonarQube runner needs access to the database\nWell then,... the db section should have its own port mapping section:\ndb:\n  ports:\n    - \"xxx:yyyy\"",
    "If I use EXPOSE $PORT in a Dockerfile, can I un-expose the port it when I use `docker run`?": "Even a couple of years later, the situation hasn't changed much. There is no UNEXPOSE but atleast there is a workaround with \"docker save\" and \"docker load\" allowing to edit the metadata of a docker image. Atleast to remove volumes and ports, I have created a little script that can automate the task, see docker-copyedit.",
    "yaml: line 8:did not find expected key": "It happens when we do our own yml file for docker, you need to indent two spaces for sub entries under image details:\nversion: '1'\nservices:\n    mariadb-ikg:\n      image: bitnami/mariadb:10.3\n      ports:\n        - 3306:3306\n      volumes:\n        - D:/docker/bitnami-mariadb/databases:/bitnami/mariadb\n      environment:\n        - MARIADB_ROOT_PASSWORD=123456\n    phpfpm-ikg:\n      image: wyveo/nginx-php-fpm:php80\n      ports:\n        - 80:80\n      volumes:\n        - D:/docker/wyveo-nginx-php-fpm/wordpress:/usr/share/nginx/html\n      depends_on:\n        - mariadb-ikg",
    "How can I create a Docker container whose timezone matches that of my local machine?": "using docker run command:\n-e TZ=`ls -la /etc/localtime | cut -d/ -f8-9`\nSource\nif you still want to use volumes you need to share /etc form the Docker UI in your MAC \"Prefernces --> Resources --> FILE SHARING\"\nUpdate\nfor docker-compose :\nunder build section use:\nargs:\n  - TZ\nand then:\nenvironment:\n    - TZ=${TZ}\nand then start it like - after re-build -:\nexport TZ=`ls -la /etc/localtime | cut -d/ -f8-9` && docker-compose up -d --build",
    "Is there a way to create Docker volume and prepopulate it with data?": "If you want to populate a volume with data before using it, you can first create it:\ndocker volume create myapplicationdata\nAnd then attach it to an ephemeral container in order to populate it with data:\ntar -C /path/to/my/files -c -f- . | docker run --rm -i -v myapplicationdata:/data alpine tar -C /data -xv -f-\nTo use a pre-existing volume in your docker-compose.yml, declare it as an external volume:\nversion: \"3\"\n\nservices:\n  myapplication:\n    image: image-name\n    container_name: container-name\n    volumes:\n      - myapplicationdata:/app/data\n    ports:\n      - \"3000:3000\"\n\nvolumes:\n  myapplicationdata:\n    external: true",
    "Installing homebrew packages during Docker build": "You have to set the PATH environment variable in the Dockerfile with:\nENV PATH=~/.linuxbrew/bin:~/.linuxbrew/sbin:$PATH\nHere is a complete working Dockerfile:\nFROM debian\nRUN apt-get update && apt-get install -y git curl binutils clang make\nRUN git clone https://github.com/Homebrew/brew ~/.linuxbrew/Homebrew \\\n&& mkdir ~/.linuxbrew/bin \\\n&& ln -s ../Homebrew/bin/brew ~/.linuxbrew/bin \\\n&& eval $(~/.linuxbrew/bin/brew shellenv) \\\n&& brew --version \\\n&& brew tap aws/tap && brew install aws-sam-cli \\\n&& sam --version\nENV PATH=~/.linuxbrew/bin:~/.linuxbrew/sbin:$PATH",
    "Dockerfile - Docker directive to switch home directory": "You can change user directory using WORKDIR in the dockerfile, this will become the working directory. So whenever you created the container the working directory will be the one that is pass to WORKDIR instruction in Dockerfile.\nWORKDIR\nDockerfile reference for the WORKDIR instruction\nFor clarity and reliability, you should always use absolute paths for your WORKDIR. Also, you should use WORKDIR instead of proliferating instructions like RUN cd \u2026 && do-something, which are hard to read, troubleshoot, and maintain.\nFROM buildpack-deps:buster\n\nRUN groupadd -r someteam --gid=1280 && useradd -r -g someteam --uid=1280 --create-home --shell /bin/bash someteam\n\n# Update and allow for apt over HTTPS\nRUN apt-get update && \\\n  apt-get install -y apt-utils\nRUN apt-get install -y apt-transport-https\nRUN apt update -y \nRUN apt install python3-pip -y\n\n# switch user from 'root' to \u2018someteam\u2019 and also to the home directory that it owns \nUSER someteam\nWORKDIR /home/someteam\nusing $HOME will cause error.\nWhen you use the USER directive, it affects the userid used to start new commands inside the container.Your best bet is to either set ENV HOME /home/aptly in your Dockerfile, which will work dockerfile-home-is-not-working-with-add-copy-instructions",
    "How to use local nuget package sources for Dockerfile dotnet restore [duplicate]": "To have all packages ready you need restore before building. To have all packages during the build you need to copy the packages.\nHere is an example in form of an experiment:\nPreparation:\nHave the sdk ready: docker pull microsoft/dotnet:2.2-sdk.\nHave src/src.csproj ready:\n<Project Sdk=\"Microsoft.NET.Sdk\">\n  <PropertyGroup>\n    <TargetFramework>netstandard2.0</TargetFramework>\n  </PropertyGroup>\n  <ItemGroup>\n    <PackageReference Include=\"Newtonsoft.Json\" Version=\"12.0.2\" />\n  </ItemGroup>\n</Project>\nHave src/Dockerfile ready:\nFROM microsoft/dotnet:2.2-sdk AS byse\nCOPY packages /root/.nuget/packages\nCOPY src src\nRUN ls /root/.nuget/packages\nWORKDIR /src\nRUN dotnet restore\nRUN ls /root/.nuget/packages\nExecution:\nRestore the Packages:\ndocker run --rm -v $(pwd)/src:/src -v $(pwd)/packages:/root/.nuget/packages -w /src  microsoft/dotnet:2.2-sdk dotnet restore\nBuild the Image:\ndocker build -t test -f src/Dockerfile .\nExpectation:\nSending build context to Docker daemon  13.77MB\nStep 1/7 : FROM microsoft/dotnet:2.2-sdk AS byse\n ---> e4747ec2aaff\nStep 2/7 : COPY packages /root/.nuget/packages\n ---> 76c3e9869bb4\nStep 3/7 : COPY src src\n ---> f0d3f8d9af0a\nStep 4/7 : RUN ls /root/.nuget/packages\n ---> Running in 8323a9ba8cc6\nnewtonsoft.json\nRemoving intermediate container 8323a9ba8cc6\n ---> d90056004474\nStep 5/7 : WORKDIR /src\n ---> Running in f879d52f81a7\nRemoving intermediate container f879d52f81a7\n ---> 4020c789c338\nStep 6/7 : RUN dotnet restore\n ---> Running in ab62a031ce8a\n  Restore completed in 44.28 ms for /src/src.csproj.\nRemoving intermediate container ab62a031ce8a\n ---> 2cd0c01fc25d\nStep 7/7 : RUN ls /root/.nuget/packages\n ---> Running in 1ab3310e2f4c\nnewtonsoft.json\nRemoving intermediate container 1ab3310e2f4c\n ---> 977e59f0eb10\nSuccessfully built 977e59f0eb10\nSuccessfully tagged test:latest\nNote that the ls steps are cached and would not print on a subsequent call. Run docker rmi test to reset.\nStep 4/7 runs before the restore and the packages are already cached.\nStep 4/7 : RUN ls /root/.nuget/packages\n ---> Running in 8323a9ba8cc6\nnewtonsoft.json\nThis can solves excessive restore times for example during automated builds.\nTo solve your network issue you can try to mount the network patch instead of the local path during the resolve step or robocopy files from your corp network into a local cache first.",
    "How to specify in Dockerfile that the image is interactive?": "Many of the docker run options can only be specified at the command line or via higher-level wrappers (shell scripts, Docker Compose, Kubernetes, &c.). Along with port mappings and network settings, the \u201cinteractive\u201d and \u201ctty\u201d options can only be set at run time, and you can\u2019t force these in the Dockerfile.",
    "Docker container/image running but there is no port number": "This line from the question helps reveal the problem;\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES\n01cc8173abfa        test_1              \"python manage.py ru\u2026\"   13 seconds ago      Exited (1) 11 seconds ago                       testing_first\nExited (1) (from the STATUS column) means that the main process has already exited with a status code of 1 - usually meaning an error. This would have freed up the ports, as the docker container stops running when the main process finishes for any reason.\nYou need to view the logs in order to diagnose why.\ndocker logs 01cc will show the logs of the docker container that has the ID starting with 01cc. You should find that reading these will help you on your way. Knowing this command will help you immensely in debugging weirdness in docker, whether the container is running or stopped.\nAn alternative 'quick' way is to drop the -d in your run command. This will make your container run inline rather than as a daemon.",
    "COPY package.json - Dockerfile": "Do not ever run nodemon in production (if that's what you tried to do). You should configure your restart in case if app crashes. Preferably, set it to always in docker-compose.yml\nThe best way to structure Dockerfile in your case:\nFROM node:latest\nWORKDIR ./app\n# please note, you already declared a WORKDIR, \n# therefore your files will be automaticaly pushed to ./app\nCOPY package.json ./\nRUN npm install -g\nCOPY ./ ./ \nEXPOSE 3000\nCMD [\"npm\", \"start\"]\nHope, that helps.",
    "apt-get commands doesn't work in docker containers": "If apt-get is working fine on the VM, you can build the container using network host mode.\ndocker build --network=host ...",
    "Docker Caching, how does it really work?": "Yes, the two images will share the same layers if you meet the prerequisites. Docker layers are reused independently of the resulting image name. The requirements to use a cached layer instead of creating a new one are:\nThe build command needs to be run against the same docker host where the previous image's cache exists.\nThe previous layer ID must match between the cache layer and the running build step.\nThe command currently being run, or the source context if you are running a COPY or ADD, must be identical. Docker does not know if you are running a command that pulls from an external changing resource (e.g. git clone or apt-get update), which can result in a false cache hit.\nYou cannot have disabled caching in your build command.\nKeep in mind that layers are immutable, once created they are never changed, just replaced with different layers with new ID's when you run a different build. When you run a container, it uses a copy-on-write RW layer specific to that container, which allows multiple containers and images to point to the same image layers when they get a cache hit.\nIf you are having problems getting the cache to match in the two builds, e.g. importing a large file and something like the file timestamp doesn't match, consider creating an intermediate image that contains the common files. Then each project can build FROM that intermediate image.",
    "Is it possible to pause a Docker image build?": "Is it possible to pause a Docker image\nno, you cannot pause the docker build command.\nYou could give a try to the Scroll Lock key, but depending on your terminal that might fail.\nYou could pipe the result of the docker build command to less -R:\ndocker build -t test . | less -R\nOnce built, you can then use the arrow keys to go up and down, use / to search for test, etc.\n-R is to keep colors\n-r  -R  ....  --raw-control-chars  --RAW-CONTROL-\n                Output \"raw\" control characters.\nAlso you can record the output to a file (I know you explicitly said you don't want this solution, but it can suit others):\ndocker build -t test . | tee build.log",
    "Failed to calculate checksum of ref [duplicate]": "All COPY commands must refer to files available in the build context, which is the directory containing your Dockerfile.\nTherefore, you should ensure that all necessary files are placed in the Dockerfile directory so that the COPY instructions in the Dockerfile can locate the source files and copy them into the Docker image.",
    "How to create tun interface inside Docker container image?": "The /dev directory is special, and Docker build steps cannot really put anything there. That also is mentioned in an answer to question 56346114.\nApparently a device in /dev isn't a file with data in it, but a placeholder, an address, a pointer, a link to driver code in memory that does something when accessed. Such driver code in memory is not something that a Docker image would hold.\nI got device creation working in a container by putting your command line code in an .sh script wrapping the app we really want to run.",
    "Can docker image choose the OS?": "Docker is composed of layers. At the beginning of any Dockerfile you specify the OS by typing e.g. FROM python:3 My belief is that if you were to add another OS. The image would retain the environment from the first OS and install the env of the second OS over that. So essentially, your image would have both environments.\nIf you create a python image from the command above and name it docker build -t 'this_python' . then make a new Dockerfile with the first line: FROM this_python so the new image has python already, and you can install anything over this.\nBest practice is to keep your docker image as small as possible. Install only what is required.\nA quick example\nFROM python:3\nFROM ubuntu:latest\n\nRUN apt-get update\nThe above Dockerfile gives you an image with Python and Ubuntu installed. But this is not how you should do it. Better is to use FROM ubuntu:latest and then install python over it.",
    "Making an Oracle JDK docker image": "Here's an Atlassian blog post that describes how to build an Oracle JDK docker container.\nAnd here is an existing Oracle JDK image on docker hub, and another.\nNote the license disclaimers in the READMEs, though. I'm not sure that is enough to bypass the \"Limitations on Redistribution\" clause in the Oracle JDK License. Probably safer to use their Dockerfiles and the blog as a basis to create your own. Just put it in a private repo rather than on docker hub.\nFor completeness, here is the Dockerfile content from the blog post:\n# AlpineLinux with a glibc-2.21 and Oracle Java 8\n\nFROM alpine:3.2\nMAINTAINER Anastas Dancha [...]\n\n# Install cURL\nRUN apk --update add curl ca-certificates tar && \\\n    curl -Ls https://circle-artifacts.com/gh/andyshinn/alpine-pkg-glibc/6/artifacts/0/home/ubuntu/alpine-pkg-glibc/packages/x86_64/glibc-2.21-r2.apk > /tmp/glibc-2.21-r2.apk && \\\n    apk add --allow-untrusted /tmp/glibc-2.21-r2.apk\n\n# Java Version\nENV JAVA_VERSION_MAJOR 8\nENV JAVA_VERSION_MINOR 45\nENV JAVA_VERSION_BUILD 14\nENV JAVA_PACKAGE       jdk\n\n# Download and unarchive Java\nRUN mkdir /opt && curl -jksSLH \"Cookie: oraclelicense=accept-securebackup-cookie\"\\\n  http://download.oracle.com/otn-pub/java/jdk/${JAVA_VERSION_MAJOR}u${JAVA_VERSION_MINOR}-b${JAVA_VERSION_BUILD}/${JAVA_PACKAGE}-${JAVA_VERSION_MAJOR}u${JAVA_VERSION_MINOR}-linux-x64.tar.gz \\\n    | tar -xzf - -C /opt &&\\\n    ln -s /opt/jdk1.${JAVA_VERSION_MAJOR}.0_${JAVA_VERSION_MINOR} /opt/jdk &&\\\n    rm -rf /opt/jdk/*src.zip \\\n           /opt/jdk/lib/missioncontrol \\\n           /opt/jdk/lib/visualvm \\\n           /opt/jdk/lib/*javafx* \\\n           /opt/jdk/jre/lib/plugin.jar \\\n           /opt/jdk/jre/lib/ext/jfxrt.jar \\\n           /opt/jdk/jre/bin/javaws \\\n           /opt/jdk/jre/lib/javaws.jar \\\n           /opt/jdk/jre/lib/desktop \\\n           /opt/jdk/jre/plugin \\\n           /opt/jdk/jre/lib/deploy* \\\n           /opt/jdk/jre/lib/*javafx* \\\n           /opt/jdk/jre/lib/*jfx* \\\n           /opt/jdk/jre/lib/amd64/libdecora_sse.so \\\n           /opt/jdk/jre/lib/amd64/libprism_*.so \\\n           /opt/jdk/jre/lib/amd64/libfxplugins.so \\\n           /opt/jdk/jre/lib/amd64/libglass.so \\\n           /opt/jdk/jre/lib/amd64/libgstreamer-lite.so \\\n           /opt/jdk/jre/lib/amd64/libjavafx*.so \\\n           /opt/jdk/jre/lib/amd64/libjfx*.so\n\n# Set environment\nENV JAVA_HOME /opt/jdk\nENV PATH ${PATH}:${JAVA_HOME}/bin",
    "Docker tomcat editing configuration files through dockerfile": "I figured it out. The problem was that the user with which I was executing the build tasks did not have sufficient rights to write stuff in those folders. What I did was add a USER root task to the Dockerfile so it executes all the commands as root.\nMy Dockerfile now looks like this:\nFROM tomcat\nUSER root\nCOPY tomcat-users.xml /usr/local/tomcat/conf/\nCOPY context.xml /usr/local/tomcat/webapps/manager/META-INF/\nCMD [\"catalina.sh\",\"run\"]",
    "How to have two JARs start automatically on \"docker run container\"": "The second CMD instruction replaces the first, so you need to use a single instruction for both commands.\nEasy (not so good) Approach\nYou could add a bash script that executes both commands and blocks on the second one:\n# start.sh\n/usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar first.jar &\n/usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar second.jar\nThen change your Dockerfile to this:\n# base image is java:8 (ubuntu)\nFROM java:8\n\n# add files to image \nADD first.jar .\nADD second.jar .\nADD start.sh .\n\n# start on run\nCMD [\"bash\", \"start.sh\"]\nWhen using docker stop it might not shut down properly, see: https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/\nBetter Approach\nTo solve this, you could use Phusion: https://hub.docker.com/r/phusion/baseimage/\nIt has an init-system that is much easier to use than e.g. supervisord.\nHere is a good starting point: https://github.com/phusion/baseimage-docker#getting_started\nInstructions for using phusion\nSadly there is not official openjdk-8-jdk available for Ubuntu 14.04 LTS. You could try with an inofficial ppa, which is used in the following explanation.\nIn your case you would need to bash scripts (which act like \"services\"):\n# start-first.sh (the file has to start with the following line!):\n#!/bin/bash\nusr/lib/jvm/java-8-openjdk-amd64/bin/java -jar /root/first.jar\n\n# start-second.sh\n#!/bin/bash\nusr/lib/jvm/java-8-openjdk-amd64/bin/java -jar /root/second.jar\nAnd your Dockerfile would look like this:\n# base image is phusion\nFROM phusion/baseimage:latest\n\n# Use init service of phusion\nCMD [\"/sbin/my_init\"]\n\n# Install unofficial openjdk-8\nRUN add-apt-repository ppa:openjdk-r/ppa && apt-get update && apt-get dist-upgrade -y && apt-get install -y openjdk-8-jdk\n\nADD first.jar /root/first.jar\nADD second.jar /root/second.jar\n\n# Add first service\nRUN mkdir /etc/service/first\nADD start-first.sh /etc/service/first/run\nRUN chmod +x /etc/service/first/run\n\n# Add second service\nRUN mkdir /etc/service/second\nADD start-second.sh /etc/service/second/run\nRUN chmod +x /etc/service/second/run\n\n# Clean up\nRUN apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\nThis should install two services which will be run on startup and shut down properly when using docker stop.",
    "How to make Dockerfile COPY work for windows absolute paths?": "All the resources need to be in the dir that you run the build, i.e. where your Dockerfile is. You cant use an absolute path from elsewhere, think of it from the build perspective - where is the build happening - in the Dockerfile? It can run commands but those resources need to be public.\nhttps://github.com/moby/moby/issues/4592\nhttps://github.com/docker/compose/issues/4857",
    "using nuget cache inside a docker build with .net core when offline": "One suggestion I have to look into Docker BuildKit if you have not done so already. BuildKit adds support for Dockerfile mounts. It supports various types of mounts one being a cache intended for this exact scenario - build cache artifacts such as NuGet packages.",
    "\"Application startup exception: System.Net.Sockets.SocketException (111): Connection refused\" with docker-compose": "You can call services by it's name, 127.0.0.0.1 or localhost is the client container itself. Try changing the connection string to below -\n\"ConnectionStrings\": {\n    \"DbContext\": \"@Host=postgres;Port=5432;Database=kitchen_table;UserID=<user>Password=<password>;\"\n  }",
    "docker host port and container port": "3306/tcp -> 127.0.0.1:3666 means port 3306 inside container is exposed on to port 3666 of host.\nMore info here.\nIf you think output of docker port command is confusing then use docker inspect command to retrieve port mapping. As mentioned here in official doc.\ndocker ps docker port docker inspect are useful commands to get the info about port mapping.\n[user@jumphost ~]$ docker run -itd -p 3666:3306 alpine sh\nUnable to find image 'alpine:latest' locally\nlatest: Pulling from library/alpine\n050382585609: Pull complete \nDigest: sha256:6a92cd1fcdc8d8cdec60f33dda4db2cb1fcdcacf3410a8e05b3741f44a9b5998\nStatus: Downloaded newer image for alpine:latest\n428c80bfca4e60e474f82fc5fe9c1c0963ff2a2f878a70799dc5da5cb232f27a\n[user@jumphost ~]$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                    NAMES\n428c80bfca4e        alpine              \"sh\"                3 seconds ago       Up 3 seconds        0.0.0.0:3666->3306/tcp   fervent_poitras\n[user@jumphost ~]$ docker port 428c80bfca4e\n3306/tcp -> 0.0.0.0:3666\n[user@jumphost ~]$ docker inspect --format='{{range $p, $conf := .NetworkSettings.Ports}} {{$p}} -> {{(index $conf 0).HostPort}} {{end}}' 428c80bfca4e\n 3306/tcp -> 3666 \n[user@jumphost ~]$\ndocker inspect comtainer-id also gives a clear mapping of the ports.\n$ docker inspect 428c80bfca4e\n     |\n     |\n\"Ports\": {\n                \"3306/tcp\": [\n                    {\n                        \"HostIp\": \"0.0.0.0\",\n                        \"HostPort\": \"3666\"\n                    }\n                ]\n            },\n     |\n     |\nHope this helps.",
    "Dockerfile entrypoint unable to switch user": "I think that your su command should be something like\nsu $USERNAME --command \"/doit.sh\"\nb/c your entrpoiny script is switching user, doing nothing, and then switching back to root.",
    "Docker, copy files in production and use volume in development": "My problem is that with only one dockerfile, for the development command a volume will be mounted at /app and also files copied with ADD . /app. I haven't tested what happens in this scenario, but I am assuming it is incorrect to have both for the same destination.\nFor this scenario, it will do as follows:\na) Add your code in host server to app folder in container when docker build.\nb) Mount your local app to the folder in the container when docker run, here will always your latest develop code.\nBut it will override the contents which you added in dockerfile, so this could meet your requirements. You should try it, no need for any complex solution.",
    "How to fix permissions for an Alpine image writing files using Cron as non root user into accessible volume": "crond or cron should be used as root, as described in this answer.\nCheck out instead aptible/supercronic, a crontab-compatible job runner, designed specifically to run in containers. It will accomodate any user you have created.",
    "What does each sha mean in a docker image": "Those are digests of image layers. The same image might be tagged with different names. But the SHA256 digest is a unique and immutable identifier you can reference.\nIf you pull an image specifying the digest, you have a guarantee that the image you\u2019re using is always the same.\nThe more details can be found in docs, here: Pull an image by digest (immutable identifier) and here: Understand images, containers, and storage drivers",
    "How-to: Dockerfile layer(s) reuse / reference": "Based on further reading:\nThere is no INCLUDE like feature currently. https://github.com/docker/docker/issues/735\nBest practices encourage to use \"RUN apt-get update && apt-get install -y\" for all package installation. But it doesn't mean that you can not use that same technique to separate package installs (e.g. package-foo and package-bar) due to maintainability. It is a tradeoff with minimizing the number of layers. https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/#/minimize-the-number-of-layers (see also how the build cache operates, identifying it as different layers)\nThank you @Sri for some lead pointers.",
    "Docker: save - produces no output": "Since you are new to docker I'd like to suggest some things that might make your life easier:\nI committed the container which resulted in a 15GB image.\ncommit is generally not recommended for standard workflows. It is not really reproducible. I would suggest creating a Dockerfile using FROM <oracle image> and doing the work in the Dockerfile.\nIf you have large datasets you probably want to manage those in volumes or a host bind mount (aka volume mount).\nI need to upload this onto a server,\nThe recommended way of doing this is using docker push to push to a registry.\nit just hangs and produces no output.\nIt could be that it's just very slow? You can also check the docker daemon log file to see if there are any warnings.\nHow to restore or attach the ENV/CMD metadata (..the dockerfile?) to a exported/imported image?\nI don't think this is possible.",
    "From Golang, how to build/run a docker-compose project?": "I'm a complete Go noob, so take this with a grain of salt, but I was looking through the Docker compose implementation of their up command - they use Cobra for their cli tooling (which seems quite common), and Viper for argument parsing. The link provided is to the Cobra command implementation, which is different from the up internals, which the command uses.\nIf you were to want to add a command which would invoke the docker compose \"up\" command as part of your golang command (which is what I think you're going for - it's what I was looking to try to do), I think you'd have to accept the fact that you'd have to basically implement your own versions of the Cobra commands they have there, replicating their existing logic. That was my take.",
    "Docker tini no such file or direcory": "I updated your COPY and ENTRYPOINT commands.\nTry the following Dockerfile:\nFROM ubuntu:latest\n\nWORKDIR /vault\n\nCOPY ./run.sh ./run.sh\nCOPY ./docker-entrypoint.sh ./docker-entrypoint.sh\nCOPY ./config/local.json ./config/local.json\nCOPY ./logs ./logs\nCOPY ./file ./file\n\nENV TINI_VERSION v0.19.0\nADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini\nRUN chmod +x /tini\nRUN chmod +x run.sh\nRUN chmod 777 docker-entrypoint.sh\n\nRUN apt-get update && apt-get install -y software-properties-common curl gnupg2 && \\\n  curl -fsSL https://apt.releases.hashicorp.com/gpg | apt-key add - && \\\n  apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" && \\\n  apt-get update && apt-get install -y \\\n  vault bash && \\\n  setcap cap_ipc_lock= /usr/bin/vault\n\nENTRYPOINT [\"/tini\", \"--\", \"bash\", \"/vault/docker-entrypoint.sh\"]",
    "I Can't build my docker file to create my docker image using Docker build": "It's hard to be a 100% sure of how to guide you with the information provided, but it looks like you may have directly provided the file path instead of the folder path (you can read more in the doc):\n# this is used to build a Dockerfile file inside directory /PATH/TO/FOLDER\ndocker build /PATH/TO/FOLDER\nIf you want to give the Dockerfile path instead of the folder path, you may need to do (again read the doc for more):\n# this is used to build a Dockerfile file given its content\ndocker build - < /PATH/TO/FOLDER/Dockerfile\n(note that there a more subtleties because in the first case you build with a context and not in the second, I let you read Docker's doc which is fairly good).",
    ".NET Core Windows authentication in docker container": "By nature, your container is isolated and does not belong to your domain, which makes Windows Authentication a well known issue. The way to achieve this is by using a technology Microsoft introduced recently called gMSA, https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-R2-and-2012/hh831782(v=ws.11)\nAbout how to use it with Docker: https://www.axians-infoma.de/techblog/windows-authentication-in-docker-containers-just-got-a-lot-easier/ https://artisticcheese.wordpress.com/2017/09/09/enabling-integrated-windows-authentication-in-windows-docker-container/\nPer Mark request, you could also use a piece of code using LDAP:\nprivate bool VerifyServerCertificateCallback(LdapConnection connection, X509Certificate certificate)\n{\n    return new X509Certificate2(certificate).Verify();\n}\n\npublic bool ValidateCredentials(string userName, string password) \n{\n    try\n    {\n        var ldapDirectoryIdentifier = new ldapDirectoryIdentifier(ldapServer.ServerAddress);\n\n        var ldapConnection = new LdapConnection(ldapDirectoryIdentifier)\n        {\n            AuthType = AuthType.Basic\n        };\n        ldapConnection.SessionOptions.ProtocolVersion = 3;\n        ldapConnection.SessionOptions.SecureSocketLayer = true;\n        ldapConnection.SessionOptions.VerifyServerCertificate = VerifyServerCertificateCallback;\n\n        ldapConnection.Bind(new NetworkCredential(string.Format(ldapServer.UserLocation, userName), password));\n\n        ldapConnection.Dispose();\n    }\n    catch (Exception exception) {\n        continue;\n    }\n    return true;\n}\nAnd in your controller:\nif (ValidateCredentials(username, password))\n{\n    ClaimsPrincipal principal = new ClaimsPrincipal(new ClaimsIdentity(\n        new List<Claim>()\n        {\n            new Claim(ClaimTypes.Name, username),\n            //...\n        },\n        /*\"...\"*/));\n\n    await HttpContext.SignInAsync(AuthSchemeName, principal);\n}",
    ".net core build error when building docker image missing Newtonsoft.Json": "In my case, When used JsonProperty in a class,\nVisual Studio intellicode auto-filled,\nusing Newtonsoft.Json;\nThen during docker build,\nwarning MSB3245: Could not resolve this reference. Could not locate the assembly \"Newtonsoft.Json\". Check to make sure the assembly exists on disk.\nWhen I checked, found out that the Visual Studio added an Assembly Reference to Newtonsoft.Json (you can find this by expanding Dependencies node in the solution explorer in Visual Studio). And I was using Linux images.\nSo in order to solve this, I removed the Assembly Reference, and added nuget package Newtonsoft.Json, then the docker build was successful.",
    "How to speed up the COPY from one image to other in Dockerfile": "It is taking time because the number of files are large . If you can compress the data folder as tar and then copy and extract will be helpful in your situation.\nOtherwise If you can take this step to running containers it will be very fast. As per your requirement you need to copy an image of your application that is created already in another image. You can use volume sharing functionality that will share a volume in between 2 or more docker containers.\nCreate 1st container:\ndocker run -ti --name=Container -v datavolume:/datavolume ubuntu\n2nd container:\ndocker run -ti --name=Container2 --volumes-from Container ubuntu\nOr you can use -v option , so with v option create your 1st and 2nd container as:\ndocker run -v docker-volume:/data-volume --name centos-latest -it centos\n\ndocker run -v docker-volume:/data-volume --name centos-latest1 -it centos\nThis will create and share same volume folder that is data-volume in both the containers. docker-volume is the volume name and data-volume is folder name in that container that will be pointing to docker-volume volume Same way you can share a volume with more than 2 containers.",
    "Modify docker image in portainer": "Portainer do not allow you to edit an image from a Dockerfile as it does not store the Dockerfile.\nI'd recommend to version your Dockerfile in a CVS, this would allow to version any changes to your Dockerfile, and then update your image via the upload method inside Portainer when needed.",
    "No route to host within docker container": "It seems it's the container having connectivity issues so your proposed solution is likely to not work, as that is only mapping a host port to a container port (considering your target URL is not the actual host).\nCheck out https://docs.docker.com/compose/compose-file/#network_mode and try setting it to host.",
    "How to connect frontend to backend via docker-compose networks": "In fact your traffic is as next:\nUser browser request page from angular container, then all pages will rendered to user's browser.\nThe front javascript code using angular HttpClient to fetch the data from express container.\nAt that time, although docker-compose setup a customized network for you which afford auto-dns to resolve angular & express, but this dns just works among containers, not effect for host.\nBut, your augular HttpClient which call http://express was happened on user's browser which not in container, so the auto-dns defintly not work for you to resolve the name express.\nFor you, if you just want to open browser from your docker host, you can use localhost, but if you also want the user from other machine to visit your web, you had to use the ip of your dockerhost.\nThen, in angular HttpClient you need to use something like http://your_dockerhost_ip:8000 to visit express container.\nIf interested, you can visit this to see User-defined bridges provide automatic DNS resolution between containers.",
    "Nuget command fails in docker file but not when run manually": "I tried it and got it working like this:\nIf you change the content of your Dockerfile like this:\nFROM sixeyed/msbuild:netfx-4.5.2-webdeploy AS build-agent\nSHELL [\"powershell\"]\nRUN [\"nuget\", \"sources\" , \"Add\", \"-Name\", \"\\\"Common Source\\\"\", \"-Source\" ,\"http://CompanyPackages/nuget/common\"]\nAnd when you then run docker build -t yourImageName .\nYou end up with this:\nSending build context to Docker daemon  2.048kB\nStep 1/3 : FROM sixeyed/msbuild:netfx-4.5.2-webdeploy AS build-agent\n ---> 0851a5b495a3\nStep 2/3 : SHELL [\"powershell\"]\n ---> Running in 10082a1c45f8\nRemoving intermediate container 10082a1c45f8\n ---> c00b912c6a8f\nStep 3/3 : RUN [\"nuget\", \"sources\" , \"Add\", \"-Name\", \"\\\"Common Source\\\"\", \"-Source\" ,\"http://CompanyPackages/nuget/common\"]\n ---> Running in 797b6c055928\nPackage Source with Name: \"Common Source\" added successfully.\nRemoving intermediate container 797b6c055928\n ---> 592db3be9f8b\nSuccessfully built 592db3be9f8b\nSuccessfully tagged nuget-tester:latest",
    "Why use label in docker-compose.yml, can't environment do the job?": "Another reason to use labels in docker-compose is to flag your containers as part of this docker-compose suite of containers, as opposed to other purposes each docker image might get used for.\nHere's an example docker-compose.yml that shares labels across two services:\nx-common-labels: &common-labels\n  my.project.environment: \"my project\"\n  my.project.maintainer: \"me@example.com\"\n\nservices:\n  s1:\n    image: somebodyelse/someimage\n    labels:\n      <<: *common-labels\n    # ...\n  s2:\n    build:\n      context: .\n    image: my/s2\n    labels:\n    <<: *common-labels\n    # ...\nThen you can do things like this to just kill this project's containers.\ndocker rm -f $(docker container ls --format \"{{.ID}}\" --filter \"label=my.project.environment\")\nre: labels vs. environment variables\nLabels are only available to the docker and docker-compose commands on your host.\nEnvironment variables are also available at run-time inside the docker container.",
    "Persisting MySQL data in Docker": "The problem:\nWhat's going on here is that you create an empty database while the Docker image is being built:\nmysql_install_db --user=mysql --verbose=1 --basedir=/usr --datadir=/var/lib/mysql --rpm > /dev/null && \\\nBut when you create a container from this image, you mount a volume on the /var/lib/mysql folder. This hides your containers data, to expose your host's folder. Thus, you're seeing an empty folder.\nThe solution:\nIf you take a look at https://hub.docker.com/_/mysql/, you'll see that this problem is addressed by creating the database when the container actually starts, not when the image is created. This answers both your questions:\nIf you start your container with your mounted volume, then the database init will be executed afterwards, and your data will actually be written in your host's FS\nYou have to pass those information as env variables\nIn other words, init your DB with a script in an ENTRYPOINT, rather than directly in the image.\nWarnings:\nSome warnings, though. The way you did your image is not really recommended, because Docker's philosophy is \"one process per container\". The difficulty you have here is that you'll have to start multiple services on the same container (apache, Mysql, etc.), so you may have to do things on both in your entrypoint, which is confusing. Also, is one service fails, your container will still be up, but not working as expected.\nI then would suggest to split what you did in as 1 image per process, then either start them all with raw Docker, or user something like docker-compose.\nAlso, this will benefit you in the way that you'll be able to use already existing and highly configurable images, from the Docker Hub : https://hub.docker.com. Less job for you and less error prone.",
    "How to run command during Docker build which requires a tty?": "Short answer : You can't do it straightly because docker build or either buildx didn't implement [/dev/tty, /dev/console]. But there is a hacky solution where you can achieve what you need but I highly discourage using it since it break the concept of CI. That's why docker didn't implement it.\nHacky solution\nFROM ubuntu:14.04\nRUN echo yes | read  #tty requirement command\nAs mentioned in docker reference document the RUN consist of two stage, first is execution of command and the second is commit to the image as a new layer. So you can do the stages manually on your own where we will provide tty to first stage(execution) and then commit the result.\nCode:\n  cd\n  cat >> tty_wrapper.sh << EOF\n  echo yes | read  ## Your command which needs tty\n  rm /home/tty_wrapper.sh \n  EOF\n  docker run --interactive --tty --detach --privileged --name name1 ubuntu:14.04\n  docker cp tty_wrapper.sh name1:/home/\n  docker exec name1 bash -c \"cd /home && chmod +x tty_wrapper.sh && ./tty_wrapper.sh \"\n  docker commit name1 your:tag\nYour new image is ready. Here is a description about the code. At first we make a bash script which wrap our tty to it and then remove itself after fist execute. Then we run a container with provided tty option(you can remove privileged if you don't need). Next step we copy wrapped bash script inside container and do the execution & commit stage on our own.",
    "Reuse steps in docker build image. Cache?": "Source\nYou need to explicitly specify the image that you want to refer Docker for caching. Try this:\ndocker build --cache-from my-registry/my-docker-cache:latest -t new_img .",
    "Docker Compose build command using Cache and not picking up changed files while copying to docker": "You can force the build to ignore the cache by adding on the --no-cache option to the docker-compose build",
    "Getting reproducible docker layers on different hosts": "After some extra googling, I found a great post describing solution to this problem.\nStarting from 1.13, docker has --cache-from option that can be used to tell docker to look at another images for layers. Important thing - image should be explicitly pulled for it to work + you still need point what image to take. It could be latest or any other \"rolling\" image you have.\nGiven that, unfortunately there is no way to produce same layer in \"isolation\", but cache-from solves root problem - how to eventually reuse some layers during ci build.",
    "debconf: delaying package configuration, since apt-utils is not installed": "Using DEBIAN_FRONTEND=noninteractive apt-get -yq install {your-pkgs} instead of apt-get -yq install {your-pkgs} should resolve the problem.\nRefer https://stackoverflow.com/a/56569081/12782026 for details.",
    "Locked package.json files in Docker container using docker-compose": "I encountered the same issue and I solved this by mounting the folder where package.json is located instead of package.json itself.\nversion: \"3\"\n\nservices:\n  node:\n    build:\n      context: ./\n      dockerfile: ./docker/Dockerfile\n    volumes:\n      - .:/Example\nMounting package.json directly as a volume seems to lock the file.",
    "How to use quotes in Dockerfile CMD": "I have had a similar problem when using CMD in the exec form, aka JSON array.\nThe TL;DR is that there seems to be a fallback mechanism that introduces /bin/sh when the CMD items cannot be parsed correctly.\nI could not find the exact rules but I have examples:\nENTRYPOINT [\"reflex\"]\nCMD [\"-r\", \".go$\"]\n\n$ docker inspect --format '{{.Config.Cmd}}' inventories_service_dev_1 \n[-r .go$]\nENTRYPOINT [\"reflex\"]\nCMD [\"-r\", \"\\.go$\"]\n\n$ docker inspect --format '{{.Config.Cmd}}' inventories_service_dev_1 \n[/bin/sh -c [\"-r\", \"\\.go$\"]]\nNotice that \\ in above.",
    "Appending to base image's ENTRYPOINT": "You can not have multiple ENTRYPOINTs, but you could get this to work by putting both commands into a start-up.ps1 and running that as your ENTRYPOINT.\nADD start-up.ps1\n\nENTRYPOINT ['powershell', '.\\start-up.ps1']",
    "Assembly specified in the dependencies manifest was not found while running docker with dotnet-core project": "To fix this error I have to changed my WORKDIR. It was reported in GitHub\nWORKDIR /app",
    "Docker History Base Image Add:sha256hash": "The docker brew debian image is made of intermediate containers, as described in \"Understand images, containers, and storage drivers\".\nSee issue 25925: each layer being stored in (for instance) /var/lib/docker/aufs/mnt/.\nSo ADD file:89ecb642d662ee7edbb868340551106d51336c7e589fdaca4111725ec64da95 would add all files found in /var/lib/docker/aufs/mnt/89ecb642d662ee7edbb868340551106d51336c7e589fdaca4111725ec64da95.\n(Note: I mentioned the (nop) part in \"Docker missing layer IDs in output\")",
    "Activate conda env and run pip install requirements in the Dockfile": "It has to be in the same line. To install in dev1 using pip:\nRUN . /code/miniconda/etc/profile.d/conda.sh && conda activate dev1\\\n    && pip install -r requirements.txt\nTo install in dev1 using conda itself for packages that are in coda channels\nRUN conda install -n dev1 packagname",
    "Assigning a command output to ARG variable in a Dockerfile": "Although not a direct answer to the question, the way I solved this problem was by creating a bash script that is responsible for starting the Dockerfile build. The script runs the command I need and then passes it as an argument to the Dockerfile. Example:\n#!/usr/bin/env bash\n\nMY_VAR=$(echo 'hello')\ndocker build --build-arg MY_VAR=${MY_VAR} -t myapp .\nAnd then the Dockerfile gets it:\nARG MY_VAR\n# MY_VAR now equals 'hello'",
    "Error when opening a VSCode remote-container from a project using a Dockerfile and devcontainer.json": "This solved the error for me:\nsudo snap install docker",
    "arm64 docker image for apple silicon M1 macbook air": "This github is the source for pre-built arm64 (Apple Silicon) docker images with PyTorch. That said, you'll still need to install fastai inside this image.\nI am currently working on a new version of my \"native\" fastai image (which uses the above image as a base image) that will not require sonisa's base image and will include Ruby (so you can run a Ruby on Rails server that can access pickle files created by fastai). However, that's still a work in progress. Watch this repo as I hope to publish the new docker file in the coming days.",
    "Install Bash on scratch Docker image": "When you start a Docker image FROM scratch you get absolutely nothing. Usually the way you work with one of these is by building a static binary on your host (or these days in an earlier Dockerfile build stage) and then COPY it into the image.\nFROM scratch\nCOPY mybinary /\nENTRYPOINT [\"/mybinary\"]\nNothing would stop you from creating a derived image and COPYing additional binaries into it. Either you'd have to specifically build a static binary or install a full dynamic library environment.\nIf you're doing this to try to debug the container, there is probably nothing else in the image. One thing this means is that the set of things you can do with a shell is pretty boring. The other is that you're not going to have the standard tool set you're used to (there is not an ls or a cp). If you can live without bash's various extensions, BusyBox is a small tool designed to be statically built and installed in limited environments that provides minimal versions of most of these standard tools.",
    "Docker: Insert certificate into ketstore": "Just like someone already stated in the comment - if you want to use the crt file that gets mounted at deployment, you have to add the keytool command to the deployment.\nThe crt you are trying to access when building the container does not exist yet.",
    "Why SHA256 digest of Docker image changes if we push the same image with same tag multiple times to the same docker repository": "Since my comment answered your question, original credit goes to the post here: https://windsock.io/explaining-docker-image-ids/\nLayers are identified by a digest in this form: algorithm:hex which looks like sha256:abcd.....\nThe hex is calculated by applying the algorithm (sha256) to the layers content. If the content changes, then the digest changes.",
    "When creating pod it go into CrashLoopBackoff. Logs show \"exec /usr/local/bin/docker-entrypoint.sh: exec format error.\"": "As mentioned in the comment above by David Maze, the issue is due to building the image on Mac M1 Pro.\nTo fix this I need to add FROM --platform=linux/amd64 <image>-<version> in the Dockerfile and build or you can run the below command while running the build\ndocker build --platform=linux/amd64 <image>-<version>\nBoth solutions will work. I added FROM --platform=linux/amd64 to the Dockerfile and it's fixed now.",
    "Docker - Couldn't create the mpm-accept mutex": "Thanks to @Bets, the problem was solved with adding the following into Dockerfile:\nRUN echo \"Mutex posixsem\" >> /etc/apache2/apache2.conf",
    "Database Permission denied after running docker-compose up": "You can try to edit the db service in docker-compose.yml with the following changes:\nvolumes:\n  # - ./init-db:/docker-entrypoint-initdb.d\n  - ./data-db:/var/lib/mysql:rw # <- permissions\nuser: mysql # <- user, should be 'mysql'",
    "Cant create image/ the Dockerfile (Dockerfile) cannot be empty": "I saw that you, don't save your dockerfile, try save changes and run docker build again.\nAlso saw same issue on github, it could be helpful\nTry to change COPY to ADD\nhttps://github.com/docker/compose/issues/5170",
    "Docker container with neo4j and password set": "You can override the login-password in the docker file:\nFROM neo4j:3.2\nENV NEO4J_AUTH=neo4j/newPassword",
    "docker run entrypoint with multiple commands": "As a style point, this gets vastly easier if your image has a CMD that can be overridden. If you only need to run one command with no initial setup, make it be the CMD and not the ENTRYPOINT:\nCMD ./some_command  # not ENTRYPOINT\nIf you need to do some initial setup and then launch the main command, make the ENTRYPOINT be a shell script that ends with the special instruction exec \"$@\". The CMD will be passed into it as parameters, and this line replaces the shell script with that command.\n#!/bin/sh\n# entrypoint.sh\n... do first time setup, run database migrations, set variables ...\nexec \"$@\"\n# Dockerfile\n...\nENTRYPOINT [\"./entrypoint.sh\"] # MUST be JSON-array syntax\nCMD ./some_command             # as before\nIf you do these things, then you can use your initial docker run form. This will replace the CMD but leave the ENTRYPOINT intact. In the wrapper-script case, your alternate command will be run as the exec \"$@\" command, so all of the first-time setup will be done first.\n# Assuming the image correctly honors the CMD\ndocker run ... \\\n  image-name \\\n  sh -c 'echo \"foo is $FOO\" && echo \"bar is $BAR\"'\nIf you really can't do this, you can override the docker run --entrypoint. This runs instead of the image's entrypoint (if you want the image's entrypoint you have to run it yourself), and the syntax is awkward:\n# Run a shell command instead of the entrypoint\ndocker run ... \\\n  --entrypoint /bin/sh \\\n  image-name \\\n  -c 'echo \"foo is $FOO\" && echo \"bar is $BAR\"'\nNote that the --entrypoint option comes before the image name, and its arguments come after the image name.",
    "Create docker image without source image (OS)": "You said \"we want to separate base image(centos or rhel) and application images and link them during run time.\" That is essentially what FROM rhel does, thanks to the layered file systems used by Docker.\nThat is, the FROM image does not become part of your image -- it remains in a separate layer. Your new image points to that rhel (or other FROM'd base layer) and then adds itself on top of it at runtime.\nSo go ahead and use FROM -- you'll get the behavior you wanted.\nFor those that find this question looking for a way to build their own base image (so you don't have to use anything as a base), you can use FROM scratch and you should read about Creating a base image.\nAnd, to be completely pedantic, the reason why Docker needs a FROM and a base including a Linux distribution's root file system is that without a root file system, there is nothing. You can't even RUN mkdir /opt/cassandra because mkdir is a program provided by the root file system.",
    "How to edit standalone.xml file dynamically in keycloak": "First, it seems in a docker container by default standalone-ha.xml is used. You can find this in /opt/jboss/tools/docker-entrypoint.sh. Second, I think after changing configuration file you'll have to restart keycloak server (container).\nNot sure what do you mean by \"dynamically\". But it will be easier to modify the file locally and build a custom docker image. Dockerfile may look like:\nFROM jboss/keycloak:6.0.1\nADD <path on your system>/standalone-ha.xml /opt/jboss/keycloak/standalone/configuration/standalone-ha.xml",
    "\"Docker is not supported on this Mac\"": "I had this error as well, and then I realized that I downloaded the wrong installation file. On the Docker site, there are two options - 'Mac with Intel Chip' or 'Mac with Apple Chip'. I have an Intel Chip and found that I accidentally downloaded the Apple Chip version. The Intel Chip version worked fine for me.",
    "Is `FROM` clause required in Dockerfile?": "Based on the official documentation it's required:\nThe FROM instruction initializes a new build stage and sets the Base Image for subsequent instructions. As such, a valid Dockerfile MUST start with a FROM instruction. The image can be any valid image \u2013 it is especially easy to start by pulling an image from the Public Repositories.\nhttps://docs.docker.com/engine/reference/builder/#from",
    "SQL Server in Docker CREATE INDEX failed because the following SET options have incorrect settings: \u2018QUOTED_IDENTIFIER\u2019": "The SQLCMD utility unfortunately defaults to QUOTED_IDENTIFIER OFF for backwards compatibility reasons. Specify the -I argument so that QUOTED_IDENTIFIER ON is used.\n/opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P $SA_PASSWORD -i $file -I",
    "How to keep Docker container volume files?": "When you add a VOLUME at docker run, what you are saying is to use the main host filesystem instead of the copy-on-write filesystem that Docker uses for images. There are two main options here:\nYou bind-mount an actual filesystem location into the image. This is what you are doing here.\nYou let Docker handle the location... in this case, Docker creates the location on the main filesystem, and then copies the contents of the image into that location to get things started.\nYou are looking to get both- you want a fixed location on your filesystem, but you want files from your image to be there. Now, there is a reason it doesn't work this way! What happens if 'auto.conf' already exists in that folder and you launch your container? What happens if you run two containers with different versions of that file pointed at the same location? That is why if you pick a real location, it does not attempt to guess what to do with conflicts between the image and the filesystem, it just goes with the filesystem.\nYou CAN achieve what you want though. There are really two options. The better one would be to have your app read from two seperate folders- one that is populated inside the image, and one that is on your filesystem. That completely avoids this problem ;) The second option is to go in and tell Docker how to handle individual files in your image.\nversion: '2'\nservices:\n  app:\n    container_name: mono\n    build: .\n    volumes:\n      # save .composer files on host to keep cache warmed up\n      - '/srv/mono/mydir:/root/mydir'\n      # Marking a volume this way will tell Docker to use THIS file \n      # from the image, even though the parent directory is a regular\n      # volume.  If you have an auto.cnf file in your directory, it\n      # will be ignored. \n      - /root/mydir/auto.cnf\n    command: sleep infinity\n......",
    "Error when installing google-chrome-stable on python image in Dockerfile": "As per the instructions here you need to add the chrome repo. So changing your dockerfile to something like the following works:\nFROM python:3.8\nWORKDIR /test\nRUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add -\nRUN echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google.list\nRUN apt-get update && apt-get install -y google-chrome-stable",
    "Docker isn't caching Alpine apk add command": "Reorder your Dockerfile and it should work.\nFROM golang:1.13.5-alpine\nRUN apk add --update docker\n\nWORKDIR /go/src/app\nCOPY src .\nRUN go get -d -v ./...\nRUN go install -v ./...\nCMD [\"app\"]\nAs you are copying before installation, so whenever you change something in src the cache will invalidate for docker installtion.",
    "Docker RUN does NOT persist files": "The reason those changes are not persisted, is that they are inside a volume the Jenkins Dockerfile marks /var/jenkins_home/ as a VOLUME.\nInformation inside volumes is not persisted during docker build, or more precisely; each build-step creates a new volume based on the image's content, discarding the volume that was used in the previous build step.\nHow to resolve this?\nI think the best way to resolve this, is to;\nAdd the files you want to modify inside jenkins_home in a different location inside the image, e.g. /var/jenkins_home_overrides/\nCreate a custom entrypoint based on, or \"wrapping\", the default entrypoint script that copies the content of your jenkins_home_overrides to jenkins_home the first time the container is started.\nActually...\nAnd just when I wrote that up; It looks like the official Jenkins image already support this out of the box; https://github.com/jenkinsci/docker/blob/683b0d6ed17016ee3211f247304ef2f265102c2b/jenkins.sh#L5-L23\nAccording to the documentation, you need to add your files to the /usr/share/jenkins/ref/ directory, and those will be copied to /var/jenkins/home upon start.\nAlso see https://issues.jenkins-ci.org/browse/JENKINS-24986",
    "Docker ARG variables not working (always empty)": "ARG steps are scoped. Before the first FROM step, the ARG only applies to FROM steps. And within each FROM step, it only applies to the lines after that ARG step until the next FROM (in a multistage build).\nTo fix this, reorder your steps:\nFROM node:current AS build-node\nARG APP_NAME='ground-station'\nWORKDIR /${APP_NAME}\nRUN echo \"APP_NAME=${APP_NAME}\"",
    "EACCES: permission denied mkdir ... while trying to use docker volumes in a node project": "So, what I gathered from looking around and seeing other people's Dockerfiles and a bit of running commands in a bash terminal in my container, is that the node:alpine image I used as base, creates a user named node inside the container.\nThe npm run start is executed as a process of node user(process with the UID of node) which does not have root privileges (which I found out while looking in the htop), so in order to make node user the owner of /app directory I added this in the docker file-\nRUN chown -R node.node /app\nThe updated Dockerfile.dev is-\nFROM node:alpine\n\nWORKDIR '/app'\n\nCOPY package.json .\nRUN npm install\nRUN chown -R node.node /app\n\nCOPY . .\n\nCMD [\"npm\", \"run\", \"start\"]\nThis has fixed the problem for me, hope it can help someone else too. :)",
    "Dockerfile: copy zip and open it": "Found a way to do it using .tar file insead of .zip and using \"ADD\" instead of \"COPY\":\nThe DOCKERFILE. now looks like this:\nFROM mcr.microsoft.com/windows/servercore:ltsc2019\nADD test3.tar c:/\nThe ADD command extracts the archive.",
    "Dockerfile CMD not accepting variables for substitution": "When you write the arguments to CMD (or ENTRYPOINT) as a JSON string, as in...\nCMD [\"/opt/jdk/bin/java\", \"-jar\", \"ssltools-domain-LATEST.jar\"]\n...the command is executed directly with the exec system call and is not processed by a shell. That means no i/o redirection, no wildcard processing...and no variable expansion. You can fix this in a number of ways:\nYou can just write it as a plain string instead, as in:\n  CMD /opt/jdk/bin/java -jar ${ARTIFACTID}-${VERSION}.${PACKAGING}\nWhen the argument is not a JSON construct, it gets passed to sh -c.\nYou can do as suggested by Philip, and pass the arguments to sh -c yourself:\n  CMD [\"sh\", \"-c\", \"/opt/jdk/bin/java -jar ${ARTIFACTID}-${VERSION}.${PACKAGING}\"]\nThose two options are basically equivalent.\nA third option is to put everything into a shell script, and then run that:\n  COPY start.sh /start.sh\n  CMD [\"sh\", \"/start.sh\"]\nThis is especially useful if you need to perform more than a simple command line.",
    "Is there any cache advantage to using ADD <url> vs RUN wget/curl <url> in a Dockerfile": "Certainly.\nThe RUN instruction will not invalidate the cache unless its text changes. So if the remote file is updated, you won't get it. Docker will use the cached layer.\nThe ADD instruction will always download the file and the cache will be invalidated if the checksum of the file no longer matches.\nI would recommend using ADD instead of RUN wget ... or RUN curl .... I imagine people use the latter as its more familiar, but the ADD instruction is quite powerful. It can untar files and set ownership. It's also considered best practice to avoid downloading any packages that are not necessary for your process to run (though there are multiple ways to accomplish this, like using multi-stage builds).\nDocs on cache invalidation:\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/#leverage-build-cache",
    "Docker healthcheck in composer file": "I believe this is similar to Docker Compose wait for container X before starting Y\nYour db_image needs to support curl.\nTo do that, create your own db_image as:\nFROM base_image:latest\nRUN apt-get update\nRUN apt-get install -y curl \nEXPOSE 3306\nThen all you should need is a docker-compose.yml that looks like this:\nversion: '2'\nservices:\n  db:\n    image: db_image\n    restart: always\n    dns:\n      - 10.\n    ports:\n      - \"${MYSQL_EXTERNAL_PORT}:${MYSQL_INTERNAL_PORT}\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:${MYSQL_INTERNAL_PORT}\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n    environment:\n      TZ: Europe/Berlin\n  main_application:\n    image: application_image\n    restart: always\n    depends_on:\n      db:\n        condition: service_healthy\n    links: \n        - db\n    dns:\n      - 10.\n    ports:\n      - \"${..._EXTERNAL_PORT}:${..._INTERNAL_PORT}\"\n    environment:\n      TZ: Europe/Berlin\n    volumes:\n      - ${HOST_BACKUP_DIR}:/...\n    volumes_from:\n      - data\n      - db",
    "Dockerfile `RUN --mount=type=ssh` doesn't seem to work": "The error has nothing to do with your private key; it is \"host key verification failed\". That means that ssh doesn't recognize the key being presented by the remote host. It's default behavior is to ask if it should trust the hostkey, and when run in an environment when it can't prompt interactively, it will simply reject the key.\nYou have a few options to deal with this. In the following examples, I'll be cloning a GitHub private repository (so I'm interacting with github.com), but the process is the same for any other host to which you're connecting with ssh.\nInject a global known_hosts file when you build the image.\nFirst, get the hostkey for the hosts to which you'll be connecting and save it alongside your Dockerfile:\n$ ssh-keyscan github.com > known_hosts\nConfigure your Dockerfile to install this where ssh will find it:\nCOPY known_hosts /etc/ssh/ssh_known_hosts\nRUN chmod 600 /etc/ssh/ssh_known_hosts; \\\n  chown root:root /etc/ssh/ssh_known_hosts\nConfigure ssh to trust unknown host keys:\nRUN sed /^StrictHostKeyChecking/d /etc/ssh/ssh_config; \\\n  echo StrictHostKeyChecking no >> /etc/ssh/ssh_config\nRun ssh-keyscan in your Dockerfile when building the image:\nRUN ssh-keyscan github.com > /etc/ssh/ssh_known_hosts\nAll three of these solutions will ensure that ssh trusts the remote host key. The first option is the most secure (the known hosts file will only be updated by you explicitly when you run ssh-keyscan locally). The last option is probably the most convenient.",
    "Run a command line when starting a docker container": "Docker execute RUN command when you build the image.\nDocker execute ENTRYPOINT command when you start the container. CMD goes as arguments to ENTRYPOINT. Both of these can be overridden when you create a container from an image. Their purpose in Dockerfile is to provide defaults for future when you or someone else will be creating containers from this image.\nConsider the example:\nFROM debian:buster\n\nRUN apt update && apt install procps\n\nENTRYPOINT [\"/usr/bin/ps\"]\nCMD [\"aux\"]\nThe RUN command adds ps command to the image, ENTRYPOINT and CMD are not executed but they will be when you run the container:\n# create a container named 'ps' using default CMD and ENTRYPOINT\ndocker run --name ps my_image\n# equivalent to /usr/bin/ps aux\n\n# start the existing container 'ps'\ndocker start ps\n# equivalent to /usr/bin/ps aux\n\n# override CMD\ndocker run my_image au\n# equivalent to /usr/bin/ps au\n\n# override both CMD and ENTRYPOINT\ndocker run --entrypoint=/bin/bash my_image -c 'echo \"Hello, world!\"'\n# will print Hello, world! instead of using ps aux\n\n# no ENTRYPOINT, only CMD\ndocker run --entrypoint=\"\" my_image /bin/bash -c 'echo \"Hello, world!\"'\n# the output is the same as above\nEach time you use docker run you create a container. The used ENTRYPOINT and CMD are saved as container properties and executed each time you start the container.",
    "How to apt-get install firefox on an openjdk:11 docker base image without \"no installation candidate\" error?": "The firefox package is only available under the Debian Unstable Repository (codename \"Sid\"). Debian Stable only has firefox-esr. To include the Sid repository in the package index update, you must add deb http://deb.debian.org/debian/ unstable main contrib non-free as a repository source for apt.\necho \"deb http://deb.debian.org/debian/ unstable main contrib non-free\" >> /etc/apt/sources.list.d/debian.list\napt-get update\napt-get install -y --no-install-recommends firefox\nIf the Sid repository does not have an up-to-date version of Firefox, the next best place to check are the Firefox PPAs (Personal Package Archive) operated by the Mozilla team themselves. PPAs are just repositories, and are added the exact same way as the Sid repository above:\nFirefox PPA\nFirefox Next PPA\nFor example,\nsudo add-apt-repository ppa:mozillateam/firefox-next\nsudo apt-get update",
    "Permission denied docker-entrypoint.sh": "I know I'm a little late but seconding @ARK, you need to give execute permissions to the entrypoint.sh. But use the following command after COPY entrypoint-base.sh /sbin/docker-entrypoint.sh (note the lowercase chmod and a RUN command) -\nRUN chmod +x /sbin/docker-entrypoint.sh",
    "Can we replace dockerfile by docker-compose.yml?": "1) Here are the official docs between COPY and ADD:\nAlthough ADD and COPY are functionally similar, generally speaking, COPY is preferred. That\u2019s because it\u2019s more transparent than ADD. COPY only supports the basic copying of local files into the container, while ADD has some features (like local-only tar extraction and remote URL support) that are not immediately obvious. Consequently, the best use for ADD is local tar file auto-extraction into the image, as in ADD rootfs.tar.xz /.\nYou can't do this in a docker-compose.yml, you'd have to use a Dockerfile if you want to add files to the image.\n2) docker-compose.yml has both an entrypoint and command override. Similar to how you'd pass a CMD at runtime with docker, you can do with a docker-compose run\ndocker run -itd mystuff/myimage:latest bash -c 'npm install && node server.js'\nYou can do this with docker-compose, assuming the service name here is myservice:\ndocker-compose run myservice bash -c 'npm install && node server.js'\nIf your use case is only running one container, it's probably hard to see the benefits of docker-compose as a beginner. If you were to add a mongodb an nginx container to your dev stack you'd start to see where docker-compose really picks up and gives you benefits. It works best when used to orchestrate several containers that need to run and communicate with each other.",
    "Jekyll site running inside docker container but localhost:4000 is not working on host": "Try to bind against 0.0.0.0. Then your site is reachable from an external ip.\nCMD [\"bundle\", \"exec\", \"jekyll\", \"serve\", \"--livereload\", \"--host\", \"0.0.0.0\"]",
    "Access logs from docker container": "You can always check direct docker output of your service with:\ndocker logs my-container-instance\nTo check the log path file you can use:\ndocker inspect my-cintainer-instance\nand find for the LogPath key on the json output.",
    "Dockerfile: Copied file not found": "TL;DR\nDon't rely on shell expansions like ~ in COPY instructions. Use COPY conf/phantomjs.sh /path/to/user/home/phantomjs.sh instead!\nDetailed explanation\nUsing ~ as shortcut for the user's home directory is a feature offered by your shell (i.e. Bash or ZSH). COPY instructions in a Dockerfile are not run in a shell; they simply take file paths as an argument (also see the manual).\nThis issue can easily be reproduced with a minimal Dockerfile:\nFROM alpine\nCOPY test ~/test\nThen build and run:\n$ echo foo > test\n$ docker built -t test\n$ docker run --rm -it test /bin/sh\nWhen running ls -l / inside the container, you'll see that docker build did not expand ~ to /root/, but actually created the directory named ~ with the file test in it:\n/ # ls -l /~/test.txt \n-rw-r--r--    1 root     root             7 Jan 16 12:26 /~/test.txt",
    "error invalid windows mount type: 'bind' on Windows 10 docker build of Windows container": "In docker settings, make sure to have buildkit set to false as below:\n  \"features\": {\n    \"buildkit\": false\n  }\nAfter this is applied and successfully restarted, the build should proceed.\nIn the docs, it's currently mentioned BuildKit is a feature \"only supported for building Linux containers\" : https://docs.docker.com/develop/develop-images/build_enhancements/\nUnfortunately in a fresh install it's value defaults to true and doesn't change to false when switching to Windows Containers.",
    "How do I prevent root access to my docker container": "As David mentions, once someone has access to the docker socket (either via API or with the docker CLI), that typically means they have root access to your host. It's trivial to use that access to run a privileged container with host namespaces and volume mounts that let the attacker do just about anything.\nWhen you need to initialize a container with steps that run as root, I do recommend gosu over something like su since su was not designed for containers and will leave a process running as the root pid. Make sure that you exec the call to gosu and that will eliminate anything running as root. However, the user you start the container as is the same as the user used for docker exec, and since you need to start as root, your exec will run as root unless you override it with a -u flag.\nThere are additional steps you can take to lock down docker in general:\nUse user namespaces. These are defined on the entire daemon, require that you destroy all containers, and pull images again, since the uid mapping affects the storage of image layers. The user namespace offsets the uid's used by docker so that root inside the container is not root on the host, while inside the container you can still bind to low numbered ports and run administrative activities.\nConsider authz plugins. Open policy agent and Twistlock are two that I know of, though I don't know if either would allow you to restrict the user of a docker exec command. They likely require that you give users a certificate to connect to docker rather than giving them direct access to the docker socket since the socket doesn't have any user details included in API requests it receives.\nConsider rootless docker. This is still experimental, but since docker is not running as root, it has no access back to the host to perform root activities, mitigating many of the issues seen when containers are run as root.",
    "Docker : java.net.ConnectException: Connection refused - Application running at port 8083 is not able to access other application on port 3000": "# The problem\nYou run with:\ndocker run -p 8083:8083 myrest-app\nBut you need to run like:\ndocker run --network \"host\" --name \"app\" myrest-app\nSo passing the flag --network with value host will allow you container to access your computer network.\nPlease ignore my first approach, instead use a better alternative that does not expose the container to the entire host network... is possible to make it work, but is not a best practice.\nA Better Alternative\nCreate a network to be used by both containers:\ndocker network create external-api\nThen run both containers with the flag --network external-api.\ndocker run --network \"external-api\" --name \"app\" -p 8083:8083 myrest-app\nand\ndocker run -d --network \"external-api\" --name \"api\" -p 3000:3000 dockerExternalId/external-rest-api\nThe use of flag -p to publish the ports for the api container are only necessary if you want to access it from your computers browser, otherwise just leave them out, because they aren't needed for 2 containers to communicate in the external-api network.\nTIP: docker pull is not necessary, once docker run will try to pull the image if does not found it in your computer. Let me know how it went...\nCall the External API\nSo in both solutions I have added the --name flag so that we can reach the other container in the network.\nSo to reach the external api from my rest app you need to use the url http://api:3000/externalrestapi/testresource.\nNotice how I have replaced localhost by api that matches the value for --name flag in the docker run command for your external api.",
    "How can I use Chinese in Alpine headless chrome?": "Install wqy-zenhei package at testing channel will fix it.\necho @edge http://nl.alpinelinux.org/alpine/edge/testing >> /etc/apk/repositories && apk add wqy-zenhei@edge",
    "Unable to locate file in docker container": "You're copying your local /app/ folder to the /app/ folder in the running Docker container (as mentioned in the comments) creating /app/app/server.py in the Docker container.\nHow to resolve\nA simple fix will be to change\nCOPY . /app\nto\nCOPY ./app/server.py /app/server.py\nExplanation\nThe command COPY works as follows:\nCOPY <LOCAL_FROM> <DOCKER_TO>\nYou're selecting everything in the folder where the Dockerfile resides, by using . in your first COPY, thereby selecting the local /app folder to be added to the Docker's folder. The destination you're allocating for it in the Docker container is also /app and thus the path in the running container becomes /app/app/.. explaining why you can't find the file.\nHave a look at the Docker docs.",
    "\"localhost didn't send data. ERR_EMPTY_RESPONSE\" and \"curl: (52) Empty reply from server\"": "CMD [\"ng\", \"serve\", \"--host=0.0.0.0\"]\nWhen an HTTP client like a web browser (or in this case, curl) sends an HTTP request, the server needs the client\u2019s IP address, so it knows where to send the response. When you run a local development server, it inspects that IP address, and won\u2019t even respond to requests that aren\u2019t coming from 127.0.0.1 (localhost). That keeps other folks on your network (say, sitting in the same Starbucks) from using the dev server as a way to hack your box.\nWhen you run a dev server in a guest container or VM, and run the client (curl) on the host machine, the server sees a different IP address, and refuses to respond. Most servers let you specify a mask of what IP addresses are valid clients. 0.0.0.0 means fugetaboutit, accept all client IPs. It gives the bouncer the night off.",
    "keytool error: java.io.FileNotFoundException (Permission denied) while calling from docker file": "The default user in docker is root. I believe it has been set to a user other than root by your organisation for security purposes. You need to change to user root and then change back to whatever user had been set by your organisation.\nENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64\n\nCOPY app-module/src/main/resources/certificates/A.crt /etc/ssl/certs/\nCOPY app-module/src/main/resources/certificates/B.crt /etc/ssl/certs/\n\n#change to user root to install certificates\nUSER root\nRUN $JAVA_HOME/bin/keytool -import -keystore $JAVA_HOME/jre/lib/security/cacerts -storepass changeit -noprompt -file /etc/ssl/certs/A.crt -alias A\nRUN $JAVA_HOME/bin/keytool -import -keystore $JAVA_HOME/jre/lib/security/cacerts -storepass changeit -noprompt -file /etc/ssl/certs/B.crt -alias B\n\n#change to user oldUser to comply with organisation standards\nUSER oldUser",
    "Error trying to create a scheduled task within Windows 2016 Core container": "The issue has to do with the Container user. By default a scheduled task is created with the current user. It's possible the container user is a special one that the Scheduled Task command cannot parse into XML.\nSo you have to pass the user /ru (and if needed the password /rp) to the schtasks command in a Windows Container.\nThis works\nFROM microsoft/windowsservercore\nRUN schtasks /create /tn \"hellotest\" /sc daily /tr \"echo hello\" /ru SYSTEM\nIt will run the command under the system account.\nIf you are a fan of Powershell (like me), you can use this\nFROM microsoft/windowsservercore\n\nSHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';\"]\n\nRUN $action = New-ScheduledTaskAction -Execute 'echo \"\"Hello World\"\"'; \\\n    $trigger = New-ScheduledTaskTrigger -Daily -At '1AM'; \\\n    Register-ScheduledTask -TaskName 'Testman' -User 'SYSTEM' -Action $action -Trigger $trigger -Description 'Container Scheduled task test';",
    "Merge two docker images": "You can't just merge the images. You have to recreate your own based on what was in each of the images you want. You can download both images and re-create the Docker files for each like this:\ndocker history --no-trunc=true image1 > image1-dockerfile\ndocker history --no-trunc=true image2 > image2-dockerfile\nSubstitute the image1 and image2 with the images you want to see the history for. After this you can use those dockerfiles to build your own image that is the combination of the two.\nThe fly in the ointment here is that any ADD or COPY commands will not reveal what was copied because you don't have access to the local file system from which the original images were created. With any luck that won't be necessary or you can get any missing bits from the images themselves.",
    "Can I use Docker buildkit to provide an ssh key to a non-root user?": "It turns out, buildkit's --mount syntax takes a uid= parameter.\nSo I was able to discover the uid for my non-root user like so:\n$ docker run -it --rm apache/airflow bash -c 'id -u $(whoami)'\n    50000\nAnd then adjust my Dockerfile to mount the socket to the ssh agent with that uid as the owner:\nFROM docker.io/apache/airflow\nUSER root\nRUN apt update && apt install -y git openssh-client && rm -rf /var/lib/apt/lists/*\nUSER airflow\nRUN mkdir -m 700 ~/.ssh\nRUN ssh-keyscan github.com > ~/.ssh/known_hosts\nRUN --mount=type=ssh,uid=50000 ssh -vvvT git@github.com\nFollowing this, I was able to authenticate over ssh with the externally-supplied key, as a non-root user.",
    "Docker Compose: Port mapping by selecting a random port within given port range": "There's no way to do this in Linux in general. You can ask the OS to use a specific port or you can ask the OS to pick a port for you, but you can't constrain the automatically-chosen port.\nIn a Compose context you can omit the host port to let the OS choose the port\nports:\n  - \"8001\" # container port only\nand you will need to use docker-compose port to find out what the port number actually is. That will be any free port number, and there is no way to limit it to a specific range.",
    "Environment variables inside Dockerfile": "You need to declare them as an argument and not as an environment variable, since you are passing an argument to the docker build command as a --build-arg argument value.\nThe Dockerfile should be something like this:\nFROM mongo:4.4.9\n\nARG MONGO_UID\nARG MONGO_GID\n\n# setup folder before switching to user\nRUN mkdir /var/lib/mongo\nRUN usermod -u ${MONGO_UID} mongodb\nRUN groupmod -g ${MONGO_GID} mongodb\nRUN chown mongodb:mongodb /var/lib/mongo\nRUN chown mongodb:mongodb /var/log/mongodb\n\nUSER mongodb\nIf you need to have them as an environment variable inside the container, then you would need to say that the ENV is equal to the ARG:\nARG MONGO_UID\nENV MONGO_UID=${MONGO_UID}\nIt's also possible to give the argument (same to ENV) a default value, in case you are interested:\nARG MONGO_UID=default\nNot having a default value will force the user to supply a value to the argument.",
    "Docker running script in entrypoint": "The problem is with your interpreter: sh Try exec form: ENTRYPOINT [\"/bin/bash\", \"-c\", \"./docker-entrypoint.sh\"]",
    "'docker build: how does version pinning invalidate the build cache?": "All docker does is look at the string you run, along with the environment you pass in, and compare it to other images in the build cache. If you have a Dockerfile that doesn't pin a package, like:\nRUN apt-get update && apt-get install -y \\\n    package-foo\nand you first run this when version 1.2 is the current latest, and then run the same command again after 1.3 gets released, docker will see the command is identical, and reuse the build cache from the previous build of the image, rather than pulling the newer version of that package.\nIf instead you specify a version like:\nRUN apt-get update && apt-get install -y \\\n    package-foo=1.2.*\nAnd then rebuild with an updated Dockerfile containing a different version pinned:\nRUN apt-get update && apt-get install -y \\\n    package-foo=1.3.*\nThe second build will be a different command to run, and therefore force docker to rerun the build without the cache from the previous run. This also has the advantage that a 1.4 release doesn't get pulled in unexpectedly (e.g. if an earlier line breaks the cache or the cache gets removed/disabled).\nIf you just want the most recent versions of these packages, regardless of the cache, then you can skip version pinning and either intentionally break the cache or disable the cache. To disable the cache, you can build with the flag --no-cache. To intentionally break the cache, you can pass a changing build arg with something like:\ndocker build --build-arg \"TIMESTAMP=$(date +%s)\" .\nAnd a Dockerfile that defines that build arg before you want to break the cache:\nARG TIMESTAMP\nRUN apt-get update && apt-get install -y \\\n    package-foo",
    "How to install libwebp in alpine Linux Docker image": "Maybe I didn't understand you right way, but you can install libwebp and libwebp-tools packages as other packages described in your question.\nThe final Dockerfile is:\nFROM openjdk:8-jdk-alpine\nRUN apk update && \\\n    apk upgrade -U && \\\n    apk add ca-certificates ffmpeg libwebp libwebp-tools && \\\n    rm -rf /var/cache/*\nNow you can find dwebp binary file by the following path:\n/ # which dwebp\n/usr/bin/dwebp\nEDIT:\nIf you want to install another libwebp version on alpine platform you need to add package repository from the previous alpine versions and define version of package you need to install.\nFor your particular case there are the following versions of libwebp package in alpine package repositories:\n0.4.4-r0 - alpine v3.3\n0.5.0-r0 - alpine v3.4\n0.5.2-r0 - alpine v3.5\n0.6.0-r0 - alpine v3.6\n0.6.0-r1 - alpine v3.7\nFor example you want to install libwebp version 0.4.4-r0. The Dockerfile is:\nFROM openjdk:8-jdk-alpine\nRUN apk update && \\\n    apk upgrade -U && \\\n    apk add ca-certificates ffmpeg && rm -rf /var/cache/*\n\nRUN echo \"http://dl-cdn.alpinelinux.org/alpine/v3.3/main\" >> /etc/apk/repositories\n\nRUN apk add --no-cache libwebp=0.4.4-r0 libwebp-tools=0.4.4-r0",
    "Start two servers in one docker container": "You can use supervisord when you want to run multiple processes in one container - like in your case npm and python server.\nCheck documentation for supervisord for more information.\nI just pick some important parts.\n1, You will need to install supervisord for docker image\nSomething like:\nCentOS: `yum install supervisor`\n\nUbuntu: `apt-get install -y supervisor`\n2, copy configuration for supervisord (supervisord.conf) to docker image .\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\nIn your case supervisord.conf should looks like example below (not tested) :\n[supervisord]\nnodaemon=true\n\n[program:npm]\ncommand=npm start\n\n[program:python]\ncommand=python server.py\nand run supervisord as CMD command in Dockerfile:\n...\n# install supervisord\n# copy supervisord configuration\n...\n# run supervisord\nCMD [\"/usr/bin/supervisord\"]",
    "pass parameters to docker entrypoint": "You can append your dynamic parameters at the end of the docker run .... You haven't specified any CMD instruction, so it'll work.\nWhat is actually run without specifying any command at the end, when running the docker run ..., is this:\nENTRYPOINT CMD (it's concatenated and there is a space in between)\nSo you can also use something like\n...\nENTRYPOINT [\"java\", \"-jar\", \"my_app.jar\"]\nCMD [\"--spring.config.location=classpath:/srv/app/configs/application.properties\"]\nwhich means, when using\ndocker run mycontainer the\njava -jar my_app.jar --spring.config.location=classpath:/srv/app/configs/application.properties\nwill be invoked (the default case), but when running\ndocker run mycontainer --spring.config.location=classpath:/srv/app/configs/some_other_application.properties -Dversion=$version\nit'll be run w/ different property file and with the system property called version (overriding the default case)",
    "Docker config : Celery + RabbitMQ": "I have similar Celery exiting problem while dockerizing the application. You should use rabbit service name ( in your case it's rabbitmq) as host name in your celery configuration.That is,\nuse broker_url = 'amqp://guest:guest@rabbitmq:5672//' instead of broker_url = 'amqp://guest:guest@localhost:5672//' .\n\nIn my case, major components are Flask, Celery and Redis.My problem is HERE please check the link, you may find it useful.",
    "Docker Default Operating System": "You always should have a FROM instruction for a Dockerfile as per documentation as mentioned by Munir. However, you can choose variety of base images, which does not have to be an OS for your Dockerfile. For example, if you are creating a docker image for your java application, you can use java image as your base images.\nFROM library/java\nHowever, at the end, if you traverse thorough those image's Dockerfile, you will end up in one or the other OS. Java is based on Debian.",
    "Dockerfile: COPY folder inside folder": "You have to define your foldername inside the directory\nCOPY folder /usr/share/nginx/html/folder\nor\nADD folder /usr/share/nginx/html/folder",
    "Build container with docker-compose, but run /etc/bash with -it option later?": "Looks like docker-compose run will become your friend. From the docker compose docs:\nRuns a one-time command against a service. For example, the following command starts the web service and runs bash as its command.\n$ docker-compose run web bash\nThe command docker-compose run <service-name> also ensures that all required containers (e.g. those providing volumes and links) will be started prior to the container <service-name>, if they are not already running.\nIn case you use any port-mappings, you might consider using docker-compose run --service-ports <service-name>. Without that option docker-compose will only map ports when using docker-compose up.\nLike with docker run you can also use the --rm option to remove containers once you are done with them.\nIf your image uses ENTRYPOINT, you should consider overriding this in your docker-compose.yml with entrypoint: /bin/bash.",
    "How to install Node.js version 16.x.x in a Debian based image (Dockerfile)? (why so hard?)": "You can use:\nFROM python:buster\n\nRUN apt-get update && \\\n apt-get install -y \\\n    nodejs npm\n\nRUN curl -fsSL https://deb.nodesource.com/setup_current.x | bash - && \\\n apt-get install -y nodejs",
    "Dockerfile FROM Insecure Registry": "Depending on your version, you may need to include the scheme in the insecure registry definition. Newer versions of buildkit should not have this issue, so an upgrade may also help.\n   ...\n   \"insecure-registries\" : [\n     \"insecure.registry.local\",\n     \"http://insecure.registry.local\"\n   ]\n   ...",
    "docker run interactive with conda environment already activated": "Each RUN statement (including docker run) is executed in a new shell, so one cannot simply activate an environment in a RUN command and expect it to continue being active in subsequent RUN commands.\nInstead, you need to activate the environment as part of the shell initialization. The SHELL command has already been changed to include --login, which is great. Now you simply need to add conda activate my_env to .profile or .bashrc:\n...\n# Create and activate the environment.\nRUN conda env create --force -f environment.yml\nRUN echo \"conda activate my_env\" >> ~/.profile\nand just be sure this is after the section added by Conda.",
    "Docker build stops on software-properties-common when run with `apt update`": "Add the following line in your Dockerfile and build again.\nRUN DEBIAN_FRONTEND=noninteractive apt-get install -y tzdata",
    "How to use a docker file with terraform": "Terraform is a provisioning tool rather than a build tool, so building artifacts like Docker images from source is not really within its scope.\nMuch as how the common and recommended way to deal with EC2 images (AMIs) is to have some other tool build them and Terraform simply to use them, the same principle applies to Docker images: the common and recommended path is to have some other system build your Docker images -- a CI system, for example -- and to publish the results somewhere that Terraform's Docker provider will be able to find them at provisioning time.\nThe primary reason for this separation is that it separates the concerns of building a new artifact and provisioning infrastructure using artifacts. This is useful in a number of ways, for example:\nIf you're changing something about your infrastructure that doesn't require a new image then you can just re-use the image you already built.\nIf there's a problem with your Dockerfile that produces a broken new image, you can easily roll back to the previous image (as long as it's still in the registry) without having to rebuild it.\nIt can be tempting to try to orchestrate an entire build/provision/deploy pipeline with Terraform alone, but Terraform is not designed for that and so it will often be frustrating to do so. Instead, I'd recommend treating Terraform as just one component in your pipeline, and use it in conjunction with other tools that are better suited to the problem of build automation.\nIf avoiding running a separate registry is your goal, I believe that can be accomplished by skipping using docker_image altogether and just using docker_container with an image argument referring to an image that is already available to the Docker daemon indicated in the provider configuration.\ndocker_image retrieves a remote image into the daemon's local image cache, but docker build writes its result directly into the local image cache of the daemon used for the build process, so as long as both Terraform and docker build are interacting with the same daemon, Terraform's Docker provider should be able to find and use the cached image without interacting with a registry at all.\nFor example, you could build an automation pipeline that runs docker build first, obtains the raw id (hash) of the image that was built, and then runs terraform apply -var=\"docker_image=$DOCKER_IMAGE\" against a suitable Terraform configuration that can then immediately use that image.\nHaving such a tight coupling between the artifact build process and the provisioning process does defeat slightly the advantages of the separation, but the capability is there if you need it.",
    "Run sqlcmd in Dockerfile": "The MS SQL containers can have SQL statements run against them inside a dockerfile that uses them.\nI think your problem is just that double quotes are being stripped from your RUN command.\nI couldn't quite decide if its a bug based on this github issue but escaping them as \\\" will work around it.\nYou can also avoid setting the SA password in your docker file by defaulting to a trusted connection:\nrun sqlcmd -Q \\\"select name from master.dbo.sysdatabases\\\"\nThis way the variables can be set at container run time like this:\ndocker run -e ACCEPT_EULA=Y -e SA_PASSWORD=bobleponge -p 1433:1433 <imageid>",
    "Using variables across multi-stage docker build": "To send variable we can use \"ARG\" solution, the \"base\" solution, and \"file\" solution.\nARG version_default=v1\n\nFROM alpine:latest as base1\nARG version_default\nENV version=$version_default\nRUN echo ${version}\nRUN echo ${version_default}\n\nFROM alpine:latest as base2\nARG version_default\nRUN echo ${version_default}\nanother way is to use base container for multiple stages:\nFROM alpine:latest as base\nARG version_default\nENV version=$version_default\n\nFROM base\nRUN echo ${version}\n\nFROM base\nRUN echo ${version}\nYou can find more details here: https://github.com/moby/moby/issues/37345\nAlso you could save the hash into a file in the first stage, and copy the file in the second stage and then read it and use it there.\nFrom what I understand you want to copy the built program into the new docker for multistage build that the output size is smaller. Basically you do not need to send a variable you need to know were you built it in the first image and copy it from there\nFROM golang:alpine as gobuilder\nRUN apk update && apk add git\n\nCOPY sources/src/ $GOPATH/src/folder/\nWORKDIR $GOPATH/src/folder/\n#get dependencies\nRUN go get -d -v\n#build the binary\nRUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -installsuffix cgo -ldflags=\"-w -s\" -o /go/bin/myGoProgram myGoSource.go\n\nFROM alpine:latest\nCOPY --from=gobuilder /go/bin/myGoProgram /usr/local/bin/myGoProgram\nENTRYPOINT [\"myGoProgram\"] # or ENTRYPOINT [\"/usr/local/bin/myGoProgram\"]",
    "how can I configure the default admin user:password with docker if i am launching docker-compose up of a docker(portainer in my case)?": "Portainer allows you to specify an encrypted password from the command line for the admin account. You need to generate the hash value for password.\nFor example, this is the hash value of password - $$2y$$05$$arC5e4UbRPxfR68jaFnAAe1aL7C1U03pqfyQh49/9lB9lqFxLfBqS\nIn your docker-compose file make following modification\nversion: '3.3'\n services:\n   portainer:\n    image: portainer/portainer\n    volumes:\n     - /var/run/docker.sock:/var/run/docker.sock\n     - ./portainer/portainer_data:/data\n    command: --admin-password \"$$2y$$05$$arC5e4UbRPxfR68jaFnAAe1aL7C1U03pqfyQh49/9lB9lqFxLfBqS\"\n    ports:\n     - \"9000:9000\"\n--admin-password This flag is used to specify an encrypted password in Portainer.\nMore information can be found in documentation - Portainer\nHope this will help you.",
    "How can I add a customized kong plugin into dockerized kong": "I tried to do same thing but could not found well-describe answer yet. You can configure simple helloworld plugin as below: (https://github.com/brndmg/kong-plugin-hello-world)\nLocal 'plugin' directory structure on Docker host:\nThen you can mount local /plugins directory and let kong load custom 'helloworld' plugin from /plugins directory\n1) using environment variables\n$ docker run -d --name kong --network=kong-net \\\n-e \"KONG_DATABASE=cassandra\" \\\n-e \"KONG_PG_HOST=kong-database\" \\ \n-e \"KONG_CASSANDRA_CONTACT_POINTS=kong-database\" \\\n**-e \"KONG_LUA_PACKAGE_PATH=/plugins/?.lua\" \\\n-e \"KONG_CUSTOM_PLUGINS=helloworld\" \\ ** \n...\n-e \"KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl\" \\\n**-v \"/plugins:/plugins\" \\**\n-p 8080:8000 -p 8443:8443 -p 8001:8001 -p 8444:8444 kong:latest\nThen, you can see configured custom plugin on http://[kong-url]:8001/\n..\n\"custom_plugins\": [\n\"helloworld\"\n],\n..\n2) Or, you can simple mount your custom kong.conf file which describes plugins you want.\n/etc/kong/kong.conf\nplugins = bundled,helloworld,jwt-crafter\n(It seems that second option is better for latest version of Kong because 'kong_custom_plugin' configuration prints 'deprecation' warning)\nFor the JWT crafter, https://github.com/foodora/kong-plugin-jwt-crafter it seems that the plugin is not maintained well so that installation using luarocks failed with errors.\n$ luarocks install kong-plugin-jwt-crafter\n....\nkong-plugin-jwt-crafter 1.0-0 depends on lua-resty-jwt ~> 0.1.10-1 (not installed)\n\nError: Could not satisfy dependency lua-resty-jwt ~> 0.1.10-1: No results matching query were found.\nInstead, you can directly to add 'resty-jwt' to official docker image, to resolve dependency, which is not included in official image. and copy \"JWT crafter\" into \"/plugins\" directory, and load.\n(Inside docker container)\n luarocks install lua-resty-jwt\nHope this helps.",
    "Why can't I connect to my local docker-compose container on Windows 10?": "Just change:\nexpose:\n  - \"8000\"\nBy\nports:\n  - \"8000:8000\"\nBtw http://localhost:80 is not working?\nRegards",
    "Is it possible to export a file while inside a docker image?": "Run your container as this:\ndocker run -v $(PWD)/local-dir/:/path/to/results/dir (...rest of the command..)\nSo any file that is created inside the container into /path/to/results/dir gets automatically reflected in your host, inside ./local-dir.\nAlternatively, you can copy any file from container to host:\ndocker cp <container-id>:/path/to/file ./local-dir",
    "Docker for windows - Internal server error": "I finally found the problem. I was using url rewrite 2.0 functionality, but forgot to install it in the docker image.\nAdding the following to my docker file solved the problem:\n# Install Url Rewrite\nADD https://download.microsoft.com/download/C/9/E/C9E8180D-4E51-40A6-A9BF-776990D8BCA9/rewrite_amd64.msi /install/rewrite_amd64.msi\nRUN msiexec.exe /i c:\\install\\rewrite_amd64.msi /passive",
    "How can I combine ENV statement in Dockerfile?": "Example from the Dockerfile reference\nhttps://docs.docker.com/engine/reference/builder/#env\nENV myName=\"John Doe\" myDog=Rex\\ The\\ Dog \\\n    myCat=fluffy",
    "How am I supposed to use a Postgresql docker image/container?": "Short and simple:\nWhat you get from the official postgres image is a ready-to-go postgres installation along with some gimmicks which can be configured through environment variables. With docker run you create a container. The container lifecycle commands are docker start/stop/restart/rm Yes, this is the Docker way of things.\nEverything inside a volume is persisted. Every container can have an arbitrary number of volumes. Volumes are directories either defined inside the Dockerfile, the parent Dockerfile or via the command docker run ... -v /yourdirectoryA -v /yourdirectoryB .... Everything outside volumes is lost with docker rm. Everything including volumes is lost with docker rm -v\nIt's easier to show than to explain. See this readme with Docker commands on Github, read how I use the official PostgreSQL image for Jira and also add NGINX to the mix: Jira with Docker PostgreSQL. Also a data container is a cheap trick to being able to remove, rebuild and renew the container without having to move the persisted data.\nCongratulations, you have managed to grasp the basics! Keep it on! Try docker-compose to better manage those nasty docker run ...-commands and being able to manage multi-containers and data-containers.\nNote: You need a blocking thread in order to keep a container running! Either this command must be explicitly set inside the Dockerfile, see CMD, or given at the end of the docker run -d ... /usr/bin/myexamplecommand command. If your command is NON blocking, e.g. /bin/bash, then the container will always stop immediately after executing the command.",
    "Installing maven in a docker build overrides JAVA 8 with JAVA 7(!)": "I found a minimal-delta solution although the point about not using apt-get for maven installs is noted. Here is the solution as the code\nFROM java:8\n\n# preserve Java 8  from the maven install.\nRUN mv /etc/alternatives/java /etc/alternatives/java8\nRUN apt-get update -y && apt-get install maven -y\n\n# Restore Java 8\nRUN mv -f /etc/alternatives/java8 /etc/alternatives/java\nRUN ls -l /usr/bin/java && java -version\nObviously, the last line is unnecessary but does confirm that the result is java 8.",
    "What are some strategies to invalidate the Dockerfile instruction cache while Downloading resources": "Solution\nDocker will NOT look at any caching mechanism before downloading using \"RUN curl\" nor ADD. It will repeat the step of downloading. However, Docker invalidates the cache if the mtime of the file has been changed https://stackoverflow.com/a/26612694/433814, among other things. https://github.com/docker/docker/blob/master/pkg/tarsum/versioning.go#L84\nHere's a strategy that I've been working on to solve this problem when building Dockerfiles with dependencies from File storage or repository such as Nexus, Amazon S3 is to retrieve the ETag from the resource, caching it, and modifying the mdtime of a cache-flag file. (https://gist.github.com/marcellodesales/721694c905dc1a2524bc#file-s3update-py-L18). It follows the approach performed in Python (https://stackoverflow.com/a/25307587), Node.js (http://bitjudo.com/blog/2014/03/13/building-efficient-dockerfiles-node-dot-js/) projects.\nHere's what we can do:\nGet the ETag of the resource and save it outside of Dockerfile\nUse an ADD instruction to add the cacheable file prior to download\nDocker will check the mtime metadata of the file to whether invalidate the cache or not.\nUse a RUN instruction as usual to download the content\nIf the previous instruction was invalidated, Docker will re-download the file. If not, the cache will be used.\nHere's a setup to demo this strategy:\nExample\nCreate a Web Server that handles HEAD requests and return an ETag header, usually returned by servers.\nThis simulates the Nexus or S3 storage of files.\nBuild an image and verify that the dependent layer will download the resource for the first time\nCaching the current value of the ETag\nRebuild the image and verify that the dependent layer will use the Cached value.\nChanging the ETag value returned by Web Server handler to simulate a change.\nIn addition, persist the change IFF the file has changed. In this cause yes...\nRebuild the image and verify that the dependent layer will be invalidated, triggering a download.\nRebuild the image again and verify that the cache was used.\n1. Node.js server\nSuppose you have the following Node.js server serving files. Let's implement a HEAD operation and return a value.\n// You'll see the client-side's output on the console when you run it.\n\nvar restify = require('restify');\n\n// Server\nvar server = restify.createServer({\n  name: 'myapp',\n  version: '1.0.0'\n});\n\nserver.head(\"/\", function (req, res, next) {\n  res.writeHead(200, {'Content-Type': 'application/json; charset=utf-8',\n        'ETag': '\"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8555}}\"'});\n  res.end();\n  return next();\n});\n\nserver.get(\"/\", function (req, res, next) {\n  res.writeHead(200, {'Content-Type': 'application/json; charset=utf-8',\n        'ETag': '\"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8555}}\"'});\n  res.write(\"The file to be downloaded\");\n  res.end();\n  return next();\n});\n\nserver.listen(80, function () {\n  console.log('%s listening at %s', server.name, server.url);\n});\n\n// Client\nvar client = restify.createJsonClient({\n  url: 'http://localhost:80',\n  version: '~1.0'\n});\n\nclient.head('/', function (err, req, res, obj) {\n  if(err) console.log(\"An error ocurred:\", err);\n  else console.log('HEAD    /   returned headers: %j', res.headers);\n});\nExecuting this will give you:\nmdesales@ubuntu [11/27/201411:10:49] ~/dev/icode/fuego/interview (feature/supportLogAuditor *) $ node testserver.js \nmyapp listening at http://0.0.0.0:8181\nHEAD    /   returned headers: {\"content-type\":\"application/json; charset=utf-8\",\n            \"etag\":\"\\\"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8555}}\\\"\",\n            \"date\":\"Thu, 27 Nov 2014 19:10:50 GMT\",\"connection\":\"keep-alive\"}\n2. Build an image based on ETag value\nConsider the following build script that caches the ETag Header in a file.\n#!/bin/sh\n\n# Delete the existing first, and get the headers of the server to a file \"headers.txt\"\n# Grep the ETag to a \"new-docker.etag\" file\n# If the file exists, verify if the ETag has changed and/or move/modify the mtime of the file\n# Proceed with the \"docker build\" as usual\nrm -f new-docker.etag\ncurl -I -D headers.txt http://192.168.248.133:8181/ && \\\n  grep -o 'ETag[^*]*' headers.txt > new-docker.etag && \\\n  rm -f headers.txt\n\nif [ ! -f docker.etag ]; then\n  cp new-docker.etag docker.etag\nelse\n  new=$(cat docker.etag)\n  old=$(cat new-docker.etag)\n  echo \"Old ETag = $old\"\n  echo \"New ETag = $new\"\n  if [ \"$old\" != \"$new\" ]; then\n    mv new-docker.etag docker.etag\n    touch -t 200001010000.00 docker.etag\n  fi\nfi\n\ndocker build -t platform.registry.docker.corp.intuit.net/container/mule:3.4.1 .\n3. Rebuilding and using cache\nBuilding this would result as follows, considering I'm using the current cache.\nmdesales@ubuntu [11/27/201411:54:08] ~/dev/github-intuit/docker-images/platform/mule-3.4 (master) $ ./build.sh \nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8555}}\"\nDate: Thu, 27 Nov 2014 19:54:16 GMT\nConnection: keep-alive\n\nOld ETag = ETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8555}}\"\nNew ETag = ETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8555}}\"\nSending build context to Docker daemon 51.71 kB\nSending build context to Docker daemon \nStep 0 : FROM core.registry.docker.corp.intuit.net/runtime/java:7\n ---> 3eb1591273f5\nStep 1 : MAINTAINER Marcello_deSales@intuit.com\n ---> Using cache\n ---> 9bb8fff83697\nStep 2 : WORKDIR /opt\n ---> Using cache\n ---> 3e3c96d96fc9\nStep 3 : ADD docker.etag /tmp/docker.etag\n ---> Using cache\n ---> db3f82289475\nStep 4 : RUN cat /tmp/docker.etag\n ---> Using cache\n ---> 0d4147a5f5ee\nStep 5 : RUN curl -o docker https://get.docker.com/builds/Linux/x86_64/docker-latest\n ---> Using cache\n ---> 6bd6e75be322\nSuccessfully built 6bd6e75be322\n4. Simulating the ETag change\nChanging the value of the ETag on the server and restarting the server to simulate the new update will result in updating the cache-flag file and invalidation of the Cache. For instance, the Etag was changed to \"465fb0d9b9f143ad691c7c3bcf3801b47284f8333\". Rebuilding will trigger a new download because the ETag file was updated, and Docker will verify that during the \"ADD\" instruction. Here, step #5 will run again.\nmdesales@ubuntu [11/27/201411:54:16] ~/dev/github-intuit/docker-images/platform/mule-3.4 (master) $ ./build.sh \nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8333}}\"\nDate: Thu, 27 Nov 2014 19:54:45 GMT\nConnection: keep-alive\n\nOld ETag = ETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8333}}\"\nNew ETag = ETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8555}}\"\nSending build context to Docker daemon 50.69 kB\nSending build context to Docker daemon \nStep 0 : FROM core.registry.docker.corp.intuit.net/runtime/java:7\n ---> 3eb1591273f5\nStep 1 : MAINTAINER Marcello_deSales@intuit.com\n ---> Using cache\n ---> 9bb8fff83697\nStep 2 : WORKDIR /opt\n ---> Using cache\n ---> 3e3c96d96fc9\nStep 3 : ADD docker.etag /tmp/docker.etag\n ---> ac3b200c8cdc\nRemoving intermediate container 4cf0040dbc43\nStep 4 : RUN cat /tmp/docker.etag\n ---> Running in 4dd38d30549a\nETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8333}}\"\n ---> 4fafbeac2180\nRemoving intermediate container 4dd38d30549a\nStep 5 : RUN curl -o docker https://get.docker.com/builds/Linux/x86_64/docker-latest\n ---> Running in de920c7a2e28\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 13.5M  100 13.5M    0     0  1361k      0  0:00:10  0:00:10 --:--:-- 2283k\n ---> 95aff324da85\nRemoving intermediate container de920c7a2e28\nSuccessfully built 95aff324da85\n5. Reusing the Cache again\nConsidering that the ETag hasn't changed, the cache-flag file will continue being the same and Docker will do a super fast build using the cache.\nmdesales@ubuntu [11/27/201411:54:56] ~/dev/github-intuit/docker-images/platform/mule-3.4 (master) $ ./build.sh \nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8333}}\"\nDate: Thu, 27 Nov 2014 19:54:58 GMT\nConnection: keep-alive\n\nOld ETag = ETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8333}}\"\nNew ETag = ETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8333}}\"\nSending build context to Docker daemon 51.71 kB\nSending build context to Docker daemon \nStep 0 : FROM core.registry.docker.corp.intuit.net/runtime/java:7\n ---> 3eb1591273f5\nStep 1 : MAINTAINER Marcello_deSales@intuit.com\n ---> Using cache\n ---> 9bb8fff83697\nStep 2 : WORKDIR /opt\n ---> Using cache\n ---> 3e3c96d96fc9\nStep 3 : ADD docker.etag /tmp/docker.etag\n ---> Using cache\n ---> ac3b200c8cdc\nStep 4 : RUN cat /tmp/docker.etag\n ---> Using cache\n ---> 4fafbeac2180\nStep 5 : RUN curl -o docker https://get.docker.com/builds/Linux/x86_64/docker-latest\n ---> Using cache\n ---> 95aff324da85\nSuccessfully built 95aff324da85\nThis strategy has been used to build Node.js, Java and other App servers or pre-built dependencies.",
    "How to organize multiple Dockerfiles, docker-compose.yaml and .dockerignore": "When you build an image, you send the Docker daemon a build context; in your Compose setup this is the directory named in the build: { context: } setting. The .dockerignore file must be in that exact directory and nowhere else. Its actual effect is to cause files to be excluded from the build context, which can result in a faster build sequence.\nThe build context's other important effect is that all Dockerfile COPY directives are considered relative to that directory; you cannot COPY from parent or sibling directories. So if files are shared between projects, you must set the context directory to some ancestor directory of all of the files that will be included, and COPY directives will be relative to that directory (even if the Dockerfiles are in per-project directories). See also How to include files outside of Docker's build context?\nIf your projects are completely separate: maybe there's a front-end and a back-end project, or in your case a producer and a consumer that share a message format but not any actual code. Then in this case:\nPut a Dockerfile, named exactly Dockerfile, in each project subdirectory\nPut a .dockerignore file in each project subdirectory (it cannot be in the parent directory)\nCOPY directives are relative to the project subdirectory\nCOPY requirements.txt ./\nIn the Compose file, you can use the shorthand build: directory syntax, since you have the standard (default) dockerfile: name\nversion: '3.8'\nservices:\n   producer:\n     build: ./python_producer\n     environment:\n       - RABBITMQ_HOST=rabbitmq\n   consumer:\n     build: ./python_consumer\n     environment:\n       - RABBITMQ_HOST=rabbitmq\n   rabbitmq:\n     image: rabbitmq:3\n     hostname: rabbitmq # RabbitMQ is very unusual in needing to set this\nIf your projects share code or other files: in your example maybe you define Python data structures for the message format in shared code. This in this case:\nPut a Dockerfile, named exactly Dockerfile, in each project subdirectory\nPut a single .dockerignore file in the project root\nCOPY directives are relative to the project root directory\nCOPY python_producer/requirements.txt ./\nIn the Compose file you need to specify context: . and dockerfile: pointing at a per-component Dockerfile\nversion: '3.8'\nservices:\n   producer:\n     build:\n       context: .\n       dockerfile: python_producer/Dockerfile\n     environment:\n       - RABBITMQ_HOST=rabbitmq\n   consumer:\n     build:\n       context: .\n       dockerfile: python_consumer/Dockerfile\n     environment:\n       - RABBITMQ_HOST=rabbitmq\n   rabbitmq:\n     image: rabbitmq:3\n     hostname: rabbitmq # RabbitMQ is very unusual in needing to set this",
    "Multiple docker-compose file with different context path": "Option 1: Use absolute paths for the contexts in both docker-compose files\nOption 2: Create a docker-compose.override.yml with the absolute paths:\nversion: \"3\"\n\nservices:\n  service1:\n    build:\n      context: /home/project1\n  service2:\n    build:\n      context: /home/project2\nand include it in the docker-compose command:\ndocker-compose -f /home/project1/docker-compose.yml -f /home/project2/docker-compose.yml -f /home/docker-compose.override.yml config\nOn linux, to avoid hard-coding of the base path in the docker-compose.override.yml, you can use PWD environment variable:\nservices:\n  service1:\n    build:\n      context: ${PWD}/project1\n  service2:\n    build:\n      context: ${PWD}/project2",
    "How do I have multiple docker images for one project in one directory?": "There are two straightforward answers in pure-Docker space.\nThe first thing you can do is package all of your code into a single image:\nFROM python:3\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"./main.py\"]\nYou can override the command part at the docker run command or in a docker-compose.yml file, so run multiple containers off the same image.\nversion: '3'\nservices:\n  crawler:\n    build: .\n    command: ./crawl.py\n  indexer:\n    build: .\n    command: ./indexer.py\nOr, you can have multiple Dockerfiles in the same directory\nversion: '3'\nservices:\n  crawler:\n    build:\n      context: .\n      dockerfile: Dockerfile.crawler\n  indexer:\n    build:\n      context: .\n      dockerfile: Dockerfile.indexer\nand those Dockerfiles could be more limited\nFROM python:3\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY common common\nCOPY crawler crawler\nCOPY crawler.py .\nCMD [\"./crawler.py\"]\nWhich approach to use depends on your code base. If it's in an interpreted language, almost all of the code is shared, and the only real difference is the command you're running, then it's probably better to just have one image. But if the library dependencies are significantly different for each dependency, or if you need to build a separate binary for each process, a separate image for each makes more sense.",
    "Docker: \"not found\" for an existing file": "If app is a shell script, then the error refers to the first line of that script, e.g. #!/bin/bash if /bin/bash doesn't exist locally. Can also be windows linefeeds breaking that line.\nIf app is an executable, then the error is from linked libraries. Use ldd app to see what that binary is linked against. With alpine, the issue is often glibc.",
    "Docker-compose port range forward": "I agree that the docker-compose ports documentation does not provide sufficient info on the syntax for the range mapping of ports. To understand the syntax, check the docker run documentation on ports.\nIn particular,\n- \"20000-20100\" means: Expose the container ports in the range 20000 to 20100 into random ports on the host machine\n- \"10000-10100:20000-20100\" means: Expose the container ports in the range 20000 to 20100 into random ports on the host machine in the range of 10000 to 10100\n- \"20000-20100:20000-20100\" similar to the above\nIn your case, all these should allow you to access the containerized application",
    "Console.log not working in a dockerized Node.js / Express app": "Remove the -d from the command you're using to run the container:\ndocker run -p 49160:8080\nThe -d option runs the container in the background, so you won't be able to see its output in your console.\nIf you want to keep the container running in the background and you want to access that container's shell, you can run the following command once your container is up and running:\ndocker exec -it <container-name> bash",
    "dotnet docker /bin/sh: 1: [dotnet,: not found": "I faced similar issue. In my case the problem was in the entry point statement in Dockerfile. I was missing a comma between two parameters \"dotnet\" and \"url for dll\".\nBefore:\nCMD [\"dotnet\" \"url to dll\"]\nAfter fix applied:\nCMD [\"dotnet\", \"url for dll\"]",
    "Why I cannot get output of tail of a file created by docker build": "The same issue occurs for me using the overlay2 storage driver on the Docker 4.9.8-moby Alpine release.\nIt seems like CMD tail is opening the /var/log/mylog.log file from the overlay layer that RUN touch /var/log/mylog.log creates.\nWhen you append to the log, a \"new\" file is created in the topmost overlay layer that the container uses for any file system changes made on top of the image while running, and this new file is actually being appended to. tail is not able to pick up the changeover correctly though, with either -f or -F.\nThe docker start and docker stop resolves the problem as the tail process starts again after /var/log/mylog.log has been updated and is then pointing at the \"new\" file in the container overlay layer. Using a slightly different CMD would workaround the issue in a similar way:\nCMD [\"sh\", \"-c\", \"touch /var/log/mylog.log && tail -f /var/log/mylog.log\"]\nThe debian:testing image includes coreutils-8.26-2 with the fix for supporting overlays magic number to remove that warning message, but still exhibits the same behaviour.\nIt's most likely an overlay issue to be fixed in the kernel. coreutils might be able to work around the issue when using -F.\nWhat you are attempting is a bit of an edge case in Docker though. Containers that use tail as the foreground process usually complete a bunch of work in a script before running tail, which includes running the commands that create the log file to be tailed. Might be why not many people have picked this up.",
    "Configure Dockerfile to use impdp command when the container is created": "Ok, I have now figured how to make it happen, after much of experimenting,reading how the cmd works (finally) and the help/inputs provided by the above comments from other users.\nBasically Docker runs only one CMD (from the docs). So if I create a dockerfile from wnameless/oracle-xe-11g as\nFrom wnameless/oracle-xe-11g\n...\n...\nCMD [\"impdp\", \"....\"]\nthen this will inherently override the CMD command described by the wnameless/oracle-xe-11g's docker file.\nSo here are the steps to be done to achieve it\nStep 1: copy the CMD's executed from the parent image (from the Dockerfile)\nIn my case that would be\n/usr/sbin/startup.sh\nStep 2: append your own CMD to the above CMD using && operation.\nhere it would be\nbin/bash  -c \"/u01/app/oracle/product/11.2.0/xe/bin/impdp system/oracle NOLOGFILE=Y\nNote that you need to include the entire path of the impdp and the whole operation inside blockquotes\nStep 3: If the parent Dockerfile contains a background running process make sure that it goes in the last\nHere it would be\n/usr/sbin/sshd -D\nThe final output should be something like this\nCMD /usr/sbin/startup.sh \n&& bin/bash  -c \"/u01/app/oracle/product/11.2.0/xe/bin/impdp\nsystem/oracle NOLOGFILE=Y ...\" \n&& /usr/sbin/sshd -D\nThat's it. This should work\nOther things to keep in mind especially when using the above oracle dockerfile is you need to set the ENV for oracle_home and also export it to the bash.bashrc as this is not done by default.\n# Add env variables for oracle_home and related\nENV ORACLE_HOME=/u01/app/oracle/product/11.2.0/xe \\\nORACLE_SID=XE\n\n#Export oracle_home and related\nRUN echo 'export ORACLE_HOME=/u01/app/oracle/product/11.2.0/xe' >> etc/bash.bashrc\nRUN echo 'export PATH=$ORACLE_HOME/bin:$PATH' >> /etc/bash.bashrc\nRUN echo 'export ORACLE_SID=XE' >> /etc/bash.bashrc",
    "How to serve static files from a Dockerized Python web app?": "If you know your app will always-and-forever have the same static assets, then just containerize them with the app and be done with it.\nBut things change, so when you need it I would recommend a Docker Volume Container approach: put your static assets in a DVC and mount that DVC in the main container so it's all pretty much \"just one app container\". You could use Docker Compose something like this:\nappdata:\n    image: busybox\n    volumes:\n        - /path/to/app/static\n    command: echo \"I'm just a volume container\"\napp:\n    build: .\n    volumes_from:\n        - appdata\n    command: \u2026\nYou can expand further by starting your container with a bootstrap script that copies initial static files into the destination path on startup. That way your app is guaranteed to always have a default set to get started, but you can add more static files as the app grows. For an example of this, pull the official Jenkins container and read /usr/local/bin/jenkins.sh.",
    "How can I use a docker image saved as tar file in my Dockerfile as parent image": "If you have a docker save tarball, you need to docker load it before it can be used. When you do, that will print out the name(s) and tag(s) of the image(s) that were loaded. You can then use that in the FROM line of your Dockerfile, like any other local image.\n$ docker load -i myimage.tar.gz\n\nLoaded image: my/image:and-its-tag\n$ head -1 Dockerfile\nFROM my/image:and-its-tag\nIf you docker push or docker save the resulting image, it will have a complete copy of the original image.\n(In normal operation you shouldn't need docker save; prefer a registry service like Docker Hub, something cloud-hosted like GCR/ACR/ECR, or running your own. You can't really directly use the saved image tarfile for anything.)",
    "I can't install specific version (1.0.2g) of openssl in docker": "What base image do you use to build an image?\nIt works pretty fine with ubuntu:16.04 base image and the same Dockerfile you provided:\nFROM ubuntu:16.04\nRUN apt-get update\nRUN apt-get install -y build-essential cmake zlib1g-dev libcppunit-dev git subversion wget && rm -rf /var/lib/apt/lists/*\n\nRUN wget https://www.openssl.org/source/openssl-1.0.2g.tar.gz -O - | tar -xz\nWORKDIR /openssl-1.0.2g\nRUN ./config --prefix=/usr/local/openssl --openssldir=/usr/local/openssl && make && make install",
    "Running simple Java Gradle app in Docker": "The fix was to specify --chown=gradle permissions on the /code directory in the Dockerfile. Many Docker images are designed to run as root, the base Gradle image runs as user gradle.\nFROM gradle:4.3-jdk-alpine\nADD --chown=gradle . /code\nWORKDIR /code\nCMD [\"gradle\", \"--stacktrace\", \"run\"]\nEthan Davis suggested using /home/gradle rather than code. That would probably work as well, but I didn't think of that.\nThe docker image maintainer should have a simple getting started type reference example that shows the recommended way to get basic usage.",
    "Secure Admin must be enabled to access the DAS remotely - Acess Glassfish Admin Console with Docker": "There are a couple of ways to do this, but the best way is probably to copy the method used in the Payara Server dockerfile. (Payara Server is derived from GlassFish and therefore the dockerfile is compatible with GlassFish too)\nTo summarise, this method creates 2 files: a tmpfile which contains the default (empty) password and the desired new password, and a pwdfile which contains just the newly changed file.\nIf the contents of the tmpfile are:\nAS_ADMIN_PASSWORD=\nAS_ADMIN_NEWPASSWORD=MyNewPassword\nThen the contents of pwdfile should be:\nAS_ADMIN_PASSWORD=MyNewPassword\nto change the password using asadmin, the first file must be used with the change-admin-password command, and the second with all future commands.\nIn docker terms, this looks like this (taken directly from the dockerfile linked above):\nENV PAYARA_PATH /opt/payara41\nENV ADMIN_USER admin\nENV ADMIN_PASSWORD admin\n\n# set credentials to admin/admin \n\nRUN echo 'AS_ADMIN_PASSWORD=\\n\\\nAS_ADMIN_NEWPASSWORD='$ADMIN_PASSWORD'\\n\\\nEOF\\n'\\\n>> /opt/tmpfile\n\nRUN echo 'AS_ADMIN_PASSWORD='$ADMIN_PASSWORD'\\n\\\nEOF\\n'\\\n>> /opt/pwdfile\n\nRUN \\\n $PAYARA_PATH/bin/asadmin start-domain && \\\n $PAYARA_PATH/bin/asadmin --user $ADMIN_USER --passwordfile=/opt/tmpfile change-admin-password && \\\n $PAYARA_PATH/bin/asadmin --user $ADMIN_USER --passwordfile=/opt/pwdfile enable-secure-admin && \\\n $PAYARA_PATH/bin/asadmin restart-domain\n\n# cleanup\nRUN rm /opt/tmpfile",
    "jar file with arguments in docker": "Set the jar file as your entrypoint and the args as your command\nAn example:\nENTRYPOINT [\"/path/to/my/java.jar\"]\nCMD [\"my\", \"default\", \"args\"]\nYou can then override the args whenever you run the container, using:\ndocker run <my-docker-image> some custom args\nMore information here: http://goinbigdata.com/docker-run-vs-cmd-vs-entrypoint/",
    "How to use COPY command in Docker build?": "COPY copies files from your host filesystem into the container. It looks like you want to copy files from one directory in the container to another. For that you need to use RUN and cp\nRUN cp -r built/* /data/\nSince you will be removing the /tempDir/ directory, you can speed things up a bit by renaming the directory:\nRUN mv built /data\nThis way you don't have to copy data around and then delete the originals.",
    "Docker: how to execute a batch file when container starts and keep the user in cmd / session": "instead of the ENTRYPOINT you can try putting something like this in your Dockerfile:\nCMD C:\\init\\init.bat && cmd",
    "creating a docker image with nginx compile options for Optional HTTP modules": "I'm somewhat of a noob with Docker, but I had to solve this same problem. I used this Dockerfile as a starting point.\nFROM centos:centos7\n\nWORKDIR /tmp\n\n# Install prerequisites for Nginx compile\nRUN yum install -y \\\n        wget \\\n        tar \\\n        openssl-devel \\\n        gcc \\\n        gcc-c++ \\\n        make \\\n        zlib-devel \\\n        pcre-devel \\\n        gd-devel \\\n        krb5-devel \\\n    openldap-devel \\\n        git\n\n# Download Nginx and Nginx modules source\nRUN wget http://nginx.org/download/nginx-1.9.3.tar.gz -O nginx.tar.gz && \\\n    mkdir /tmp/nginx && \\\n    tar -xzvf nginx.tar.gz -C /tmp/nginx --strip-components=1 &&\\\n    git clone https://github.com/kvspb/nginx-auth-ldap.git /tmp/nginx/nginx-auth-ldap\n\n# Build Nginx\nWORKDIR /tmp/nginx\nRUN ./configure \\\n        --user=nginx \\\n        --with-debug \\\n        --group=nginx \\\n        --prefix=/usr/share/nginx \\\n        --sbin-path=/usr/sbin/nginx \\\n        --conf-path=/etc/nginx/nginx.conf \\\n        --pid-path=/run/nginx.pid \\\n        --lock-path=/run/lock/subsys/nginx \\\n        --error-log-path=/var/log/nginx/error.log \\\n        --http-log-path=/var/log/nginx/access.log \\\n        --with-http_gzip_static_module \\\n        --with-http_stub_status_module \\\n        --with-http_ssl_module \\\n        --with-http_spdy_module \\\n        --with-pcre \\\n        --with-http_image_filter_module \\\n        --with-file-aio \\\n        --with-ipv6 \\\n        --with-http_dav_module \\\n        --with-http_flv_module \\\n        --with-http_mp4_module \\\n        --with-http_gunzip_module \\\n        --add-module=nginx-auth-ldap && \\\n    make && \\\n    make install\n\nWORKDIR /tmp\n\n# Add nginx user\nRUN adduser -c \"Nginx user\" nginx && \\\n    setcap cap_net_bind_service=ep /usr/sbin/nginx\n\nRUN touch /run/nginx.pid\n\nRUN chown nginx:nginx /etc/nginx /etc/nginx/nginx.conf /var/log/nginx /usr/share/nginx /run/nginx.pid\n\n# Cleanup after Nginx build\nRUN yum remove -y \\\n        wget \\\n        tar \\\n        gcc \\\n        gcc-c++ \\\n        make \\\n        git && \\\n    yum autoremove -y && \\\n    rm -rf /tmp/*\n\n# PORTS\nEXPOSE 80\nEXPOSE 443\n\nUSER nginx\nCMD [\"/usr/sbin/nginx\", \"-g\", \"daemon off;\"]",
    "Dockerfile issue - Why is the binary dlv not being found - No such file or directory": "You built dlv in alpine-based distro. dlv executable is linked against libc.musl:\n# ldd dlv \n        linux-vdso.so.1 (0x00007ffcd251d000)\n        libc.musl-x86_64.so.1 => not found\nBut then you switched to glibc-based image debian:buster-slim. That image doesn't have the required libraries.\n# find / -name libc.musl*                                        \n<nothing found>\nThat's why you can't execute dlv - the dynamic linker fails to find the proper lib.\nYou need to build in glibc-based docker. For example, replace the first line\nFROM golang:bullseye AS builder\nBTW. After you build you need to run the container in the priviledged mode\n$ docker build . -t try-dlv\n...\n$ docker run --privileged --rm try-dlv\nAPI server listening at: [::]:40000\n2022-10-30T10:51:02Z warning layer=rpc Listening for remote connections (connections are not authenticated nor encrypted)\nIn non-priviledged container dlv is not allowed to spawn a child process.\n$ docker run --rm try-dlv\nAPI server listening at: [::]:40000\n2022-10-30T10:55:46Z warning layer=rpc Listening for remote connections (connections are not authenticated nor encrypted)\ncould not launch process: fork/exec /app/fooapp: operation not permitted\nReally Minimal Image\nYou use debian:buster-slim to minimize the image, it's size is 80 MB. But if you need a really small image, use busybox, it is only 4.86 MB overhead.\nFROM golang:bullseye AS builder\n\n# Build Delve for debugging\nRUN go install github.com/go-delve/delve/cmd/dlv@latest\n\n# Create and change to the app directory.\nWORKDIR /app\nENV CGO_ENABLED=0\n\n# Retrieve application dependencies.\nCOPY go.* ./\nRUN go mod download\n\n# Copy local code to the container image.\nCOPY . ./\n\n# Build the binary.\nRUN go build -o fooapp .\n\n# Download certificates\nRUN set -x && apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y \\\n    ca-certificates \n\n# Use the official Debian slim image for a lean production container.\nFROM busybox:glibc\n\nEXPOSE 8000 40000\n\n# Copy the binary to the production image from the builder stage.\nCOPY --from=builder /app/fooapp /app/fooapp \n# COPY --from=builder /app/ /app\n\nCOPY --from=builder /go/bin/dlv /dlv\n\nCOPY --from=builder /etc/ssl /etc/ssl\n\n# Run dlv as pass fooapp as parameter\nCMD [\"/dlv\", \"--listen=:40000\", \"--headless=true\", \"--api-version=2\", \"--accept-multiclient\", \"exec\", \"/app/fooapp\"]\n# ENTRYPOINT [\"/bin/sh\"]\nThe image size is 25 MB, of which 18 MB are from dlv and 2 MB are from Hello World application.\nWhile choosing the images care should be taken to have the same flavors of libc. golang:bullseye links against glibc. Hence, the minimal image must be glibc-based.\nBut if you want a bit more comfort, use alpine with gcompat package installed. It is a reasonably rich linux with lots of external packages for just extra 6 MB compared to busybox.\nFROM golang:bullseye AS builder\n\n# Build Delve for debugging\nRUN go install github.com/go-delve/delve/cmd/dlv@latest\n\n# Create and change to the app directory.\nWORKDIR /app\nENV CGO_ENABLED=0\n\n# Copy local code to the container image.\nCOPY . ./\n\n# Retrieve application dependencies.\nRUN go mod tidy\n\n# Build the binary.\nRUN go build -o fooapp .\n\n# Use alpine lean production container.\n# FROM busybox:glibc\nFROM alpine:latest\n\n# gcompat is the package to glibc-based apps\n# ca-certificates contains trusted TLS CA certs\n# bash is just for the comfort, I hate /bin/sh\nRUN apk add gcompat ca-certificates bash\n\nEXPOSE 8000 40000\n\n# Copy the binary to the production image from the builder stage.\nCOPY --from=builder /app/fooapp /app/fooapp \n# COPY --from=builder /app/ /app\n\nCOPY --from=builder /go/bin/dlv /dlv\n\n# Run dlv as pass fooapp as parameter\nCMD [\"/dlv\", \"--listen=:40000\", \"--headless=true\", \"--api-version=2\", \"--accept-multiclient\", \"exec\", \"/app/fooapp\"]\n# ENTRYPOINT [\"/bin/bash\"]",
    "Docker is pushing all layers instead of the last one": "Docker invalidates COPY layers once the context changes -- regardless of what the next steps actually depend on. Copy files at the last possible moment -- in your case, copy requirements.txt first, and the rest later. Like this:\nFROM dr_prof_patrick/my_app:my_app_base_image\n\nWORKDIR /my_app\n\nCOPY requirements.txt .\n\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD [\"python3\", \"main.py\"]\nAlso take a look at your .dockerignore and don't copy useless files. The best strategy I see used is to use .dockerignore as a whitelist, not a blacklist, by ignoring everything first and then un-ignoring the files you need:\n*\n!requirements.txt",
    "Use GPU on python docker image": "TensorFlow image split into several 'partial' Dockerfiles. One of them contains all dependencies TensorFlow needs to operate on GPU. Using it you can easily create a custom image, you only need to change default python to whatever version you need. This seem to me a much easier job than bringing NVIDIA's stuff into Debian image (which AFAIK is not officially supported for CUDA and/or cuDNN).\nHere's the Dockerfile:\n# TensorFlow image base written by TensorFlow authors.\n# Source: https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/tools/dockerfiles/partials/ubuntu/nvidia.partial.Dockerfile\n# -------------------------------------------------------------------------\nARG ARCH=\nARG CUDA=10.1\nFROM nvidia/cuda${ARCH:+-$ARCH}:${CUDA}-base-ubuntu${UBUNTU_VERSION} as base\n# ARCH and CUDA are specified again because the FROM directive resets ARGs\n# (but their default value is retained if set previously)\nARG ARCH\nARG CUDA\nARG CUDNN=7.6.4.38-1\nARG CUDNN_MAJOR_VERSION=7\nARG LIB_DIR_PREFIX=x86_64\nARG LIBNVINFER=6.0.1-1\nARG LIBNVINFER_MAJOR_VERSION=6\n\n# Needed for string substitution\nSHELL [\"/bin/bash\", \"-c\"]\n# Pick up some TF dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        build-essential \\\n        cuda-command-line-tools-${CUDA/./-} \\\n        # There appears to be a regression in libcublas10=10.2.2.89-1 which\n        # prevents cublas from initializing in TF. See\n        # https://github.com/tensorflow/tensorflow/issues/9489#issuecomment-562394257\n        libcublas10=10.2.1.243-1 \\ \n        cuda-nvrtc-${CUDA/./-} \\\n        cuda-cufft-${CUDA/./-} \\\n        cuda-curand-${CUDA/./-} \\\n        cuda-cusolver-${CUDA/./-} \\\n        cuda-cusparse-${CUDA/./-} \\\n        curl \\\n        libcudnn7=${CUDNN}+cuda${CUDA} \\\n        libfreetype6-dev \\\n        libhdf5-serial-dev \\\n        libzmq3-dev \\\n        pkg-config \\\n        software-properties-common \\\n        unzip\n\n# Install TensorRT if not building for PowerPC\nRUN [[ \"${ARCH}\" = \"ppc64le\" ]] || { apt-get update && \\\n        apt-get install -y --no-install-recommends libnvinfer${LIBNVINFER_MAJOR_VERSION}=${LIBNVINFER}+cuda${CUDA} \\\n        libnvinfer-plugin${LIBNVINFER_MAJOR_VERSION}=${LIBNVINFER}+cuda${CUDA} \\\n        && apt-get clean \\\n        && rm -rf /var/lib/apt/lists/*; }\n\n# For CUDA profiling, TensorFlow requires CUPTI.\nENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n\n# Link the libcuda stub to the location where tensorflow is searching for it and reconfigure\n# dynamic linker run-time bindings\nRUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 \\\n    && echo \"/usr/local/cuda/lib64/stubs\" > /etc/ld.so.conf.d/z-cuda-stubs.conf \\\n    && ldconfig\n# -------------------------------------------------------------------------\n#\n# Custom part\nFROM base\nARG PYTHON_VERSION=3.7\n\nRUN apt-get update && apt-get install -y --no-install-recommends --no-install-suggests \\\n          python${PYTHON_VERSION} \\\n          python3-pip \\\n          python${PYTHON_VERSION}-dev \\\n# Change default python\n    && cd /usr/bin \\\n    && ln -sf python${PYTHON_VERSION}         python3 \\\n    && ln -sf python${PYTHON_VERSION}m        python3m \\\n    && ln -sf python${PYTHON_VERSION}-config  python3-config \\\n    && ln -sf python${PYTHON_VERSION}m-config python3m-config \\\n    && ln -sf python3                         /usr/bin/python \\\n# Update pip and add common packages\n    && python -m pip install --upgrade pip \\\n    && python -m pip install --upgrade \\\n        setuptools \\\n        wheel \\\n        six \\\n# Cleanup\n    && apt-get clean \\\n    && rm -rf $HOME/.cache/pip\nYou can take from here: change python version to one you need (and which is available in Ubuntu repositories), add packages, code, etc.",
    "Running Jest test with Dockerfile": "RUN and CMD aren't commands, they're instructions to tell Docker what do when building your container. So e.g.:\nRUN if [ \"$runTests\" = \"True\" ]; then \\\n    RUN npm test; fi\ndoesn't make sense, RUN <command> runs a shell command but RUN isn't defined in the shell, it should just be:\nARG runTests  # you need to define the argument too\nRUN if [ \"$runTests\" = \"True\" ]; then \\\n    npm test; fi\nThe cleaner way to do this is to set up npm as the entrypoint, and start as the specific command:\nENTRYPOINT [ \"npm\" ]\nCMD [ \"start\" ]\nThis allows you to build the container normally, it doesn't require any build arguments, then run an NPM script other than start in the container, e.g. to run npm test:\ndocker run <image> test\nHowever, note that this means all of the dev dependencies need to be in the container. It looks (from ENV NODE_ENV=production) like you intend this to be a production build, so you shouldn't be running the tests in the container at all. Also despite having as builder this isn't really a multi-stage build. The idiomatic script for this would be something like:\n# stage 1: copy the source and build the app\n\nFROM node:10-alpine as builder\nARG TOKEN\n\nWORKDIR /app\n\nCOPY .npmrc-pipeline .npmrc\n\nCOPY package*.json ./\nRUN npm ci\n\nCOPY . .\nRUN npm run build\n\n# stage 2: copy the output and run it in production\n\nFROM node:10-alpine\n\nWORKDIR /app\n\nENV PORT=3000\nENV NODE_ENV=production\n\nCOPY --from=builder /app/package*.json ./\nRUN npm ci\n\nCOPY --from=builder /* your build output */\n\nEXPOSE 3000\n\nENTRYPOINT [ \"npm\" ]\nCMD [ \"start\" ]\nSee e.g. this Dockerfile I put together for a full-stack React/Express app.",
    "Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections": "You can't access service which on the host using localhost from container, you had to use the ip address of your host to access.\nThis because: default docker will use bridge which will setup a internal network for your container, so when container use localhost, it doesn't mean the host, it mean the container self's network.\nIf insist on, a ugly solution is use --net=host.\nSomething like next:\nsudo docker run --net=host -p 8080:8080 -t djtijare/a2i-web:v1\nThen you can use localhost to visit host's service from container.",
    "Docker RUN multiple instance of a image with different parameters": "Docker containers are started with an entrypoint and a command; when the container actually starts they are simply concatenated together. If the ENTRYPOINT in the Dockerfile is structured like a single command then the CMD in the Dockerfile or command: in the docker-compose.yml contains arguments to it.\nThis means you should be able to set up your docker-compose.yml as:\nservices:\n  my.app1:\n    image: ${DOCKER_REGISTRY}my/app\n    ports:\n     - 5000:80\n    command: [80, db1.db]\n  my.app2:\n    image: ${DOCKER_REGISTRY}my/app\n    ports:\n     - 5001:80\n    command: [80, db2.db]\n(As a side note: if one of the options to the program is the port to listen on, this needs to match the second port in the ports: specification, and in my example I've chosen to have both listen on the \"normal\" HTTP port and remap it on the hosts using the ports: setting. One container could reach the other, if it needed to, as http://my.app2/ on the default HTTP port.)",
    "How to run copy command via docker RUN": "This is because RUN cp ... is not the same as COPY. COPY copies files from your host machine to the image, RUN runs inside the container during it's build process, and fails because there really is \"No such file or directory\" in there.\nAnd looking at the COPY documentation, there really is not a way to copy multiple files to multiple destinations with one COPY, only multiple sources to one destination.\nWhat you probably can do, if you really want, is to COPY everything first to one directory, for example /tmp, and then use the RUN cp /tmp/rdkafka.ini /etc/php/7.0/mods-available/ && ....",
    "Why && rather than a new RUN": "It is optimisation for docker image layer. I also recommend to read Best practices for writing Dockerfiles\nThere is also interesting presentation from DockerCon EU 2017.",
    "Advantage of using docker-compose file version 3 over a shellscript?": "Readability\nCompare your sample shell script to a YAML version of same:\nservices:\n  api_cntr:\n    image: api_img\n    network: net1\n    ports:\n      - 5000:5000\n  message_service:\n    image: redis\n    network: net1\n    ports:\n      - 6379:6379\n  celery_worker1:\n    image: celery_worker_img\n    network: net1\n  flower_hud:\n    image: flower_hud_img\n    network: net1\n    ports:\n      - 5555:5555    \nTo my eye at least, it is much easier to determine the overall architecture of the application from reading the YAML than from reading the shell commands.\nCleanup\nIf you use docker-compose, then running docker-compose down will stop and clean up everything, remove the network, etc. To do that in your shell script, you'd have to separately write a remove section to stop and remove all the containers and the network.\nMultiple inheriting YAML files\nIn some cases, such as for dev & testing, you might want to have a main YAML file and another that overrides certain values for dev/test work.\nFor instance, I have an application where I have a docker-compose.yml as well as docker-compose.dev.yml. The first contains all of the production settings for my app. But the \"dev\" version has a more limited set of things. It uses the same service names, but with a few differences.\nAdds a mount of my code directory into the container, overriding the version of the code that was built into the image\nExposes the postgres port externally (so I can connect to it for debugging purposes) - this is not exposed in production\nUses another mount to fake a user database so I can easily have some test users without wiring things up to my real authentication server just for development\nNormally the service only uses docker-compose.yml (in production). But when I am doing development work, I run it like this:\ndocker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d\nIt will load the normal parameters from docker-compose.yml first, then read docker-compose.dev.yml second, and override only the parameters found in the dev file. The other parameters are all preserved from the production version. But I don't require two completely separate YAML files where I might need to change the same parameters in both.\nEase of maintenance\nEverything I described in the last few paragraphs can be done using shell scripts. It's just more work to do it that way, and probably more difficult to maintain, and more prone to mistakes.\nYou could make it easier by having your shell scripts read a config file and such... but at some point you have to ask if you are just reimplementing your own version of docker-compose, and whether that is worthwhile to you.",
    "How to use a python library that is constantly changing in a docker image or new container?": "During development it is IMO perfectly fine to map/mount the hostdirectory with your ever changing sources into the Docker container. The rest (the python version, the other libraries you are dependent upon you can all install in the normal way in the the docker container.\nOnce stabilized I remove the map/mount and add the package to the list of items to install with pip. I do have a separate container running devpi so I can pip-install packages whether I push them all the way to PyPI or just push them to my local devpi container.\nDoing speed up container creation even if you use the common (but more limited) python [path_to_project/setup.py] develop. Your Dockerfile in this case should look like:\n # the following seldom changes, only when a package is added to setup.py\n COPY /some/older/version/of/project/plus/dependent/packages /older/setup\n RUN pip /older/setup/your_package.tar.gz\n\n # the following changes all the time, but that is only a small amount of work\n COPY /latest/version/of/project     \n RUN python [path_to_project/setup.py] develop\nIf the first copy would result in changes to files under /older/setup then the container gets rebuilt from there.\nRunning python ... develop still makes more time and you need to rebuild/restart the container. Since my packages all can also be just copied in/linked to (in addition to be installed) that is still a large overhead. I run a small program in the container that checks if the (mounted/mapped) sources change and then reruns anything I am developing/testing automatically. So I only have to save a new version and watch the output of the container.",
    "Where does dockerized jetty store its logs?": "Why you aren't seeing logs\n2 things to note:\nRunning docker run -it jetty bash will start a new container instead of connecting you to your existing daemonized container.\nAnd it would invoke bash instead of starting jetty in that container, so it won't help you to get logs from either container.\nSo this interactive container won't help you in any case.\nBut also...\nJettyLogs are disabled anyways\nAlso, you won't see the logs in the standard location (say, if you tried to use docker exec to read the logs, or to get them in a volume), quite simply because the Jetty Docker file is aptly disabling logging entirely.\nIf you look at the jetty:9.2.10 Dockerfile, you will see this line:\n&& sed -i '/jetty-logging/d' etc/jetty.conf \\\nWhich nicely removes the entire line referencing the jetty-logging.xml default logging configuration.\nWhat to do then?\nReading logs with docker logs\nDocker gives you access to the container's standard output.\nAfter you did this:\ndocker run --name='abc' -d -p 10908:8080 -v /var/log/abc:/var/log/jetty me/abc:latest\nYou can simply do this:\ndocker logs abc\nAnd be greeted with somethig similar to this:\nRunning Jetty: \n2015-05-15 13:33:00.729:INFO::main: Logging initialized @2295ms\n2015-05-15 13:33:02.035:INFO:oejs.SetUIDListener:main: Setting umask=02\n2015-05-15 13:33:02.102:INFO:oejs.SetUIDListener:main: Opened ServerConnector@73ec519{HTTP/1.1}{0.0.0.0:8080}\n2015-05-15 13:33:02.102:INFO:oejs.SetUIDListener:main: Setting GID=999\n2015-05-15 13:33:02.106:INFO:oejs.SetUIDListener:main: Setting UID=999\n2015-05-15 13:33:02.133:INFO:oejs.Server:main: jetty-9.2.10.v20150310\n2015-05-15 13:33:02.170:INFO:oejdp.ScanningAppProvider:main: Deployment monitor [file:/var/lib/jetty/webapps/] at interval 1\n2015-05-15 13:33:02.218:INFO:oejs.ServerConnector:main: Started ServerConnector@73ec519{HTTP/1.1}{0.0.0.0:8080}\n2015-05-15 13:33:02.219:INFO:oejs.Server:main: Started @3785ms\nUse docker help logs for more details.\nCustomize\nObviously your other option is to revert what the default Dockerfile for jetty is doing, or to create your own dockerized Jetty.",
    "docker-compose Equivalent to Docker Build --secret Argument": "Turns out I was a bit ahead of the times. docker compose v.2.5.0 brings support for secrets.\nAfter having modified the Dockerfile as explained above, we must then update the docker-compose to defined secrets.\ndocker-compose.yml\nservices:\n  my-cool-app:\n    build:\n      context: .\n      secrets:\n        - github_user\n        - github_token\n...\nsecrets:\n  github_user:\n    file: secrets_github_user\n  github_token:\n    file: secrets_github_token\nBut where are those files secrets_github_user and secrets_github_token coming from? In your CI you also need to export the environment variable and save it to the default secrets file location. In our project we are using Tasks so we added these too lines.\nNote that we are running this task from our CI, so you could do it differently without Tasks for example.\n- printenv GITHUB_USER > /root/project/secrets_github_user\n- printenv GITHUB_TOKEN > /root/project/secrets_github_token\nWe then update the CircleCI config and add two environment variable to our job:\n.config.yml\n  name-of-our-job:\n    environment:\n      DOCKER_BUILDKIT: 1\n      COMPOSE_DOCKER_CLI_BUILD: 1\nYou might also need a more recent Docker version, I think they introduced it in a late 19 release or early 20. I have used this and it works:\n    steps:\n      - setup_remote_docker:\n          version: 20.10.11\nNow when running your docker-compose based commands, the secrets should be successfully mounted through docker-compose and available to correctly build or run your Dockerfile instructions!",
    "invalid empty ssh agent socket, make sure SSH_AUTH_SOCK is set How to set SSH_AUTH_SOCK for docker build?": "It seems that either the --ssh argument doesn't accept empty values as an argument.\neval $(ssh-agent)\nset \"DOCKER_BUILDKIT=1\" && docker build --ssh default=${SSH_AUTH_SOCK} -f docker/Dockerfile -t basketball_backend_api_core .\nor you may need to run ssh-add to add private key identities to the authentication agent first for this to work.\nbefore_script:\n  ##\n  ## Install ssh-agent if not already installed, it is required by Docker.\n  ## (change apt-get to yum if you use an RPM-based image)\n  ##\n  - 'command -v ssh-agent >/dev/null || ( apt-get update -y && apt-get install openssh-client -y )'\n\n  ##\n  ## Run ssh-agent (inside the build environment)\n  ##\n  - eval $(ssh-agent -s)\n\n  ##\n  ## Add the SSH key stored in SSH_PRIVATE_KEY variable to the agent store\n  ## We're using tr to fix line endings which makes ed25519 keys work\n  ## without extra base64 encoding.\n  ## https://gitlab.com/gitlab-examples/ssh-private-key/issues/1#note_48526556\n  ##\n  - echo \"$SSH_PRIVATE_KEY\" | tr -d '\\r' | ssh-add -\nfrom Gitlab's docs.",
    "dial tcp 127.0.0.1:8080: connect: connection refused. go docker app": "For communicating between multiple docker-compose clients, you need to make sure that the containers you want to talk to each other are on the same network.\nFor example, (edited for brevity) here you have one of the docker-compose.yml\n# sport_app docker-compose.yml\nversion: '3'\nservices:\n  go-sports-entities-hierarchy:\n    ...\n    networks:\n      - some-net\n  go-sports-events-workflow\n    ...\n    networks:\n      - some-net\nnetworks:\n  some-net:\n    driver: bridge\nAnd the other docker-compose.yml\n# user_management app docker-compose.yml\nversion: '3'\nservices:\n  postgres:\n    ...\n    networks:\n      - some-net\n  go-user-management\n    ...\n    networks:\n      - some-net\nnetworks:\n  some-net:\n    external: true\nNote: Your app\u2019s network is given a name based on the project name, which is based on the name of the directory it lives in, in this case a prefix user_ was added.\nThey can then talk to each other using the service name, i.e. go-user-management, etc.\nYou can, after running the docker-compose up --build commands, run the docker network ls command to see it, then docker network inspect bridge, etc.",
    "How do I detect the interactive flag in a container?": "At build time, you won't know what the runtime environment will look like. So there's no way to modify the RUN statements based on the -it flags or lack there of, we don't have time travel to look into the future for that, and the same image could be run multiple times with different flags.\nYou could do this within the entrypoint script. /dev/stdin which is mapped for interactive containers points to /proc/self/fd/0 which will point to different things based on whether you have -i, -it, or neither:\n$ docker run -it --rm busybox ls -al /dev/stdin\nlrwxrwxrwx    1 root     root            15 Jun 18 14:43 /dev/stdin -> /proc/self/fd/0\n\n$ docker run -it --rm busybox ls -al /proc/self/fd/0\nlrwx------    1 root     root            64 Jun 18 14:43 /proc/self/fd/0 -> /dev/pts/0\n\n$ docker run -i --rm busybox ls -al /proc/self/fd/0\nlr-x------    1 root     root            64 Jun 18 14:50 /proc/self/fd/0 -> pipe:[9858991]\n\n$ docker run --rm busybox ls -al /proc/self/fd/0\nlrwx------    1 root     root            64 Jun 18 14:43 /proc/self/fd/0 -> /dev/null\nSo if you stat the link to see if it's going to /dev/null or not, that would give you the answer to the question you asked.\nSide note, if you only care about the -t, there's another check in shell you can run, [ -t 0 ], e.g.:\n$ docker run -it --rm busybox /bin/sh -c 'if [ -t 0 ]; then echo tty; else echo no tty; fi'\ntty\n\n$ docker run -i --rm busybox /bin/sh -c 'if [ -t 0 ]; then echo tty; else echo no tty; fi'\nno tty\n\n$ docker run --rm busybox /bin/sh -c 'if [ -t 0 ]; then echo tty; else echo no tty; fi'\nno tty\nHowever, best practice with docker is to run the app itself in the foreground, as pid 1. And if you need to enter the container for debugging in a development environment, use docker exec for that. Your entrypoint then becomes:\nENTRYPOINT [ \"/usr/local/apache-tomcat-v8.5.55/bin/catalina.sh\", \"run\" ]\nAnd by doing that, you avoid an extra shell in pid 1 that could block container stop signals from gracefully stopping the app.\nThen to debug, you'd exec after the run, using the container name:\ndocker container run -dp 8080:8080 -n tomcat-app <mydockername>\ndocker container exec -it tomcat-app /bin/sh",
    "Access docker within Dockerfile?": "No, you can't do this.\nYou need access to your host's Docker socket somehow. In a standalone docker run command you'd do something like docker run -v /var/run/docker.sock:/var/run/docker.sock, but there's no way to pass that option (or any other volume mount) into docker build.\nFor running unit-type tests (that don't have external dependencies) I'd just run them in your development or core CI build environment, outside of Docker, and run run docker build until they pass. For integration-type tests (that do) you need to set up those dependencies, maybe with a Docker Compose file, which again will be easier to do outside of Docker. This also avoids needing to build your test code and its additional dependencies into your image.\n(Technically there are two ways around this. The easier of the two is the massive security disaster that is opening up a TCP-based Docker socket; then your Dockerfile could connect to that [\"remote\"] Docker daemon and launch containers, stop them, kill itself off, impersonate the host for inbound SSH connections, launch a bitcoin miner that lives beyond the container build, etc...actually it allows any process on the host to do any of these things. The much harder, as @RaynalGobel suggests in a comment, is to try to launch a separate Docker daemon inside the container; the DinD image link there points out that it requires a --privileged container, which again you can't have at build time.)",
    "Docker-compose: Set a variable in env file and use it in Dockerfile": "You need to pass the build argument in docker compose\nversion '2'\n\nservices:\n    php:\n        build: \n          dockerfile: php7-fpm\n          args:\n            TIMEZONE: ${TIMEZONE}\n        volumes:\n            - ${APP_PATH}:/var/www/app\n            - ./logs:/var/www/logs\nThe environment are passed to the running container and not to the buildfile. For the you need to pass args in the build section",
    "Installing specific version of node.js and npm in ubuntu image with Dockerfile": "You can just follow the usual Ubuntu install instructions, just within the RUN statement in your Dockerfile\nRUN curl -sL https://deb.nodesource.com/setup_6.x | bash - \\\n    && apt-get install -y nodejs\nDocs",
    "How to create a copy of exisiting docker image": "Simply write a Dockerfile starting with:\nFROM debian:latest\n...\n(using the FROM directive)\nThat will create a local image based on debian, and since debian is already downloaded, it won't be downloaded again.\nNote: it is best to avoid the \"latest\" tag: see \"Docker: The latest Confusion\" by Stack Overflow contributor Adrian Mouat.\nUsing actual labels is more precise:\ndocker pull debian:7.8\ndocker pull debian:wheezy\nIf wanted to do something in ubuntu is there a way when: I just execute command docker copy \"image_name\" and then do whatever I want to (run image, clone some git repo, install some packages, test it) , and then just delete it docker rmi \"image_name\" (when I'm done with image) .\nYes: you can docker run --it <image> bash (for images which includes bash), and exit that bash: your container will be exited: you can then docker commit <containerrid> newimage, and you will get a copy of the original image.",
    "Docker - Multiple duplicate volume declarations; what happens?": "Take a look at https://docs.docker.com/reference/builder/#volume - the VOLUME command is declaring a mount point so it can be used by other hosts with the --volumes-from as well the VOLUME command tells docker that the contents of this directory is external to the image. While the -v /dir1/:/dir2/ will mount dir1 from the host into the running container at dir2 location.\nIn other words, you can use both together and docker will mount the -v properly.",
    "Docker bundle install cache issues when updating gems": "I found two possible solutions that use external data volume for gem storage: one and two.\nBriefly,\nyou specify an image that is used to store gems only\nin your app images, in docker-compose.yml you specify the mount point for BUNDLE_PATH via volumes_from.\nwhen your app container starts up, it executes bundle check || bundle install and things are good to go.\nThis is one possible solution, however to me it feels like it goes slightly against the docker way. Specifically, bundle install to me sounds like it should be part of the build process and shouldn't be part of the runtime. Other things, that depend on the bundle install like asset:precompile are now a runtime task as well.\nThis is a vaiable solution but I'm looking forward to something a little more robust.",
    "forward udp multicast from eth0 to docker0": "After a lot of frustrating days of trying out a number of things... finally something worked:\nUsing Pipework (https://github.com/jpetazzo/pipework), the following command worked but there is a catch -\npipework eth2 $(docker run -d hipache /usr/sbin/hipache) 50.19.169.157/24\nrunning a docker container by only running the above command did not quite help me. I had to run tcpdump -i eth2 on my host to capture packets on eth2 interface, which then started to forward the packets to the docker container.\nAny idea why is worked and not just running the command??",
    "How to run a shell script using dockerfiles CMD": "A Docker container will stop when its main process completes. In your case, this means the two Java applications will be forked to the background (because of the nohup call) then the script will immediately complete and the container will exit.\nThere are a few solutions:\nThe quickest and easiest solution is to just remove nohup call from the second java call. That way the script won't exit until the second Java application exits.\nUse a process manager such as runit or supervisord to manage the processes.\nPut the jars in separate containers and call Java directly (this would seem to be the best solution to me).",
    "How to use docker to test multiple compiler versions": "I would separate the parts of preparing the compiler and doing the calculation, so the source doesn't become part of the docker container.\nPrepare Compiler\nFor preparing the compiler I would take the ARG approach but without copying the data into the container. In case you wanna fast retry while having enough resources you could spin up multiple instances the same time.\nARG COMPILER=gcc:4.8\nFROM ${COMPILER}\nENV DEBIAN_FRONTEND noninteractive\n\n# Install tools (cmake, ninja, etc)\n# this will cause bloat if the FROM layer changes\nRUN <<EOF\n  apt update\n  apt install -y cmake ninja-build\n  rm -rf /var/lib/apt/lists/*\nEOF\n\n# Set the work directory\nVOLUME /src\nWORKDIR /src\nCMD [\"cmake\"]\nBuild it\nHere you have few options. You could either prepare a volume with the sources or use bind mounts together with docker exec like this:\n#bash style \nfor compiler in gcc:4.9 gcc:4.8 gcc:5.1\ndo \n  docker build -t mytag-${compiler} --build-arg COMPILER=${compiler} .\n  # place to clean the target folder\n  docker run -v $(pwd)/src:/src  mytag-${compiler} \ndone\nAnd because the source is not part of the docker image you don't have bloat. You can also have two mounts, one for a readonly source tree and one for the output files.\nNote: If you remove the CMake command you could also spin up the docker containers in parallel and use docker exec to start the build. The downside of this is that you have to take care of out of source builds to avoid clashes on the output folder.",
    "In Dockerfile, COPY all contents of current directory except one directory": "I don't think there is an easy solution to this problem.\nIf you need vendor for RUN composer install and you're not using a multistage build then it doesn't matter if you remove the vendor folder in the copy command. If you've copied it into the build earlier then it's going to be present in your final image, even if you don't copy it over in your COPY step.\nOne way to get around this is with multi-stage builds, like so:\nFROM debian as base\nCOPY . /var/task/\nRUN rm -rf /var/task/vendor\nFROM debian\nCOPY --from=base /var/task /var/task\nIf you can use this pattern in your larger build file then the final image will contain all the files in your working directory except vendor.\nThere's still a performance hit though. You're still going to have to copy the entire vendor directory into the build, and depending on what docker features you're using that will still take a long time. But if you need it for composer install then there's really no way around this.",
    "Why isn't docker reusing docker-compose's cache layers?": "With Docker-compose 1.25+ (Dec. 2019), try and use:\nCOMPOSE_DOCKER_CLI_BUILD=1 docker-compose build\nThat is what is needed to enable the docker-cli, instead of the own internal docker-compose build.\nSee also \"Faster builds in Docker Compose 1.25.1 thanks to BuildKit Support\".\nBut be aware of docker-compose issue 7336, when using it with DOCKER_BUILDKIT=1 (in addition of COMPOSE_DOCKER_CLI_BUILD=1)",
    "Cypress could not verify that this server is running when using Docker and Docker Compose": "localhost in Docker is always \"this container\". Use the names of the service blocks in the docker-compose.yml as hostnames, i.e., http://web:8080\n(Note that I copied David Maze's answer from the comments)",
    "docker-compose use environment variables from .env file": "So what you are doing is wrong. Reason being there is environment variable to be passed to Dockerfile while building. So there are two possible solutions\nUse Builder Arguments\nFROM mybaseimage\nMAINTAINER Zeinab Abbasimazar\nARG IP\nARG JMX_PORT\nARG WS_PORT\nRUN echo ${IP}\nRUN echo ${JMX_PORT}\nRUN echo ${WS_PORT}\nAnd then pass these build arguments using compose\nversion: \"2\"\nservices:\n  mynode:\n    build:\n      context: .\n      dockerfile: Dockerfile-mynode\n      args:\n        - IP=${IP}\n        - JMX_PORT=${JMX_PORT}\n        - WS_PORT=${WS_PORT}\n    environment_file:\n      - .env\n    ports:\n      - \"${JMX_PORT}:${JMX_PORT}\"\n      - \"${WS_PORT}:${WS_PORT}\"\nAlso you can load environment variables using environment_file. now this is only available when the container is started and they don't apply to your compose file or to your build file.\nAlso adding the build arguments will only make them available during the build and not during the container start. If you need to do that you need to define the variable twice\nFROM mybaseimage\nMAINTAINER Zeinab Abbasimazar\nARG IP\nARG JMX_PORT\nARG WS_PORT\nENV IP=${IP} JMX_PORT=${JMX_PORT} WS_PORT=${WS_PORT}\nRUN env\nCopying the environment file\n.env\nexport NAME=TARUN\nDockerfile\nFROM alpine\nCOPY .env /tmp/.env\nRUN cat /tmp/.env > /etc/profile.d/tarun.sh && chmod +x /etc/profile.d/tarun.sh\nSHELL [\"sh\",\"-lc\"]\nRUN env\nIn the RUN cat /tmp/.env > /etc/profile.d/tarun.sh && chmod +x /etc/profile.d/tarun.sh we create profile file which should be loaded when a shell is executed with profile.\nIn SHELL [\"sh\",\"-lc\"] we change default shell from sh -c to sh -l so it can load our profile also.\nOutput of the last step is\nStep 5/5 : RUN env\n ---> Running in 329ec287a5a6\nHOSTNAME=e1ede117fb1e\nSHLVL=1\nHOME=/root\nPAGER=less\nPS1=\\h:\\w\\$\nNAME=TARUN\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nPWD=/\nCHARSET=UTF-8\n ---> 6ab95f46e940\nAs you can see the environment variable is there.\nNote: This .env format cannot be loaded in docker-compose as we have an export statement which is a bash command",
    "How should I handle Perl module updates when maintaining docker images?": "You could use the standard module installer for your underlying OS on your docker image.\nFor example, if its RedHat then use yum and only use CPAN when they are not available\nFROM centos:centos7\n  RUN  yum -y install cpanm gcc perl perl-App-cpanminus perl-Config-Tiny &&  yum clean all\n  RUN cpanm install Some::Module; rm -fr root/.cpanm; exit 0\ntaken from here and modified\nI would try to have a base image which the actual applications use\nI would also avoid doing things interactively (e.g. script a dockerfile) as you want to be able to repeat the build when upstream dependencies change, which docker hub does for you.\nEDIT You can convert perl modules into your own packages using dh-make-perl\nYou can load these into your own Ubuntu repo using reprepro or a paid solution of Artifactory\nThese can then be installed using apt-get when you use your repo as a source from within a dockerfile.\nWhen I have tried a similar thing before There are a few problems\nYour apps don't work with the latest version of modules\nThere are far more dependencies than you expected\nSome modules wont package\nBenefits are\nYou keep the build tools (gcc, etc) off the app servers\nYou know much more about your dependencies",
    "Dockerfile - How to copy files from a local folder? [duplicate]": "The key thing you are missing is the build's context, relevant from the COPY part of the docs:\nThe path must be inside the context of the build; you cannot COPY ../something /something, because the first step of a docker build is to send the context directory (and subdirectories) to the docker daemon.\nhttps://docs.docker.com/engine/reference/builder/#copy\nDescription of \"context\" here.\nhttps://docs.docker.com/engine/reference/commandline/build/\nBut essentially, when you say \"docker build directory-with-docker-file\" COPY only sees files in (and below) the directory with the Dockerfile.\nWhat you probably want to do is compile \"swagger\" during the docker build, and then put it in the path you want.\nA key thing to remember is that a Dockerfile is generally meant to be reproducible such that if you run docker build on ten different hosts, the exact same image will be produced. Copying files from arbitrary locations on the host wouldn't lead to that.",
    "Dockerfile - How to append PATH using ENV instruction?": "Apparently Docker doesn't let you use environment variables defined outside of your Dockerfile within an ENV or ARG declaration.\nAs a workaround, you can pass the names/directories to your Dockerfile explicitly using ARG:\nFROM golang:1.14.10\n    \n# set default to `root`\nARG USERNAME=root\n\nENV PATH=$PATH:/$USERNAME/go/bin\n\nRUN echo $PATH\nYou can then pass the USERNAME via docker build --build-arg USERNAME=myuser\nDepending on your usecase you can also do this using a RUN or ENTRYPOINT.",
    "How to run `httpd` in Docker in detached mode using CMD in Dockerfile?": "You should delete CMD [\"httpd\"], see this:\nCMD [\"httpd-foreground\"]\nThere is already a foreground httpd there.\nFinally, Why CMD [\"httpd\"] won't work?\nThe CMD defined in Dockerfile would be acting as PID1 of your container. In docker, if PID1 exits, then, the container will also exit.\nIf use CMD [\"httpd-foreground\"], the apache process will always be in front, so the process will not exit, then the container is alive.\nIf use CMD [\"httpd\"], the httpd will directly exit after executing, then PID1 exits, so the container exits.",
    "Why is my final docker image in this multi-stage build so large?": "Buried deep in the Docker documentation I found that my ARG and ENV definitions were cleared when I started the final FROM. Redefining them solved the issue:\n# Configure environment and build settings.\nFROM golang:alpine AS buildstage\nARG name=ddmnh\nENV GOPATH=/gopath\n\n# Create the working directory.\nWORKDIR ${GOPATH}\n\n# Copy the repository into the image.\nADD . ${GOPATH}\n\n# Move to GOPATH, install dependencies and build the binary.\nRUN cd ${GOPATH} && go get ${name}\nRUN CGO_ENABLED=0 GOOS=linux go build ${name}\n\n# Multi-stage build, we just use plain alpine for the final image.\nFROM alpine:latest\nARG name=ddmnh\nENV GOPATH=/gopath\n\n# Copy the binary from the first stage.\nCOPY --from=buildstage ${GOPATH}/${name} ./${name}\nRUN chmod u+x ./${name}\n\n# Expose Port 80.\nEXPOSE 80\n\n# Set the run command.\nCMD ./ddmnh",
    "Import-Module in the Docker PowerShell image": "Because it happens in a wrong \"context\". For this to work the way you want it to work you need to use both these commands in 1 powershell session:\npwsh -command \"Import-Module Microsoft.PowerShell.Management; Get-Module\"\nelse it creates a layer where it ran that command, but when powershell stops all the imports are gone (and when the layer is done, it shuts down the container, so it doesnt preserve session state, only os state).\nMy dockerfile (working example):\nFROM microsoft/powershell:ubuntu16.04\nRUN pwsh -c \"Install-Module AzureRM.netcore -Force\"\n\nCMD [ \"pwsh\" ]",
    "Docker - [Internal] load build context: when try to build an Image": "Same issue, but I made a bandaid with:\nsudo chown $USER:$USER *\nThe problem was really me being too fast and loose with docker volumes which caused some of the files/directories in the directory with Dockerfile to be owned by root instead of \"me\".\nThe permanent fix for me is presumably to stop playing fast and loose with volume mounts.",
    "Specify multiple files in ARG to COPY in Dockerfile": "because it treats src/hi src/there as a single item.\nHow can I \"expand\" the files argument into multiple files to copy?\nThat seems unlikely considering the Dockerfile format mentions, regarding arguments:\nwhitespace in instruction arguments, such as the commands following RUN, are preserved\nAnd that is not limited to RUN.\nCOPY, however, also includes:\nEach <src> may contain wildcards and matching will be done using Go\u2019s filepath.Match rules.\nIt would not work in your case.\nRemain the multi-stage builds approach\nCOPY src (the all folder)\nRemove everything you don't want:\nRUN find pip ${files} -maxdepth 1 -mindepth 1 -print | xargs rm -rf\nBuild your actual image based on the resulting state of the first image.\nYou can pass as one argument the folders you want to preserve\ndocker build --build-arg=\"files=! -path \"src/hi\" ! -path \"src/there\"\" .\nSee an example in \"Docker COPY files using glob pattern?\".",
    "VS Code Remote-Containers: cannot create directory \u2018/home/appuser\u2019:": "What works for me is to create a non-root user in my Dockerfile and then configure the VS Code dev container to use that user.\nStep 1. Create the non-root user in your Docker image\nARG USER_ID=1000\nARG GROUP_ID=1000\nRUN groupadd --system --gid ${GROUP_ID} MY_GROUP && \\\n    useradd --system --uid ${USER_ID} --gid MY_GROUP --home /home/MY_USER --shell /sbin/nologin MY_USER\nStep 2. Configure .devcontainer/devcontainer.json file in the root of your project (should be created when you start remote dev)\n\"remoteUser\": \"MY_USER\" <-- this is the setting you want to update\nIf you use docker compose, it's possible to configure VS Code to run the entire container as the non-root user by configuring .devcontainer/docker-compose.yml, but I've been happy with the process described above so I haven't experimented further.\nYou might get some additional insight by reading through the VS Code docs on this topic.",
    "RUN command in dockerfile produces different result than manually running same commands inside container": "First of all, a little bit of background: the platform detection script which runs during the build uses uname(1) utility (thus uname(2) system call) to identify the hardware it runs on:\nroot@6e4b69adfd4c:/gcc-4.8.5# grep 'uname -m' config.guess \nUNAME_MACHINE=`(uname -m) 2>/dev/null` || UNAME_MACHINE=unknown\nOn your 64-bit machine uname -m returns x86_64. However, there is a system call which allows overriding this result: personality(2). When the process calls personality(2), it and its subsequent forks (children) start seeing the fake results when calling uname(2). So, there is the possibility to ask the kernel to provide the fake hardware information in uname(2).\nThe base image you use (jnickborys/i386-ubuntu:12.04) contains the 32-bit binaries and defines the entrypoint /usr/bin/linux32, which calls personality(PER_LINUX32) to ask the kernel to pretend that it runs on 32-bit hardware and to return i686 in uname(2) (this may be checked using docker inspect and strace respectively). This makes possible to pretend that the containerized process runs in 32-bit environment.\nWhat is the difference between executing the build in RUN directive and manually in the container?\nWhen you execute the build in RUN, Docker does not use the entrypoint to run the commands. It uses what is specified in the SHELL directive instead (default is /bin/sh -c). This means that the personality of the shell running the build is not altered, and it (and the child processes) sees the real hardware information - x86_64, thus, you get x86_64-unknown-linux-gnu build system type in 32-bit environment and the build fails.\nWhen you run the build manually in the container (e.g. after starting it using docker run -it jnickborys/i386-ubuntu:12.04 and then performing the same steps as in the Dockerfile), the entrypoint is called, thus, the personality is altered, and the kernel starts reporting that it runs on 32-bit hardware (i686), so you get i686-pc-linux-gnu build system type, and the build runs correctly.\nHow to fix this? Depends on what do you want. If your goal is to build gcc for 64-bit environment, just use the 64-bit base image. If you want to build for 32-bit environment, one of your options is to alter the SHELL being used for RUNs before these RUNs:\nSHELL [\"/usr/bin/linux32\", \"/bin/sh\", \"-c\"]\nThis will make Docker execute RUNs with altered personality, so, the build system type will be detected correctly (i686-pc-linux-gnu) and the build will succeed. If required, you may change the SHELL back to /bin/sh -c after the build.",
    "Teradata Database Docker Image": "As mention by dnoeth. Currently there's only a VMWare image.",
    "Using a dockerfile argument in a RUN statement": "Well, I found a solution that works but it's definitely not ideal. It appears variables simply aren't expanded in RUN statements (at least on Windows, haven't tried Linux). However, the COPY statement will expand them. So, I can copy my file to a temp file with a hard coded name, and then use that:\nCOPY ./ /inetpub/wwwroot/\nCOPY ${transform} /inetpub/wwwroot/Web.Current.config\nRUN powershell -executionpolicy bypass /Build/Transform.ps1 -xml \"/inetpub/wwwroot/web.config\" -xdt \"/inetpub/wwwroot/Web.Current.config\"\nIn this particular situation, it works for me.\nUpdate: Found another method that works\nThis is perhaps a better method using environment variables:\nFROM someimage\nARG transform\nENV TransformFile ${transform}\n\nCOPY ./ /inetpub/wwwroot/\nRUN powershell -executionpolicy bypass /Build/Transform.ps1 -xml \"/inetpub/wwwroot/web.config\" -xdt \"/inetpub/wwwroot/$ENV:TransformFile\"\nIn this case, Docker will evaluate the parameter transform and set it to the environment variable TransformFile. When I use it in the PowerShell script, it's no longer Docker that's evaluating the argument, but Powershell itself. Thus, the Powershell syntax for interpolating an environment variable must be used.",
    "Dockerfile can't copy files. (\"cp\" command)": "Approach #1\nI believe that the below statements of the Dockerfile can be changed\nFrom:\nADD . /var/www\nWORKDIR /var/www\nRUN cp config.php.sample config.php\nRUN cp shared_config.php.sample shared_config.php\nRUN cp vendor/propel/propel1/build.properties.sample vendor/propel/propel1/build.properties\nTo:\nADD config.php.sample /var/www/config.php\nADD shared_config.php.sample /var/www/shared_config.php\nADD vendor/propel/propel1/build.properties.sample /var/www/vendor/propel/propel1/build.properties \nOf course, you also use COPY instead of ADD as well in the above.\nApproach #2\nChange below statements from:\nRUN cp config.php.sample config.php\nRUN cp shared_config.php.sample shared_config.php\nRUN cp vendor/propel/propel1/build.properties.sample vendor/propel/propel1/build.properties\nTo:\nRUN [\"cp\",  \"config.php.sample\", \"config.php\"]\nRUN [\"cp\", \"shared_config.php.sample\", \"shared_config.php\"]\nRUN [\"cp\", \"vendor/propel/propel1/build.properties.sample\", \"vendor/propel/propel1/build.properties\"]\nFor more details, refer documentation.\nHope this is helpful.",
    "Cannot run PlayWright inside docker image": "I think this is a known issue in Playwright. You can see the detailed discussion here, and particularly see this comment\nThe solution I've seen mostly working is to either delete the node_modules entirely and then re-install it again, or in case of a docker image, try the solution mentioned in this link, by setting the playwright browsers to a specific path",
    "ARM64 Geckodriver for Linux": "anyone having this same issue try this process step by step.\nA) Install Firefox\nsudo apt install firefox\nfirefox --version\nB) geckodriver - For arm64\nsudo apt install firefox-geckodriver\nC) Install selenium\npip3 install selenium\npip3 install --upgrade requests\nD) Script to test\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver import FirefoxOptions\nfrom selenium.webdriver.common.keys import Keys\n\nopts = FirefoxOptions()\nopts.add_argument(\"--headless\")\nbrowser = webdriver.Firefox(options=opts)\nbrowser.get('https://google.com/')\nprint('Title: %s' % browser.title)\ntime.sleep(2)\nbrowser.quit()\n** Tested & working on Ubuntu v20 & arm64",
    "Docker cannot find Resource on classpath": "Seems like you aren't building Fat Jar. You can utilize spring-boot-maven-plugin for that.\n<plugin>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-maven-plugin</artifactId>\n    <version>2.0.1.RELEASE</version>\n</plugin>\nThen change your Dockerfile like:\nFROM java:8\nVOLUME /tmp\nENV tom=dev\nCOPY build/libs/demo-0.0.1-SNAPSHOT.jar /app/app.jar\nWORKDIR /app\nENTRYPOINT [\"java\",\"-Djava.security.egd=file:/dev/./urandom\",\"-Dprofile=${tom}\",\"-jar\",\"app.jar\"]",
    "Create Docker Image For JRE FROM scratch": "The hotspot sources do not currently support statically linking. See http://mail.openjdk.java.net/pipermail/hotspot-dev/2013-September/010810.html for more info.",
    "Docker container fails to run, Error : python3: can't open file 'flask run --host=0.0.0.0': [Errno 2] No such file or directory": "The best use for ENTRYPOINT is to set the image\u2019s main command, allowing that image to be run as though it was that command (and then use CMD as the default flags).\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/#entrypoint\nlots of people seem to miss this point about the ENTRYPOINT and CMD Dockerfile-instructions.\nthe ENTRYPOINT instruction supposed to run some executable, which should run every time you start the container, such as starting your server.\nthe CMD supposed to include the flags provided to that executable, so they can be easily overridden when running the container.\ni am not sure you are supposed to have more then one CMD instruction. if you need to run commands during the build process, you can use the RUN instruction - for example:\nRUN mkdir some/dir\nnow:\nrun.py is the main python flask file for execution\nhence i suggest you define it as your entrypoint:\nENTRYPOINT [ \"./run.py\" ]\ncommands that you may also want to run every time the container starts, such as flask run --host=0.0.0.0 you can:\nmove that command to sit inside the run.py file\nor\nkeep the CMD [ \"flask\", \"run\", \"--host=0.0.0.0\" ] line. this command will be passed as an argument to the run.py entrypoint, so you may execute it in there. that way you can easily override the command when running the container with alternative arguments.\nthis stuff is also in the docs:\nUnderstand how CMD and ENTRYPOINT interact\nBoth CMD and ENTRYPOINT instructions define what command gets executed when running a container. There are few rules that describe their co-operation.\nDockerfile should specify at least one of CMD or ENTRYPOINT commands.\nENTRYPOINT should be defined when using the container as an executable.\nCMD should be used as a way of defining default arguments for an ENTRYPOINT command or for executing an ad-hoc command in a container.\nCMD will be overridden when running the container with alternative arguments.",
    "yum update / apk update / apt-get update not working behind proxy": "It's quite bad idea to set http(s)_proxy as system wide variable. U only need to make that package manager work's over proxy. If u still want to set http(s)_proxy don't forget about no_proxy or all your traffic will try to go via proxy host. For ubuntu i prefer to use something like this\nFROM ubuntu\nARG PROXY=false\nARG PROXY_URL=\"http://proxy:8080\"\n\nRUN if [ \"$PROXY\" = true ] ; then echo 'Acquire::http::Proxy \"'$PROXY_URL'\";' >> /etc/apt/apt.conf ; fi && \\\n  apt-get update && \\\n  apt-get install -y vim\nAnd execute it like this on server without internet connection, but local execute will work without proxy\ndocker build -t ubuntu-with-proxy --build-arg PROXY=true .\nCentos also can handle proxy configuration inside yum.conf\nFROM centos\nARG PROXY=false\nARG PROXY_URL=\"http://proxy:8080\"\n\nRUN if [ \"$PROXY\" = true ] ; then echo 'proxy=\"$PROXY_URL\";' >> /etc/yum.conf ; fi && \\\n  yum install -y vim\nAnd execute it like this on server without internet connection, but local execute will work without proxy\ndocker build -t centos-with-proxy --build-arg PROXY=true .\nBut i can't find such solution for alpine\nI think that something like for Centos/Ubuntu could be achieved in Alpine with this, but i haven't test this yet.\nFROM alpine\nARG PROXY=false\nARG PROXY_URL=\"http://proxy:8080\"\n\nRUN if [ \"$PROXY\" = true ] ; then echo \"http_proxy = $PROXY_URL\" > /etc/wgetrc && echo \"use_proxy = on\" >> /etc/wgetrc ; fi && \\\n  apk add -U vim\nAnd again execution\ndocker build -t alpine-with-proxy --build-arg PROXY=true .",
    "How to read docker env variables into Java code": "This is weird but, restarting the docker container just worked fine for me. Turns out , i have to restart the container when ever I update the network connection using \"--network\". Thanks",
    "Docker build leads to \"no space left on device\" on Windows 10": "I had the same problem on Windows. One of this two settings should solve this problem:\nIncrease Memory that Docker is using. If is it 2GB, add more\nIncrease \"Disk image max size\" - initially is 60, move it to 80 GB\nThis should be enough, but depends on complexity of what you are building. In some cases increase of Swap would be needed. The initial Swap of Docker is 1024 MB\nThanks",
    "docker run with volume changes directory permission to root": "This is because you use the root user inside the container.\nYou can use the -u param to set the user you use outside (e.g. -u `id -u $USER`), but if you need root inside the container, you have to chown it manually.",
    "Unable to run flask app in debug mode using Docker": "A sample (simplifed) runthru demostrating file edits with no need for container restarts outlined below for your reference.\napp.py\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    return \"Hello, World\"\n\nif __name__ == '__main__':\n    app.run(debug=True,host='0.0.0.0',port=80)\nYou will need to specify the port for the flask development server in order to match the exposed container port of 80.\nscreenshot can be viewed here\nSummary of Steps in screenshot (MAC OS X):\nstarting with Empty directory\nCreate app.py\ndocker run\ncurl localhost (this will display Hello, World)\nedit app.py\ncurl localhost (this should display the new edits)",
    "ERROR [internal] load metadata for docker.io/library/python.alpine3.8:latest": "change docker daemon.json \"buildkit\" on the last line to FALSE, like so:\n  {\n        \"builder\": { \"gc\": { \"defaultKeepStorage\": \"20GB\", \"enabled\": true } },\n        \"experimental\": false,\n        \"features\": { \"buildkit\": false }\n    }",
    "Dockerfile COPY vs \"docker run ... cp\"": "Without knowing what the files are for, we can only take wild guesses.\nVolumes are used for data persistence, whereas COPY is used for data that is needed in running the process, but may be destroyed.\nOne possible valid scenario for why data is copied into the volume instead of using the COPY command is that persistent data needs to be initialized and they don't want that initialization data to add bloat to the container image. Like I said, it's just a wild guess.\nAnother guess is that the Dockerfile is shared between developers and the initialization data between groups may vary, so one set of developers might copy different data into the volume.\nWhatever the data is, if you shut down and remove the container, data created via COPY just vanishes with the container, but data moved into the volume via cp on the host stays in the directory that was mounted. That data may have changed while the container was running from what was originally placed in it, but it doesn't reset when you remove the container and spawn a new container from the image.\nYou should ask the developer what all the files are for, and whether the files need to persist or whether they can just be \"ephemeral\". This will probably answer your questions as to why they are copied the way they are.",
    "Exclude a folder from a docker build COPY command without .dockerignore": "As long as .dockerignore is not an option, there are 3 methods that will work:\nExclude the folder from COPY by multiple copy instructions (one for each letter):\n COPY ./file_that_always_exists.txt ./[^s]* .        # All files that don't start with 's'\n COPY ./file_that_always_exists.txt ./s[^o]* .       # All files that start with 's', but not 'so'\n COPY ./file_that_always_exists.txt ./so[^l]* .      # All files that start with 'so', but not 'sol'\n COPY ./file_that_always_exists.txt ./sol[^v]* .     # All files that start with 'sol', but not 'solv'\n COPY ./file_that_always_exists.txt ./solv[^e]* .    # All files that start with 'solv', but not 'solve'\n COPY ./file_that_always_exists.txt ./solve[^r]* .   # All files that start with 'solve', but not 'solver'\nDisadvantage: This will flatten the folders structures, also, imagine that you have more than one folder to do that for :(\nNote, the requirement for file_that_always_exists.txt (e.g. can be Dockerfile) is to avoid the error COPY failed: no source files were specified if there are no files that match the copy step.\nCopy all folders and then in a different layer delete the unwanted folder:\n COPY . .\n RUN rm -rf ./solver\nDisadvantage: The content of the folder will still be visible in the Docker image, and if you are trying to decrease the image size this will not help.\nManually specify the files and folders you would like to copy (\ud83d\ude2d):\n COPY [\"./file_to_copy_1.ext\", \"file_to_copy_2.ext\", \"file_to_copy_3.ext\", \".\"]\n COPY ./folder_to_copy_1/ ./folder_to_copy_1/\n # ...\n COPY ./folder_to_copy_n/ ./folder_to_copy_n/\nDisadvantage: You have to manually write all files and folders, but more annoyingly, manually update the list when the folder hierarchy changes.\nEach method has it's own advantages and disadvantages, choose the one that most fit your application requirements.",
    "setfacl in Dockerfile has no effect": "Any idea why docker does not correctly apply the acl changes when running setfacl in the Dockerfile?\nDon't take this as an authoritative answer, because I'm just guessing.\nDocker images have to run on a variety of distributions, with different storage backends (possibly even more when you facter in image registries, like hub.docker.com). Even those that are filesystem based may be backed by different filesystems with different capabilities.\nThis means that in order for Docker images to run reliably and reproducibly in all situations, they have to minimize the number of extended filesystem features they preserve.\nThis is probably why the extended attributes necessary to implement filesystem ACLs are not preserved as part of the image.\nIt works in a container because at this point the files are stored on a specific local filesystem, so you can take advantage of any features supported by that filesystem.",
    "Docker-compose: error when trying to run mongo image": "change your docker-compose.yml file to:\nversion: '2'\nservices:\n    db:\n        image: mongo:3.4.10\n        command: mongod --dbpath /data/db\n        volumes:\n            - ./data/mongodb:/data/db",
    "Build go dependencies in separate Docker layer": "I've got a nasty hack that seems to work:\nFROM golang:1.12\n\nWORKDIR /src\n\nCOPY go.mod go.sum ./\nCOPY vendor/ ./vendor\nRUN go build -mod=vendor $(cat deps|grep -v mypackage | grep -v internal)\n\nCOPY . .\nRUN go build -mod=vendor\n...\ngo list -f '{{join .Deps \"\\n\"}}'  > deps\ndocker build .",
    "Access raspistill / pi camera inside a Docker container": "I've had the same problem trying to work with camera interface from docker container. With suggestions in this thread I've managed to get it working with the below dockerfile.\nFROM node:12.12.0-buster-slim\n\nEXPOSE 3000\n\nENV PATH=\"$PATH:/opt/vc/bin\"\n\nRUN echo \"/opt/vc/lib\" > /etc/ld.so.conf.d/00-vcms.conf\n\nCOPY \"node_modules\" \"/usr/src/app/node_modules\"\nCOPY \"dist\" \"/usr/src/app\"\n\nCMD ldconfig && node /usr/src/app/app.js\nThere are 3 main points here:\nAdd /opt/vc/bin to your PATH so that you can call raspistill without referencing the full path.\nAdd /opt/vc/lib to your config file so that raspistill can find all dependencies it needs.\nReload config file (ldconfig) during container's runtime rather than build-time.\nThe last point is the main reason why Anton's solution didn't work. ldconfig needs to be executed in a running container so either use similar approach to mine or go with entrypoint.sh file instead.",
    "curl doesn't work during docker build": "I could workaround the problem doing something like this:\ndocker build --add-host central.maven.org:151.101.56.209 .\nbut I'm not happy with that. I would like to say Docker to use my DNS instead of set fixed IP. It would be more elegant.",
    "unable to access environment variables from docker compose env file": "You are not setting ENV at build time, so docker-compose will able to read env file as you set configuration in the compose file, but docker run will not read the env as you need to specify env file\nYou need to specify env-file in docker run command\ndocker run --env-file .env my-app-test app.py\nor to just check ENV\ndocker run --env-file .env --entrypoint printenv my-app-test\nor run the stack\ndocker-compose up",
    "Docker image search using SHA hash": "You could list all the images with docker images and find a particular one:\ndocker images --no-trunc -q | grep <image_hash>\nOr you want to search via a chunk of hash number:\ndocker images -q | grep <image_hash>",
    "Dockerfile parse error line 7: COPY requires at least two arguments, but only one was provided. Destination could not be determined": "The full, proper way to do it is:\nCOPY --chown=node:node . ./\nThat is because COPY expects 2 arguments (as the error message says), it just doesn't like the second bare dot.",
    "Keeping node_modules up to date in docker": "This is an old question but in case anyone else comes here with the same issue, I struggled with this too and this is how I could achieve the desired behaviour:\ndocker-compose up --build --renew-anon-volumes\nAccording to the documentation on docker-compose up https://docs.docker.com/engine/reference/commandline/compose_up/\nOption Short Description\n--renew-anon-volumes -V Recreate anonymous volumes instead of retrieving data from the previous containers.\nWith this, you not only build a new image, but also override the original volume used to store node_modules. Note that it doesn't work with named volumes. If you use named volumes, you'd need to manually remove that volume with docker volume rm <volume-name> before running docker-compose up again.\nThe understanding of how volumes work from the original question seems sound. Readers may also find the following discussion in the Github issue beneficial to understanding it.\nhttps://github.com/moby/moby/issues/30647#issuecomment-276882545",
    "How to set container id when I run docker-compose up?": "the -t option to docker build doesn't set something called CONTAINER ID. In fact, it has nothing to do with a container. The output of docker build is an image, which is named based on the -t option. docker build -t myorg:myimage . creates an image called myorg:myimage that you can use later to build containers, or push to the docker registry so that you can later use it to build a container.\nThe equivalent in docker-compose is docker-compose build, not docker-compose up. To specify an image tag in docker-compose build, you use both the build and the image tags on a service in the compose file- in that case, using docker-compose build will build the image based on the build directive, and tag the output using the image tag.",
    "Failed to solve with frontend xxx: rpc error: code = Unknown desc = (...) out: `exit status 2: gpg: decryption failed: No secret key`": "try download first the docker image and run command for build image, it worked me",
    "Enable gpu support by default on docker containers": "Here's the right config to set in /etc/docker/daemon.json :\n{\n    \"runtimes\": {\n        \"nvidia\": {\n            \"args\": [],\n            \"path\": \"nvidia-container-runtime\"\n        }\n    },\n    \"default-runtime\": \"nvidia\"\n}\nDon't forget to sudo service docker restart",
    "How to set multiple commands in one yaml file with Kubernetes?": "command: [\"/bin/sh\",\"-c\"]\nargs: [\"command one; command two && command three\"]\nExplanation: The command [\"/bin/sh\", \"-c\"] says \"run a shell, and execute the following instructions\". The args are then passed as commands to the shell. In shell scripting a semicolon separates commands, and && conditionally runs the following command if the first succeed. In the above example, it always runs command one followed by command two, and only runs command three if command two succeeded.\nAlternative: In many cases, some of the commands you want to run are probably setting up the final command to run. In this case, building your own Dockerfile is the way to go. Look at the RUN directive in particular.",
    "Docker compose build error - Project file does not exist": "you have issues with your working directory try using absolute path instead of relative path in your working directory in docker file",
    "Missing `node_modules` in Docker Container": "Then why don't you create this directory at build (ie adding the following line in your Dockerfile before RUN npm install) ?\nRUN mkdir <workdir>/node_modules\n(Replace <workdir> with the actual default working dir of the keymetrics/pm2 image)",
    "How to override docker-compose values in multiple combined files?": "I would actually strongly suggest just not using the \"target\" command in your compose files. I find it to be extremely beneficial to build a single image for local/staging/production - build once, test it, and deploy it in each environment. In this case, you change things using environment variables or mounted secrets/config files.\nFurther, using compose to build the images is... fragile. I would recommend building the images in a CI system, pushing them to a registry, and then using the image version tags in your compose file- it is a much more reproducible system.",
    "Docker stuck at npm install": "So, this might be a temporary solution till this is properly addressed, but you can use\nnpm config set registry http://registry.npmjs.org/\nI have used it for docker environment and it worked fine.",
    "Setup git via windows docker file": "I've solved issue with GUI through usage of MinGit and by putting information about mingit into environment/path variable. I've used following approach:\nRUN Invoke-WebRequest 'https://github.com/git-for-windows/git/releases/download/v2.12.2.windows.2/MinGit-2.12.2.2-64-bit.zip' -OutFile MinGit.zip\n\nRUN Expand-Archive c:\\MinGit.zip -DestinationPath c:\\MinGit; \\\n$env:PATH = $env:PATH + ';C:\\MinGit\\cmd\\;C:\\MinGit\\cmd'; \\\nSet-ItemProperty -Path 'HKLM:\\SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Environment\\' -Name Path -Value $env:PATH",
    "MySQL in Docker frozen at root password config": "The accepted answer may be true in some abstract sense, but it's completely irrelevant to the matter at hand. You need a way to specify the password statically. And unless you are using the official image, you'll need that whether or not you follow the \"one process, one container\" dogma.\nThe answer here tells how, but it leaves out a key setting: you still have to tell debconf to use the Noninteractive front-end, as described here.\nHere's an example of a working Dockerfile based on the above.\nFROM ubuntu:latest\nMAINTAINER Jonathan Strange <jstrange@norrell.edu>\nRUN apt-get update \\\n    && apt-get install -y apt-utils \\                                           \n    && { \\\n        echo debconf debconf/frontend select Noninteractive; \\\n        echo mysql-community-server mysql-community-server/data-dir \\\n            select ''; \\\n        echo mysql-community-server mysql-community-server/root-pass \\\n            password 'JohnUskglass'; \\\n        echo mysql-community-server mysql-community-server/re-root-pass \\\n            password 'JohnUskglass'; \\\n        echo mysql-community-server mysql-community-server/remove-test-db \\\n            select true; \\\n    } | debconf-set-selections \\\n    && apt-get install -y mysql-server apache2 python python-django \\\n        python-celery rabbitmq-server git\nThis is not too terribly different from what the official Dockerfile does -- though they handle the actual password config somewhat differently.\nSome people have had success by setting the DEBIAN_FRONTEND environment variable to noninteractive, like so:\nENV DEBIAN_FRONTEND noninteractive\nHowever, that doesn't seem to work in all cases. Using debconf directly has proven more reliable for me.",
    "Docker building issue: Error checking context: can't stat": "I had a similar issue until recently . On 18.04, Ubuntu installs Docker via Snap.\nBy default, SNAP packages are not allowed to access /media, or any other / root folder. The SNAP sandbox will deny access unless you have explicitly granted access to removable-media. ( /mnt or /media )\nTo grant docker access to removable-media:\nsudo snap connect docker:removable-media\nAlternatively, move your project into your /home/user directory and try building from there.",
    "/bin/sh: 1: [\u201cnpm\u201d,: not found on docker-compose up [duplicate]": "You have the wrong quotes in your dockerfile:\napp    | /bin/sh: 1: [\u201cnpm\u201d,: not found\ndoesn't match the quotes in the example you pasted:\nCMD [\"npm\", \"start\"]\nDouble check your Dockerfile to correct your quotes.",
    ".bash_profile does not work with docker php image": "Had a similar issue and the easiest solution was to use the -l option to bash to make bash act as if it had been invoked as a login shell.\ndocker run --rm -it $IMAGE /bin/bash -l\nbash will then read in ~/.bash_profile",
    "Restore a SQL Server DB.bak in a Dockerfile": "A friend and I puzzled through this together and eventually found this solution. Here's what the docker file looks like:\nFROM microsoft/mssql-server-linux\nENV MSSQL_SA_PASSWORD=myPassword\nENV ACCEPT_EULA=Y\n\nCOPY ./My_DB.bak /var/opt/mssql/backup/My_DB.bak\nCOPY restore.sql restore.sql\nRUN (/opt/mssql/bin/sqlservr --accept-eula & ) | grep -q \"Starting database restore\" && /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P 'myPassword' -d master -i restore.sql\n*Note that I moved the SQL restore statement to a .sql file.",
    "Docker: Set export PATH in Dockerfile": "$PATH is a list of actual directories, e.g., /bin:/usr/bin:/usr/local/bin:/home/dmaze/bin. No expansion ever happens while you're reading $PATH; if it contains ~/bin, it looks for a directory named exactly ~, like you might create with a shell mkdir \\~ command.\nWhen you set $PATH in a shell, PATH=\"~/bin:$PATH\", first your local shell expands ~ to your home directory and then sets the environment variable. Docker does not expand ~ to anything, so you wind up with a $PATH variable containing a literal ~.\nThe best practice here is actually to avoid needing to set $PATH at all. A Docker image is an isolated filesystem space, so you can install things into the \"system\" directories and not worry about confusing things maintained by the package manager or things installed by other users; the Dockerfile is the only thing that will install anything.\nRUN pip install awscli\n# without --user\nBut if you must set it, you need to use a Dockerfile ENV directive, and you need to specify absolute paths. ($HOME does seem to be well-defined but since Docker containers aren't usually multi-user, \"home directory\" isn't usually a concept.)\nENV PATH=\"$HOME/.local/bin:$PATH\"\n(In a Dockerfile, Docker will replace $VARIABLE, ${VARIABLE}, ${VARIABLE:-default}, and ${VARIABLE:+isset}, but it doesn't do any other shell expansion; path expansion of ~ isn't supported but variable expansion of $HOME is.)",
    "Vert.x based application crashes on docker container": "Judging by FileResolver.java, vert.x tries to create a \".vertx\" directory in the current working directory by default. You have configured a user called \"daemon\", are you sure that this user has write access to the working dir in the docker image? If not, change the permissions as outlined in docker-image-author-guidance, or revert to using the root user.",
    "How to fix 'Hash Sum Mismatch' in Docker on mac": "Found this answer from here https://forums.docker.com/t/hash-sum-mismatch-writing-more-data-as-expected/45940/2 This bothered me for a day\nRUN echo \"Acquire::http::Pipeline-Depth 0;\" > /etc/apt/apt.conf.d/99custom && \\\n    echo \"Acquire::http::No-Cache true;\" >> /etc/apt/apt.conf.d/99custom && \\\n    echo \"Acquire::BrokenProxy    true;\" >> /etc/apt/apt.conf.d/99custom\n\nRUN apt-get update && apt-get upgrade -y \\\n    && apt-get install -y \\",
    "set NODE_ENV variable in production via docker file": "Remove the equal sign\nENV NODE_ENV production",
    "Docker: Can't read class path resource from spring boot application": "Since you are using springboot you can try to use the following annotation for loading your classpath resource. Worked for me because I had the same exception. Be aware that the directory \"location\" must be under the src/main/resources folder:\n@Value(\"classpath:/location/GeoLite2-City.mmdb\")\nprivate Resource geoLiteCity;\nWithout springboot you could try:\ntry (InputStream inputStream = getClass().getClassLoader().getResourceAsStream(\"/location/GeoLite2-City.mmdb\")) {\n... //convert to file and other stuff\n}\nAlso the answers before were correct as the use of \"/\" is not good at all and File.separator would be the best choice.",
    "Cant access my Docker DotNet core website": "Finally.\nI found this blog post: http://dotnetliberty.com/index.php/2015/11/26/asp-net-5-on-aws-ec2-container-service-in-10-steps/\nEven though it used an old version of the dotnet core there was an important point I had overseen;\nNotice that I\u2019ve provided an extra parameter to the dnx web command to tell it to serve on 0.0.0.0 (rather than the default localhost). This will allow our web application to serve requests that come in from the port forwarding provided by Docker which defaults to 0.0.0.0.\nWhich is pretty damn important.\nSolution:\nvar host = new WebHostBuilder()\n            .UseKestrel()\n            .UseStartup<Startup>()\n            .UseUrls(\"http://0.0.0.0:5000\")\n            .Build();\nOld code:\nvar host = new WebHostBuilder()\n            .UseKestrel()\n            .UseStartup<Startup>()\n            .UseUrls(\"http://localhost:5000\")\n            .Build();\nWhich is wery frustrating since it seemed to work perfect in Linux and windows and app starting up in Docker, but never getting any requests. Hope this helps some other poor souls out there :)",
    "Visual Studio generated Dockerfile does not work with manual docker build": "Your command fails, because the build context is wrong. Visual studio is using the solution root folder as build context, you are (probably) using the project's dockerfile's location. You can read more about the docker build command here.\nYour command should look similar to this:\ndocker build -f \"<path-to-your-dockerfile>\" -t some-name \"<!!!path-to-your-solution-folder!!!>\"\nYou can see the exact command executed by visual studio in the output window, with \"Container Tools\" selected from the dropdown box.",
    "Docker includes invalid characters \"${PWD}\" for a local volume name": "Path expansion is different in each shell.\nFor PowerShell use: ${pwd} \nFor cmd.exe \"Command Prompt\" use: %cd%\nbash, sh, zsh, and Docker Toolbox Quickstart Terminal use: $(pwd) \nNote, if you have spaces in your path, you'll usually need to quote the path.\nAlso answered here: Mount current directory as a volume in Docker on Windows 10",
    "Docker : execute commands as a non-root user": "1- Execute docker command with non-root user\nIf this is your case and don't want to run docker command with root user, follow this link . create a docker group and add your current user to it.\n$ sudo groupadd docker\n$ sudo usermod -aG docker $USER\n2- Execute commands inside docker! with non-root user\nIf I'm right you want to use a non-root user inside docker not the root!\nThe uid given to your user in the docker is related to the root docker images you are using, for example alphine or ubuntu:xenial as mentioned in this article\nBut you can simple change the user inside docker by changing a little bit as follow in your Dockerfile and add a new user and user it. like this:\n RUN adduser -D myuser\n USER myuser\n ENTRYPOINT [\u201csleep\u201d]\n CMD [\u201c1000\u201d]\nthen in the docker file, if you gain the /bin/bash and execute id command in it, you will see that the id of user inside docker is changed.\nUpdate:\nIf you have a ready to use Dockerfile, then create a new Dockerfile, for example it's name is myDocker, and put code below in it:\n from myDockerfile\n RUN adduser -D myuser\n USER myuser\n ENTRYPOINT [\u201csleep\u201d]\n CMD [\u201c1000\u201d]\nthen save this file,and build it:\n$ docker build -t myNewDocker .\n$ docker run myNewDocker <with your options>",
    "invalid header field value \"oci runtime error while running docker image": "Your docker-entrypoint.sh isn't executable, you need to add a RUN chmod 755 /docker-entrypoint.sh after the COPY command in your Dockerfile and rebuild the image.",
    "nginx: [emerg] open() \"/run/nginx.pid\" failed (13: Permission denied)": "UPDATE\nIn order to fix my \"/var/run/nginx.pid\" permission denied error.\nI had to add nginx.pid permission errors inside my dockerfile for the new user to work.\nBelow are the changes i made in my dockerfile\nRUN touch /run/nginx.pid \\\n && chown -R api-gatway:api-gatway /run/nginx.pid /cache/nginx",
    "Docker-compose args are not passed to Dockerfile": "There are two problems here. ARG is only used at build time, when creating the image, and CMD defines a step at run time, when running your container. ARG gets implemented as an environment variable for RUN steps, so it is up to the shell to expand the environment variable. And the json syntax doesn't run a shell. So to do this with CMD, you need to make two changes.\nFirst, you need to save your ARG as an ENV value that gets saved to the image metadata and used to setup the environment when creating the container.\nAnd second, you need to switch from an exec/json syntax for running CMD to run a shell that will expand these variables. Docker does this for you with the string syntax.\nThe end result looks like:\nFROM golang:1.11\n\n// stuff...\n\nARG broker\nENV broker=${broker}\nARG queue\nENV queue=${queue}\n\nCMD go run /go/src/github.com/org/project/cmd/some-service/some-service.go --broker \"$broker\" --queue \"$queue\"\nAs an aside, you should also note that every argument in exec syntax needs to be a separate array entry, e.g.:\nCMD [\"go\", \"run\", \"/go/src/github.com/org/project/cmd/some-service/some-service.go\", \"--broker $broker\", \"--queue $queue\"]\nis similar to running:\ngo run /go/src/github.com/org/project/cmd/some-service/some-service.go \"--broker $broker\" \"--queue $queue\"\nwhen you really wanted to run:\nCMD [\"go\", \"run\", \"/go/src/github.com/org/project/cmd/some-service/some-service.go\", \"--broker\", \"your_broker\", \"--queue\", \"your_queue\"]\nwhich would be similar to:\ngo run /go/src/github.com/org/project/cmd/some-service/some-service.go --broker \"your_broker\" --queue \"your_queue\"\n(Note I removed the variables from my example because they do not work in the exec syntax.)",
    "Docker build image - glob error { Error: EPERM: operation not permitted, scandir": "The map_files directory is a representation of the files a process currently has memory mapped by the kernel. This info is also contained in the maps file in the same directory.\nAs these files are a representation of memory, they change frequently. If a process creates a directory listing and then processes the list, the files might not exist by the time the process gets to them.\nIf the build is reporting files in /proc, a search has likely started from the / directory in the container and is recursively searching everything on the filesystem.\nUse a directory other than / as the WORKDIR in your Dockerfile\nFROM node:latest\n\nWORKDIR /app\nCOPY package.json npm-shrinkwrap.json /app/\nRUN npm install --production\nCOPY . /app/\nEXPOSE 8080\n\nRUN npm run deploy",
    "How does Java application know it is running within a Docker container": "Solution\nCheck control group of the init process simply by /proc/1/cgroup .\nif it is initiated normally all hierarchies have in / value\nif it is initiated from docker container they have /docker/<container_id> value.\nWhen running inside docker /proc/1/cgroup has values similar to :\n11:perf_event:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n10:memory:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n9:cpuset:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n8:net_cls,net_prio:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n7:pids:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n6:cpu,cpuacct:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n5:blkio:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n4:freezer:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n3:hugetlb:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n2:devices:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n1:name=systemd:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\nNote: As @JanisKirsteins informed me, If you run your application in amazon ec2 you might want to change the condition to line.contains(\"/ecs\") instead. because in /proc/1/cgroups you will find pattern similar to: /ecs/<uuid>/<uuid>\nIn Java\npublic static Boolean isRunningInsideDocker() {\n\n        try (Stream < String > stream =\n            Files.lines(Paths.get(\"/proc/1/cgroup\"))) {\n            return stream.anyMatch(line -> line.contains(\"/docker\"));\n        } catch (IOException e) {\n            return false;\n        }\n    }\nLive code checking\noutside docker: running outside docker\ninside docker : running inside docker\nMore Info\nhttps://tuhrig.de/how-to-know-you-are-inside-a-docker-container/\nHow to determine if a process runs inside lxc/Docker?\nHow to check if a process is running inside docker container",
    "Docker Error response from daemon: Error processing tar file(exit status 1): no space left on device": "first run docker image prune to clean up all dangling images\nif this didn't help you might need to check this answer\nDocker error : no space left on device",
    "how to include class library reference into docker file": "You need to move the Dockerfile to the solution level, and reference the projects using [Folder]/[Project].csproj. Here is my Dockerfile (located in the solution level):\nFROM mcr.microsoft.com/dotnet/aspnet:3.1 AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/sdk:3.1 AS build\nWORKDIR /src\nCOPY [\"SumApi/SumApi.csproj\", \"SumApi/\"]\nCOPY [\"Shared/Shared.csproj\", \"Shared/\"]\nRUN dotnet restore \"SumApi/SumApi.csproj\"\nCOPY . .\nWORKDIR \"/src/SumApi\"\nRUN dotnet build \"SumApi.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"SumApi.csproj\" -c Release -o /app/publish\n\nFROM base AS final\nWORKDIR /app`enter code here`\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"SumApi.dll\"]\nLearned it from here: https://imranarshad.com/dockerize-net-core-web-app-with-dependent-class-library/",
    "Failed to parse 'app.server' as an attribute name or function call when running docker": "I fixed this error on a coworker's app where they had the following in app.py:\nserver = Flask(__name__) # define flask app.server\napp = dash.Dash(\n    __name__,\n    server=server,\n)\nAnd then in an index.py they were doing:\nfrom app import app\n\nserver = app.server # I add this part here\nAnd then the Docker CMD was:\nCMD [\"gunicorn\", \"index:server\", \"-b\", \":8050\"]\nAnd it worked.",
    "Dockerfile for tesseract 4.0": "The solution was to upgrade to Ubuntu 18.04:\nFROM ubuntu:18.04\nRUN apt-get update \\\n    && apt-get install tesseract-ocr -y \\\n    python3 \\\n    #python-setuptools \\\n    python3-pip \\\n    && apt-get clean \\\n    && apt-get autoremove\n\nADD . /home/App\nWORKDIR /home/App\nCOPY requirements.txt ./\nCOPY . .\n\nRUN pip3 install -r requirements.txt\n\nVOLUME [\"/data\"]\nEXPOSE 5000 5000\nCMD [\"python3\",\"OCRRun.py\"]",
    "How to synchronize host folder in container folder with Docker": "You have to map volume of your docker container directory with host directory.\nFor example :\ndocker run -v <host_dir>:<container_dir> -other options imagename \nHere both directory synchronised vice or versa.\nHost directory and container directory must be available.",
    "pip install from custom whl file in Dockerfile": "Install all wheels without listing them explicitly:\npip install /path/to/*.whl",
    "E: Unable to locate package redis-server": "I am able to install redis-server as well as python. I added RUN apt-get update in Dockerfile. It updated and got redis installed. And there was one more thing in my case. I had already run 'apt-get update' which created an image before. All the time it was referring to the image and was not updating. Hence I used --no-cache=True and made it.\nFROM ubuntu:14.04\n\nRUN apt-get update\n\nRUN apt-get -y install redis-server",
    "How can I set the time zone in Dockerfile using gliderlabs/alpine:3.3": "The usual workaround is to mount /etc/localtime, as in issue 3359\n$ docker run --rm busybox date\nThu Mar 20 04:42:02 UTC 2014\n$ docker run --rm -v /etc/localtime:/etc/localtime:ro  busybox date\nThu Mar 20 14:42:20 EST 2014\n$ FILE=$(mktemp) ; echo $FILE ; echo -e \"Europe/Brussels\" > $FILE ; docker run --rm -v $FILE:/etc/timezone -v /usr/share/zoneinfo/Europe/Brussels:/etc/localtime:ro busybox date\n/tmp/tmp.JwL2A9c50i \nThu Mar 20 05:42:26 CET 2014\nThe same thread mentions (for ubuntu-based image though), but you already tried it.\nRUN echo Europe/Berlin > /etc/timezone && dpkg-reconfigure --frontend noninteractive tzdata\n(And I referred before to a similar solution)\nAnother option would be to build your own gliderlabs/docker-alpine image with builder/scripts/mkimage-alpine.bash.\nThat script allows you to set a timezone.\n    [[ \"$TIMEZONE\" ]] && \\\n        cp \"/usr/share/zoneinfo/$TIMEZONE\" \"$rootfs/etc/localtime\"\nYou can see that image builder script used in Digital Ocean: Alpine Linux:\nGenerate Alpine root file system\nEnsure Docker is running locally.\nDownload and unzip gliderlabs/docker-alpine.\nwget -O docker-alpine-master.zip https://github.com/gliderlabs/docker-alpine/archive/master.zip\nunzip docker-alpine-master.zip\nBuild the builder (export the right timezone first).\nexport TIMEZONE=xxx\ndocker build -t docker-alpine-builder docker-alpine-master/builder/\nBuild the root file system (change v3.3 to the Alpine version you want to build).\ndocker run --name alpine-builder docker-alpine-builder -r v3.4\nCopy the root file system from the container.\ndocker cp alpine-builder:/rootfs.tar.gz .\nOnce you have the rootfs.tar.gz on your own filesystem, you can use it (as mentioned here) to build your own Alpine image, with the following Dockerfile:\nFROM SCRATCH\nADD rootfs.tar.gz /\nOnce built, you can use that Alpine image with the right timezone.",
    "How can I print the value of a variable, either env or arg, at \"docker build\" time?": "Maybe I'm missing something, but can't you just echo it?\nIf you take this Dockerfile\nFROM ubuntu\nARG test\nRUN echo $test\nand build it with\ndocker build --build-arg test=hello . --progress=plain\nit prints hello.",
    "M1 Mac Docker Issues with apt-get update": "The package list grabbed by the curl command references the architecture of your system, and as there are no mssql-tools or msodbcsql17 packages for arm64 architecture at the moment, I was getting the unable to locate error. Current solution is changing the first line of the Dockerfile to specify the platform with FROM --platform=linux/amd64 python:3.7",
    "How to import a mysql dump file into a Docker mysql container": "The answer to your question is given in the docker hub page of MySQL.\nInitializing a fresh instance\nWhen a container is started for the first time, a new database with the specified name will be created and initialized with the provided configuration variables. Furthermore, it will execute files with extensions .sh, .sql and .sql.gz that are found in /docker-entrypoint->initdb.d. Files will be executed in alphabetical order. You can easily populate your mysql services by mounting a SQL dump into that directory and provide custom images with contributed data. SQL files will be imported by default to the database specified by the MYSQL_DATABASE variable.\nIn your docker-compose.yml use:\nvolumes:\n  - ${PWD}/config/start.sql:/docker-entrypoint-initdb.d/start.sql\nand that's it.",
    "NestJS minimize dockerfile": "For reducing docker image size you can use\nMulti-stage build\nNpm prune\nWhile using multi-stage build you should have 2(or more) FROM directives, as usual, the first stage does build, and the second stage just copies build from the first temporary layer and have instructions for run the app. In our case, we should copy dist & node_modules directories.\nThe second important moment its correctly split dependencies between 'devDependencies' & 'dependencies' in your package.json file.\nAfter you install deps in the first stage, you should use npm prune --production for remove devDependencies from node modules.\nFROM node:12.14.1-alpine AS build\n\n\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\nCOPY . ./\n\nRUN npm run build && npm prune --production\n\n\nFROM node:12.14.1-alpine\n\nWORKDIR /app\nENV NODE_ENV=production\n\nCOPY --from=build /app/dist /app/dist\nCOPY --from=build /app/node_modules /app/node_modules\n\nEXPOSE 3000\nENTRYPOINT [ \"node\" ]\nCMD [ \"dist/main.js\" ]\nIf you have troubles with node-gyp or just want to see - a full example with comments in this gist:\nhttps://gist.github.com/nzvtrk/cba2970b1df9091b520811e521d9bd44\nMore useful references:\nhttps://docs.docker.com/develop/develop-images/multistage-build/\nhttps://docs.npmjs.com/cli/prune",
    "Java Testcontainers - Cannot connect to exposed port": "There's some good advice in other answers; I'll augment these with a couple of other tips:\nAs already suggested:\nabsolutely do add the LogConsumer so that you can see the container's log output - maybe something useful will appear there, now or in the future. It's always good to have.\nset a breakpoint after the container is started, just before you start your client.\nAdditionally, I'd hope the following things make the difference. While paused at the breakpoint:\nrun docker ps -a in the terminal\nfirstly, check that your container is running and hasn't exited. If it has exited, have a look at the logs for the container from the terminal.\nsecondly, check the port mapping in the docker ps output. You should see something like 0.0.0.0:32768->24999/tcp (first port number is random, though).\nevaluate container.getFirstMappedPort() in your IDE and check that the port number you get back is the same as the randomised exposed port. Unless you have a very unusual Docker installation on your local machine, this container should be reachable at localhost: + this port.\nif you've got this far then it's likely that there's something wrong with either the container or the client code. You could try connecting a different client to the running container - even something like nc could help if you don't have another POP3 client handy.\nAnother thing to try is to run the container manually, just to reduce the amount of indirection taking place. The Testcontainers code snippet you've given is equivalent to:\ndocker run -p 24999 immerfroehlich/emailfilter:latest\nYou might find that this helps you divide up the problem space into smaller pieces.",
    "Docker - Writing python output to a csv file in the current working directory": "You can bind your host directory, I would suggest using a WORKDIR & replace ADD with COPY -\nDOCKERFILE\nFROM python:3\nWORKDIR /mydata\nCOPY scraper.py ./\nRUN pip install pandas\nCMD [\"python3\",\"./scraper.py\"]\nRun it -\ndocker run -v ${PWD}:/data ex_scraper\nYou should now be able to see the CSV in your current directory on host.",
    "How to do --rm and --restart in dockerfile?": "Dockerfile is used to specify instructions to build an image.\nOnce the image is built, you can start a container from that image using docker run command. --rm and --restart are options for docker run, which means those commands apply to a container. Using the --restart flag you can specify a restart policy for a container. --rm flag is used to remove the file system on the container when it exits.\nI hope you can see why the functionality provided by those two flags doesn't belong in the Dockerfile. If not, you should really read more about Docker (esp. images vs containers).\nADDITION:\n--rm removes the file system and cleans up the container. restart is used only to restart a container, and the file system disappearing between restarts would be extremely unpleasant. Also note that a restart after file system removal is more like a \"fresh start\" than a \"restart\". So basically they're mutually exclusive. Using them together will result in an error. Doesn't matter where you do it.",
    "Docker Ubuntu image missing prompt, color and completion?": "You're missing -t flag to allocate a pseudo-tty for your container:\ndocker run -it --name=\"TEST\" ubuntu:14.04 /bin/bash",
    "Windows Docker: Issues installing Visual C++ Redistributable in Windows Docker": "I found I was able to install this onto a docker image with the following code:\nSHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';\"]\nRUN [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12; `\n    Invoke-WebRequest \"https://aka.ms/vs/17/release/vc_redist.x64.exe\" -OutFile \"vc_redist.x64.exe\"; `\n    Start-Process -filepath C:\\vc_redist.x64.exe -ArgumentList \"/install\", \"/passive\", \"/norestart\" -Passthru | Wait-Process; `\n    Remove-Item -Force vc_redist.x64.exe;\nSource: https://github.com/microsoft/dotnet-framework-docker/issues/15",
    "Docker build cache keeps growing": "The build cache is part of buildkit, and isn't visible as images or containers in docker. Buildkit itself talks directly to containerd, and only outputs the result to docker. You can prune the cache with:\ndocker builder prune\nAnd there are flags to keep storage based on size and age. You can see more in the docker documentation: https://docs.docker.com/engine/reference/commandline/builder_prune/",
    "How to fix error occurring during Docker image build: \"E: Unsupported file /tmp given on commandline\"": "you need to edit the last line in apt-get command change less \\ to less\ndocker thinks that RUN cd \"/tmp\" is a parameter for apt-get\nanyway you should use WORKDIR if you want to use /tmp for further steps",
    "I am getting max depth exceeded while building the docker image for Microsoft SQL stored procedure": "This happens because your Dockerfile has exceeded the 125 layers limit for docker images.\nIf you add the Dockerfile to the question I can help you to simplify it. As a first suggestion, try to group commands in only one RUN",
    "What if I change a Dockerfile while a build is running out of it?": "Not until you re-run it. The first version of the Dockerfile would be in memory and have no awareness of your changes.",
    "Error: Cannot find module @rollup/rollup-linux-x64-musl on docker container": "I encountered the same issue when trying to dockerize a react app using vite.\nI managed to solve the issue by adding a named volume to the docker-compose.yml file, like this:\ndashboard:\n    container_name: dashboard\n    image: dashboard\n    depends_on:\n      - postgres\n    build:\n      context: ../../packages/dashboard\n      dockerfile: Dockerfile\n    ports:\n      - \"5173:5173\"\n    volumes:\n      - ../../packages/dashboard:/app\n      - node_modules:/app/node_modules\nvolumes:\n  pgdata: {}\n  node_modules: {}\nTo be completely honest, I don't know why it helped. I queried Perplexity (the LLM tool) and it suggested that it has something to do with ARM64 processors (I'm using M2 Pro Mackbook Pro).\nPerplexity's explanation goes like this:\nThe root cause seems to be that when you mount your local node_modules directory into the Docker container using the volumes section, it tries to use the binaries from your local machine, which are likely compiled for a different architecture (e.g., x86_64) and are incompatible with the ARM64 architecture inside the container.\nOne of its references was this GitHub discussions about the same problem.",
    ".net 6 minimal api docker issue: error CS5001: Program does not contain a static 'Main' method suitable for an entry point": "The error message is not explicative.\nThe real error is on the line of COPY ./src/ ./. With this cmd you copy the CONTENT of the src folder (test) into the ROOT of the container.\nIt should be COPY ./src/ /src/ or (better) ./src /src\nTry to comment out ALL except the first 9 rows, build the container and run\ndocker run --rm -it <imagename> /bin/sh to see yourself.\nSince you try to build an empty folder the compiler raise the error that not found the main method (in reality it not find anything...)",
    "Set today's date as environment variable": "ARG should be what you are looking for:\nFROM base\n\n# to be able to use in Dockerfile\nARG now\n\n# to store the value as environment variable in the image\nENV build_date=$now\nNow you can build this with\n# pass value explicitly\ndocker build --build-arg now=\"$(date +%Y%m%d)\" .\n\n# pass value from environment\nexport now=\"$(date +%Y%m%d)\"\ndocker build --build-arg now .\nThis still requires to run date on the host since doing this inside the Dockerfile is not possible unfortunately:\nThe only way to execute arbitrary commands in the build is within a RUN statement; but\nThe only way to persist a variable into the environment of an image/container is with an ENV statement which can only use environment variables from outside the build\nYou could use a custom ENTRYPOINT tough and inject the date to the environment from a file:\nFROM base\n\nRUN date +%Y%m%d > /build-timestamp\nCOPY entrypoint.sh /entrypoint.sh\nENTRYPOINT /entrypoint.sh\nentrypoint.sh:\n#!/bin/bash\n\nexport BUILD_TIMESTAMP=\"$(cat /build-timestamp)\"\nexec \"$@\"",
    "How do I change the dll name when I dotnet publish?": "You can use this command to perform the build and rename the assembly:\ndotnet msbuild -r -p:Configuration=Release;AssemblyName=foo\nOn Linux/macOS you will have to add quotes around the command, like this:\ndotnet msbuild -r '-p:Configuration=Release;AssemblyName=foo'\nHowever, there can be unintended side effects due to a global property being set. You should read this open issue from Sep 2019 as it speaks directly to your question regarding Docker and renaming the output: https://github.com/dotnet/msbuild/issues/4696\nAlso, I know you wanted to avoid editing the .csproj file, but in case you aren't aware, you can add the AssemblyName property and set the output name in that manner.\nSuch as:\n  <PropertyGroup>\n    <OutputType>Exe</OutputType>\n    <TargetFramework>netcoreapp3.1</TargetFramework>\n    <AssemblyName>foo</AssemblyName>\n  </PropertyGroup>\nThis will create foo.dll (and other files as well, e.g. .pdb, .deps.json, etc.)",
    "Run Laravel app in Docker container, but autoload.php file does not exist": "I just had the same issue.\nFor some reason the vendor folder can not be created or written to when your apps root volume is mounted to mirror your local environment (- ./:/var/www).\nAdd the volume \"/var/www/vendor\" to your application. This will create the folder in the container and prevent that folder from being mirrored. This allows Laravel to control that folder and create what it needs in it.\nIf someone has a explanation of why the vendor folder can not be created or written to when mirrored it would be appreciated!\n  #PHP Service\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: digitalocean.com/php\n    container_name: bumapp\n    restart: unless-stopped\n    volumes:\n       - ./:/var/www\n       -  /var/www/vendor\n       - ./docker/php/local.ini:/usr/local/etc/php/conf.d/local.ini\n    tty: true\n    environment:\n      SERVICE_NAME: app\n      SERVICE_TAGS: dev\n    working_dir: /var/www\n    networks:\n      - app-network",
    "Require environment variables to be given to image when run using `-e`": "In detach mode it not possible to print message that env is required, in your word when running with -d, but you can try a workaround:\nDockerfile\nFROM alpine\nCOPY entrypoint.sh /usr/bin/\nRUN chmod +x /usr/bin/entrypoint.sh\nENTRYPOINT [\"entrypoint.sh\"]\nentrypoint.sh\n#!/bin/sh\necho \"starting container $hostname\"\nif [ -z \"$REQUIRED_ENV\" ]; then\n  echo \"Container failed to start, pls pass -e REQUIRED_ENV=sometest\"\n  exit 1\nfi\necho \"starting container with $REQUIRED_ENV\"\n#your long-running command from CMD\nexec \"$@\"\nSo when you run with\ndocker run -it --name envtest  --rm env-test-image \nit will exit with the message\nstarting container \nContainer failed to start, pls pass -e REQUIRED_ENV=sometest\nThe workaround with detach mode\ndocker run -it --name envtest  -d --rm env-test-image && docker logs envtest",
    "unhealthy docker container not restarted by docker native health check": "Docker only reports the status of the healthcheck. Acting on the healthcheck result requires an extra layer running on top of docker. Swarm mode provides this functionality and is shipped with the docker engine. To enable:\ndocker swarm init\nThen instead of managing individual containers with docker run, you would declare your target state with docker service or docker stack commands and swarm mode will manage the containers to achieve the target state.\ndocker service create -d  --net=host applicationname:temp\nNote that host networking and publishing ports are incompatible (they make no logical sense together), net requires two dashes to be a valid flag, and changing the pid namespace is not supported in swarm mode. Many other features should work similar to docker run.\nhttps://docs.docker.com/engine/reference/commandline/service_create/",
    "kubernetes deployment with args": "The problem was in your case is container is not found after finishing it's task. You told to execute a shell script to your conatainer. And after doing that the container is finished. That's why you can't see whether the files were created or not. Also it didn't put any logs. So you need to keep alive the container after creating the files. You can do that by putting a infinite while loop. Here it comes:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: hello\nlabels:\n    app: hi\nspec:\nreplicas: 1\nselector:\n    matchLabels:\n    app: hi\ntemplate:\n    metadata:\n    labels:\n        app: hi\n    spec:\n    containers:\n    - name: hi\n        image: busybox\n        args:\n        - /bin/sh\n        - \"-c\"\n        - \"touch /tmp/healthy; touch /tmp/liveness; while true; do echo .; sleep 1; done\"\n        ports:\n        - containerPort: 80\nSave it to hello-deployment.yaml and run,\n$ kubectl create -f hello-deployment.yaml\n$ pod_name=$(kubectl get pods -l app=hi -o jsonpath='{.items[0].metadata.name}')\n$ kubectl logs -f $pod_name\n$ kubectl exec -it -f $pod_name -- ls /tmp",
    "How to always use the newest version of a Docker Base Image?": "solution:\ndocker build --pull\nexplanation:\n--pull Always attempt to pull a newer version of the image\nhttps://docs.docker.com/engine/reference/commandline/build/",
    "Use private npm repo in Docker": "I'm guessing the package utilities@0.1.9 is your private package? If so, it would seem your auth token either isn't being used or doesn't have access to that package for some reason.\nYou could try writing the ~/.npmrc file rather than using the config set, this would just be a case of using:\nRUN echo -e \"//private.repo/:_authToken=... > ~/.npmrc\nThis will cause your docker user to then authenticate using that token against the registry defined. This is how we setup auth tokens for npm for the most part.\nOn a side note, you might want to consider not using multiple RUN commands one after another. This causes a new image layer to be created for every single command and can bloat the size of your container massively. Try using && \\ at the end of your commands and then placing the next command on a new line without the RUN bit. For example:\nFROM keymetrics/pm2:latest-alpine\n\nRUN mkdir -p /app\n\nWORKDIR /app\n\nCOPY package.json ./\nCOPY .npmrc ./\n\nRUN npm config set registry http://private.repo/:_authToken=$AUTH_TOKEN && \\\n  npm install utilities@0.1.9 && \\\n  apk update && apk add yarn python g++ make && rm -rf /var/cache/apk/* && \\\n  set NODE_ENV=production && \\\n  npm config set registry https://registry.npmjs.org/ && \\\n  npm install\n\nCOPY . /app\n\nRUN ls -al -R\n\nEXPOSE 51967\n\nCMD [ \"pm2-runtime\", \"start\", \"pm2.json\" ]\nIt should be just as readable but the final image should be smaller and potentially a bit faster to build.",
    "Issues with COPY when using multistage Dockerfile builds -- no such file or directory": "To follow-up my comment, the path you set with the WORKDIR is absolute and should be specified in the same way in the COPY --from=build command.\nSo this could lead to the following Dockerfile:\nFROM golang:latest AS build\n\nENV SRC_DIR=/go/src/github.com/grafana/grafana/\nENV GIT_SSL_NO_VERIFY=1\n\nCOPY . $SRC_DIR\nWORKDIR $SRC_DIR\n\n# Building of Grafana\nRUN \\\n  npm run build && \\\n  go run build.go setup && \\\n  go run build.go build\n\n# Create final stage containing only required artifacts\nFROM scratch\n\nENV SRC_DIR=/go/src/github.com/grafana/grafana/\nWORKDIR $SRC_DIR\n\nCOPY --from=build ${SRC_DIR}/bin/grafana-server ${SRC_DIR}/bin/grafana-server\n\nEXPOSE 3001\n\nCMD [\"./bin/grafana-server\"]\n(only partially tested)",
    "deploy stack to remote swarm": "You are trying to reach the remote Docker Daemon to push your compose.yml. But the problem is by default Docker Daemon is only bound to unix socket.\nTo do so, on your remote server, you will have to alter /usr/lib/systemd/system/docker.service file and change ExecStart to...\nExecStart=/usr/bin/docker daemon -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock\nthen,\nsystemctl daemon-reload\nand restart\nservice docker restart\nI wouldn't recommend you this setup without securing the Daemon with TLS. If you don't use TLS anyone can reach out to your server and deploy containers.\nHope it helps!",
    "Dockerfile: create and mount disk image during build": "You would need the --privileged or --cap-add functionality that docker run does have, but that is not supported for docker build. So, as of current Docker version, you can't.\nSee this comment:\nA significant number of docker users want the ability to --cap-add or --privileged in the build command, to mimic what is there in the run command.\nThat's why this ticket has been open for 3 years with people constantly chiming in even though the maintainers aren't interested in giving the users what they want in this specific instance.\nAs an alternative you can move that RUN commands to a script that should run when container starts (and adding the mentioned --privileged flag, or --cap-add=SYS_ADMIN)",
    "ADD and COPY to merge the contents of a dir with the one already on the build": "Docker has multi-stage builds since 17.05: Use multi-stage builds\nFROM busybox as builder\nADD build/image-base.tgz /tmproot/\n\nFROM alpine:latest  \n...\nCOPY --from=builder /tmproot /\n...\nAnother example:\nFROM busybox as builder\nCOPY src/etc/app /tmproot/etc/app\nCOPY src/app /tmproot/usr/local/app\n\nFROM alpine:latest  \n...\nCOPY --from=builder /tmproot /\n...",
    "How to copy a file from container to host using copy in docker-py": "copy is a deprecated method in docker and the preferred way is to use put_archive method. So basically we need to create an archive and then put it into the container. I know that's sounds weird, but that's what the API supports currently. If you, like me, think this can be improved, feel free to open an issue/feature request and I'll upvote it.\nHere is a code snippet on how to copy a file to the container :\ndef copy_to_container(container_id, artifact_file):\n    with create_archive(artifact_file) as archive:\n        cli.put_archive(container=container_id, path='/tmp', data=archive)\n\ndef create_archive(artifact_file):\n    pw_tarstream = BytesIO()\n    pw_tar = tarfile.TarFile(fileobj=pw_tarstream, mode='w')\n    file_data = open(artifact_file, 'r').read()\n    tarinfo = tarfile.TarInfo(name=artifact_file)\n    tarinfo.size = len(file_data)\n    tarinfo.mtime = time.time()\n    # tarinfo.mode = 0600\n    pw_tar.addfile(tarinfo, BytesIO(file_data))\n    pw_tar.close()\n    pw_tarstream.seek(0)\n    return pw_tarstream",
    "Change entrypoint of a k8s Pod, but keep the CMD": "That's not a thing.\nENTRYPOINT (in Dockerfile) is equal to command: (in PodSpec)\nCMD (in Dockerfile) equals args: (in PodSpec)\nSo just override command but not args.",
    "How to avoid start of Gradle daemon inside Dockerfile": "This question has already an extensive answer on the Gralde forums: Using \u2013no-daemon, but still see a process called \u201cGradle Worker Daemon 1\u201d.\nIn short: The Gradle daemon process is the one executing the build and started always no matter on what has been specified on the command line. If --no-daemon is specified, the process is terminated after build completion.\nOriginal answer from the Gradle forums:\nMy question is why is the daemon being created when we specify --no-daemon?\nThe process run by Gradle to execute a build is the same whether or not you enable or disable the daemon. The behavior of the process after a build completes is the difference.\nWith the daemon enabled, the process will continue running in the background and can reused for a subsequent build. With the daemon disabled, the process is terminated at the end of the build. Even with the daemon disabled, you will still see a process labeled as a daemon. It doesn\u2019t mean it will continue running in the background like a daemon.",
    "How to expose docker container port and call from postman?": "Looks like your server.js is listening on port 6000 of your container. You need to bind port 6000 of your container to port 6000 of your host (You're currently binding port 80 of the container to your host's port 6000)\ndocker run -p 6000:6000 ... <image>\nAlso make sure your process is listening on host 0.0.0.0 (instead of localhost). Container's localhost is not the same as your host's localhost\napp.listen('0.0.0.0',port, () => console.log(`Example app listening on port ${port}!`));",
    "How to access Docker build context path inside Dockerfile": "You don't need docker build context location known inside your dockerfile.\nWhat you need to know is:\nLocation of you build context. (say C:\\work\\Personal\\mycontext which contains files and folders that you need to copy inside docker container)\nLocation of dockerfile (say C:\\work\\Personal\\API\\api-service\\Dockerfile)\nAlso you need to know relative file path structure of your context. Like\n- C:\\work\\Personal\\mycontext\n  |\n   - scripts\n     |\n     - start.sh\n  |\n  - create.py\n  |\n  - target\n    |\n    - maven\n      |\n      - abc.jar\nIn this case your dockerfile will contain appropriate commands like COPY scripts /scripts that copy these files assuming its running from the context folder C:\\work\\Personal\\mycontext\nYour docker build command will be\ndocker build -f C:\\work\\Personal\\API\\api-service\\Dockerfile -t image:version1 C:\\work\\Personal\\mycontext\nNote: Here -f option specify location of dockerfile in this case its C:\\work\\Personal\\API\\api-service\\Dockerfile and C:\\work\\Personal\\mycontext specify location of docker build context.\nIrrespective of location from where the docker build commands runs, it will work as long as you provide exact location of dockerfile and docker build context.\nMore info here.",
    "Does docker EXPOSE refer to the container port or the host port?": "The EXPOSE instruction documents the port on which an application inside the container is expected to be listening. The important word there is \"documents\". It does not change the behavior of docker running your container, it does not publish the port, and does not impact the ability to connect between containers.\nWhether or not you expose the port, you need to separately publish the port to access it from outside the container network. And whether or not you expose the port, you can connect between containers on the same docker network.\nThere are various tools that can use this image metadata to automatically discover your application. This includes the -P flag to publish all container ports on random high numbered host ports. You will also see reverse proxies (like traefik) use this when querying the docker engine to determine the default port to use for your container.",
    "How to get docker architecture, like amd64, arm32v7, in alpine linux?": "Image recipes for different architectures have their own Dockerfiles, so it's just a matter of picking the right one to work with (hope I got your question right).\nThere are available Node images for Alpine targeting various architectures, for example:\ndocker pull ppc64le/node:8-alpine\nIs the Node.js 8.12.0 image for PPC64LE on Alpine 3.8.\nEdit after clarification:\nFor a multiarch Dockerfile that builds differently depending on the target architecture, you could resolve arch name on runtime, by checking the result of uname -m and using shell conditionals, for example:\nRUN /bin/ash -c 'set -ex && \\\n    ARCH=`uname -m` && \\\n    if [ \"$ARCH\" == \"x86_64\" ]; then \\\n       echo \"x86_64\" && \\\n       apk add some-package; \\\n    else \\\n       echo \"unknown arch\" && \\\n       apk add some-other-package; \\\n    fi'",
    "Preventing access to code inside of a docker container": "Anybody who has your image can always do\ndocker run -u root imagename sh\nAnybody who can run Docker commands at all has root access to their system (or can trivially give it to themselves via docker run -v /etc:/hostetc ...) and so can freely poke around in /var/lib/docker to see what's there. It will have all of the contents of all of the images, if scattered across directories in a system-specific way.\nIf your source code is actually secret, you should make sure you're using a compiled language (C, Go, Java kind of) and that your build process doesn't accidentally leak the source code into the built image, and it will be as secure as anything else where you're distributing binaries to end users. If you're using a scripting language (Python, JavaScript, Ruby) then intrinsically the end user has to have the code to be able to run the program.",
    "Run docker as root verus non-root": "The docker group grants privileges equivalent to the root user\nBy default yes. This is also true for any user that can run a docker container on the machine.\nThe reason is that by default when you are running as root inside the container, this will map to root on the host machine. Thus you can bind some sensitive folders from the host onto the container, and execute privileged actions on those mounts since the user inside the container is root (pid 0).\nThe solution for that is to enable the user-namespace that basically would map the root user inside the container into a non-root user on the machine.\nSecond question (run as root user): assume I followed the steps above (create docker group and add user to it). Yet I specify \"USER root\" in a Dockerfile (example below). When I run this container, it will run as root regardless of the setting above, correct?\nThere are several points here:\nBy default, USER root is the default, so you don't have to specify it. (Unless the base image explicitly sets a user other than root)\nFrom the perspective of the host machine, a docker container is just a normal process. Every process has an owner. This owner is the host machine user that executed the docker runcommand. The USER root instruction has nothing to do with this owner. The USER instruction only specifies the user inside the container that will start the process inside the container that is different from the owner of the container.",
    "Docker build: error: ER_NOT_SUPPORTED_AUTH_MODE: MySQL client": "version: '3'\n\nservices:\n\n  db:\n    image: mysql\n    command: --default-authentication-plugin=mysql_native_password\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWOR=example\nI found solution on Docker hub in mysql image. Its set \"command\" for docker-compose.yml that change mode of auth.",
    "What is a Docker build stage?": "A stage is the creation an image. In a multi-stage build, you go through the process of creating more than one image, however you typically only tag a single one (exceptions being multiple builds, building a multi-architecture image manifest with a tool like buildx, and anything else docker releases after this answer).\nEach stage, building a distinct image, starts from a FROM line in the Dockerfile. One stage doesn't inherit anything done in previous stages, it is based on its own base image. So if you have the following:\nFROM alpine as stage1\nRUN apk add your_tool\n\nFROM alpine as stage2\nRUN your_tool some args\nyou will get an error since your_tool is not installed in the second stage.\nWhich stage do you get as output from the build? By default the last stage, but you can change that with the docker image build --target stage1 . to build the stage with the name, stage1 in this example. The classic docker build will run from the top of the Dockerfile until if finishes the target stage. Buildkit builds a dependency graph and builds stages concurrently and only if needed, so do not depend on this ordering to control something like a testing workflow in your Dockerfile (buildkit can see if nothing in the test stage is needed in your release stage and skip building the test).\nWhat's the value of multiple stages? Typically, its done to separate the build environment from the runtime environment. It allows you to perform the entire build inside of docker. This has two advantages.\nFirst, you don't require an external Makefile and various compilers and other tools installed on the host to compile the binaries that then get copied into the image with a COPY line, anyone with docker can build your image.\nAnd second, the resulting image doesn't include all the compilers or other build time tooling that isn't needed at runtime, resulting in smaller and more secure images. The typical example is a java app with maven and a full JDK to build, a runtime with just the jar file and the JRE.\nIf each stage makes a separate image, how do you get the jar file from the build stage to the run stage? That comes from a new option to the COPY command, --from. An oversimplified multi-stage build looks like:\nFROM maven as build\nCOPY src /app/src\nWORKDIR /app/src\nRUN mvn install\n\nFROM openjdk:jre as release\nCOPY --from=build /app/src/target/app.jar /app\nCMD java -jar /app/app.jar\nWith that COPY --from=build we are able to take the artifact built in the build stage and add it to the release stage, without including anything else from that first stage (no layers of compile tools like JDK or Maven get added to our second stage).\nHow is the FROM x as y and the COPY --from=y /a /b working together? The FROM x as y is defining an image name for the duration of this build, in this case y. Anywhere later in the Dockerfile that you would put an image name, you can put y and you'll get the result of this stage as your input. So you could say:\nFROM upstream as mybuilder\nRUN apk add common_tools\n\nFROM mybuilder as stage2\nRUN some_tool arg2\n\nFROM mybuilder as stage3\nRUN some_tool arg3\n\nFROM minimal_base as release\nCOPY --from=stage2 /bin2 /\nCOPY --from=stage3 /bin3 /\nNote how stage2 and stage3 are each FROM mybuilder that is the output of the first stage.\nThe COPY --from=y allows you to change the context where you are copying from to be another image instead of the build context. It doesn't have to be another stage. So, for example, you could do the following to get a docker binary in your image:\nFROM alpine\nCOPY --from=docker:stable /usr/local/bin/docker /usr/local/bin/\nFurther documentation on this is available at: https://docs.docker.com/develop/develop-images/multistage-build/",
    "PHP cannot write in docker container": "Apache runs PHP with the user www-data, this user needs to have write access on your host directory ./public_html\nTo fix that, go to your docker-compose directory and execute the following command to change the owner of your public_htmldirectory and all files inside.\nsudo chown -R www-data:www-data public_html\nAfter that you need to allow users in the group \"www-data\" to edit files\n# To change all the directories to 775 \n# (write for user & group www-data, read for others):\nfind public_html -type d -exec chmod 775 {} \\;\n\n# To change all the files to 664\n# (write for user & group www-data, read for others):\nfind public_html -type f -exec chmod 664 {} \\;\nIn order for your current user to edit these files you need to add it to the www-data group :\nsudo usermod -aG www-data $USER",
    "Docker build argument": "Docker has ARG that you can use here\nFROM    ubuntu:14.04\n\nMAINTAINER Karl Morrison\n\nARG myVarHere\n\nRUN do-something-here myVarHere\nAnd then build using --build-arg:\ndocker build --build-arg myVarHere=value",
    "How to run Redis docker container for Mac Silicon?": "You should use arm64v8/redis instead of the default. So, replace:\nFROM redis:alpine\nFor:\nFROM arm64v8/redis:alpine\nMore info here: https://hub.docker.com/r/arm64v8/redis\nAlternatively, you can use the --platform arg or use the TARGETPLATFORM, as explained here:\nhttps://nielscautaerts.xyz/making-dockerfiles-architecture-independent.html",
    "Install homebrew using Dockerfile": "The eval happens in the first RUN statement, but is not persisted through to the next one. You want to join the two.\nRUN git clone https://github.com/Homebrew/brew ~/.linuxbrew/Homebrew \\\n&& mkdir ~/.linuxbrew/bin \\\n&& ln -s ../Homebrew/bin/brew ~/.linuxbrew/bin \\\n&& eval $(~/.linuxbrew/bin/brew shellenv) \\\n&& brew --version\nGenerally speaking, any environment changes you perform in one shell instance (one RUN statement, or your ENTRYPOINT) will be lost as soon as that instance terminates. See also Global environment variables in a shell script",
    "Connect to VPN with Podman": "It turns out that you are blocked by SELinux: after running the client container and trying to access /dev/net/tun inside it, you will get the following AVC denial in the audit log:\ntype=AVC msg=audit(1563869264.270:833): avc:  denied  { getattr } for  pid=11429 comm=\"ls\" path=\"/dev/net/tun\" dev=\"devtmpfs\" ino=15236 scontext=system_u:system_r:container_t:s0:c502,c803 tcontext=system_u:object_r:tun_tap_device_t:s0 tclass=chr_file permissive=0\nTo allow your container configuring the tunnel while staying not fully privileged and with SELinux enforced, you need to customize SELinux policies a bit. However, I did not find an easy way to do this properly.\nLuckily, there is a tool called udica, which can generate SELinux policies from container configurations. It does not provide the desired policy on its own and requires some manual intervention, so I will describe how I got the openvpn container working step-by-step.\nFirst, install the required tools:\n$ sudo dnf install policycoreutils-python-utils policycoreutils udica\nCreate the container with required privileges, then generate the policy for this container:\n$ podman run -it --cap-add NET_ADMIN --device /dev/net/tun -v $PWD:/vpn:Z --name ovpn peque/vpn\n$ podman inspect ovpn | sudo udica -j - ovpn_container\n\nPolicy ovpn_container created!\n\nPlease load these modules using: \n# semodule -i ovpn_container.cil /usr/share/udica/templates/base_container.cil\n\nRestart the container with: \"--security-opt label=type:ovpn_container.process\" parameter\nHere is the policy which was generated by udica:\n$ cat ovpn_container.cil \n(block ovpn_container\n    (blockinherit container)\n    (allow process process ( capability ( chown dac_override fsetid fowner mknod net_raw setgid setuid setfcap setpcap net_bind_service sys_chroot kill audit_write net_admin ))) \n\n    (allow process default_t ( dir ( open read getattr lock search ioctl add_name remove_name write ))) \n    (allow process default_t ( file ( getattr read write append ioctl lock map open create  ))) \n    (allow process default_t ( sock_file ( getattr read write append open  ))) \n)\nLet's try this policy (note the --security-opt option, which tells podman to run the container in newly created domain):\n$ sudo semodule -i ovpn_container.cil /usr/share/udica/templates/base_container.cil\n$ podman run -it --cap-add NET_ADMIN --device /dev/net/tun -v $PWD:/vpn:Z --security-opt label=type:ovpn_container.process peque/vpn\n<...>\nERROR: Cannot open TUN/TAP dev /dev/net/tun: Permission denied (errno=13)\nUgh. Here is the problem: the policy generated by udica still does not know about specific requirements of our container, as they are not reflected in its configuration (well, probably, it is possible to infer that you want to allow operations on tun_tap_device_t based on the fact that you requested --device /dev/net/tun, but...). So, we need to customize the policy by extending it with few more statements.\nLet's disable SELinux temporarily and run the container to collect the expected denials:\n$ sudo setenforce 0\n$ podman run -it --cap-add NET_ADMIN --device /dev/net/tun -v $PWD:/vpn:Z --security-opt label=type:ovpn_container.process peque/vpn\nThese are:\n$ sudo grep denied /var/log/audit/audit.log\ntype=AVC msg=audit(1563889218.937:839): avc:  denied  { read write } for  pid=3272 comm=\"openvpn\" name=\"tun\" dev=\"devtmpfs\" ino=15178 scontext=system_u:system_r:ovpn_container.process:s0:c138,c149 tcontext=system_u:object_r:tun_tap_device_t:s0 tclass=chr_file permissive=1\ntype=AVC msg=audit(1563889218.937:840): avc:  denied  { open } for  pid=3272 comm=\"openvpn\" path=\"/dev/net/tun\" dev=\"devtmpfs\" ino=15178 scontext=system_u:system_r:ovpn_container.process:s0:c138,c149 tcontext=system_u:object_r:tun_tap_device_t:s0 tclass=chr_file permissive=1\ntype=AVC msg=audit(1563889218.937:841): avc:  denied  { ioctl } for  pid=3272 comm=\"openvpn\" path=\"/dev/net/tun\" dev=\"devtmpfs\" ino=15178 ioctlcmd=0x54ca scontext=system_u:system_r:ovpn_container.process:s0:c138,c149 tcontext=system_u:object_r:tun_tap_device_t:s0 tclass=chr_file permissive=1\ntype=AVC msg=audit(1563889218.947:842): avc:  denied  { nlmsg_write } for  pid=3273 comm=\"ip\" scontext=system_u:system_r:ovpn_container.process:s0:c138,c149 tcontext=system_u:system_r:ovpn_container.process:s0:c138,c149 tclass=netlink_route_socket permissive=1\nOr more human-readable:\n$ sudo grep denied /var/log/audit/audit.log | audit2allow\n\n\n#============= ovpn_container.process ==============\nallow ovpn_container.process self:netlink_route_socket nlmsg_write;\nallow ovpn_container.process tun_tap_device_t:chr_file { ioctl open read write };\nOK, let's modify the udica-generated policy by adding the advised allows to it (note, that here I manually translated the syntax to CIL):\n(block ovpn_container\n    (blockinherit container)\n    (allow process process ( capability ( chown dac_override fsetid fowner mknod net_raw setgid setuid setfcap setpcap net_bind_service sys_chroot kill audit_write net_admin )))\n\n    (allow process default_t ( dir ( open read getattr lock search ioctl add_name remove_name write )))\n    (allow process default_t ( file ( getattr read write append ioctl lock map open create  )))\n    (allow process default_t ( sock_file ( getattr read write append open  )))\n\n    ; This is our new stuff.\n    (allow process tun_tap_device_t ( chr_file ( ioctl open read write )))\n    (allow process self ( netlink_route_socket ( nlmsg_write )))\n)\nNow we enable SELinux back, reload the module and check that the container works correctly when we specify our custom domain:\n$ sudo setenforce 1\n$ sudo semodule -r ovpn_container\n$ sudo semodule -i ovpn_container.cil /usr/share/udica/templates/base_container.cil\n$ podman run -it --cap-add NET_ADMIN --device /dev/net/tun -v $PWD:/vpn:Z --security-opt label=type:ovpn_container.process peque/vpn\n<...>\nInitialization Sequence Completed\nFinally, check that other containers still have no these privileges:\n$ podman run -it --cap-add NET_ADMIN --device /dev/net/tun -v $PWD:/vpn:Z peque/vpn\n<...>\nERROR: Cannot open TUN/TAP dev /dev/net/tun: Permission denied (errno=13)\nYay! We stay with SELinux on, and allow the tunnel configuration only to our specific container.",
    "How to accept the license agreement when building rti-connext-dds-5.3.1 with docker build?": "You can use the env variable \"RTI_NC_LICENSE_ACCEPTED=yes\". Your dockerfile will look something like this:\nFROM ubuntu:bionic\n\nARG DEBIAN_FRONTEND=noninteractive\nRUN apt-get update && \\\n    apt-get install -y apt-utils debconf-utils gnupg2 lsb-release && \\\n    apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 421C365BD9FF1F717815A3895523BAEEB01FA116 && \\\n    echo \"deb http://packages.ros.org/ros2/ubuntu `lsb_release -sc` main\" > /etc/apt/sources.list.d/ros2-latest.list && \\\n    apt-get update \nRUN RTI_NC_LICENSE_ACCEPTED=yes apt-get install rti-connext-dds-5.3.1\n\nWORKDIR /home",
    "Unable to execute *catkin* commands using *RUN* in Dockerfile": "Your second case will build because CMD is not executed during the build. It simply defines the default command to execute when starting the container. See the docker CMD docs for more information.\nThe root of your problem is that ~/.bashrc is not being called in the shell used by the RUN instruction. As a result, the environment variables are not present to allow catkin_make or other ros commands to work. I address this issue in my images by running a command similar to the below.\nRUN . /opt/ros/kinetic/setup.sh && \\\n    catkin_make\nYou will need to activate the environment in each RUN instruction that needs those environment variables because the shell is not re-used between RUN commands.\nEdited to include the improvement from David Maze.",
    "Can i reference a Dockerfile in a Dockerfile?": "Generally things in Docker space like the docker run command and the FROM directive will use a local image if it exists; it doesn't need to be pushed to a repository. That means you can build your first image and refer to it in the later Dockerfile by name. (There's no way to refer to the other Dockerfile per se.)\nNewer versions of Docker have an extended form of the Dockerfile COPY command which\naccepts a flag --from=<name|index>.... In case a build stage with a specified name can\u2019t be found an image with the same name is attempted to be used instead.\nSo if you ahead of time run\ndocker build -t build-env ~/build\nthen the exact syntax you show in your proposed Dockerfile will work\nFROM some-image\nCOPY --from=build-env /built .\nand it doesn't matter that the intermediate build image isn't actually pushed anywhere.",
    "Docker bind mount volumes do not propagate changes events watched by angular `ng serve` execution": "In order to make the example working, replace step 3 by the commands below:\n// Create the new app\ndocker run --rm --mount type=bind,src=$PWD,dst=/volumes my_angular_image new my-app --directory app --style scss\n// Change ownership of the generated app\nsudo chown -R $USER:$USER .\n// Configure angular-cli polling:\nsed -i 's/\\\"styleExt\\\": \\\"scss\\\",/\"styleExt\": \"scss\", \"poll\": 1000,/g' $PWD/app/.angular-cli.json\nCredits:\n@PavelAgarkov's answer and its usefull links.",
    "How to prevent Docker from re-running pip installs every time I modify code [duplicate]": "This question, Docker how to run pip requirements.txt only if there was a change?, seems to pertain to my situation. Every time I modify the code I invalidate the Docker build cache, even though requirements.txt is unchanged. So to avoid having to re-run pip installs every time, it is recommended to COPY the requirements.txt and RUN pip install -r requirements.txt in a separate step.",
    "making docker containers communicate through port": "I think you have 3 options here to get this working:\nCreate a docker network to connect the hosts:\ndocker network create --driver bridge sample-sendr-rcv-test\ndocker run  --name=\"testListen\" --env LISTEN_HOST=\"0.0.0.0\" --env LISTEN_PORT=\"5555\" --network=sample-sendr-rcv-test -d docker.io/ayonnayihan/sample-sendr-rcv-test:receiver0.1\ndocker run --name=\"testTalk\" --env SEND_HOST=\"testListen\" --env SEND_PORT=\"5555\" --network=sample-sendr-rcv-test -d docker.io/ayonnayihan/sample-sendr-rcv-test:sender0.1\nUse docker-compose with a docker-compose.yml like:\nversion: '2'\nservices:\n  sender:\n    image: docker.io/ayonnayihan/sample-sendr-rcv-test:sender0.1\n    # build: sender\n    environment:\n      SEND_HOST: receiver\n      SEND_PORT: 5555\n  receiver:\n    image: docker.io/ayonnayihan/sample-sendr-rcv-test:receiver0.1\n    # build: receiver\n    environment:\n      LISTEN_HOST: '0.0.0.0'\n      LISTEN_PORT: 5555\nUse host networking:\ndocker run  --name=\"testListen\" --env LISTEN_HOST=\"127.0.0.1\" --env LISTEN_PORT=\"5555\" --net=host -d docker.io/ayonnayihan/sample-sendr-rcv-test:receiver0.1\ndocker run --name=\"testTalk\" --env SEND_HOST=\"localhost\" --env SEND_PORT=\"5555\" --net=host -d docker.io/ayonnayihan/sample-sendr-rcv-test:sender0.1\nThe third option is the most similar to what you are currently doing but I would not recommend it for reasons explained below. Either of the other options will work but may not be worth learning docker-compose if you are just starting with docker.\nThe reason you are having an issue is that the containers each have their own idea of 'localhost' because they are in a different network namespace. This means that 'localhost' on your 'testTalk' container does not resolve to the host where your listener is running. When you use --net=host (option 3 above) you are removing the separate namespace for the containers, thus removing some of the security benefits of using docker.",
    "Creating bash script from Dockerfile strips comments": "You're right, it's a little weird that Docker interprets that as a Dockerfile comment instead of a comment inside a string. As a workaround, I got the following to work\nFROM ubuntu:latest\n\nRUN printf \"#!/bin/bash \\\n\\n# This is a bash comment inside the script \\\n\\nls -l\\n\" > /script.sh\n\nRUN cat /script.sh\nResults in this output\nStep 3 : RUN cat /script.sh\n ---> Running in afc19e228656\n#!/bin/bash \n# This is a bash comment inside the script \nls -l\nIf you move \\n to the beginning of the comment line it still generates the correct output but no longer treats that line as a Dockerfile comment line.\nAssuming I found the right command parsing code, and I'm reading it correctly, Docker strips comments out before attempting to parse the line to see if it has any commands on it.",
    "build docker image from local (unpublished) image": "I think you have to specify the version explicitly. When I did, I was able to build. When I did not, I got the same error.\ndocker build -t munchkin/base:latest -f baseimage .\nAnd then you can use this images.",
    "npm WARN config only Use `--omit=dev`": "It's a weird message to say \"Don't use A, use B instead.\" Changing from the one to the other is what I did to get rid of the warnings on my four containers.\nIt appears that npm-ci no longer includes an \"only\" option, however the --omit=dev seems to do the same thing:\nhttps://docs.npmjs.com/cli/v9/commands/npm-ci#omit\nI think the --only=prod may date back to node 12-14ish, I couldn't find any official references to date it, just that the syntax was used in blog posts dating around 2020 at the latest.",
    "How to solve network connection when RUN yarn install in docker image build?": "I drop this answer here, because nothing about proxy helped in my case.\nTL;DR The version check URL redirects and that causes confusion to yarn\nSo, I went on to examine typical archaic tools, like verbosity check, curl and dig.\nSo, first thing first, I tried to check verbose output:\nBingo #1.\nverbose 0.181073051 Performing \"GET\" request to \"https://yarnpkg.com/latest-version\".\n[1/4] Resolving packages...                                                       \nsuccess Already up-to-date.                                                                                                                                          \nDone in 0.12s.                                                                                                                                                       \ninfo There appears to be trouble with your network connection. Retrying...                                                                                           \nverbose 33.254841183 Performing \"GET\" request to \"https://yarnpkg.com/latest-version\".                                                                               \ninfo There appears to be trouble with your network connection. Retrying...                                                                                           \nverbose 66.292530305 Performing \"GET\" request to \"https://yarnpkg.com/latest-version\".                                                                               \ninfo There appears to be trouble with your network connection. Retrying...                                                                                           \nverbose 99.329186881 Performing \"GET\" request to \"https://yarnpkg.com/latest-version\".                                                                               \ninfo There appears to be trouble with your network connection. Retrying...                                                                                           \nverbose 132.366749502 Performing \"GET\" request to \"https://yarnpkg.com/latest-version\".\nWhy not getting an answer from yarnpkg.com? This is insane... So, let's see if this resolves:\n$ dig yarnpkg.com\n\n; <<>> DiG 9.18.7-1+b1-Debian <<>> yarnpkg.com\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 55602\n;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 4096\n; COOKIE: 9cf3b6f8b290069dbbe8ca8763501b11ea79617323c673e6 (good)\n;; QUESTION SECTION:\n;yarnpkg.com.                   IN      A\n\n;; ANSWER SECTION:\nyarnpkg.com.            300     IN      A       104.18.126.100\nyarnpkg.com.            300     IN      A       104.16.171.99\n\n;; Query time: 11 msec\n;; SERVER: 192.168.1.1#53(192.168.1.1) (UDP)\n;; WHEN: Wed Oct 19 18:43:13 EEST 2022\n;; MSG SIZE  rcvd: 100\nIt does!!! (?)\nSo, let's see what I get from it, by using curl\n$ curl https://yarnpkg.com/latest-version\nRedirecting to https://classic.yarnpkg.com/latest-version\nWHAT? A redirect? Ok, this is new..\nI tried to set the url to check version not to be yarnpkg.com, but classic.yarnpkg.com, but I couldn't find the yarn configuration variable to use.\nSo, I used /etc/hosts.\n; <<>> DiG 9.18.7-1+b1-Debian <<>> classic.yarnpkg.com\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 35092\n;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 4096\n; COOKIE: 72636a6924283d51c4127a8463501bbb42e53d34835e8402 (good)\n;; QUESTION SECTION:\n;classic.yarnpkg.com.           IN      A\n\n;; ANSWER SECTION:\nclassic.yarnpkg.com.    300     IN      CNAME   yarnpkg.netlify.com.\nyarnpkg.netlify.com.    20      IN      A       3.64.200.242\nyarnpkg.netlify.com.    20      IN      A       34.141.11.154\n\n;; Query time: 59 msec\n;; SERVER: 192.168.1.1#53(192.168.1.1) (UDP)\n;; WHEN: Wed Oct 19 18:46:03 EEST 2022\n;; MSG SIZE  rcvd: 138\nSet the first IP to yarnpkg.com\n# /etc/hosts\n34.141.48.9     yarnpkg.com\nBINGO!!! Yarn command finished instantly.\n$ yarn \nyarn install v1.22.19\n[1/4] Resolving packages...\nsuccess Already up-to-date.\nDone in 0.12s.\n$",
    "Docker SDK for Python: how to build an image with custom Dockerfile AND custom context": "Even if the documentation refers to the \"path within the build context to the Dockerfile\", it works for a Dockerfile outside the build context if an absolute path is specified.\nUsing my project tree:\nclient.images.build(\n    path = '/opt/project/src/',\n    dockerfile = '/opt/project/dockerfile/Dockerfile.name',\n    tag = 'image:version'\n)",
    "Use latest curl version on docker": "You can use the downloaded packages directly to solve this problem by installing with the make command.\nFROM debian:10.7\n\nRUN apt-get update && \\\n    apt-get install --yes --no-install-recommends wget build-essential libcurl4 && \\\n    wget https://curl.se/download/curl-7.74.0.tar.gz && \\\n    tar -xvf curl-7.74.0.tar.gz && cd curl-7.74.0 && \\\n    ./configure && make && make install\nNote that it requires running ./configure.\nAfter installation curl will work perfectly in the version you need, in this case, version 7.74.0.\nIf you want to optimize your container, remove the build-essential package, it alone will consume more than 200MB of storage. To do this, add at the end of the compilation:\napt-get autoremove build-essential",
    "Permission denied to run npm install in alpine-chrome docker image": "Try to use the user root\nFROM zenika/alpine-chrome:86-with-node\nUSER root",
    "Azure Devops not supporting build-args with Docker@2": "Is there any other way to pass arguments from Azure Devops through to a dockerfile?\nYes. The BuildAndPush Command doesn't support adding argument.\nBut the Build command support it.\nYou could split buildandpush into Docker build task and docker push task.\nHere is an example:\nsteps:\n- task: Docker@2\n  displayName: Docker Build \n  inputs:\n    command: build\n    repository: $(imageRepository)\n    containerRegistry: $(dockerRegistryServiceConnection)\n    dockerfile: $(dockerfilePath)\n    tags: 5.12.4\n    arguments: '--build-arg test_val=\"$(build.buildid)\" '\n- task: Docker@2\n  displayName: Docker Push\n  inputs:\n    command: push\n    repository: $(imageRepository)\n    containerRegistry: $(dockerRegistryServiceConnection)\n    tags: 5.12.4\nDocker Build Step Result:",
    "systemctl not found while building mongo image": "To be precise\nRUN ln -s /bin/echo /bin/systemctl\nRUN apt-get -qqy install  mongodb-org",
    "Docker: Error code 127 when executing shell script": "Docker is executing the install-deps.sh script. The issue is with a command inside install-deps.sh that is not recognized when docker attempts to run the script.\nAs you can see the script returns an error code of 127 meaning that a command within the file does not exist.\nFor instance - try this:\ntouch test.sh\necho \"not-a-command\" >> test.sh\nchmod 755 test.sh\n/bin/sh -c \"./test.sh\"\nOutput:\n/root/test.sh: line 1: not-a-command: command not found\nNow check the exit code:\necho $?\n127\nI would suggest running the script inside an interactive environment to identify/install the command that is not found.",
    "How to mount a host directory with docker-compose, with the \"~/path/on/host\" to be specified when running the host, not in the docker-compose file": "version: '3'\n\nservices:\n  my-app:\n    build:\n      context: .\n      dockerfile: ./path/to/Dockerfile\n    volumes:\n      - ~/path/on/host:/path/on/container\n    ports:\n      - \"3000:3000\"\nThen you can start your service with docker-compose up -d. Make sure to stop and remove first the container you started using docker commands otherwise you will get conflicts.\nEDIT BASED ON COMMENT:\nCreate a .env file with the contents:\nHOST_PATH=~/path/on/host\nand change your docker-compose.yml:\nversion: '3'\n\nservices:\n  my-app:\n    build:\n      context: .\n      dockerfile: ./path/to/Dockerfile\n    volumes:\n      - ${HOST_PATH}:/path/on/container\n    ports:\n      - \"3000:3000\"",
    "what is the difference between creating dockerfile for x86, armv7 32 and arm 64": "Every Dockerfile starts with a\nFROM <base_image>\ndeclaration, so you will have to choose a base image that will be able to run on your system/architecture and build on top of it.\nFrom here:\nDocker Official Images\nSee Docker's documentation for a good high-level overview of the program.\nArchitectures other than amd64?\nSome images have been ported for other architectures, and many of these are officially supported (to various degrees).\nArchitectures officially supported by Docker, Inc. for running Docker: (see download.docker.com) - IBM z Systems (s390x): https://hub.docker.com/u/s390x/ - ARMv7 32-bit (arm32v7): https://hub.docker.com/u/arm32v7/ - Windows x86-64 (windows-amd64): https://hub.docker.com/u/winamd64/ - Linux x86-64 (amd64): https://hub.docker.com/u/amd64/\nOther architectures built by official images: (but not officially supported by Docker, Inc.)\nIBM POWER8 (ppc64le): https://hub.docker.com/u/ppc64le/\nx86/i686 (i386): https://hub.docker.com/u/i386/\nARMv8 64-bit (arm64v8): https://hub.docker.com/u/arm64v8/\nARMv6 32-bit (arm32v6): https://hub.docker.com/u/arm32v6/ (Raspberry Pi 1, Raspberry Pi Zero)\nARMv5 32-bit (arm32v5): https://hub.docker.com/u/arm32v5/\n\nYou may also find other users/sources that use Docker Hub to upload their images. While doing some tests with ffmpeg on a Raspberry Pi, I decided to trust the images provided by resin.io (update: now they are called balena.io and here is their Docker hub: hub.docker.com/u/balena)\nIf you are interested in learning how an image is created, you can check its Dockerfile. For example for Node.js on arm64v8 see the Dockerfiles here",
    "How to clone git repo using Dockerfile": "If you don't want to install git you can use multi stage builds in your Dockerfile,\nFROM alpine/git:latest\nWORKDIR /clone-workspace\nRUN git clone https://github.com/umairnow/LocalizableGenerator.git\n\nFROM mattes/hello-world-nginx\nCOPY --from=0 /clone-workspace/LocalizableGenerator /path/to/file",
    "How to set RAM memory of a Docker container by terminal or DockerFile": "Don't rely on /proc/meminfo for tracking memory usage from inside a docker container. /proc/meminfo is not containerized, which means that the file is displaying the meminfo of your host system.\nYour /proc/meminfo indicates that your Host system has 2G of memory available. The only way you'll be able to make 6G available in your container without getting more physical memory is to create a swap partition.\nOnce you have a swap partition larger or equal to ~4G, your container will be able to use that memory (by default, docker imposes no limitation to running containers).\nIf you want to limit the amount of memory available to your container explicitly to 6G, you could do docker run -p 5311:5311 --memory=2g --memory-swap=6g my-linux, which means that out of a total memory limit of 6G (--memory-swap), upto 2G may be physical memory (--memory). More information about this here.\nThere is no way to set memory limits in the Dockerfile that I know of (and I think there shouldn't be: Dockerfiles are there for building containers, not running them), but docker-compose supports the above options through the mem_limit and memswap_limit keys.",
    "How to get a docker container to the state: dead for debugging?": "You've asked for a dead container.\nTL;DR: This is how to create a dead container\nDon't do this at home:\nID=$(docker run --name dead-experiment -d -t alpine sh)\ndocker kill dead-experiment\ntest \"$ID\" != \"\" && chattr +i -R /var/lib/docker/containers/$ID\ndocker rm -f dead-experiment\nAnd voila, docker could not delete the container root directory, so it falls to a status=dead:\ndocker ps -a -f status=dead\nCONTAINER ID        IMAGE         COMMAND       CREATED             STATUS        PORTS         NAMES\n616c2e79b75a        alpine        \"sh\"          6 minutes ago       Dead                        dead-experiment\nExplanation\nI've inspected the source code of docker and saw this state transition:\ncontainer.SetDead()\n// (...)\nif err := system.EnsureRemoveAll(container.Root); err != nil {\n    return errors.Wrapf(err, \"unable to remove filesystem for %s\", container.ID)\n}\n// (...)\ncontainer.SetRemoved()\nSo, if docker cannot remove the container root directory, it remain as dead and does not continue to the Removed state. So I've forced the file permissions to not permit root remove files (chattr -i).\nPS: to revert the directory permissions do this: chattr -i -R /var/lib/docker/containers/$ID",
    "Change hostname after running a container": "Some discussions here: https://github.com/docker/docker/issues/8902\nWhat I got from above discussion is\nadd SYS_ADMIN cap when run the container: https://github.com/docker/docker/issues/8902#issuecomment-218911749\nuse nsenter https://github.com/docker/docker/issues/8902#issuecomment-241129543",
    "Cannot change owner of Docker Volume directory to non-root user": "When you declare a directory as a VOLUME, you effectively can't use it in a Dockerfile any more. The basic reason is that volumes are set up when the container is run, not built.\nIn this case, you could simply move the VOLUME statement to the end of the Dockerfile. Any data in the image at that directory will be copied into the volume when the container is started.",
    "Pip3 is unable to install requirements.txt during docker build": "pip3 freeze outputs the package and its version installed in the current environment, no matter the package installed by pip or with other methods.\nIn fact, apt-clone==0.2.1 comes from debian package repo, not from pypi.org, see next:\n$ pip3 freeze | grep apt-clone\n$ apt-get install -y apt-clone\n$ dpkg -l apt-clone\nDesired=Unknown/Install/Remove/Purge/Hold\n| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\n|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\n||/ Name           Version      Architecture Description\n+++-==============-============-============-=================================\nii  apt-clone      0.4.1        all          Script to create state bundles\n$  dpkg -L apt-clone\n/.\n/usr\n/usr/share\n/usr/share/doc\n/usr/share/doc/apt-clone\n/usr/share/doc/apt-clone/copyright\n/usr/share/doc/apt-clone/changelog.gz\n/usr/share/man\n/usr/share/man/man8\n/usr/share/man/man8/apt-clone.8.gz\n/usr/lib\n/usr/lib/python3\n/usr/lib/python3/dist-packages\n/usr/lib/python3/dist-packages/apt_clone.py\n/usr/lib/python3/dist-packages/apt_clone-0.2.1.egg-info\n/usr/bin\n/usr/bin/apt-clone\n$ pip3 freeze | grep apt-clone\napt-clone==0.2.1\nYou could see from above, the apt_clone.py & apt_clone-0.2.1.egg-info are installed by debian package apt-clone, the 0.4.1 is just the debian package version, while 0.2.1 is the python package version.\nSo, for apt-clone similar, you need to install them with apt although they are seen in pip3 freeze.",
    "If I am using docker-compose.yml why would I still need a Dockerfile \u2013 an if so, how many?": "In your example, you are using already built containers (node:15.8.0 / redis), that's why docker-compose doesn't need any Dockerfile.\nThe Dockerfile is needed when you want to build your own image. In that case, docker-compose will need instructions on how to build your image.\nSo just to summarize - if you want to use some existing container \"as is\" - you don't need Docekrfile. docker-compose will pull the image from docker registry and it'll be ready to use without any modifications. On the other hand, if you want to build your own container (e.g. building an application), you'll need Dockerfile.",
    "Leading spaces inside Dockerfile for readability": "You can indent lines in Dockerfile, but usually it's used only when breaking long command lines, like:\nRUN export ADMIN_USER=\"mark\" \\\n    && echo $ADMIN_USER > ./mark \\\n    && unset ADMIN_USER\nYou can use indenting for instructions, but i, personally, wouldn't do that -- each instruction creates new layer and it's logical to place them with equal indent. As extra indenting like:\nFROM python:3.8-buster\n  RUN pip --no-cache-dir install poetry gunicorn\nwould look like it introduces sub-layers(and Docker doesn't have such concept).\nBut again, that's personal, and if you and your team agrees on that formatting standard -- there's a bunch of linters that would allow you to use any formatting standard with little(or no) tweaking:\nHaskell Dockerfile Linter -- check it online\nFROM:latest -- check it online\ndockerfile-lint",
    "Setting alias in Dockerfile not working: command not found": "The problem is that the alias only exists for that intermediate layer in the image. Try the following:\nFROM ubuntu\n\nRUN apt-get update && apt-get install python3-pip -y\n\nRUN alias python=python3\nTesting here:\n\u2770mm92400\u2759~/sample\u2771\u2714\u227b docker build . -t testimage\n...\nSuccessfully tagged testimage:latest\n\n\u2770mm92400\u2759~/sample\u2771\u2714\u227b docker run -it testimage bash\nroot@78e4f3400ef4:/# python\nbash: python: command not found\nroot@78e4f3400ef4:/#\nThis is because a new bash session is started for each layer, so the alias will be lost in the following layers.\nTo keep a stable alias, you can use a symlink as python does in their official image:\nFROM ubuntu\n\nRUN apt-get update && apt-get install python3-pip -y \n\n# as a quick note, for a proper install of python, you would\n# use a python base image or follow a more official install of python,\n# changing this to RUN cd /usr/local/bin \n# this just replicates your issue quickly \nRUN cd \"$(dirname $(which python3))\" \\\n    && ln -s idle3 idle \\\n    && ln -s pydoc3 pydoc \\\n    && ln -s python3 python \\ # this will properly alias your python\n    && ln -s python3-config python-config\n\nRUN python -m pip install -r requirements.txt\nNote the use of the python3-pip package to bundle pip. When calling pip, it's best to use the python -m pip syntax, as it ensures that the pip you are calling is the one tied to your installation of python:\npython -m pip install -r requirements.txt",
    "Error: cannot find automatically a registry where to push images - Kamel Kubernetes": "You need to set the container registry where kamel can pull/push images\nFor example\nkamel install --registry=https://index.docker.io/v1/",
    "docker-compose env file set by command line": "The -e flag is meant to pass the environment variables to the container. Add your environment before your docker run command to assign the environment variable to the docker engine so that the environment variable interpolation can be done\ndocker-compose run -e ENVIROMENT=local spring-app\n[...]\nERROR: Couldn't find env file: /Users/sabhat/code/scratch/.env\nENVIROMENT=local docker-compose run spring-app\n[...]\nStarting scratch_docker-mariadb_1\nAs an aside, hope you are aware that docker-compose run is meant to run a one-time command for a service - and it doesn't map the ports and also overrides the run commands defined in the service. You should be using docker-compose up to bring up the entire set of containers",
    "Does the hash '#' in a Dockerfile comment need to be in column 1?": "Looking at the Docker CLI files:\nOn the file parser line 45 we find\nline := strings.TrimLeftFunc(string(scannedBytes), unicode.IsSpace)\nIt trims empty spaces from the left. So if the non-first space character would be a # that would count as a comment for any code that follows the left trim.\nThe isSpace function checks for the following characters\n'\\t', '\\n', '\\v', '\\f', '\\r', ' ', U+0085 (NEL), U+00A0 (NBSP).\nThese would all be removed by the code on line 45 until they encounter a character that does not fit these specifications.\n# Nothing trimmed\n           # 1 tab 7 spaces trimmed\n    0 # 4 spaces trimmed\nThen on line 48 we find where it tests if it's a comment\n  if len(line) > 0 && !strings.HasPrefix(line, \"#\") {\nSo any space characters that are stripped by strings.TrimLeftFunc will not \"invalidate\" a comment.\nSo in conclusion on your question Does the hash '#' in a Dockerfile comment need to be in column 1? the answer is no, it can be preceded by space characters and still remain a comment.\n# Nothing trimmed   < -- comment\n# 1 tab 7 spaces trimmed < -- comment\n0 # 4 spaces trimmed  < -- not a comment",
    "Copy root folder to docker root path?": "Docker daemon runs within the context of the current directory. So you will need to copy the files to the directory from where your are running the Dockerfile.\nRefer: https://github.com/moby/moby/issues/4592",
    "Dockerfile pass environments on docker compose build": "You can set build arguments with docker compose as described here:\ndocker-compose build [--build-arg key=val...]\ndocker-compose build --build-arg REP_USER=myusername --build-arg REP_PASS=mypassword\nBtw, AFAIK build arguments are a compromise between usability and deterministic building. Docker aims to build in a deterministic fashion. That is, wherever you execute the build the produced image should be the same. Therefore, it appears logical that the client ignores the environment (variables) it is executed in.",
    "What's the best way to setup playwright in Apache Airflow in Docker?": "The best way to do it is using playwright docker image as base image, then you won't need to install its dependencies. Take a look to the documentation here: https://playwright.dev/docs/docker",
    "docker-compose up gives 'failed to read dockerfile: error from sender'": "The error is telling you that the Dockerfile was not found, because the path doesn't exist. That's because it is trying to enter the path as folder.\nThe system cannot find the path specified.\nThis comes because you made a mistake in the compose build syntax. There are 2 ways it can be used.\n1. The simple form:\nThis is using ./users/ as context, expecting a Dockerfile to be in this directory.\nuser:\n  build: ./user\n2. The complex form:\nuser:\n  build:\n    context: ./\n    dockerfile: ./users/Dockerfile\nThis lets you separate the context and where the Dockerfile is. In this example, the current folder is used as context, and the Dockerfile is taken from ./users/Dockerfile. It is also useful when you have a different name for your Dockerfile. I.E. Dockerfile.dev.\nNote that this is just an example, I don't know if this would make sense in your project. You need to know what context is the correct one.\nWhat do I mean by context?\nThe docker build command builds Docker images from a Dockerfile and a \u201ccontext\u201d. A build\u2019s context is the set of files located in the specified PATH or URL. The build process can refer to any of the files in the context. For example, your build can use a COPY instruction to reference a file in the context.\nAs example:\ndocker build --file /path/to/Dockerfile /path/to/context",
    "Disadvantage of using --allow-releaseinfo-change for apt-get update": "Potentially - \"Yes, it does add a risk.\"\nThis is evident from reading the documentation on the --allow-releaseinfo-change option in man apt-get. Specifically, man apt-secure describes and discusses its role in ensuring the integrity of the archives from which updates & upgrades are drawn. This risk applies to both signed and unsigned repositories.\nThe documentation further suggests that this risk may be mitigated through the use of speciality options that limit acceptable changes to certain fields (labels) in the repo; for example, suite. See the documentation for all the details.",
    "Failed to install llvm-lite with a Dockerfile": "Firstly, LLVM-Lite requires LLVM.\nSo, it's necessary to install the LLVM correctly, to do this:\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    libedit-dev  \\\n    llvm-{version} \\\n    llvm-{version}-dev\nThen, set the environment variable used when building LLVM-Lite and install pip package:\nRUN LLVM_CONFIG=/usr/bin/llvm-config-{version} pip install llvmlite=={version}\nThis will solve your problem.\nTo find out which version of LLVM is compatible with each version of LLVM-Lite go to:\nhttps://github.com/numba/llvmlite#compatibility",
    "Why do I need to mount Docker volumes for both php and nginx? And how to copy for production?": "The reason you need to provide your sources for both nginx and php is rather simple. The webserver needs all of your assets (images, stylesheets, javascript etc.) so that it can serve them to the client. PHP is interpreted serverside hence it requires your PHP source files and any referenced file (configs etc.) in order to execute them. In this case you are using PHP-FPM which is decoupled from the webserver and runs in a standalone fashion.\nMany projects cleanly seperate frontend and backend code and only provide the frontend sources to the webserver and the backend code to the PHP container.\nAnother quick tip regarding docker: It is generally a good idea to compact RUN statements in the Dockerfile to avoid excessive layers in the image. Your Dockerfile could be compacted to:\nFROM php:7.2-fpm-alpine\n\nRUN docker-php-ext-install pdo pdo_mysql \\\n    && chown -R www-data:www-data /var/www \\\n    && chmod 755 /var/www",
    "I am getting a returned a non-zero code: 8 when building my docker file": "If you go to this link, then 3.4.13 doesn't exist anymore\nhttps://www.apache.org/dist/zookeeper/\nYou can change to ENV ZOOKEEPER_VERSION 3.4.14, or just use an existing Zookeeper Docker image",
    "How to create a docker container which every night make a backup of mysql database?": "You could use the cron service from your host system to run the following command as described in the documentation for the mysql docker image:\ncrontab example for running the command every night at 2:00 am:\n00 02 * * * /usr/bin/docker exec db-mysql sh -c 'exec mysqldump --all-databases -uroot -p\"my-secret-pw\"' > /some/path/on/your/host/all-databases.sql\nAlternatively you could run another container designed just for this task such as deitch/mysql-backup:\ndocker run --name db-mysql -d \\\n    -e MYSQL_ROOT_PASSWORD=my-secret-pw \\\n    -e MYSQL_USER=my-user \\\n    -e MYSQL_PASSWORD=my-user-password \\\n    -e MYSQL_DATABASE=my-db \\\n    mysql:latest\n\ndocker run -d --restart=always \\\n    --name=db-backup \\\n    -e DB_DUMP_BEGIN=0200 \\\n    -e DB_SERVER=db-mysql \\\n    -e DB_USER=my-user \\\n    -e DB_PASS=my-user-password \\\n    -e DB_NAMES=my-db \\\n    -e DB_DUMP_TARGET=/db \\\n    -v /somewhere/on/your/host/:/db \\\n    databack/mysql-backup\nYou also need to make sure the /somewhere/on/your/host/ folder is writable by users of group 1005:\nsudo chgrp 1005 /somewhere/on/your/host/\nsudo chmod g+rwX /somewhere/on/your/host/\nBut this container must have a mean to connect to your db-mysql container. For that you create a docker network and connect both containers to it:\ndocker network create mysql-backup-net\ndocker network connect mysql-backup-net db-backup\ndocker network connect mysql-backup-net db-mysql",
    "How to create a file using touch in Dockerfile or docker-compose?": "I am not sure about the docker-compose.yml but the dockerfile that you have seems to be working for me.\nThe Dockerfile looks like this,\nFROM python:3.6-slim\n\nRUN mkdir /app\nWORKDIR /\nRUN touch /app/modbus.db\nBuild the dockerfile,\ndocker build -t test .\nSending build context to Docker daemon  2.048kB\nStep 1/4 : FROM python:3.6-slim\n ---> 903e8a0f0681\nStep 2/4 : RUN mkdir /app\n ---> Using cache\n ---> c039967bf463\nStep 3/4 : WORKDIR /\n ---> Using cache\n ---> c8c81ac01f50\nStep 4/4 : RUN touch /app/modbus.db\n ---> Using cache\n ---> 785916fe4cea\nSuccessfully built 785916fe4cea\nSuccessfully tagged test:latest\nBuild the container,\ndocker run -dit test\n52cde500cda015f170140ae9e7174a0367b29265a49a3742173946b686179fb3\nI ssh'ed into the container and was able to find the file.\ndocker exec -it 52cde500cda015f170140ae9e7174a0367b29265a49a3742173946b686179fb3 /bin/bash\nroot@52cde500cda0:/# ls\napp  bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\nroot@52cde500cda0:/# cd app\nroot@52cde500cda0:/app# ls\nmodbus.db ",
    "Docker: Is the server running on host localhost error": "Your application tries to connect to PostgreSQL running on localhost. PostgreSQL, though, is obviously not running on localhost. You'll have to add a container to your docker compose configuration that starts PostgreSQL. Then, configure your Python application to use that name instead of localhost.",
    "Different process are running as PID 1 when running CMD/ENTRYPOINT in shell form when the base images is centos vs ubuntu:trusty": "This is the behavior of bash. Docker is still running the command with a shell which you can identify with an inspect:\n$ docker inspect test-centos-entrypoint --format '{{.Config.Entrypoint}}'\n[/bin/sh -c ping localhost]\nYou can see the version of /bin/sh (note the GNU bash part):\n$ docker exec -it quicktest /bin/sh --version\nGNU bash, version 4.2.46(2)-release (x86_64-redhat-linux-gnu)\nCopyright (C) 2011 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n\nThis is free software; you are free to change and redistribute it.                               \nThere is NO WARRANTY, to the extent permitted by law.\nThe ubuntu version of /bin/sh (possibly dash) doesn't even support the --version flag and is not linked to bash. But if you change the ubuntu image to use bash instead of /bin/sh, you'll see the behavior matching centos:\n$ cat df.ubuntu-entrypoint\nFROM ubuntu:trusty\nENTRYPOINT [ \"/bin/bash\", \"-c\", \"ping localhost\" ]\n\n$ DOCKER_BUILDKIT=0 docker build -t test-ubuntu-entrypoint -f df.ubuntu-entrypoint .\nSending build context to Docker daemon  23.04kB\nStep 1/2 : FROM ubuntu:trusty\n ---> 67759a80360c\nStep 2/2 : ENTRYPOINT [ \"/bin/bash\", \"-c\", \"ping localhost\" ]\n ---> Running in 5c4161cafd6b\nRemoving intermediate container 5c4161cafd6b\n ---> c871fe2e2063\nSuccessfully built c871fe2e2063\nSuccessfully tagged test-ubuntu-entrypoint:latest\n\n$ docker run -d --name quicktest2 --rm test-ubuntu-entrypoint\n362bdc75e4a960854ff17cf5cae62a3247c39079dc1290e8a85b88114b6af694\n\n$ docker exec -it quicktest2 ps -ef\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 13:05 ?        00:00:00 ping localhost\nroot         8     0  0 13:05 pts/0    00:00:00 ps -ef",
    "Possible to get the git branch name in a dockerfile?": "Dockerfile supports build arguments and environment variables. So its possible to specify which branch to use and then decide command based upon the branch. For example, to pass build args use docker build--build-arg =. Then you can set a variable based upon the git_branch variable that decides the env to use.\nFROM busybox\nARG git_branch\nRUN echo \u201cBuilding $git_branch\u201d",
    "How to user docker exec with zsh": "It looks like zsh is not installed on that image as /bin/zsh would likely be the path. You can create a new Dockerfile that uses the base image and installs zsh, or you can install it within the container temporarily and launch from bash.",
    "Docker Python script can't find file": "There are two issues I've identified so far. Maya G points out a third in the comments below.\nIncorrect conditional logic\nYou need to replace:\nif len(sys.argv) >= 2:\n    sys.exit('ERROR: Received 2 or more arguments. Expected 1: Input file name')\nWith:\nif len(sys.argv) > 2:\n    sys.exit('ERROR: Received more than two arguments. Expected 1: Input file name')\nBear in mind that the first argument given to the script is always its own name. This means you should be expecting either 1 or 2 arguments in sys.argv.\nIssues with locating the default file\nAnother problem is that your docker container's working directory is /home/aws, so when you execute your Python script it will try to resolve paths relative to this.\nThis means that:\nwith open('inputfile.txt') as f:\nWill be resolved as /home/aws/inputfile.txt, not /home/aws/myapplication/inputfile.txt.\nYou can fix this by either changing the code to:\nwith open('myapplication/inputfile.txt') as f:\nOr (preferred):\nwith open(os.path.join(os.path.dirname(__file__), 'inputfile.txt')) as f:\n(Source for the above variation)\nUsing CMD vs. ENTRYPOINT\nIt also seems like your script apparently isn't receiving myapplication/inputfile.txt as an argument. This might be a quirk with CMD.\nI'm not 100% clear on the distinction between these two operations, but I always use ENTRYPOINT in my Dockerfiles and it's given me no grief. See this answer and try replacing:\nCMD [\"python\", \"/myapplication/script.py\", \"/myapplication/inputfile.txt\"]\nWith:\nENTRYPOINT [\"python\", \"/myapplication/script.py\", \"/myapplication/inputfile.txt\"]\n(thanks Maya G)",
    "Docker compose doesn't recognize 'env_file'": "Like many other version related issues, updating to v1.7.1 of docker-compose resolved the issue, works like a charm!",
    "MySQL bind-address in a Docker container": "sed is usually the weapon of choice for such tasks. Taken from the official mysql dockerfile:\nRUN sed -Ei 's/^(bind-address|log)/#&/' /etc/mysql/my.cnf\nThe command comments out lines starting with bind-address or log in my.cnf or conf.d/*.",
    "In Dockerfile how to copy file from network drive": "Not easily, considering ADD or COPY uses the Dockerfile context (the current folder or below) to seek their resources.\nIt would be easier to cp that file first to the Dockerfile folder (before a docker build .), and leave in said Dockerfile a COPY myfile /opt/files/file directive.\nOr you could run the container, and use a docker cp //somenetwork/somefiles/myfile /opt/files/file to add that file at rintime",
    "Setting conditional variables in a Dockerfile": "Ok, was not complete. Here a full working solution:\nDockerfile:\nFROM ubuntu:18.04\n\nARG TARGETARCH\n\nARG DOWNLOAD_amd64=\"x86_64\"\nARG DOWNLOAD_arm64=\"aarch64\"\nWORKDIR /tmp\nARG DOWNLOAD_URL_BASE=\"https://download.url/path/to/toolkit-\"\nRUN touch .env; \\\n    if [ \"$TARGETARCH\" = \"arm64\" ]; then \\\n    export DOWNLOAD_URL=$(echo $DOWNLOAD_URL_BASE$DOWNLOAD_arm64) ; \\\n    elif [ \"$TARGETARCH\" = \"amd64\" ]; then \\\n    export DOWNLOAD_URL=$(echo $DOWNLOAD_URL_BASE$DOWNLOAD_amd64) ; \\\n    else \\\n    export DOWNLOAD_URL=\"\" ; \\\n    fi; \\\n    echo DOWNLOAD_URL=$DOWNLOAD_URL > .env; \\\n    curl ... #ENVS JUST VALID IN THIS RUN!\n \nCOPY ./entrypoint.sh ./entrypoint.sh\nENTRYPOINT [\"/bin/bash\", \"entrypoint.sh\"]\nentrypoint.sh\n#!/bin/sh\n\nENV_FILE=/tmp/.env\nif [ -f \"$ENV_FILE\" ]; then\n    echo \"export \" $(grep -v '^#' $ENV_FILE | xargs -d '\\n') >> /etc/bash.bashrc\n    rm $ENV_FILE\nfi\n\ntrap : TERM INT; sleep infinity & wait\nTest:\n# bash\nroot@da1dd15acb64:/tmp# echo $DOWNLOAD_URL\nhttps://download.url/path/to/toolkit-aarch64\nNow for Alpine:\nDockerfile\nFROM alpine:3.13\nRUN apk add --no-cache bash\nEntrypoint.sh\nENV_FILE=/tmp/.env\nif [ -f \"$ENV_FILE\" ]; then\n    echo \"export \" $(grep -v '^#' $ENV_FILE) >> /etc/profile.d/environ.sh\n    rm $ENV_FILE\nfi\nAlpine does not accept xargs -d. But not that interesting here due to the fact URL does not contain any blank..\nTesting: Alpine just uses that for login shells.. so:\ndocker exec -it containername sh --login\necho $DOWNLOAD_URL",
    "Docker context on remote server \u201cError response from daemon: invalid volume specification\u201d": "You are getting invalid volume specification: \u2018C:\\Users\\user\\fin:/fin:rw\u2019 in your production environment is because, the host path C:\\Users\\user\\fin isn't available. You can remove it when you are deploying or change it to an absolute path which is available in your production environment as below.\nvolumes:\n    - '/root:/fin:rw'\nwhere /root is a directory available in my production environment.\n /path:/path/in/container mounts the host directory, /path at the /path/in/container\n\n path:/path/in/container creates a volume named path with no relationship to the host.\nNote the slash at the beginning. if / is present it will be considered as a host directory, else it will be considered as a volume",
    "Docker-compose invalid. Additional properties are not allowed": "I think the problem is resulted from the fact that you don't indent the \"networks\" property.\nI used to get \"(root) Additional property agent is not allowed\", but indent solves the problem.",
    "Unable to install sklearn when building docker image": "Try to downgrade your Python version, because Scikit Learn don't support Python 3.9 yet.",
    "Docker downloads newer image for supposedly-cached digest": "Given that this doesn't happen for every run, and likely wouldn't happen if you tested locally, the issue doesn't appear to be with your Dockerfile or FROM line. Docker does not automatically clean the cache, so you'll want to investigate what external processes are deleting the cache. Since you are running your builds in Jenkins with a kubernetes plugin, the issue appears to be from that plugin cleaning up build agents after a timeout. From the documentation, you can see various settings to tune this builder:\npodRetention Controls the behavior of keeping slave pods. Can be 'never()', 'onFailure()', 'always()', or 'default()' - if empty will default to deleting the pod after activeDeadlineSeconds has passed.\nactiveDeadlineSeconds If podRetention is set to 'never()' or 'onFailure()', pod is deleted after this deadline is passed.\nidleMinutes Allows the Pod to remain active for reuse until the configured number of minutes has passed since the last step was executed on it.\nOne method to workaround ephemeral build agents is to use the --cache-from option in the docker build command. With the classic build (vs buildkit) you need to first pull this image locally. That image would be from a previous build, and you can use multiple images for your cache, which is particularly useful for multi-stage builds since you'll need to pull a cache for each stage. This flag tells docker to trust the image pulled from a registry since normally only locally built images are trusted (there's a risk someone could inject a malicious image that claims to have run steps in a popular image but includes malware in the tar of that layer).",
    "Why does docker create containers while building images?": "For each line daemon creates a new image and each instruction runs independently. Docker Daemon uses intermediate images to accelerate the docker build process. Build cache indicates this.",
    "OSX Docker is nearly always consuming about 4GB of RAM with no active containers": "Docker runs natively only on Linux. On OSX there is a LinuxKit VM for Docker Desktop for Mac underneath to emulate Linux. This of course adds some overhead. It's meant to be used for development and not for production.\nHere is some explanation about the memory usage.",
    "Is the lowercase \"dockerfile\" file name supported and how long has it been supported?": "I guess this was the change -\nhttps://github.com/spf13/docker/commit/7b1a2bbf701dfc961d9e2e00cc2e56544bb162b4\nThis change was pushed on 17 Feb 2015. Probably we didn't realise it because it looks for a Dockerfile and falls back to dockerfile.",
    "How do I answer install prompts (other than with \"yes\") automatically?": "If you want to script a terminal interaction, you could use expect on Linux (that might not be very easy; you need to predict the interactions).\nRemember that terminal emulators are complex and arcane things (because terminals like VT100 have been complex). See termios(3), pty(7) and read The Tty demystified.",
    "Docker for Windows building added prefix `/var/lib/docker/tmp/` for COPY?": "When you run the build command\ndocker build .\nThe . in that command indicates the \"build context\" that gets sent to the (possibly remote) docker engine. All of the COPY and ADD commands must be from inside this directory (or from a previous stage with multi stage builds). You can also exclude files from this context using the .dockerignore file. The tmp directory is a uniquely generated internal directory of docker's consisting of this build context.\nTo fix your issue, you need to make sure ProcessFiles/ProcessFiles.csproj exists in the directory where you run your build from, and that you didn't exclude it from the context with the .dockerignore file.\nEdit: based on your comments, change your copy command to:\nCOPY ProcessFiles.csproj ProcessFiles/",
    "Docker. Spring application. set & get environment variable": "In java code you are using java system property, but not the system environment variable. In order to pass system property to java process you need to specify -Dkey=value in running command.\nSo if this is tomcat you can set in $JAVA_OPTS=\"... -DJDBC_CONNECTION_STRING=$JDBC_CONNECTION_STRING\"",
    "Why do Docker official images not use USER as required by \"best practices\"": "Fundamentally, USER is not possible in official images. It conflicts with the requirement that \"A beginning user should be able to docker run official-image bash without needing to learn about --entrypoint\". If you don't have root, you can't edit config files, install packages like strace... or particularly, fixup UIDs in volumes. Realistically, the official image style is considered (a) best practice. (So the Docker userguide should put the emphasis on running daemons as non-root and less on USER specifically)\nIMO this is a problem. The popular examples you can learn from don't show the need to set fixed UIDs. Otherwise if you update with a base image that adds another user, you'll have to intervene manually. The Best Practices say you should consider setting fixed UIDs, but they don't even show an example of it. So prominent examples of simple Dockerfiles that use USER aren't setting fixed UIDs. The official images don't set fixed UIDs either - pretending like this isn't a problem - but then brute-force data volumes with chown, because their entrypoint scripts run as root. Not very impressive.\nTechnically, the official Dockerfiles could be fixed by adding even more chown and UID swapping to the Dockerfile, but that seems undesirable.\nI suppose the other alternative would be a path-dependent update. That is, keep the chown around until everyone's done their automatic updates to fixed UIDs (a couple of months?), and then drop it.",
    "\"correct\" way to manage database schemas in docker": "We use Postgres and Docker where I work and we ended up doing the following:\nCopy the Dockerfile from the official Postgres repo so you can make your own image.\nModify docker-entrypoint.sh (https://github.com/docker-library/postgres/blob/8f80834e934b7deaccabb7bf81876190d72800f8/9.4/docker-entrypoint.sh), which is what is called when the container starts.\nAt the top of docker-entrypoint.sh, I put in the following:\n# Get the schema\nurl=$(curl -s -u ${GIT_USER}:${GIT_PASSWORD} \"${SQL_SCRIPT_URL}\" | python -c 'import sys, json; print json.load(sys.stdin)[\"download_url\"]')\ncurl ${url} > db.sh\nchmod +x db.sh\ncp db.sh ./docker-entrypoint-initdb.d\nThis basically downloads a shell script from Github that initializes the schema for the database. We do this to manage versions of the schema, so when you start your container you can tell it which schema to use via an ENV variable.\nSome notes about the code:\nWe need to refactor to pull stuff from Github using a private key instead of user credentials.\nThe ./docker-entrypoint-initdb.d directory is a place where docker-entrypoint.sh will look to run init scripts for the database. You can move files to that location however you want. Do this if downloading from Github is not applicable.",
    "Set condition based on CPU-Arch in Dockerfile": "As Daniel mentioned, Docker provides predefined environmental variables that are accessible at container build time.\nWe don't need to do anything Docker specific with these however, we can simply use them in a bash script condition:\nFROM python:3-bullseye\n\nRUN apt-get update && apt-get install -y ffmpeg firefox-esr npm\nARG TARGETARCH\nRUN if [ $TARGETARCH = \"arm64\" ]; then \\\n        apt-get install -y libavformat-dev libavdevice-dev python3-av \\\n    ; fi\nRUN npm install -g webdriver-manager\nRUN webdriver-manager update --gecko\nNote the necessary specification of ARG TARGETARCH to make that environmental variable accessible by dispatched processes at build time.",
    "Is there a tool to convert an ansible-playbook to a Dockerfile? [closed]": "Not to convert into a Dockerfile bit to spit out a container image on the other side of the build pipeline, you could use ansible-bender.",
    "Get image tag inside Dockerfile": "Use a bash script with the tag wanted in the first positional argument of the script:\n#!/bin/bash\ndocker build --build-arg sw-ver=$1 -t my_image:$1 .\nThen tun the script:\nscript.sh 1.2.3",
    "Why Docker build need to use /dev/shm?": "The --shm-size option of docker build sets the /dev/shm size for intermediate containers that are started as part of the build process.",
    "How to CMD Powershell script in Dockerfile": "See this discussion:\nMicrosoft removed powershell and other pieces from base nanoserver image, You need to use image with built in poweshell.\nAnd, also, I found the offical doc:\nhttps://learn.microsoft.com/en-us/windows-server/get-started/nano-in-semi-annual-channel\nPowerShell Core, .NET Core, and WMI are no longer included by default, but you can include PowerShell Core and .NET Core container packages when building your container.\nAnd this is the Dockerfile which could guide you on how to install powershell in nanoserver:\n# escape=`\n# Args used by from statements must be defined here:\nARG fromTag=1709\nARG InstallerVersion=nanoserver\nARG InstallerRepo=mcr.microsoft.com/powershell\nARG NanoServerRepo=mcr.microsoft.com/windows/nanoserver\n\n# Use server core as an installer container to extract PowerShell,\n# As this is a multi-stage build, this stage will eventually be thrown away\nFROM ${InstallerRepo}:$InstallerVersion  AS installer-env\n\n# Arguments for installing PowerShell, must be defined in the container they are used\nARG PS_VERSION=6.2.0-rc.1\n\nARG PS_PACKAGE_URL=https://github.com/PowerShell/PowerShell/releases/download/v$PS_VERSION/PowerShell-$PS_VERSION-win-x64.zip\n\nSHELL [\"pwsh\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';\"]\n\nARG PS_PACKAGE_URL_BASE64\n\nRUN Write-host \"Verifying valid Version...\"; `\n    if (!($env:PS_VERSION -match '^\\d+\\.\\d+\\.\\d+(-\\w+(\\.\\d+)?)?$' )) { `\n        throw ('PS_Version ({0}) must match the regex \"^\\d+\\.\\d+\\.\\d+(-\\w+(\\.\\d+)?)?$\"' -f $env:PS_VERSION) `\n    } `\n    $ProgressPreference = 'SilentlyContinue'; `\n    if($env:PS_PACKAGE_URL_BASE64){ `\n        Write-host \"decoding: $env:PS_PACKAGE_URL_BASE64\" ;`\n        $url = [System.Text.Encoding]::Unicode.GetString([System.Convert]::FromBase64String($env:PS_PACKAGE_URL_BASE64)) `\n    } else { `\n        Write-host \"using url: $env:PS_PACKAGE_URL\" ;`\n        $url = $env:PS_PACKAGE_URL `\n    } `\n    Write-host \"downloading: $url\"; `\n    [Net.ServicePointManager]::SecurityProtocol = [Net.ServicePointManager]::SecurityProtocol -bor [Net.SecurityProtocolType]::Tls12; `\n    New-Item -ItemType Directory /installer > $null ; `\n    Invoke-WebRequest -Uri $url -outfile /installer/powershell.zip -verbose; `\n    Expand-Archive /installer/powershell.zip -DestinationPath \\PowerShell\n\n# Install PowerShell into NanoServer\nFROM ${NanoServerRepo}:${fromTag}\n\nARG VCS_REF=\"none\"\nARG PS_VERSION=6.2.0-rc.1\nARG IMAGE_NAME=mcr.microsoft.com/powershell\n\nLABEL maintainer=\"PowerShell Team <powershellteam@hotmail.com>\" `\n      readme.md=\"https://github.com/PowerShell/PowerShell/blob/master/docker/README.md\" `\n      description=\"This Dockerfile will install the latest release of PowerShell.\" `\n      org.label-schema.usage=\"https://github.com/PowerShell/PowerShell/tree/master/docker#run-the-docker-image-you-built\" `\n      org.label-schema.url=\"https://github.com/PowerShell/PowerShell/blob/master/docker/README.md\" `\n      org.label-schema.vcs-url=\"https://github.com/PowerShell/PowerShell-Docker\" `\n      org.label-schema.name=\"powershell\" `\n      org.label-schema.vcs-ref=${VCS_REF} `\n      org.label-schema.vendor=\"PowerShell\" `\n      org.label-schema.version=${PS_VERSION} `\n      org.label-schema.schema-version=\"1.0\" `\n      org.label-schema.docker.cmd=\"docker run ${IMAGE_NAME} pwsh -c '$psversiontable'\" `\n      org.label-schema.docker.cmd.devel=\"docker run ${IMAGE_NAME}\" `\n      org.label-schema.docker.cmd.test=\"docker run ${IMAGE_NAME} pwsh -c Invoke-Pester\" `\n      org.label-schema.docker.cmd.help=\"docker run ${IMAGE_NAME} pwsh -c Get-Help\"\n\n# Copy PowerShell Core from the installer container\nENV ProgramFiles=\"C:\\Program Files\" `\n    # set a fixed location for the Module analysis cache\n    LOCALAPPDATA=\"C:\\Users\\ContainerAdministrator\\AppData\\Local\" `\n    PSModuleAnalysisCachePath=\"$LOCALAPPDATA\\Microsoft\\Windows\\PowerShell\\docker\\ModuleAnalysisCache\" `\n    # Persist %PSCORE% ENV variable for user convenience\n    PSCORE=\"$ProgramFiles\\PowerShell\\pwsh.exe\" `\n    # Set the default windows path so we can use it\n    WindowsPATH=\"C:\\Windows\\system32;C:\\Windows\"\n\n    # Set the path\nENV PATH=\"$WindowsPATH;${ProgramFiles}\\PowerShell;\"\n\nCOPY --from=installer-env [\"\\\\PowerShell\\\\\", \"$ProgramFiles\\\\PowerShell\"]\n\n# intialize powershell module cache\nRUN pwsh `\n        -NoLogo `\n        -NoProfile `\n        -Command \" `\n          $stopTime = (get-date).AddMinutes(15); `\n          $ErrorActionPreference = 'Stop' ; `\n          $ProgressPreference = 'SilentlyContinue' ; `\n          while(!(Test-Path -Path $env:PSModuleAnalysisCachePath)) {  `\n            Write-Host \"'Waiting for $env:PSModuleAnalysisCachePath'\" ; `\n            if((get-date) -gt $stopTime) { throw 'timout expired'} `\n            Start-Sleep -Seconds 6 ; `\n          }\"\n\nCMD [\"pwsh.exe\"]",
    "How to activate a conda environment before running Docker commands": "Do this with an ENTRYPOINT in your Dockerfile.\nsrc/entrypoint.sh\n#!/bin/bash\n\n# enable conda for this shell\n. /opt/conda/etc/profile.d/conda.sh\n\n# activate the environment\nconda activate my_environment\n\n# exec the cmd/command in this process, making it pid 1\nexec \"$@\"\nsrc/Dockerfile\n# ...\nCOPY ./entrypoint.sh ./entrypoint.sh\nRUN chmod +x ./entrypoint.sh\nENTRYPOINT [\"./entrypoint.sh\"]",
    "Why do we build and publish as two steps when publish also builds?": "According to the book .NET Microservices: Architecture for Containerized .NET Applications (Microsoft EBook), the first build instruction is redundant because the publish instruction also builds, and it is right after the first build instruction. Page 94 (86), line 10.\nHere is a short excerpt from the book:\n1 FROM microsoft/dotnet:2.1-aspnetcore-runtime AS base\n2 WORKDIR /app\n3 EXPOSE 80\n4\n5 FROM microsoft/dotnet:2.1-sdk AS build\n6 WORKDIR /src\n7 COPY src/Services/Catalog/Catalog.API/Catalog.API.csproj \u2026\n8 COPY src/BuildingBlocks/HealthChecks/src/Microsoft.AspNetCore.HealthChecks \u2026\n9 COPY src/BuildingBlocks/HealthChecks/src/Microsoft.Extensions.HealthChecks \u2026\n10 COPY src/BuildingBlocks/EventBus/IntegrationEventLogEF/ \u2026\n11 COPY src/BuildingBlocks/EventBus/EventBus/EventBus.csproj \u2026\n12 COPY src/BuildingBlocks/EventBus/EventBusRabbitMQ/EventBusRabbitMQ.csproj \u2026\n13 COPY src/BuildingBlocks/EventBus/EventBusServiceBus/EventBusServiceBus.csproj \u2026\n14 COPY src/BuildingBlocks/WebHostCustomization/WebHost.Customization \u2026\n15 COPY src/BuildingBlocks/HealthChecks/src/Microsoft.Extensions \u2026\n16 COPY src/BuildingBlocks/HealthChecks/src/Microsoft.Extensions \u2026\n17 RUN dotnet restore src/Services/Catalog/Catalog.API/Catalog.API.csproj\n18 COPY . .\n19 WORKDIR /src/src/Services/Catalog/Catalog.API\n20 RUN dotnet build Catalog.API.csproj -c Release -0 /app\n21\n22 FROM build AS publish\n23 RUN dotnet publish Catalog.API.csproj -c Release -0 /app\n24\n25 FROM base AS final\n26 WORKDIR /app\n27 COPY --from=publish /app\n28 ENTRYPOINT [\"dotnet\", \"Catalog.API.dll\"]\nFor the final optimization, it just happens that line 20 is redundant, as line 23 also builds the application and comes, in essence, right after line 20, so there goes another time-consuming command.",
    "How to build my own custom Ubuntu ISO with docker": "So incase anyone finds this post. The way to resolve the dns issue is to make sure your resolv.conf file in the chroot is actually pointing to a proper dns servers. Some apps like cubic already do this for you.",
    "Cannot access nodejs app on browser at localhost:4200 (docker run -p 4200:4200 ....)": "EDIT\nhttps://stackoverflow.com/a/48286174/2663059\nHas the solution.\nEDIT\nFirst of all . Why you have done this .\n#EXPOSE 4200\nin Dockerfile. # in front means comment in dockerfile . Use this\n EXPOSE 4200\n#EXPOSE 4200 means you have not expose the port .\nAnd next check that container running your node server inside the Docker or not. How can you check is this. docker exec -it nept0 bash or can try\ndocker exec -it nept0 /bin/bash\nThen you can run\ncurl localhost:4200 or the get api of the node.js if that working fine your node is working fine #EXPOSE 4200 was culprit .\nYou can read for docker exec here more https://docs.docker.com/engine/reference/commandline/exec/\nLet me know if any issue.",
    "Stop a Docker container after executing code": "Note: the default CMD for python:3 is python3.\nexit code 143 means SIGTERM as mentioned here. That is what docker sends.\nSo you need for your python3 application to process SIGTERM signal gracefully\nDon't forget that your python app, once completed and exited the main function, would cause the container to automatically stop and exit.\nThe OP adds in the comments:\nIn the meantime, I have found out that handling the SIGTERM in the Docker environment works perfectly fine.\nHowever using the same code in Docker on CloudFoundry does not prevent the container from crashing.\nIn CloudFoundry you need an application that is always running and not just doing a task and then stopping like a script does.\nEven stopping without errors is detected as a crash in CloudFoundry.\nI transformed my script into a REST server by using the flask framework. Now it is always running but only doing its task when being called via its url.",
    "Passing a list of arguments to docker at build / run time": "Build time\nThe best I can think of is to pass in a comma-separated list:\n docker build --build-arg repos=repo1,repo2\nDocker won't parse this into a list for you, but the scripts run during the build could split the string into a list.\nRun time\nIf you define your command using ENTRYPOINT then any trailing parameters to docker run get appended to the entry point command.\nSo if your Dockerfile contains:\n   ENTRYPOINT echo hello\nThen:\n   docker run myimage glorious world \n... will run the command\n   echo hello glorious world",
    "Daemonized buildbot start": "Buildbot bootstrap is based on Twisted's \".tac\" files, which are expected to be started using twistd -y buildbot.tac. The buildbot start script is actually just a convenience wrapper around twistd. It actually just run twistd, and then watches for the logs to confirm buildbot successfully started. There is no value added beyond this log watching, so it is not strictly mandatory to start buildbot with buildbot start. You can just start it with twistd -y buildbot.tac.\nAs you pointed up the official docker image is starting buildbot with twistd -ny buildbot.tac If you look at the help of twistd, -y means the Twisted daemon will run a .tac file, and the -n means it won't daemonize. This is because docker is doing process watching by itself, and do not want its entrypoint to daemonize.\nThe buildbot start command also has a --nodaemon option, which really only is 'exec'ing to twistd -ny. So for your dockerfile, you can as well us twistd -ny or buildbot start --nodaemon, this will work the same.\nAnother Docker specific is that the buildbot.tac is different. It configured the twistd logs to output to stdout instead of outputing to twisted.log. This is because docker design expects logs to be in stdout so that you can configure any fancy cloud log forwarder independently from the application's tech.",
    "Cannot Compile the typescript in Docker": "In the best practice when you create a Dockerfile, you should only user one CMD and one ENTRYPOINT.\nIn your case, It should be:\nCOPY code/ /usr/local/lib\nWORKDIR /usr/local/lib\n\nRUN npm install -g koa@2\nRUN npm install -g sequelize\nRUN npm install -g mysql\nRUN npm install -g typescript\n\nRUN sh -c rm webApp.js\nRUN sh -c tsc webApp.ts\nCMD [\"node\", \"webApp.js\"]",
    "What is the difference b/w a docker image generated by docker-compose build vs docker build?": "There are no differences between the actual image that gets built between docker-compose build and a \"manual\" docker build in terms of the contents of the image.\nThe difference is only in naming/tagging of the build result, which docker-compose does automatically for you. Other than that, the docker-compose build is no different behind the scenes and simply a wrapper for the normal docker build.",
    "Docker with ngnix and angular stuck at start worker process 30": "The container is up and running, it didn't stuck there. The container is run in attached mode by default. You can either run the container in detached mode using command -\ndocker run -d -p 8080:80 -v $(pwd)/dist:/usr/share/nginx/html nginx-angular\nOr leave it open and try checking container status in new terminal using command -\nsudo docker ps",
    "Dockerfile fails when installing a python local package with permission denied": "I hit this issue as well. I wound up building a wheel and then adding and installing the wheel in the image.\nlocal\npython setup.py bdist_wheel\nDockerfile\nFROM jupyter/scipy-notebook\nRUN conda install ipdb lxml -y\nADD dist/my_package-0.0.1-py3-none-any.whl my_package-0.0.1-py3-none-any.whl\nRUN pip install my_package-0.0.1-py3-none-any.whl",
    "How to NOT inherit labels from base images in Docker?": "I don't think there is a way to remove labels. I'm using my own 'namespace' when I add labels in my image so that I can find the ones I want easily later. Example label can be com.mycompanyname.foo set to value bar.",
    "Chrome/Chromium inside Docker and the sandbox": "Enable user namespaces in your kernel. >>another relevant thread<<",
    "Docker build fails on `add-apt-repository: not found`": "You can build an image using the current Dockerfile you have. I am assuming you are having an issue when you try to build 2 separate images.\nThat is because add-apt-repository will not be recognizable until there is software-properties-common or python-software-properties installed.\nIf both the runs are in a Dockerfile and you build an image using\ndocker build -t mydockerimage .\nYou will not see any issue because the second layer/run will build on top of the first, so the add-apt-repository is recognizable and you will not have any issue.\nI hope this answers your query.",
    "installing mongodb in a docker container": "I'm hoping you know that you don't have to go to the effort because there is already an official image on Docker Hub.\nHowever, your error message gives a hint to what went wrong:\nmongodb-org/3.4/multiverse/binary-amd64/Packages.gz     Hash Sum mismatch\nYou can correct this if you are determined to make your own image.\nOne place I would start is looking at the Docker best practices for run. You have split some related to the same dependency over multiple lines in your code, which can cause caching to mismatch versions and hashes. This is likely the problem.\nIf you aren't satisfied with using the official image directly you can at least look at how it was built here: https://github.com/docker-library/mongo/blob/c02ca4cce8c69e5069b75cb574d1b99d7b4edaeb/3.4/Dockerfile",
    "Simple docker containers: Build dedicated image or mount config as volume?": "If you 'bake in' the nginx config (your second approach)\nADD nginx.conf /etc/nginx/\nit makes your docker containers more portable - i.e. they can be downloaded and run on any server capable of running docker and it will just work.\nIf you use option 1, mounting the config file at run time, then you are transferring one of your dependencies to outside of your container. This makes it a dependency that must be managed outside of docker.\nIn my opinion it is best to put as many dependencies inside your dockerfiles as possible because it makes them more portable and more automated (great for CI Pipelines for example)\nThere are reasons for mounting files at run time and these are usually centred around environmentally specific settings (although these can largely be overcome within docker too) or 'sensitive' files that application developers shouldn't or couldn't have access to. For example ssl certificates, database passwords, etc",
    "wkhtmltopdf not working with .net 8 and docker file": "Official NET8 docker images are based on Debian 12 (\"bookworm\") which doesn't support old libssl1, this means that wkhtmltopdf binaries for older Debian (10,11) will not work.\nFortunatelly wkhtmltopdf build for \"bookworm\" is available, it can be downloaded here: https://github.com/wkhtmltopdf/packaging/releases\nTo include wkhtmltopdf add these lines to 'Dockerfile':\nRUN apt-get update && apt-get install -y --no-install-recommends wget ca-certificates fontconfig libc6 libfreetype6 libjpeg62-turbo libpng16-16 libssl3 libstdc++6 libx11-6 libxcb1 libxext6 libxrender1 xfonts-75dpi xfonts-base zlib1g\nRUN wget https://github.com/wkhtmltopdf/packaging/releases/download/0.12.6.1-3/wkhtmltox_0.12.6.1-3.bookworm_amd64.deb\nRUN dpkg -i wkhtmltox_0.12.6.1-3.bookworm_amd64.deb",
    "'403 Forbidden' apt-get update Ubuntu Dockerfile": "Can you add:\napt-get --allow-releaseinfo-change update\nbefore apt-update command",
    "\"Permission denied\" on file when running a docker container": "You may try this simple one.\nFROM alpine\nCOPY . /home/guestuser/bin/gateway\nRUN apk add libressl-dev\nRUN apk add libffi-dev\nWORKDIR /home/guestuser/bin/\nRUN chmod -R 755 /home/guestuser\nCMD [\"/bin/bash\", \"/home/guestuser/bin/gateway\"]\nOtherwise, run sleep command login to container and see your commands works manually",
    "How to connect to remote host from a docker container": "It could be different reasons why it doesn't work.\nNetworking misconfiguration: docker container runs in an isolated network environment, it knows nothing about postgres server on your localhost. If the connection to abc.com doesn't work either, there could be a problem with dns resolution, you should try to use an ip of the host to troubleshoot it.\nPostgres server misconfiguration: take a look to pg_hba.conf and postgresql.conf, there are settings for access restrictions. Check the connectivity from container with the commands pg_isready and psql.",
    "DOCKER How do I set an password and username on docker run with an windows image?": "The default shell for Windows images is cmd.exe. Therefore the ARG and ENV should be dereferenced the same way as in any windows cmd: %myarg%.\nSo in your case dereferencing should be done like: RUN net USER /ADD %user% %password% && RUN net localgroup Administrators %user% /ADD\nAlso, ENV statement should be placed after FROM statement, in order to have the environment variables available inside the container.\nOne can also change the shell to powershell using: SHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue'; $verbosePreference='Continue';\"]\nIn this case dereferencing would have the syntax: $env:myenv",
    "ERROR: unsatisfiable constraints: curl (missing): while building for jmeter dockerfile": "The error indicates that Alpine apk package management tool wasn't able to install curl, fontconfig and other packages due to not being able to connect to http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/ host and get the files from there.\nEnsure that your host machine has Internet and if it does follow recommendations from the My docker container has no internet answers.\nAlso be aware that currently JMeter 5.2 is out so I would recommend at least changing this line:\nENV MIRROR https://www-eu.apache.org/dist/jmeter/binaries\nto this one:\nENV MIRROR https://archive.apache.org/dist/jmeter/binaries\notherwise your Dockerfile will not work even if you resolve Internet connectivity issues.\nOptionally you can ramp-up JMETER_VERSION to match the latest stable JMeter release",
    "How to run Symfony console command inside the docker container": "Copy from https://docs.docker.com/compose/reference/exec/\nTo disable this behavior, you can either the -T flag to disable pseudo-tty allocation.\ndocker-compose exec -T nginx <command>\nOr, set COMPOSE_INTERACTIVE_NO_CLI value as 1\nexport COMPOSE_INTERACTIVE_NO_CLI=1\nFor php bin/console to run you need to run from app container like below.\ndocker-compose exec -T app php bin/console",
    "Conda not found when trying to build Docker image": "# Install Peddy\nRUN INSTALL_PATH=~/anaconda \\\n    && wget http://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh \\\n    && bash Miniconda2-latest* -fbp $INSTALL_PATH \\\n    && PATH=$INSTALL_PATH/bin:$PATH\nThe last part of the above updates a PATH variable that will only exist in the shell running the command. That shell exits immediately after setting the PATH variable, and the temporary container used to execute the RUN command exits. The result of the RUN command is to gather the filesystem changes into a layer of the docker image being created. Any environment variable changes, background processes launched, or anything else not part of the container filesystem is lost.\nInstead, you'll want to update the image environment with:\n# Install Peddy\nRUN INSTALL_PATH=~/anaconda \\\n    && wget http://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh \\\n    && bash Miniconda2-latest* -fbp $INSTALL_PATH \\\nENV PATH=/root/anaconda/bin:$PATH\nIf the software permits it, I would avoid installing in the /root home directory and instead install it somewhere like /usr/local/bin, making it available if you change the container to run as a different user.",
    "docker build - Avoid ADDing files only needed at build time": "Relying on docker commit indeed amounts to a hack :) and its use is thus mentioned as inadvisable by some references such as this blog article.\nI only see one possible approach for the kind of use case you mention (copy a one-time .deb package, install it and remove the binary immediately from the image layer):\nYou could make remotely available to the docker engine that builds your image, the .deb you'd want to install, and replace the COPY + RUN directives with a single one, e.g., relying on curl:\nRUN curl -OL https://example.com/foo.deb && dpkg -i foo.deb && rm -f foo.deb\nIf curl is not yet installed, you could run beforehand the usual APT commands:\nRUN apt-get update -y -q \\\n  && DEBIAN_FRONTEND=noninteractive apt-get install -y -q --no-install-recommends \\\n    ca-certificates \\\n    curl \\\n  && apt-get clean \\\n  && rm -rf /var/lib/apt/lists/*\nMaybe there is another possible solution (but I don't think the multi-staged builds Docker feature would be of some help here, as all perms would be lost by doing e.g. COPY --from=build / /).",
    "Not able to run source command from Dockerfile": "This is very similar to this question: How to solve 'ttyname failed: Inappropriate ioctl for device' in Vagrant?\nIn ~/.profile there is a line:\nmesg n || true\nThat is incompatible. In order to fix it, make it conditional upon if tty is available (credit Gogowitsch):\nRUN sed -i ~/.profile -e 's/mesg n || true/tty -s \\&\\& mesg n/g'\nThis sed command replaces mesg n || true (try it, and ignore if it fails) with tty -s && mesg n, (only try it if it will succeed) which makes the error message go away",
    "Add exchanges in rabbitmq with Dockerfile or docker-compose": "There seems to be two ways to address this issue:\nby referencing rabbitmq.config file via volume\nvia temporary configuration container and entrypoint script\nOption #2 appeals to me more, as the configuration is performed via REST calls.",
    "Dockerfile Public Key Permission Denied using Git (Bitbucket)": "Could not open a connection to your authentication agent.\nThat seems expected: the agent started in your Dockerfile in one layer would not be running in the next layer created by the next line of the Dockerfile: each container run from each line is then stopped and committed as an image.\nEven if you put both commands on the same line, the agent would still be running after said unique line.\nThat agent starting + ssh-add command should be part of your CMD script, which will run as well a foreground process.\nMeaning the Dockerfile should end with CMD script, with 'script' being the path of a (COPY'ed) script which includes what you want to run in your container, and that would start with the ssh agent and the ssh-add command.\nThe OP Chris points out in the comments:\nlayers are executed serially, with the current layer not having any context to prior ones.\nBased on that \"oh snap\" moment, I went on to consolidate all RUN commands into a single RUN command using \"&& \\\".\nEverything is working as expected.",
    "Reusable docker image for AngularJS": "EDIT : Better option is to use build args\nInstead of passing URL at docker run command, you can use docker build args. It is better to have build related commands to be executed during docker build than docker run.\nIn your Dockerfile,\nARG URL \nAnd then run\ndocker build --build-arg URL=<my-url> .\nSee this stackoverflow question for details",
    "External properties file using Spring Boot and Docker": "I think you only need to mount the volume to the conf folder e.g.\ndocker run -d -p 8080:8080 -v /opt/gpm/config:/conf --name gpm gpm-web:1.0",
    "Docker run results in \"host not found in upstream\" error": "I have solved this. There are two things at play.\nOne is how it works locally and the other is how it works in Docker Cloud.\nLocal workflow\ncd into root of project, where Dockerfile is located\nbuild image: docker build -t media-saturn:dev .\nrun the builded image: docker run -it --add-host=\"my-server-address.com:123.45.123.45\" -p 80:80   media-saturn:dev\nDocker cloud workflow\nAdd extra_host directive to your Stackfile, like this\nand then click Redeploy in Docker cloud, so that changes take effect\nextra_hosts:\n'my-server-address.com:123.45.123.45'\nOptimization tip\nignore as many folders as possible to speed up process of sending data to docker deamon\nadd .dockerignore file\ntypically you want to add folders like node_modelues, bower_modules and tmp\nin my case the tmp contained about 1.3GB of small files, so ignoring it sped up the process significantly",
    "Efficient Dockerfile for many apt-get packages": "I think you need to run apt-get update only once within the Dockerfile, typically before any other apt-get commands.\nYou could just first have the large list of known programs to install, and if you come up with a new one then just add a new RUN apt-get install -y abc to you Dockerfile and let docker continue form the previously cached command. Periodically (once a week, one a month?) you could re-organize them as you see fit or just run everything in a single command.\nI suppose I could combine them when dependencies are more stable, but I find I'm always overly optimistic about when that is.\nOh you actually mentioned this solution already, anyway there is no harm doing these \"tweaks\" every now and then. Just run apt-get update only once.",
    "/home/web/.gem/ruby/2.2.0/gems/redis-3.2.1/lib/redis/connection/ruby.rb:152:in `getaddrinfo': getaddrinfo: Name or service not known (SocketError)": "The REDIS_URL environment variable can be used to pass the Redis url to both Sidekiq and the Redis gem.\nThis environment variable can be set in the docker-compose.yml like this:\ndb:  \n  image: postgres\n  ports:\n    - \"5432\"\n\nredis:  \n  image: redis\n  ports:\n    - \"6379\"\n\nweb:  \n  build: .\n  command: bundle exec rails s -b 0.0.0.0\n  volumes:\n    - .:/app\n  ports:\n    - \"3000:3000\"\n  links:\n    - db\n    - redis\n  environment:\n    REDIS_URL: \"redis://redis:6379\"\nAs Sidekiq and Redis gems will use REDIS_URL by default, you also need to make sure that you are not overriding this default behavior in your config files. You don't need the file config/initializers/redis.rb anymore, and your config/app-config.yml should contain only your namespace:\ndefault: &default\n  redis_namespace: 'RAILS_CACHE'\n\ndevelopment:\n  <<: *default",
    "How to copy local wp-content files to Wordpress container using Dockerfile": "Today I ran into the same problem. I wanted to add theme when building the image by using COPY. However, it did not work. This was because I already set my wp-content folder as a volume. It seems you cannot COPY into a volume.\nThe discussion in the following link helped me realise this:\nhttps://github.com/docker-library/wordpress/issues/146\nBelow I have added my WordPres Dockerfile and docker-compose file. After commenting out the volume everything worked as expected.\nI sure hope you do not need it anymore after two years, haha.\nReference: https://docs.docker.com/samples/wordpress/\nDockerfile\nFROM wordpress:latest\n\nWORKDIR /var/www/html\n\nCOPY ./wp-content/ ./wp-content/\nRUN chmod -R 755 ./wp-content/\ndocker-compose.yml\nversion: \"3\"\nservices:\n  db:\n    build:\n      context: .\n      dockerfile: ./compose/local/db/Dockerfile\n    image: new_db_image_name\n    container_name: new_db_image_name\n    command: '--default-authentication-plugin=mysql_native_password'\n    volumes:\n      - db_data:/var/lib/mysql\n    restart: always\n    environment:\n      - MYSQL_ROOT_PASSWORD=somewordpress\n      - MYSQL_DATABASE=wordpress\n      - MYSQL_USER=wordpress\n      - MYSQL_PASSWORD=wordpress\n    expose:\n      - 3306\n      - 33060\n  wordpress:\n    build:\n      context: .\n      dockerfile: ./compose/local/wordpress/Dockerfile\n    image: new_wordpress_image_name\n    container_name: new_wordpress_image_name\n#    volumes:\n#      - wp-content:/var/www/html/wp-content\n    ports:\n      - 80:80\n    restart: always\n    environment:\n      - WORDPRESS_DB_HOST=db\n      - WORDPRESS_DB_USER=wordpress\n      - WORDPRESS_DB_PASSWORD=wordpress\n      - WORDPRESS_DB_NAME=wordpress\n\nvolumes:\n  db_data:\n#  wp-content:",
    "Issue creating OSM tile server using Docker": "I would recommend using a different volume name from the example provided as this worked for me.",
    "Conditionally expose port in Dockerfile": "I also looking for the solution.\nI known EXPOSE just some kind of docs for image. but I think it's really useful.\ndirectly NO\nseems the answer is NO for now.\nSO I have to create 2 (or more) dockerfile for that purpose.\nworkaround\nmaybe build from STDIN (generate dockerfile by other program) can be a workaround.\nBut it's really not that straightforward and effective\n// It is no different from dynamically creating a dockerfile.\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/#pipe-dockerfile-through-stdin\nhttps://docs.docker.com/build/building/context/#text-files\nhttps://docs.docker.com/engine/reference/builder/#environment-replacement:~:text=Dockerfile through STDIN",
    "Dynamically get JAR filename in DockerFile": "Instead of copying the jar file inside the dockerfile, you can mount the folder in which jar is created. And you will not be needed to be worried with the filename.\nWhat you want can be done using environment variables. You can create an --env-file and keep your version number there. And inside dockerfile, refer to value of variable declared in env file with {VERSIION}. By following this process, you may want to use this same version in your pom file. For that, take a look at this question.\nThe answer links environment variables by setting env.path and detailed guide is here.",
    "docker-compose build download few pom dependencies each time": "My previous answer didn't make sense (I mistakenly thought that Maven dependencies were handled at run time and they are handled at ONBUILD instructions instead). But I'll try to give a second explanation of why these specific dependencies are not cached.\nThe downloaded dependencies listed in the question are from the builtin plugins for Maven clean. In the parent Dockerfile we download the project dependencies at https://github.com/heroku/docker-java/blob/master/Dockerfile#L14 (everything in your POM). Then, later on in the Dockerfile it runs the clean at https://github.com/heroku/docker-java/blob/master/Dockerfile#L18. Since the maven-clean-plugin isn't part of your POM, it gets downloaded automatically at the clean step (it isn't cached from the previous Dockerfile instructions).\nSo, if you wanted to cache the maven-clean-plugin as well, you may need to add it as a dependency in your POM (you might be able to get away with just <scope>import</scope>).",
    "Docker running out of memory when loading large sql dump": "Surprisingly, it can be a bad dump, try to recreate it and retry. This helps to me.",
    "Build a Docker Image from private git repository": "Replacing git:// with ssh:// will solve the problem.",
    "Making a Docker build stage depend on another stage without using anything from it": "Can I have stage X fail if stage Y fails, even though X doesn't use anything from Y?\nUsing the modern BuildKit engine, if stage X is the final image and it doesn't use anything from Y, Docker won't even run the earlier stage.\nI'd suggest running your unit-test sequence outside of Docker, before you build an image. You should be able to configure your CI system to do the equivalent of:\nyarn install\nyarn test\ndocker build .\nThis also would let you omit the test code and dependencies from the final image, resulting in a smaller image and marginally faster build.\nIf it's really important to you to use no tool other than Docker, a trick from classic Makefiles is to have your test stage create an empty file on success, and then have the later stage copy that file. Now there is \"a dependency\" even though it's not actually a meaningful file.\nFROM workspace AS test\nRUN yarn test \\\n && touch /.tests-successful\n\nFROM nginx:1.23.1-alpine\nCOPY --from=test /.tests-successful /\nCOPY --from=compile /app/build /usr/share/nginx/html",
    "Spring boot apps port mapping in docker container": "Rather than providing an exact succinct solution, let me also explain why doesn't it work for you, I think it will be more valuable for our colleagues who will ever read this post.\nSo Starting with a spring boot part.\nSpring boot knowns nothing about the docker.\nWhen you put server.port: 8080 it only means that spring boot application starts the web server that is ready to get accept connections on that port.\nIf you want to change that mapping externally without modifying the application.yaml you can do that by passing the additional parameter:\njava -jar myapp.jar --server.port=8081\nThis will effectively override the definition in yaml. There are other ways as well (environment variable for example). Spring boot supports many different ways of configurations and there are ways that take precedence over the configuration specified in yaml. Among them SERVER_PORT environment variable for exmaple.\nYou can read the official spring boot documentation chapter for more information\nAll the above happens regardless the presence of docker in the setup.\nNow, as for the docker part.\nWhen you run the container with -p A:B this means that it will forward the port A in the host machine to port B inside the container.\nSo this is why it doesn't work: you run\ndocker run -d -p 8081:8081 -it user-service\nBut no-one is ready to accept the connections on port 8081 inside the container. So its not precise to say that your docker service doesn't work - it starts but you can't call it.\nSo the simplest way wound be running with something like -p 8081:8080 so that from the host machine's standpoint the ports won't clash, but the container will be accessible.\nIf you want to also change the port in the docker container (for whatever reason) you can read the above about the spring boot part (with --server.port and everything)",
    "How to install vim in a docker image based on mysql version 8?": "If you take a look at the Tag for 8.0 you can see that the base uses a different version of Oracle linux (8 vs 7). Yum is not installed in 8. Instead, there's a minimal installer (microdnf). So this substitution should work for you:\nmicrodnf install -y vim",
    "Cannot start apache automatically with docker": "Docker services have to be running in the foreground. In your Dockerfile, RUN service apache2 restart will start apache as background process. Hence the container will exit.\nTo run apache in the foreground, add the following to the Dockerfile.\nCMD [\"/usr/sbin/apachectl\", \"-D\", \"FOREGROUND\"]\nFROM ubuntu:latest\nRUN apt-get update && apt-get install -y apache2 php libapache2-mod-php php-mcrypt php-mysql php-cli php-curl php-xml php-intl php-mbstring git vim composer curl\n\nCOPY . /var/www/example\nCOPY vhost.conf /etc/apache2/sites-available/example.conf\n\nRUN a2ensite example\nRUN chown -R www-data:www-data /var/www/example/logs\nCMD [\"/usr/sbin/apachectl\", \"-D\", \"FOREGROUND\"]",
    "running netcat inside docker container": "That's never going to work. There are several problems with your Dockerfile.\n1\nSetting ENTRYPOINT to /bin/bash means that docker run ... is simply going to start bash. Read this question about ENTRYPOINT and CMD.\n2\nSince you're in non-interactive mode, bash is going to exit immediately. Consider:\nhost$ docker run nc-ubuntu\nhost$\nVs:\nhost$ docker run -it nc-ubuntu\nroot@e3e1a1f4e453:/# \nThe latter, because of the -it (which allocates a tty device, which bash requires in interactive mode), gets a bash prompt.\n3\nNeither invocation will cause the container to run netcat...and even if it did, nothing in your Dockerfile would generate the hello daemon response you're expecting.\n4\nThe nc command line is incorrect. The syntax is:\nnc -l -p <port>\nSo you would need:\nCMD [\"nc\", \"-l\", \"-p\", \"1234\"]\n5\nIf you actually want nc to provide you with the hello daemon response, you would need to add an appropriate -c command to your nc command line, as in:\nCMD [\"nc\", \"-l\", \"-p\", \"1234\", \"-c\", \"echo hello daemon\"]\nThis makes the final Dockerfile look like:\nFROM ubuntu\nRUN apt-get update \\\n  && DEBIAN_FRONTEND=noninteractive apt-get install -y \\\n    net-tools \\\n  && apt-get clean \\\n  && rm -rf /var/lib/apt/lists/*\nRUN apt-get update \\\n  && DEBIAN_FRONTEND=noninteractive apt-get install -y \\\n    netcat \\\n  && apt-get clean \\\n  && rm -rf /var/lib/apt/lists/*\nEXPOSE 1234\nCMD [\"nc\", \"-l\", \"-p\", \"1234\", \"-c\", \"echo hello daemon\"]\nAnd if I build that:\ndocker build -t nc-ubuntu .\nAnd run that:\ndocker run -d  -i -p 1234:1234 --name daemon  nc-ubuntu\nI can then telnet to port 1234 on my host and see the expected response:\nhost$ telnet localhost 1234\nTrying ::1...\nConnected to localhost.\nEscape character is '^]'.\nhello daemon\nConnection closed by foreign host.\nAt this point, the container will have exited because nc exits after accepting a single connection (without additional parameters), and a Docker contain exits when the foreground process exits.\nI don't have access to the book so I can't tell if this is do to a problem with the book or if you have made a mistake in your implementation, but I would suggest that there are a number of online Docker tutorials that are probably at least as good.",
    "Can not install package within docker debian:jessie": "I don't what's causing the 'unable to locate package' error, but your apt-get invocations are missing -y, which means you're going to get:\nAfter this operation, 33.8 MB of additional disk space will be used.\nDo you want to continue? [Y/n] Abort.\nThe command '/bin/sh -c apt-get install git-core' returned a non-zero code: 1\nOtherwise, your Dockerfile worked just fine for me:\nStep 1 : RUN apt-get -qq update\n ---> Running in 0430a990fa81\n ---> 54f88a02d81e\nRemoving intermediate container 0430a990fa81\nStep 2 : RUN apt-get install git-core\n ---> Running in 0fdad2e3c35b\nReading package lists...\nBuilding dependency tree...\nReading state information...\nThe following extra packages will be installed:\n  ca-certificates git git-man less libcurl3-gnutls liberror-perl libidn11\n  librtmp1 libssh2-1 libx11-6 libx11-data libxau6 libxcb1 libxdmcp6 libxext6\n  libxmuu1 openssh-client patch rsync xauth\nSuggested packages:\n  gettext-base git-daemon-run git-daemon-sysvinit git-doc git-el git-email\n  git-gui gitk gitweb git-arch git-cvs git-mediawiki git-svn ssh-askpass\n  libpam-ssh keychain monkeysphere ed diffutils-doc openssh-server\nRecommended packages:\n  ssh-client\nThe following NEW packages will be installed:\n  ca-certificates git git-core git-man less libcurl3-gnutls liberror-perl\n  libidn11 librtmp1 libssh2-1 libx11-6 libx11-data libxau6 libxcb1 libxdmcp6\n  libxext6 libxmuu1 openssh-client patch rsync xauth\n0 upgraded, 21 newly installed, 0 to remove and 1 not upgraded.\nNeed to get 8,059 kB of archives.\nAfter this operation, 33.8 MB of additional disk space will be used.\nDo you want to continue? [Y/n] Abort.\nThe command '/bin/sh -c apt-get install git-core' returned a non-zero code: 1\nERROR: failed to build larsks/sodocker:latest",
    "Facing the following error while trying to create a Docker Container using a DockerFile -> \"error from sender: open .Trash: operation not permitted\"": "The Dockerfile should be inside a folder. Navigate to that folder and then run docker build command. I was also facing the same issue but got resovled when moved the docker file inside a folder",
    "apt-get not found in Docker": "as tkausl said: you can only use one base image (one FROM).\nalpine's package manager is apk not apt-get. you have to use apk to install packages. however, pip is already available.\nthat Dockerfile should work:\nFROM python:3.6-alpine\n\nRUN apk update && \\\n    apk add --virtual build-deps gcc python-dev musl-dev\n\nWORKDIR /app\nADD . /app\nRUN pip install -r requirements.txt\nEXPOSE 5000\nCMD [\"python\", \"main.py\"]",
    "Invalid framework identifier Dotnet restore, docker build": "The problem was that i needed to copy my Directory.Build.props file and now is working. Just added this line to my dockerfile\nCOPY Directory.Build.props ./",
    "Non-root user in Docker": "Add RUN useradd -s /bin/bash user before the USER directive.",
    "Install nodejs in python slim buster": "This should work:\nFROM python:3.7-slim-buster\n\n# setup dependencies\nRUN apt-get update\nRUN apt-get install xz-utils\nRUN apt-get -y install curl\n\n# Download latest nodejs binary\nRUN curl https://nodejs.org/dist/v14.15.4/node-v14.15.4-linux-x64.tar.xz -O\n\n# Extract & install\nRUN tar -xf node-v14.15.4-linux-x64.tar.xz\nRUN ln -s /node-v14.15.4-linux-x64/bin/node /usr/local/bin/node\nRUN ln -s /node-v14.15.4-linux-x64/bin/npm /usr/local/bin/npm\nRUN ln -s /node-v14.15.4-linux-x64/bin/npx /usr/local/bin/npx\nTo run node start it with docker run -it <containerName> /bin/bash Then node, npm and npx are available",
    "Docker-compose volume doesn't save state of container for Keycloak": "Using default database location you may try this option with docker-compose:\nkeycloak:\n    image: quay.io/keycloak/keycloak:14.0.0\n    container_name: keycloak\n    environment:\n      KEYCLOAK_USER: admin\n      KEYCLOAK_PASSWORD: admin\n    ports:\n      - \"8082:8080\"\n    restart: always\n    volumes:\n      - .local/keycloak/:/opt/jboss/keycloak/standalone/data/\nFound similar answer with plain docker https://stackoverflow.com/a/60554189/6916890\ndocker run --volume /root/keycloak/data/:/opt/jboss/keycloak/standalone/data/",
    "Visual studio docker tools publish port when debugging": "Port mapping must be specified during docker run command. EXPOSE command within dockerfile normally used for documentation purposes.\nSolution: in your Visual Studio project file add the following:\n  <PropertyGroup>\n    <DockerfileRunArguments>-p 5000:5000</DockerfileRunArguments>\n  </PropertyGroup>\nReferences:\nhttps://learn.microsoft.com/en-us/visualstudio/containers/container-msbuild-properties?view=vs-2019 https://docs.docker.com/engine/reference/builder/#expose",
    "Angular Development Docker Container": "As I suspected, it was an Angular config issue. All I needed to do was change my package.json file.\nFrom:\n\"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"ng serve\"\nTo:\n\"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"ng serve --host 0.0.0.0 --disable-host-check\"\nMy Dockerfile stayed the same.",
    "Go's time doesn't work under the docker image from scratch": "I think the proper way to solve this is to import tzdata as the library wants it to be. You can make use of the TZ environment variable.\nThe solution is indicated in a Dockerfile which I found on GitHub here\nTo wrap up everything:\nFROM golang:alpine AS build\nRUN apk update && apk add ca-certificates && apk add tzdata\nWORKDIR /app\nADD . .\nRUN CGO_ENABLED=0 GOOS=linux go build -o myapp\n\nFROM scratch AS final\nCOPY --from=build /usr/share/zoneinfo /usr/share/zoneinfo\nCOPY --from=build /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/\nCOPY --from=build /app/myapp /\n\nENV TZ Australia/Sydney\nENTRYPOINT [\"/myapp\"]",
    "Installing curl inside nginx docker image": "For Nginx image use Debian as OS use below : -\nFROM nginx\nRUN apt-get update && apt-get install -y curl && apt-get clean",
    "How to set Docker ENTRYPOINT To NPM?": "The issue seems to be due to the single quotes in the ENTRYPOINT. Could you check if using the double quotes the issue solves?\nENTRYPOINT [\"/usr/local/bin/npm\", \"run\"]\nAccording to the Docker guide, there are only two forms to write the ENTRYPOINT:\nexec form (preferred): ENTRYPOINT [\"executable\", \"param1\", \"param2\"]\nshell form: ENTRYPOINT command param1 param2",
    "how to resolve warning ''using a subshell to avoid having to cd back'": "You can invoke the commands in a subshell (...):\n(\n   cd ./folder_02\n   docker build . -t image_name\n)\n(\n   cd ./folder_03\n   something else\n)\nIn scripts that I want to preserve environment I use pushd+popd.\npushd ./folder_02 >/dev/null\ndocker build . -t image_name\npopd >/dev/null\n\npushd ./folder_03 >/dev/null\nsomething soemthing\npopd >/dev/null",
    "apache not starting in alpine image docker": "Try this :\nFROM alpine:latest\nRUN \\\n    apk add --no-cache \\\n    apache2-proxy \\\n    apache2-ssl \\\n    apache2-utils \\\n    curl \\\n    git \\\n    logrotate \\\n    openssl\n\nENV APACHE_RUN_USER www-data\nENV APACHE_RUN_GROUP www-data\nENV APACHE_LOG_DIR /var/log/apache2\n\nWORKDIR /var/www/localhost/htdocs\nCOPY  index.html  /var/www/localhost/htdocs \n\nEXPOSE 80\n\nCMD [\"/usr/sbin/httpd\", \"-D\", \"FOREGROUND\"]",
    "Docker getting exited just after start": "Edit: In my original post I mention: \"try to think like with VMs\". I recently fell into this, which says not to do so:\nStop thinking about a container as a mini-VM and instead start thinking about it as just a process.\nalso, worth-reading article: Containers are not VMs\nOriginal post:\nThe logic with Docker containers is that they are supposed to have a service up and running. If this service stops, they exit and go to \"stopped\" state. (As you learn more about Docker, you will understand how this works and you will be able to use ENTRYPOINT and CMD). But let's skip this for a while and try to think like with VMs, run a new container and get inside to type some commands...\nthis succeeds:\ndocker container create -it --name test ubuntu\n445cad0a3afea97494635361316e5869ad3b9ededdd6db46d2c86b4c1461fb75\n$ docker container start test\ntest\n$ docker container exec -it test bash\nroot@445cad0a3afe:/# your are inside, you can type your commands here!\nwhy yours failed...\nwhen you created the container, you didn't use the -i flag which helps Keep STDIN open even if not attached. This practically means that when the container starts, it uses the CMD set in the official ubuntu Dockerfile, which is bash and then exits immediately.\ndocker attach VS docker exec --it bash\nYou can test this with an image like nginx. If you run a new nginx container and try to attach to it, you will see that logs from nginx are being printed out and you are not able to type any command in the shell. This happens because the CMD of the image is the following:\n# Define default command.\nCMD [\"nginx\"]\nTo be able to \"attach\" to a container like that but also be able to use the shell (some others may also mention this like doing something equivalent to ssh to the container), you will have to run:\ndocker container exec -it your_container_name bash\nI suggest you also read:\nIs it possible to start a shell session in a running container (without ssh)\nDocker - Enter Running Container with new TTY\nHow do you attach and detach from Docker's process?\nWhy docker container exits immediately\n~jpetazzo: If you run SSHD in your Docker containers, you're doing it wrong!",
    "Dockerfile fails to build": "If your host is an Ubuntu VM, it could be an invalid /etc/resolve.conf. Look at the /etc/resolv.conf on the host Ubuntu VM. If it contains nameserver 127.0.1.1, that is wrong.\nRun these commands on the host Ubuntu VM to fix it:\nsudo vi /etc/NetworkManager/NetworkManager.conf\n# Comment out the line `dns=dnsmasq` with a `#`\n\n# restart the network manager service\nsudo systemctl restart network-manager\n\ncat /etc/resolv.conf\nNow /etc/resolv.conf should have a valid value for nameserver, which will be copied by the docker containers.",
    "OpenShift 3.1 - Prevent Docker from caching curl resource": "According to the OpenShift docs (https://docs.openshift.com/enterprise/3.1/dev_guide/builds.html#no-cache) you can force builds to not be cached using the following syntax:\nstrategy:\n  type: \"Docker\"\n  dockerStrategy:\n    noCache: true\nThis will mean that no steps are cached, which will make your builds slower but will mean you have the correct artifact version in your build.",
    "Dockerized Vue app - hot reload does not work": "After many days I managed to add hot reload by adding in the webpack configuration file this config:\ndevServer: {\n      public: '0.0.0.0:8080'\n    }\nAfter digging to the official vue js repo, specifically to serve.js file found the public option which:\nspecify the public network URL for the HMR client\nIf you do not want to edit your webpack config, you can do this directly from docker-compose file in the command:\ncommand: npm run serve -- --public 0.0.0.0:8080",
    "what \"--from=builder\" do in Dockerfile": "builder is the name of a previous stage in the multi-stage Dockerfile. It is probably building the application manager. Once the builder is complete, this current stage copies the manager file into the current image /workspace directory. The entrypoint simply gives the program that will run when the container starts.\nYou can find more detailed explanations here:\nhttps://docs.docker.com/engine/reference/builder/",
    "How do I run Prisma migrations in a Dockerized GraphQL + Postgres setup?": "Late answer to my own question: Like @Athir suggested I now separated the two processes and created two docker-compose.yml files: one named docker-compose.migrate.yml that's responsible for running the migration and one named docker-compose.yml which is the main application.\nMy docker-compose.migrate.yml:\nversion: '3'\nservices:\n  prisma-migrate:\n    container_name: prisma-migrate\n    build: ./api/prisma\n    env_file:\n      - .env\n    environment:\n      DB_HOST: <secret>\n    depends_on:\n      - db\n\n  db:\n    image: postgres:13\n    container_name: db\n    restart: always\n    env_file:\n      - .env\n    environment:\n      DB_PORT: 5432\n    ports:\n      - ${DB_PORT}:5432\n    volumes:\n      - ${POSTGRES_VOLUME_DIR}:/var/lib/postgresql/data\nWith the following Prisma Dockerfile:\nFROM node:14\n\nRUN echo $DATABASE_URL\n\nWORKDIR /app\n\nCOPY ./package.json ./\nCOPY . ./prisma/\n\nRUN chmod +x ./prisma/wait-for-postgres.sh\n\nRUN npm install\nRUN npx prisma generate\n\nRUN apt update\nRUN apt --assume-yes install postgresql-client\n\n# Git will replace the LF line-endings with CRLF, causing issues while executing the wait-for-postgres shell script\n# Install dos2unix and replace CRLF (\\r\\n) newlines with LF (\\n)\nRUN apt --assume-yes install dos2unix\nRUN dos2unix ./prisma/wait-for-postgres.sh\n\nCMD sh ./prisma/wait-for-postgres.sh ${DB_HOST} ${POSTGRES_USER} npx prisma migrate deploy && npx prisma db seed --preview-feature\nEDIT: wait-for-postgres.sh:\n#!/bin/sh\n# wait-for-postgres.sh\n\nset -e\n  \nhost=\"$1\"\nuser=\"$2\"\nshift\nshift\ncmd=\"$@\"\n  \nuntil PGPASSWORD=$POSTGRES_PASSWORD psql -h \"$host\" -U \"$user\" -c '\\q'; do\n  >&2 echo \"Postgres is unavailable - sleeping\"\n  sleep 1\ndone\n  \n>&2 echo \"Postgres is up - executing command\"\n\nexec $cmd\nEDIT: example .env file:\n# ---- DB ----\nDB_HOST=localhost\nDB_PORT=5432\nDB_SCHEMA=example\n\nPOSTGRES_DB=example\nPOSTGRES_USER=example\nPOSTGRES_VOLUME_DIR=/path/where/you/want/to/store\nPOSTGRES_PASSWORD=example\n\nDATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${DB_HOST}:${DB_PORT}/${POSTGRES_DB}?schema=${DB_SCHEMA}",
    "'cannot normalize nothing' error on Docker Hub multi-stage build with shared ARG [duplicate]": "It turns out that I need to renew ARG on each step I want to reuse it. Quoting docs:\nAn ARG instruction goes out of scope at the end of the build stage where it was defined. To use an arg in multiple stages, each stage must include the ARG instruction.\nThe fixed Dockerfile is:\nARG BUILD_DIR=/build\n\nFROM alpine:latest as build\nARG BUILD_DIR # <----------------------------------\nRUN apk add --no-cache gcc musl-dev\nWORKDIR $BUILD_DIR\nRUN echo -e '\\n\\\n#include <stdio.h>\\n\\\nint main (void)\\n\\\n{ puts (\"Hello, World!\");}'\\\n>> hello.c\nRUN cc -o hello hello.c\n\nFROM alpine:latest\nARG BUILD_DIR # <----------------------------------\nCOPY --from=build $BUILD_DIR/hello /app\nCMD [\"/app\"]\nIt means that an ARG value from the code in my initial question does not even work. Interesting, that my Docker For Mac ignores the problem and silently uses empty BUILD_DIR argument for WORKDIR producing successful build. While Docker Hub automated builder fails with an error explicitly.",
    "Passing arguments to a docker container when calling docker-compose": "There are two \"halves\" to the command line Docker ultimately runs, the entrypoint and the command. There are corresponding Docker Compose entrypoint: and command: directives to set them. Their interaction is fairly straightforward: they're just concatenated together to produce a single command line.\nGiven what you show, in your Dockerfile you could write\nENTRYPOINT [\"dotnet\", \"myproj.dll\"]\nand then in your docker-compose.yml\ncommand: myArg1=hallo myArg2=world\nand that would ultimately launch that process with those arguments.\nIf you want to be able to specify this from the command line when you spin up the Compose stack, Compose supports ${VARIABLE} references everywhere so you could write something like\ncommand: myArg1=${MY_ARG1:-hallo} world\nand then launch this with a command like\nMY_ARG1=greetings docker-compose up\nA very typical pattern in Docker is to use ENTRYPOINT for a wrapper script that does some first-time setup, and then runs the command as the main container process with exec \"$@\". The entrypoint gets the command-line arguments as parameters and can do whatever it likes with them, including reformatting them or ignoring them entirely, and you could also use this to implement @Kevin's answer of configuring this primarily with environment variables, but translating that back to var=value type options.",
    "Problems running conda update in a dockerfile": "You have to add anaconda to the PATH during build time with the ENV variable before executing anaconda inside the Dockerfile.\nRUN bash Anaconda3-2018.12-Linux-x86_64.sh -b && \\\n    echo \"export PATH=\"/root/anaconda3/bin:$PATH\"\" >> ~/.bashrc && \\\n    /bin/bash -c \"source ~/.bashrc\"\nENV PATH /root/anaconda3/bin:$PATH\nRUN conda update --all\nUpdating PATH in .bashrc will make it possible to call conda inside the container when run with docker run, but not in other RUN statements in the docker file.",
    "Docker-compose copy config": "You need to add a volumes reference in your docker-compose.yml.\nversion: 2\nservices:\n  myService:\n    image: serviceA from registry\n     # Would like to modify a config file's content...\n    volumes:\n      - /path/to/host/config.conf:/path/to/container/config/to/overwrite/config.conf\n\n  mainAPI:\n     ...\nSince 3.3, you can also create your own docker config file to overwrite the existing one. This is similar to above, except the docker config is now a copy of the original instead of binding to the host config file.\nversion: 3.3\nservices:\n  myService:\n    image: serviceA from registry\n     # Would like to modify a config file's content...\n    configs:\n      - source: myconfigfile\n        target: /path/to/container/config/to/overwrite/config.conf\n\n  mainAPI:\n     ...\n\nconfigs:\n  myconfigfile:\n    file: ./hostconfigversion.conf\nSee https://docs.docker.com/compose/compose-file/#long-syntax for more info.",
    "What Docker scratch contains by default?": "The scratch image contains nothing. No files. But actually, that can work to your advantage. It turns out, Go binaries built with CGO_ENABLED=0 require absolutely nothing, other than what they use. There are a couple things to keep in mind:\nWith CGO_ENABLED=0, you can't use any C code. Actually not too hard.\nWith CGO_ENABLED=0, your app will not use the system DNS resolver. I don't think it does by default anyways because it's blocking and Go's native DNS resolver is non-blocking.\nYour app may depend on some things that are not present:\nApps that make HTTPS calls (as in, to other services, i.e. Amazon S3, or the Stripe API) will need ca-certs in order to confirm HTTPS certificate authenticity. This also has to be updated over time. This is not needed for serving HTTPS content.\nApps that need timezone awareness will need the timezone info files.\nA nice alternative to FROM scratch is FROM alpine, which will include a base Alpine image - which is very tiny (5 MiB I believe) and includes musl libc, which is compatible with Go and will allow you to link to C libraries as well as compile without setting CGO_ENABLED=0. You can also leverage the fact that alpine is regularly updated, using its tzinfo and ca-certs.\n(It's worth noting that the overhead of Docker layers is amortized a bit because of Docker's deduplication, though of course that is negated by how often your base image is updated. Still, it helps sell the idea of using the quite small Alpine image.)\nYou may not need tzinfo or ca-certs now, but it's better to be safe than sorry; you can accidentally add a dependency without realizing it breaks your build. So I recommend using alpine as your base. alpine:latest should be fine.\nBonus: If you want the advantages of reproducible builds inside Docker, but with small image sizes, you can use the new Docker multi-stage builds available in Docker 17.06+.\nIt works a bit like this:\nFROM golang:alpine\nADD . /go/src/github.com/some/gorepo  # may need some go getting if you don't vendor\nRUN go build -o /app github.com/some/gorepo\n\nFROM scratch  # or alpine\nCOPY --from=0 /app /app\nENTRYPOINT [\"/app\"]\n(I apologize if I've made any mistakes, I'm typing that from memory.)\nNote that when using FROM scratch you must use the exec form of ENTRYPOINT, because the shell form won't work (it depends on the Docker image having /bin/sh, which it won't.) This will work fine in Alpine.",
    "Answer '29' to apt-get install prompt for xorg": "There are quite a few issues I see with the Dockerfile provided.\nDefining a volume inside the Dockerfile provides little to no value. It's much better to define this in your docker-compose.yml or as part of your run command. I've got a blog post going through the issues with this.\nSplitting up the apt-get update from the later apt-get install commands can result in situations where the apt-get install will fail. There's a section in the best practices about this.\nFor your error message, I'd run apt-get in the noninteractive mode. You can also preconfigure debconf if you need a non-default value set during install.\nSplitting each apt-get into a separate RUN command is creating lots of excessive layers, these should be merged where possible. This is also described in the best practices documentation.\nHere's a sample of an install command that works for me taking the above into account:\nFROM ubuntu:16.04\n\nRUN apt-get update \\\n && DEBIAN_FRONTEND=noninteractive apt-get install -y \\\n      build-essential \\\n      xorg \\\n      libssl-dev \\\n      libxrender-dev \\\n      wget \\\n      gdebi \\\n      libxrender1 \\\n      xfonts-utils \\\n      xfonts-base \\\n      xfonts-75dpi",
    "Docker echo to /etc/hosts not working": "Docker manages /etc/hosts. It does this to make container linking work. You can ask docker to append to the hosts file when starting the container with\ndocker run -it --add-host foo:192.168.99.100",
    "How to use Laravel docker container & MySQL DB with a Vue one?": "According to this article you will need to follow the steps below.\nMake your project folder look like this: (d: directory, f: file)\nd: backend\nd: frontend\nd: etc\n    d: nginx\n        d: conf.d\n            f: default.conf.nginx\n    d: php\n        f: .gitignore\n    d: dockerize\n        d: backend\n            f: Dockerfile\nf: docker-compose.yml\nAdd docker-compose.yml\n    version: '3'\n    services:\n    www:\n        image: nginx:alpine\n        volumes:\n            - ./etc/nginx/conf.d/default.conf.nginx:/etc/nginx/conf.d/default.conf\n        ports:\n            - 81:80\n        depends_on:\n            - backend\n            - frontend\n\n    frontend:\n        image: node:current-alpine\n        user: ${UID}:${UID}\n        working_dir: /home/node/app\n        volumes:\n            - ./frontend:/home/node/app\n        environment:\n            NODE_ENV: development\n        command: \"npm run serve\"\n\n    backend:\n        build:\n            context: dockerize/backend\n        # this way container interacts with host on behalf of current user.\n        # !!! NOTE: $UID is a _shell_ variable, not an environment variable!\n        # To make it available as a shell var, make sure you have this in your ~/.bashrc (./.zshrc etc):\n        # export UID=\"$UID\"\n        user: ${UID}:${UID}\n\n        volumes:\n            - ./backend:/app\n            # custom adjustments to php.ini\n            # i. e. \"xdebug.remote_host\" to debug the dockerized app\n            - ./etc/php:/usr/local/etc/php/local.conf.d/\n        environment:\n            # add our custom config files for the php to scan\n            PHP_INI_SCAN_DIR: \"/usr/local/etc/php/conf.d/:/usr/local/etc/php/local.conf.d/\"\n        command: \"php artisan serve --host=0.0.0.0 --port=8080\"\n\n    mysql:\n        image: mysql:5.7.22\n        container_name: mysql\n        restart: unless-stopped\n        tty: true\n        ports:\n            - \"4306:3306\"\n        volumes:\n            - ./etc/mysql:/var/lib/mysql\n        environment:\n            MYSQL_DATABASE: tor\n            MYSQL_USER: root\n            MYSQL_PASSWORD: root\n            MYSQL_ROOT_PASSWORD: root\n            SERVICE_TAGS: dev\n            SERVICE_NAME: mysql\nAdd default.conf.nginx\n    server {\n        listen 81;\n        server_name frontend;\n        error_log  /var/log/nginx/error.log debug;\n\n        location / {\n            proxy_pass http://frontend:8080;\n        }\n\n        location /sockjs-node {\n            proxy_pass http://frontend:8080;\n            proxy_set_header Host $host;\n            # below lines make ws://localhost/sockjs-node/... URLs work, enabling hot-reload\n            proxy_http_version 1.1;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"Upgrade\";\n        }\n\n        location /api/ {\n            # on the backend side, the request URI will _NOT_ contain the /api prefix,\n            # which is what we want for a pure-api project\n            proxy_pass http://backend:8080/;\n            proxy_set_header Host localhost;\n        }\n    }\nAdd Dockerfile\nFROM php:fpm-alpine\n\nRUN apk add --no-cache $PHPIZE_DEPS oniguruma-dev libzip-dev curl-dev \\\n    && docker-php-ext-install pdo_mysql mbstring zip curl \\\n    && pecl install xdebug redis \\\n    && docker-php-ext-enable xdebug redis\nRUN mkdir /app\nVOLUME /app\nWORKDIR /app\n\nEXPOSE 8080\nCMD php artisan serve --host=0.0.0.0 --port=8080\nDON'T FORGET TO ADD vue.config.js to your frontend folder\n// vue.config.js\nmodule.exports = {\n    // options...\n    devServer: {\n        disableHostCheck: true,\n        host: 'localhost',\n        headers: {\n            'Access-Control-Allow-Origin': '*',\n            'Access-Control-Allow-Headers': 'Origin, X-Requested-With, Content-Type, Accept'\n        },\n        watchOptions: {\n            poll: true\n        },\n        proxy: 'http://localhost/api',\n    } \n}\nRun sudo docker-compose up\nIf you want to do migrations run this: sudo docker-compose exec backend php artisan migrate",
    "How to remove <none> images after building successfull": "docker rmi `docker images | grep \"<none>\" | awk {'print $3'}`",
    "Start redis container with backup dump.rdb": "You can use docker-compose.yml like :\nversion: '3'\nservices:\n  redis:\n    image: redis:alpine\n    container_name: \"redis\"\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - /data/redis:/data\n\n  server:\n    build: ./src\n    image: hubName:imageName\n    container_name: containerName\n    links: \n      - redis\n    depends_on:\n      - \"redis\"\n    ports:\n      - \"8443:8443\"\n    restart: always\nLet's copy your dump.rdb to /data/redis folder on your host machine then start docker-compose.\nAbout redis persistance,you must have docker volume and have two types for redis persinstance: RDB and AOF\nRDB: The RDB persistence performs point-in-time snapshots of your dataset at specified intervals ( example: 60 seconds or if there're at least 10000 keys have been changed)\nAOF: logs every write operation received by the server(eg: SET command) , that will be played again at server startup, reconstructing the original dataset\nFor more: https://redis.io/topics/persistence\nYou should decide base on your critical data level. In this case you have rdb dump so you can use RDB, it's default option",
    "How to delete files sent to Docker daemon build context": "What happens when you run docker build . command:\nDocker client looks for a file named Dockerfile at the same directory where your command runs. If that file doesn't exists, an error is thrown.\nDocker client looks a file named .dockerignore. If that file exists, Docker client uses that in next step. If not exists nothing happens.\nDocker client makes a tar package called build context. Default, it includes everything in the same directory with Dockerfile. If there are some ignore rules in .dockerignore file, Docker client excludes those files specified in the ignore rules.\nDocker client sends the build context to Docker engine which named as Docker daemon or Docker server.\nDocker engine gets the build context on the fly and starts building the image, step by step defined in the Dockerfile.\nAfter the image building is done, the build context is released.\nSo, your build context is not replicated anywhere but in the image you just created if only it needs all the build context. You can check image sizes by running this: docker images. If you see some unused or unnecessary images, use docker rmi unusedImageName.\nIf your image does'nt need everything in the build context, I suggest you to use .dockerignore rules, to reduce build context size. Exclude everything which are not necessary for the image. This way, the building process will be shorter and you will see if there is any misconfigured COPY or ADD steps in the Dockerfile.\nFor example, I use something like this:\n# .dockerignore\n* # exclude everything\n!build/libs/*.jar # include just what I need in the image\nhttps://docs.docker.com/engine/reference/builder/#dockerignore-file\nhttps://docs.docker.com/engine/docker-overview/",
    "Expand ARG value in CMD [Dockerfile]": "The problem here is that ARG params are available only during image build.\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.\nhttps://docs.docker.com/engine/reference/builder/#arg\nCMD is executed at container startup where ARG variables aren't available anymore.\nENV variables are available during build and also in the container:\nThe environment variables set using ENV will persist when a container is run from the resulting image.\nhttps://docs.docker.com/engine/reference/builder/#env\nTo solve your problem you should transfer the ARG variable to an ENV variable.\nadd the following line before your CMD:\nENV RUNTIME_ENV ${RUNTIME_ENV}\nIf you want to provide a default value you can use the following:\nENV RUNTIME_ENV ${RUNTIME_ENV:default_value}\nHere are some more details about the usage of ARG and ENV from the docker docs.",
    "How to copy files and folders to a working directory in Dockerfile?": "Solution\nIn order to copy files and folders to a working directory, you can use the following in your Dockerfile:\nWORKDIR /working/directory/path\nCOPY . .\nThis is an easy way to change to a working directory and copy everything from your host source.\nPlease Note\nThis will copy everything from the source path --more of a convenience when testing locally.",
    "How to use RUN clone git in dockerfile": "The best solution is to change docker working directory using WORKDIR. So your Dockerfile should look like this:\nFROM python:3\nRUN git clone https://github.com/user/mygit.git\nWORKDIR mygit\nCMD [ \"python3\", \"./aa.py\" ]",
    "Ubuntu dockerfile - mailutils install": "Use the following dockerfile:\nFROM ubuntu:latest\nENV DEBIAN_FRONTEND=\"noninteractive\"\nRUN apt-get update && apt-get install -y mailutils\nThe important part is setting debconf to noninteractive mode.",
    "'DEBIAN_FRONTEND=noninteractive' not working inside shell script with apt-get": "Drop sudo in your script, there is point to use it if you're running as root. This is also the reason that DEBIAN_FRONTEND has no effect - sudo drops your current user's environment for security reasons, you'd have to use with -E option to make it work.",
    "Cannot start service app: OCI runtime create failed: container_linux.go:349": "Set the permission to your executable it should work.\nRUN chmod +x ./main\n# Command to run the executable\nCMD [\"./main\"]",
    "how to run .sh file when container is running using dockerfile": "At a purely mechanical level, the quotes are causing trouble. When you say\nRUN \"sh test.sh\"\nit tries to run a single command named sh\\ test.sh; it does not try to run sh with test.sh as a parameter. Any of the following will actually run the script\nRUN [\"sh\", \"test.sh\"]\nRUN sh test.sh\nRUN chmod +x test.sh; ./test.sh\nAt an operational level you'll have a lot of trouble running that command in the server container at all. The big problem is that you need to run that command after the server is already up and running. So you can't run it in the Dockerfile at all (no services are ever running in a RUN command). A container runs a single process and you need that process to be the Elasticsearch server itself, so you can't do this directly in ENTRYPOINT or CMD either.\nThe easiest path is to run this command from the host:\ndocker build -t my/elasticsearch .\ndocker run -d --name my-elasticsearch -p 9200:9200 my/elasticsearch\ncurl http://localhost:9200  # is it alive?\n./test.sh\nIf you have a Docker Compose setup, you could also run this from a separate container, or you could run it as part of the startup of your application container. There are some good examples of running database migrations in an ENTRYPOINT script for your application container running around, and that's basically the pattern you're looking for.\n(It is theoretically possible to run this in an entrypoint script. You have to start the server, wait for it to be up, run your script, stop the server, and then finally exec \"$@\" to run the CMD. This is trickier for Elasticsearch, where you might need to connect to other servers in the same Elasticsearch cluster lest your state get out of sync. The official Docker Hub mysql does this, for a non-clustered database server; see its rather involved entrypoint script for ideas.)",
    "Can't build Docker multi-stage image using ARG in COPY instruction": "OK. I discovered it's an open issue on Docker side. https://github.com/moby/moby/issues/35018\n--> ARG/ENV substitution is NOT working for -- values in ADD and COPY.\nFor my case, there is a work around while waiting for correction in Docker. https://github.com/docker/cli/issues/996\nARG HELLO_VERSION\nFROM hello-world:${HELLO_VERSION:-latest} as hello\n\nFROM ubuntu:18.04\nCOPY --from=hello /hello /hello",
    "How to nuke everything inside my Docker containers and start a new?": "So apparently docker system prune has some additional options, and the proper way to nuke everything was docker system prune --all --volumes. The key for me was probably --volumes, as those would probably hold cached packages that had to be rebuilt.\nThe segmentation fault is gone now \\o/",
    "How to build an image of a project using docker-compose with Dockerfile?": "TL;DR\nA single Dockerfile is usually not enough to replace a whole containers orchestration made with docker-compose and is not necessarly a good choice.\nAbout converting a docker-compose.yml file to a Dockerfile :\nYou can pass some informations from your docker-compose.yml file to your Dockefile (the command to run for instance) but that wouldn't be equivalent and you can't do that with all the docker-compose.yml file content.\nYou can replace your docker-compose.yml file with commands lines though (as docker-compose is precisely to replace it).\nBUT\nKeep in mind that Dockerfiles and docker-compose serve two whole different purposes.\nDockerfile are meant for image building, to define the steps to build your images.\ndocker-compose is a tool to start and orchestrate containers to build your applications (you can add some informations like the build context path or the name for the images you'd need, but not the Dockerfile content itself).\nSo asking to \"convert a docker-compose.yml file into a Dockerfile\" isn't really relevant.\nThat's more about converting a docker-compose.yml file into one (or several) command line(s) to start containers by hand.\nThe purpose of docker-compose is precisely to get rid of these command lines to make things simpler (it automates it).\nMultiple processes in a single container :\nFrom the docker documentation :\nIt\u2019s ok to have multiple processes, but to get the most benefit out of Docker, avoid one container being responsible for multiple aspects of your overall application\nSo you can if your entrypoint permits you to launch several processes, or if you use a supervisor, but maybe that's not necessarly the best idea.\nEDIT\nSince I'm not sure it's clear for you either, here is the difference between a container and an image.\nYou really should check this out and try to understand this before working with Docker since it's a very necessary thing to know.",
    "What happens when the base image of my image gets updated?": "Your image, as downloaded by someone, will always remain the same. An image relies on specific layers to give the image it's SHA256 checksum. Modifying parent layers would modify the checksum used to reference the image, so that would become a new image. The only way for that image to change is if the image is referenced by a tag and the local tag changes, either manually or by pulling the image tag again.\ndocker build will use the a local image first by default. You either need to run docker build --pull, separately docker pull or docker rmi IMAGE for the build to use the latest tagged image.\nThe Docker Hub build service has a build feature to automatically rebuild when any specified image's are updated in the hub.",
    "Waiting for a Docker container to be ready": "This will successfully wait for Postgres to start. (Specifically line 6)\nservices:\n  practice_docker: \n    image: dockerhubusername/practice_docker\n    ports: \n      - 80:3000\n    command: bash -c 'while !</dev/tcp/db/5432; do sleep 1; done; npm start'\n    depends_on:\n      - db\n    environment:\n      - DATABASE_URL=postgres://postgres:password@db:5432/practicedocker\n      - PORT=3000   \n  db:\n    image: postgres\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=password\n      - POSTGRES_DB=practicedocker",
    "docker container can't use `service sshd restart`": "The build process only builds an image. Processes that are run at that time (using RUN) are no longer running after the build, and are not started again when a container is launched using the image.\nWhat you need to do is get sshd to start at container runtime. The simplest way to do that is using an entrypoint script.\nDockerfile:\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod +x /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\nCMD [\"whatever\", \"your\", \"command\", \"is\"]\nentrypoint.sh:\n#!/bin/sh\n\n# Start the ssh server\n/etc/init.d/ssh restart\n\n# Execute the CMD\nexec \"$@\"\nRebuild the image using the above, and when you use it to start a container, it should start sshd before running your CMD.\nYou can also change the base image you start from to something like Phusion baseimage if you prefer. It makes it easy to start some services like syslogd, sshd, that you may wish the container to have running.",
    "Is there a way to set the Docker container's mac address in docker-compose.yml file?": "You can use mac_address: xyz to configure the service.\nservices:\n  my_container:\n    mac_address: 00-50-56...\nBoth forms of MAC address (00:50:56... and 00-50-56...) should work.\nThis used to be a \"legacy\" option but appears to now be fully supported.",
    "Ollama in Docker pulls models via interactive shell but not via RUN command in the dockerfile": "Try this:\nDockerfile:\nFROM ollama/ollama\n\nCOPY ./run-ollama.sh /tmp/run-ollama.sh\n\nWORKDIR /tmp\n\nRUN chmod +x run-ollama.sh \\\n    && ./run-ollama.sh\n\nEXPOSE 11434\nrun-ollama.sh:\n#!/usr/bin/env bash\n\nollama serve &\nollama list\nollama pull nomic-embed-text\n\nollama serve &\nollama list\nollama pull qwen:0.5b-chat",
    "how to resolve failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount3554908678/Dockerfile: [duplicate]": "Ah, got it!\nYou ran the following command: C:\\Users\\User>docker build -t javaapp1 from C:\\Users\\User directory, which don't contain your Dockerfile. To get it working, first change the directory to one that contains your Dockerfile, which is C:\\Users\\User\\Desktop\\New folder 1 and then run docker build command\nHope it helped! \u270c\ufe0f",
    "Is it possible to build AOSP inside a docker container with alpine flavour?": "Android team has provided a Dockerfile, docker build command to build the image, and docker run command to run a container : https://android.googlesource.com/platform/build/+/master/tools/docker\nAs mentioned there, you can mount your local AOSP source tree to the container using -v option in docker run command.\nI have not tried to build AOSP in a Docker container. But in this question, @VasileM has mentioned that he uses the Android provided Docker instance to build AOSP.",
    "Best practice - Anonymous volume vs bind mount": "You're missing a key third option, named volumes. If you declare:\nversion: '3'\nvolumes:\n  build: {}\nservices:\n  cache:\n    image: ...\n    volumes:\n      - build:/build\nDocker Compose will create a named volume for you; you can see it with docker volume ls, for example. You can explicitly manage named volumes' lifetime, and set several additional options on them which are occasionally useful. The Docker documentation has a page describing named volumes in some detail.\nI'd suggest that named volumes are strictly superior to anonymous volumes, for being able to explicitly see when they are created and destroyed, and for being able to set additional options on them. You can also mount the same named volume into several containers. (In this sequence of questions you've been asking, I'd generally encourage you to use a named volume and mount it into several containers and replace volumes_from:.)\nNamed volumes vs. bind mounts have advantages and disadvantages in both directions. Bind mounts are easy to back up and manage, and for content like log files that you need to examine directly it's much easier; on MacOS systems they are extremely slow. Named volumes can run independently of any host-system directory layout and translate well to clustered environments like Kubernetes, but it's much harder to examine them or back them up.\nYou almost never need a VOLUME directive. You can mount a volume or host directory into a container regardless of whether it's declared as a volume. Its technical effect is to mount a new anonymous volume at that location if nothing else is mounted there; its practical effect is that it prevents future Dockerfile steps from modifying that directory. If you have a VOLUME line you can almost always delete it without affecting anything.",
    "How can I run a cron in MariaDB container?": "Elaborating on @k0pernikus's comment, I would recommend to use a separate container that runs cron. The cronjobs in that container can then work with your mysql database.\nHere's how I would approach it:\n1. Create a Cron Docker Container\nYou can set up a cron container fairly simply. Here's an example Dockerfile that should do the job:\nFROM alpine\nCOPY ./crontab /etc/crontab\nRUN crontab /etc/crontab\nRUN touch /var/log/cron.log\nCMD crond -f\nJust put your crontab into a crontab file next to that Dockerfile and you should have a working cron container.\nAn example crontab file:\n* * * * * mysql -h mysql --execute \"INSERT INTO database.table VALUES 'v';\"\n2. Add the cron container to your docker-compose.yml as a service\nMake sure you add your cron container to the docker-compose.yml, and put it in the same network as your mysql service:\nnetworks:\n    my_network:\nservices:\n    mysql:\n        image: mariadb\n        networks:\n          - my_network\n    cron:\n        image: my_cron\n        depends_on: \n          - mysql\n        build:\n            context: ./path/to/my/cron-docker-folder\n        networks:\n          - my_network",
    "Mysql installation for alpine linux in docker": "First of all - why do you want to install mysql-server on that specific image? I would suggest using 2 separate containers - one for the mysql database and one for the application you are developing using that openjdk image.\nYou can simply use this gist https://gist.github.com/karlisabele/d0bebe3d27fc44a57d1db9a9abdff45a to create a setup where your Java application can connect to mysql database using database (the service name) as hostname for mysql server.\nSee https://hub.docker.com/_/mysql for additional configurations on mysql image (you should define MYSQL user, password, db name etc...)\nUPDATE\nI updated the gist and added the environment variables necessary for the mysql to work... Replace the qwerty values with your own and you should be able to access the database from your Java application via database:3306 using the username and password provided in the environment variables\nThe volumes definition at the end of the file tells docker daemon that we want to create a persistent volume somewhere in the host file system and use mysql as an alias. So that when we define the database service volumes we can simply use the mysql:/var/lib/mysql and it will actually mount an obscure folder that docker created on your filesystem. This will allow the mysql database to have persistent state everytime you start the service (because it will mount the data from your host machine) and you don't have to worry about the actual location of the volume\nYou can see all volumes by running docker volumes ls",
    "Use console output in Dockerfile": "Dockerfiles don't support shell syntax in general, except for some very limited environment variable expansion.\nThey do support ARGs that can be passed in from the command line, and an ARG can be used to define the image FROM. So you could start your Dockerfile with\nARG tag\nFROM company:${tag:-latest}\nand then build the image with\ndocker build --build-arg tag=$(cd $PWD/../;  echo ${PWD##*/}) .\n(which is involved enough that you might want to write it into a shell script).\nAt a very low level, it's also worth remembering that docker build works by making a tar file of the current directory, sending it across an HTTP connection to the Docker daemon, and running the build there. Once that process has happened, any notion of the host directory name is lost. In other words, even if the syntax worked, docker build also doesn't have the capability to know which host directory the Dockerfile is in.",
    "Dockerfile ~ Correct Syntax for Adding Windows Folders": "Apparently, the problem was that Docker does not support absolute paths as input paths.\nI was finally able to get it to work by putting the \"Bar\"-Folder in the same directory as the Dockerfile and then using the following ADD Statement in the Dockerfile:\nADD Bar C:/Bar/\nIf I am mistaken, and it is possible to use absolute paths as the source path, please correct me",
    "yum -y install not assuming yes in docker build": "It looks like you're missing the -y on the yum update.\nAlso, you should split those commands out to separate RUN commands. In this case, it doesn't make too much difference, but splitting the echos onto different lines will make it clearer.\nYou should keep the update and installs in the same command though\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/#run",
    "Install JDK 8 update 172 in dockerfile with ubuntu image": "I think oracle has fixed broken web8upd.\nSo now dockerfile specified on github works perfectly !\nJust copy-pasting same dockerfile with some modifications :\nFROM ubuntu:16.04\n\n# To solve add-apt-repository : command not found\nRUN apt-get -y install software-properties-common\n\n# Install Java\nRUN \\\n  echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | debconf-set-selections && \\\n  add-apt-repository -y ppa:webupd8team/java && \\\n  apt-get update && \\\n  apt-get install -y oracle-java8-installer --allow-unauthenticated && \\\n  rm -rf /var/lib/apt/lists/* && \\\n  rm -rf /var/cache/oracle-jdk8-installer\n\n\n# Define commonly used JAVA_HOME variable\nENV JAVA_HOME /usr/lib/jvm/java-8-oracle\nPlease note : rm -rf /var/lib/apt/lists/* will remove all lists fetched by apt-get update.\nSo if you want to install more things after installing Java, remove rm -rf /var/lib/apt/lists/* otherwise you have to run apt-get update again.",
    "Docker containers serving different subdomains on port 80": "Yes you can. using a proxy.\nThere is a project by jwilder/nginx-proxy which allows you to give your hostname via an enviroment variable which will than route your request to the appropriate container.\nA good example of this implemented is given here: https://blog.florianlopes.io/host-multiple-websites-on-single-host-docker/",
    "confused with CMD and ENTRYPOINT in dockerfile": "The mongod Dockerfile references docker-entrypoint.sh.\nThat script docker-entrypoint.sh starts by testing if the first parameter begins with '-' (as in '--auth'):\nif [ \"${1:0:1}\" = '-' ]; then\n    set -- mongod \"$@\"\nfi\nSo the arguments become mongod --auth, not just --auth.\nMeaning:\nif you have to pass any argument after mongod, you don't have to type mongod first when using docker run: it will be added for you in docker-entrypoint.sh\nif you don't have any argument to pass after mongod, you don't have to type mongod either: CMD will provide it for you to the ENTRYPOINT docker-entrypoint.sh.",
    "docker-compose: console does not return to host when creating database container": "To run the container in the background with compose, use the detach option:\ndocker-compose up -d",
    "Docker: Error response from daemon: no such id:": "In order to facilitate the usage of docker exec, make sure you run your container with a name:\ndocker run -d --name aname.cont ...\nI don't see an entrypoint or exec directove in the Dockerfile, so do mention what you want to run when using docker run -d\n(I like to add '.cont' as a naming convention, to remember that it is a container name, not an image name)\nThen a docker exec aname.cont bash should work.\nCheck that the container is still running with a docker ps -a",
    "CUDA to docker-container": "You can start with a CUDA Docker image and then install Python, for example:\nFROM nvidia/cuda:12.1.1-runtime-ubuntu20.04\n\n# Install Python\nRUN apt-get update && \\\n    apt-get install -y python3-pip python3-dev && \\\n    rm -rf /var/lib/apt/lists/*\nNote: User @chronoclast has suggested additionally installing python-is-python3 to fix the broken symlink to the default Python, in which case the Python installation step would instead be:\nRUN apt-get update && \\\n    apt-get install -y python3-pip python3-dev python-is-python3 && \\\n    rm -rf /var/lib/apt/lists/*",
    "How to install curl from Dockerfile": "I just tested the RUN apt-get update\n&& apt-get install -y curl line and it works well.\nLook at how i did it,\nDefine a stage with .NET core image as base, which will end up being used as final image.\nDefine an other stage based on dotnet SDK image as build in order to build and publish the application, then reuse the base image as the final image.\nHere's my dockerfile which works fine for building and packaging dotnet apps while installing packages like curl and jq.\n#See https://aka.ms/containerfastmode to understand how Visual Studio uses this Dockerfile to build your images for faster debugging.\n\nFROM dotnet/aspnet:5.0 AS base\n\nWORKDIR /app\n\nRUN apt-get update \\\n    && apt-get install -y curl jq \n    \nEXPOSE 80\nEXPOSE 443\n\nFROM dotnet/sdk:5.0 AS build\n\nWORKDIR /src\n\n\nCOPY [\"PROJECT.csproj\", \"project/\"]\n\nRUN dotnet restore \"project/project.csproj\" \nCOPY . .\nWORKDIR \"/src/API\"\n\nRUN dotnet build \"project.csproj\" -c Release -o /app/build\nFROM build AS publish\n\nRUN dotnet publish \"project.csproj\" -c Release -o /app/publish\n\nRUN dotnet test \"../project.Test\" \\ \n    /p:CollectCoverage=true \\ \n    /p:CoverletOutputFormat=opencover \\ \n    /p:CoverletOutput=\"./../coverage\" \\\n    /p:Include=\\\"[project.BLL]*,[project.Model]*\\\"\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"project.dll\"]\n\n\nYou can see that the final image is the one that depends on the base image with installed curl and jq\nPlease tick the answer if it helped you !",
    "Docker: WORKDIR create a layer?": "This seems to be a documentation \"bug\" in the best practices document. A better way to have phrased this is that only those commands you mention will create layers that increase the total build size.\nNote that the layers you're seeing are 0 bytes, as opposed to the FROM, ADD, COPY which each have an associated size (the amount they contribute to the total filesystem growth).\nCmp   Size  Command                                                   \n    5.6 MB  FROM 8792aa27a60beb9                                       \n    2.1 MB  ADD add.tar.gz / # buildkit                                \n    1.0 MB  COPY copy /copy # buildkit                                 \n       0 B  RUN /bin/sh -c rm /bin/arch # buildkit                     \n       0 B  WORKDIR /etc\nA layer exists for WORKDIR - but it's metadata. A layer exists for the rm command, but all it adds is a \"file deleted\" marker - not actual file contents. So they contribute nothing to the eventual filesystem size in the image.\nAs for FROM, that doesn't, in and of itself, create a layer - it imports a starting image. The reference could be clearer about that, I guess, but either way it would confuse someone. They must assume it's obvious that FROM contributes to the size (unless you are using FROM scratch).\nI wasn't able to find a definitive reference about this. A reading of the source code might be the only such reference available.",
    "How to change php-fpm default port?": "I think the problem is not the sed command itself, it's related to the wrong file you mentioned for it.\n/usr/local/etc/php-fpm.d/zz-docker.conf\nthis is the file you are trying to change the port in it but inside your docker-compose file you are mapping something else\n./docker/php-fpm/config/www.conf:/usr/local/etc/php-fpm.d/www.conf",
    "Docker multi-stage build fails if we use CMD in dockerfile": "In a multistage build, you may copy files from a previous step. Each step is considered as an individual, private image(in the scope of the multistage build).\nCMD instruction however is not invoked at build time, it only applies at runtime as clearly stated in the official docs:\nThe main purpose of a CMD is to provide defaults for an executing container.\nSince you are currently building the result image, CMD is never executed thus you get the error you have reported.\nIn the other hand, RUN instruction executes during build time making its result available for the next step. Quoting again from docs:\nThe RUN instruction will execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the Dockerfile.\nIt should be clear by now why the multistage build completes successfully when RUN is used in contrast to CMD instruction.\nYour confusion started from wrongly assuming that the below is true:\nI thought in the intermittent container, the cmd would execute which should make both the commands equivalent right?",
    "Next.js. Required files for docker image": "I know it has been over 5 months since this question was asked, but I was running in the same issue.\nI solved it by setting up a multistage build in docker and only copy the required files to run a production nextjs app.\nMy Dockerfile\n# Build the app\nFROM node:14-alpine as build\nWORKDIR /app\n\nCOPY . .\nRUN npm ci\nRUN npm run build\nCOPY ./.next ./.next\n\n\n# Run app\nFROM node:14-alpine\n\n# Only copy files required to run the app\nCOPY --from=build /app/.next ./\nCOPY --from=build /app/package.json ./\nCOPY --from=build /app/package-lock.json ./\n\nEXPOSE 3000\n\n# Required for healthcheck defined in docker-compose.yml\n# If you don't have a healthcheck that uses curl, don't install it\nRUN apk --no-cache add curl\n\n# By adding --production npm's devDependencies are not installed\nRUN npm ci --production\nRUN ./node_modules/.bin/next telemetry disable\n\nRUN addgroup -g 1001 -S nodejs\nRUN adduser -S nextjs -u 1001\n\nUSER nextjs\nCMD [\"npm\", \"start\"]\nAn excerpt from my package.json\n  \"dependencies\": {\n    \"next\": \"^9.5.5\",\n    \"next-compose-plugins\": \"^2.2.0\",\n    \"react\": \"^16.12.0\",\n    \"react-dom\": \"^16.12.0\"\n  },",
    "Permanently change the tomcat port from Dockerfile": "With lots of effort, I found the solution to change the internal port of tomcat container\nmy Dockerfile is\nFROM tomcat:7.0.107\nRUN sed -i 's/port=\"8080\"/port=\"4287\"/' ${CATALINA_HOME}/conf/server.xml\nADD ./tomcat-cas/war/ ${CATALINA_HOME}/webapps/\nCMD [\"catalina.sh\", \"run\"]\nHere ADD ./tomcat-cas/war/ ${CATALINA_HOME}/webapps/ part is not necessary unless you want to initially deploy some war files. And also I don't add EXPOSE 4287, because if I did so, the tomcat server not binding to the port 4287 then it always binding to the 8080 default port.\nJust build the image and run\ndocker build -f Dockerfile -t test/tomcat-test:1.0 .\ndocker run -d -p 4287:4287 --name tomcat-test test/tomcat-test:1.0",
    "pip search finds tensorflow, but pip install does not": "It looks like tensorflow only publishes wheels (and only up to 3.6), and Alpine linux is not manylinux1-compatible due to its use of musl instead of glibc. Because of this, pip cannot find a suitable installation candidate and fails. Your best options are probably to build from source or change your base image.",
    "Django - Mysql Database is not created in Docker": "I'm not sure what you mean by \"I logged in mysql image shell but didn't find mywebsite database\"\nYou are migrated the DB successfully, which means, the DB connections are valid and working.\n\nIn your docker-compose.yml file, the port mapping done like this, '33060:3306', which means the db's port 3306 is mapped to host machine's port 33060. So, this may be the issue (it's not an issue, kind of typo)\n\nHow to check the DB contents?\nMETHOD-1: check through django-shell of web container\n1. run docker-compose up\n2. open a new terminal in the same path and run docker ps\nyou'll get something like below\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                     NAMES\n795093357f78        django_1_11_web     \"python manage.py ru\u2026\"   34 minutes ago      Up 11 minutes       0.0.0.0:8000->8000/tcp    django_1_11_web_1\n4ae48f291e34        mysql:5.7           \"docker-entrypoint.s\u2026\"   34 minutes ago      Up 12 minutes       0.0.0.0:33060->3306/tcp   django_1_11_db_1\n3.Get into the web container by docker exec -it 795093357f78 bash command, where 795093357f78 is the respective container id\n4. now you're inside the container. Then, run the command python manage.py dbshell. Now you will be in MYSQL shell of mywebsite (Screenshot)\n5. run the command show tables;. It will display all the tables inside the mywebsite DB\n\nMETHOD-2: check through db container\n1. repeat the steps 1 and 2 in above section\n2. get into db container by docker exec -it 4ae48f291e34 bash\n3. Now you'll be in bash terminal of MYSQL. Run the following commmand mysql -u root -p and enter the password when prompt\n4. now you're in MYSQL server. run the command, show databases;. This will show all the databases in the server.",
    "docker build is failing when snowflake dependency is included in the pip requirements": "alpine linux is very tiny and they had to remove everything so you will have to install it yourself. Some pip packages require to be compiled/build on your machine but alpine is missing these depedencies, so install them, then do your pip install and then remove them cause they are not required anymore and you will reduce the image size.\nThe snowflake docs also say you have to install libffi-dev and openssl-dev\nFROM python:alpine3.6\n\nCOPY . /sample-app\nWORKDIR /sample-app\n\nRUN apk add --update --no-cache --virtual build-deps gcc python3-dev musl-dev libc-dev linux-headers libxslt-dev libxml2-dev \\\n&& apk add libffi-dev openssl-dev \\\n&& pip install --upgrade pip setuptools \\\n&& pip install -r requirements.txt \\\n&& apk del build-deps\n\nCMD [ \"python\", \"runner.py\" ]",
    "What is the best practice to get the file name that was defined in the pom.xml, from within the docker file?": "Usually you define the <finalName> in your pom file to keep the name static..\nThe default for the final name is defined like this:\n <build>\n    <finalName>${project.artifactId}-${project.version}</finalName>\n    ..\n </build>\nThis means if you release your artifact etc. you have to change the Dockerfile...The simplest solution is to change the definition in your pom ilke this:\n <build>\n    <finalName>${project.artifactId}</finalName>\n    ..\n </build>\nThan you need to change the Dockerfile only if you change your artifactId which usually does not happen very often...\nUpdate\nWhat you could do is to provide arguments to your Dockerfile like:\n#!/bin/bash\nPOM_VERSION=$(mvn -q help:evaluate -Dexpression=project.version -DforceStdout=true)\necho \"POM Version: $POM_VERSION\"\ndocker build --no-cache \\\n    --build-arg APPVERSION=$POM_VERSION \\\n    --rm=true -t user-registration .\nOne word about the line: POM_VERSION=.. Starting with maven-help-plugin version 3.1.0 it is possible to extract things from the pom file like this in particular without any grep/awk vodoo.\nThe Dockerfile can look like this:\n# FROM alpine:3.6 (plus Open JDK?)\nFROM openjdk:8u131-jre-alpine\nARG APPVERSION\nRUN echo \"Building ${APPVERSION}\"\nRUN mkdir /usr/local/service/\nCOPY target/user-registration-${APPVERSION}.jar /usr/local/service/user-registration.jar\n# 8080 application port\n# 8081 admin port.\nEXPOSE 10080 10081\nCMD [\"java\", \"-XX:MaxRAM=128m\", \"-jar\", \"/usr/local/service/user-registration.jar\"]\nThe problem here is simply that CMD does not support ENV,ARGS expanding which means you need to do the copy by using a version as above. You could use the ARG at several points but not at all locations...",
    "Docker Run Script to catch interruption signal": "Yes it is possible to achieve what you want, but before presenting the corresponding code I have to comment on your question's code which contained some issues:\nThe line trap 'echo \"Exiting with a 137 signal.\"' 137 0 9 is incorrect because 137 is not a valid signal number (see for example the Wikipedia article on signals).\nMaybe you just encountered 137 as it is the exit code corresponding to the signal 9 (given that 137 = 128 + 9, see this appendix in the bash doc.)\n0 (EXIT) and 9 (KILL) are valid signal numbers, but in practice it is better to only trap 2 (INT) and 15 (TERM), as suggested in this SE/Unix answer.\nIndeed while the INT and TERM signals can be used for \"graceful termination\", the KILL signal means the process must be killed immediately and as mentioned in man trap:\nSetting a trap for SIGKILL or SIGSTOP produces undefined results. [\u2026] Trapping SIGKILL or SIGSTOP is syntactically accepted by some historical implementations, but it has no effect. Portable POSIX applications cannot attempt to trap these signals.\nSetting the trap at the end of the entrypoint script is a bad strategy, as it is useless in this place. Instead, I suggest that you define a cleanup function (the last instruction of which being exit), then set a trap on this function at the beginning of the script, and run your (non-terminating) application afterwards.\nHence the following proof-of-concept:\nDockerfile\nFROM debian:latest\nWORKDIR /app\n\nCOPY entrypoint.bash ./\nENTRYPOINT [\"/bin/bash\", \"./entrypoint.bash\"]\nentrypoint.bash\n#!/bin/bash\n\ncleanup() {\n    echo \"Cleaning up...\"\n    exit\n}\n\ntrap cleanup INT TERM\n\nwhile :; do\n    echo \"Hello! ${SECONDS} secs elapsed...\"\n    sleep 1s\ndone\nTo test it, you just need to run:\n$ docker build -t test-trap .\n$ docker run -d --name=TEST-TRAP test-trap\n  # wait a few seconds\n$ docker stop TEST-TRAP\n$ docker logs -f TEST-TRAP\nHello! 0 secs elapsed...\nHello! 1 secs elapsed...\nHello! 2 secs elapsed...\nHello! 3 secs elapsed...\nCleaning up...",
    "Unable to connect outside database from Docker container App": "Container's default network is \"bridge\",you should choose macvlan or host network.\nmethod 1\ndocker run -d --net host image\nthis container will share your host IP address and will be able to access your database.\nmethod 2\nUse docker network create command to create a macvlan network,refrence here\nthen create your container by\ndocker run -d --net YOURNETWORK image\nThe container will have an IP address which is the same gateway with its host.",
    "Moving node.js to a docker container, receiving a cannot file module error": "During your image build, you install some things into /usr/src/app, and then you set it as the working directory. Here are the lines from Dockerfile where that happens:\nRUN mkdir -p /usr/src\nRUN mkdir -p /usr/src/app && cp -a /tmp/node_modules /usr/src/app && cp -a /tmp/package-lock.json /usr/src/app\nRUN cp -a /tmp/dist/application /usr/src/app && cp -a /tmp/dist/config /usr/src/app && cp -a /tmp/dist/domain /usr/src/app && cp -a /tmp/dist/infrastructure /usr/src/app\nRUN cp -a /tmp/dist/routes /usr/src/app && cp -a /tmp/dist/types /usr/src/app && cp -a /tmp/dist/webapp.js /usr/src/app && cp -a /tmp/package.json /usr/src/app\nRUN chmod 755 /usr/src/app/webapp.js\nRUN mkdir -p /usr/src/app/bin\n\nCOPY Account/bin /usr/src/app/bin\nCOPY Account/.env /usr/src/app/.env\n\nWORKDIR /usr/src/app\nAfter the build phase, all of that is baked into your image and ready to be used. But at runtime, you told docker-compose:\nvolumes:\n  - ./Account:/usr/src/app/\nThis is an overlay mount. Whatever is built into the image at /usr/src/app is completely ignored and replaced by the contents of ./Account from the directory where docker-compose.yml is located.\nI don't know enough about your project to tell you how to properly fix this, but that's where your error is probably coming from. All the work done during build to construct /usr/src/app is being undone by mounting another directory on top of it at runtime.\nIf you remove that volume mount, all of /usr/src/app is still there and ready to use. But that may have other side effects that you will need to account for to make your app do its job.",
    "couldn't start Celery with docker-compose": "First of all the celery image is deprecated in favour of standard python image more info here.\nWORKDIR sets the working directory for all the command after it is defined in the Dockerfile, which means the command which you are try to run will run from that directory. Docker image for celery sets the working directory to /home/user.\nSince your code is mounted on /celery_smaple and the working directory is /home/user, Celery is not able to find your python module.\nOne alternative is to cd into the mounted directory and execute the command:\ncelery:\n    image: celery:3.1.25\n    command: \"cd /celery_sample && celery worker -A my_celery -l INFO\"\n    volumes:\n      - .:/celery_sample\n    networks:\n      - webnet\nnotice the command\nAnd another one is to create your own image with WORKDIR set to /celery_sample eg:\nFROM python:3.5\nRUN pip install celery==3.1.25\nWORKDIR /celery_sample\nafter building you own image you can use the compose file by changing the image of celery service\nEdit\nYou need to link the services to one another in order to communicate:\nversion: \"3\"\nservices:\n  web:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    command: \"python my_celery.py\"\n    ports:\n      - \"8000:8000\"\n    networks:\n      - webnet\n    volumes:\n      - .:/celery_sample\n    links:\n      - redis\n\n  redis:\n    image: redis\n    networks:\n      - webnet\n\n  celery:\n    image: celery:3.1.25\n    command: \"celery worker -A my_celery -l INFO\"\n    volumes:\n      - .:/home/user\n    networks:\n      - webnet\n    links:\n      - redis\n\nnetworks:\n  webnet:\nand your configuration file should be:\n## Broker settings.\nBROKER_URL = 'redis://redis:6379/0'    \n## Using the database to store task state and results.\nCELERY_RESULT_BACKEND = 'redis://redis:6379/0'\nonce you have linked the services in compose file you can access the service by using the service name as the hostname.",
    "Docker cannot connect application to MySQL": "docker-compose will by default create virtual network were all the containers/services in the compose file can reach each other by an IP address. By using links, depends_on or network aliases they can reach each other by host name. In your case the host name is the service name, but this can be overridden. (see: docs)\nYour script in my_common_package container/service should then connect to mysql on port 3306 according to your setup. (not localhost on port 3306)\nAlso note that using expose is only necessary if the Dockerfile for the service don't have an EXPOSE statement. The standard mysql image already does this.\nIf you want to map a container port to localhost you need to use ports, but only do this if it's necessary.\nservices:\n   mysql:\n     image: mysql:5.6\n     container_name: test_mysql_container\n     environment:\n       - MYSQL_ROOT_PASSWORD=test\n       - MYSQL_DATABASE=My_Database\n       - MYSQL_USER=my_user\n       - MYSQL_PASSWORD=my_password\n     volumes:\n       - db_data:/var/lib/mysql\n     ports:\n       - \"3306:3306\"\nHere we are saying that port 3306 in the mysql container should be mapped to localhost on port 3306.\nNow you can connect to mysql using localhost:3306 outside of docker. For example you can try to run your testsql.py locally (NOT in a container).\nContainer to container communication will always happen using the host name of each container. Think of containers as virtual machines.\nYou can even find the network docker-compose created using docker network list:\n1b1a54630639        myproject_default             bridge              local\n82498fd930bb        bridge                        bridge              local\n.. then use docker network inspect <id> to look at the details.\nAssigned IP addresses to containers can be pretty random, so the only viable way for container to container communication is using hostnames.",
    "How to _directly_ COPY files to \"Program Files (x86)\" folder in a Windows container?": "You have to use double backslashes inside the braces\nFROM microsoft/windowsservercore\n\nCOPY [\"test.txt\", \"c:\\\\program files\\\\WindowsPowerShell\\\\Modules\\\\test.txt\"]\nIt is interpreted as JSON.",
    "Docker unable to start an interactive shell if the image has an entry script": "When you set and entry point in a docker container. It is the only thing it will run. It's the one and only process that matters (PID 1). Once your entry_point.sh script finishes running and returns and exit code, docker thinks the container has done what it needed to do and exits, since the only process inside it exits.\nIf you want to launch a shell inside the container, you can modify your entry point script like so:\n#!/bin/bash\n\necho \"UPDATING GIT REPO\";\n\ncd /home/tool/cloned_github_tools_root\ngit pull\ngit submodule init\ngit submodule update\n\necho \"Entrypoint ended\";\n\n/bin/bash \"$@\"\nThis starts a shell after the repo update has been done. The container will now exit when the user quits the shell.\nThe -i and -t flags will make sure the session gives you an stdin/stdout and will allocate a psuedo-tty for you, but they will not automatically run bash for you. Some containers don't even have bash in them.",
    "How will a Docker application with ubuntu as base image work on Windows?": "Now to run this hello-world application, Is docker going to install the whole ubuntu to run the application?\nNo, the ubuntu image used is not \"the whole ubuntu\". It is a trimed-down version, without the all X11 graphic layer. Still 180 MB though: see \"Docker Base Image OS Size Comparison\".\nThese days, you would rather use an Alpine image (5 MB): see \"Docker Official Images are Moving to Alpine Linux\"\nRegarding the hello-world application specifically, there is no Ubuntu or Alpine involved. Just 1.8 KB of C machine-code, which makes only direct calls to the Linux kernel of the host.\nThat Linux host is used by docker container through system calls: see \"What is meant by shared kernel in Docker?\"\nOn Windows, said Linux host was provided by VirtualBox VM running a boot2docker VM, built from a TinyCore distro.\nWith the more recent \"Docker for Windows\", that same VM is run through the Hyper-V Windows feature.",
    "Passing Tomcat parameters to Docker": "The typical method for docker containers is passing via environment variables.\nExpanding on a solution to pass the port via command line the server.xml needs to be modified so it takes in properties from JAVA_OPTS\nFor example in server.xml\n<GlobalNamingResources>\n    <Resource Name=\"jdbc/Addresses\"\n        auth=\"Container\"\n        type=\"javax.sql.Datasource\"\n        username=\"auser\"\n        password=\"Secret\"\n        driverClassName=\"com.mysql.jdbc.Driver\"\n        description=\"Global Address Database\"\n        url=\"${jdbc.url}\" />\n</GlobalNamingResources>\nThen you can pass value of ${jdbc.url} from properties on the command line.\nJAVA_OPTS=\"-Djdbc.url=jdbc:mysql:mysqlhost:3306/\"\nWhen running the docker image you use the -e flag to set this environment variable at run time\n$ docker run -it -e \"JAVA_OPTS=-Djdbc.url=jdbc:mysql:mysqlhost:3306/\" --rm myjavadockerimage /opt/tomcat/bin/deploy-and-run.sh\nOptionally also add a --add-host if you need to map mysqlhost to a specific ip address.",
    "Subversion export/checkout in Dockerfile without printing the password on screen": "The Dockerfile RUN command is always executed and cached when the docker image is build so the variables that svn needs to authenticate must be provided at build time. You can move the svn export call when the docker run is executed in order to avoid this kind of problems. In order to do that you can create a bash script and declare it as a docker entrypoint and pass environment variables for username and password. Example\n# base image\nFROM ubuntu\n\nENV REPOSITORY_URL http://subversion.myserver.com/path/to/directory\n\n# install subversion client\nRUN apt-get -y update && apt-get install -y subversion\n\n# make it executable before you add it here otherwise docker will coplain\nADD docker-entrypoint.sh /enrypoint.sh\n\nENTRYPOINT /entrypoint.sh\ndocker-entrypoint.sh\n#!/bin/bash\n\n# maybe here some validation that variables $REPO_USER $REPO_PASSOWRD exists.\n\n\nsvn export --username=\"$REMOTE_USER\" --password=\"$REMOTE_PASSWORD\" \"$REPOSITORY_URL\"\n\n# continue execution\npath/to/file.sh\nRun your image:\ndocker run -e REPO_USER=jane -e REPO_PASSWORD=secret your/image\nOr you can put the variables in a file:\n.svn-credentials\nREPO_USER=jane\nREPO_PASSWORD=secret\nThen run:\ndocker run --env-file .svn-credentials your/image\nRemove the .svn-credentials file when your done.",
    "How to copy files from shared directory in multiple Dockerfile?": "You could define a container dedicated to keep your script in a volume (as a data volume container)\nscripts:\n  volumes:\n    - /path/to/scripts\napplication1:\n  volumes_from:\n    - scripts\napplication2:\n  volumes_from:\n    - scripts\nThe /path/to/scripts folder will be shared in each application.\nThe scripts Dockerfile should create /path/to/scripts and COPY the script.sh in it.",
    "Use wget instead of curl for healthchecks in ASP.NET Core docker images": "It's possible to specify a healthcheck via the docker run CLI, or in a docker-compose.yml. I prefer to do it in the Dockerfile.\nConfigure\nFirst note that the ASP.NET Core docker images by default expose port 80, not 5000 (so the docs linked in the question are incorrect).\nThis is the typical way using curl, for a non-Alpine image:\nHEALTHCHECK --start-period=30s --interval=5m \\\n  CMD curl --fail http://localhost:80/healthz || exit 1\nBut curl is unavailable in an Alpine image. Instead of installing it, use wget:\nHEALTHCHECK --start-period=30s --interval=5m \\\n  CMD wget --spider --tries=1 --no-verbose http://localhost:80/healthz || exit 1\nHEALTHCHECK switches documented here.\nwget switches documented here. --spider prevents the download of the page (similar to an HTTP HEAD), --tries=1 allows docker to control the retry logic, --no-verbose (instead of --quiet) ensures errors are logged by docker so you'll know what went wrong.\nTest\nFor full status:\n$ docker inspect --format '{{json .State.Health }}' MY_CONTAINER_NAME | jq\nOr:\n$ docker inspect --format '{{json .State.Health }}' MY_CONTAINER_NAME | jq '.Status'\n# \"healthy\"\n\n$ docker inspect --format '{{json .State.Health }}' MY_CONTAINER_NAME | jq '.Log[].Output'\n# \"Connecting to localhost:80 (127.0.0.1:80)\\nremote file exists\\n\"",
    "executable file not found in $PATH: unknown": "Two things: Make sure the file is marked as executable. And since /mydir isn't in your path, you need to tell Docker to look for the script in the current directory by adding ./ in front of the name.\nFROM mcr.microsoft.com/powershell:ubuntu-focal\nRUN apt-get -qq -y update && \\\n    apt-get -qq -y upgrade && \\\n    apt-get -qq -y install curl ca-certificates python3-pip exiftool mkvtoolnix\nRUN pip3 install gallery-dl yt-dlp\nWORKDIR /mydir\nCOPY gallery-dl.ps1 .\nRUN chmod +x gallery-dl.ps1\nENTRYPOINT [\"./gallery-dl.ps1\"]",
    "Can\u2019t install Choclatey into windows docker container": "I managed to fix this frustrating problem today, here is what I did for when the next person has this issue, as Docker is not going to fix it any time soon.\nWhat I did, is in the desktop app on Windows / Mac you can edit the Daemon file. Under Settings in the Docker App under Docker Engine, I added the line at the bottom of the file just above the last curly brace. \"dns\": [ \"Your DNS Address Here\", \"8.8.8.8\" ]\nThis then allows the Docker Containers all that you now build to use your host's DNS server. Technically if you can access: https://chocolatey.org/install.ps1 then you should be able to access the choco repository.\nI have also built the image in https://github.com/jasric89/vsts-agent-docker/tree/master/windows/servercore/10.0.14393 and labeled it in the repo:\nmicrosoft/windowsservercore:10.0.14393.1358\nI then set: RUN choco feature enable --name allowGlobalConfirmation before my first Choco Install command, this enables choco to install all the files and not error.\nWith all that set my Docker File Ran and built the image. Well in my Test Env now testing in my prod env. :)\nLinks that helped me:\nhttps://github.com/moby/moby/issues/24928 https://github.com/jasric89/vsts-agent-docker/blob/master/windows/servercore/10.0.14393/standard/VS2017/Dockerfile https://docs.chocolatey.org/en-us/troubleshooting https://github.com/moby/moby/issues/25537\nUPDATE:\nI ran into this problem again on a fresh build and after following my own instructions it didn't quite work.\nI realised after a couple more hours of testing that Docker on Windows uses the Hyper-V Network setup.\nWithin my Hyper V Switch Manager, I did not have a network, with Internet Access. Also when I tried to change the default switch it would not let me. Therefore I had to create a new network within the Hyper-V Network.\nI then had to edit the docker Daemon file, in the Docker Settings to tell it to use the right network, and also I put in the DNS settings I specified before in my Anwser.\nHere is my full docker daemon file:\n{\n  \"registry-mirrors\": [],\n  \"insecure-registries\": [],\n  \"bridge\": \"Internet\",\n  \"dns\": [\n    \"YOUR Local DNS Address Here\",\n    \"8.8.8.8\"\n  ],\n  \"debug\": false,\n  \"experimental\": false\n}\nRun code snippetExpand snippet",
    "Reuse user in multi-stage Dockerfile": "TL;DR;\nIt is not possible to re-use the same user in multiple stages of the docker build without re-creating the user (same UID and GID at least) in each stage as each FROM is starting from a clean slate FROM image in which a user UID=42000 and GID=42000 is unlikely to already exist.\nI am not aware of any recommendation against building as the root user inside a container. It is recommended to run services as unprivileged users however certain containers processes must be run as the root user (i.e. sshd):\nThe best way to prevent privilege-escalation attacks from within a container is to configure your container\u2019s applications to run as unprivileged users. For containers whose processes must run as the root user within the container, you can re-map this user to a less-privileged user on the Docker host. The mapped user is assigned a range of UIDs which function within the namespace as normal UIDs from 0 to 65536, but have no privileges on the host machine itself.\nTip: The Haskell Dockerfile Linter will complain if the last user is root which you can configure as a git pre-commit hook to catch things like that before committing teh codez.",
    "docker-compose - Containers have no internet during build process": "my problem was solved by adding network: host to build section.\nstill no idea why my docker bridge network is not working.\nversion: '3.7'\nservices:\n  admin-panel:\n    network_mode: host\n    container_name: react-admin\n    build:\n      context: Admin-Panel\n      # the line below fixed it\n      network: host\n    volumes:\n      - ./Admin-Panel:/app\n      - /app/node_modules\n    ports:\n      - '5000:5000'\n    stdin_open: true\n    tty: true",
    "Ubuntu and Docker: Error response from daemon: error while creating mount source path": "It was just a permissions issue. I moved the source directory to /home/myuser/directory/ and worked.",
    "Dockerfile using variable built using Shell Command": "ARG is your friend.\nDockerfile ARG\nDocker compose args\nDockerfile\nARG VER=latest\nFROM ubuntu:${VER}\nThe above dockerfile defines a build argument named VER, which is default to latest.\ndocker-compose.yml\nversion: '3'\nservices:\n  demo:\n    image: enix223/demo\n    build:\n      context: .\n      args:\n        VER: ${VERSION}\nWe substitute image build arg VER with environment variable VERSION.\nStart container using shell env variable\nVERSION=\"$(cat /etc/lsb-release | grep -o 'DISTRIB_RELEASE.*' |  cut -f2- -d=)\" docker-compose up -d\nStart container using .env file\ncat > .env << EOF\nVERSION=\"$(cat /etc/lsb-release | grep -o 'DISTRIB_RELEASE.*' |  cut -f2- -d=)\"\nEOF\n\ndocker-compose up -d",
    "I need to create a kafka image with topics already created": "I had to do it too! What if I did not want to use wurstmeister images? I decided to make a custom script which will do the job, and run this script in a separate container.\nRepository\nhttps://github.com/yan-khonski-it/kafka-compose\nNote, it will work with kafka versions that use zookeeper. Is Zookeeper a must for Kafka?\nTo start kafka with all your topics and zookeeper - docker-compose up -d.\nImplementation details.\ndocker-compose.yml\n# These services are kafka related. This docker-compose allows to start kafka locally quickly.\n\nversion: '2.1'\n\nnetworks:\n  demo-network:\n    name: demo-network\n    driver: bridge\n\nservices:\n  zookeeper:\n    image: \"confluentinc/cp-zookeeper:${CONFLUENT_PLATFORM_VERSION}\"\n    container_name: zookeeper\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 32181\n      ZOOKEEPER_TICK_TIME: 2000\n    ports:\n      - 32181:32181\n    hostname: zookeeper\n    networks:\n      - demo-network\n\n  kafka:\n    image: \"confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION}\"\n    container_name: kafka\n    hostname: kafka\n    ports:\n      - 9092:9092\n      - 29092:29092\n\n    environment:\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n      KAFKA_BROKER_ID: 1\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_HOST://kafka:29092\n      LISTENERS: PLAINTEXT://0.0.0.0:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    depends_on:\n      - \"zookeeper\"\n    networks:\n      - demo-network\n\n  # Automatically creates required kafka topics if they were not created.\n  kafka-topics-creator:\n    build:\n      context: kafka-topic-creator\n      dockerfile: Dockerfile\n    container_name: kafka-topics-creator\n    depends_on:\n      - zookeeper\n      - kafka\n    environment:\n      ZOOKEEPER_HOSTS: \"zookeeper:32181\"\n      KAFKA_TOPICS: \"topic_v1 topic_v2\"\n    networks:\n      - demo-network\nThen I have a directory kafka-topics-creator. Here, I have three files create-kafka-topics.sh, Dockerfile, README.md.\nDockerfile\n# It is recommened to use same version as kafka broker is used.\n# So no additional images are pulled.\nFROM confluentinc/cp-kafka:4.1.2\n\nWORKDIR usr/bin\n\n# Once it is executed, this container is not needed.\nCOPY create-kafka-topics.sh create-kafka-topics.sh\nENTRYPOINT [\"./create-kafka-topics.sh\"]\ncreate-kafka-topics.sh\n#!/bin/bash\n\n# Simply wait until original kafka container and zookeeper are started.\nsleep 15.0s\n\n# Parse string of kafka topics into an array\n# https://stackoverflow.com/a/10586169/4587961\nkafkatopicsArrayString=\"$KAFKA_TOPICS\"\nIFS=' ' read -r -a kafkaTopicsArray <<< \"$kafkatopicsArrayString\"\n\n# A separate variable for zookeeper hosts.\nzookeeperHostsValue=$ZOOKEEPER_HOSTS\n\n# Create kafka topic for each topic item from split array of topics.\nfor newTopic in \"${kafkaTopicsArray[@]}\"; do\n  # https://kafka.apache.org/quickstart\n  kafka-topics --create --topic \"$newTopic\" --partitions 1 --replication-factor 1 --if-not-exists --zookeeper \"$zookeeperHostsValue\"\ndone\nREADME.md - so other people know how to use it.Always document your stuff - good advise.\n# Creates kafka topics automatically.\n\n## Parameters\n`ZOOKEEPER_HOSTS` - zookeeper hosts,  I used value `\"zookeeper:32181\"` to run it locally.\n\n`KAFKA_TOPICS` - space separated list of kafka topics. Example, `topic_1, topic_2, topic_3`.\n\nNote, this container should run only **after** your original kafka broker and zookeeper are running.\nAfter this container creates topics, it is not needed anymore.\nHow to check that the topics were created.\nOne solution is to check logs of kafka-topics-creator container.\ndocker logs kafka-topics-creator should print\n$ docker logs kafka-topics-creator\nWARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\nCreated topic \"topic_v1\".\nWARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\nCreated topic \"topic_v2\".",
    "--pid=host To Set through DockerFile": "You can put pid: \"host\" in your docker-compose.yml file to make it work. It's documented here.\npid: \"host\"\nSets the PID mode to the host PID mode. This turns on sharing between container and the host operating system the PID address space. Containers launched with this flag can access and manipulate other containers in the bare-metal machine\u2019s namespace and vice versa.",
    "How can i decrease Spring Boot application memory usage in Docker?": "You need to pass something like -m 50m to limit memory available for the container along with -Xmx and Xms for JVM.\nFollowing article explains it well.\nJVM Memory Allocation in Docker Container",
    "cannot execute RUN mkdir in a Dockerfile with space in directory name": "Came across same problem. None of the answers worked for me. I finally got it working by escaping space with `\nRUN mkdir \"C:\\Program` Files\\Microsoft` Passport` RPS\"\nCOPY . \"C:\\Program` Files\\Microsoft` Passport` RPS\"\nAnother approach is to use Shell, and declare escape explictly\nWhile the JSON form is unambiguous and does not use the un-necessary cmd.exe, it does require more verbosity through double-quoting and escaping. The alternate mechanism is to use the SHELL instruction and the shell form, making a more natural syntax for Windows users, especially when combined with the escape parser directive\n# escape=`\n\nFROM microsoft/nanoserver\nSHELL [\"powershell\",\"-command\"]\nRUN New-Item -ItemType Directory C:\\Example\nADD Execute-MyCmdlet.ps1 c:\\example\\\nRUN c:\\example\\Execute-MyCmdlet -sample 'hello world'",
    "How to run `git clone` in docker file?": "You can pass credentials as arguments to container. This should work\nFROM alpine:3.8\n\nRUN apk update && apk upgrade && \\\n    apk add --no-cache bash git openssh\n\nARG username\nARG password\n\nRUN git clone https://${username}:${password}@github.com/username/repository.git\n\nENTRYPOINT [\"sleep 10\"]\nbut it might be unsafe if you want to distribute that image\nthen build\ndocker build \\\n    --no-cache \\\n    -t git-app:latest \\\n    --build-arg username=user \\\n    --build-arg password=qwerty \\\n    .",
    "How to bind docker container ports to the host using helm charts": "EXPOSE informs Docker that the container listens on the specified network ports at runtime but does not actually make ports accessible. only -p as you already mentioned will do that:\ndocker run -p :$HOSTPORT:$CONTAINERPORT\nOr you can opt for a docker-compose file, extra file but also do the thing for you:\nversion: \"2\"\nservices:\n  my_service:\n    build: .\n    name: my_container_name\n    ports:\n      - 80:8080\n    .....\nEdit:\nIf you are using helm you have just to use the exposed docker port as your targetPort :\napiVersion: v1\nkind: Service\nmetadata:\n  name: {{ template \"fullname\" . }}\n  labels:\n    chart: \"{{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }}\"\nspec:\n  type: {{ .Values.service.type }}\n  ports:\n  - port: {{ .Values.service.externalPort }}\n    targetPort: {{ .Values.service.internalPort }} #8080\n    protocol: TCP\n    name: {{ .Values.service.name }}\n  selector:\n    app: {{ template \"fullname\" . }}",
    "How to set system-wide environment variables in dockerfile from a file inside the docker container?": "When you run this command:\ndocker run -it xyz:latest echo $SDKTARGETSYSROOT\nThe $SDKTARGETSYSROOT is expanded by your local shell. In no case were you actually testing environment variables inside of your container. This would work:\ndocker run -it xyz:latest sh -c 'echo $SDKTARGETSYSROOT'\nThe single quotes inhibit variable expansion in your local shell, and we need to explicitly call sh -c ... because Docker does not, by default, execute commands using a shell. Compare:\n$ docker run -it alpine echo $HOME\n/home/lars\nWith:\n$ docker run -it alpine echo '$HOME'\n$HOME\nWith:\n$ docker run -it alpine sh -c 'echo $HOME'\n/root\nOtherwise, several of your approaches look like a reasonable way to do what you want.",
    "Does Docker execute the entrypoint when a container is used in a multistage build?": "From official documentation:\nOnly the last ENTRYPOINT instruction in the Dockerfile will have an effect.",
    "Docker: basic example dockerfile to run html file": "To server a html file on http you will need a web server, so to do this all you need to do is get an docker image of httpd server, put your html file in the root directory of the webserver and expose the service on some port(suppose 8080), Let's do it one by one.\n1.) Create a docker file with this content\nFROM httpd:2.4\n\nCOPY ./public-html/ /usr/local/apache2/htdocs/\n2.)\ndocker build -t my-apache2 .\n3.)\ndocker run -dit -p 8080:80 --name my-running-app my-apache2 \nThat's it. Your html page should be now available at http://yourip:8080/public-html",
    "Docker-compose error: invalid reference format: repository name must be lowercase": "It turns out that by repository it meant \u2018service\u2019. I updated service name as show bellow and it works.\n services:\n      Pricing.api: => pricing.api with lowercase \u2018p\u2019\n        environment:\n          - ASPNETCORE_ENVIRONMENT=Development\n        ports:\n          - \"80\"\nIt should have said\ninvalid reference format. Service name must be lowercase .\nThis is a confusing error message. This is something Docker team has to fix imo.",
    "Node App Can't Read File System in Docker Image": "The challenge is identifying the expected location of node script execution and the actual working directory in docker. This difference will account for discrepancies between what you expect the file path of \".\" to be and what it actually is.\nThe easiest way to determine if you are executing with the context you think you are is to console out the Fs.realpathSync(\".\") I would expect that the location is not where you thought you were executing from (but it should match with what your WORKDIR is set to in your Docker image). (You can also prove this is your problem by changing your \".\" paths to absolute ones temporarily.)\nChange your working directory to point to where you expect your \".\" to be (in your case WORKDIR /src) and you should be able to use the \".\" the way you expect.",
    "Docker Compose: Accessing my webapp from the browser": "Fairly new here as well, but did you publish your ports on your docker-compose file?\nYour Dockerfile will simply expose the ports but not open access to your host. Publishing (with the -p flag on a docker run command or ports on a docker-compose file. Will enable access from outside the container (see ports in the docs) Something like this may help:\nports:\n  - \"8080:8080\"",
    "How to use STOPSIGNAL instruction within Docker?": "A SIGKILL is a signal which stops the process immediately, without letting the process exit cleanly, so SIGKILL will not allow Tomcat to shutdown gracefully and remove it's PID file.\nSIGTERM and SIGINT both tell Tomcat to run the shutdown hook (deleting the PID file) and shutting down gracefully.\nSIGTERM is equivalent to running kill <pid> and is also the default for docker.\nSIGINT is equivalent to pressing ctrl-C.",
    "How to set environment variables via env-file": "I think\n docker run\ntakes all parameters before the image and the command. If I do\ndocker run -t --env-file=env.list ubuntu sh -c \"while true; do echo world; sleep 100 ;done\"\nand then\ndocker exec -it container_id env\nI get\nHOSTNAME=195f18677a91\nTERM=xterm\nACCOUNT_ID=my_account_id\nACCOUNT_PASSWORD=my_secret_password\nHOME=/root\nTry\ndocker run -p 8080:8080 --env-file=~/env.list -t myname/myapplication",
    "Installing libraries in ubuntu image using Docker": "You need to run apt-get update e.g:\nRUN apt-get update && apt-get install -y sl\nYou can also tidy up after yourself to save a bit of disk space:\nRUN apt-get update && apt-get install -y sl && rm -r /var/lib/apt/lists/*",
    "Docker's heredoc example for RUN is not working": "You need to use the buildkit toolkit for that:\n$ DOCKER_BUILDKIT=1 docker build .",
    "Caching Go Depencencies in Docker": "I think you can just start doing that and see. For me, if I had to use this, I would choose a different mount path, just to isolate the local environment.\nRUN --mount=type=cache,target=/Users/me/Library/Caches go mod download\nI don't see any problem mounting downloaded packages. In this example, it is used for apt.\nHowever, I think we can also take a step back and consider different approaches to not to be bothered by waiting for Docker build.\nWe can have scheduled builds (nightly build and etc), I don't see any reason to do local docker build that frequent, especially your dependencies shouldn't change that often. Maybe I am wrong, but I don't think we need to.\nWe can also further break down go mod download and leverage Docker build cache E.g.\nRUN go mod download package-never-change\nRUN go mod download package-changes-frequently\nEven\nRUN go build ./package1\nRUN go build ./package2\nIt maybe look a little bit tedious, but under certain circumstances it could be useful, for example, when BuildKit is not supported.",
    "nestjs Docker build error: can not find tsconfig.build.json": "In your last step, you never copy over the tsconfig.build.json file or the tsconfig.json. Though, I don't see why you're using start:dev when you've already built the server in the docker image. You should just be calling node dist/main",
    "Create SQL Server docker image with restored backup database using purely a Dockerfile": "If you know where the data directory is in the image, and the image does not declare that directory as a VOLUME, then you can use a multi-stage build for this. The first stage would set up the data directory as you show. The second stage would copy the populated data directory from the first stage but not the backup file. This trick might depend on the two stages running identical builds of the underlying software.\nFor SQL Server, the Docker Hub page and GitHub repo are both tricky to find, and surprisingly neither talks to the issue of data storage (as @HansKillian notes in a comment, you would almost always want to store the database data in some sort of volume). The GitHub repo does include a Helm chart built around a Kubernetes StatefulSet and from that we can discover that a data directory would be mounted on /var/opt/mssql.\nSo I might write a multi-stage build like so:\n# Put common setup steps in an initial stage\nFROM mcr.microsoft.com/mssql/server:2019-latest AS setup\nENV MSSQL_PID=Developer\nENV SA_PASSWORD=Password1?  # (weak password, easily extracted with `docker inspect`)\nENV ACCEPT_EULA=Y           # (legally probably the end user needs to accept this not the image builder)\n\n# Have a stage specifically to populate the data directory\nFROM setup AS data\n# (copy-and-pasted from the question)\nUSER mssql\nCOPY rmsdev.bak /  # not under /var/opt/mssql\nRUN ( /opt/mssql/bin/sqlservr & ) | grep -q \"Service Broker manager has started\" \\\n    && /opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P $SA_PASSWORD -Q 'RESTORE DATABASE rmsdev FROM DISK = \"/rmsdev.bak\" WITH MOVE \"rmsdev\" to \"/var/opt/mssql/data/rmsdev.mdf\", MOVE \"rmsdev_Log\" to \"/var/opt/mssql/data/rmsdev_log.ldf\", NOUNLOAD, STATS = 5' \\\n    && pkill sqlservr\n\n# Final stage that actually will actually be run.\nFROM setup\n# Copy the prepopulated data tree, but not the backup file\nCOPY --from=data /var/opt/mssql /var/opt/mssql\n# Use the default USER, CMD, etc. from the base SQL Server image\nThe standard Docker Hub open-source database images like mysql and postgres generally declare a VOLUME in their Dockerfile for the database data, which forces the data to be stored in a volume. The important thing this means is that you can't set up data in the image like this; you have to populate the data externally, and then copy the data tree outside of the Docker image system.",
    "Dockerfile Healthcheck with environment variable": "Just use ${PORT} - no workarounds needed\nTo reference an environment variable in a Dockerfile, just use wrap it inside a ${\u2026} and for then the runtime value is used e.g. for the healthcheck.\nYou can also set a default like this ${PORT:-80} I don't remember where I saw/read this. But it works :)\nSo my Dockerfile looks like this\nFROM node:lts-alpine\nRUN apk add --update curl\n\nENV NODE_ENV production\nENV HOST '0.0.0.0'\nEXPOSE ${PORT:-80}\n\n# Other steps\n\nHEALTHCHECK --interval=5m --timeout=3s \\\n  CMD curl -f http://localhost:${PORT}/health || exit 1\n\nCMD [\"node\", \"server.js\"]",
    "Docker run Error: /bin/sh: 1: python: not found": "Since you are only install python3 inside the docker image as shown here\nRUN apt-get update && apt-get install -y python3 python3-pip\nSo you will need to run python3 instead of python in this line: CMD python .main.py\nAnd you have a typo in the script name. It should be main.py instead of .main.py. Or it should be ./main.py\nSo change it to CMD python3 ./main.py\nAnd if you still have error, you probably need to add this line in the Dockerfile above line of EXPOSE 5000:\nWORKDIR /opt/MyApp-test",
    "Docker: Cannot launch .Net Core 3.1 Web Api on browser after docker run": "In my case it was due to having SSL enabled inthe project, but after removing everything related to https it started working as expected.\nHaving SSL enabled implies installing a SSL certificate inside the docker container and enabling it, things I wasn't able to do as I don't have a certificate and I'm just learning docker.\nSteps I've done:\nIn Startup.cs I've disabled https redirection\n//app.UseHttpsRedirection();\nAnd in project properties under \"Debug\" section I unchecked \"Enable SSL\".\nThen I was able to run the container as expected.",
    "ENV, RUN produce layers, images or containers? (understanding docs question)": "The Docker as containerization system based on two main concepts of image and container. The major difference between them is the top writable layer. When you generate a new container the new writable layer will be put above the last image layer. This layer is often called the container layer.\nAll the underlying image content remains unchanged and each change in the running container that creating new files, modifying existing files and so on will be copied in this thin writable layer.\nIn this case, Docker only stores the actual containers data and one image instance, which decreases storage usage and simplifies the underlying workflow. I would compare it with static and dynamic linking in C language, so Docker uses dynamic linking.\nThe image is a combination of layers. Each layer is only a set of differences from the layer before it.\nThe documentation says:\nOnly the instructions RUN, COPY, ADD create layers. Other instructions create temporary intermediate images, and do not increase the size of the build.\nThe description here is neither really clear nor accurate, and generally speaking these aren't the only instructions that create layers in the latest versions of Docker, as the documentation outlines.\nFor example, by default WORKDIR creates a given path if it does not exist and change directory to it. If the new path was created WORKDIR will generate a new layer.\nBy the way, ENV doesn't lead to layer creation. The data will be stored permanently in image and container config and there is no easy way to get rid of it. Basically, there are two options, how to organize workflow:\nTemporal environment variables, they will be available until the end of the current RUN directive:\nRUN export NAME='megatron' && echo $NAME # 'megatron'\nRUN echo $NAME # blank\nClean environment variable, if there is no difference for you between the absence of env or blank content of it, then you could do:\nENV NAME='megatron'\n# some instructions\nENV NAME=''\nRUN echo $NAME\nIn the context of Docker, there is no distinction between commands and instructions. For RUN any commands that don't change filesystem content won't trigger permanent layers creation. Consider the following Dockerfile:\nFROM alpine:latest\nRUN echo \"Hello World\" # no layer\nRUN touch file.txt # new layer\nWORKDIR /no/existing/path # new layer\nIn the end, the output would be:\nStep 1/4 : FROM alpine:latest\n ---> 965ea09ff2eb\nStep 2/4 : RUN echo \"Hello World\"\n ---> Running in 451adb70f017\nHello World\nRemoving intermediate container 451adb70f017\n ---> 816ccbd1e8aa\nStep 3/4 : RUN touch file.txt\n ---> Running in 9edc6afdd1e5\nRemoving intermediate container 9edc6afdd1e5\n ---> ea0040ec0312\nStep 4/4 : WORKDIR /no/existing/path\n ---> Running in ec0feaf6710d\nRemoving intermediate container ec0feaf6710d\n ---> f2fe46478f7c\nSuccessfully built f2fe46478f7c\nSuccessfully tagged envtest:lastest\nThere is inspect command for inspecting Docker objects:\ndocker inspect --format='{{json .RootFS.Layers}}' <image_id>\nWhich shows us the list of SHA of three layers getting FROM, second RUN and WORKDIR directives, I would recommend using dive for exploring each layer in a docker image.\nSo why does it say removing intermediate container and not removing intermediate layer? Actually to execute RUN commands Docker needs to instantiate a container with the intermediate image up to that line of the Dockerfile and run the actual command. It will then \"commit\" the state of the container as a new intermediate image and continue the building process.",
    "Why does docker have to create an image from a dockerfile then create a container from the image instead of creating a container from a Dockerfile?": "the Dockerfile is the recipe to create an image\nthe image is a virtual filesystem\nthe container is the a running process on a host machine\nYou don't want every host to build its own image based on the recipe. It's easier for some hosts to just download an image and work with that.\nCreating an image can be very expensive. I have complicated Dockerfiles that may take hours to build, may download 50 GB of data, yet still only create a 200 MB image that I can send to different hosts.\nSpinning up a container from an existing image is very cheap.\nIf all you had was the Dockerfile in order to spin up image-containers, the entire workflow would become very cumbersome.",
    "How run jboss-cli on start docker container with Dockerfile": "This works for me:\n   RUN /bin/sh -c '$JBOSS_HOME/bin/standalone.sh -c=standalone-full.xml &' && \\\n      sleep 10 && \\\n      cd /tmp && \\\n      $JBOSS_HOME/bin/jboss-cli.sh --connect --command=\"module add --name=org.postgresql --resources=$JBOSS_HOME/standalone/configuration/postgresql-42.2.5.jar --dependencies=javax.api,javax.transaction.api,javax.servlet.api\" && \\\n      $JBOSS_HOME/bin/jboss-cli.sh --connect --command=:shutdown \n\n\n    # User root to modify war owners\n    USER root\n\n    CMD [\"/opt/eap/bin/standalone.sh\", \"-c\", \"standalone-full.xml\", \"-b\", \"0.0.0.0\",\"-bmanagement\",\"0.0.0.0\"] ",
    "Include .env file in `go build` command": "You cannot include non-go files in the go build process. The Go tool doesn't support \"embedding\" arbitrary files into the final executable.\nYou should use go build to build your executable then, any non-go files, e.g. templates, images, config files, need to be made available to that executable. That is; the executable needs to know where the non-go files are on the filesystem of the host machine on which the go program is running, and then open and read them as needed. So forget about embeding .env into main, instead copy .env together with main to the same location from which you want to run main.\nThen the issue with your dockerfile is the fact that the target host only copies the final executable file from go-compile (COPY --from=go-compile /app/main /app/main), it doesn't copy any other files that are present in the go-compile image and therefore your main app cannot access .env since they are not on the same host.\nAs pointed out in the comments by @mh-cbon, there do exist 3rd-party solutions for embedding non-go files into the go binary, one of which is gobuffalo/packr.",
    "Receiving pull access denied error while trying to run docker-compose.yml file": "You have an extra 'b' in rabbitmq image name, it should be rabbitmq:latest and not rabbbitmq:latest.",
    "The command returned a non-zero code: 127": "This is a PATH related issue and profile. When you use sh -c or bash -c the profile files are not loaded. But when you use bash -lc it means load the profile and also execute the command. Now your profile may have the necessary path setup to run this command.\nEdit-1\nSo the issue with the original answer was that it cannot work. When we had\nENTRYPOINT [\"/bin/bash\", \"-lc\", \"ocp-indent\"]\nCMD [\"--help\"]\nIt finally translates to /bin/bash -lc ocp-indent --help while for it to work we need /bin/bash -lc \"ocp-indent --help\". This cannot be done by directly by using command in entrypoint. So we need to make a new entrypoint.sh file\n#!/bin/sh -l\nocp-indent \"$@\"\nMake sure to chmod +x entrypoint.sh on host. And update the Dockerfile to below\nFROM ocaml/opam\n\nWORKDIR /workdir\n\nRUN opam init --auto-setup\nRUN opam install --yes ocp-indent\nSHELL [\"/bin/sh\", \"-lc\"]\nCOPY entrypoint.sh /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\nCMD [\"--help\"]\nAfter build and run it works\n$ docker run f76dda33092a\nNAME\n       ocp-indent - Automatic indentation of OCaml source files\n\nSYNOPSIS\nOriginal answer\nYou can easily test the difference between both using below commands\ndocker run -it --entrypoint \"/bin/sh\" <image id> env\ndocker run -it --entrypoint \"/bin/sh -l\" <image id> env\ndocker run -it --entrypoint \"/bin/bash\" <image id> env\ndocker run -it --entrypoint \"/bin/bash -l\" <image id> env\nNow either you bash has correct path by default or it will only come when you use the -l flag. In that case you can change the default shell of your docker image to below\nFROM ocaml/opam\n\nWORKDIR /workdir\n\nRUN opam init --auto-setup\nRUN opam install --yes ocp-indent\nSHELL [\"/bin/bash\", \"-lc\"]\nRUN ocp-indent --help\n\nENTRYPOINT [\"/bin/bash\", \"-lc\", \"ocp-indent\"]\nCMD [\"--help\"]",
    "Dockerfile ADD doesn't work": "Your source directory is correct \u2014 '.'. Your destination dir may lead to issues (~/var). I would specify the absolute path of the home dir intended e.g.\n/home//var. Furthermore, you can inspect the Docker image at the layer the build broke \u2014 in your case:\ndocker run --rm -it 64f1a5a8e039 bash  \nFurther details here: How can I inspect the file system of a failed `docker build`?",
    "Stop Solr gracefully when it running in docker": "When running $SOLR_HOME/bin/solr stop, the kernel sends a SIGQUIT signal to the process. To duplicate this in docker, you need to run docker kill --signal=\"SIGQUIT\" <ContainerName> rather than docker stop. Docker Stop sends a SIGTERM signal by default.",
    "Can't npm install dependencies when building docker image": "It's a problem of nodejs installation which was covered here: what are the differences between node.js and node?\nBreifly, there are three options to fix this: creating symlink yourself, using nvm, or installing nodejs-legacy instead of nodejs:\nRUN apt-get -y install nodejs-legacy",
    "boot2docker / docker \"Error. image library/.:latest not found\"": "You should run docker build first (which actually uses your Dockerfile):\ndocker build --tag=imagename .\nOr\ndocker build --tag=imagename -f yourDockerfile .\nThen you would use that image tag to docker run it:\ndocker run imagename",
    "How can i include volume data in docker image": "Depending on the nature of the data, you might also see it as the part of the image. In such case you can carry these files with the image. You could structure this as a base image and an image for a particular build, which would be built on a host that has access to the files needed (e.g. CI build node):\nFROM mybase\nADD <source of installation> /usr/local/data\nADD <source of the home data> /var/local/data\nThis new image (possibly versioned per build) would be pulled with the contents of the /usr/local/data and /var/local/data onto the target environment.\nWhen running on environment (production) you might still use the data container technique if needed:\ndocker run --it -v /usr/local/data -v /var/local/data --name my_app_data_container <my_repo>/<my_app>:<build> /bin/false\ndocker run -d --volumes-from my_app_data_container --name my_app_daemon <my_repo>/<my_app>:<build>",
    "How does the \"RUN\" instruction actually work in a Dockerfile": "The RUN statement executes commands in a container running the current image layer with the result becoming the next layer.\nConsider these two scenarios...\nDockerfile\nFROM alpine:3\nRUN apk add vim\ndocker run\n% docker run -it --rm alpine:3\n/ # apk add vim\nBoth do exactly the same thing but the first commits the change to the next image layer. The second is ephemeral only.",
    "How to install node in .NET 6 docker container": "publish stage doesn't have base stage. Try install node in build stage\ntry adding RUN apt-get... and RUN curl -sL de... after \"as build\" stage\n#base stage\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 AS base\n...\n\n#build stage\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS build\n\nRUN apt-get update -yq && apt-get upgrade -yq && apt-get install -yq curl git nano\nRUN curl -sL https://deb.nodesource.com/setup_16.x | bash - && apt-get install -yq nodejs build-essential\n\n\nWORKDIR /src\n\n...",
    "How to copy contents of folder to /app via dockerfile?": "The Dockerfile documentation for the COPY directive notes that it has two forms, a space-separated form and a JSON-array form, and it notes\nThis latter [JSON array] form is required for paths containing whitespace.\nSo applying that to your specific path, you would get\nCOPY [\"src/Shared Settings\", \"/app\"]\nThis is broadly true in a Dockerfile in general: the only quoting in native Dockerfile syntax is to write things in a JSON array. Wrapping a WORKDIR or a COPY command in single quotes isn't documented as having an effect. A string-form RUN or CMD is an apparent exception to this, but only because these commands are run via a shell and there the shell's quoting rules apply.",
    "How to specify and use a cert file during a docker build": "I finally figured it out.\nAll works with this -\nFROM alpine:3.14.1\n\nCOPY trusted-certs.pem /root/trusted-certs.pem\nRUN SSL_CERT_FILE=~/trusted-certs.pem apk add ca-certificates\nRUN update-ca-certificates\nRUN apk update && apk upgrade\nRUN apk add curl\nRUN curl https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip -o awscliv2.zip\nEven cleaner :\nFROM alpine\n\nCOPY ./trusted-certs.pem /usr/local/share/ca-certificates/\nRUN cat /usr/local/share/ca-certificates/trusted-certs.pem >> /etc/ssl/certs/ca-certificates.crt\n\nRUN apk update && apk add --no-cache jq\nRUN apk add curl\nRUN curl https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip -o awscliv2.zip",
    "How to use \"bind mount\" in Docker": "The binding -v \"/WorkSpace/d/data-volumes-03-adj-node-code:/app\" ruins everything! It mounts contents from host directory on /app path of the container, so all modules installed during building of the docker image will be lost.\nWhat can be done? Install node modules somewhere else and change default node path to it. So your Dockerfile becomes (Removed COPY . . because you want bind mount your project files on the container so it is unnecessary):\nFROM node:14\n \nWORKDIR /modules\n \nCOPY package.json .\n \nRUN npm install \n\nENV NODE_PATH=/modules/node_modules\n \nEXPOSE 80\n\nWORKDIR /app\n \nCMD [\"node\", \"server.js\"]\nAnd to run your docker image:\ndocker run -d -p 3000:80 --name feedback-app -v feedback:/app/feedback -v \"/c/WorkSpace/d/data-volumes-03-adj-node-code:/app\" -v /modules/node_modules feedback-node:volumes",
    "microk8s Kubernetes Service connection refused": "It's due to you are using the ClusterIP which is only accessible internally from CLuster.\nHere you require the Nginx ingress controller to expose the service. Or you can try with the Host IP once to connect with the service.\nYou can also try the command\nkubectl port-forward svc/hello 8080:8080\nonce the port is forwarded you can hit the curl on localhost:8080.\nHowever, for production use-case it's always suggested to use the ingress for managing the cluster traffic.\nIngress basically work as the Proxy it's same as Nginx.\nIngress is configuration object which managed by the Ingress controller. When you enable the ingress you require the Ingress controller in minikube it will be there however for other Cluster on GKE & EKS you have to setup manually.\nHere is good example for the implementation of the ingress in minikube :\nhttps://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/\ningress example\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\nspec:\n  rules:\n    - host: hello-world.info\n      http:\n        paths:\n          - path: /\n            backend:\n              service:\n                name: hello\n                port:\n                  number: 8080",
    "permission denied in Dockerfile": "Solved. the solution was in 2 steps:\nre-install docker.\nchanging owner to node, like this:\nFROM node:10\nRUN mkdir -p /home/node/app && chown -R node:node /home/node/app\nWORKDIR /home/node/app\nCOPY package.json .\nUSER node\nRUN npm install\n...",
    "Getting connection reset when accessing running docker container": "I finally was able to get everything working.\nChanged docker subnet settings, cause somehow subnets created by docker-compose were interfering with host's public interface. Added code below to /etc/docker/daemon.json\n{ \"dns\": [\"10.10.1.222\"], \"default-address-pools\": [ {\"base\":\"10.10.0.0/16\",\"size\":24} ] }\nRestart docker service\nForced docker compose to use default bridge network. In docker-compose.yaml\nnetwork_mode: bridge\nDown containers created by compose, then re-up\nRemoved all networks created by compose\ndocker network rm\nNote: From my question, docker routes were deleted because somehow they were making host unreachable on reboot. Changing docker subnets and removing all other networks except the default solved that problem.\ndocker network ls \n\n\n\nNETWORK ID          NAME                DRIVER              SCOPE\n15d807ad77d4        bridge              bridge              local\n6cd415046a23        host                host                local\n6e9b8dfc500e        none                null                local\nEdit\nAs per @OlivierMehani comment, while using 20.10.0.0/16 subnet worked for me, it might cause problems as 20.10.0.0/16 is public address range. To be safe, use IP ranges 10.0.0.0 - 10.255.255.255, 172.16.0.0 to 172.31.255.255 and 192.168.0.0 to 192.168.255.255. Just make sure the subnet you pick is not being used in your organization especially by the docker host.",
    "How to mount the folder as volume that docker image build creates?": "You are using a Bind Mount which will \"hide\" the content already existing in your image as you describe - /host_path/configs being empty, /docker_container/configs will be empty as well.\nYou can use named Volumes instead which will automatically populate the volume with content already existing in the image and allow you to perform updates as you described:\nservices:\n  backend:\n    # ...\n    #\n    # content of /docker_container/configs from the image\n    # will be copied into backend-volume\n    # and accessible at runtime\n    volumes:\n    - backend-volume:/docker_container/configs\n\nvolumes:\n  backend-volume:\nAs stated in the Volume doc:\nIf you start a container which creates a new volume [...] and the container has files or directories in the directory to be mounted [...] the directory\u2019s contents are copied into the volume",
    "How to Install Node 8.15 on alpine:3.9?": "Why you are installing with NVM when we have nodejs in alpine offical repository? each Docker image should represent a version of nodejs. So I will not suggest NVM in this case also will keep the image small.\nYou can find version alpine-pacakge-nodejs v8.x.\nFROM alpine:3.9\n\nENV METEOR_VERSION=1.8.1\nENV METEOR_ALLOW_SUPERUSER true\nENV NODE_VERSION 8.15\nRUN apk add --no-cache --repository=http://dl-cdn.alpinelinux.org/alpine/v3.8/main/ nodejs=8.14.0-r0 npm \n\nRUN node --version\noutput\nStep 6/6 : RUN node --version\n ---> Running in 9652a49223fa\nv8.14.0",
    "How to run/host: Multiple environments on same machine with docker-compose": "That was really silly on my part. I missed out an important point in the documentation on docker-compose. You need to specify COMPOSE_PROJECT_NAME environment variable, if not specified then it will pick up the folder name where your compose file resides. Just name this environment variable differently for your environment and you are good to go.",
    "How to install scikit-learn, pandas and numpy in a docker image?": "Simply add numpy and scikit-learn to PySEAL's requirements file.\nYour final requirements file should be:\npybind11\ncppimport\njupyter\nnumpy\nscikit-learn\nAnd run build-docker.sh again.",
    "Go build: build output \"api\" already exists and is a directory": "Your default go build is attempting to output the same name as the directory. You could change your build and ENTRYPOINT line to refer to \"go build -o apiserver\".",
    "What is docker node:9-stretch?": "The node 9 build was dropped after this commit https://github.com/nodejs/docker-node/commit/b22fb6c84e3cac83309b083c973649b2bf6b092d. You can find Dockerfile in diff.\nThe node:9-stretch image you can pull build before the commit, and persisted in docker hub. The 9-stretch tag exists in Tags page, as for now https://hub.docker.com/_/node?tab=tags&page=18.",
    "Difference between CMD echo 'Hello world' and CMD [\"echo\", ''Hello world'] in a dockerfile?": "There is not much difference in your simple example, but there will be a difference if you need shell features such as variable substitution e.g. $HOME.\nThis is the shell form. It will invoke a command shell:\nCMD echo 'Hello world'\nThis is the exec form. It does not invoke a command shell:\nCMD [\"/usr/bin/echo\", \"Hello World\"]\nThe exec form is parsed as a JSON array, which means that you should be using double-quotes around words, not single-quotes, and you should give the full path to an executable. The exec form is the preferred format of CMD.",
    "CORS blocking API call from Dockerized React project": "Your node application should support CORS, if you are using express, you should add the following lines in the app.js file\nconst cors = require('cors');\napp.use(cors());\napp.options('*', cors());",
    "Dockerizing Postfix Relay server": "Postfix v3.3.0 added support for container:\nContainer support: Postfix 3.3 will run in the foreground with \"postfix start-fg\".\nIf you are using a lower version, you might need to use supervisord or an infinite loop or an infinite sleep just to stop the container from exiting.",
    "Dockerfile: Inherit environmental variables from shell": "You could define default values with ARG:\nARG build_var=default_value\nENV ENV_VAR=$build_var\nand then override at build time:\ndocker build --build-arg build_var=$HOST_VAR",
    "Mount a local directory as volume in container, from the Dockerfile": "You do not have access to control things like host volume mounts inside the Dockerfile or image build process. Allowing this would allow malicious image creators to make an image that mounts directories on the host without the permission of the admin of that host. A security breach that allowed a popular base image to mount the filesystem could be used to send private data off-site and inject login credentials on countless machines. The only way to mount a volume is at run time at the explicit request of the admin running the container, and to the directory they provide.",
    "Docker: adding rsa keys to image from outside of build context": "What about using a build argument? Do something like this in your Dockerfile:\nARG rsakey\nRUN test -n \"${rsakey}\" && { \\\n      mkdir -p -m 700 /root/.ssh; \\\n      echo \"${rsakey}\" > /root/.ssh/id_rsa; \\\n      chmod 600 /root/.ssh/id_rsa; \\\n    } || :\nThen, when you build the image, use the --build-arg option:\ndocker build -t sshtest --build-arg rsakey=\"$(cat /path/to/id_rsa)\" .\nThis will inject the key into the image at build time without requiring it to live in your build context.",
    "Docker secrets in build-time": "Note: Docker secrets are only available to swarm services, not to standalone containers. To use this feature, consider adapting your container to run as a service with a scale of 1.\nThe answer is no. Secretes are only available in docker swarm. Swarm doesn't build images and it doesn't accept a Dockerfile.",
    "Running Wildfly Swarm with KeyCloak on docker image": "It looks like the Swarm Keycloak Server reads the keycloak*.db in the dir executed java(means user.dir) in default. The swarm process in container doesn't read /opt/keycloak*.db because java runs on /.\nYou can change the data dir with wildfly.swarm.keycloak.server.db sysprop. https://github.com/wildfly-swarm/wildfly-swarm/blob/2017.6.1/fractions/keycloak-server/src/main/java/org/wildfly/swarm/keycloak/server/runtime/KeycloakDatasourceCustomizer.java#L52\nPlease give it a try in Dockerfile;\nENTRYPOINT [\"java\", \"-jar\", \"/opt/login-service-swarm.jar\", \"-Dwildfly.swarm.keycloak.server.db=/opt/keycloak\"]\nOr, you can also use -w option with docker run.\n$ docker run --help\n-w, --workdir string              Working directory inside the container\nThe following command is supposed to work as well.\ndocker run -p 8180:8180 -w /opt login-service-swarm-v1\nP.S.\nI recommend using Volume or Volume Container instead of adding the data files in Dockerfile. https://docs.docker.com/engine/tutorials/dockervolumes/",
    "docker run complains file not found": "The /usr/src/testapp/ directory is not in the PATH environment variable, so /bin/sh complains.\nChange the last line to:\nCMD ./TestAppStart",
    "Docker Elasticsearch Plugin with Request": "We were able to get this working. We just had to add the script to the CMD command at the end of our Dockerfile so it ran after the ElasticSearch startup script.\nIt looks like you can only have one command per file so we had to look at the base Elastic image (ElasticSearch Docker GitHub) and add to it.\nCMD [\"/bin/bash\", \"bin/es-docker\", \"search-guard/run-sgadmin.sh\"]",
    "Using COPY on dockerfile for apache build": "Understand that you do not want to mount the volumes and instead wanted have those files part of the image so that it can be shared. At least assuming this.\nAs per the Docker documentation\nCOPY obeys the following rules:\n- The path must be inside the context of the build; you cannot COPY ../something /something, because the first step of a docker build is to send the context directory (and subdirectories) to the docker daemon.\n- If is a directory, the entire contents of the directory are copied, including filesystem metadata.\nYou might got the the problem by now. In your Dockerfile i.e., COPY statement is the problem since it is referring to absolute path which is not following the 1st rule from the above. So, htdocs should be available in the local directory from which you execute the docker build command.\nThe following changes need to made before building the image:\nHope you might have already created a directory(which you are building image ) and this directory contains Dockerfile, httpd-custom.conf files.\nNow, go into above directory and copy /opt/mw/apache-2.2.31-instance1/htdocs to current directory. So, that htdocs directory can be now part of context ( as mentioned in the docs) while building the image.\nChange the contents of Dockerfile to the following(mainly COPY command):\nFROM httpd:2.2.31\n\nRUN mkdir -p /opt/mw/apache-test/logs\n\nADD ./httpd-custom.conf /usr/local/apache2/conf/httpd.conf\nCOPY htdocs /usr/local/apache2/htdocs\nNow you should be able to build it successfully.\nFor just demo, used a light weight busybox and create a directory in the same context (to simulate your case) and it does as you see below:\n$ more Dockerfile \nFROM busybox\nCOPY data_folder  /opt/data_folder\nCMD [\"ls\", \"/opt/data_folder\"]\n\n$ ls\ndata_folder  Dockerfile\n$ ls data_folder/\ntest.txt\nBuild Image:\n$ sudo docker build  -t dirtest .\nSending build context to Docker daemon 3.584 kB\nStep 1 : FROM busybox\n ---> e02e811dd08f\nStep 2 : COPY data_folder /opt/data_folder\n ---> b6b1a9555825\nRemoving intermediate container b682e0467803\nStep 3 : CMD ls /opt/data_folder\n ---> Running in 3b05f08ceafc\n ---> b73190fc1fd9\nRemoving intermediate container 3b05f08ceafc\nSuccessfully built b73190fc1fd9\nRunning above Image in a Container which shows directory 'data_folder' is copied and showing its contents. In your case, it is htdocs\n$ sudo docker run -it --rm --name testdirtest dirtest\ntest.txt",
    "Docker SIGTERM not being delivered to node.js/coffee app when started with flags": "TL;DR Use a Javascript loader file instead of the coffee executable when you need to use extended node options to avoid the technicalities of signals with forked processes under Docker.\nrequire('coffee-script').register();\nrequire('./whatever.coffee').run();\nThen\nnode --max_old_space_size=384 app.js\nNow, onto the technicalities...\n\nDocker and signals\nThe initial process in a container is PID 1 in the containers namespace. PID 1 (or the init process) is treated as a special case by the kernel with regards to signal handling.\nIf the init process does not install a signal handler, that signal won't be sent to it.\nSignals do not propagate automatically from an init process, the process must manage this.\nSo a docker process is expected to handle signals itself.\n\nCoffeescripts --nodejs option\nAs you have noted, coffee will fork a child node process when it has the --nodejs option to be able to pass the extra options on.\nThis initially presents some odd behaviour outside of docker with signal handling (on osx at least). A SIGINT or SIGTERM will be forwarded onto the child but also kill the parent coffee process immediately, no matter how you handle the signal in your code (which is running in the child).\nA quick example\nprocess.on 'SIGTERM', -> console.log 'SIGTERM'\nprocess.on 'SIGINT', -> console.log 'SIGINT'\n\ncb = -> console.log \"test\"\nsetTimeout cb, 5000\nWhen you run this and ctrl-c, the signal is forwarded on to the child process and handled. The parent process closes immediately though and returns to the shell.\n$ coffee --nodejs --max_old_space_size=384 block_signal_coffee.coffee \n^C\nSIGINT\n$ <5ish second pause> test\nThen the child process with your code continues running in the background for 5 seconds and eventually outputs test.\n\nDocker and coffee --nodejs\nThe main problem is the parent coffee process does not handle any signals in code, so the signals don't arrive and are not forwarded onto the child. This probably requires a change to coffeescript's launcher code to fix.\nThe signal quirk coffee --nodejs presents outside of Docker could also be bad if it happened under Docker. If the main container process (the parent of the fork) exits before your signal handlers have a chance to complete in the child, the container will close around them. This scenario is unlikely to happen if the above problem is fixed by just forwarding signals onto the child.\nAn alternative to using the suggested javascript loader or fixing coffee scripts loader, would be to use an actual init process, like runit or supervisor but that adds another layer of complexity in between docker and your service.",
    "connect to mysql database from docker container": "At first you shouldn't expose mysql 3306 port if you not want to call it from host machine. At second links are deprecated now. You can use network instead. I not sure about compose v.1 but in v.2 all containers in common docker-compose file are in one network (more about networks) and can be resolved by name each other. Example of docker-compose v.2 file:\nversion: '2'\nservices:\n  web:\n    build: .\n    restart: always\n    volumes:\n      - .:/app/\n    ports:\n      - \"8000:8000\"\n      - \"80:80\"\n  mysql1:\n    image: mysql:latest\n    volumes:\n      - \"/var/lib/mysql:/var/lib/mysql\"\n    environment:\n      MYSQL_ROOT_PASSWORD: secretpass\nWith such configuration you can resolve mysql container by name mysql1 inside web container.",
    "Docker - how to add new python dependencies to the existing docker image?": "Once your container is in the right state (scikit-learn is installed, the script is executed), stop it (docker stop) and commit it as a new image.\nSee docker commit in order to commit a container\u2019s file changes or settings into a new image.\nThen you can run that new image (with the same parameters as before), except the container created from that new image will have the previous steps already there.\nBut the other approach is to build your image from the tenserflow udacity Dockerfile.\nFROM gcr.io/tensorflow/tensorflow:latest\nMAINTAINER Vincent Vanhoucke <vanhoucke@google.com>\nRUN pip install scikit-learn\nRUN rm -rf /notebooks/*\nADD *.ipynb /notebooks/\nWORKDIR /notebooks\nCMD [\"/run_jupyter.sh\"]\nThat image, by default, will execute the right command.",
    "How to use curl command to get manifest v2 schema version 2": "I believe the Accept type for a v2 schema is \"application/vnd.docker.distribution.manifest.v2+json\" according to the docker registry code, see:\nhttps://github.com/docker/distribution/blob/f4b6fc8d681c42137b7d2bb544b087462bc34d47/manifest/schema2/manifest.go#L15",
    "Dockerfile and dev/test/prod environment": "Why not copy in your Dockerfile all 3 config files, and then docker run -e config=file1 (or file2 or file3) and use the value of config environment variable to get the required config file .Check the doc http://docs.docker.com/reference/commandline/cli/#run\nYou can also use the --env-file=[]  Read in a line delimited file of environment variables of the docker run command.\nYou can check that environment variable is available with docker run -it -e mytag=abc123 ubuntu:latest env | grep mytag shows mytag=abc123",
    "Docker npm install --production not working": "I suspect your Dockerfile probably has something like COPY . . but you don't set .dockerignore correctly, e.g. you didn't add node_modules to your .dockerignore (check COPY with docker but with exclusion for the further information about .dockerignore)\nI made the same mistake too and it should have nothing do with nodejs14 or nodejs16\nBTW, npm install --only=prod[uction] is npm 6.x format and npm install --production is npm 8.x format. One difference is that in npm 8.x if you set NODE_ENV production npm 8.x will only install dependencies even if you run npm install but for npm6.x npm install --only will ignore NODE_ENV",
    "How to create index.html using dockerfile?": "The file already exists and is owned by root:\nRUN ls -al index.html\n ---> Running in 27f9d0ae6240\nlrwxrwxrwx 1 root root 25 Dec 23 12:08 index.html \n-> ../../doc/HTML/index.html\nremove it and re-create it:\nFROM centos:7\n#update and install nginx section\nRUN yum update -y\nRUN yum install -y epel-release\nRUN yum install -y nginx\nRUN yum install -y vim\n#create path and add index.html\nWORKDIR /usr/share/nginx/html\n\nRUN rm index.html\nRUN touch index.html\n\nEXPOSE 80/tcp\n\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\nDo note that you should, in general, combine multiple RUN commands together.",
    "H2 console not working on Docker (remote connections ('webAllowOthers') are disabled)": "i had the same problem. I found this answer after researching this problem.\nBefore starting jar in the Dockerfile, you can make it start by giving the following arguments. (\"-web -webAllowOthers -tcp -tcpAllowOthers -browser\")\nMy Dockerfile;\nFROM openjdk:8-jdk-alpine\nADD target/*.jar app.jar\nENTRYPOINT [\"java\", \"-jar\", \"/app.jar\", \"-web -webAllowOthers -tcp -tcpAllowOthers -browser\"]\nEXPOSE 8080\nIn this way, I hope there will be no problems.",
    "Can we control case sensitivity of contents in .dockerignore file?": "It seems like the .dockerignore file is case-insensitive on Windows and case-sensitive on Linux (and thus likely MacOS).\nFrom the Docker documentation:\nThe CLI interprets the .dockerignore file as a newline-separated list of patterns similar to the file globs of Unix shells. [...] Matching is done using Go\u2019s filepath.Match rules. A preprocessing step removes leading and trailing whitespace and eliminates . and .. elements using Go\u2019s filepath.Clean.\nIt seems like aforementioned functions make use of Go's glob, which is case-insensitive or case-sensitive depending on your operating system",
    "How to use maven local repository in Multi-stage docker build?": "The short answer is that you cannot. The only way you could use the maven repository during build would be to copy it inside the image in the first stage. But you cannot do that because normally the location of the maven repository is outside your build context. Of course you can change that for your project (place the .m2 folder in your current project) and then this approach might work. I am not recommending this approach, I am merely mentioning it as an option.\nHowever, I think you can solve your issue by following the best practice of not using Docker during development. While Docker is an awesome tool, it does slow down development. The build and push of the image should be delegated to your CI/CD pipeline (Jenkins, Gitlab CI, etc.). During day to day activities it is better and faster to just run your maven builds locally.",
    "Building rust project in docker causes Cargo to get stuck downloading dependencies": "So turns out the issue was with the COPY . . command, as it just copied everything into /, which was then attempted to be compiled (At least that's my belief).",
    "creating a separate docker-compose configuration for production and development": "The exact list would depend on your environment/ops team requirements, but this is what seems to be useful besides ports/existing volumes:\nNetworks\nThe default network might not work for your prod environment. As an example, your ops team might decide to put nginx/php-fpm/mariadb on different networks like in the following example (https://docs.docker.com/compose/networking/#specify-custom-networks) or even use a pre-existing network\nMysql configs\nThey usually reside in a separate dir i.e. /etc/my.cnf and /etc/my.cnf.d. These configs are likely to be different between prod/dev. Can\u2019t see it in your volumes paths\nPhp-fpm7\nHaven\u2019t worked with php-fpm7, but in php-fpm5 it also had a different folder with config files (/etc/php-fpm.conf and /etc/php-fpm.d) that is missing in your volumes. These files are also likely to differ once your handle even a moderate load (you\u2019ll need to configure number of workers/timeouts etc)\nNginx\nSame as for php-fpm, ssl settings/hostnames/domains configurations are likely to be different\nLogging\nThink on what logging driver might fit your needs best. From here:\nDocker includes multiple logging mechanisms to help you get information from running containers and services. These mechanisms are called logging drivers.\nYou can easily configure it in docker-compose, here's an example bring up a dedicated fluentd container for logging:\nversion: \"3\"\n\nservices:\n  randolog:\n    image: golang\n    command: go run /usr/src/randolog/main.go\n    volumes:\n      - ./randolog/:/usr/src/randolog/\n    logging:\n      driver: fluentd\n      options:\n        fluentd-address: \"localhost:24224\"\n        tag: \"docker.{{.ID}}\"\n\n  fluentd:\n    build:\n      context: ./fluentd/\n    ports:\n      - \"24224:24224\"\n      - \"24224:24224/udp\"",
    "How to use bash profile in dockerfile": "You shouldn't try to edit shell dotfiles like .bash_profile in a Dockerfile. There are many common paths that don't go via a shell (e.g., CMD [\"python\", \"myapp.py\"] won't launch any sort of shell and won't read a .bash_profile). If you need to globally set an environment variable in an image, use the Dockerfile ENV directive.\nFor a Python application, you should just install your application into the image's \"global\" Python using pip install. You don't specifically need a virtual environment; Docker provides a lot of the same isolation capabilities (something you pip install in a Dockerfile won't affect your host system's globally installed packages).\nA typical Python application Dockerfile (copied from https://hub.docker.com/_/python/) might look like\nFROM python:3\nWORKDIR /usr/src/app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY . .\nCMD [\"python\", \"./your-daemon-or-script.py\"]\nOn your last question, source is a vendor extension that only some shells provide; the POSIX standard doesn't require it and the default /bin/sh in Debian and Ubuntu doesn't provide it. In any case since environment variables get reset on every RUN command, RUN source ... (or more portably RUN . ...) is a no-op if nothing else happens in that RUN line.",
    "Bind .m2 file to docker on build stage": "You should mount the content of your project into the docker image and the $HOME/.m2/ into the image instead of copying everything into the image and building a new image..\nThe $PWD is the local directory where your pom.xml file is located and the src directory exists...\ndocker run -it --rm \\\n  -v \"$PWD\":/usr/src/mymaven \\ (1)\n  -v \"$HOME/.m2\":/root/.m2 \\ (2)\n  -v \"$PWD/target:/usr/src/mymaven/target\" \\ (3)\n  -w /usr/src/mymaven \\ (4)\n  maven:3.5-jdk-8-alpine \\ (5)\n  mvn clean package\ndefines the location of your working directory where pom.xml is located.\ndefines the location where you have located your local cache.\ndefines the target directory to map it into the image under the given path\ndefines the working directory.\ndefines the name of the image to be used.\nSo you don't need to create an new image to build your stuff with Maven. Simply run an existing image via the following command:\ndocker run -it --rm \\\n  -v \"$PWD\":/usr/src/mymaven \\\n  -v \"$HOME/.m2\":/root/.m2 \\\n  -v \"$PWD/target:/usr/src/mymaven/target\" \\ \n  -w /usr/src/mymaven \\\n  maven:3.5-jdk-8-alpine mvn clean package",
    "Printing output of shell script running inside a docker container": "Given a Docker image whose tag is $DOCKER_IMAGE:\ndocker container run -it --rm $DOCKER_IMAGE\n-i keeps STDIN open\n-t allocates a pseudo-TTY\n--rm automatically removes the container when it exits\nSee docker container run for all the options.",
    "Putting files in a Docker image": "#Base image\nFROM centos:latest\n\n#Update image\nRUN yum install -y git # and so on\n\nCOPY /home/username/lyve-SET-1.1.4f/ /lyve-SET-1.1.4f/\n\nENTRYPOINT <entrypoint of your image>\nCMD <arguments of your entrypoint>\nNow, if you build this image, everything will be executed except ENTRYPOINT and CMD\nThat means when you will build your image, your local data will be copied into your container. And it will be always in that image.\nNow push this image into registry.\nWhen user will pull this image, that image will contain data you have copied. No worries.\nIf you need to change your data, you need to build again and push.\nNote: In this way, you need to build & push every time you change code\nAnother option:\nIf you do not want to build & push every time you change code, you need to do copy-part in run-time.\nWrite a script that will clone git repository, when user will run docker.\nThar means, use this\nCOPY script.sh /script.sh\nENTRYPOINT [\"./script.sh\"]\nIn this script.sh shell, do you git clone and other things you need to do.\nIn this way, when user will run your image, they will always get the latest code you have in git repository",
    "Running Multiple python processes in Docker": "Docker docs has examples of how to do this. If you're using Python, then using supervisord is a good option.\nFROM ubuntu:latest\nRUN apt-get update && apt-get install -y supervisor\nRUN mkdir -p /var/log/supervisor\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\nCOPY my_first_process my_first_process\nCOPY my_second_process my_second_process\nCMD [\"/usr/bin/supervisord\"]\nThe advantage of this over running a bunch of background processes is you get better job control and processes that exit prematurely will be restarted.",
    "Is it possible to create container in multiple host using a single docker compose file?": "No, docker-compose alone won't achieve this. Managing containers across multiple hosts is generally the job of schedulers. The Docker Ecosystem: Scheduling and Orchestration is an older article, but should give an introduction.\nThe scheduler provided by Docker is called Swarm. Use Compose with Swarm is a good place to start to understand how to use them together.\nThis part of the answer is time limited, but during Mentor Week there are a few free courses you can take to learn more about Swarm. In your case Operations Beginner and Intermediate may be interesting.",
    "can't open fuse device in a docker container when mounting a davfs2 volume": "Thank's, successfully mounted by adding\n--privileged --cap-add=SYS_ADMIN --device /dev/fuse\nin to docker run command.",
    "Docker - how to automatically POST a request on a REST container after launch?": "You can create another container that wait for the the rabbitmq to expose its REST API and then perform the actions you want (Sending the HTTP POST in your case)\nHere is an example of the docker-compose file\nconnect:\n    build: kafka-connect\n    links:\n      - kafka\n    ports:\n      - \"8083:8083\"\n    environment:\n        CONNECT_BOOTSTRAP_SERVERS: kafka:9092\n        CONNECT_GROUP_ID: connect-cluster-A\n\nconnect-init:\n    build:\n        context: .\n    depends_on:\n        - connect\n    links:\n        - connect\n    command: [\"/wait-for-it.sh\", \"-t\", \"300\", \"connect:8083\", \"--\", \"/init.sh\"]\nHere is the Docker file for the init container. You can use this wait-for-it.sh file\nFROM bash\nRUN apk add --no-cache curl\nADD init.sh wait-for-it.sh /\nAnd the init.sh file contains your HTTP POST (You can add other commands to this file if you need)\ncurl -X POST \\\n    http://connect:8083/connectors \\\n    -d '{\"name\":\"my-project\",\"config\":{\"name\":\"my-project\",\"topics\":\"my-topic\",\"tasks.max\":2,\"connector.class\":\"the.package.to.my.connector.class\"}}'",
    "How to give Dockerfile input parameters from docker run command": "This has behaved this way because:\nThe RUN commands in the Dockerfile are executed when the Docker image is built (like almost all Dockerfile instructions) - ie. when you run docker build\nThe docker run command runs when the container is run from the image.\nSo when you run docker run and set the value to \"hifi\", the image already exists which has a directory called \"dx\" in it. The directory creation task has already been performed - updating the environment variable to \"hifi\" won't change it.\nYou cannot set a Dockerfile build variable at run time. The build has already happened.\nIncidentally, you're over-writing the value of the zk variable right before you create the directory. If you did successfully pass \"hifi\" into the docker build, it would be over-written and the folder would always be called \"dx\".",
    "Docker-compose volumes doesn't copy any files": "A volume is mounted in a container. The Dockerfile is used to create the image, and that image is used to make the container. What that means is a RUN ls inside your Dockerfile will show the filesystem before the volume is mounted. If you need these files to be part of the image for your build to complete, they shouldn't be in the volume and you'll need to copy them with the COPY command as you've described. If you simply want evidence that these files are mounted inside your running container, run a\ndocker exec $container_name ls -l /\nWhere $container_name will be something like ${folder_name}_app_1, which you'll see in a docker ps.",
    "Nightmare.js with Docker": "What you can do is to put all your files in a subdirectory, say app/ and in your Dockerfile do:\nADD app/ /usr/src/app/\nof course in your app folder there will be env.sh, package.json, tux.js and the lib directory\nThat way if you need to add more files, you wont have to add them manually in your dockerfile.\nPS: It works with COPY too",
    "Creating a testing infrastructure with Pytest , Selenium Grid and Docker": "I think you're almost there.\nI would add a pytest service to the docker-compose.yml, and remove the volumes_from and external_links from the hub image (so that it looks more like the example from the blog post you linked).\nThe pytest service will run the python code, but it should connect to hub to run selenium. I believe somewhere in the org_QA_folder there is configuration that tries to start up selenium locally. It needs to be reconfigured to use the hub server instead.",
    "How to sync the time of a java application running on docker container?": "Mapping localtime and timezone works perfectly.\nExample:\ndocker run -d -v /var/lib/elasticsearch:/var/lib/elasticsearch -v /etc/localtime:/etc/localtime:ro -v /usr/share/zoneinfo/America/Buenos_Aires:/etc/timezone:ro -p 80:80/tcp -p 9200:9200/tcp -p 514:514/udp petergrace/elk",
    "Error using mount command within Dockerfile": "Is there a way to be able to use mount with overlay while building the container?\nShort answer: No, and there won't be any time soon.\nIt seems the consensus is that any kind of privileged operation during build breaks the image portability contract, as it could potentially modify the host system. If a different system were to then pull and run such an image instead of building it from source, the host would be in an invalid state, from the perspective of the resultant container.\nRemember, docker build works by completing each step/layer in the Dockerfile using (loosely) these actions:\nRun the image of the last step/layer as a new container\nComplete the operations for the current step/layer within the container\nCommit the container (incl. new state) to a new image\nRepeat for any further steps\nTherefore, it's clearly possible for a privileged build operation to break out of the temporary container and touch the host in this scenario. No bueno.\nSo, what do?\nSolution 1 (baked-in mount)\nUPDATE \u2014 2015-10-24: Fail. See Solution 2 below for a working implementation.\nNote: YMMV depending upon Docker version, storage/graph driver, etc. Here's my docker info, for comparison:\nContainers: 12\nImages: 283\nStorage Driver: overlay\n Backing Filesystem: extfs\nExecution Driver: native-0.2\nLogging Driver: json-file\nKernel Version: 4.1.10-040110-generic\nOperating System: Ubuntu 15.04\nCPUs: 4\nTotal Memory: 7.598 GiB\nName: agthinkpad\nID: F6WH:LNV4:HH66:AHYY:OGNI:OTKN:UALY:RD52:R5L5:ZTGA:FYBT:SWA4\nWARNING: No swap limit support\nWell, I'm surprisingly finding it nigh impossible to bake the mount into an image via docker commit. The /proc filesystem, where a file representation of mount metadata is written by the kernel (more specifically /proc/self/mounts, from within a container), doesn't appear to be persisted by Docker at all. From what I can tell, anyway \u2014 /var/lib/docker/overlay/<container-root-lower>/root/proc is empty, and /var/lib/docker/overlay/<container-root-upper>/upper/proc doesn't exist.\nI thought /proc could be manipulated via a volume, and did indeed find some 2+ year old references to bind mounting /proc:/proc to achieve things that probably nobody should even be attempting (like this, possibly? \ud83d\ude09), but it appears this doesn't work at all anymore. Attemping to bind mount /proc to a host directory or even just make a volume of it is now a fatal error, even with docker run --privileged:\nCode: System error\n\nMessage: \"/var/lib/docker/overlay/c091a331f26bed12f22f19d73b139ab0c5b9971ea24aabbfad9c6482805984c9/merged/proc\"\n cannot be mounted because it is located inside \"/proc\"\n\nFrames:\n---\n0: setupRootfs\nPackage: github.com/opencontainers/runc/libcontainer\nFile: rootfs_linux.go@37\n---\n1: Init\nPackage: github.com/opencontainers/runc/libcontainer.(*linuxStandardInit)\nFile: standard_init_linux.go@52\n---\n2: StartInitialization\nPackage: Error response from daemon: Cannot start container c091a331f26bed12f22f19d73b139ab0c5b9971ea24aabbfad9c6482805984c9: [8] System error: \"/var/lib/docker/overlay/c091a331f26bed12f22f19d73b139ab0c5b9971ea24aabbfad9c6482805984c9/merged/proc\" cannot be mounted because it is located inside \"/proc\"\nIn terms of a method that doesn't require running mount \u2026 inside containers spawned from the image via a startup/entrypoint script, I'm really not sure where to go from this point. Because /proc/self/mounts is managed by the kernel, let alone not writable, this may never be possible. Hopefully I'm overlooking something and someone can point me in the right direction.\nIf you *must* do this during image compilation you can roll your own builder script by doing something like the following: \u2014 Optionally, first create a `Dockerfile` and use the stock builder: FROM mybaseimg|scratch COPY ./a /tmp/a RUN foo \u2026 `docker build -t mynewimg .` \u2014 Write a shell script using a combination of: `CID=$(docker create mynewimg)`, [`docker cp \u2026`](https://docs.docker.com/reference/commandline/cp/), `docker start $CID`, `docker exec|run \u2026 $CID \u2026`, `docker stop $CID`, `docker commit $CID mynewimg`, etc. *(Edit: Preferably, use the [API](https://docs.docker.com/reference/api/remote_api_client_libraries/)!)* When you need to apply a privileged operation, you can escalate the `docker run` command, but `--privileged` is total overkill here. If you *think* you shouldn't need `--privileged` for something you almost certainly *don't*. For an OverlayFS/AuFS mount you will need to initialize `docker run` with `--cap-add=[SYS_ADMIN]`, and for Ubuntu (and possibly other) hosts with `AppArmor`, you also need `--security-opt=[apparmor:unconfined]` (or preferably an AppArmor profile which relaxes the necessary restrictions, instead of `unconfined`). I'm not sure about `SELinux`.\nSolution 2 (runtime mounting)\nhost$uname -a\nLinux agthinkpad 4.1.10-040110-generic #201510030837 SMP Sat Oct 3 12:38:41 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\nhost$mkdir /tmp/overlay-test && cd /tmp/overlay-test\n./Dockerfile:\nFROM debian:jessie\n\nRUN apt-get update && apt-get install -y curl jq\n\nWORKDIR /usr/local/sbin\n\n# Locate and fetch the latest version of gosu\nRUN [\"/bin/bash\", \"-c\", \"curl -o ./gosu -sSL \\\"$( \\\n      curl -s https://api.github.com/repos/tianon/gosu/releases/latest \\\n      | jq --raw-output \\\n        '.assets[] | select(.name==\\\"gosu-'$(dpkg --print-architecture)'\\\") | .browser_download_url' \\\n    )\\\" && chmod +x ./gosu\"]\n\nCOPY ./entrypoint.sh ./entrypoint\nRUN chmod +x ./entrypoint\n\n# UPPERDIR and WORKDIR **MUST BE ON THE SAME FILESYSTEM**, so\n#  instead of creating a VOLUME for UPPERDIR we have to create a \n#  parent directory for both UPPERDIR and WORKDIR, and then make\n#  it the VOLUME. \nRUN [\"/bin/bash\", \"-c\", \"mkdir -p /var/overlay-test/{lower,upper/{data,work}} /mnt/overlay-test\"]\nVOLUME /var/overlay-test/upper\n\n# Create a file named FOO in the lower/root branch\nRUN touch /var/overlay-test/lower/FOO\n\nENTRYPOINT [\"entrypoint\"]\n./entrypoint.sh:\n#!/bin/bash\nset -e\n\ncd /var/overlay-test\nmount -t overlay -o lowerdir=lower,upperdir=upper/data,workdir=upper/work overlay /mnt/overlay-test\nchown -R \"$DUID\":\"$DGID\" ./\nchown root: ./upper/work\nchmod 0750 ./upper/work\n\ncd /mnt/overlay-test\nexec gosu \"$DUID\":\"$DGID\" $@\nhost$docker build -t overlay-test ./\nSuccessfully built 582352b90f53\nOK, let's test it!\nNote: Under an Ubuntu 15.04 host I had trouble deleting (overlay whiteout) any files that exist in the lowerdir via the mounted directory inside the container. This bug appears to be the culprit: https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1480411 \u2014 Edit: rm works all of a sudden and I can see the char file in upper/data, so I can only assume this is fixed and I received an updated package.\nhost$docker run -it --name=overlay-test --env=\"DUID=$(id -u)\" --env=\"DGID=$(id -g)\" --cap-add=SYS_ADMIN --security-opt=apparmor:unconfined overlay-test /bin/bash\noverlay-test$id\nuid=1000 gid=1000 groups=1000\noverlay-test$mount | grep '/mnt/overlay-test'\noverlay on /mnt/overlay-test type overlay (rw,relatime,lowerdir=lower,upperdir=upper/data,workdir=upper/work)\noverlay-test$pwd\n/mnt/overlay-test\noverlay-test$ls -Al | sed '/^t/d'\n-rw-r--r-- 1 1000 1000    0 Oct 24 03:54 FOO\noverlay-test$touch BAR\noverlay-test$ls -Al | sed '/^t/d'\n-rw-r--r-- 1 1000 1000    0 Oct 24 04:21 BAR\n-rw-r--r-- 1 1000 1000    0 Oct 24 03:54 FOO\noverlay-test$ls -Al /var/overlay-test/{lower/,upper/*} | sed '/^t/d'\nls: cannot open directory /var/overlay-test/upper/work: Permission denied\n\n/var/overlay-test/lower:\n-rw-r--r-- 1 1000 1000 0 Oct 24 03:54 FOO\n\n/var/overlay-test/upper/data:\n-rw-r--r-- 1 1000 1000 0 Oct 24 04:21 BAR\nSo far so good\u2026let's try importing the volume from another container:\noverlay-test$exit\nhost$docker run --rm --user=\"$(id -u):$(id -g)\" --volumes-from=overlay-test debian:jessie /bin/bash -c \"ls -Al /var/overlay-test/upper/* | sed '/^t/d'\"\nls: cannot open directory /var/overlay-test/upper/work: Permission denied\n\n/var/overlay-test/upper/data:\n-rw-r--r-- 1 1000 1000 0 Oct 24 05:32 BAR\nSuccess! Note that you could also RUN echo the mount spec >> /etc/fstab in your Dockerfile and then mount -a inside the entrypoint script, but this method was quirky, in my experience. I'm not sure why, but as there isn't a functional difference between the two methods, I didn't bother to investigate any further.\nClean-up: host$docker rm -v overlay-test && docker rmi overlay-test\nDocumentation for container runtime security in Docker:\nhttps://docs.docker.com/reference/run/#security-configuration https://docs.docker.com/reference/run/#runtime-privilege-linux-capabilities-and-lxc-configuration",
    "Docker automated build shows empty Dockerfile": "The Dockerfile on the hub gets updated after the build runs. Also, the pending for the build means it just hasn't run yet. Sometimes it takes a while. Do you have your github account Linked in the Settings section of the Docker repository? You can reach the status page to see if anything is broken here: https://status.docker.com/.\nBy the way, I have several projects on Docker hub, I went to one of them just now and clicked 'Build' (this isn't necessary if your github is linked, the auto build does that), and it is stuck in Pending. Sometimes it takes a LONG time.\nThere has been at least one outage at Docker registry in the last 3 months while I was actively using it. This might not be an outage, but, it could be :-( It could also be a big load on Docker hub.",
    "Docker: SvelteKit app cannot be accessed from browser": "Are you sure that sveltekit listens on port 5050?\nBecause when I start it up with npm run dev (vite dev) it usually takes port 5173 and if it is already used it counts 1 up until it reaches a free port.\nAdd in the package.json at the dev command a --port=5050.\nFull string:\n\"dev\": \"vite dev --port=5050\",",
    "\u201cUnable to load DLL 'SQLite.Interop.dll' or one of its dependencies\u201d in Docker, but locally it works": "We have experienced the issue updating from System.Data.SQLite.Core 1.0.113.6 to 1.0.117.\nLocally and in build pipeline everything works fine, but as soon as our app deployed in a docker container running mcr.microsoft.com/dotnet/runtime:6.0-nanoserver-1809 image - application throws the named exception Unable to load DLL 'SQLite.Interop.dll': The specified module could not be found.\nI found this thread at sqlite forum which clarified the situation a bit: https://sqlite.org/forum/info/0c471ca86ae7a836\nQuote from the forum:\nThis actually seems to be caused by some extra dependencies in the latest version of SQLite.Interop.dll.\nUsing dumpbin I compared the dependencies between version 1.0.113.7 and 1.0.114.4 and these new dependencies were added:\nmscoree.dll\n         1801473F0 Import Address Table\n         180186090 Import Name Table\n                 0 time date stamp\n                 0 Index of first forwarder reference\n\n                      6F StrongNameSignatureVerificationEx\n                      71 StrongNameTokenFromAssembly\n                      62 StrongNameFreeBuffer\n                       F CorBindToRuntimeEx\n                      61 StrongNameErrorInfo\n\nWINTRUST.dll\n         1801473E0 Import Address Table\n         180186080 Import Name Table\n                 0 time date stamp\n                 0 Index of first forwarder reference\n\n                      82 WinVerifyTrust \nmscoree.dll is not available in in the mcr.microsoft.com/dotnet/core/runtime image I was using. If I copy it in from a different image, SQLite.Interop.dll is loaded successfully\nWe ended up reverting back to version 1.0.113.6 as we didn't want to introduce extra complexity to docker image. I hope the issue with the latest System.Data.SQLite.Core will be resolved in one of the coming versions.",
    "Why does Docker gets stuck after running npm install?": "I solved it by updating docker and waiting :) . Yes, docker hangs up there but give it time it surely will move on and finish the process.",
    "Docker-compose with podman?": "The upcoming Podman 3.0 supports the Docker REST API well enough to be used as back-end for docker-compose. It is planned to be released in a few weeks (see Podman releases).\nCaveats:\nRunning Podman as root is supported, but not yet running as a normal user, i.e. running \"rootless\". (See feature request)\nFunctionality relating to Swarm is not supported\nTo enable Podman as the back-end for docker-compose, run\n sudo systemctl enable --now podman.socket\nPodman will then listen on the UNIX domain socket /var/run/docker.sock\nSee also: https://www.redhat.com/sysadmin/podman-docker-compose",
    "How to ensure that my docker image / container will still work 100 years from now?": "Well, yes. When you build a docker image it stores locally on your host machine. As your image already includes everything you need. You can store it in private docker registry and it will work even after 100 years",
    "Impact of yum install on Docker layer size": "Each RUN instruction creates new docker layer.\nDocker itself is not that smart enough to detect that instruction actually did nothing.\nIt faithfully stores new docker layer in resulting image.\nThat's why you need to try to minimize amount of docker instructions if possible.\nIn your case you can use just one RUN instructon:\nRUN yum -y install nano which && yum -y clean all && rm -fr /var/cache \nUPDATE\nLet's make an experiment:\nFROM centos\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\n10 RUN instructions, 9 of them \"doing nothing\".\nLet's build and look for intermediate images\n$ docker build .\n...\n$ docker images -a\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n<none>              <none>              fbd86aedc782        5 seconds ago       263MB\n<none>              <none>              ca70a4bbe722        7 seconds ago       261MB\n<none>              <none>              bd11e0ab02fb        9 seconds ago       259MB\n<none>              <none>              68c20ddfcaad        11 seconds ago      257MB\n<none>              <none>              314a6501ad23        13 seconds ago      255MB\n<none>              <none>              42a62294a5e7        16 seconds ago      253MB\n<none>              <none>              16fad39b9c27        18 seconds ago      251MB\n<none>              <none>              6769fe69c9e1        19 seconds ago      249MB\n<none>              <none>              49cef483e732        21 seconds ago      248MB\n<none>              <none>              c4c92c39f2a4        23 seconds ago      246MB\ncentos              latest              0d120b6ccaa8        3 weeks ago         215MB\nI see that each next docker image layer for \"doing nothing\" adds ~2Mb. (I don't know about ~24 Mb that was in OP question)\nUPDATE 2\nBy advice from emix: Using dive I immediately found files that was changed with every layer in /var/rpm and /var/log",
    "Docker input file and save in output": "Generally, the docker container cannot break out into the host machine.\nHowever, you can mount a local directory from the host machine into the container. The files created in the mount point, inside the container, will also be visible on the host machine.\nIn the example below I am mounting the working directory from the host machine inside the container. My current directory contains an input-file.\nThe container cats the content of the input-file and appends it to the output-file\n// The initial wiorking directory content\n.\n\u2514\u2500\u2500 input-file\n\n// Run my dummy container and ask it to cat the content of the input file into the output file\ndocker run -v $(pwd):/root/some-path ubuntu /bin/bash -c \"cat /root/some-path/input-file >> /root/some-path/output-file\"\n\n// The outcome\n.\n\u251c\u2500\u2500 input-file\n\u2514\u2500\u2500 output-file",
    "Passing a ENV variable for LABEL in Dockerfile": "LABEL sets image-level metadata, so it will be the same for all containers created from this image.\nIf you want to override a label for a container, you have to specify it using a special --label key:\ndocker run --label GROUP=mygroup nginx:test_label\nCurrently you're passing the environment variable to the container during running. The Dockerfile is processed on build stage. If you want to have this variable substituted in the image, you have to pass it on build:\ndocker build --build-arg GROUP=mygroup -t nginx:test_label .",
    "Hyperledger Indy data is not being mounted in Kubernetes volume directory": "Mounting in docker is consistent with standard behaviour of mounting on Linux. Linux mount command docs say\nThe previous contents (if any) and owner and mode of dir become invisible, and as long as this filesystem remains mounted\nThis is as well the way things work in Docker. If you mount a local directory, or an existing named docker volume, the content of filesystem in the container on the location of the mount will be shadowed (or we can call it \"overriden\").\nSimplified example of what is going on\nHaving dockerfile\nFROM alpine:3.9.6\n\nWORKDIR /home/root/greetings\nRUN echo \"hello world\" > /home/root/greetings/english.txt\nCMD sleep 60000\nAnd build it docker build -t greetings:1.0 .\nNow create following docker-compose.yml:\nversion: '3.7'\n\nservices:\n  greetings:\n    container_name: greetings\n    image: greetings:1.0\n    volumes:\n      - ./empty:/home/root/greetings\nand create empty directory empty next to it.\nStart it docker-compose up -d. While the container is running, let's get into container and see what the filestructure inside looks like. docker exec -ti greetings sh. Now when we are inside, if you run ls /home/root/greetings you'll see that the directory is empty - even though in the Dockerfile we have baked file /home/root/greetings/english.txt into the image's filesystem.\nNamed docker containers behave more desirably, if the named docker container is new and doesn't contain any data. If you mount such container on location in container where there already is some data, the named volume will get this data copied on it.\nYou can try this by adjusting the docker-compose.yml to this\nversion: '3.7'\n\nservices:\n  greetings:\n    container_name: greetings\n    image: greetings:1.0\n    volumes:\n      - greetingsvol:/home/root/greetings\n\nvolumes:\n  greetingsvol:\n    driver: local\nand if you repeat the procedure and exec yourself into the container, you'll see that file /home/root/greetings/english.txt is still there.\nThat's because when you cd yourself into /home/root/greetings, you are not looking at actual container's filesystem, but at mounted device - the name docker volume - which has been initialized by copy of container's original files on that given location. (Assuming docker volume greetingsvol did not previously exist.)\nSolution to your problem\nYou are mounting directory /var/kubeshare on your host to container's /var/lib/indy/sandbox. Let's see what the container stores on that location on startup (indypool is how I named built indy sandbox image on my localhost)\ndocker run --rm indypool ls /var/lib/indy/sandbox\ndomain_transactions_genesis\nkeys\npool_transactions_genesis\nSo if you mount your local directory onto /var/lib/indy/sandbox, it will shadow these files and the pool will fail to start up (and therefore consequently won't create files such as node1_additional_info.json etc).\nSo I think you have 2 options:\nUnless you have a good reason not to, use named docker volumes.\nCopy the original image data from container's /var/lib/indy/sandbox into your /var/kubeshare. Then you keep everything else as was. That way, the directory will be shadowed by new filesystem containing exactly the same data as the container expects to find there.",
    "Git not working in NanoServer (in Docker)": "First, you don't have to run the GIt for Windows setup.\nYou could also try and uncompress the Portable extractable archive PortableGit-2.26.0-64-bit.7z.exe anywhere you want in the image, and then add to the path:\nset GH=C:\\path\\to\\git\nset PATH=%GH%\\bin;%GH%\\usr\\bin;%GH%\\mingw64\\bin;%PATH%",
    "How to pass command line argument from oc start-build to dockerfile to set environment variable inside dockerfile": "You may try automate editing of the YAML manifest file with the BuildConfig (e.g. with the yaml python package) to add entries to the buildArgs array, which is located in the dockerStrategy definition of the BuildConfig. For example:\ndockerStrategy:\n...\n  buildArgs:\n    - name: \"foo\"\n      value: \"bar\"\nRefer to the relevant Openshift docs for more details.",
    "docker build --build-arg SSH_PRIVATE_KEY=\"$(cat ~/.ssh/id_rsa)\" returning empty": "I went ahead and used ONVAULT toool to handle the ssh keys. https://github.com/dockito/vault.\nAlso, I had misconfigured my .ssh/config file. The new file looks like this\nHost *\n  IgnoreUnknown AddKeysToAgent,UseKeychain\n  AddKeysToAgent yes\n  UseKeychain yes\n  IdentityFile ~/.ssh/id_rsa \nI hope it helps someone in future.",
    "Why dotnet command is not found in dockerfile?": "FROM ubuntu:16.04\nFROM microsoft/dotnet:2.2-sdk as build-env\nIn the above lines FROM ubuntu:16.04 will be totally ignored as there should be only one base image, so the last FROM will be considered as a base image which is FROM microsoft/dotnet:2.2-sdk not the ubuntu.\nSo if your base image is FROM microsoft/dotnet:2.2-sdk as build-env then why to bother to run these complex script to install dotnet?\nYou are good to go to check version of dotnet.\nFROM microsoft/dotnet:2.2-sdk as build-env\nRUN dotnet --version\noutput\nStep 1/6 : FROM microsoft/dotnet:2.2-sdk as build-env\n ---> f13ac9d68148\nStep 2/6 : RUN dotnet --version\n ---> Running in f1d34507c7f2\n\n> 2.2.402\n\nRemoving intermediate container f1d34507c7f2\n ---> 7fde8596c331",
    "Extending CouchDB Docker image": "Most of the standard Docker database images include a VOLUME line that prevents creating a derived image with prepopulated data. For the official couchdb image you can see the relevant line in its Dockerfile. Unlike the relational-database images, this image doesn\u2019t have any support for scripts that run at first startup.\nThat means you need to do the initialization from the host or from another container. If you can directly interact with it using its HTTP API, then this could look like:\n# Start the container\ndocker run -d -p 5984:5984 -v ... couchdb\n\n# Wait for it to be up\nfor i in $(seq 20); do\n  if curl -s http://localhost:5984 >/dev/null 2>&1; then\n    break\n  fi\n  sleep 1\ndone\n\n# Create the database\ncurl -XPUT http://localhost:5984/db",
    "Does exposing ports in a Dockerfile base image, expose them in a final image?": "The EXPOSE directive seems to be for documentation purposes mostly (only?).\nFrom the documentation:\nThe EXPOSE instruction does not actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published\nSo I do not see any reason to declare this anywhere other than the final step / Dockerfile.\nThat said, any EXPOSE directive in a parent image - either multi-stage or not - will be reflected in the metadata of any subsequent child image.\nExample:\n# Dockerfile\nFROM scratch AS base\nEXPOSE 80\n\nFROM base\nENV HELLO world\nThen run:\n$ docker build -t temp .\n$ docker image inspect temp\nWhich will output (among other things):\n\"ExposedPorts\": {\n  \"80/tcp\": {}\n},",
    "WordPress image with pre-installed plugins using Dockerfile": "So this is what's happening:\nWhen you are building your custom image, you add the plugin folder /var/www/html/wp-content/plugins/preferred-languages/ and that works just fine.\nYou can test that by simply running docker run -it --rm arslanliaqat/wordpresswithplugin sh and cd /var/www/html/wp-content/plugins and you should see the folder.\nThe reason the folder is missing when you are using your docker-compose.yml file is because you are mounting the volume \"over\" the folder that's already there. Try removing the volumes declaration from wp service in the docker-compose.yml file and then you should be able to see your plugin folder.\nI would suggest you use the wordpress:php7.1-apache for your wp service and mount your plugin folder the same way you are mounting wordpress\nExample:\nversion: '3.3'\nservices:\n  wp:\n    image: \"wordpress:php7.1-apache\"\n    volumes:\n      - './wordpress:/var/www/html'\n      - './preferred-languages:/var/www/html/wp-content/plugins/preferred-languages'\n    ports:\n      - \"8000:80\"\n    environment:\n      WORDPRESS_DB_PASSWORD: qwerty\n  mysql:\n    image: \"mysql:5.7\"\n    environment:\n      MYSQL_ROOT_PASSWORD: qwerty\n    volumes:  \n      - \"my-datavolume:/var/lib/mysql\"\nvolumes: \n  my-datavolume:\nIs there a specific reason you need the plugin to be in the image already?\nUPDATED\nI created a simple gist which should accomplish what you want to do. The entrypoint lacks checks for already existing theme/plugin directories etc, but this should serve as POC\nhttps://gist.github.com/karlisabe/16c0ccc52bdf34bee5f201ac7a0c45f7",
    "Docker: Pulling an image by digest, that internally uses a tag": "You're correct that an image pulled by digest is effectively (!) unchangeable.\nThe image digest is a SHA-256 hash computed from the layers that constitute the image. As a result it's highly improbable that a different image would share the same digest.\nOnce created an image's layers don't change. So even if the FROM image were changed, your existing images would not be changed by it.\nHowever, if you rebuilt your images using the new (same-tagged) FROM image, your image's digest would change and this would be a signal to you that's something has changed.\nIt is possible (and a good practice) to use digests in FROM statements too (for the reasons you cite) but few developers do this. You may wish to ensure your Dockerfiles use digests in FROM statements to ensure you're always using the same image sources.\nHowever, it's turtles all the way down (or up) though and so you are recursively delegating trust to images from which yours are derived all the way up to SCRATCH.\nThis is one reason why image vulnerability tools are recommended.\nI explored this for my own education recently:\nhttps://medium.com/google-cloud/adventures-w-docker-manifests-78f255d662ff",
    "Alternative to using --squash when building docker images on Windows using local files": "We ended up setting up nginx to provide files when building. On our build server, the machine building our docker images and the server that has the installer files have a very good connection between them, so downloading huge files is not a real problem.\nWhen it comes to --squash, it is bugged for Docker on Windows. Here is the relevant issue for it:\nhttps://github.com/moby/moby/issues/31468\nThere is an issue to move --squash out of experimental, but it doesn't seem to have a lot of support:\nhttps://github.com/moby/moby/issues/38657\nThe alternative that some people propose instead of --squash is multi stage build, discussion here:\nhttps://github.com/moby/moby/issues/34565\nThere is an alternative to --squash, if you have local installer files, you don't want to set up a web server, and you would like your docker image to be small, and you are running Windows: Use mapped drives.\nIn Windows, you can share folders with other users on your network. Docker containers are like another computer that is running on your physical machine, and it can access these network drives.\nFirst set up a new user, for example username share and password password1. Create a folder somewhere on your computer. Then right click it, click properties, and then go to the Sharing tab and click \"Share\". Find the user that you have just created, using the little dropdown menu and Find people ..., and share the folder with this user.\nCreate a folder somewhere for your test project. Create a batch file setupshare.bat that looks like this:\n@echo off\nfor /f \"tokens=2 delims=:\" %%i in ('ipconfig ^| findstr \"Default Gateway\"') do (\n    set hostip=%%i\n    goto :end\n)\n:end\nset hostip=%hostip: =%\nnet use O: \\\\%hostip%\\vms /USER:share password1\nThe first part of this file is only to find the ip address that the docker container can use to access its host computer. It is not the most pretty thing I've ever put together, so let me know if there's a better way!\nIt uses a for-loop, as that is the way to save the output of a command to a variable in batch files. The command is ipconfig, and we pipe it to findstr and searches for Default Gateway. We need to use ^| instead of just | because it is in a for-loop. The first part of the for-loop divides each line from the command on the delimiter, which is : in this case, and we only take the second token. The for-loop only handles the first line, if there are multiple entries with a Default Gateway. This script doesn't work if there are multiple entries and the first one is not the correct one.\nThe line set hostip=%hostip: =% is to remove a space at the start of the string.\nWe then have the IP address that we want to use stored in hostip. We use this in the net use command, which will map O:\\ to shared folder vms on the machine with IP hostip. We use the username share and the password password1. Note that this is a very bad way of handling passwords, as they kind of should be secret!\nWith a batch file like this, we can set up a Dockerfile in this way:\n# escape=`\nFROM mcr.microsoft.com/dotnet/core/sdk:3.0\n\nCOPY setupshare.bat .\n\nRUN setupshare.bat && `\n    copy O:\\file.txt file.txt\nThe RUN command will first call setupshare.bat that sets up the network share properly. We can then use any file that we shared, for example a huge installer, and install the things that we want. In this case I have only shared a test file file.txt to see that it works, so just change that line.\nI would still advice everyone to just set up a little web server, for example nginx, and use the standard way of writing Dockerfiles, with downloading files and running it in the same RUN command. That's what people expect when they see a Dockerfile, and it should be a more robust solution.\nWe can also hope that the Docker people either makes a COPY command that can copy, run, and delete installers in the same layer, or that --squash is implemented properly.",
    "'vc_redist.x64 does not install in microsoft/nanoserver image": "Visual C++ Redistributable Packages cannot be installed in NanoServer. However, you can use the binary dlls manually. The redistributable files are installed with Visual Studio.\nSteps:\nOpen Visual Studio Installer, make sure you check Desktop Development with C++. At right details panel, check all versions you want to install:\nMSVC v143 - VS 2022\nMSVC v142 - VS 2019\nMSVC v141 - VS 2017\nMSVC v140 - VS 2015\nFind your VS installation folder, for example, VS2022 should be like C:\\Program Files\\Microsoft Visual Studio\\2022\\Community and VS2019 should be C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community. Then go to the CRT folder. I have installed VS2022 and want to use MSVC v142, so the full path should be: C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Redist\\MSVC\\14.29.30133\\onecore\\x64\\Microsoft.VC142.CRT\nCopy everything under this folder to your application's local folder, or C:\\Windows\\System32 in your NanoServer image.",
    "Can't recreate Conda environment in docker": "I had a similar problem and I find multiple ways to solve it. The main problem with your approach is conda is not platform independent, so will force the environments to use pip.\n1. Conda Like Solution\nChange your my_env.yml so that all the dependencies apart from pip goes under the pip dependency. Notice that the syntax is different when you move under the pip.\nFor instance:\nname: myenv\nchannels:\n  - defaults\ndependencies:\n   - pip=18.1\n   - pip:\n     - wheel==0.32.3\nThen go to your Dockerfile and add the following line:\nRUN conda env update -n base --file myenv.yml\n2. Good old Pip way\nExport your conda environment into a pip requirements file as at this answer\nconda install pip\npip freeze > requirements.txt\nThen go to your Docker file and add the following line:\nRUN python -m pip install -r requirements.txt",
    "Specify \"privileged\" container mode in dockerfile?": "You can't give privileged mode in Dockerfile. You can only run by --privileged when start docker by command line. There is one other way, that you can try start you docker container via Docker API\nAnd set request param for auto run with privileged mode.\nAs I know, normal case you need to run docker in privileged mode is you wanna run docker in docker. What BIRD container is ?",
    "Docker-compose Error : cannot restrict inter-container communication": "Per WARNING message the bridge-nf-call-iptables is disabled, run code below to address that warning:\nsudo sysctl net.bridge.bridge-nf-call-iptables=1\nsudo sysctl net.bridge.bridge-nf-call-ip6tables=1\nAlso, make sure the br_netfilter module is enabled, to do that run command below and make sure br_netfilter is listed in the linux.kernel_modules:\n lxc profile show docker\nIf it's not listed, copy all values listed in the linux.kernel_modules and add ,br_netfilter to the end of the copied value, than put all together in the command below instead of <[COPIED_LIST]>:\nlxc profile set docker linux.kernel_modules <[COPIED_LIST]>",
    "Is there a way to use If condition in Dockefile?": "Docker 17.05 and later supports a kind of conditionals using multi-stage build and build args. Have a look at https://medium.com/@tonistiigi/advanced-multi-stage-build-patterns-6f741b852fae\nFrom the blog post:\nARG BUILD_VERSION=1\nFROM alpine AS base\nRUN ...\nFROM base AS branch-version-1\nRUN touch version1\nFROM base AS branch-version-2\nRUN touch version2\nFROM branch-version-${BUILD_VERSION} AS after-condition\nFROM after-condition \nRUN ...\nAnd then use docker build --build-arg BUILD_VERSION=value ...",
    "Docker Returned a non-zero code 100": "I encountered the same problem. It solved by adding\nUSER root\nbefore running any Linux commands.\nP.S: Seems to be an old question but I thought it may help somebody if put it as an answer.",
    "How to track file changes within a Docker Container": "Check\ninotify docker image https://github.com/pstauffer/docker-inotify\nor\nhttps://hub.docker.com/r/coppit/inotify-command/\nor\nhttps://hub.docker.com/r/coppit/inotify-command/~/dockerfile/",
    "Unable to create node_modules folder at host and mount host folder to container": "starting fresh..\nThe following is, I believe, what you are asking for, but I'm not sure it's what you want.\nWhat is the actual use case you are trying to address?\nIf it is to be able to develop your app, modifying files on your local system and have them reflected in the docker container this is not the way to do it..\nI have used the npm module serve as an easy way to keep the container running.\nI have used a named volume to make things easier. The volume data is defined in docker-compose and mounted on /var/www/app.\nfyi, you could also try moving the VOLUME instruction in your original Dockerfile to the bottom as at https://docs.docker.com/engine/reference/builder/#volume:\nChanging the volume from within the Dockerfile: If any build steps change the data within the volume after it has been declared, those changes will be discarded.\nBut this still only defines the volume mount point in the container, you still need to mount it on the host at run time.\nApp files:\n$ tree\n.\n\u251c\u2500\u2500 app\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 package.json\n\u2514\u2500\u2500 docker-compose.yml\n\n1 directory, 3 files\n\n$ cat app/Dockerfile\nFROM node:6.3\nRUN mkdir -p /var/www/app\nWORKDIR /var/www/app\nCOPY . /var/www/app\nRUN npm install\n\n$ cat app/package.json\n{\n  \"name\": \"app\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"start\": \"serve\"\n  },\n  \"dependencies\": {\n    \"serve\": \"*\"\n  }\n}\n\n$ cat docker-compose.yml\nversion: \"2\"\n\nservices:\n  app:\n    build: ./app\n    ports:\n      - \"3000\"\n    command: npm start\n    volumes:\n      - data:/var/www/app\nvolumes:\n  data:\nBuild the app:\n$ docker-compose build app\nBuilding app\nStep 1/5 : FROM node:6.3\n---> 0d9089853221\nStep 2/5 : RUN mkdir -p /var/www/app\n---> Using cache\n---> 18cc43628367\nStep 3/5 : WORKDIR /var/www/app\n---> Using cache\n---> b8fa2b1a2624\nStep 4/5 : COPY . /var/www/app\n---> de38a6c3f784\nStep 5/5 : RUN npm install\n---> Running in 5a035f687de1\nnpm info it worked if it ends with ok\nnpm info using npm@3.10.3\nnpm info using node@v6.3.1\nnpm info attempt registry request try #1 at 1:19:21 PM\nnpm http request GET https://registry.npmjs.org/serve\n...\nnpm info ok\n---> 76b99c707ac1\nRemoving intermediate container 5a035f687de1\nSuccessfully built 76b99c707ac1\nSuccessfully tagged 47173020_app:latest\nBring up the app\n$ docker-compose up -d app\nRecreating 47173020_app_1 ...\nRecreating 47173020_app_1 ... done\nCheck the app logs\n$ docker-compose logs app\nAttaching to 47173020_app_1\napp_1  | npm info it worked if it ends with ok\napp_1  | npm info using npm@3.10.3\napp_1  | npm info using node@v6.3.1\napp_1  | npm info lifecycle app@0.0.0~prestart: app@0.0.0\napp_1  | npm info lifecycle app@0.0.0~start: app@0.0.0\napp_1  |\napp_1  | > app@0.0.0 start /var/www/app\napp_1  | > serve\napp_1  |\napp_1  | serve: Running on port 5000\nNow the volume you are looking for on the host is on the docker host running as a vm on your local machine and can only be accessed from a running container.\nI have taken the following from Inspecting Docker Volumes on a Mac/Windows the Easy Way which you can refer to for more detail.\nList the docker volumes\n$ docker volume ls\nlocal               47173020_data\nInspect the volume to get it's mount point\n$ docker volume inspect 47173020_data\n[\n    {\n        \"CreatedAt\": \"2017-11-13T13:15:59Z\",\n        \"Driver\": \"local\",\n        \"Labels\": null,\n        \"Mountpoint\": \"/var/lib/docker/volumes/47173020_data/_data\",\n        \"Name\": \"47173020_data\",\n        \"Options\": {},\n        \"Scope\": \"local\"\n    }\n]\nStart up a simple container, mount \"/docker\" on the host root, and list the files in the volume which are the ones copied and created by the Dockerfile\n$ docker run --rm -it -v /:/docker alpine:edge ls -l /docker/var/lib/docker/volumes/47173020_data/_data\ntotal 12\n-rw-r--r--    1 root     root           119 Nov 13 12:30 Dockerfile\ndrwxr-xr-x  153 root     root          4096 Nov 13 13:14 node_modules\n-rw-r--r--    1 root     root           144 Nov 13 13:03 package.json",
    "Why httpd container exits immediately without any error in Docker": "So your Webserver is not startet as a foreground process, that is why the container is stopped immediately.\nI think you should change\nENTRYPOINT [\"apache2ctl\", \"-D\", \"FOREGROUND\"]\nto\nCMD [\"apache2ctl\", \"-D\", \"FOREGROUND\"]\nBecause you want that command to run when the container is mounted.\nThe ENTRYPOINT directive declares the executable which is used to execute the CMD.\nENTRYPOINT [\"apache2ctl\", \"-D\", \"FOREGROUND\"]\nresults in:\n$  apache2ctl -D FOREGROUND <command either from the run command line or the CMD directive>\nThe default command from Ubuntu image I could find is /bin/bash. So when you run the container using:\ndocker run -it mg:httpd\nthe resulting command that is executed will be:\n$  apache2ctl -D FOREGROUND /bin/bash\nwhat obviously does not make much sense. I would recommend to go over that post, it explains the details very well.\nAfter the change described above it will look like that:\n(default) ENTRYPOINT [\"/bin/sh\", \"-c\"]\nCMD [\"apache2ctl\", \"-D\", \"FOREGROUND\"]\nwhat results in:\n$  /bin/sh -c \"apache2ctl -D FOREGROUND\"",
    "Docker secret not working for MYSQL_ROOT_PASSWORD_FILE": "I had the same issue running linux containers on a windows machine when I created the secret inline.\necho mypassword | docker secret create mysql_db_root -\nThe actual password gets saved with a space, carriage return, and new line at the end. This can be seen if you look at the bytes in the file. In the running mysql container you can use the od command like the following example below to view the bytes. Or outside the container you can dump the /run/secrets/mysql_db_root content to a file and view with your favorite hex editor on windows to see the added bytes.\nod -xa /run/secrets/mysql_db_root\nTo resolve the issue create the secret by putting it in a file and then using the file to generate the docker secret. See the mysql github issue for more details.\ndocker secret create mysql_db_root mypasswordfile.txt",
    "Docker: how to use SQL file in Directory": "Just add a volume mapping to map a local folder to the /docker-entrypoint-initdb.d container folder, for example : ./init-db:/docker-entrypoint-initdb.d. This file will be loaded on the first container startup.\nConsidering the docker-compose.yml bellow :\ndrop your sql files into /path-to-sql-files-on-your-host host folder)\nrun docker-compose down -v to destroy containers and volumes\nrun docker-compose up to recreate them.\n-\nversion: '3'\n\nservices:\n   db:\n     image: mysql:5.7\n     volumes:\n       - db_data:/var/lib/mysql\n       - /path-to-sql-files-on-your-host:/docker-entrypoint-initdb.d\n     restart: always\n     environment:\n       MYSQL_ROOT_PASSWORD: somewordpress\n       MYSQL_DATABASE: wordpress\n       MYSQL_USER: wordpress\n       MYSQL_PASSWORD: wordpress\n\n   wordpress:\n     depends_on:\n       - db\n     image: wordpress:latest\n     ports:\n       - \"8000:80\"\n     restart: always\n     environment:\n       WORDPRESS_DB_HOST: db:3306\n       WORDPRESS_DB_USER: wordpress\n       WORDPRESS_DB_PASSWORD: wordpress\nvolumes:\n    db_data:",
    "Docker: Unable to correct problems, you have held broken packages": "Debian is setup to only allow one mail transport agent, and your install command is trying to include two, ssmtp and sendmail/sendmail-bin. Since they conflict with each other, you'll need to remove one of these from your install command.",
    "Copying a directory into a docker image while skipping given sub directories": "I considered the method explained by @nwinkler and added few steps to make the build consistent.\nNow my context directory structure is as follows\n-Dockerfile\n-MyApp\n  -libs\n  -classes\n  -resources\n-.dockerignore\n-libs\nI copied the libs directory to the outer of the MyApp directory. Added a .dockerignore file which contains following line\nMyApp/libs/*\nUpdated the Dockerfile as this\nADD libs /opt/MyApp/libs\n## do other operations\nADD MyApp /opt/MyApp\nBecause dockerignore file ignores MyApp/lib directory, there is no risk in over-writing libs directory I have copied earlier.",
    "Docker-compose failing to run a jar file but works with Dockerfile": "You have two syntax errors in your docker-compose.yml file:\nDocker-Compose does not support the add command. If you want to add a file to your container, you either have to do that in a Dockerfile (and use that from the compose file), or map in the file through a volume.\n  volumes:\n    - \"./farr-api-0.1.0.jar:/app.jar\"\nThe environment section expects an array - you need to prefix the JAVA_OPTS line with a dash:\n  environment:\n    - JAVA_OPTS=\"\"\nYou can find more details in the Docker-Compose documentation",
    "Dockerfile build /bin/sh -c returned a non-zero code: 1": "I solved my problem ! Thanks for all, who tried to help me ! After each command (RUN, CMD and else) Docker makes container, save changes on docker image and delete container before next command.Docker also compress directories and files each command iteration. You should know it before do anything, if you don't want to get an exception or error..\nThis is working code :\nFROM ubuntu:16.04\nMAINTAINER S.K.\nRUN apt-get update\nRUN apt-get install curl -y\nRUN curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.32.1/install.sh | bash \\\n&& export NVM_DIR=\"$HOME/.nvm\" && [ -s \"$NVM_DIR/nvm.sh\" ] && . \"$NVM_DIR/nvm.sh\" \\\n&& nvm install 6.9.1 \\\n&& npm i express -g \\\n&& npm i nunjucks -g \\\n&& npm i nodemon -g \\\n&& npm i gulp -g \\\nRUN mkdir -p ./PROJECT\nEXPOSE 1520",
    "Unable to delete some untagged docker images due to conflict": "Make sure the image is actually dangling (meaning it is not referenced by any other image, or is not parent of an image)\ndocker images --filter \"dangling=true\" -q --no-trunc\nIf it is dangling (and should be removed), then there is a couple of pending bug reporting the impossibility to delete such images: issue 13625, issue 12487.",
    "Using docker-compose Mysql + App": "This is a known issue and is yet to be addresses. Check out this link Docker services need to wait\nYou are absolutely right that the MySql service is not yet up while your App container comes up very quickly and hence when it tries to connect to MySql, the connection is refused. On the application side what you can do is that you could write some retry logic. Something on these lines\nwhile(numberOfRetries > 0) {\n\ntry {\n\n  // Try to connect to MySql\n} catch(Exception e) {\n\n     numberOfRetries--;\n\n     if(numberOfRetries == 0)\n       // log the exception \n}\nHope this helps.",
    "Docker CMD weirdness when ENTRYPOINT is a shell script": "Note that the default entry point/cmd for an official centos 6 image is:\nno entrypoint\nonly CMD [\"/bin/bash\"]\nIf you are using the -c command, you need to pass one argument (which is the full command): \"echo foo\".\nNot a series of arguments (CMD [\"echo\", \"foo\"]).\nAs stated in dockerfile CMD section:\nIf you use the shell form of the CMD, then the <command> will execute in /bin/sh -c:\nFROM ubuntu\nCMD echo \"This is a test.\" | wc -\nIf you want to run your <command> without a shell then you must express the command as a JSON array and give the full path to the executable\nSince echo is a built-in command in the bash and C shells, the shell form here is preferable.",
    "How do I add big HTTP files in a Dockerfile and exclude them from image layers?": "I accepted Mykola Gurovs answer because in one of his comments he pointed out an idea that helped me to solve this issue.\nHere is what I did to have proper caching and cache invalidation as well as having the big installer file excluded:\nFROM debian:jessie\n...\nRUN apt-get install -y curl\nENV PROJECT_VERSION x.y.z-SNAPSHOT\n...\nADD http://nexus:8081/service/local/artifact/maven/resolve?r=public&g=my.group.id&a=installer&v=${PROJECT_VERSION}&e=sh&c=linux64 ${INSTALL_DIR}/installer.xml\nRUN curl --silent \"http://nexus:8081/service/local/artifact/maven/content?r=public&g=my.group.id&a=installer&v=${PROJECT_VERSION}&e=sh&c=linux64\" \\\n        --output ${INSTALL_DIR}/installer.sh \\\n    && sh ${INSTALL_DIR}/installer.sh <someArgs> \\\n    && rm ${INSTALL_DIR}/installer.sh\n...\nThe first ADD downloads the Maven metadata for the requested artifact. That XML file is quite small. It uses proper caching so whenever the metadata on the Nexus has been modified the cache gets invalidated.\nThe ADD and all its following instructions are executed without re-using any cached versions in that case.\nIf the metadata on the server did not change since the last download the ADD and the following RUN instruction which executes curl are taken from the image layer cache. And in the RUN it is possible to download, execute and remove the temporary big installer file in one step without having it stored in any image layers.",
    "Unable to run docker image created from ISO": "You're building an image from ubuntu's iso but what you really want is building an image from an ubuntu install.\nWhat you have to do is :\ninstall ubuntu from the iso inside a VM\nmake an archive of the VM's / (this can be done with cd / && tar -cvpzf backup.tar.gz --exclude=/backup.tar.gz --one-file-system /)\nimport with docker (this can be done with docker import /path/to/your/backup.tar.gz)\nI hope that helps",
    "Docker will not attach to an image": "Run docker ps -a to show all containers, not just running containers. You'll see docker_sa_1 listed as a stopped container. This is because it crashed immediately on start-up. Unfortunately, fig doesn't show the logs for you (or shut down the stack automatically) when this happens.\nRun docker logs docker_sa_1 to see the output. Hopefully, there will be a nice Nginx error message for you. If you can't find anything, then remove the sa entry from your fig.yml, do a fig up to get everything else started, then run\ndocker run -it --link=docker_php_1:php-fpm -v $PWD/svn:(?) -v $PWD/cert:(?) -p 8080:80 nginx\n(You'll need to fill in the ?s with the path bits you left out) This is equivalent to what Fig's doing, except we're starting the container interactive with an attached tty instead of attaching later. If you still can't get any error messages, run\ndocker run -it --link=docker_php_1:php-fpm -v $PWD/svn:(?) -v $PWD/cert:(?) -p 8080:80 nginx /bin/bash\nto get a live shell on the container. Then try starting Nginx yourself and dig for log files after it crashes.",
    "Docker Expose ports dynamically": "With Weave network for Docker any port your application might open would be accessible from inside the network with no external intervention, unlike the aforementioned ambassador patter. However, those would be only accessible from the subnet the application is on. You the ports you statically expose will remain NATed by Docker too and would be externally accessible, but ephemeral once would be internal-only.",
    "Curl error (6) on amazonlinux docker container during `yum install`, but no issues with other image": "I'm betting that you're running on a machine where Docker runs with \"old\" seccomp settings. See this thread for https://github.com/amazonlinux/amazon-linux-2023/issues/80#issuecomment-1017798237 discussion and options.",
    "Docker compose fails with \"failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount[...]/Dockerfile: no such file or directory\"": "I can definitely reproduce this having a an empty php folder, so missing the Dockerfile, with the following minimal example.\nFile hierarchy:\n.\n\u251c\u2500\u2500 docker-compose.yml\n\u2514\u2500\u2500 php\n##  ^-- mind this is an empty folder, not a file\nAnd the minimal docker-compose.yml:\nversion: \"3.9\"\nservices:\n  php-apache-environment:\n    container_name: php-apache\n    build: ./php\nRunning docker compose up yields the same error as yours:\nfailed to solve: rpc error: code = Unknown desc = failed to solve with frontend dockerfile.v0: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount2757070869/Dockerfile: no such file or directory\nSo, if you create a Dockerfile in the php folder, e.g.:\n.\n\u251c\u2500\u2500 docker-compose.yml\n\u2514\u2500\u2500 php\n    \u2514\u2500\u2500 Dockerfile\nWith a content like\nFROM php:fpm\nThen the service starts working:\n$ docker compose up    \n[+] Running 1/0\n \u283f Container php-apache  Created                               0.1s\nAttaching to php-apache\nphp-apache  | [14-Apr-2023 08:42:10] NOTICE: fpm is running, pid 1\nphp-apache  | [14-Apr-2023 08:42:10] NOTICE: ready to handle connections\nAnd if your file describing the image inside the folder php has a different name than the standard one, which is Dockerfile, then you have to adapt your docker-compose.yml, using the object form of the build parameter:\nversion: \"3.9\"\nservices:\n  php-apache-environment:\n    container_name: php-apache\n    build: \n      context: ./php\n      dockerfile: Dockefile.dev # for example\nRelated documentation: https://docs.docker.com/compose/compose-file/build/#build-definition",
    "error: failed to solve: openjdk:13: failed to do request: Head \"https://registry-1.docker.io/v2/library/openjdk/manifests/13\": x509": "This is coming months late and I assume that you have figured it out already but for the sake of someone else that has the same issue, I'll share what worked for me.\nFirst thing I did was to check the name of the container for the buildkit:build-stable-1 image, as this was automatically generated for me:\ndocker ps\nNext, stop the container:\ndocker stop buildx_buildkit_trusting_moore0\nFinally, remove the container:\ndocker rm buildx_buildkit_trusting_moore0\nAfterwards, I started the build process. I did encounter this issue a few times but stopping and removing the container always worked. I am relatively new to docker so I am not sure why this happens, but I hope this helps someone.",
    "Build a image with dotnet core 6 using dockerfile get an error NETSDK1064": "Removing --no-restore from the dotnet publish should do the trick.\nFrom the docs https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-publish:\nYou don't have to run dotnet restore because it's run implicitly by all commands that require a restore to occur, such as dotnet new, dotnet build, dotnet run, dotnet test, dotnet publish, and dotnet pack. To disable implicit restore, use the --no-restore option",
    "Docker build fails due to kafka undefined error": "So somehow this seems to depend on cgo when building. I managed to work around this by compiling with cgo, but linking the dependencies statically.\nFROM golang AS builder\n\nWORKDIR /app\nCOPY . .\nRUN go mod download && \\\n    go build -o myapp -ldflags '-linkmode external -w -extldflags \"-static\"'\n\n\n# works also with alpine\nFROM busybox \n\nCOPY --from=builder /app/myapp myapp\nCMD [\"./myapp\"]\nI have used the code below, from their repo, to test. Which gave me initially the kind of error you have shown.\npackage main\n\nimport (\n    \"fmt\"\n\n    \"github.com/confluentinc/confluent-kafka-go/kafka\"\n)\n\nfunc main() {\n\n    p, err := kafka.NewProducer(&kafka.ConfigMap{\"bootstrap.servers\": \"localhost\"})\n    if err != nil {\n        panic(err)\n    }\n\n    defer p.Close()\n\n    // Delivery report handler for produced messages\n    go func() {\n        for e := range p.Events() {\n            switch ev := e.(type) {\n            case *kafka.Message:\n                if ev.TopicPartition.Error != nil {\n                    fmt.Printf(\"Delivery failed: %v\\n\", ev.TopicPartition)\n                } else {\n                    fmt.Printf(\"Delivered message to %v\\n\", ev.TopicPartition)\n                }\n            }\n        }\n    }()\n\n    // Produce messages to topic (asynchronously)\n    topic := \"myTopic\"\n    for _, word := range []string{\"Welcome\", \"to\", \"the\", \"Confluent\", \"Kafka\", \"Golang\", \"client\"} {\n        p.Produce(&kafka.Message{\n            TopicPartition: kafka.TopicPartition{Topic: &topic, Partition: kafka.PartitionAny},\n            Value:          []byte(word),\n        }, nil)\n    }\n\n    // Wait for message deliveries before shutting down\n    p.Flush(15 * 1000)\n}",
    "Golang docker file for debug": "You need to compile dlv itself with the static linking flags. Without that, dlv will have dynamic links to libc which doesn't exist within an alpine image. Other options include switching your production image to be debian based (FROM debian) or change to golang image to be alpine based (FROM golang:1.14.7-alpine). To compile dlv without dynamic links, the following Dockerfile works:\nFROM golang:1.14.7 AS builder\nRUN CGO_ENABLED=0 go get -ldflags '-s -w -extldflags -static' github.com/go-delve/delve/cmd/dlv\nRUN mkdir /app\n\nADD . /app\nWORKDIR /app\nRUN CGO_ENABLED=0 GOOS=linux go build -gcflags=\"all=-N -l\" -o main ./...\n\nFROM alpine:3.12.0 AS production\nCOPY --from=builder /app .\nCOPY --from=builder /go/bin/dlv /\nEXPOSE 8000 40000\nENV PORT=8000\nCMD [\"/dlv\", \"--listen=:40000\", \"--headless=true\", \"--api-version=2\", \"--accept-multiclient\", \"exec\", \"./main\"]    \nTo see the dynamic links, build your builder image and run ldd against the output binaries:\n$ docker build --target builder -t test-63403272 .\n[+] Building 4.6s (11/11) FINISHED                                                                                                                                                                                 \n => [internal] load build definition from Dockerfile                                                                                                                                                          0.0s\n => => transferring dockerfile: 570B                                                                                                                                                                          0.0s\n => [internal] load .dockerignore                                                                                                                                                                             0.0s\n => => transferring context: 2B                                                                                                                                                                               0.0s\n => [internal] load metadata for docker.io/library/golang:1.14.7                                                                                                                                              0.2s\n => [builder 1/6] FROM docker.io/library/golang:1.14.7@sha256:1364cfbbcd1a5f38bdf8c814f02ebbd2170c93933415480480104834341f283e                                                                                0.0s\n => [internal] load build context                                                                                                                                                                             0.0s\n => => transferring context: 591B                                                                                                                                                                             0.0s\n => CACHED [builder 2/6] RUN go get github.com/go-delve/delve/cmd/dlv                                                                                                                                         0.0s\n => CACHED [builder 3/6] RUN mkdir /app                                                                                                                                                                       0.0s\n => [builder 4/6] ADD . /app                                                                                                                                                                                  0.1s\n => [builder 5/6] WORKDIR /app                                                                                                                                                                                0.0s\n => [builder 6/6] RUN CGO_ENABLED=0 GOOS=linux go build -gcflags=\"all=-N -l\" -o main ./...                                                                                                                    4.0s\n => exporting to image                                                                                                                                                                                        0.2s\n => => exporting layers                                                                                                                                                                                       0.2s\n => => writing image sha256:d2ca7bbc0bb6659d0623e1b8a3e1e87819d02d0c7f0a0762cffa02601799c35e                                                                                                                  0.0s\n => => naming to docker.io/library/test-63403272                                                                                                                                                              0.0s\n\n$ docker run -it --rm test-63403272 ldd /go/bin/dlv\n        linux-vdso.so.1 (0x00007ffda66ee000)\n        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007faa4824d000)\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007faa4808c000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007faa48274000)\nLibc is a common missing library when switching to alpine since it uses musl by default.",
    "docker : how to share ssh-keys between containers?": "For doing ssh without password you to need to create passwordless user along with configuring SSH keys in the container, plus you will also need to add ssh keys in the sources container plus public key should be added in the authorized of the destination container.\nHere is the working Dockerfile\nFROM openjdk:7\n\nRUN apt-get update && \\\n    apt-get install -y openssh-server vim \n\nEXPOSE 22\n\n\nRUN useradd -rm -d /home/nf2/ -s /bin/bash -g root -G sudo -u 1001 ubuntu\nUSER ubuntu\nWORKDIR /home/ubuntu\n\nRUN mkdir -p /home/nf2/.ssh/ && \\\n    chmod 0700 /home/nf2/.ssh  && \\\n    touch /home/nf2/.ssh/authorized_keys && \\\n    chmod 600 /home/nf2/.ssh/authorized_keys\n\nCOPY ssh-keys/ /keys/\nRUN cat /keys/ssh_test.pub >> /home/nf2/.ssh/authorized_keys\n\nUSER root\nENTRYPOINT service ssh start && bash\ndocker-compose will remain same, here is the testing script that you can try.\n#!/bin/bash\nset -e\necho \"start docker-compose\"\ndocker-compose up -d\necho \"list of containers\"\ndocker-compose ps\necho \"starting ssh test from f-db to f-app\"\ndocker exec -it f-db sh -c \"ssh -i /keys/ssh_test ubuntu@f-app\"\nFor further detail, you can try the above working example docker-container-ssh\ngit clone git@github.com:Adiii717/docker-container-ssh.git\ncd docker-container-ssh; \n./test.sh\nYou can replace the keys as these were used for testing purpose only.",
    "Windows Docker Container has no NAT IP Address. Cannot access container locally": "That's a known bug under docker windows. It is fixed in 19.03. So try updating your docker engine.",
    "How to build a new app from scratch inside a docker container?": "You can follow \"How to use Docker for Node.js development\" by Cody Craven:\nIt does use Docker itself to develop, not just deploy/run a NodeJS application.\nExample:\n# This will use the node:8.11.4-alpine image to run `npm init`\n# with the current directory mounted into the container.\n#\n# Follow the prompts to create your package.json\ndocker run --init --rm -it -v \"${PWD}:/src\" -w /src node:8.11.4-alpine npm init\nThen you can setup an execution environment with:\nFROM node:8.11.4-alpine AS dev\nWORKDIR /usr/src/app\nENV NODE_ENV development\nCOPY . .\n# You could use `yarn install` if you prefer.\nRUN npm install\nAnd build your app:\n# Replace YOUR-NAMESPACE/YOUR-IMAGE with the name you would like to use.\ndocker build -t YOUR-NAMESPACE/YOUR-IMAGE:dev --target dev .\nAnd run it:\n# The `YOUR COMMAND` portion can be replaced with whatever command you\n# would like to use in your container.\ndocker run --rm -it --init -v \"${PWD}:/usr/src/app\" YOUR-NAMESPACE/YOUR-IMAGE:dev YOUR COMMAND\nAll without node installed on your workstation!",
    "Running Java Swing GUI application on Docker": "Did some more research and by hit and trial following code seems to launch the GUI ,there are some errors after that but that must be due to some other issues in the GUI itself:\nFROM openjdk:8\n\n# Set environment\n\nENV JAVA_HOME /opt/jdk\n\nENV PATH ${PATH}:${JAVA_HOME}/bin   \n\n# COPY myJarFolder from local repository to the image\n\nCOPY ./myJarFolder /usr/local/myJarFolder\n\n# Start the image with the jar file as the entrypoint\n\nENTRYPOINT [\"java\", \"-jar\", \"/usr/local/myJarFolder/myJarFile.jar\"]\n\n# EOF",
    "docker container exits on entry points": "Here's what I see happening in this sequence:\nENTRYPOINT [\"bash\", \"/docker-entrypoint.sh\"]\nCMD [\"bash\", \"/home/test/app/start.sh\"]\nWhen you start the container, Docker runs bash /docker-entrypoint.sh bash /home/test/app/start.sh. However, the entrypoint script never looks at its command-line arguments at all, so any CMD you specify here or any command you give at the end of the docker run command line gets completely ignored.\nWhen that entrypoint script runs:\nexec gunicorn ... &\nexec service nginx start\n# end of file\nit starts gunicorn as a background process and continues to the next line; then it replaces itself with a service command. That service command becomes the main container process and has process ID 1. It starts nginx, and immediately returns. Now that the main container process has returned, the container exits.\nFor this code as you've written it, you should delete the exec lines at the end of your entrypoint script and replace them with just\nexec \"$@\"\nwhich will cause the shell to run its command-line parameters (that is, the Dockerfile CMD).\nHowever, there's a readily available nginx Docker image. Generally if you need multiple processes, it's easier and better to run them as two separate containers on the same Docker network. This avoids the complexities around trying to get multiple things running in the same container.",
    "How to run docker:dind to start with a shell": "Just like Andreas Wederbrand said. you can just\ndocker run -it docker:dind sh\nand if you want to use Dockerfile. Just write like this.\nFROM docker:dind\nCMD [\"sh\"]\nIt shouldn't overwrite ENTRYPOINT. You can try to inspect docker:dind image.\ndocker inspect docker:dind\nyou can see entrypoint is a shell script file.\n\"Entrypoint\": [\n  \"dockerd-entrypoint.sh\"\n   ],\nof course, we can find this file in container. get inside the docker\ndocker run -it docker:dind sh\nand then\ncat /usr/local/bin/dockerd-entrypoint.sh\nmore about entrypoint you can see\nhttps://medium.freecodecamp.org/docker-entrypoint-cmd-dockerfile-best-practices-abc591c30e21",
    "How to determine the specific version of a Docker image built from a latest/stable tag?": "As suggested by this answer\ndocker inspect --format='{{index .RepoDigests 0}}' $IMAGE\nThis will give you the sha256 hash of the image.\nThen you can use a service like MicroBadger to get more info about that specific build.\nIf you want to recreate the Dockerfile you can use docker history to examine the layer history:\n$ docker history docker\n\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n3e23a5875458        8 days ago          /bin/sh -c #(nop) ENV LC_ALL=C.UTF-8            0 B\n8578938dd170        8 days ago          /bin/sh -c dpkg-reconfigure locales &&    loc   1.245 MB\nbe51b77efb42        8 days ago          /bin/sh -c apt-get update && apt-get install    338.3 MB\n4b137612be55        6 weeks ago         /bin/sh -c #(nop) ADD jessie.tar.xz in /        121 MB\n750d58736b4b        6 weeks ago         /bin/sh -c #(nop) MAINTAINER Tianon Gravi <ad   0 B\n511136ea3c5a        9 months ago                                                        0 B    \nKeep in mind that if the image has been manually tampered with, I don't know how reliable this output would be.\nFinally if you want to go full hacker mode, this old thread on the Docker community forums has some info.\nI'm not sure how you can get the tag, because I don't believe this is stored in the image itself, but in the repository. So you'd have to query the repository itself, or get a full list of image history and go detective on it.",
    "Running interactive container with power shell": "you need to run your container with the -it switch. this will make you container interactive, so you can poke around\n docker run -it test",
    "Connecting couchdb container from another container": "You forgot to include the port for the connection:\nDB_URL: http://admin:password@couchdb:5984",
    "Docker how to use env_file variable in Dockerfile during build": "You can map the environment variable into the container by adding an environment option in your docker compose yaml file:\nversion: '2'\n\nservices:\n  web:\n    build: .\n    env_file:\n      - /etc/web.env\n    command: python3 manage.py runserver 0.0.0.0:8000\n    volumes:\n      - ./backend:/code\n    ports:\n      - \"8000:8000\"\n    environment:\n      - STAGE: ${STAGE}",
    "How to specify source and target for a VOLUME in Dockerfile?": "You can't mount a VOLUME in a Dockerfile specifying both source (host) and destination (container) paths.\nThis is because this idea does not fit the basic concept that Docker image is portable and independent of host.",
    "Unable to access docker container from the port mapped by docker": "I think your problem is that grunt is binding to localhost:9000 - which is internal to the container so the port you're publishing won't have any effect.\nIt needs to be listening on 0.0.0.0:9000 - I couldn't tell you off hand what your Gruntfile.js should say for that to happen, but off-hand it looks like, out of the box, grunt serve will only serve from localhost.",
    "Cannot call chown inside Docker container (Docker for Windows)": "You have similar issues illustrating the same error message in mongo issues 68 or issue 74\nThe host machine volume directory cannot be under /Users (or ~). Try:\ndocker run --name mongo -p 27017:27017 -v /var/lib/boot2docker/my-mongodb-data/:/data/db -d mongo --storageEngine wiredTiger\nThe PR 470 adds:\nWARNING: because MongoDB uses memory mapped files it is not possible to use it through vboxsf to your host (vbox bug).\nVirtualBox shared folders are not supported by MongoDB (see docs.mongodb.org and related jira.mongodb.org bug).\nThis means that it is not possible with the default setup using Docker Toolbox to run a MongoDB container with the data directory mapped to the host.",
    "how can I add git submodule into git repo as normal directory?": "Seems you cannot do that. The name .git is hard-coded in the source code: https://github.com/git/git/blob/fe9122a35213827348c521a16ffd0cf2652c4ac5/dir.c#L1260\nProbably one way is to make a script which renames .git to something else and back before and after adding it into repo like\nIn working directory under scripts\nmv .git hidden-git\nIn Dockerfile\nRUN mv $JENKINS_HOME/scriptler/scripts/hidden-git     \n$JENKINS_HOME/scriptler/scripts/.git\nAlternatively, probably it's possible to pass GIT_DIR environment variable into the plugin, so it could use another name.",
    "Compatability of Dockerfile RUN Commands Cross-OS (apt-get)": "As I commented in your question, you can add FROM statement to specify which relaying OS you want. for example:\nFROM docker.io/centos:latest\nRUN yum update -y\nRUN yum install -y java\n...\nnow you have to build/create the image with:\ndocker build -t <image-name> .\nThe idea is that you'll use the OS you are familiar with (for example, CentOS) and build an image of it. Now, you can take this image and run it above Ubuntu/CentOS/RHEL/whatever... with\ndocker run -it <image-name> bash\n(You just need to install docker in the desired OS.",
    "Dockerfile production/build/debug/test environment": "The answer might be straightforward: just create 4 Dockerfiles one depending on another.\nYou can add a volume to share build from sources part. The question is whether you want result assets to be included in the image or build it from sources each time.\nCreate 4 folders to have Dockerfile in each.\nProduction\nproduction/Dockefile:\nFROM  # put server here\nCOPY  # put config here\n# some other option\n# volume sharing?\nBuild\nbuild/Dockerfile:\n# install dependencies\nADD # add sources here\nRUN # some building script\nDebug\ndebug/Dockefile:\n# ideally, configure production or build image\nTest\ntest/Dockefile:\nFROM # import production\n# install test dependencies\nRUN # test runner\nThere are also several options. 1. Use .gitignore with negative pattern (or ADD?)\n*\n!directory-i-want-to-add\n!another-directory-i-want-to-add\nPlus use docker command specifying dockerfiles and context:\ndocker build -t my/debug-image -f docker-debug .\ndocker build -t my/serve-image -f docker-serve .\ndocker build -t my/build-image -f docker-build .\ndocker build -t my/test-image -f docker-test .\nYou could also use different gitignore files.\nMount volumes Skip sending context at all, just use mounting volumes during run time (using -v host-dir:/docker-dir).\nSo you'd have to:\ndocker build -t my/build-image -f docker-build . # build `build` image (devtools like gulp, grunt, bundle, npm, etc)\ndocker run -v output:/output my/build-image build-command # copies files to output dir\ndocker build -t my/serve-image -f docker-serve . # build production from output dir\ndocker run my/serve-image # production-like serving from included or mounted dir\ndocker build -t my/serve-image -f docker-debug . # build debug from output dir\ndocker run my/serve-image # debug-like serving (uses build-image with some watch magic)",
    "how to do docker-compose pip install with a proxy?": "Does your change work when doing a docker build manually (not inside composer)?\nOnce you know you have it working with a normal docker build, you should be able to force docker-compose to rebuild using docker-compose build.",
    "Why do some docker images have binary blob layers and some have layer.tar files?": "The first one with layer.tar is docker archive image. The second with blobs/sha256 is OCI image. https://snyk.io/blog/container-image-formats/",
    "Switch USER for single RUN command in Dockerfile": "Or can I at least store some reference to the original USER during the build process somehow?\nYes, with an ENV variable from the parent image, as discussed here: Store and Restore Inherited Dockerfile USER setting\nFrom the Dockerfile reference documentation:\nA stage inherits any environment variables that were set using ENV by its parent stage or any ancestor. Refer here for more on multi-staged builds.\nParent Image:\nENV unprivilegeduser=safeuser\nRUN groupadd -r unprivileged && useradd --no-log-init -r -g unprivileged $unprivilegeduser\nUSER $unprivilegeduser\nDependent image:\n#Switch to root for a single RUN command\nUSER root \nRUN doRootStuff\nUSER $unprivilegeduser\nIt needs three lines instead of a single, elegant \"RUNAS\" command that you wanted, but it is far cleaner than other alternatives like sudo and docker inspect ContainerConfig.User etc.\nAs per the Dockerfile guidance:\nAvoid installing or using sudo as it has unpredictable TTY and signal-forwarding behavior that can cause problems. If you absolutely need functionality similar to sudo, such as initializing the daemon as root but running it as non-root, consider using \u201cgosu\u201d.",
    "Rancher desktop, docker file shell commands are not working on mac m1 chip": "Downgrade to Rancher Desktop 1.4.1.\nThis works but if downgrading is not an option there are workarounds on the Rancher Desktop Issues site for similar problems like this: qemu workaround\nAs a temporary workaround, as root in the VM:\nCreate /etc/conf.d/qemu-binfmt, with contents binfmt_flags=\"POCF\"\nRun rc-update --update\nRun rc-service qemu-binfmt restart\nEasy way to connect to the VM and run those commands is (source):\ndocker run -it --rm --privileged --pid=host justincormack/nsenter1",
    "How to interrupt the building of a Docker container in VSCode?": "When you click the container button on the left bottom corner during the build\ncontainer build button\nit gives this option to close the connection, which kills the build.\nclose the connection",
    "Git Actions Job fails because of Username and Password required": "Please make sure that you have configured secret keys in correct place.\nLink should be:\nhttps://github.com/{github_account}/{project_name}/settings/secrets/actions\nOn this page you should have both DOCKERHUB_TOKEN and DOCKERHUB_USER.\nIf you don't have them, create new token here.",
    "How to solve /bin/sh: 1: source: not found during making docker image in MacOS(Golang)?": "It seems like .env file is not contained in your image.\nTry to execute source .env after copying .env file into the image.",
    "Windows 10 Docker Build Error: \"error from sender: open LogFiles\\WMI\\RtBackup: Access is denied.\"": "I had this issue just now. Oddly, it was fixed when I changed directory one level up.",
    "NestJS: Copying Assets not working on Linux": "I had a similar problem trying to dockerize my project on a Linux host (yarn build on local worked just fine, but assets were not copied in the docker image).\nOriginal Setup\nDockerfile:\nFROM node:16-bullseye AS builder\nWORKDIR /usr/app\nCOPY package.json ./\nCOPY yarn.lock ./\nCOPY src ./\nCOPY nest-cli.json ./\nCOPY tsconfig.build.json ./\nCOPY tsconfig.json ./\nRUN yarn install\nRUN yarn build\n...\nnest-cli.json:\n{\n  ...,\n  \"compilerOptions\": {\n    \"assets: [ \"modules/mails/templates/**/*.hbs\" ],\n    ...\n  }\n}\nFix\nThe paths specified within 'compilerOptions.assets' are always relative to the 'src' folder within the project. Inspecting my docker image I realized that the content of the 'src' folder was being copied in the root of the build folder inside docker.\nWhen the content of 'src' is moved to the root, NestJS is able to build the project successfully, but since assets are looked for within the 'src' folder, no files are copied. Changing this in my Dockerfile fixed the problem:\nCOPY src ./src # Instead of 'COPY src ./'",
    "PermissionError: [Errno 13] Permission denied: '/.cache' - Error occurred while building a docker image": "in your terminal\nmkdir ./.cache\nadd your code\nos.environ['SENTENCE_TRANSFORMERS_HOME'] = './.cache'",
    "Docker - Run Oracle DB image and execute init script": "I have the same problem and as far as I know, files in \"docker-entrypoint-initdb.d\" are executed automatically, is that right? But I'm not sure that is this particular image \"store/oracle/database-enterprise:12.2.0.1-slim\" the scripts placed in this path are executed automatically.",
    "Mount docker volume to host machine path": "Looks like a permission issue.\n-v /codecoveragereports:/app/***/codecoveragereports is mounting a directory under the root / which is dangerous and you may not have the permission.\nIt's better to mount locally, like -v $PWD/codecoveragereports:/app/***/codecoveragereports, where $PWD is an environment variable equal to the current working directory.",
    "Docker With a .Net Core API": "So I was able to get the above working...it partly had to do with the --no-restore, and there's a bug associated with it. To get it to work I did the following:\nFROM microsoft/dotnet:2.1-sdk AS build\nWORKDIR /api\n\nCOPY ./*.sln ./\n# COPY ./NuGet.config /root/.nuget/NuGet/\n\nCOPY src/*/*.csproj ./\nRUN for file in $(ls *.csproj); do mkdir -p src/${file%.*}/ && mv $file src/${file%.*}/; done\n\n# COPY test/*/*.csproj ./\n# RUN for file in $(ls *.csproj); do mkdir -p test/${file%.*}/ && mv $file test/${file%.*}/; done\n\nRUN dotnet restore\n\nCOPY . .\n\nRUN dotnet publish -c Release -o out\n\nFROM microsoft/dotnet:2.1-aspnetcore-runtime AS runtime\nWORKDIR /api\nCOPY --from=build /api/src/api/out .\nENTRYPOINT [\"dotnet\", \"api.dll\"]\nI left the comments in the above code, in case you need an example for tests.",
    "GenerateRuntimeConfigurationFiles task failed unexpectedly": "I think some of the commands in the Dockerfile need to be changed.\nFROM microsoft/dotnet:2.1-sdk AS build-env\nWORKDIR /app\n\nCOPY *.csproj ./\nRUN dotnet restore\n\n# Copy everything else and build\nCOPY . ./\nRUN dotnet publish -c Release -o out\n\n# Build runtime image\nFROM microsoft/dotnet:2.1.1-aspnetcore-runtime\nWORKDIR /app\nCOPY --from=build-env /app/out .\nENTRYPOINT [\"dotnet\", \"MyProject.Api.dll\"]\nReferences:\nhttps://docs.docker.com/engine/examples/dotnetcore/#prerequisites\nhttps://raw.githubusercontent.com/dotnet/dotnet-docker/master/samples/dotnetapp/Dockerfile",
    "How to execute a sql file in Docker Oracle 12?": "Follow this link to install an Oracle 12c Docker image. I installed it on my Mac.\nhttps://dzone.com/articles/creating-an-oracle-database-docker-image\nDownload the Oracle Instant Client which includes SQL* Plus.\nOR\nDownload the latest command line tool SQLcl\nNow, to run a script you may Connect to your database using the easy connect with sqlplus or sql commandline.\nsqlplus user/pwd@//localhost:1521/ORCLPDB1 yourpath/yourscript.sql",
    "run java code on docker": "Try following\nMy env: Mac-Os Sierra-10.12.6\nDocker version:\nDocker version 18.03.0-ce, build 0520e24\nTo Check Java version in container:\nexecute $docker run -it docker-hello-world:latest\nget ContainerID by executing $docker ps\nget shell of docker image docker exec -i -t <CONTAINER ID>\nExecute following command\n$java -version\nopenjdk version \"1.8.0_151\"\nOpenJDK Runtime Environment (IcedTea 3.6.0) (Alpine 8.151.12-r0)\nOpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)\nCreate a directory (anyname here I gave a), I created at Desktop,\ncd ~/Desktop\nmkdir a\ncreate Ave.java file in ~/Desktop/a directory\nAlso, have Dockerfile in the same directory(/a)\nConsidering there is no package name in Ave.java\n$javac Ave.java\nnow folder /a will have 3 files, Ave.java, Ave.class, Dockerfile\nExecute following command\n$docker build -t docker-hello-world:latest .\nConsole Logs:\nSending build context to Docker daemon  5.632kB\nStep 1/4 : FROM alpine:latest\n ---> 3fd9065eaf02\nStep 2/4 : ADD Ave.class Ave.class\n ---> 8b94ae6de674\nStep 3/4 : RUN apk --update add openjdk8-jre\n ---> Running in f12eb4589a34\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz\n(1/39) Installing libffi (3.2.1-r4)\n(2/39) Installing libtasn1 (4.12-r3)\n(3/39) Installing p11-kit (0.23.2-r2)\n(4/39) Installing p11-kit-trust (0.23.2-r2)\n(5/39) Installing ca-certificates (20171114-r0)\n(6/39) Installing java-cacerts (1.0-r0)\n(7/39) Installing libxau (1.0.8-r2)\n(8/39) Installing libbsd (0.8.6-r1)\n(9/39) Installing libxdmcp (1.1.2-r4)\n(10/39) Installing libxcb (1.12-r1)\n(11/39) Installing libx11 (1.6.5-r1)\n(12/39) Installing libxcomposite (0.4.4-r1)\n(13/39) Installing libxext (1.3.3-r2)\n(14/39) Installing libxi (1.7.9-r1)\n(15/39) Installing libxrender (0.9.10-r2)\n(16/39) Installing libxtst (1.2.3-r1)\n(17/39) Installing alsa-lib (1.1.4.1-r2)\n(18/39) Installing libbz2 (1.0.6-r6)\n(19/39) Installing libpng (1.6.34-r1)\n(20/39) Installing freetype (2.8.1-r2)\n(21/39) Installing libgcc (6.4.0-r5)\n(22/39) Installing giflib (5.1.4-r1)\n(23/39) Installing libjpeg-turbo (1.5.2-r0)\n(24/39) Installing libstdc++ (6.4.0-r5)\n(25/39) Installing openjdk8-jre-lib (8.151.12-r0)\n(26/39) Installing java-common (0.1-r0)\n(27/39) Installing krb5-conf (1.0-r1)\n(28/39) Installing libcom_err (1.43.7-r0)\n(29/39) Installing keyutils-libs (1.5.10-r0)\n(30/39) Installing libverto (0.3.0-r0)\n(31/39) Installing krb5-libs (1.15.2-r1)\n(32/39) Installing lcms2 (2.8-r1)\n(33/39) Installing nspr (4.17-r0)\n(34/39) Installing sqlite-libs (3.21.0-r0)\n(35/39) Installing nss (3.34.1-r0)\n(36/39) Installing pcsc-lite-libs (1.8.22-r0)\n(37/39) Installing lksctp-tools (1.0.17-r0)\n(38/39) Installing openjdk8-jre-base (8.151.12-r0)\n(39/39) Installing openjdk8-jre (8.151.12-r0)\nExecuting busybox-1.27.2-r7.trigger\nExecuting ca-certificates-20171114-r0.trigger\nExecuting java-common-0.1-r0.trigger\nOK: 81 MiB in 50 packages\nRemoving intermediate container f12eb4589a34\n ---> 82d9ecfcc95e\nStep 4/4 : ENTRYPOINT [\"java\", \"-Djava.security.egd=file:/dev/./urandom\", \"Ave\"]\n ---> Running in 28f2df6fb544\nRemoving intermediate container 28f2df6fb544\n ---> bbf098575e6a\nSuccessfully built bbf098575e6a\nSuccessfully tagged docker-hello-world:latest\nExecute command as suggested by @Siking\n$docker run -it docker-hello-world:latest\nhere is snapshot of output:",
    "The command '/bin/sh -c bundle install' returned a non-zero code: 10": "I've made these changes in Dockerfile and it works like a charm:\nFROM phusion/passenger-ruby24\n# Set correct environment variables.\nENV HOME /root\n# Use baseimage-docker's init process.\nCMD [\"/sbin/my_init\"]\n# Additional packages: we are adding the netcat package so we can\n# make pings to the database service\nRUN apt-get update && apt-get install -y -o Dpkg::Options::=\"--force-confold\" netcat\n# Enable Nginx and Passenger\nRUN rm -f /etc/service/nginx/down\n# Add virtual host entry for the application. Make sure\n# the file is in the correct path\nRUN rm /etc/nginx/sites-enabled/default\nADD webapp.conf /etc/nginx/sites-enabled/webapp.conf\n# In case we need some environmental variables in Nginx. Make sure\n# the file is in the correct path\nADD rails-env.conf /etc/nginx/main.d/rails-env.conf\n# Install gems: it's better to build an independent layer for the gems\n# so they are cached during builds unless Gemfile changes WORKDIR /tmp\n\nCOPY Gemfile* /tmp/\nWORKDIR /tmp\nRUN bundle install\n\n# Copy application into the container and use right permissions: passenger\n# uses the app user for running the application \n# RUN mkdir /home/me/Desktop/sg/docker-kubernets\nCOPY . /home/me/Desktop/sg/docker-kubernets\nRUN usermod -u 1000 app\nRUN chown -R app:app /home/me/Desktop/sg/docker-kubernets\nWORKDIR /home/me/Desktop/sg/docker-kubernets\n\n# Clean up APT when done.\nRUN apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\nEXPOSE 80\nruby version Ruby 2.4.2p198 (2017-09-14 revision 59899) [x86_64-linux]\nI hope be helpful!\nRegards!",
    "Postgres Shutdown Immediately after docker compose up?": "Add a link between your web and postgres image like so:\nversion: '2'\nservices:\n  postgres:\n    restart: always\n    image: sameersbn/postgresql:9.6-2\n    ports:\n      - \"5432:5432\"\n    environment:\n      - DB_USER=shorturl\n      - DB_PASS=shorturl\n      - DB_NAME=shorturl\n  web:\n    build: .\n    ports:\n     - \"4000:4000\"\n    volumes:\n     - .:/shortlr\n    depends_on:\n     - postgres\n    links:\n     - postgres\n    command: [\"./wait-for-it.sh\",\"postgres:5432\", \"--\", \"npm\", \"start\"]\nAlthough you should use networks instead because links is a legacy option. Read the docs on networks here",
    "Docker 's Error: libselinux conflicts with fakesystemd": "Try running this inside the container you create, before any installation is made:\nyum swap -y fakesystemd systemd && yum clean all\nyum update -y  && yum clean all\nOr inside a Dockerfile at the begining before the first RUN you have tipped:\nRUN yum swap -y fakesystemd systemd && yum clean all \\\n    && yum update -y  && yum clean all\nHope was useful!",
    "Host Volumes Not getting mounted on 'Docker-compose up'": "This shouldn't work for you on OSX, yet alone Debian. Here's why:\nWhen you add ./command.sh to the volume /home/docker/django/django/ the image builds fine, with the file in the correct directory. But when you up the container, you are mounting your local directory \"on top of\" the one you created in the image. So, there is no longer anything there...\nI recommend adding command.sh to a different location, e.g., /opt/django/ or something, and changing your docker command to ./opt/command.sh.\nOr more simply, something like this, here's the full code:\n# Dockerfile\nFROM python:2.7\nRUN mkdir -p /home/docker/django/\nWORKDIR /home/docker/django/\n\n# docker-compose.yml\ndjango:\n    build: django\n    command: ./command.sh\n    volumes:\n        - ./django/:/home/docker/django/",
    "Dockerfile and issue making work bower install": "Are you perhaps mounting a volume from your host into the container through your docker-compose.yml file or the -v parameter in your docker run command?\nI ask because you mentioned you don't have bower installed on your machine, so you presumably don't have the bower_components folder in the host copy of your codebase. If you then run your container and mount the volume containing the source on your host into the container then the bower_components folder would be lost, explaining why it seems to disappear from the container.\nI would first test to see if this is indeed the problem by not mounting any volumes from your host and seeing if the bower_components folder then remains after you build and run the container. You could then install bower on your host and run bower install so the mounted volume doesn't cause issues while you're in development.",
    "Environment variables with double asterisks in Dockerfile": "It's non-official convention to use these variables as template variables. They will be replaced in run-time.\nOr you can replace them using -e switch of docker run.\nFor example:\nENV MYSQL_USER admin\nENV MYSQL_PASS **Random**\n\n# Replication ENV\nENV REPLICATION_MASTER **False**\nENV REPLICATION_SLAVE **False**\nIf you take a look on start script you can see the following:\nif [ \"$MYSQL_PASS\" = \"**Random**\" ]; then\n    unset MYSQL_PASS\nfi\n\nPASS=${MYSQL_PASS:-$(pwgen -s 12 1)}\nIf variable value is **Random** let's replace it with a randomly generated password.",
    "How to detect if the current script is running in a docker build?": "Try this :\n#!/bin/bash\n\n# myscript.sh\n\nisDocker(){\n    local cgroup=/proc/1/cgroup\n    test -f $cgroup && [[ \"$(<$cgroup)\" = *:cpuset:/docker/* ]]\n}\n\nisDockerBuildkit(){\n    local cgroup=/proc/1/cgroup\n    test -f $cgroup && [[ \"$(<$cgroup)\" = *:cpuset:/docker/buildkit/* ]]\n}\n\nisDockerContainer(){\n    [ -e /.dockerenv ]\n}\n\nif isDockerBuildkit || (isDocker && ! isDockerContainer)\nthen\n  echo \"I am in a docker build\"\nelse\n  echo \"I am not in a docker build\"\nfi",
    "How to specify user and group owner of a mounted device in Docker container?": "I found a hacky solution: I installed sudo in the container and allowed the non-root user to execute chgrp without password, then added sudo chgrp myuser /dev/kvm to my entrypoint script.\nHere an example for kvm access for jenkins:\nFROM jenkins/jenkins:2.339-jdk11\nUSER root\n\nRUN apt-get update && apt-get install -y sudo && \\ \n    rm -rf /var/lib/apt/lists/* && \\\n    echo \"jenkins ALL = (root) NOPASSWD: /bin/chgrp\" >> /etc/sudoers\n\nUSER jenkins\n\nCOPY initscript.sh /initscript.sh\n\nENTRYPOINT [ \"/initscript.sh\" ]\nAnd the initscript file\n#!/bin/bash\nsudo chgrp jenkins /dev/kvm\n/sbin/tini -- /usr/local/bin/jenkins.sh\nmaybe this works for your setup, too.",
    "Change hostname from within a Windows container": "The following worked for me -\nrename-computer.ps1:\n$ComputerName = \"New Name\"\n   \nRemove-ItemProperty -path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters\" -name \"Hostname\" \nRemove-ItemProperty -path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters\" -name \"NV Hostname\" \n\nSet-ItemProperty -path \"HKLM:\\SYSTEM\\CurrentControlSet\\Control\\Computername\\Computername\" -name \"Computername\" -value $ComputerName\nSet-ItemProperty -path \"HKLM:\\SYSTEM\\CurrentControlSet\\Control\\Computername\\ActiveComputername\" -name \"Computername\" -value $ComputerName\nSet-ItemProperty -path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters\" -name \"Hostname\" -value $ComputerName\nSet-ItemProperty -path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters\" -name \"NV Hostname\" -value  $ComputerName\nSet-ItemProperty -path \"HKLM:\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\" -name \"AltDefaultDomainName\" -value $ComputerName\nSet-ItemProperty -path \"HKLM:\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\" -name \"DefaultDomainName\" -value $ComputerName\nDockerfile:\nCOPY \"rename-computer.ps1\" \"C:/\"  \nRUN powershell -command \"Set-ExecutionPolicy RemoteSigned\" \\\n && powershell -command \"C:/rename-computer.ps1 ; hostname\"  \\\n && powershell -command \"Set-ExecutionPolicy Restricted\" \\\n && <your installer runs here> \nSources -\nhttps://serverfault.com/a/731832/845133\nhttps://gist.github.com/timnew/2373475",
    "\"No matching distribution found for fastapi \" using pip in Ubuntu Docker": "In my case I got this error because I forgot to configure my proxy. As soon as I did\nexport http_proxy=[proxy info]\nexport https_proxy=[proxy info]\nthe error was resolved.\nNote that this may happen even if some of your packages are installing fine. For example, my NPM proxy was configured separately, so it wasn't affected.",
    "Why do I still need to do COPY in my Dockerfile if I'm using a Bind Mount?": "This is an old question, but if anyone else runs into this problem in the future . . .\n@camba1's answer is close, but not correct.\nData Persistence through docker compose down:\nBind Mounts: WILL PERSIST in mount location\nCOPY INTO: WILL PERSIST in image\nData Persistence through Image Deletion:\nBind Mounts: WILL PERSIST in mount location\nCOPY INTO: WILL NOT PERSIST\nData was stored directly in image and will be lost when removed\nTheoretically, your code should work as it is, and there should not be a need for a redundant COPY INTO command.\nMy best guess, since you are using the chown flag in your COPY INTO command, is that there is a file permission issue with accessing the data as the node user/group without the COPY INTO explicitly setting the R/W/X permissions/ownership.\nTo quickly test this, you could try to lower the permissions (safely and for debugging only) on the bound directories directly so that the node user may access them.",
    "Docker Alpine Linux python (missing)": "From this issue on the Docker's repo:\nThis was \"broken\" while updating our base from alpine:3.11 to alpine:3.12.\nIn order to fix it you need to specify the version of Python directly, e.g.:\napk add python2\n// or\napk add python3",
    "Installing ruby on rails by using Dockerfile \"Could not locate gemfile\" during build": "When you build it, make sure you are in your Rails project root folder. You should already have run rails new your-project-name in this folder to generate this.\nThe command ONBUILD ADD . /opt/app will copy the current folder recursively into the Docker image, so it must already contain Gemfile and all other parts of the Rails project.",
    "Force step to be cached in Dockerfile": "Docker's layer cache essentially says \"if I start from image 01234567, and RUN some command, then I will get image 2468ace0\". If something has changed and you now have image 13579bdf instead, there's no way to short-circuit this besides running the command again.\nThere's a couple of approaches that can help mitigate this:\nRun heavy-weight commands that don't actually depend on the application code early in your Dockerfile. RUN apt-get install before you COPY ..\nMinimize the amount of stuff in one image. Don't try to put two separate applications with lengthy builds in the same image.\nUse multi-stage builds, where each stage can independently take advantage of the cache, and you'll have a relatively cheap COPY into the final image.\nPre-compile some part of the sequence on the host, and COPY the result into the image rather than RUN lengthy-install. (This was an extremely useful technique before there were multi-stage builds, and is still relevant.)",
    "Docker socket permissions within container": "Simple change UID of the user,\nRUN useradd -ms /bin/bash -d /usr/local/myuser -u ${UID} myuser\nwhere UID can be taken via\nid \nin terminal",
    "docker build inside docker build": "I don't think this is possible. Even if it was, it sounds like it might be overcomplicated.\nIf you'd like to do something a little better than a DinD (Docker-in-Docker) - you can try building your images with buildah which does not require a Docker daemon (hence - is safer) to build an image and performs better (at least for me) than what I saw with DinD scenarios.",
    "Trying to secure Nginx with Let's Encrypt but can't access .pem files in /etc/letsencrypt/live using a non-root user": "I do not think that is a best practice. You should not access your droplet with a root user either\nYou should setup the droplet with a new user with sudo grants, and remove root access completely, as explained in this guide: https://www.digitalocean.com/community/tutorials/initial-server-setup-with-ubuntu-16-04\nThen you should verify the droplet and store the SSL certificates using certbot nginx plugin, like explained here:\nhttps://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-16-04\nStep 5 of the last guide explains the renew certificate part.",
    "How to connect a Docker container of Grafana to a Docker conatiner of MySql?": "As stated here:\ngo-mysql: authentication plugin not supported while connecting from go app container to mysql container\nThe problem was compatibility between the Grafana versions and the MySQL version. Once moving to docker image mysql:5.7 (had to migrate the data too) - the issue was resolved (need to also change the COLLATION and CHAR_SET in the DB to downgrade from version 8.0.12 to 5.7).",
    "Check if Docker image exists in cloud repo": "This script will build only if image not exist.\nupdate for V2\nfunction docker_tag_exists() {\n    curl --silent -f -lSL https://hub.docker.com/v2/repositories/$1/tags/$2 > /dev/null \n}\nUse above function for v2\n#!/bin/bash\nfunction docker_tag_exists() {\n    curl --silent -f -lSL https://index.docker.io/v1/repositories/$1/tags/$2 > /dev/null \n\n}\n\nif docker_tag_exists library/node 9.11.2-jessie; then\n    echo \"Docker image exist,....\"\n    echo \"pulling existing docker...\"\n    #so docker image exist pull the docker image\n    docker pull node:9.11.2-jessie\nelse \n    echo \"Docker image not exist remotly....\"\n    echo \"Building docker image...\"\n    #build docker image here with absoult or retlative path\n    docker build -t nodejs .\n\nfi\nWith little modification from the link below. If the registry is private u check this link With username and password",
    "how to install anaconda / miniconda on Linux silently": "can be achieved by bash miniconda.sh -b (thanks @darthbith)\nThe command line usage for this can only be seen with -h flag but not --help, so I missed it.\nTo install the anaconda to another place, use the -p option:\nbash anaconda.sh -b -p /some/path",
    "Static code analysis of Dockerfiles?": "Although this question is 2 years old, however there are two ways to do static analysis of the Dockerfile.\nusing FromLatest\nusing Hadolint\nOption#2 is mostly preferable since this can be used as an automated process inside CICD pipelines.\nHadolint also provide ways to exclude messages/errors using \".hadolint.yml\"",
    "Jekyll-Assets: Missing assets when building site": "TL;DR: Deleting _site before build caused the problem. Also deleting .jekyll-cache before build solved the problem for me.\nI had a very similar experience with Jekyll-assets. During development it occurs that some images were missing in the _site/assets folder. After a bit trying out I found a way to reproduce it.\nCheckout a clean version of the code\nBuild with bundle exec Jekyll build\nRun bundle exec jekyll clean (which deletes, among others, the _site folder)\nBuild with bundle exec Jekyll build again\nAfter these steps I had missing files in the _site/assets folder. What solved the problem for me was additional to using the jekyll clean to delete the folder .jekyll-cache. I quiet not deep enough into jekyll-assets to understand how the caching in this folder works, but at least it solves the problem. If the folder is excluded from version control, and in case it's a generated cache folder it should be, you can also use git clean -xdf but be careful with this, it deletes any file, that is not under version control! I would be grateful if you could verify my solution according to your concrete case.\nI'm sorry that I could not provide you a real explanation for why this happens, but I hope that my answer could help you anyway.",
    "spring boot application in docker terminates immediately": "I don't find any reason to use anapsix/alpine-java:latest\nI recommand you to use the openjdk:8-jre-alpine, light weight and less in size comparitively.\ncreate a war files instead of jar.\nuse the following\nDockerfile\nFROM openjdk:8-jre-alpine\n\nENV SPRING_OUTPUT_ANSI_ENABLED=ALWAYS \\\n    SLEEP_TIME=0 \\\n    JAVA_OPTS=\"\"\n\n# add directly the war\nADD *.war /app.war\n\nEXPOSE 8080\nCMD echo \"The application will start in ${SLEEP_TIME}s...\" && \\\n    sleep ${SLEEP_TIME} && \\\n    java ${JAVA_OPTS} -Djava.security.egd=file:/dev/./urandom -jar /app.war",
    "Starting a service during Docker build": "These *ctl programs usually come in with a few utilities to start / stop / monitor the status of the service.\nIf your case I think the best idea is to have a simple bash script you can run at build time that does this:\nstart ejabberd\nmonitor the status at intervals\nif the status of the process is up, run your command\nLook at this:\nroot@158479dec020:/# ejabberdctl status\nFailed RPC connection to the node ejabberd@158479dec020: nodedown\nroot@158479dec020:/# echo $?\n3\nroot@158479dec020:/# ejabberdctl start \nroot@158479dec020:/# echo $?\n0\nroot@158479dec020:/# ejabberdctl status\nThe node ejabberd@158479dec020 is started with status: started\nejabberd 16.01 is running in that node\nroot@158479dec020:/# echo $?\n0\nroot@158479dec020:/# ejabberdctl stop  \nroot@158479dec020:/# echo $?\n0\nroot@158479dec020:/# ejabberdctl status\nFailed RPC connection to the node ejabberd@158479dec020: nodedown\nroot@158479dec020:/# echo $?\n3\nSo this tells us that if you run a ejabberd status and the daemon is not running you receive exit code 3, 0 if it's up and running instead.\nThere you go with your bash script:\nfunction run() {\n  ejabberdctl start # Repeating just in case...\n  ejabberdctl status &>/dev/null\n\n  if [ $? -eq 0 ]; then\n    echo \"Do some magic here, ejabberd is running...\"\n    exit 0\n  fi \n\n  echo \"Ejabberd still down...\"\n}\n\nwhile true; do run; sleep 1; done\nAnd this is what you'd get at the CLI:\nroot@158479dec020:/# ./check.sh \nEjabberd still down...\nDo some magic here, ejabberd is running...\nroot@158479dec020:/# ejabberdctl stop\nroot@158479dec020:/# ./check.sh \nEjabberd still down...\nEjabberd still down...\nDo some magic here, ejabberd is running...",
    "Using ENTRYPOINT in a Dockerfile makes the base image's CMD be ignored": "See https://github.com/moby/moby/issues/5147 - it explains when and why this behaviour was implemented.",
    "Install Docker in Alpine Docker": "I managed to do that the easy way\ndocker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock -v /usr/bin/docker:/usr/bin/docker --privileged docker:dind sh\nI am using this command on my test env!",
    "Docker mount an directory of container to another container": "If files already exist in the directory of the container you're mounting as a volume, the contents of the directory will be copied to the volume. See Populate a volume using a container .\nThere's no way to mount a container's directory as a volume without copying. An alternative is to not store the data in a container and instead create a volume on the host and populate it with the data. This way, the data will only exist on once.",
    "docker-compose up giving error": "I had the same unhelpful error when I tried to run docker-compose on Windows from a project I created in Linux. If you clone a Linux project on Windows, by default Git adds Windows style line endings which mess up any Bash scripts. If this is the case run git config --global core.autocrlf false to configure Git to never change the line endings. Then delete repository and re-clone it.\nAlternatively if you created the project initially on Windows, many times the IDE adds Windows style line endings. If this is the case then in Notepad++, Click edit menu > EOL Conversion > Unix.",
    "How to handle make missing from alpine docker image?": "If you added RUN apk add --no-cache make and still there is an issue, add this as well to your DockerFile:\nRUN apk add g++\nAlpine images are lightweight and do not have utilities in them, adding g++ solves that. You can check the output by using the which make command, for me it returns /usr/bin/make\nRef: https://mroldan.medium.com/alpine-sh-make-not-found-1e87ab87c56",
    "Docker build does not proceed after finishing RUN statement": "I just ran into this same problem myself. Trying to install a Yocto SDK in a docker image. The script appears to be stuck after it completes. It turns out you need to be patient. I let an installation run sit while I ran an errand and I came back to a successfully installed SDK. Unfortunately, running with -D to enable debugging did not reveal why the installation takes as long as it does.",
    "Dockerfile build from parent directory": "There is no problem. In Dockerfile you cant get out (access parent folder) of build context, not Dockerfile's folder.\nLeave you structure as it was, and explicitly specify path to Dockerfile:\ndocker build -t app -f application-build/Dockerfile . \ndocker build -t celery -f celery-build/Dockerfile . \nAnd inside your Dockerfiles remember that path is /application. So you can easily copy:\nDockerfile\n...\nCOPY code /code\n...",
    "docker multistage build fails with multiple --build-arg": "The reason is because IMAGE_TWO is not in the same scope check this https://docs.docker.com/engine/reference/builder/#scope\nBasically the ARG IMAGE_TWO is still part of the first stage and goes out of scope when that stage ends and will not be part of the second stage.\nDeclaring the arguments at the beginning allow the IMAGE_TWO to be in the second stage.\nARG IMAGE_ONE\nARG IMAGE_TWO\nFROM ${IMAGE_ONE}\nRUN cat /etc/debian_version\n\nFROM ${IMAGE_TWO}\nRUN cat /etc/debian_version\ndocker build --build-arg=IMAGE_ONE=debian:7 --build-arg=IMAGE_TWO=debian:8 .",
    "What happens to files in docker container when host is shut down ungracefully?": "The files are deleted. Containers are ephemeral in nature. When the container starts it creates a new writable layer on top of the image, and whatever changes you make are on that layer. If you are not storing/mapping that layer data to some persistent location, then once the container or Docker daemon or host machine restarts all the data is lost.",
    "Docker container keeps stopping after 'docker start'": "Your Docker image doesn\u2019t actually do anything, container stop when finish its job. Since here no foreground process running it will start and then immediately stop. To confirm your container have no issues, try to put below code into a docker-compose.yml(in same folder as the Dockerfile) and run docker-compose up, now you will see your container is running without exiting.\nversion: '3'\nservices:\n  my-service:\n    build: .\n    tty: true    \nPlease have a look here Docker official tutorial it will guide you to how to work with docker.",
    "How to fix \"Illegal option\" error when compiling a dockerfile?": "After a lot of research, I found the solution.\nThe problem is that Windows uses \\r\\n as an end of line, whereas unix only uses \\n. So, inside my download.sh file, there was a ^M character that led to the error /bin/sh: illegal option\nThe solution was to copy the code from the downlod.sh file and convert it to LF:\nUse the text editor such as Notepad++ at the Windows machine to convert\nGo to menu Edit -> EOL Conversion -> Unix (LF)\nOnce that is done, the docker build works correctly.\nThank you all for the support.",
    "Docker: no such option: --use-wheel": "Your issue isn't due to missing dependencies ( wheel is installed in the build.sh script you referenced: https://github.com/ryansb/sklearn-build-lambda/blob/master/build.sh#L18 )\nuse-wheel was deprecated and no longer exists for pip.\nYou can achieve the same by omitting the --use-wheel entries from the script. Take a look at the Python 3.6 PR on the linked repository: https://github.com/ryansb/sklearn-build-lambda/pull/16/files#diff-0b83f9dedf40d7356e5ca147a077acb4",
    "How to set mysql system variables such as max_allowed_packet max_allowed_packet, character_set_server and collation-server through Dockerfile?": "There are several approaches, this is one:\nCMD [\"--character-set-server=utf8mb4\", \"--collation-server=utf8mb4_unicode_ci\", \"--max-allowed-packet=1073741824\"]\nAs an alternative you can put a custom config file in /etc/mysql/conf.d\ncustom-mysql.cnf:\n[mysqld]\nmax_allowed_packet = 1073741824\ncharacter-set-server = utf8mb4\ncollation-server = utf8mb4_unicode_ci\nDockerfile:\nFROM mysql:latest\nCOPY ./custom-mysql.cnf /etc/mysql/conf.d/\nMore here",
    "Dockerfile for tomcat": "This is what i did to solve this:\nDockerfile\nFROM tomcat\n\nMAINTAINER richard\n\nRUN apt-get update && apt-get -y upgrade\n\nWORKDIR /usr/local/tomcat\n\nCOPY tomcat-users.xml /usr/local/tomcat/conf/tomcat-users.xml\nCOPY context.xml /usr/local/tomcat/webapps/manager/META-INF/context.xml\n\nEXPOSE 8080\nI'm copying those two files in order to access the manager app from outside. If you want it too, add the following to your context and tomcat-users files\nContext.xml\n<Context antiResourceLocking=\"false\" privileged=\"true\" >\n    <!-- <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\"\n        allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\" /> -->\n    <Manager sessionAttributeValueClassNameFilter=\"java\\.lang\\.(?:Boolean|Integer|Long|Number|String)|org\\.apache\\.catalina\\.filters\\.CsrfPreventionFilter\\$LruCache(?:\\$1)?|java\\.util\\.(?:Linked)?HashMap\"/>\n</Context>\ntomcat-users.xml\n<tomcat-users xmlns=\"http://tomcat.apache.org/xml\"\n              xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n              xsi:schemaLocation=\"http://tomcat.apache.org/xml tomcat-users.xsd\"\n              version=\"1.0\">\n  <user username=\"admin\" password=\"secret\" roles=\"manager-gui\"/>\n</tomcat-users>\nThen you can build it and run it:\ndocker build -t name/tomcat .\ndocker run -d -p 8080:8080 --name some_name name/tomcat\nDeploy your application as follows:\ndocker cp some/app.war some_name:/usr/local/tomcat/webapps/app.war",
    "Trying to start the kubernetes in Docker-Desktop but it's stuck": "Just follow these steps\nstop docker for desktop\nremove the folder ~/Library/Group\\ Containers/group.com.docker/pki\n   rm -rf ~/Library/Group\\ Containers/group.com.docker/pki\nstart docker for destkop\nFound the solution here\nAnd given that every time I try to start \"Docker for Desktop\" Kubernetes get stuck, to better investigate for me was useful remove the kube autostart from the configuration.\nJust edit the file:\n~/Library/Group\\ Containers/group.com.docker/settings.json\nAnd change kubernetesEnabled value to false",
    "How to edit docker mysql image and increase connection limit": "you can run the container like this:\ndocker run -d -e \"MYSQL_ROOT_PASSWORD=test\" mysql --max_connections=10000\nor make your own Dockerfile:\nFROM mysql\nCMD [ \"--max_connections=10000\" ]\nthen build it:\ndocker build -t my_mysql .\nand run it:\ndocker run -d -e \"MYSQL_ROOT_PASSWORD=test\" my_mysql",
    "how can i pass arguments or bypass it in docker build process? [duplicate]": "You could set the environment variables DEBIAN_FRONTEND=noninteractive and DEBCONF_NONINTERACTIVE_SEEN=true in your Dockerfile, before RUN sudo apt-get install php libapache2-mod-php -y.\nYour Dockerfile should look like this:\nFROM ubuntu:18.04\n\n\nRUN apt-get update && \\\n       apt-get install -y --no-install-recommends apt-utils && \\\n       apt-get -y install sudo\n\nRUN sudo apt-get install apache2 -y\nRUN sudo apt-get install mysql-server -y\n\n\n## for apt to be noninteractive\nENV DEBIAN_FRONTEND noninteractive\nENV DEBCONF_NONINTERACTIVE_SEEN true\n\n## preesed tzdata, update package index, upgrade packages and install needed software\nRUN echo \"tzdata tzdata/Areas select Europe\" > /tmp/preseed.txt; \\\n    echo \"tzdata tzdata/Zones/Europe select Berlin\" >> /tmp/preseed.txt; \\\n    debconf-set-selections /tmp/preseed.txt && \\\n    apt-get update && \\\n    apt-get install -y tzdata\n\n\n\nRUN sudo apt-get install php libapache2-mod-php -y\nRUN rm -rf /var/www/html/\nCOPY . /var/www/html/\nWORKDIR /var/www/html/\nEXPOSE 80\nRUN chmod -R 777 /var/www/html/app/tmp/\n\nCMD systemctl restart apache2\nYou should change Europe and Berlin with wath you want.",
    "How to keep an infinite loop running in order to not close a container in docker": "You can do this by putting the commands you want to execute into a script, and setting the script to be the command Docker runs when it starts a container:\nFROM sixeyed/ubuntu-with-utils\n\nRUN echo 'ping localhost &' > /bootstrap.sh\nRUN echo 'sleep infinity' >> /bootstrap.sh\nRUN chmod +x /bootstrap.sh\n\nCMD /bootstrap.sh\nWhen you build an image from this Dockerfile and run a container from the image, it will start ping in the background and sleep in the foreground, so you can daemonize the container with docker run -d and it will keep running.\nThis is not ideal though - Docker only monitors the last process it started when it ran the container, so it will be checking on sleep rather than ping. If the ping command errors the container will keep running. Typically, you want the real application to be the only thing you start in the CMD.",
    "openssh-server doesn't start in Docker container": "When building a Dockerfile you would create an image. But you can't create an image with an already running ssh daemon or any running service else. First if you create a running container out of the image you can start services inside. E.g. by appending the start instruction to the docker run command:\nsudo docker run -d mysshserver service ssh start\nYou can define a default command for your docker image with CMD. Here is an example Dockerfile:\nFROM ubuntu:14.04.1\nMAINTAINER Thomas Steinbach\nEXPOSE 22\nRUN apt-get install -y openssh-server\nCMD service ssh start && while true; do sleep 3000; done\nYou can build an run this image with the following two commands:\nsudo docker build -t sshtest .\nsudo docker run -d -P --name ssht sshtest\nNow you can connect to this container via ssh. Note that in the example Dockerfile no user and no login was created. This image is just for example and you can start an ssh connection to it, but not login.",
    "How to set port to run static website as nginx docker container?": "The problem is that are assuming just using EXPOSE 3499 changes the port on your nginx config. The nginx is still running on port 80 within the container.\nEXPOSE is to say your intentions, that the image does intend to expose 3499. But the server inside needs to be configured to make sure the port it listens on is the same.\nSince you are using the official docker nginx image, you can read it's documentation below\nhttps://hub.docker.com/_/nginx\nUsing environment variables in nginx configuration\nOut-of-the-box, nginx doesn't support environment variables inside most configuration blocks. But envsubst may be used as a workaround if you need to generate your nginx configuration dynamically before nginx starts.\nHere is an example using docker-compose.yml:\nweb:\n  image: nginx\n  volumes:\n   - ./mysite.template:/etc/nginx/conf.d/mysite.template\n  ports:\n   - \"8080:80\"\n  environment:\n   - NGINX_HOST=foobar.com\n   - NGINX_PORT=80\n  command: /bin/bash -c \"envsubst < /etc/nginx/conf.d/mysite.template > /etc/nginx/conf.d/default.conf && exec nginx -g 'daemon off;'\"\nThe mysite.template file may then contain variable references like this:\nlisten ${NGINX_PORT};\nSo as you see, you need to do some work in the image to make sure your nginx actually listens on 3499\nUpdate: 23rd July 2019\nIn case you don't want to do it using environment variable. Then you can overwrite the file using docker-compose\nSo you will keep a local file default.conf where you will change the contents\nserver {\n    listen       3499;\n    server_name  localhost;\n\n    #charset koi8-r;\n    #access_log  /var/log/nginx/host.access.log  main;\n\n    location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n    }\n}\nAnd mount this file in your docker-compose.yml\nweb:\n  image: nginx\n  volumes:\n   - ./default.conf:/etc/nginx/conf.d/default.conf\n  ports:\n   - \"8080:3499\"\nAnd that will make sure that the file is correct. You can even copy that in the Dockerfile, if you don't want to override at run-time",
    "Error: Starting container process caused \"exec: \\\"/docker-entrypoint.sh\\\": permission denied\"": "That is most likely a Linux file permission issue on config/docker-entrypoint.sh. If your host is Linux/Mac, you can run:\nchmod 755 config/docker-entrypoint.sh\nFor more on linux permissions, here's a helpful article: https://www.linux.com/learn/understanding-linux-file-permissions",
    "How can I set an alias in the .bashrc of my Docker image": "simply use RUN to add an alias to bash_profile.\nFROM ubuntu\nMAINTAINER Mojtaba Yeganeh\n\nRUN echo \"alias python=/home/user/python3.6\" >> ~/.bash_profile",
    "Docker - MySQL commands within Dockerfile using RUN (ERROR 2002)": "I think the issue might be that in the service hasn't started within the container used to build your Dockerfile.\nTry starting and configuring MySQL server within a single step. As a reference please check this file: https://github.com/dockerfile/mysql/blob/master/Dockerfile",
    "How can I automatically run a docker container on/after image build?": "What a Dockerfile does, is create an image not a container.\nWhen you do docker build -t <tagname> . and will see the result (tagged) if you run docker images.\nYou can then run that image to create a container.\nYou can also specify a CMD to run (by default) in the Dockerfile. That would be (I think)\nCMD [ \"catalina.sh\", \"run\" ] \nSo:\ndocker build -t my_tomcat_app .\ndocker run -d -p 8888:8080 --name my_tomcat_container my_tomcat_app\nThis will create a container from your image, and run whatever the CMD said. You can poke around inside it with docker exec\ndocker exec -it my_tomcat_container bash\nBut it'll stay running (because of the -d flag) and show up in docker ps.",
    "Docker Compose: exposing ports only to other containers": "Just don't publish ports: for the thing you don't want accessible from outside Docker space.\nversion: '3.8'\nservices:\n  backend:\n    build: .\n    # no ports:\n  proxy:\n    image: nginx\n    volumes: ['./nginx.conf:/etc/nginx/nginx.conf']\n    ports:\n      - '80:80'\nYour Nginx configuration can still proxy_pass http://backend:8080, but the Flask application won't be directly reachable from outside of Docker.\nports: specifically allows a connection from outside Docker into the container. It's not a required option. Connections between containers don't require ports:, and if ports: remap one of the application ports, that remapping is ignored.\nTechnically this setup only allows connections from within the same Docker network, so if you had other services in this setup they could also reach the Flask container. On a native-Linux host there are tricks to directly contact the container, but they don't work on other host OSes or from another system. IME usually these limitations are acceptable.",
    "Next JS app builds in the local but failed inside Docker": "There's an issue importing images in Next 12.1.6 with node 18.\nhttps://github.com/vercel/next.js/issues/38020\nAs noted in the comments, npm i sharp will likely resolve the issue.",
    "Cannot find module '/dist/main' when using docker multi-stage builds": "the main.js file in dist is located on /dist/src/main.js",
    "Docker build image failed? Failed to compute cache key: \"/app\" not found: not found": "The APP folder does not exist in your project\nYou are trying to add an APP folder that does not exist into your docker container and it is complaining because there is no APP folder to add into the container\nIn the line\nCOPY ./APP /APP\nThe ./APP must reflect a folder that is in your project",
    "Way to force docker to accept and proceed with building the image with a non zero response/code": "yum check-update is expected to exit with status 100 if updates are available, as described in its documentation:\ncheck-update\nImplemented so you could know if your machine had any updates that needed to be applied without running it interactively. Returns exit value of 100 if there are packages available for an update. Also returns a list of the packages to be updated in list format. Returns 0 if no packages are available for update. Returns 1 if an error occurred. Running in verbose mode also shows obsoletes.\nSimilarly, the docker RUN command is expected to terminate on any nonzero exit status. If you want to force the command to ignore an exit status of 100 (but still treat other failures as erroneous), you can do so as follows:\nRUN yum -y check-update || { rc=$?; [ \"$rc\" -eq 100 ] && exit 0; exit \"$rc\"; }\nThat the Docker RUN command treats any nonzero exit status as a failure is standard UNIX convention (the only successful exit status is 0), and is explicitly implemented in dockerfile/containerbackend.go:\nif status := <-waitC; status.ExitCode() != 0 {\n    close(finished)\n    logCancellationError(cancelErrCh,\n        fmt.Sprintf(\"a non-zero code from ContainerWait: %d\", status.ExitCode()))\n    return &statusCodeError{code: status.ExitCode(), err: status.Err()}\n}",
    "what is docker run -w flag?": "Just run docker run --help in your shell.\n-w, --workdir string                 Working directory inside the container\nhttps://docs.docker.com/engine/reference/run/#workdir",
    "Error installing nodejs version 12 on alpine linux": "You should not install nodejs-current, as this package is helpful to install *current version of nodejs from edge repository where nodejs version does not exist.\nIn your case, nodejs 12.x package already exists so You should install nodejs if you want to install an older version instead of nodejs-current.\nFROM alpine:3.9\nENV ALPINE_MIRROR \"http://dl-cdn.alpinelinux.org/alpine\"\nRUN echo \"${ALPINE_MIRROR}/v3.11/main/\" >> /etc/apk/repositories\nRUN apk add nodejs --repository=\"http://dl-cdn.alpinelinux.org/alpine/v3.11/main/\"\nRUN node --version\noutput\nRemoving intermediate container a201832610e0\n ---> b0919df78aef\nStep 5/5 : RUN node --version\n ---> Running in cd7950f9303b\nv12.15.0\nRemoving intermediate container cd7950f9303b\n ---> ce54af976f81\nSuccessfully built ce54af976f81",
    "Web container cannot call backend container from docker compose": "The important thing for this setup is that your actual front-end code is not running in Docker, it's running in your browser. That means it has no idea about Docker networking, containers, or anything else; the URL you give it has to be one that reaches a published port on your host. That's why localhost works here (if the browser and containers are running on the same host) but backend doesn't.\nA typical approach to this is to set up some sort of reverse proxy that can both host the front-end application code and proxy to the back-end application. (For example, set up Nginx where its /api route proxy_pass http://backend:8098, and its / route either try_files a prebuilt Javascript application or proxy_pass http://frontend:8080.)\nIf you do this, then e.g. http://localhost:8900 is the front-end and http://localhost:8900/api is the back-end, from the browser's point of view. This avoids the CORS issues @coedycode hints at in their answer; but it also means that the front-end code can use a relative URL /api (with no host name) and dodge this whole problem.\n+-------------+                  | Docker >           /     +----------+\n+-------------+                  |                 /------> | frontend |\n|             |  localhost:8900  |    +-------+    |        +----------+\n|   Browser   | ---------------> | -> | nginx | -> +\n|             |                  |    +-------+    | /api   +----------+\n|             |                  |                 \\------> | backend  |\n+-------------+                  |                          +----------+",
    "In a dockerfile, can we have multiple RUN commands into one RUN command?": "Yes you can and its a good practice\nInstead of doing this\nRUN python -m pip install --upgrade pip\nRUN python -m pip install --upgrade setuptools\nRUN pip install -r requirements.txt \nTry this\nRUN python -m pip install --upgrade pip &&\\\n    python -m pip install --upgrade setuptools &&\\\n    pip install -r requirements.txt \nAdvantages with that approach\nEach instruction in the Dockerfile adds an extra layer to the docker image The number of instructions and layers should be kept to a minimum as it ultimately affects the build performance and time",
    "Do Dockerfile RUN commands run in a login shell environment?": "to get the same environment as what they would run in?\nI understand \"As they would run by default without specfifying a command\"\nThat will simply depend on how your container is actually configured to run:\nif the ENTRYPOINT or CMD is configured to run a login shell, you should use a login shell\nif the ENTRYPOINT or CMD is configured to run a non-login shell, you should use a non-login shell\nYou can identify this by running docker inspect on your container or docker image inspect which will give you ENTRYPOINT and CMD\nSame principle if you first run the container then create a shell using docker exec -it bash [--login]\nFor example, using this Dockerfile:\nFROM alpine\n\nRUN apk add bash\nRUN echo \"export MYVAR=frombashrc\" > /root/.bashrc\nRUN echo \"export MYVAR=fromprofile\" > /root/.bash_profile\n\nENTRYPOINT [\"/bin/sh\", \"-c\"]\nAnd running:\n$ docker build . -t mybashimage\n$ docker run -it --name bashcontainer mybashimage \"bash --login -c 'env && sleep 60'\"\nHOSTNAME=4aeb776a8c56\nMYVAR=fromprofile\n...\nIn another shell while container is running:\n# Running a non-login shell does not have same effect\n$ docker exec -it bashcontainer bash\nbash-4.4# env\nHOSTNAME=5f44398152bf\nMYVAR=frombashrc\n...\n\n# Running login shell doe\n$ docker exec -it bashcontainer bash --login -c 'env'\nHOSTNAME=5f44398152bf\nMYVAR=fromprofile\n...",
    "docker build: Returned a non-zero code: 5": "It is most likely failing on the wget command. That RUN line has 6 separate commands. The other 5 would all likely produce some error message if they failed. The wget has the --quiet flag passed which is suppressing output.\nTo debug further, try removing the --quiet flag from the wget.\nFrom man wget, EXIT STATUS of 5 = SSL verification error. Perhaps you are behind a corporate proxy and do not have the SSL certs installed in the container that is running the wget.",
    "Why does docker-compose create an empty host folder when mounting named volume?": "As an implementation detail, Docker actually uses the Linux kernel filesystem mount facility whenever it mounts a volume. To mount a volume it has to be mounted on to a directory, so if the mount target doesn't already exist, it creates a new empty directory to be the mount point. If the mount point is itself inside a mounted volume, you'll see the empty directory get created, but the mount won't get echoed out.\n(If you're on a Linux host, try running mount in a shell while the container is running.)\nThat is:\n/container_root/app is a bind mount to /host_path/app; they are they same underlying files.\nmkdir /container_root/app/node_modules creates /host_path/app/node_modules too.\nMounting something else on /container_root/app/node_modules doesn't cause anything to be mounted on /host_path/app/node_modules.\n...which leaves an empty /host_path/app/node_modules directory.\nThe first time you start a container, and only then, if you mount an empty volume into a container, the contents from the image get copied into the volume. You're telling Docker this directory contains critical data that needs to be persisted for longer than the lifespan of the container. It is not a magic \"don't use the host directory volume\" knob, and if you do things like change your package.json file, Docker will not update the contents of this volume.",
    "'RUN pip install -r requirements.txt' not working": "Several issues I'm seeing:\nThe ADD command you use creates a file called ToDoApp/ToDoApp, it doesn't even create a sub directory.\nADD is unneeded (you're not extracting a tar or downloading from a URL) so that can switch to a COPY.\nYou need to copy your code.\nThe RUN commands can be reordered for better cache efficiency.\nUse relative paths and the WORKDIR correctly.\nArgs need to be separated when you use the json syntax\nThe resulting Dockerfile looks like:\nFROM python:3\n\n#set envionment variables\nENV PYTHONUNBUFFERED 1\n\n# run this before copying requirements for cache efficiency\nRUN pip install --upgrade pip\n\n#set work directory early so remaining paths can be relative\nWORKDIR /ToDoApp\n\n# Adding requirements file to current directory\n# just this file first to cache the pip install step when code changes\nCOPY requirements.txt .\n\n#install dependencies\nRUN pip install -r requirements.txt\n\n# copy code itself from context to image\nCOPY . .\n\n# run from working directory, and separate args in the json syntax\nCMD [\"python\", \"./manage.py\", \"runserver\", \"0.0.0.0:8000\"]",
    "Docker Build Error : The command '/bin/sh -c apt-get install \u2013y apache2' returned a non-zero code: 100": "I'm not sure how, but I think you're using an en dash instead of a hyphen in front of your y.\nYou want -y rather than \u2013y\nIf you look closely there's a subtle difference.",
    "What is the purpose of Dockerfile command \"Volume\"?": "Defining a volume in a Dockerfile has the advantage of specifying the volume location inside the image definition as documentation from the image creator to the user of the image. That's just about the only upside.\nIt was added to docker very early on, quite possibly when data containers were the only way to persist data. We now have a solution for named volumes that has obsoleted data containers. We have also added the compose file to define how containers are run in an easy to understand and reuse syntax.\nWhile there is the one upside of self documented images, there are quite a few downsides, to the point that I strongly recommend against defining a volume inside the image to my clients and anyone publishing images for general reuse:\nThe volume is forced on the end user, there's no way to undefine a volume in the image.\nIf the volume is not defined at runtime (with a -v or compose file), the user will see anonymous volumes in their docker volume ls that have no association to what created them. These are almost always useless wastes of disk space.\nThey break the ability to extend the image since any changes to a volume in an image after the VOLUME line are typically ignored by docker. This means a user can never add their own initial volume data, which is very confusing because docker gives no warning that it is ignoring the user changes during the image build.\nIf you need to have a volume as a user a runtime, you can always define it with a -v or compose file, even if that volume is not defined in the Dockerfile. Many users have the misconception that you must define it in the image to be able to make it a named volume at runtime.\nThe ability to use --volumes-from is unaffected by defining the volume in the image, but I'd encourage you to avoid this capability. It does not exist in swarm mode, and you can get all the same capabilities along with more granularity by using a named volume that you mount in two containers.",
    "How do you kill a docker containers default command without killing the entire container?": "If you need to restart the process that's running the container, then simply run a:\ndocker restart $container_name_or_id\nExec'ing into a container shouldn't be needed for normal operations, consider that a debugging tool.\nRather than changing the script that gets run to automatically restart, I'd move that out to the docker engine so it's visible if your container is crashing:\ndocker run --restart=unless-stopped ...\nWhen a container is run with the above option, docker will restart it for you, unless you intentionally run a docker stop on the container.\nAs for why killing pid 1 in the container shuts it down, it's the same as killing pid 1 on a linux server. If you kill init/systemd, the box will go down. Inside the namespace of the container, similar rules apply and cannot be changed.",
    "How to Dockerize a tomcat app": "Looking at your code this is what I could gleam:\nYou have some java files stored in current directory (.)\nWhen you call COPY you copy all these contents to /app_name\nYou create a .war on the file\nThere are some things to note, first is that the app_name.war is not on the host disk, it is currently inside of the docker file system. What this means is that you cannot COPY the .war.\nWhat you are really after is this: RUN cp app_name.war  $CATALINA_BASE/webapps/app_name.war\nThis would look like the following: Dockerfile\nFROM tomcat:6\nENV APP_ROOT /app_name\nRUN apt-get update && apt-get install -y default-jdk\nCOPY . $APP_ROOT/\nWORKDIR $APP_ROOT\nRUN jar -cvf app_name.war *\nRUN cp app_name.war $CATALINA_BASE/webapps/app_name.war\nAdding the docker COPY reference here as it explains the command in detail. It might also be helpful for you to make a script called provision.sh, then do something like:\nCOPY provision.sh /tmp/provision.sh\nRUN sh /tmp/provision.sh\nThat way you can put all your building, configuring and other in a single script that you can test locally (again if it helps)\nEDIT: Adding mention about building locally and copying into dockerfile\nYou can build the .war on your machine, use COPY to put is on the machine. Dockerfile\nFROM tomcat:6\nENV APP_ROOT /app_name\nRUN apt-get update && apt-get install -y default-jdk\n\nCOPY app_name.war $CATALINA_BASE/webapps/app_name.war\nWORKDIR $APP_ROOT\nThe above copies the file app_name.war then add it to the filesystem of the container at the path $CATALINA_BASE/webapps/app_name.war. So for that you do this:\nBuild the .war on your machine with java\nPut .war in directory with Dockerfile\nCOPY app_name.war into the container's filesystem",
    "If docker uses the underlying linux os, why specify the OS in the FROM line of a Dockerfile": "I'm confused, though, as to why most Dockerfiles specify the OS in the FROM line of the Dockerfile. I thought that as it was using the underlying OS, then the OS wouldn't have to be defined.\nI think your terminology may be a little confused.\nDocker indeed uses the host kernel, because Docker is nothing but a way of isolating processes running on the host (that is, it's not any sort of virtualization, and it can't run a different operating system).\nHowever, the filesystem visible inside the container has nothing to do with the host. A Docker container can run programs from any Linux distribution. So if I am on a Fedora 24 Host, I can build a container that uses an Ubuntu 14.04 userspace by starting my Dockerfile with:\nFROM ubuntu:14.04\nProcesses running in this container are still running on the host kernel, but there entire userspace comes from the Ubuntu distribution. This isn't another \"operating system\" -- it's still the same Linux kernel -- but it is a completely separate filesystem.\nThe fact that my host is running a different kernel version than maybe you would find in an actual Ubuntu 14.04 host is almost irrelevant. There are going to be a few utilities that expect a particular kernel version, but most applications just don't care as long as the kernel is \"recent enough\".\nSo no, there is no virtualization in Docker. Just various (processes, filesystem, networking, etc) sorts of isolation.",
    "Docker Compose port issue. Cannot launch docker project on localhost": "The service you're running is only listening to the containerlocalhost interface, so nothing outside the container can access it. It needs to listen on 0.0.0.0.",
    "ASPNET Core 3.1 - Dockerfile changes don\u00b4t work when run in Visual Studio": "Nothing like reading the official documentation...\nhttps://learn.microsoft.com/en-us/visualstudio/containers/container-build?view=vs-2019#debugging\nDebugging\nWhen building in Debug configuration, there are several optimizations that Visual Studio does that help with the performance of the build process for containerized projects. The build process for containerized apps is not as straightforward as simply following the steps outlined in the Dockerfile. Building in a container is much slower than building on the local machine. So, when you build in the Debug configuration, Visual Studio actually builds your projects on the local machine, and then shares the output folder to the container using volume mounting. A build with this optimization enabled is called a Fast mode build.\nIn Fast mode, Visual Studio calls docker build with an argument that tells Docker to build only the base stage. Visual Studio handles the rest of the process without regard to the contents of the Dockerfile. So, when you modify your Dockerfile, such as to customize the container environment or install additional dependencies, you should put your modifications in the first stage. Any custom steps placed in the Dockerfile's build, publish, or final stages will not be executed.\nThis performance optimization only occurs when you build in the Debug configuration. In the Release configuration, the build occurs in the container as specified in the Dockerfile.\nI put RUN touch test.txt and ENV my_variable value in the first lines (where Dockerfile build base stage) and it works.\nA tip refers to RUN touch test.txt, this needs to be executed (WORKDIR) in another folder different from the folder mapped with the source code in the hostlocal (/app).\nExample:\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base\nWORKDIR /tmp\nRUN touch teste.txt\nWORKDIR /app\nENV my_variable value\nEXPOSE 80\nEXPOSE 443\nMy final Dockerfile is:\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base\nWORKDIR /tmp\nRUN touch teste.txt\nRUN echo \"teste1\" > teste1.txt\nWORKDIR /app\nENV my_variable value\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/core/sdk:3.1-buster AS build\nWORKDIR /src\nCOPY [\"MyTestProject/TestProject.csproj\", \"MyTestProject/\"]\nRUN dotnet restore \"MyTestProject/TestProject.csproj\"\nCOPY . .\nWORKDIR \"/src/MyTestProject\"\nRUN dotnet build \"TestProject.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"TestProject.csproj\" -c Release -o /app/publish\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENV my_variable value\nRUN touch teste.txt\nENTRYPOINT [\"dotnet\", \"TestProject.dll\"]",
    "Docker Swarm : docker stack deploy results on untagged images <none>": "This is normal. When using Swarm Stacks, it converts your tag into the SHA256 of that image and downloads that specific ID. This guarantees each node has the same image. If it only depended on the tag, you could potentially re-use the tag in a docker push, and it's possible for different nodes to download different images.\nYou can see the tag used in a docker service ls or docker stack ps <stackname>",
    "File Upload by docker container": "Problem\nYou're expecting your code inside the docker container to locate a file in your host's filesystem. A docker container is a fully contained(as the name suggests) runtime environment with its private filesystem, network, etc. You can not directly access the host's filesystem from your docker container unless you have some requisite configurations (mounting) in your docker run .. command or the compose file.\nSolution\nYou'll have to mount the volume /home/storage inside the docker container to read/write from/to that location from within the container. To do so, after changing the filesystem permissions in your host (which you have already), use the following run command to start the container:\ndocker run -d --name file-upload-service -v /home/storage:/home/storage -p 9104:9104 file-upload-service:latest\nThe -v flag in the above command tells docker to mount the /home/storage directory of the host machine (the left side of :) to the /home/storage directory of the container (the right side of :). You can change these directories according to your use case.",
    "run neo4j with docker-compose - neo4j not accessible from localhost:7474": "Adding network_mode: \"bridge\" to the docker-compose.yml file and accessing to the docker-machine ip the image works correctly\ndocker-compose.yml\nversion: '3'\n\nservices:\n  neo4j:\n    image: neo4j:latest\n    network_mode: \"bridge\"\n    ports:\n      - \"7474:7474\"\n      - \"7687:7687\"\n    environment:\n      - NEO4J_dbms_security_procedures_unrestricted=apoc.*\n      - NEO4J_apoc_import_file_enabled=true\n      - NEO4J_dbms_shell_enabled=true\n    volumes:\n      - ./plugins:/plugins\n      - ./data:/data\n      - ./import:/import",
    "\"From ubuntu\" in docker container": "The statment FROM ubuntu:14.04 means use the ubuntu image as a base image. The ubuntu image is not an OS. This image \"mimics\" an Ubuntu OS, in the sense that it has a very similar filesystem structure to an Ubuntu os and has many tools available that are typically found on Ubuntu.\nThe main and fundamental difference is that the Docker Ubuntu image does not have it own linux kernel. It uses the kernel of the host machine where the container is running.\nMoreover, the size difference between the Docker image (73MB) and an Ubuntu iso(around 1Gb) is very significant.",
    "How to add a custom CA-Certificate on an extended (node.js) docker image": "Why not just switch user to root to run the command to add the cert then switch back?\nFROM nodered/node-red-docker\n\nADD DigiCertCA.crt /usr/local/share/ca-certificates/\nUSER root\nRUN update-ca-certificates\nUSER node-red\n\n\nADD settings.js /data/settings.js\n\nRUN npm install node-red-contrib-ttn\nRUN npm install node-red-contrib-influxdb\nRUN npm install node-red-admin\nRUN npm install node-red-node-geohash \n\nCMD [\"npm\", \"start\", \"--\", \"--userDir\", \"/data\"]",
    "docker: failed to register layer: re-exec error: exit status 1: output: ProcessBaseLayer . The system cannot find the path specified": "You have selected Windows Containers at the time of setup rather than Linux Containers so only Windows-based images will run. To resolve this, right-click the tray icon and select 'Switch to Linux Containers' and it will start working.",
    "Neo4j + Docker - unable to create JVM": "I ran into this same issue. Turned out to be a the line endings on the neo4j.conf file. I used the VS code to switch the line endings to 'LF' and ran docker-compose up and everything worked out. Hope this helps.\nVisual Studio Code: How to show line endings",
    "Prevent volume creation on docker run": "Not once it's be added to the Dockerfile. I'd personally avoid doing volumes inside of the Dockerfile since it ends up creating anonymous volumes as you've seen, but also because it breaks attempts to modify that directory in child images or even later steps inside of the same Dockerfile.",
    "\"ACCES: permission denied\" while trying to upload a file on Strapi": "I cannot really replicate this problem, but as a Strapi and Docker enthusiast I can think of some of the causes.\nIn Strapi you have to generate a token which you'll use to run requests towards the endpoints. You can have either a public token (only GET requests, meant for the public) and a private token (meant to upload, modify or delete stuff). So, probably, you are using a bad token.\nOr, the FTP system where you are trying to upload the file has some permission problems on the output folder. Maybe the FTP is inside the Docker Container itself? In this case, it could be easy to fix. Entering the container with the docker exec -it <container_id> /bin/bash command, you can give -R 777 permissions on the output folder (which is /opt/app/public/uploads as it's printed in logs).\nProbably the problem is this line of code in your Dockerfile:\nRUN chown -R node:node /opt/app\nMaybe this node is not the user who is uploading files to /opt/app? Or you have to launch this command with sudo? (I know it's nearly impossible inside a Dockerfile)\nAlso, as I saw in this post, it could be a problem of how you installed npm dependencies. Try running sudo npm install -g --unsafe-perm=true --allow-root in the Dockerfile.\nHope it was useful enough!",
    "How to build docker image for multiple platforms with cross-compile?": "Both of your images were built for your build platform rather than the target platform with this line:\nFROM --platform=$BUILDPLATFORM ...\nYou don't want --platform=$BUILDPLATFORM on the target stage. This is used on the intermediate stages that either cross compile or output non-platform specific results that you can copy into the target stage.\nMore details on these variables defined by buildkit is available at: https://docs.docker.com/engine/reference/builder/#automatic-platform-args-in-the-global-scope\nI'd also change your RUN step to a COPY step, which eliminates the need for any emulation on the build server. This means running:\necho \"console.log('hello world!');\" > main.js\nin the same directory with your Dockerfile, to create the main.js file in your build context. Then change the Dockerfile to have:\nFROM node:18-alpine as node\nWORKDIR /usr/src/app\nCOPY main.js .\nEXPOSE 4000\nCMD [ \"node\", \"main.js\" ]\nWith that you can build multi-platform images using your original docker buildx build command.",
    "Docker COPY recursive --chmod --chown": "Short answer: You have to actually give exec permissions to your folder and files by replacing your chmod with --chmod=700. And you can look up more details below or just google `Linux file permissions.\nLong answer: So i recreated your example and as you can see bellow i have some scripts that i'm running in various stages before my server actually starts. I'm following good production docker practices as you do i can see and i also use your method to change permissions on the scripts file.\n    \n# App files\nCOPY --chown=node:node --from=builder /usr/src/app/build ./build\n# Migration files\nCOPY --chown=node:node ./migrations ./migrations\n# Scripts\nCOPY --chown=node:node --chmod=600 ./scripts ./scripts\n# Client\nCOPY --chown=node:node ./client ./build/client\nEXPOSE 80\nAnd no surprise, i get a similar error.\n    sh: ./scripts/wait-for.sh: Permission denied\n    npm ERR! code ELIFECYCLE\n    npm ERR! errno 126\n    npm ERR! webpage-server@0.0.1 preprod: `./scripts/wait-for.sh page-db:5432 -- npm run migrate -- --config build/src/config/database.json -e production `\n    npm ERR! Exit status 126\nThe problem here is the code you are giving to your chmod. Your goal is to make them executable and for security reasons make them executable for only the node(default) docker user. Now these decimal numbers we give to our chmod are just sugar to our eyes, in reality, these will be converted to binary and assigned to every one of the permission variables for the directory and its child files/folders(in this case). You have 4 values in the Linux file system permissions, one bit to indicate file or directory, 3 bit to define user access to file/dir, another 3 for the group of users and 3 more for everyone else, you want to give execute permissions to the current user so you will need 111=7 so the final chmod will look more like this:\n    # App files\n    COPY --chown=node:node --from=builder /usr/src/app/build ./build\n    # Migration files\n    COPY --chown=node:node ./migrations ./migrations\n    # Scripts\n    COPY --chown=node:node --chmod=700 ./scripts ./scripts\n    # Client\n    COPY --chown=node:node ./client ./build/client\n    EXPOSE 80\nAnd as you can see the problem no longer persists\n    > webpage-server@0.0.1 preprod /usr/src/app\n    > ./scripts/wait-for.sh page-db:5432 -- npm run migrate -- --config build/src/config/database.json -e production \n    \n    \n    > webpage-server@0.0.1 migrate /usr/src/app\n    > db-migrate up \"--config\" \"build/src/config/database.json\" \"-e\" \"production\"\n    \n    [INFO] No migrations to run\n    [INFO] Done\n    \n    > webpage-server@0.0.1 prod /usr/src/app\n    > npm run start\n    \n    \n    > webpage-server@0.0.1 start /usr/src/app\n    > node build/src/server.js\n    \n    18:28:17 info: Initializing thanos webpage Server version: 0.0.1. Environment: development\n    18:28:17 info: /usr/src/app/build/src\n    18:28:17 info: Thanos web page server is listening on port -> 80",
    "Rust in Docker image: exec no such file or directory": "My problem was, that I need to make sure that Rust compiles into a static binary. It looks like MUSL is one way to do that.\nThis is now my updated Dockerfile:\n# Build: docker build --platform linux/arm64/v8 -t dasralph/ping:arm64_0.1.0 --push .\n# Run: docker run -p 8080:8080 ping\n# Test: curl http://localhost:8080/\n\n# STAGE 1 is to build the binary\n# Use rust-based image for container\nFROM rust:1.61.0-alpine AS builder\n\n# Adding necessary packages\nRUN apk update\nRUN apk add pkgconfig openssl openssl-dev musl-dev\n\nRUN rustup target add aarch64-unknown-linux-musl\nRUN rustup toolchain install stable-aarch64-unknown-linux-musl\n\n# Set working directory in container; make directory if not exists\nRUN mkdir -p /usr/src/ping\nWORKDIR /usr/src/ping\n\n# Copy all files from local computer to container\nCOPY Cargo.toml .\nCOPY Cargo.lock .\nCOPY .env.docker .env\nCOPY src src\n\n# Build release application\nRUN cargo build --target aarch64-unknown-linux-musl --release\n\n\n# STAGE 2 is to have smallest image possible by including only necessary binary\n# Use smallest base image\nFROM shinsenter/scratch\n\n# Copy application binary from STAGE 1 image to STAGE 2 image\nCOPY --from=builder /usr/src/ping/target/aarch64-unknown-linux-musl/release/ping /\n\nEXPOSE 8080\n\nENTRYPOINT [\"/ping\"]",
    "How can I ignore dockerfile non-zero return codes?": "You could use || to bypass the exit code. Something like next:\nOld Dockerfile:\nFROM ubuntu:16.04\nRUN lll\nRUN pwd\nOld execution:\n$ docker build -t abc:1 .\nSending build context to Docker daemon  2.048kB\nStep 1/3 : FROM ubuntu:16.04\n ---> 065cf14a189c\nStep 2/3 : RUN lll\n ---> Running in b7f9a2fd7f6d\n/bin/sh: 1: lll: not found\nThe command '/bin/sh -c lll' returned a non-zero code: 127\nNew Dockerfile:\nFROM ubuntu:16.04\nRUN lll || :\nRUN pwd\nNew execution:\n$ docker build -t abc:1 .\nSending build context to Docker daemon  2.048kB\nStep 1/3 : FROM ubuntu:16.04\n ---> 065cf14a189c\nStep 2/3 : RUN lll || :\n ---> Running in 9a0b3ebea003\n/bin/sh: 1: lll: not found\nRemoving intermediate container 9a0b3ebea003\n ---> af76014cf9aa\nStep 3/3 : RUN pwd\n ---> Running in a4766055c81e\n/\nRemoving intermediate container a4766055c81e\n ---> c82c077ed8ea\nSuccessfully built c82c077ed8ea\nSuccessfully tagged abc:1\nExplain for cmd1 || cmd2:\nIf cmd1 exit code is zero, the cmd2 will not execute, if cmd1 exit code is non-zero, the cmd2 will run. Here, : means a empty command which will result in zero exit code, then it will cheat docker build to let it not exit.\nSo, for you it could be:\nRUN mega-login ${email} ${password} || :",
    "docker-compose python: can't open file './main.py': [Errno 2] No such file or directory": "The problem is in the lines.\nCOPY . /proxy-scraper-checker-master\nCMD [ \"python\", \"main.py\" ]\nYou are copying the entire directory into /proxy-scraper-checker-master, so then your main.py file would be /proxy-scraper-checker-master/proxy-scraper-checker-master/main.py.\nTo debug this, you can enter a bash terminal within the container and look around the directory structure to find main.py. docker-compose will have built the image, so you can find the image name with docker images, or you can rebuild it.\njakub@dash:/tmp/so$ docker build --tag my_python .\njakub@dash:/tmp/so$ docker run --rm -it my_python bash\n# At this point, we are inside the Docker container.\nroot@924a7f854119:/proxy-scraper-checker-master# pwd\n/proxy-scraper-checker-master\nroot@924a7f854119:/proxy-scraper-checker-master# ls\nDockerfile  docker-compose.yml  proxy-scraper-checker-master  requirements.txt\nroot@924a7f854119:/proxy-scraper-checker-master# realpath proxy-scraper-checker-master/main.py \n/proxy-scraper-checker-master/proxy-scraper-checker-master/main.py\nAt this point, we have found the path to main.py. To fix the original issue, we can change the CMD in the Dockerfile to the following:\nCMD [ \"python\", \"/proxy-scraper-checker-master/proxy-scraper-checker-master/main.py\" ]\nWe can improve the Dockerfile to remove redundant COPY instructions. Using the below Dockerfile, the absolute path to main.py is /app/proxy-scraper-checker-master/main.py. We can reference it with the relative path proxy-scraper-checker-master/main.py because we are currently in /app (thanks to the WORKDIR instruction). We could also reference it with the absolute path.\nFROM python:3.8-slim\n\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends libc-dev\n\nWORKDIR /app\n\nCOPY . .\n\nRUN pip install --no-cache-dir -r requirements.txt\n\nCMD [ \"python\", \"proxy-scraper-checker-master/main.py\" ]\nI recommend reading the COPY documentation to understand its behavior.",
    "Docker CMD - when should shell form be used?": "Shell form will invoke a command shell and do the usual command processing that the shell typically handles (like substitution of environment variables such as $HOME). The exec form doesn't do that.\nThat is closely related to the SHELL directive.\nYou can have multiple SHELL commands in the Dockerfile, but only one CMD. CMD is used to specify what the container should run when it starts. The SHELL directive will overwrite the default shell that is used by the shell-form of various commands (RUN, CMD, ENTRYPOINT).\nUsing this Dockerfile illustrates this better than I could explain it:\nFROM python:3.6\nRUN echo $PATH\nSHELL [\"/bin/bash\" ,\"-c\"]\nRUN echo $PATH\nRUN [\"echo\", \"$PATH\"]\nCOPY run.sh /run.sh\n\nENTRYPOINT [\"/run.sh\"]\nWill result in this when running docker build:\n$ docker build .\n\nSending build context to Docker daemon   5.12kB\nStep 1/7 : FROM python:3.6\n ---> 5bf410ee7bb2\nStep 2/7 : RUN echo $PATH\n ---> Running in 3a08d7c4450c\n/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nRemoving intermediate container 3a08d7c4450c\n ---> 85b4da5d8e5d\nStep 3/7 : SHELL [\"/bin/bash\" ,\"-c\"]\n ---> Running in da1b90ac14f2\nRemoving intermediate container da1b90ac14f2\n ---> ed747f0862a6\nStep 4/7 : RUN echo $PATH\n ---> Running in 5c6a86e133ff\n/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nRemoving intermediate container 5c6a86e133ff\n ---> 8ec42f23d390\nStep 5/7 : RUN [\"echo\", \"$PATH\"]\n ---> Running in cc0650a6d8e8\n$PATH\nRemoving intermediate container cc0650a6d8e8\n ---> 8b11432adb3a\nStep 6/7 : COPY run.sh /run.sh\n ---> a168c58738e7\nStep 7/7 : ENTRYPOINT [\"/run.sh\"]\n ---> Running in f9e28048d139\nRemoving intermediate container f9e28048d139\n ---> d20920ea562c\nSuccessfully built d20920ea562c\nNotice that when it ran the shell mode commands (using both the default shell and bash), $PATH was expanded, but not when run using exec mode.",
    "Can't modify files created in docker container": "By default, Docker containers runs as root. This has two issues:\nIn development as you can see, the files are owned by root, which is often not what you want.\nIn production this is a security risk (https://pythonspeed.com/articles/root-capabilities-docker-security/).\nFor development purposes, docker run --user $(id -u) yourimage or the Compose example given in the other answer will match the user to your host user.\nFor production, you'll want to create a user inside the image; see the page linked above for details.",
    "Can't install Python math library in Docker image": "math is a built in library for python. You don't need to install it. Just remove it from requirements.txt.\nIf you are having a similar problem importing other Python modules in a Docker image using the requirements.txt file, make sure it is not one of Python's many, many other built-in functions. The complete list is here: https://docs.python.org/3.8/py-modindex.html\n(Select the appropriate version from the dropdown menu at the top of the page.)",
    "Docker images lacking /tmp?": "I had the same error as you and as BMitch suggested, adding this layer in my Dockerfile fixed it for me.\nRUN chmod 1777 /tmp"
}